<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
        <title>Explore</title>
    </head>
    <body>
        <h1>Explore DHQ</h1>
		<p>In 2024 DHQ implemented an article recommendation feature, designed to help readers navigate the journal and find current and past articles on particular topics. In keeping with the journal's experimental nature, we offer multiple sets of recommended articles, based on different ways of identifying similarities between articles. These methods are complementary, in that the articles recommended by each approach will often be different, encouraging distinct paths for the reader to explore. You can read more about these methods below. We encourage readers to experiment with all of them! We also welcome <a href="https://docs.google.com/forms/d/e/1FAIpQLSf5_iMIvqnu5vfulGlwanoYoyM4njnzaVrGHyzNwoVYqLK0-g/viewform?usp=sf_link">feedback via a short survey</a> on how the methods perform for different readers and in different domains. Information gathered through this survey will also be used to support DHQ's research on recommendation methods.</p>
    	<p>The article recommendation data is currently updated monthly. Because articles are published on a rolling basis, there may be a short period before newly published articles appear in the recommendations. Recommendations for any given article will change each time the index is updated. This is largely because of the addition of new articles, but in the case of shared keywords, it is also partially due to the randomization process used to break ties between articles with the same number of shared keywords. Finally, we currently offer only five recommendations from each method, but in the future we plan to make the entire set of recommendations visible for exploration.
</p>
    	
    	<h2>Document embeddings</h2>
    	<p>As one approach, article recommendations are generated based on article titles and abstracts using transformers, namely, the Allen Institute for Artificial Intelligence's <a href="https://github.com/allenai/specter">SPECTER</a> model. SPECTER has been trained on a massive amount of diverse scientific papers to learn how similar they are and make recommendations based on those similarities. Using the pre-trained <a href="https://huggingface.co/allenai/specter">HuggingFace implementation</a> of SPECTER, we generated embeddings for our similar papers task. The title and abstract for each paper are concatenated and treated as the textual input. (A technical overview of SPECTER can be found in <a href="https://arxiv.org/abs/2004.07180">SPECTER: Document-level Representation Learning using Citation-informed Transformers</a>.) Because this approach relies on the language in the title and abstract, and the semantic connections between terms they include, the recommendations generated using this approach tend to reveal connections that transcend disciplinary areas – for example, articles focusing on machine learning, or on social justice issues, might cluster together even though they might not be categorized together using the shared keywords approach described below.</p>
    	
    	<h2>Shared keywords</h2>
    	<p>In contrast to the document embedding approach, which uses titles and abstracts for similarity assessment, shared keyword recommendations are based on the direct overlap between the <a href="https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords">DHQ-assigned keywords</a> of each article. To support this feature, the DHQ editors manually assign multiple topic keywords to each article, which are stored in the DHQ metadata. Article recommendations are then generated by identifying, for each article, the other DHQ articles that share the highest number of DHQ-assigned keywords. In cases where multiple articles have the same number of shared keywords, a random selection is made from those articles each time the recommendations are regenerated. (The alternative would be to display all of the candidate articles, but in some cases there might be a very high number of these, which would be difficult to display. A planned feature for the future will be to offer readers the ability to browse the entire list of related articles.)</p>
    	<p>The DHQ topic keyword list is designed around concepts rather than specific terms, recognizing that there are often multiple terms for the same broad concept area. To better support discovery and to simplify the keywording process, DHQ uses topic keywords to identify broad concept areas for each article (e.g. “code studies”, “cultural heritage”) rather than trying to determine which more precise term might best apply in a specific case (“critical code studies”, “software studies”). In the future, the more specific terms may be used in advanced searching, and they can also be updated and expanded as usage changes. This approach also lays the foundation for multilingual keywording in the future, since it separates concepts from language-specific wordings.</p>
    	<p>The topic keywords list was developed by DHQ starting from keyword lists associated with the calls for proposals from the conferences of the Association for Computers and the Humanities (ACH) and the Alliance of Digital Humanities Organizations (ADHO), with consolidation and expansion to accommodate the topical range of DHQ articles. The journal also invites authors to contribute keywords for their own articles. These typically have more local (or field-specific) relevance and are not used directly in the recommendations, but the DHQ editors review them regularly to discover terms and concepts that should be added to the DHQ-wide keywording system.
</p>
    	<p>We <a href="mailto:dhqinfo@digitalhumanities.org">welcome recommendations</a> for additional keywords that should appear on the DHQ list, and also recommendations for keywords that should be applied to specific articles. </p>
    	
    	<h2>Term frequency-inverse document frequency</h2>
    	<p>The third recommendation system we offer uses the Term Frequency-Inverse Document Frequency (TF-IDF) approach. TF-IDF is a measure of how important specific words are in a document, adjusted for the overall frequency distribution of words. Using this method, we discover similarities between articles by focusing on words that are rare across the entire corpus but appear frequently within related articles.  Specifically, we are using the <a href="https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html">Best Matching 25 (BM25) algorithm</a>. This approach evaluates the full text of articles—including titles, abstracts, and body texts—to compute similarity scores that reflect the frequency and importance of words across documents. Compared to a vanilla TF-IDF, BM25 adjusts for term frequency and document length, allowing it to handle the natural variation in document sizes and the informativeness of terms within DHQ. 
</p>
    	<p>Through this method, readers can discover articles that share in-depth textual similarities, often revealing nuanced connections that might not be apparent through keyword or embedding-based methods alone. 
</p>
        	
    </body>
</html>
