<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt><!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>first name(s) <dhq:family>family name</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation/>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110--></idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date><!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND"><!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords"><!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors--><!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp=""/>
            </keywords>
            <keywords scheme="#authorial_keywords"><!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc><!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000722/000722.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract><!--Include a brief abstract of the article-->
            <p/>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <front>
         <titlePage>
            <docTitle>
               <titlePart type="Title">Augmenting access to embodied knowledge archives: a computational framework</titlePart>
            </docTitle>
         </titlePage>
         <p>Giacomo Alliata, Yumeng Hou, and Sarah Kenderdine</p>
         <p>Laboratory for Experimental Museology, EPFL, Switzerland</p>
      </front>
      <body>
         <div>
            <head>Abstract</head>
            <p>With the burgeoning use of digital technologies in safeguarding intangible and living heritage, memory institutions have produced a significant body of material yet to be made accessible for public transmission. The quest for new ways to unlock these massive collections has intensified, especially the embodied knowledge embedded in complex formats, such as audiovisual and motion capture data.</p>
            <p>Aiming to address the gap between living heritage archives and embodied knowledge experience, this work inspects a computational framework incorporating machine intelligence, archival science, and digital museology. By reflecting on how embodied knowledge is sustained and potentially valorised through human interaction, we devise a series of methods utilising vision-based feature extraction, pose estimation, movement analysis and machine learning. The goal is to augment the archival experience with new modes of exploration, representation, and embodiment.</p>
            <p>This article reports the computational procedures and algorithmic tools inspected through two use cases. In the first example, we visualise the whole <hi rend="italic">Prix de Lausanne </hi>archive, a collection of 50 years of video recordings of dance performances, for archival exploration through the dancers’ poses. In another experimentation, movement encoding is employed to allow multimodal data search via embodied cues in the <hi rend="italic">Hong Kong Martial Arts Living Archive</hi>, a comprehensive documentation of the living heritage of martial arts chiefly based on motion-capturing masters’ performances.</p>
            <p>Though holding different application purposes, both projects operate on the proposed framework and extract archive-specific features to create a meaningful representation of human bodies, which reveals the versatile applications that computational capacities can achieve for embodied knowledge archives. The practices also represent a model of interdisciplinary involvement where the archivists, computists, artists and knowledge holders join hands to renew strategies for archival exploration and heritage interpretation in a new light.</p>
            <p>Keywords: intangible cultural heritage; embodied knowledge; computational archival science; movement computing; access</p>
         </div>
         <div>
            <head>1 Introduction</head>
            <p>Living Heritage, also known as intangible culture, encompasses the practices, expressions, spaces, and knowledge that safeguard the rich diversity of human creativity and collective heritage, as defined by UNESCO. The preservation and exploration of living heritage have been of paramount importance in understanding and celebrating the cultural fabric of our society. To further enhance our engagement with this heritage, advancements in digital technologies have opened new possibilities for capturing and experiencing embodied knowledge.</p>
            <p>In recent years, there has been a growing trend in the digital capturing of living heritage and embodied knowledge (Hou et al., 2022). These endeavours aim to document and preserve not only physical artefacts but also the intangible aspects of culture, such as rituals, performances, and traditional practices. However, the GLAM sectors, encompassing galleries, libraries, archives, and museums, face significant challenges in effectively managing and providing access to these vast digital repositories (Jaillant, 2022).</p>
            <p>One of the key challenges faced by the galleries, libraries, archives and museums (GLAM) sectors is the sheer scale of the digitised collections. As more and more heritage materials are digitised, the volume of data grows exponentially, making it increasingly difficult to navigate and explore these archives manually, essentially turning these collections into “dark archives” inaccessible to the public. (Fossati et al., 2012) further highlights how non-expert audiences are in need of more intuitive modes of access to discover these collections. To address this challenge, recent research has embarked on applying computational analysis, the application of artificial intelligence (AI) in particular, to unlock various new modes of archival experience. Leveraging the transformative potential of AI,<note> By ‘’transformative potential’’, we mean the potential of AI to augment the data available by extracting new features that can open innovative venues for accessing the archive.</note> we can enhance the retrieval, visualisation, and navigation of multimodal cultural data materials (Aske and Giardinetti, 2023, Cameron et al., 2023, Hou et al., 2023). Further, by involving the collaborative efforts of cultural domain experts in configuring AI tools, there is a promise to alleviate the gap between data science and the GLAM sectors, highlighted for instance through various uses cases on the implementation of AI strategies in Swedish Heritage Organizations (Griffin et al., 2023), and enable users to interact with digitised archives in meaningful yet trustworthy contexts (Jaillant and Rees, 2023).</p>
            <p>To tackle these challenges, we propose a computational framework that augments the archival experience of embodied knowledge. Our approach intends to extract human movements in the form of skeleton data and process it, showing its versatility through multiple use cases. By revalorizing and visualising knowledge, memory, and experience, we strive to enhance the cognitive reception and engagement of users, ultimately improving the overall archival user experience. Our experiments are conducted on two exemplar collections: the <hi rend="italic">Prix de Lausanne</hi>, an audiovisual dataset of dance performances, and the <hi rend="italic">Hong Kong Martial Arts Living Archive</hi> (HKMALA) archive, combining both videos and motion capture recordings of Hong Kong martial arts performers. This work might be of interest to professionals of Cultural Heritage institutions wishing to explore new methods of access to their collections as well as researchers on Computational Humanities with a focus on embodied knowledge archives.</p>
            <p>By exploring this computational framework, we hope to contribute to the ongoing efforts in leveraging visual AI technologies to transform the way we interact with and appreciate living heritage. Through the synthesis of cultural heritage and cutting-edge computational methods, we can bridge the gap between the past and the present, enabling a deeper understanding and appreciation of our shared human history.</p>
         </div>
         <div>
            <head>2 Related Work</head>
            <div>
               <head>2.1 Embodied knowledge as part of Living Heritage</head>
               <p>Though partly embedded in architectural sensorium and memory objects, intangible cultural heritage (ICH) is inherently living and manifests through bodily expressions, individual practices and social enactment - on the multiple layers of embodiment. Recent research has revealed a stress shift in documenting, describing and presenting the living nature of ICH by transforming intangible practices into tangible choreographic objects. For instance, the <hi rend="italic">i-Treasures </hi>project (2013-2017) carried out a series of digitization on the living heritage of folk dances, folk singing, craftsmanship, and contemporary music composition in an attempt to sustain traditional know-how via creative presentation (Dimitropoulos et al., 2014, Grammalidis and Dimitropoulos, 2015). Taking a more public-facing approach, the <hi rend="italic">Terpsichore </hi>project (2016 to 2020) integrates computational models, such as semantic web technologies and machine learning, with storytelling to facilitate data accessibility and affordance of digitised materials (Doulamis et al., 2017). Famous choreographers have also been keen on documenting their practices with audiovisual recordings, resulting in extensive collections such as William Forsythe’s <hi rend="italic">Performative Archive</hi>, recently acquired by the ZKM | Center for Art and Media Karlsruhe (2023). Whilst with a different focus from performing arts, in the Far East, the <hi rend="italic">Hong Kong Martial Arts Living Archive </hi>(HKMALA) project (2012-present) forged a multimodal digital archiving paradigm by motion-capturing martial art sequence performances of renowned masters. The MoCap records are situated within a digital collation capturing different facets of Hong Kong’s martial tradition, encompassing rituals, histories, re-enactments, and stories of practitioners (Chao et al., 2018).</p>
               <p>The embodied facet of ICH not only refers to the knowledge of the body but also of that which is dwelling in and enacted through the body (Craig et al., 2018). Moves and gestures are considered typical mediums to express and process mindfulness. In various cases, they also mediate interactive processes, such as human-environment communication and knowledge formation. Movement data usually documents the active dimension of mankind’s creativity over time. Hence, as Tim Ingold’s argument delineates, instead of targeting solely result-driven examinations (Salazar Sutil, 2018), the ideal usage of such data should facilitate a new mode of computation conveying the open-ended information embedded in bodily practices.<note> Tim Ingold argued that some movement is “automatic and rhythmically responsive” to its surroundings and “along multiple pathways of sensory participation” (Ingold, 2011).</note> In accordance with this ideal, Motion Capture (MoCap) technologies have gained increasing popularity in ICH archiving. MoCap allows data collection to be neutral, simultaneous, “beyond the frame and within the volume” (Delbridge, 2015), thereby fostering a perceivable human presence in the virtual CH environments, as surveyed by (Chalmers et al., 2021).</p>
            </div>
            <div>
               <head>2.2 Computational (embodied) archives</head>
               <p>Digitization of archives allows researchers and practitioners to apply modern computational methods to these collections. A recent review has outlined multiple axes on which AI can improve archival practices by automating recordkeeping processes and improving access to these collections as well as fostering new forms of digital archives (Colavizza et al., 2021). Manovich (2020) has put forward a new way to analyse large datasets of cultural items through computational means, a method he has named “cultural analytics”, while other researchers have focused specifically on computer vision models to better understand large visual cultural collections through “distant viewing” (Impett, 2020<hi rend="italic">b</hi>).</p>
               <p>From the perspective of experimental museology, computational approaches can help improve the “civic value” of cultural archives (Edmondson and Edmonson, 2004). Through its operationalisation, the archive is augmented on three main levels (Kenderdine et al., 2021). First, it enters the social and immersive dimension of situated museological experiences. Second, archivists engage in new interdisciplinary exchanges with media experts and the computer science community to solve many of the technical challenges cultural institutions face today. Third, new narratives can be imagined by extracting novel features and revealing “hidden structures” in the dataset (Olesen et al., 2016).</p>
               <p>These hold especially for audiovisual recordings, one of the primary examples of collections used to capture and document embodied practices. As the foremost mnemonic records of the 21st century, these moving image archives are part of our daily lives. In these last decades, major broadcasting institutions have digitised their entire collections. For instance, in Switzerland, the Radio Télévision Suisse (RTS) has more than 200,000 hours of footage (RTSArchives, 2018), while in the United Kingdom, the British Broadcasting Corporation (BBC) preserves more than a million recorded hours (Wright, 2017). In parallel, we are observing many advances in the field of visual AI dedicated to human pose estimation with state-of-the-art models such as OpenPose (Cao et al., 2019), OpenPifPaf (Kreiss et al., 2021) and BlazePose (Bazarevsky et al., 2020). These AI algorithms can reliably extract human poses and movements from these large moving image collections at scale, essentially creating computational embodied archives. The wealth of new data extracted by this process can then be further processed, unlocking new modes of access and novel ways of exploring these archives.</p>
            </div>
            <div>
               <head>2.3 Human bodies as a way of access</head>
               <p>In the field of digital humanities, various projects have relied on such approaches to analyse, explore and better understand visual collections through human bodies. In digital art history, for instance, researchers have undertaken a comprehensive analysis of large art corpora through human gestures (Bernasconi et al., 2023, Impett, 2020<hi rend="italic">a</hi>). Similarly, in collaboration with choreographer Wayne McGregor, the Google Arts &amp; Culture Lab has developed the <hi rend="italic">Living Archive</hi>, a web-based interface to navigate the collection of postures in McGregor’s choreographies and create new movements in the choreographer’s style (McGregor and Lab, 2019). Furthermore, movement computing has gained popularity as an intelligent approach to transforming the performed cultures into “choreographic objects” and is used for analysing, visualising, and interacting with datasets of dance heritage (Doulamis et al., 2017). Aristidou et al. (2018, 2019) developed a Labanotation (Guest, 1977) based framework for transforming movements in Cypriot dances to a high-dimensional feature model and constructing a deep-learned <hi rend="italic">motion signature </hi>for similarity analysis. Improved from <hi rend="italic">deep signature </hi>encoding (Aristidou et al., 2019), Sedmidubsky et al. (2020) invented a text-like representation of 3D skeleton sequences and employed the benefits of text-learning models in a more complicated context. Likewise, El Raheb et al. (2018) combined posture recognition and Benesh movement notation (Benesh and Benesh, 1977) to assist with multimedia annotation and interactive learning. In addition to a solely choreographic focus, the <hi rend="italic">Nrityakosha </hi>project synthesised a marriage of detection algorithms and semantic models. The researchers related embodied attributes to concepts of Indian classical dances and, correspondingly, created a specialised ontology for describing knowledge in the multimedia archive (Mallik and Chaudhury, 2012, Mallik et al., 2011). By applying the methodology of “distant viewing” (Arnold and Tilton, 2019) to embodied knowledge archives, Broadwell and Tangherlini (2020) proposed a computational analysis of K-pop dance, leveraging human-pose estimation algorithms applied to audiovisual recordings of famous K-pop groups and idols performances. Finally, readers will find additional interesting use cases in the comprehensive review provided by Bardiot (2021).</p>
            </div>
         </div>
         <div>
            <head>3 Our Computation Framework</head>
            <div>
               <head>3.1 Rationale</head>
               <p>The high-level goal of our computational framework is to enrich an embodied knowledge archive by extracting human skeleton data. This wealth of new data, in the form of sets of keypoints, captures both static postures (when looking at individual frames) as well as dynamic motions (when adding the temporal sequences of skeletons from successive frames). By applying motion extraction algorithms, we can therefore operationalise the abstract features of human poses and movements, essentially translating them into vectors that can then be further processed through other computational methods. Such a process augments the archive and unlocks a multitude of new scenarios, examples of which will be discussed through our two use cases. In particular, we investigate visualisations of the whole archive through human poses as well as motion-based retrieval.</p>
            </div>
            <div>
               <head>3.2 Methodology</head>
               <p>Based on the relevant literature reviewed above, we propose the following general framework to augment embodied knowledge archives and create new modes of experiencing them. This method applies to both moving image and motion captures archives, with the main difference in the first step: extracting human poses. Indeed, when working with videos, human pose estimation models such as OpenPose (Cao et al., 2019) are required to extract human postures in the form of a “skeleton”      (a list of keypoint edge pairs, as defined in the COCO dataset (Lin et al., 2014))       Depending on the model, these can be both in two or three dimensions and handle single or multiple persons at the same time. Furthermore, we stress that these models can sometimes fail to accurately detect full bodies, especially in situations where part of the body is occluded. The number of keypoints detected can also have an impact of how much these models are able to capture features specific to a certain discipline. While MoCap data generally comes with skeleton information, post-processing, normalisation and format conversion are often necessary to produce clean and operable skeleton datasets.</p>
               <p>Once skeleton data is extracted, poses need to be normalised so that they can be meaningfully compared. This involves scaling the skeleton data with respect to the image sizes, in the case of video datasets, as well as centering and scaling with respect to pose size. Subsequently, feature vectors can be computed based on this normalised set of keypoints. We note that these features can vary a lot from one application to another, especially if one is working with static poses (as in our first use case) or on motions (as in the second one).</p>
               <p>These steps result in a computational embodied archive, in which each element is a human pose, as a set of normalised skeleton keypoints, linked to its corresponding frame or timestamp in the recorded item and potentially augmented with additional computed features. The wealth of data extracted can then be further processed for a variety of scenarios, of which we present two examples. The first use case of this paper, on a collection of dance performance video recordings, will explore how the whole dataset can be mapped in two dimensions through the poses dancers take in their performances. Afterwards, we present a second scenario on a dataset of martial arts extending to motion-based similarity retrieval.</p>
            </div>
         </div>
         <div>
            <head>4 USE CASE 1 – ACCESSING A DANCE PERFORMANCES AUDIOVISUAL ARCHIVE VIA HUMAN POSES</head>
            <div>
               <head>4.1 The Prix de Lausanne audiovisual archive</head>
               <p>The Prix de Lausanne is a competition for young dancers held yearly in Lausanne, Switzerland since 1973<note> https://www.prixdelausanne.org/ </note>. Nearly all editions have been recorded and digitised, recording in a rich dataset of 1445 mp4 videos of individual dance performances, across forty-two years. This audiovisual collection is documented with information on the dancers and their chosen performances. This metadata, although valuable, does not document the embodied elements of dance performances, limiting how one can access such an archive. With our solution, we augment the data available with computational methods and employ the extracted features to map the whole dataset in two dimensions through the human poses embedded in the material, revealing its hidden structures and creating new connections between similar poses.</p>
            </div>
            <div>
               <head>4.2 Building feature vectors for the dancers’ poses</head>
               <p>We extracted human skeletons data with MediaPipe BlazePose GHUM 3D model (Bazarevsky et al., 2020), mainly chosen for its robustness out-of-the-box and rapidity of inference for single human estimation. In the case of the Prix de Lausanne, since the videos record one participant at a time on stage, we found this model to be the most suitable. We also observe only occasional cases of occlusions when the dancers are wearing particularly large costumes. The BlazePose algorithm outputs skeleton data as a set of 33 keypoints in three-dimensional space, normalised to the image size. Compared to models trained on the COCO dataset, MediaPipe algorithm detects more keypoints, including both hands and foots index fingers, allowing us to compute the wrists and ankles angles. Although not specifically trained on dance recordings, we believe these additional features help to better capture specific poses of dance performances, where even the position of hands and feets are crucial. Videos are processed to extract one pose every five seconds, a cautious design decision to avoid over-collecting similar poses merely due to temporal closeness. We do note however this is potentially not the best solution to ensure a full coverage of the diversity of poses, and future research in this direction could improve the results presented even further. It is worth noting that not all frames necessarily have a detected pose, since in some cases the dancer is too far away from the camera, or because some videos are not properly cut to the exact dance performance and thus have irrelevant frames at the beginning and/or the end. We measure the rate of detection by taking the mean of the ratio of poses detected over the number of frames per video, obtaining a mean detection rate of 76.80% across all the videos. The pose extraction process results in a total of 27672 poses.</p>
               <p>Once these poses have been extracted, we normalise them following a two-step procedure. We first subtract the pose centre, defined as the mean point between the left and right hips. We then scale the pose by its size, defined as the maximum between the torso size and the maximum distance between the pose centre and any keypoint, where the torso size is computed as the distance between the centre of the shoulders and the pose centre (the centre of the hips). The procedure ensures meaningful comparison between the poses by normalising them. An intuitive sample of normalised poses with their corresponding frames is provided in Figure 1.</p>
               <figure>
                  <head/>
                  <graphic url="media/image1.jpg"/>
               </figure>
               <p>Fig. 1. Sample of normalised poses with corresponding frames. Only the main keypoints are displayed for ease of visualization (the nose, the shoulders, the elbows, the wrists, the hips, the knees and the ankles).</p>
               <p>Subsequently, two feature vectors are built and tested. On one hand, we simply take the vector of keypoints and flattened it, resulting in a 99-dimensional vector. On the other hand, we build a custom feature vector, taking a combination of pose lengths (distances between two keypoints) and pose angles (angles defined by three keypoints). Table 1 reports the features computed.</p>
               <table>
                  <row role="data">
                     <cell>Category</cell>
                     <cell>Feature</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <p>Distances between joints</p>
                     </cell>
                     <cell>
                        <p>Left elbow - Right elbow</p>
                        <p>Left wrist - Right wrist</p>
                        <p>Left knee - Right knee</p>
                        <p>Left ankle - Right ankle</p>
                        <p>Left hip - Left wrist</p>
                        <p>Right hip - Right wrist</p>
                        <p>Left hip - Left ankle</p>
                     </cell>
                     <cell>
                        <p>Right hip - Right ankle</p>
                        <p>Left ankle - Left wrist</p>
                        <p>Right ankle - Right wrist</p>
                        <p>Left shoulder - Right ankle</p>
                        <p>Right shoulder - Left ankle</p>
                        <p>Left wrist - Right ankle</p>
                        <p>Right wrist - Left ankle</p>
                     </cell>
                  </row>
                  <row role="data">
                     <cell>
                        <p>Angles</p>
                     </cell>
                     <cell>
                        <p>Left hip - Left knee - Left ankle</p>
                        <p>Right hip - Right knee - Right ankle</p>
                        <p>Left shoulder - Left elbow - Left wrist</p>
                        <p>Right shoulder - Right elbow - Right wrist</p>
                        <p>Left hip - Left shoulder - Left elbow</p>
                        <p>Right hip - Right shoulder - Right elbow</p>
                     </cell>
                     <cell>
                        <p>Left shoulder - Left hip - Left knee</p>
                        <p>Right shoulder - Right hip - Right knee</p>
                        <p>Left elbow - Left wrist - Left index</p>
                        <p>Right elbow - Right wrist - Right index</p>
                        <p>Left knee - Left ankle - Left foot index</p>
                        <p>Right knee - Right ankle - Right foot index</p>
                     </cell>
                  </row>
               </table>
               <p>Table 1. A list of custom features used to represent dance poses in the Prix de Lausanne archive.</p>
            </div>
            <div>
               <head>4.3 Mapping the Prix de Lausanne archive</head>
               <p>One of the fundamental applications unlocked by computational archives is the possibility to visualise the whole collection. To this end, we apply dimensionality reduction (DR) algorithms to embed the high-dimensional feature spaces in two or three dimensions, in order to create a novel way of visualising and navigating the whole archive. We test both standard DR methods such as Principal Component Analysis (PCA) and t-distributed Stochastic Neighbour Embedding (tSNE) as well as a more recent approach with Uniform Manifold Approximation and Projection (UMAP). We leverage scikit-learn (Pedregosa et al., 2011) implementations for the two first algorithms (Halko et al., 2011, Martinsson et al., 2011, van der Maaten and Hinton, 2008) and the official UMAP Python implementation for the latter (McInnes et al., 2018). For tSNE and UMAP we compare using cosine and Euclidean metrics as the distance measure to compute the projection. We therefore test five options on 27672 poses, with both sets of feature <figure>
                     <head/>
                     <graphic url="media/image2.jpeg"/>
                  </figure>vectors. Figure 2 shows the resulting embeddings in two dimensions.</p>
               <p>Fig. 2. Embeddings of 27672 poses in two dimensions. The top row (blue) uses the flattened vectors of keypoints and the bottom row (red) uses the custom features computed. From left to right, the algorithms used are PCA, tSNE (cosine metric), tSNE (Euclidean metric), UMAP (cosine metric), UMAP (Euclidean metric).</p>
               <p>We can immediately observe that PCA performs poorly on this dataset, as expected due to its linear nature that cannot preserve much variance with just two dimensions (only 49% with the flattened vectors of keypoints, and 45% with the custom features). Both tSNE and UMAP however give much more interesting results, where structures in the dataset can be observed. Since we do not notice meaningful differences between the two sets of feature vectors used, we pursue further examinations with the flattened vectors of keypoints to exploit the heuristic quality of the algorithms independent of the choice of features. Therefore, we set out to investigate tSNE and UMAP behaviours more in-depth, by studying the effect of their hyperparameters. For tSNE, we observe how the embedding in two dimensions behaves for different perplexities <hi rend="italic">𝜖</hi>, the main parameter for this algorithm, related to the number of neighbours considered for any given point during the computation. For UMAP, we investigate the number of nearest neighbours <hi rend="italic">𝑛</hi>, controlling how UMAP balances local versus global structure in the data, and the minimum distance <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 the algorithm is allowed to pack points together in the low-dimensional space. We use the Euclidean distance as it seems to produce slightly clearer results. Figure 3 shows how tSNE behaves for different values of <hi rend="italic">𝜖</hi>, while Figure 4 displays the results of UMAP with varying <hi rend="italic">n</hi> and <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 values.</p>
               <figure>
                  <head/>
               </figure>
               <p>Fig. 3. Effect of the perplexity 𝜖 on the tSNE embeddings of 27672 poses (flattened vectors of keypoints, Euclidean distance). Lower values of 𝜖 result in a more granular embedding.</p>
               <figure>
                  <head/>
               </figure>
               <p>Fig. 4. Effect of the number of neighbours <hi rend="italic">𝑛</hi> and the minimum distance <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 on the UMAP embeddings of 27672 poses (flattened vectors of keypoints, Euclidean distance). Lower values of <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 result in a more packed embedding but global structures appear to be quite stable.</p>
               <p>To conclude our exploration of the Prix de Lausanne moving image collection, we set out to produce a map of the human poses embedded in the archive. Following the outlined procedure, we extract the keypoints, compute the corresponding feature vectors and apply UMAP (with Euclidean distance, <hi rend="italic">n</hi> = 50, <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 = 0.5) on the flattened vectors of keypoints to create Figure 5, where we display the corresponding pose at each point. For the sake of visibility, we only show a portion of all the poses extracted, resulting in a map of 2768 poses. Through this visualisation, we can verify the effectiveness of our approach in grouping together similar poses, <figure>
                     <head/>
                     <graphic url="media/image5.png"/>
                  </figure>thus unlocking a new way of seeing an embodied knowledge archive.</p>
               <p>Fig. 5. Map of 2768 dancers’ poses, based on the 2D embeddings of the vectors of flattened keypoints with UMAP (Euclidean distance, <hi rend="italic">n</hi> = 50, <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 = 0.5).</p>
            </div>
         </div>
         <div>
            <head>5 Use Case 2: Accessing Martial Art Recordings through Motion Traits</head>
            <p>In the first use case, the human skeletons extracted have been treated as static postures, thus operating on individual frames of the moving image recordings. For the purpose of visualising the whole archive in two dimensions, this is suitable as we can draw these skeletons, using them as glyphs representing frames in the collection, to create compelling images as in Figure 5     . However, one might argue that embodied knowledge archives are not only about human postures but also (if not more) about human motions. This second use case thus investigates such an approach, proposing an innovative motion-based retrieval mode. In the context of the Hong Kong Martial Arts Living Archive (HKMALA) project, a retrieval system allows users to select a specific motion within the collection (by choosing the start and end frames of a video sequence for instance) and returns similar motions in the archive.      </p>
            <div>
               <head>5.1 Hong Kong Martial Arts Living Archive</head>
               <p>Traditional martial arts are considered knowledge treasures of humanities sustained through generations by diverse ethnic groups. Among the genres, Southern Chinese martial art (SCMA) is arguably one of the most prolonged systems embodying the mind-body ideologies in the Far East, yet now facing challenges in knowledge transmission and the risk of being lost. In preserving the living heritage of SCMA, the <hi rend="italic">Hong Kong Martial Arts Living Archive </hi>(HKMALA) inspects a comprehensive set of digitisation tools to capture the martial practices, with a chief focus on motion capturing (MoCap) the form sequence performances (or known as or <hi rend="italic">taolu</hi>). Since its origination in 2012, HKMALA has built a 4D motion archive spanning over 20 styles and 130 sets of empty-hand and weapon sequences.<note> The concept of four-dimensional space (4D) denotes a dynamic 3D space moving through time.</note> The archive also collates various historical and reconstituted documentation of the martial cultures, encompassing rituals, traditions, armaments and objects preserved in Hong Kong      (Chao et al., 2018).</p>
               <p>The HKMALA’s exceptional volumes hold the promise to enable various scholarly inquiries. However, its current archival organisation is based on a manual catalogue reflecting how the content has been curated for exhibitions. Besides, the data consists of multiple modalities, such as texts, images, videos and MoCap data, hence impeding public access and dissemination at scale. There is a need for an effective yet meaningful way of data access.      </p>
               <p>Addressing the aforementioned challenges, we propose devising a motion-based retrieval framework that leverages machine learning to encode motion-wise information in multimodal recordings. The approach facilitates content retrieval through embodied cues, operating seamlessly across MoCap and moving image data formats. In the following paragraphs, we aim to provide a technical overview of the technical procedures. For a more comprehensive understanding of the implementation details, readers are invited to refer to the descriptions in (Hou et al., 2023).</p>
            </div>
            <div>
               <head>5.2 Encoding and retrieving martial arts movement</head>
               <p>     Chinese martial arts are renowned for their strong emphasis on form training, utilizing codified movement sequences to practise fundamental techniques and technique combinations. While less known, yet equally important, is the stress on combative tactics that involve a mindful operation of whole-body movement. Thus, training is imparted through a combination of freeform combats and integration into <hi rend="italic">taolu</hi> practices (Ma, 2003). Given that assessment methods in Chinese martial arts typically consider visually discernible characteristics and mechanical parameters, we apply the same configurations to model martial arts movement. Designed in compatibility with Laban metrics (Guest, 1977) for movement analysis, the approach aims to articulate both qualitative and quantitative qualities regarding the technical stances, ballistic activities, and reactions to the opponent or surroundings, as shown in Table 2 in the Appendix.</p>
               <p>The technical procedures commence with the extraction of 3D human skeletons from the MoCap data or the extraction of 2D poses from video recordings, followed by depth estimation to reconstruct the 3D structure. It then computes the coordinates and joint angles from raw skeletons into basic kinematic metrics, which are then normalized for further processing. On this basis, feature augmentation is employed to enrich the kinematic feature set, incorporating metrics that reflect linear and angular motion characteristics, along with the transition probabilities of motion time series. Finally, the best 50 features are selected utilizing the variance threshold approach and fed for training a discriminative neural network.</p>
               <p>     Holding a hypothesis that the characteristics of a long motion sequence can be represented by its segmentations     , our approach to the encoding task is set to pinpoint representative motion units within the sequence     . Firstly, we gathered high-dimensional features from equally segmented media data and employed deep learning to train a latent space, embedding them into a      vector space where semantically similar dimensions are put closer while dissimilar ones are set apart. Subsequently     , K-means clustering is applied      to identify cluster centres as representatives. To create      a descriptive and compact scheme for efficient retrieval, we encoded the inverse document frequency (IDF) of each representative into a normalised histogram, named <hi rend="italic">metamotion</hi>, to represent the distribution of archetypal qualities within a motion sequence.</p>
            </div>
            <div>
               <head>5.3 A motion-based retrieval engine</head>
               <p>To facilitate the envisioned scenario of exploratory archival browsing, a retrieval system has been     deployed to allow querying archival sequences containing similar movements. The system integrates different retrieval algorithms, including      Locality-sensitive hashing (LSH) and Ball-tree methods, which prove effective in achieving optimal      retrieval efficiency and outperforms the baseline approach in existing research (see 4. Experimental evaluation in (Hou et al., 2023)).      </p>
               <p>
                  <figure>
                     <head/>
                     <graphic url="media/image6.png"/>
                  </figure>On the user end     , an interactive search engine has been deployed enable motion-based data exploration across formats.      An intuitive search case is illustrated in Figure 6, showcasing a cross-model retrieval result from a hybrid of videos and MoCap data. Additionally, Figure 7 demonstrates the initial development of the archival browser, presenting the retrieved items with ontological descriptions for the conceptual entities associated with the query item. This design aims to improve data explainability and interoperability for future reuse, with ontology annotation sourced from the scholarly work of the Martial Art Ontology (MAon).<note> The Martial Art Ontology (MAon) V1.1, https://purl.org/maont/techCorpus.</note>
               </p>
               <p>Fig. 6. The top-5 similar sequences retrieved by the Query from a MoCap dataset (left) and a video dataset (right) (Hou et al., 2023).</p>
               <p>
                  <figure>
                     <head/>
                     <graphic url="media/image7.jpg"/>
                  </figure>Fig. 7. Illustration of a motion search example supplemented with ontological representations of the concepts of techniques.</p>
            </div>
         </div>
         <div>
            <head>6 Discussion</head>
            <div>
               <head>6.1 Visual AI toward “augmented archives”</head>
               <p>When working with cultural heritage archives, including the born-digital ones, it is common to expect metadata organisation in an archive-specific curatorial structure with topical domain-language descriptions about the archival items. For instance, both the Prix de Lausanne and HKMALA collections warrant a series of textual tags indicating the content of dance and martial art performances and additional descriptions providing knowledge about the performers. Such documentation, although resourceful, entails a mode of interpretation and access rather limited to expert users. One needs to know what dance performances <hi rend="italic">The Giselle </hi>or <hi rend="italic">Nutcracker </hi>are to look for them in the archive, or understand how martial arts are grounded on a series of <hi rend="italic">taolu</hi>. Furthermore, visual and embodied features are difficult to capture verbally. Even though models like Labanotation have been developed to describe movement components, especially in performing arts, they require a reasonably deep understanding of the knowledge in order to encode and decode. Hence, it is arguably very hard, if not impossible, to capture embodied knowledge features into literal formats or in an easily accessible way. Therefore, this kind of metadata, although necessary and useful in many domains, is not suitable for access to the general public. It does not support “casual modes of access”, fundamentals for museum-goers and general audiences (Whitelaw et al., 2015).</p>
               <p>In this article, we introduce      a computational framework aimed at enhancing cultural      archives      by enriching metadata      with embodied knowledge      descriptors derived from audiovisual and multimodal materials.      Utilising computer vision models, we extract and transform human poses and skeleton features into meaningful data extractions, which potentially foster a range of usage scenarios     . At the archival level, curators can operationalise      their collections as ‘’data sources’’ (Kenderdine et al., 2021),      envisioning new ways for querying, annotating and analysing whole datasets to forge novel narrative paradigms.     . In parallel, the enriched metadata improves data accessibility and diversified query channels improve data findability, augmenting the archives' compliance with FAIR (findability, accessibility, interoperability, and reusability) data principles, regardless of variations in language, topics, and data formats. Furthermore, our approach demonstrates the potential for interpreting data traits within and across cultural contexts. This not only contributes to the reusability of archival content but also resonates     with the findings of (Colavizza et al., 2021),      which highlights the      growing role of AI in archival practices, particularly in      “automation of recordkeeping processes'' and improving “organising and accessing archives”. Our experimentation, as exemplified through two distinctive heritage archives, represents an innovative step forward with a focus on embodied knowledge, allowing queries through more perceptive than quantitative channels, which we believe is more natural and humanistic.     </p>
            </div>
            <div>
               <head>6.2 Archive navigation and serendipitous discovery</head>
               <p>The operationalisation of embodied knowledge archives through visual AI, resulting in “augmented archives”, provides a vast array of data that can further be processed in order to create new modes of access adapted for general audiences. Following the principles of “generous interfaces” (Whitelaw et al., 2015), our method supports explorative behaviours, encouraging a new paradigm of “seeking the unknown” and avoiding the tendency of only searching for what is known (Winters and Prescott, 2019). By laying out the full archive through the lens of humanly understandable concepts such as postures taken by the dancers, users can more easily comprehend the archive, without needing specific knowledge about this topic. They do not need to have a specific goal in mind or to be looking for something in particular. They can wander around, browsing like an “information flaneur” (Dörk et al., 2011), enjoying the Prix de Lausanne archive in a way that traditional modes of access, based on querying through the metadata information or on grid-like and lists visualisations, could not offer. Furthermore, each pose links to a timestamp in a dance performance, and thus by grouping together similar poses in the low-dimensional space, the dimensionality reduction also creates serendipitous discoveries. Indeed, this mode of access rewards users for simply browsing the collection and stumbling upon new performances as they move from a pose to their neighbours on the map. Figure 8 explicates this process by showcasing poses similar to an input pose with the corresponding video frames.</p>
               <p>This new mode of access is enhanced by dimensionality reduction algorithms. However, one must take care in how to decide which algorithm to employ, and with which parameters. In this work, we have analysed Principal Component Analysis, t-distributed Stochastic Neighbour Embedding and Uniform Manifold Approximation and Projection. PCA is a well-established dimensionality reduction technique, but, due to its linear nature, it often fails to properly capture a real-life dataset with only a handful of dimensions. This is clearly confirmed in Figure 2, where all embeddings generated with tSNE and UMAP show much more interesting structures without creating a large clump of points like PCA. Regarding the choice of feature vectors used, as mentioned, we fail to observe noticeable differences between the flattened vectors of keypoints and the custom features computed. We hypothesise the custom features (reported in Table 1) are not more discriminative than the base keypoints. In subsequent analysis, we decided to use the flattened vectors of keypoints, but we believe this offers us an opportunity for more collaboration, since involving dance experts could help in crafting a more precise and adequate set of features, based on Labanotation for instance (Guest, 1977). To gain a deeper understanding of these algorithms, we have investigated the effect of parameters on tSNE and UMAP.</p>
               <p>Figure 3 shows that increasing the perplexity yields very different layouts, with lower values displaying numerous small groupings of items while higher values reveal more large-scale structures. This is in accordance with expectations, since with higher perplexities tSNE considers more points when computing the vicinity of each item, thus better capturing global structures. Surprisingly, when increasing the number of neighbours <hi rend="italic">𝑛</hi> considered with UMAP in Figure 4, the outputs appear to be much more stable, with global structures already visible with <hi rend="italic">𝑛</hi> = 50. It is in this case the second parameter, <hi rend="italic">𝑑</hi>𝑚𝑖𝑛, that affects more the results, yielding more sprayed-out mappings the higher it is (since the minimum distance directly controls how packed the points can be in the low dimensional space). These results indicate that when creating archive visualisation like the map of poses in Figure 5, for instance, higher <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 might be more suitable to avoid overlaps between similar poses. However, if one were to apply clustering algorithms on these embeddings to group together similar poses, the more fine-grained and packed structures obtained with lower <hi rend="italic">𝑑</hi>𝑚𝑖𝑛 would potentially yield better results. Therefore, we argue it is not a matter of which embedding is better but rather which embedding is more adapted to the specific <figure>
                     <head/>
                     <graphic url="media/image8.jpg"/>
                  </figure>narrative or mode of access sought.</p>
               <p>Fig. 8. Examples of similar poses in the Prix de Lausanne archive (top 5 matches). Simplified poses and corresponding video frames are displayed. Notice how connections between different performers are discovered.</p>
            </div>
            <div>
               <head>6.3 Limitations and future work directions</head>
               <p>Although we believe our computational framework to be well thought out, there are still some limitations we would like to highlight, to hopefully later address them.</p>
               <p>First, human pose extraction from monocular videos is never perfect. Human bodies can be wrongfully estimated or missed due to the influence of occlusion, monotonous colour patterns, and camera angles, to name a few. Indeed, through our “naive” approach to the Prix de Lausanne Archive, we only achieve a detection rate of 76.80%.<note> A “naive” approach in pattern recognition implies a straightforward and easy-to-implement algorithm that finds all matching occurrences of a given input.</note> Furthermore, by checking a sample of skeletons extracted with the corresponding frames as in Figure 8, we noticed that sometimes keypoints were not correctly extracted. One possible reason is that the pose estimation is done systematically every five seconds without a measure of their quality or significance. Nevertheless, our approach proves sufficient for the use case described, as it still produces enough data to map the whole archive properly. Further developments could yield more interesting and precise results, for instance, by extracting skeleton data on a finer temporal granularity and then filtering only to keep the “better” poses.<note> One would first need to define what is a ‘’better’’ pose, however.</note>
               </p>
               <p>Second, the feature vectors computed for the Prix de Lausanne are somewhat naive. Taking the lesson from the HKMALA feature computing, collaboration with dance experts could facilitate the design of more relevant dance-specific features with precise measurements able to better capture and compare human bodies during dance performances. Nonetheless, our results demonstrate that even naive methods can produce new modes of access to embodied knowledge archives. Thus, we are confident our method generalises well to other archives and diverse embodied disciplines.</p>
               <p>Lastly, unlike standard computer vision challenges, it is necessary but challenging to quantify what defines a good estimation of poses or movement segments in the archival context, yet the standard varies in people and cultural themes. To this end, we resorted to evaluating the distribution and quality of embeddings, supplemented with an ad-hoc expert review of small sampling sets. Future improvement is suggested to integrate expert review dynamically along with the model training process, such as by intaking expert judgement as a score and feeding it back to the model, so as to enable a human-in-the-loop machine learning process.</p>
            </div>
            <div>
               <head>6.4 Outlook: towards a new archival experience. </head>
               <p>The research presented in this paper has been conducted within the larger context of “computational museology” (Kenderdine et al., 2021), a discipline whose goal is to develop new modes of access to cultural heritage through computational approaches, specifically designed for situated experiences in museum settings. To this end, further work will rely on leveraging the findings highlighted in this paper to create interactive installations to access these embodied knowledge archives. We contend the implementation presented in this paper establishes a robust foundation for developing interactive modes of access tailored to meet the requirements of casual users within situated experiences.  In particular, two main directions will be pursued.     On one hand, based on the fundamental concept of placing visitors “inside” the archive (Shen et al., 2019) rather than looking at it on a plain screen, immersive environments (IEs) will be employed to create situated experiences in which users can navigate the whole archive. In practice, dimensionality reduction techniques will be employed to compute a mapping in two or three dimensions in order to generate a virtual world in which each point, or glyph, represents a pose (and its corresponding frame, or clip). This virtual cloud of human bodies will then be displayed in large IEs, allowing visitors to freely navigate and discover the archive. Applying immersive technologies to the mediation of cultural heritage already has some interesting applications, in particular in the context of archeological enquiry (Sciuto et al., 2023), and we believe such technologies can also serve a purpose for Embodied Knowledge archives.</p>
               <p>On the other hand, an interactive retrieval system can be developed, either based on pose or motion similarity. Users could strike a pose or perform a certain movement, detected in real-time with motion-tracking solutions, and the system would retrieve relevant clips from the collection. Such an experience would yield an interesting performative aspect that sees the visitor as a co-creator with the machine, essentially transforming them into a variable of the generative system, and other people around as an audience, witnessing the exchange in a “third-person’s perspective” (Mul and Masson, 2018).</p>
            </div>
         </div>
         <div>
            <head>7 Conclusion</head>
            <p>Embodied knowledge archives are an integral part of our Living Heritage and hold important aspects of our cultures. Yet, it is still difficult to explore them, in particular for more casual audiences that lack specific knowledge on the topic of the collection. To answer this challenge, we have proposed in this work a computational framework that leverages motion extraction AI to augment these datasets with a wealth of rich data, enabling new modes of analysis and access to the important collections.</p>
            <p>The method proposed is applied to two embodied knowledge archives, on dance and on martial arts performances, showcasing its application to multimedia content and diverse access scenarios. In the former example, we devise a method to visualise a whole collection in two dimensions through the human poses embedded in the archival material, revealing its structure and fostering serendipitous discoveries. On the latter, we extend to motion encoding from static poses and devise a motion-based query system, offering a new way to search an embodied knowledge archive. These scenarios showcase how our computational framework can operationalise this type of collection and unlock a variety of new modes of access suitable for non-expert audiences.</p>
         </div>
         <div>
            <head>Acknowledgements</head>
            <p>Authors are grateful to the <hi rend="italic">Prix de Lausanne </hi>for the opportunity to work on their audiovisual archive, as part of the SNSF's Sinergia grant <hi rend="italic">Narratives from the Long Tail: Transforming Access to Audiovisual Archives</hi> (grant number CRSII5_198632).</p>
            <p>The <hi rend="italic">Hong Kong Martial Arts Living Archive</hi> is a longitudinal research collaboration between the International Guoshu Association, City University of Hong Kong, and the Laboratory for Experimental Museology (eM+), EPFL.</p>
         </div>
         <div>
            <head>References</head>
            <p>Aristidou, A., Cohen-Or, D., Hodgins, J. K., Chrysanthou, Y. and Shamir, A. (2018), ‘Deep motifs and motion signatures’, <hi rend="italic">ACM Transactions on Graphics (TOG) </hi>37(6), 1–13.</p>
            <p>Aristidou, A., Shamir, A. and Chrysanthou, Y. (2019), ‘Digital dance ethnography: organizing large dance collections’, <hi rend="italic">Journal on Computing and Cultural Heritage (JOCCH) </hi>12(4), 1–27.</p>
            <p>Arnold, T., &amp; Tilton, L. (2019). Distant viewing: analyzing large visual corpora. Digital Scholarship in the Humanities, 34(Supplement_1), i3-i16.</p>
            <p>Aske, K. and Giardinetti, M. (2023), ‘(Mis) matching metadata: Improving accessibility in digital visual archives through the EyCon project’, <hi rend="italic">ACM Journal on Computing and Cultural Heritage</hi>.</p>
            <p>Bardiot, C. (2021). Performing arts and digital humanities: From traces to data. John Wiley &amp; Sons.</p>
            <p>Bazarevsky, V., Grishchenko, I., Raveendran, K., Zhu, T., Zhang, F. and Grundmann, M. (2020), ‘Blazepose: On-device real-time body pose tracking’, <hi rend="italic">arXiv preprint arXiv:2006.10204</hi>.</p>
            <p>Benesh, R. and Benesh, J. (1977), <hi rend="italic">Reading dance: The birth of choreology</hi>, ISBS.</p>
            <p>Bernasconi, V., Cetinić, E. and Impett, L. (2023), ‘A computational approach to hand pose recognition in early modern paintings’, <hi rend="italic">Journal of Imaging </hi>9(6), 120.</p>
            <p>Broadwell, P., &amp; Tangherlini, T. R. (2021). Comparative K-Pop Choreography Analysis through Deep-Learning Pose Estimation across a Large Video Corpus. DHQ: Digital Humanities Quarterly, 15(1).</p>
            <p>Cameron, S., Franks, P. and Hamidzadeh, B. (2023), ‘Positioning paradata: A conceptual frame for ai processual documentation in archives and recordkeeping contexts’, <hi rend="italic">ACM Journal on Computing and Cultural Heritage</hi>.</p>
            <p>Cao, Z., Hidalgo Martinez, G., Simon, T., Wei, S. and Sheikh, Y. A. (2019), ‘Openpose: Realtime multi-person 2d pose estimation using part affinity fields’, <hi rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</hi>.</p>
            <p>Chalmers, A., Parkins, J., Webb, M. and Debattista, K. (2021), Realistic humans in virtual cultural heritage, <hi rend="italic">in </hi>‘International Conference on Emerging Technologies and the Digital Transformation of Museums and Heritage Sites’, Springer, pp. 156–165.</p>
            <p>Chao, H., Delbridge, M., Kenderdine, S., Nicholson, L. and Shaw, J. (2018), Kapturing kung fu: Future proofing the hong kong martial arts living archive, <hi rend="italic">in </hi>‘Digital Echoes’, Springer, pp. 249–264.</p>
            <p>Colavizza, G., Blanke, T., Jeurgens, C. and Noordegraaf, J. (2021), ‘Archives and ai: An overview of current debates and future perspectives’, <hi rend="italic">ACM Journal on Computing and Cultural Heritage (JOCCH) </hi>15(1), 1–15.</p>
            <p>Craig, C. J., You, J., Zou, Y., Verma, R., Stokes, D., Evans, P. and Curtis, G. (2018), ‘The embodied nature of narrative knowledge: A cross-study analysis of embodied knowledge in teaching, learning, and life’, <hi rend="italic">Teaching and Teacher Education </hi>71, 329–340.</p>
            <p>Delbridge, M. (2015), <hi rend="italic">Motion capture in performance: an introduction</hi>, Springer.</p>
            <p>Dimitropoulos, K., Manitsaris, S., Tsalakanidou, F., Nikolopoulos, S., Denby, B., Al Kork, S., Crevier-Buchman, L., Pillot-Loiseau, C., Adda-Decker, M., Dupont, S. et al. (2014), Capturing the intangible an introduction to the i-treasures project, <hi rend="italic">in </hi>‘2014 international conference on computer vision theory and applications (VISAPP)’, Vol. 2, IEEE, pp. 773–781.</p>
            <p>Dörk, M., Carpendale, S. and Williamson, C. (2011), The Information Flaneur: A Fresh Look at Information Seeking, <hi rend="italic">in </hi>‘Proceedings of the SIGCHI Conference on Human Factors in Computing Systems’, pp. 1215–1224.</p>
            <p>Doulamis, A. D., Voulodimos, A., Doulamis, N. D., Soile, S. and Lampropoulos, A. (2017), Transforming intangible folkloric performing arts into tangible choreographic digital objects: The terpsichore approach., <hi rend="italic">in </hi>‘VISIGRAPP (5: VISAPP)’, pp. 451–460.</p>
            <p>Edmondson, R. and Edmonson, R. (2004), <hi rend="italic">Audiovisual archiving: philosophy and principles</hi>, Unesco Paris.</p>
            <p>El Raheb, K., Kasomoulis, A., Katifori, A., Rezkalla, M. and Ioannidis, Y. (2018), A web-based system for annotation of dance multimodal recordings by dance practitioners and experts, <hi rend="italic">in </hi>‘Proceedings of the 5th International Conference on Movement and Computing’, pp. 1–8.</p>
            <p>Fossati, G. et al. (2012), ‘Found footage filmmaking, film archiving and new participatory platforms’, <hi rend="italic">Found Footage. Cinema Exposed. Amsterdam:</hi>
            </p>
            <p>
               <hi rend="italic">Amsterdam University Press/EYE Film Institute Netherlands </hi>pp. 177–184.</p>
            <p>Grammalidis, N. and Dimitropoulos, K. (2015), ‘Intangible treasures-capturing the intangible cultural heritage and learning the rare know-how of living human treasures’.</p>
            <p>Guest, A. H. (1977), <hi rend="italic">Labanotation: or, kinetography Laban: the system of analyzing and recording movement</hi>, number 27, Taylor &amp; Francis.</p>
            <p>Griffin, G., Wennerström, E. &amp; Foka, A. (2023), AI and Swedish Heritage Organisations: challenges and opportunities. AI &amp; Soc. https://doi.org/10.1007/s00146-023-01689-y</p>
            <p>Halko, N., Martinsson, P. G. and Tropp, J. A. (2011), ‘Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions’, <hi rend="italic">SIAM Review </hi>53(2), 217–288.</p>
            <p>URL: <hi rend="italic">https://doi.org/10.1137/090771806</hi>
            </p>
            <p>Hou, Y., Kenderdine, S., Picca, D., Egloff, M. and Adamou, A. (2022), ‘Digitizing intangible cultural heritage embodied: State of the art’, <hi rend="italic">Journal on Computing and Cultural Heritage (JOCCH) </hi>15(3), 1–20.</p>
            <p>Hou, Y., Seydou, F. M. and Kenderdine, S. (2023), ‘Unlocking a multimodal archive of southern chinese martial arts through embodied cues’, <hi rend="italic">Journal of Documentation</hi>.</p>
            <p>Impett, L. (2020<hi rend="italic">a</hi>), Analyzing gesture in digital art history, <hi rend="italic">in </hi>‘The Routledge Companion to Digital Humanities and Art History’, Routledge, pp. 386–407.</p>
            <p>Impett, L. L. (2020<hi rend="italic">b</hi>), Painting by numbers: Computational methods and the history of art, Technical report, EPFL.</p>
            <p>Ingold, T. (2011), <hi rend="italic">Being alive: Essays on movement, knowledge and description</hi>, Taylor &amp; Francis.</p>
            <p>Jaillant, L. (2022), <hi rend="italic">Archives, access and artificial intelligence: working with born-digital and digitized archival collections</hi>, Bielefeld University Press.</p>
            <p>Jaillant, L. and Rees, A. (2023), ‘Applying ai to digital archives: trust, collaboration and shared professional ethics’, <hi rend="italic">Digital Scholarship in the Humanities </hi>38(2), 571–585.</p>
            <p>Kenderdine, S., Mason, I. and Hibberd, L. (2021), Computational archives for experimental museology, <hi rend="italic">in </hi>‘Emerging Technologies and the Digital Transformation of Museums and Heritage Sites: First International Conference, RISE IMET 2021, Nicosia, Cyprus, June 2–4, 2021, Proceedings 1’, Springer, pp. 3–18.</p>
            <p>Kreiss, S., Bertoni, L. and Alahi, A. (2021), ‘Openpifpaf: Composite fields for semantic keypoint detection and spatio-temporal association’, <hi rend="italic">IEEE Transactions on Intelligent Transportation Systems </hi>23(8), 13498–13511.</p>
            <p>Król, H. and Mynarski, W. (2005), ‘Cechy ruchu-charakterystyka i możliwości parametryzacji [features of movement-characteristics and capabilities of parametryzation]’, <hi rend="italic">Katowice: Akademia Wychowania Fizycznego</hi>     .</p>
            <p>Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. (2014). Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 (pp. 740-755). Springer International Publishing.</p>
            <p>Ma, M. (2003), <hi rend="italic">Wu Xue Tan Zhen (Examination of Truth in Martial Studies)</hi>, 2 vols, Lion Books, Taipei.</p>
            <p>Mallik, A. and Chaudhury, S. (2012), ‘Acquisition of multimedia ontology: an application in preservation of cultural heritage’, <hi rend="italic">International journal of multimedia information retrieval </hi>1(4), 249–262.</p>
            <p>Mallik, A., Chaudhury, S. and Ghosh, H. (2011), ‘Nrityakosha: Preserving the intangible heritage of indian classical dance’, <hi rend="italic">Journal on Computing and Cultural Heritage (JOCCH) </hi>4(3), 1–25.</p>
            <p>Manovich, L. (2020), <hi rend="italic">Cultural analytics</hi>, Mit Press.</p>
            <p>Martinsson, P.-G., Rokhlin, V. and Tygert, M. (2011), ‘A randomized algorithm for the decomposition of matrices’, <hi rend="italic">Applied and Computational Harmonic Analysis </hi>30(1), 47–68.</p>
            <p>URL: <hi rend="italic">https://www.sciencedirect.com/science/article/pii/S1063520310000242</hi>
            </p>
            <p>McGregor, W. and Lab, G. A. C. (2019), ‘Living archive’. Accessed: 30 June 2023.</p>
            <p>URL: <hi rend="italic">https://artsexperiments.withgoogle.com/living-archive</hi>
            </p>
            <p>McInnes, L., Healy, J., Saul, N. and Grossberger, L. (2018), ‘Umap: Uniform manifold approximation and projection’, <hi rend="italic">The Journal of Open Source Software </hi>3(29), 861.</p>
            <p>Mul, G. and Masson, E. (2018), ‘Data-based art, algorithmic poetry: Geert mul in conversation with eef masson’, <hi rend="italic">TMG Journal for Media History </hi>21(2).</p>
            <p>Olesen, C. G., Masson, E., Van Gorp, J., Fossati, G. and Noordegraaf, J. (2016), ‘Data-driven research for film history: exploring the jean desmet collection’, <hi rend="italic">Moving Image: The Journal of the Association of Moving Image Archivists </hi>16(1), 82–105.</p>
            <p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E. (2011), ‘Scikit-learn: Machine learning in Python’, <hi rend="italic">Journal of Machine Learning Research </hi>12, 2825–2830.</p>
            <p>RTSArchives (2018), ‘Le nouveau site RTSarchives’.</p>
            <p>URL: <hi rend="italic">https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html</hi>
            </p>
            <p>Salazar Sutil, N. (2018), ‘Section editorial: Human movement as critical creativity: Basic questions for movement computing’, <hi rend="italic">Computational Culture: a Journal of Software Studies </hi>(6).</p>
            <p>Sciuto, C., Foka, A., Lindmark, M., &amp; Robertsson, J. (2023). Exploring Fragmented Data: Environments, People and the Senses in Virtual Reality. In Capturing the Senses: Digital Methods for Sensory Archaeologies (pp. 85-103). Cham: Springer International Publishing.</p>
            <p>Sedmidubsky, J., Budikova, P., Dohnal, V. and Zezula, P. (2020), Motion words: A text-like representation of 3d skeleton sequences, <hi rend="italic">in </hi>‘European Conference on Information Retrieval’, Springer, pp. 527–541.</p>
            <p>Shen, H., Bednarz, T., Nguyen, H., Feng, F., Wyeld, T., Hoek, P. J. and Lo, E. H. (2019), ‘Information visualisation methods and techniques: State-of-the-art and future directions’, <hi rend="italic">Journal of Industrial Information Integration </hi>16, 100–102.</p>
            <p>Van der Maaten, L. and Hinton, G. (2008), ‘Visualizing data using t-sne’, <hi rend="italic">Journal of Machine Learning Research </hi>9(86), 2579–2605. URL: <hi rend="italic">http://jmlr.org/papers/v9/vandermaaten08a.html</hi>
            </p>
            <p>Whitelaw, M. et al. (2015), ‘Generous interfaces for digital cultural collections’.</p>
            <p>Winters, J. and Prescott, A. (2019), ‘Negotiating the born-digital: a problem of search’, Archives and Manuscripts 47(3), 391–403.</p>
            <p>Wright, R. (2017), ‘The future of television archives - digital preservation coalition’.</p>
            <p>URL: <hi rend="italic">https://www.dpconline.org/blog/wdpd/the-future-of-television-archives</hi>
            </p>
            <p>ZKM | Center for Art and Media Karlsruhe (2023), ‘William Forsythe: Improvisation Technologies. The Website Project’ Accessed: 30 June 2023 URL: https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project </p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl/>
         </listBibl>
      </back>
   </text>
</TEI>
