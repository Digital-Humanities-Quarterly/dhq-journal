<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">Augmenting Access to Embodied Knowledge Archives: A Computational Framework</title>
            <dhq:authorInfo>
               <dhq:author_name>Giacomo <dhq:family>Alliata</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0001-8281-4451</idno>
               <dhq:affiliation>Laboratory for Experimental Museology, EPFL, Switzerland</dhq:affiliation>
               <email>giacomo.alliata@epfl.ch</email>
               <dhq:bio>
                 <p>Giacomo Alliata is pursuing a Ph.D. at the Laboratory for Experimental Museology (eM+) at EPFL, Switzerland. After obtaining a 
                   Master's degree in Digital Humanities at EPFL, Giacomo decided to specialise himself in the field of experimental museology, 
                   leveraging the newest technologies to create interactive and innovative ways to approach cultural and heritage collections. He 
                   believes this kind of digital installation can offer a more compelling exploration of large archives, turning visitors to cultural 
                   exhibitions from mere spectators into true actors of the experience. His research interests include the science of interactions, 
                   theories of embodiment, and the narrative component of immersive environments for cultural heritage.
                 </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Yumeng <dhq:family>Hou</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-7908-0693</idno>
               <dhq:affiliation>Laboratory for Experimental Museology, EPFL, Switzerland</dhq:affiliation>
               <email>yumeng.hou@epfl.ch</email>
               <dhq:bio>
                 <p>Yumeng Hou conducts research at the intersection of digital humanities, intangible cultural heritage, and computational archives. 
                    She is currently completing her Ph.D. at the Laboratory for Experimental Museology (eM+) at EPFL, Switzerland, with a defence date 
                    in the summer of 2024. Prior to her doctoral studies, Hou obtained her MSc in Computer Science from EPFL in 2017 and a BEng in 
                    Digital Media Technology from Zhejiang University in 2014. She has interdisciplinary experience in research, teaching, and industry, 
                    spanning fields such as digital museology, data visualisation, visual analytics, human-computer interaction, and media cloud 
                    technologies.
                 </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Sarah <dhq:family>Kenderdine</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-7190-9946</idno>
               <dhq:affiliation>Laboratory for Experimental Museology, EPFL, Switzerland</dhq:affiliation>
               <email>sarah.kenderdine@epfl.ch</email>
               <dhq:bio>
                 <p>Professor Sarah Kenderdine researches at the forefront of interactive and immersive experiences for galleries, libraries, archives, 
                    and museums. In widely exhibited installation works, she has amalgamated tangible and intangible cultural heritage with new media 
                    art practice, especially in the realms of interactive cinema, augmented reality, and embodied narrative. Sarah has produced 90 
                    exhibitions and installations for museums worldwide, including a museum complex in India, and she has received a number of major 
                    international awards for this work. In 2017, Sarah was appointed professor at the École Polytechnique fédérale de Lausanne (EPFL), 
                    Switzerland where she built the Laboratory for Experimental Museology (eM+), exploring the convergence of cultural heritage, imaging 
                    technologies, immersive visualisation, digital aesthetics, and cultural (big) data. Since 2017, Sarah is the director and lead 
                    curator of EPFL Pavilions, a new art/science initiative. In 2020, she was named in the <title rend="quotes">Museum Influencer List 
                    2020 – The Power 10</title> by Blooloop, as well as <title rend="quotes">Switzerland's Top 100 Digital Shapers</title> by Bilanz in 
                    2020 and 2021. In 2021, Sarah was appointed to be a corresponding fellow of The British Academy.
                 </p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000722</idno>
            <idno type="volume">018</idno>
            <idno type="issue">3</idno>
            <date when="2024-07-19">19 July 2024</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#cultural_heritage"/>
               <term corresp="#access"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item>intangible cultural heritage</item>
                  <item>embodied knowledge</item>
                  <item>computational archival science</item>
                  <item>movement computing</item>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000722/000722.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
           <p>With the burgeoning use of digital technologies in safeguarding intangible and living heritage, memory institutions have produced a significant body 
              of material yet to be made accessible for public transmission. The quest for new ways to unlock these massive collections has intensified, 
              especially in the case of embodied knowledge embedded in complex formats, such as audiovisual and motion capture data.
           </p>
           <p>This study examines a computational workflow that combines posture recognition and movement computing to bridge the gap in accessing digital 
              archives that capture living knowledge and embodied experiences. By reflecting on how embodied knowledge is sustained and potentially 
              valorised through human interaction, we devise a series of methods utilising vision-based feature extraction, pose estimation, movement analysis, 
              and machine learning. The goal is to augment the archival experience with new modes of exploration, representation, and embodiment.
           </p>
           <p>This article reports the computational procedures and algorithmic tools inspected through two use cases. In the first example, we visualise the 
              archives of the <name>Prix de Lausanne</name>, a collection of 50 years of video recordings of dance performances, for archival exploration through 
              the dancers' poses. In another experiment, movement encoding is employed to allow multimodal data search via embodied cues in the 
              <name>Hong Kong Martial Arts Living Archive</name>, a comprehensive documentation of the living heritage of martial arts that is chiefly comprised 
              of motion-captured performances by masters.
           </p>
           <p>Though holding different application purposes, both projects operate on the proposed framework and extract archive-specific features to create a 
              meaningful representation of human bodies, which reveals the versatile applications that computational capacities can achieve for embodied knowledge 
              archives. The practices also represent a model of interdisciplinary involvement where the archivists, computists, artists, and knowledge holders 
              join hands to renew strategies for archival exploration and heritage interpretation in a new light.
           </p>
         </dhq:abstract>
         <dhq:teaser>
           <p>This study examines a computational workflow that combines posture recognition and movement computing to bridge the gap in accessing 
              digital archives that capture living knowledge and embodied experiences. By analysing and visualising such archives through bodily 
              features, we aim to enhance archival interaction in the context of digital museology, as demonstrated through two use cases.
           </p>
         </dhq:teaser>
      </front>
      <body>
          <div>
            <head>1 Introduction</head>
              <p>Living heritage, also known as intangible culture, encompasses the practices, expressions, spaces, and knowledge that safeguard the rich 
                 diversity of human creativity and collective heritage, as defined by <name>UNESCO</name>. The preservation and exploration of living heritage 
                 has been of paramount importance in understanding and celebrating the cultural fabric of our society. To further enhance our engagement with this 
                 heritage, advancements in digital technologies have opened new possibilities for capturing and experiencing embodied knowledge.
              </p>
              <p>In recent years, there has been a growing trend in the digital capturing of living heritage and embodied knowledge 
                 <ptr target="#hou_et_al_2022"/>. These endeavours aim to document and preserve not only physical artefacts but also the intangible aspects of 
                 culture, such as rituals, performances, and traditional practices. However, GLAM sectors, encompassing galleries, libraries, archives, and 
                 museums, face significant challenges in effectively managing and providing access to these vast digital repositories 
                 <ptr target="#jaillant_2022"/>.
              </p>
              <p>One of the key challenges faced by GLAM sectors is the sheer scale of digitised collections. As more and more heritage materials are digitised, 
                 the volume of data grows exponentially, making it increasingly difficult to navigate and explore these archives manually and essentially turning 
                 these collections into <quote rend="inline">dark archives</quote> inaccessible to the public. Giovanna Fossati et al. further highlight how 
                 non-expert audiences are in need of more intuitive modes of access to discover these collections <ptr target="#fossati_et_al_2012"/>. To address 
                 this challenge, recent research has embarked on applying computational analysis, artificial intelligence (AI) in particular, to unlock various 
                 new modes of archival experience. Leveraging the transformative potential of AI<note>By <word>transformative potential</word>, we mean the 
                 potential of AI to augment the data available by extracting new features that can open innovative venues for accessing the archive.</note>, we 
                 can enhance the retrieval, visualisation, and navigation of multimodal cultural data materials <ptr target="#aske_giardinetti_2023"/>. 
                 <ptr target="#cameron_franks_hamidzadeh_2023"/> <ptr target="#hou_seydou_kenderdine_2023"/>. Further, by involving the collaborative efforts of 
                 cultural domain experts in configuring AI tools, there is a promise to alleviate the gap between data science and GLAM sectors, highlighted 
                 through various uses cases on the implementation of AI strategies in Swedish heritage organizations 
                 <ptr target="#griffin_wennerstrom_foka_2023"/>, and  enable users to interact with digitised archives in meaningful yet trustworthy contexts 
                 <ptr target="#jaillant_rees_2023"/>.
              </p>
              <p>To tackle these challenges, we propose a computational framework that augments the archival experience of embodied knowledge. Our approach 
                 intends to extract human movements in the form of skeleton data and process it, showing its versatility through multiple use cases. By 
                 revalorizing and visualising knowledge, memory, and experience, we strive to enhance the cognitive reception and engagement of users, ultimately 
                 improving the overall archival user experience. Our experiments are conducted on two exemplar collections: the archives of the 
                 <name>Prix de Lausanne</name>, comprising an audiovisual dataset of dance performances, and the <name>Hong Kong Martial Arts Living Archive</name> 
                 (HKMALA), which contains both videos and motion capture recordings of <name>Hong Kong</name> martial arts performers. This work might be of 
                 interest to professionals of cultural heritage institutions wishing to explore new methods of access to their collections, as well as researchers 
                 of computational humanities with a focus on embodied knowledge archives.
              </p>
              <p>By exploring this computational framework, we hope to contribute to the ongoing efforts to leverage visual AI technologies to transform the way 
                 we interact with and appreciate living heritage. Through the synthesis of cultural heritage and cutting-edge computational methods, we can bridge 
                 the gap between the past and the present, enabling a deeper understanding and appreciation of our shared human history.
              </p>
          </div>
          <div>
            <head>2 Related Work</head>
              <div>
                <head>2.1 Embodied Knowledge as Part of Living Heritage</head>
                  <p>Though partly embedded in architectural sensorium and memory objects, intangible cultural heritage (ICH) is inherently living and manifests 
                     through bodily expressions, individual practices, and social enactment — on the multiple layers of embodiment. Recent research has revealed a 
                     stress shift in documenting, describing, and presenting the living nature of ICH by transforming intangible practices into tangible 
                     choreographic objects. For instance, the i-Treasures project (2013-2017) works to digitize the living heritage of folk dances, folk singing, 
                     craftsmanship, and contemporary music composition in an attempt to sustain traditional know-how via creative presentation 
                     <ptr target="#dimitropoulos_et_al_2014"/> <ptr target="#grammalidis_dimitropoulos_2015"/>. Taking a more public-facing approach, the 
                     Terpsichore project (2016 to 2020) integrates computational models, such as semantic web technologies and machine learning, with storytelling 
                     to facilitate data accessibility and affordance of digitised materials <ptr target="#doulamis_et_al_2017"/>. Famous choreographers have also 
                     been keen on documenting their practices with audiovisual recordings, resulting in extensive collections such as <name>William 
                     Forsythe</name>'s Performative Archive, recently acquired by the <name>ZKM Center for Art and Media Karlsruhe</name>. While its focus differs 
                     from the performing arts, the <name>Hong Kong Martial Arts Living Archive</name> (HKMALA) project (2012-present) forges a multimodal digital 
                     archiving paradigm by motion-capturing martial art sequence performances of renowned masters. The motion-captured (MoCap) records are 
                     situated within a digital collation capturing different facets of <name>Hong Kong</name>'s martial arts traditions, encompassing rituals, 
                     histories, re-enactments, and stories of practitioners <ptr target="#chao_et_al_2018"/>.
                  </p>
                  <p>The embodied facet of ICH not only refers to the knowledge of the body but also that which is dwelling in and enacted through the body 
                     <ptr target="#craig_et_al_2018"/>. Movements and gestures are considered typical mediums to express and process mindfulness. In various 
                     cases, they also mediate interactive processes, such as human-environment communication and knowledge formation. Movement data usually 
                     documents the active dimension of mankind's creativity over time. Hence, as <name>Tim Ingold</name>'s delineates, instead of targeting solely 
                     result-driven examinations, the ideal usage of such data should facilitate a new mode of computation conveying the open-ended information 
                     embedded in bodily practices <ptr target="#salazar_2018"/> <ptr target="#ingold_2011"/>.<note><name>Tim Ingold</name> argues that some 
                     movement is <quote rend="inline">automatic and rhythmically responsive</quote> to its surroundings and <quote rend="inline">along multiple 
                     pathways of sensory participation</quote> <ptr target="#ingold_2011"/>.</note> In accordance with this ideal, MoCap technologies have gained 
                     increasing popularity in ICH archiving. MoCap allows data collection to be neutral, simultaneous, <quote rend="inline">beyond the frame and 
                     within the volume</quote> <ptr target="#delbridge_2015"/>, thereby fostering a perceivable human presence in the virtual CH environments, as 
                     surveyed by <name>Alan Chalmers</name> et al. <ptr target="#chalmers_et_al_2021"/>.</p>
              </div>
              <div>
                <head>2.2 Computational (Embodied) Archives</head>
                  <p>Digitization of archives allows researchers and practitioners to apply modern computational methods to these collections. A recent review by 
                     <name>Colavizza</name> et al. has outlined multiple axes on which AI can improve archival practices by automating recordkeeping processes and 
                     improving access to these collections as well as fostering new forms of digital archives <ptr target="#colavizza_et_al_2021"/>. Additionally, 
                     <name>Lev Manovich</name> has put forward a new way to analyse large datasets of cultural items through computational means, a method he has 
                     named <quote rend="inline">cultural analytics</quote> <ptr target="#manovich_2020"/>. Other researchers, such as Leonardo Impett, have focused 
                     specifically on computer vision models to better understand large visual cultural collections through <quote rend="inline">distant 
                     viewing</quote> <ptr target="#impett_2020b"/>.
                  </p>
                  <p>From the perspective of experimental museology, computational approaches can help improve the <quote rend="inline">civic value</quote> of 
                     cultural archives <ptr target="#edmondson_2004"/>. Through its operationalisation, the archive is augmented on three main levels 
                     <ptr target="#kenderdine_mason_hibberd_2021"/>. First, it enters the social and immersive dimension of situated museological experiences. 
                     Second, archivists engage in new interdisciplinary exchanges with media experts and the computer science community to solve many of the 
                     technical challenges cultural institutions face today. Third, new narratives can be imagined by extracting novel features and revealing 
                     <quote rend="inline">hidden structures</quote> in the dataset <ptr target="#olesen_et_al_2016"/>.
                  </p>
                  <p>These hold especially true for audiovisual recordings, one of the primary examples of collections used to capture and document embodied 
                     practices. As the foremost mnemonic records of the 21st century, these moving image archives are part of our daily lives. In these last 
                     decades, major broadcasting institutions have digitised their entire collections. For instance, in <name>Switzerland</name>, the 
                     <name>Radio Télévision Suisse</name> (<name>RTS</name>) has more than 200,000 hours of footage <ptr target="#rtsarchives_2018"/>, while in the 
                     <name>United Kingdom</name>, the <name>British Broadcasting Corporation</name> (<name>BBC</name>) preserves more than a million recorded 
                     hours <ptr target="#wright_2017"/>. In parallel, we are observing many advances in the field of visual AI dedicated to human pose estimation  
                     with state-of-the-art models such as OpenPose <ptr target="#cao_et_al_2019"/>, OpenPifPaf <ptr target="#kreiss_bertoni_alahi_2021"/>, and 
                     BlazePose <ptr target="#bazarevsky_et_al_2020"/>. These AI algorithms can reliably extract human poses and movements from large moving image 
                     collections at scale, essentially creating computational embodied archives. The wealth of new data extracted by this process can then be 
                     further processed, unlocking new modes of access and novel ways of exploring these archives.
                  </p>
              </div>
              <div>
                <head>2.3 Human Bodies as a Way of Access</head>
                  <p>In the field of digital humanities, various projects have relied on such approaches to analyse, explore, and better understand visual 
                     collections through human bodies. In digital art history, for instance, researchers have undertaken a comprehensive analysis of large art 
                     corpora through human gestures <ptr target="#bernasconi_cetnic_impett_2023"/> <ptr target="#impett_2020a"/>. Similarly, in collaboration with 
                     choreographer <name>Wayne McGregor</name>, the <name>Google Arts &amp; Culture Lab</name> has developed the Living Archive, a web-based 
                     interface to navigate the collection of postures in <name>McGregor</name>'s choreographies and create new movements in the choreographer's 
                     style <ptr target="#mcgregor_lab_2019"/>. Furthermore, movement computing has gained popularity as an intelligent approach to transform 
                     the performed cultures into <quote rend="inline">choreographic objects</quote>, and it is used for analysing, visualising, and interacting 
                     with datasets of dance heritage <ptr target="#doulamis_et_al_2017"/>. <name>Andreas Aristidou</name> et al. developed a Labanotation 
                     <ptr target="#guest_1977"/> based framework for transforming movements in Cypriot dances to a high-dimensional feature model and constructing 
                     a deep-learned <term>motion signature</term> for similarity analysis <ptr target="#aristidou_et_al_2018"/> 
                     <ptr target="#aristidou_shamir_chrysanthou_2019"/>. Improved from <term>deep signature</term> encoding 
                     <ptr target="#aristidou_shamir_chrysanthou_2019"/>, <name>Jan Sedmidubsky</name> et al. invented a text-like representation of 3D 
                     skeleton sequences and employed the benefits of text-learning models in a more complicated context <ptr target="#sedmidubsky_et_al_2020"/>. 
                     Likewise, <name>Katerina El Raheb</name> et al. combines posture recognition and <name>Benesh</name> movement notation 
                     <ptr target="#benesh_benesh_1977"/> to assist with multimedia annotation and interactive learning. In addition to a solely choreographic 
                     focus, the Nrityakosha project synthesises a marriage of detection algorithms and semantic models. The researchers related embodied 
                     attributes to concepts of Indian classical dances and, correspondingly, created a specialised ontology for describing knowledge in the 
                     multimedia archive <ptr target="#mallik_chaudhury_2012"/> <ptr target="#mallik_chaudhury_ghosh_2011"/>. By applying the methodology of 
                     <quote rend="inline">distant viewing</quote> <ptr target="#arnold_tilton_2019"/> to embodied knowledge archives, <name>Peter Broadwell</name> 
                     and <name>Timothy R. Tangherlini</name> propose a computational analysis of K-pop dance, leveraging human-pose estimation algorithms applied 
                     to audiovisual recordings of famous K-pop groups and idols performances <ptr target="#broadwell_tangherlini_2021"/>. Finally, readers will 
                     find additional interesting use cases in the comprehensive review provided by <name>Clarisse Bardiot</name> in 
                     <title rend="italic">Performing Arts and Digital Humanities: From Traces to Data</title> <ptr target="#bardiot_2021"/>.
                  </p>
              </div>
              </div>
          <div>
            <head>3 Our Computation Framework</head>
              <div>
                <head>3.1 Rationale</head>
                  <p>The high-level goal of our computational framework is to enrich an embodied knowledge archive by extracting human skeleton data. This wealth 
                     of new data, in the form of sets of keypoints, captures both static postures (when looking at individual frames) as well as dynamic motions 
                     (when adding the temporal sequences of skeletons from successive frames). By applying motion extraction algorithms, we can therefore 
                     operationalise the abstract features of human poses and movements, essentially translating them into vectors that can then be further 
                     processed through other computational methods. Such a process augments the archive and unlocks a multitude of new scenarios, examples of which 
                     will be discussed through our two use cases. In particular, we investigate visualisations of the whole archive through human poses as well 
                     as motion-based retrieval.
                  </p>
              </div>
              <div>
                <head>3.2 Methodology</head>
                  <p>Based on the relevant literature reviewed above, we propose the following general framework to augment embodied knowledge archives and create 
                     new modes of experiencing them. This method applies to both moving image and MoCap archives, with the main difference in the first step: 
                     extracting human poses. Indeed, when working with videos, human pose estimation models such as OpenPose <ptr target="#cao_et_al_2019"/> 
                     are required to extract human postures in the form of a <q>skeleton</q>, or a list of keypoint edge pairs, as defined in the COCO dataset 
                     <ptr target="#lin_et_al_2014"/>. Depending on the model, these can be in two or three dimensions and handle single or multiple persons at the 
                     same time. Furthermore, we stress that these models can sometimes fail to accurately detect full bodies, especially in situations where part 
                     of the body is occluded. The number of keypoints detected can also impact how much these models are able to capture features specific to a 
                     certain discipline. While MoCap data generally comes with skeleton information, post-processing, normalisation, and format conversion are 
                     often necessary to produce clean and operable skeleton datasets.
                  </p>
                  <p>Once skeleton data is extracted, poses need to be normalised so that they can be meaningfully compared. This involves scaling the skeleton 
                     data with respect to the image sizes, in the case of video datasets, as well as centring and scaling with respect to pose size. Subsequently, 
                     feature vectors can be computed based on this normalised set of keypoints. We note that these features can vary a lot from one application 
                     to another, especially if one is working with static poses (as in our first use case) or with motions (as in our second use case).
                  </p>
                  <p>These steps result in a computational embodied archive, in which each element is a human pose, as a set of normalised skeleton keypoints, 
                     linked to its corresponding frame or timestamp in the recorded item and potentially augmented with additional computed features. The wealth 
                     of data extracted can then be further processed for a variety of scenarios, of which we present two examples. The first use case of this 
                     paper, which addresses a collection of dance performance video recordings, will explore how the whole dataset can be mapped in two dimensions 
                     through the poses dancers take in their performances. Afterwards, we present a second scenario on a dataset of martial arts performances, 
                     extending to motion-based similarity retrieval.
                  </p>
              </div>
          </div>
          <div>
            <head>4 Use Case 1: Accessing an Audiovisual Dance Performance Archive via Human Poses</head>
              <div>
                <head>4.1 The <name>Prix de Lausanne</name> Audiovisual Archive</head>
                  <p>The <name>Prix de Lausanne</name> is a competition for young dancers held yearly in <name>Lausanne</name>, <name>Switzerland</name> since 
                     1973<note>See <ref target="https://www.prixdelausanne.org/">https://www.prixdelausanne.org</ref></note>. Nearly each year of the competition 
                     has been recorded and digitised, resulting in a rich dataset of 1,445 mp4 videos of individual dance performances across forty-two years, 
                     along with information on the dancers and their chosen performances. This metadata, although valuable, does not document the embodied 
                     elements of dance performances, limiting how one can access such an archive. With our solution, we augmented the available data using 
                     computational methods and employed the extracted features to map the whole dataset in two dimensions through the human poses embedded in the 
                     material, revealing its hidden structures and creating new connections between similar poses.
                  </p>
              </div>
              <div>
                <head>4.2 Building Feature Vectors for the Dancers' Poses</head>
                  <p>We extracted human skeleton data using the MediaPipe BlazePose GHUM 3D model <ptr target="#bazarevsky_et_al_2020"/>, which was mainly chosen 
                     for its out-of-the-box robustness and rapidity of inference for single human estimation. In the case of the <name>Prix de Lausanne</name>, 
                     since the videos record one participant at a time on stage, we found this model to be the most suitable. We also observed only occasional 
                     cases of occlusions when the dancers are wearing particularly large costumes. The BlazePose algorithm outputs skeleton data as a set of 33 
                     keypoints in three-dimensional space, normalised to the image size. Compared to models trained on the COCO dataset, MediaPipe algorithm 
                     detects more keypoints, including the index fingers and index toes of the two hands and the two feet, allowing us to compute the angles of 
                     the wrists and ankles. Although the algorithm was not specifically trained on dance recordings, we believe these additional features help to 
                     better capture specific poses of dance performances, where even the positions of the hands and feet are crucial. Videos were processed to 
                     extract one pose every five seconds, a cautious design decision to avoid over-collecting similar poses merely due to temporal closeness. We do 
                     note however this is potentially not the best solution to ensure a full coverage of the diversity of poses, and future research in this 
                     direction could improve the results presented even further. It is worth noting that not all frames necessarily had a detected pose, since in 
                     some cases the dancer was too far away from the camera, or because some videos were not properly cut to the exact dance performance and thus 
                     had irrelevant frames at the beginning and/or the end. We measured the rate of detection by taking the mean of the ratio of poses detected 
                     over the number of frames per video, obtaining a mean detection rate of 76.80% across all the videos. The pose extraction process resulted in 
                     a total of 27,672 poses.
                  </p>
                  <p>Once these poses had been extracted, we normalised them following a two-step procedure. We first subtracted the pose centre, defined as the 
                     mean point between the left and right hips. We then scaled the pose by its size, defined as the maximum between the torso size and the maximum 
                     distance between the pose centre and any keypoint, where the torso size was computed as the distance between the centre of the shoulders and 
                     the pose centre (the centre of the hips). The procedure ensures meaningful comparison between the poses by normalising them. An intuitive 
                     sample of normalised poses with their corresponding frames is provided in Figure 1.
                  </p>
                  <figure xml:id="figure01">
                    <head>Sample of normalised poses with corresponding frames. Only the main keypoints are displayed for ease of visualization (the nose, the 
                      shoulders, the elbows, the wrists, the hips, the knees, and the ankles).</head>
                    <figDesc>Random sample of frames from the <name>Prix de Lausanne</name> videos displayed alongside the corresponding human pose extracted. The 
                      human pose is represented by its skeleton of keypoints. Only the main keypoints are displayed for ease of visualisation (the nose, the 
                      shoulders, the elbows, the wrists, the hips, the knees, and the ankles).</figDesc>
                      <graphic url="resources/images/figure01.png"/>
                  </figure>
                  <p>Subsequently, two feature vectors were built and tested. For the first, we simply took the vector of the keypoints and flattened it, 
                     resulting in a 99-dimensional vector. For the second, we built a custom feature vector, taking a combination of pose lengths (distances 
                     between two keypoints) and pose angles (angles defined by three keypoints). Table 1 shows the features computed.
                  </p>
                  <table>
                    <head>A list of custom features used to represent dance poses in the <name>Prix de Laussanne</name> archive.</head>
                      <row role="label">
                        <cell role="label">Category</cell>
                        <cell role="label">Feature</cell>
                      </row>
                      <row>
                        <cell role="label">Pose Lengths<lb/>(Distances Between Joints)</cell>
                        <cell>Left elbow — Right elbow<lb/>Left wrist — Right wrist<lb/>Left knee — Right knee<lb/>Left ankle — Right ankle
                          <lb/>Left hip — Left wrist<lb/>Right hip — Right wrist<lb/>Left hip — Left ankle<lb/>Right hip — Right ankle<lb/>
                          Left ankle — Left wrist<lb/>Right ankle — Right wrist<lb/>Left shoulder — Right ankle<lb/>Right shoulder — Left ankle<lb/>
                          Left wrist — Right ankle<lb/>Right wrist — Left ankle
                        </cell>
                      </row>
                      <row>
                        <cell role="label">Pose Angles<lb/>(Defined by Three keypoints)</cell>
                        <cell>Left hip — Left knee — Left ankle<lb/>Right hip — Right knee — Right ankle<lb/>Left shoulder — Left elbow — Left wrist<lb/>
                          Right shoulder — Right elbow — Right wrist<lb/>Left hip — Left shoulder — Left elbow<lb/>Right hip — Right shoulder — Right 
                          elbow<lb/>Left shoulder — Left hip — Left knee<lb/>Right shoulder — Right hip — Right knee<lb/>Left elbow — Left wrist — Left 
                          index finger<lb/>Right elbow — Right wrist — Right index finger<lb/>Left knee — Left ankle — Left index toe<lb/>Right knee — Right 
                          ankle — Right index toe
                        </cell>
                      </row>
                  </table>
              </div>
              <div>
                <head>4.3 Mapping the <name>Prix de Lausanne</name> Archive</head>
                  <p>One of the fundamental applications unlocked by computational archives is the possibility to visualise whole collections. To this end, we 
                     apply dimensionality reduction (DR) algorithms to embed the high-dimensional feature spaces in two or three dimensions, in order to create a 
                     novel way of visualising and navigating the whole archive. We tested both standard DR methods, such as Principal Component Analysis (PCA), 
                     and t-distributed Stochastic Neighbour Embedding (tSNE), as well as a more recent approach, Uniform Manifold Approximation and Projection 
                     (UMAP). We leveraged scikit-learn implementations for the two first algorithms and the official UMAP Python implementation for the latter 
                     <ptr target="#pedregosa_et_al_2011"/> <ptr target="#halko_martinsson_tropp_2011"/> <ptr target="#martinsson_rokhlin_tvgert_2011"/> 
                     <ptr target="#van-der-maaten_hinton_2008"/> <ptr target="#mcinnes_et_al_2018"/>. For tSNE and UMAP we compared using cosine and Euclidean 
                     metrics as the distance measure to compute the projection. We therefore tested five options on 27,672 poses, with both sets of feature 
                     vectors. Figure 2 shows the resulting embeddings in two dimensions.
                  </p>
                  <figure xml:id="figure02">
                    <head>Embeddings of 27,672 poses in two dimensions. The top row (blue) uses the flattened vectors of keypoints, while the bottom row (red) 
                      uses the custom features computed. From left to right, the algorithms used are PCA, tSNE (cosine metric), tSNE (Euclidean metric), UMAP 
                      (cosine metric), and UMAP (Euclidean metric).</head>
                    <figDesc>Scatter plots of the embeddings of 27,672 poses in two dimensions, produced by different Dimensionality Reduction algorithms with 
                      different parameters (from left to right, the algorithms used are PCA, tSNE with cosine metric, tSNE with Euclidean metric, UMAP with cosine 
                      metric, and UMAP with Euclidean metric). The top row (blue) uses the flattened vectors of keypoints, while the bottom row (red) uses the 
                      custom features computed.</figDesc>
                      <graphic url="resources/images/figure02.png"/>
                  </figure>
                  <p>We can immediately observe that PCA performs poorly on this dataset, as expected due to its linear nature that cannot preserve much variance 
                     with just two dimensions (only 49% with the flattened vectors of keypoints and 45% with the custom features). Both tSNE and UMAP, however, 
                     give much more interesting results, where structures in the dataset can be observed. Since we did not observe meaningful differences between 
                     the two sets of feature vectors used, we pursued further examinations with the flattened vectors of keypoints to exploit the heuristic 
                     quality of the algorithms independent of the choice of features. Therefore, we set out to investigate tSNE and UMAP behaviours in more depth 
                     by studying the effects of their hyperparameters. For tSNE, we observed how the embedding in two dimensions behaves for different 
                     perplexities (<formula rend="inline" notation="tex">\(ϵ\)</formula>), the main parameter for this algorithm, related to the number of 
                     neighbours considered for any given point during the computation. For UMAP, we investigated the number of nearest neighbours 
                     (<formula rend="inline" notation="tex">\(𝑛\)</formula>), controlling how UMAP balances local versus global structure in the data, as well as 
                     the minimum distance (<formula rend="inline" notation="tex">\(𝑑_{min}\)</formula>) the algorithm is allowed to pack points together in the 
                     low-dimensional space. We used the Euclidean distance as it seemed to produce slightly clearer results. Figure 3 shows how tSNE behaves for 
                     different values of <formula rend="inline" notation="tex">\(ϵ\)</formula>, while Figure 4 displays the results of UMAP with varying 
                     <formula rend="inline" notation="tex">\(𝑛\)</formula> and <formula rend="inline" notation="tex">\(𝑑_{min}\)</formula> values.
                  </p>
                  <figure xml:id="figure03">
                    <head>Effect of the perplexity (ϵ) on the tSNE embeddings of 27,672 poses (flattened vectors of keypoints, Euclidean distance). Lower values 
                      of ϵ result in a more granular embedding.</head>
                    <figDesc>Scatter plots of the embeddings of 27,672 poses in two dimensions showing the effect of the main parameter perplexity on the tSNE 
                      algorithm (on the flattened vectors of keypoints with Euclidean distance). The figure shows that lower perplexities result in a more granular 
                      embedding, losing much of the global structure.</figDesc>
                      <graphic url="resources/images/figure03.png"/>
                  </figure>
                  <figure xml:id="figure04">
                    <head>Effect of the number of neighbours 𝑛 and the minimum distance 𝑑<hi rend="subscript">𝑚𝑖𝑛</hi> on the UMAP embeddings of 27,672 poses 
                      (flattened vectors of keypoints, Euclidean distance). Lower values of 𝑑<hi rend="subscript">𝑚𝑖𝑛</hi> result in a more packed embedding, but 
                      global structures appear to be quite stable.</head>
                    <figDesc>Scatter plots of the embeddings of 27,672 poses in two dimensions showing the effect of the main parameters number of neighbours and 
                      minimum distance on the UMAP algorithm (on the flattened vectors of keypoints with Euclidean distance). The figure shows that lower minimum 
                      distances result in a more packed embedding, but global structures appear to be quite stable across different values for the number of 
                      neighbours.</figDesc>
                      <graphic url="resources/images/figure04.png"/>              
                  </figure>
                  <p>To conclude our exploration of the <name>Prix de Lausanne</name> moving image collection, we set out to produce a map of the human poses 
                     embedded in the archive. Following the outlined procedure, we extracted the keypoints, computed the corresponding feature vectors, and 
                     applied UMAP (with Euclidean distance, <formula rend="inline" notation="tex">\(𝑛\)</formula> = 50, 
                     <formula rend="inline" notation="tex">\(𝑑_{min}\)</formula> = 0.5) on the flattened vectors of keypoints to create Figure 5, where we display 
                     the corresponding pose at each point. For the sake of visibility, we only show a portion of all the poses extracted, resulting in a map of 
                     2,768 poses. Through this visualisation, we can verify the effectiveness of our approach in grouping together similar poses, thus unlocking a 
                     new way of seeing an embodied knowledge archive.
                  </p>
                  <figure xml:id="figure05">
                    <head>Map of 2,768 dancers' poses, based on the 2D embeddings of the vectors of flattened keypoints with UMAP (Euclidean distance, 
                      𝑛 = 50, 𝑑<hi rend="subscript">𝑚𝑖𝑛</hi> = 0.5).</head>
                    <figDesc>Visualisation of 2,768 dancers' poses placed in a 2D plot based on the 2D embeddings of the flattened vectors of keypoints, obtained 
                      with the UMAP algorithm (Euclidean distance, 𝑛 = 50, 𝑑<hi rend="subscript">𝑚𝑖𝑛</hi> = 0.5).</figDesc>
                      <graphic url="resources/images/figure05.png"/>
                  </figure>
              </div>
          </div>
          <div>
            <head>5 Use Case 2: Accessing Martial Art Recordings through Motion Traits</head>
              <p>In the first use case, the extracted human skeletons were been treated as static postures, thus operating on individual frames of the moving 
                 image recordings. This is suitable for the purpose of visualising the whole archive in two dimensions because we can draw these skeletons, using 
                 them as glyphs representing frames in the collection to create compelling images, as in Figure 5. However, one might argue that embodied knowledge 
                 archives are not only about human postures but also, if not more so, about human motions. This second use case thus investigates such an approach, 
                 proposing an innovative motion-based retrieval mode. In the context of the <name>Hong Kong Martial Arts Living Archive</name> 
                 (<name>HKMALA</name>) project, a retrieval system allows users to select a specific motion within the collection (by choosing the start and end 
                 frames of a video sequence, for instance) and returns similar motions in the archive.
              </p>
                <div>
                  <head>5.1 <name>Hong Kong Martial Arts Living Archive</name></head>
                    <p>Traditional martial arts are considered knowledge treasures of humanities sustained through generations by diverse ethnic groups. Among the 
                       genres, Southern Chinese martial arts (SCMA) is arguably one of the most prolonged systems embodying Chinese mind-body ideologies, yet it is 
                       now facing challenges in knowledge transmission and the risk of being lost. In preserving the living heritage of SCMA, the 
                       <name>Hong Kong Martial Arts Living Archive</name> inspects a comprehensive set of digitisation tools to capture the martial practices, 
                       with a chief focus on motion capturing form sequence performances, or <term>taolu</term>. Since its origination in 2012, <name>HKMALA</name> 
                       has built a 4D motion archive spanning over 20 styles and 130 sets of empty-hand and weapon sequences.<note>The concept of four-dimensional 
                       (4D) space denotes a dynamic 3D space moving through time.</note> The archive also collates various historical and reconstituted 
                       materials documenting martial cultures, encompassing rituals, traditions, armaments, and objects preserved in <name>Hong Kong</name> 
                       <ptr target="#chao_et_al_2018"/>.
                    </p>
                    <p>The <name>HKMALA</name>'s exceptional volumes hold the promise to enable various scholarly inquiries. However, its current archival 
                       organisation is based on a manual catalogue that merely indicates how the content has been curated for exhibitions. Additionally, the data 
                       consists of multiple modalities, such as texts, images, videos, and MoCap data, thereby impeding public access and dissemination at scale. 
                       There is a clear need for an effective yet meaningful way of data access.
                    </p>
                    <p>Addressing the aforementioned challenges, we propose devising a motion-based retrieval framework that leverages machine learning to encode 
                       motion-wise information in multimodal recordings. The approach facilitates content retrieval through embodied cues, operating seamlessly 
                       across MoCap and moving image data formats. In the following paragraphs, we aim to provide an overview of the technical procedures. For a 
                       more comprehensive understanding of the implementation process, readers are invited to refer to the descriptions in 
                       <ptr target="#hou_seydou_kenderdine_2023"/>.
                    </p>
                </div>
                <div>
                  <head>5.2 Encoding and Retrieving Martial Arts Movements</head>
                    <p>Chinese martial arts are renowned for their strong emphasis on form training, which utilizes codified movement sequences to practise 
                       fundamental techniques and technique combinations. Less known, yet equally important, is the stress on combative tactics that involve  
                       mindful operation of whole-body movement. Thus, training is imparted through a combination of freeform combat and integration into 
                       <term>taolu</term> practices <ptr target="#ma_2003"/>. Given that assessment methods in Chinese martial arts typically consider visually 
                       discernible characteristics and mechanical parameters, we applied the same configurations to model martial arts movements. Designed in 
                       compatibility with Laban metrics for movement analysis <ptr target="#guest_1977"/>, the approach aims to articulate both qualitative and 
                       quantitative qualities regarding the technical stances, ballistic activities, and reactions to the opponent or surroundings, as shown in 
                       Table 1 in <ptr target="#hou_seydou_kenderdine_2023"/>.
                    </p>
                    <p>The process began with the extraction of 3D human skeletons from MoCap data, or the extraction of 2D poses from video recordings, followed 
                       by depth estimation to reconstruct the 3D structure. The coordinates and joint angles were then computed from raw skeletons into basic 
                       kinematic metrics, which were normalized for further processing. Feature augmentation was employed to enrich the kinematic feature set, 
                       incorporating metrics that reflect linear and angular motion characteristics, along with the transition probabilities of motion time 
                       series. Finally, the top 50 features with the highest variance were selected using the variance threshold approach and used to train a 
                       discriminative neural network.
                    </p>
                    <p>Holding a hypothesis that the characteristics of a long motion sequence can be represented by its segmentations, our encoding method was 
                       aimed to pinpoint representative motion units within the sequence. Firstly, we gathered high-dimensional features from equally segmented 
                       media data and employed deep learning to train a latent space, embedding them into a vector space where semantically similar dimensions are 
                       clustered together while dissimilar ones are set apart. Subsequently, K-means clustering was applied to identify cluster centres as 
                       representatives. To create a descriptive and compact scheme for efficient retrieval, we encoded the inverse document frequency (IDF) of 
                       each representative into a normalised histogram, or <term>metamotion</term>, to represent the distribution of archetypal qualities within a 
                       motion sequence.
                    </p>
                </div>
                <div>
                  <head>5.3 A Motion-Based Retrieval Engine</head>
                    <p>To facilitate the envisioned scenario of exploratory archival browsing, a retrieval system was deployed to allow querying archival 
                       sequences containing similar movements. The system integrates different retrieval algorithms, including locality-sensitive hashing (LSH) and 
                       Ball-tree methods, which prove effective in achieving optimal retrieval efficiency, outperforming the baseline approach in existing 
                       research.<note>See <ptr target="#hou_seydou_kenderdine_2023" loc="section 4"/>.</note>
                    </p>
                    <p>On the user end, an interactive search engine was deployed to enable motion-based data exploration across formats. An intuitive search case 
                       is illustrated in Figure 6, showcasing a cross-model retrieval result from a hybrid of videos and MoCap data. Additionally, Figure 7 
                       demonstrates the initial development of the archival browser, presenting the retrieved items with ontological descriptions for the 
                       conceptual entities associated with the query item. This design aims to improve data explainability and interoperability for future reuse, 
                       with ontology annotations sourced from the scholarly work of <name>The Martial Art Ontology</name> (<name>MAon</name>).<note>See 
                       <name>The Martial Art Ontology</name> (<name>MAon</name>), v1.1, 
                       <ref target="https://purl.org/maont/techCorpus">https://purl.org/maont/techCorpus</ref>.</note>
                    </p>
                    <figure xml:id="figure06">
                      <head>An example of the top-5 similar sequences retrieved by a query using a MoCap dataset (left) and a video dataset (right) 
                        <ptr target="#hou_seydou_kenderdine_2023"/>.</head>
                      <figDesc>Top-5 similar sequences retrieved by a query featuring a staff performance. On the left are the top-5 MoCap sequences retrieved, 
                        and on the right are the top-5 video retrieval results. The results all exhibit certain similarities in movement, regardless of the use of 
                        a weapon.</figDesc>
                        <graphic url="resources/images/figure06.png"/>
                    </figure>
                    <figure xml:id="figure07">
                      <head>Illustration of a motion search example supplemented with ontological representations of the concepts of techniques.</head>
                      <figDesc>An illustration of a configurable search interface conducting motion search. The query involves a technique with a whipping punch 
                        and turning stance, supported by ontological representations of technical concepts. Five media sequences containing similar movements are 
                        retrieved.</figDesc>
                        <graphic url="resources/images/figure07.png"/>
                    </figure>
                </div>
          </div>
          <div>
            <head>6 Discussion</head>
              <div>
                <head>6.1 Visual AI Toward <q>Augmented Archives</q></head>
                  <p>At cultural heritage archives, including born-digital archives, metadata is commonly organised according to an archive-specific curatorial 
                     structure with topical domain-language descriptions of archival items. For instance, both the <name>Prix de Lausanne</name> and 
                     <name>HKMALA</name> collections warrant a series of textual tags describing the contents of dance and martial art performances, as well as 
                     information about performers. Such documentation, although useful, entails a mode of interpretation and access rather limited to expert users. 
                     One needs to know what dance performances such as <title rend="italic">The Giselle</title> or <title rend="italic">The Nutcracker</title> are 
                     to look for them in the <name>Prix de Laussane</name> archive, and one must understand that martial arts are comprised of a series of 
                     <term>taolu</term> in order to understand <name>HKMALA</name> collections. Furthermore, the visual and embodied features represented in these 
                     collections are difficult to capture verbally. Models that describe movement, such as those based on notation systems like Labanotation, have 
                     shown promise. However, such models typically necessitate the alignment between the movement and the notation language, as well as an 
                     advanced understanding of the subject matter by the annotator, in order to effectively encode and decode the knowledge behind movements. As 
                     a result, it is very difficult, if not impossible, to capture embodied knowledge features using literal formats or in an easily accessible 
                     way. Therefore, this kind of metadata, although necessary and useful in many domains, is not suitable for access by the general public. It 
                     does not support the <quote rend="inline">casual modes of access</quote> that are fundamental for museum-goers and general audiences 
                     <ptr target="#whitelaw_2015"/>.
                  </p>
                  <p>In this article, we have introduced a computational framework aimed at enhancing cultural archives by enriching metadata with embodied 
                     knowledge descriptors derived from audiovisual and multimodal materials. Utilising computer vision models, we extract and transform human 
                     poses and skeleton features into meaningful data features, which potentially foster a range of usage scenarios. At the archival level, 
                     curators can operationalise their collections as <quote rend="inline">data sources</quote> <ptr target="#kenderdine_mason_hibberd_2021"/>, 
                     envisioning new ways for querying, annotating, and analysing whole datasets to forge novel narrative paradigms. In parallel, the enriched 
                     metadata improves data accessibility while the diversified query channels improve data findability, augmenting the archives' compliance with 
                     FAIR (findability, accessibility, interoperability, and reusability) data principles, regardless of variations in language, topic, and data 
                     format. Furthermore, our approach demonstrates the potential for interpreting data traits within and across cultural contexts. This not only 
                     contributes to the reusability of archival content but also resonates with the findings of Colavizza et al., which highlight the growing role 
                     of AI in archival practices, particularly in the <quote rend="inline">automation of recordkeeping processes</quote> and the improvement of 
                     procedures for <quote rend="inline">organising and accessing archives</quote> <ptr target="#colavizza_et_al_2021"/>. Our experimentation, as 
                     exemplified through two distinctive heritage archives, represents an innovative step forward with a focus on embodied knowledge, allowing 
                     queries through more perceptive than quantitative channels, which we believe is more natural and humanistic.
                  </p>
              </div>
              <div>
                <head>6.2 Archive Navigation and Serendipitous Discovery</head>
                  <p>The operationalisation of embodied knowledge archives through visual AI, resulting in <q>augmented archives</q>, provides a vast array of data 
                     that can further be processed in order to create new modes of access adapted for general audiences. Following the principles of 
                     <quote rend="inline">generous interfaces</quote> <ptr target="#whitelaw_2015"/>, our method supports explorative behaviours, encouraging a 
                     new paradigm of <quote rend="inline">seeking the unknown</quote> <ptr target="#winters_prescott_2019"/>. By laying out the full archive 
                     through the lens of easily understood concepts, such as dancers' postures, users can more readily comprehend the archive without specific 
                     knowledge about the topic. Users do not need to have a specific goal in mind or be looking for something in particular. Instead, they can 
                     wander around, browsing like an <quote rend="inline">information flaneur</quote> and enjoying the <name>Prix de Lausanne</name> archive in a 
                     way that traditional modes of access, based on querying metadata or skimming grids and lists of information, could not 
                     offer <ptr target="#dork_carpendale_williamson_2011"/>. Furthermore, each pose links to a timestamp in a dance performance, grouping together 
                     similar poses in the low-dimensional space to create serendipitous discoveries. Indeed, this mode of access rewards users for simply browsing 
                     the collection and stumbling upon new performances as they move from pose to pose on the map. Figure 8 explicates this process by showcasing 
                     poses similar to an input pose with the corresponding video frames.
                  </p>
                  <figure xml:id="figure08">
                    <head>Examples of similar poses in the <name>Prix de Lausanne</name> archive (top 5 matches). Simplified poses and corresponding video frames 
                      are displayed. Notice how connections between different performers are discovered.</head>
                    <figDesc>Two examples of searching for similar poses in the <name>Prix de Lausanne</name> videos. In each row, an input frame/pose pair is 
                      matched with its top-5 nearest neighbours (displaying both the original frames and the corresponding extracted human poses). The figure 
                      demonstrates how connections between different performers are discovered.</figDesc>
                      <graphic url="resources/images/figure08.png"/>
                  </figure>
                  <p>This new mode of access is enhanced by dimensionality reduction algorithms. However, one must take care in deciding which algorithm to employ 
                     and with what parameters. In this work, we have analysed Principal Component Analysis (PCA), t-distributed Stochastic Neighbour Embedding 
                     (tSNE), and Uniform Manifold Approximation and Projection (UMAP). PCA is a well-established dimensionality reduction technique, but, due to 
                     its linear nature, it often fails to properly capture a real-life dataset with only a handful of dimensions. This is clearly confirmed in 
                     Figure 2, where all embeddings generated with tSNE and UMAP show much more interesting structures without creating a large clump of points 
                     like PCA does. Regarding the choice of feature vectors used, as mentioned, we did not observe noticeable differences between the flattened 
                     vectors of keypoints and the custom features computed. We hypothesise that the custom features (reported in Table 1) are not more 
                     discriminative than the base keypoints. Although in subsequent analyses we decided to use the flattened vectors of keypoints, we believe 
                     this offers us an opportunity for more collaboration, since involving dance experts could help us craft a more precise and adequate set of 
                     features, based on Labanotation, for instance <ptr target="#guest_1977"/>. To gain a deeper understanding of these algorithms, we have 
                     investigated the effect of parameters on tSNE and UMAP.
                  </p>
                  <p>Figure 3 shows that increasing the perplexity yields very different layouts, with lower values displaying numerous small groupings of items 
                     while higher values reveal more large-scale structures. This is in accordance with expectations, since with higher perplexities tSNE considers 
                     more points when computing the vicinity of each item, thus better capturing global structures. Surprisingly, in Figure 4, when increasing the 
                     number of neighbours (<formula rend="inline" notation="tex">\(𝑛\)</formula>) considered with UMAP, the outputs appear to be much more stable, 
                     with global structures already visible with <formula rend="inline" notation="tex">\(𝑛\)</formula> = 50. It is in this case that the second 
                     parameter, <formula rend="inline" notation="tex">\(𝑑_{min}\)</formula>, affects more the results, yielding more sprayed-out mappings the 
                     higher it is (since the minimum distance directly controls how packed the points can be in the low dimensional space). These results indicate 
                     that when creating archive visualisations, Like the map of poses in Figure 5, for instance, higher 
                     <formula rend="inline" notation="tex">\(𝑑_{min}\)</formula> might be more suitable to avoid overlaps between similar poses. However, if one 
                     were to apply clustering algorithms on these embeddings to group together similar poses, the more fine-grained and packed structures obtained 
                     with lower <formula rend="inline" notation="tex">\(𝑑_{min}\)</formula> would potentially yield better results. Therefore, we argue that it is 
                     not a matter of which embedding is better overall but rather which embedding is better adapted to the specific narrative or mode of access 
                     sought.
                  </p>
              </div>
              <div>
                <head>6.3 Limitations and Future Work Directions</head>
                  <p>Although we believe our computational framework to be well thought out, there are still some limitations we would like to highlight, hopefully 
                     to later address them.
                  </p>
                  <p>First, human pose extraction from monocular videos is never perfect. Human bodies can be incorrectly estimated or missed due to the influence 
                     of occlusion, monotonous colour patterns, or camera angles, to name a few. Indeed, through our <q>naive</q> approach to the <name>Prix de 
                     Lausanne</name> archive, we only achieved a detection rate of 76.80%.<note>A <q>naive</q> approach in pattern recognition implies a 
                     straightforward and easy-to-implement algorithm that finds all matching occurrences of a given input.</note> Furthermore, by checking a sample 
                     of skeletons extracted with the corresponding frames, as in Figure 8, we noticed that keypoints were not always correctly extracted. One 
                     possible reason is that pose estimation was done systematically every five seconds without a measure of quality or significance. Nevertheless, 
                     our approach proves sufficient for the use case described, as it still produces enough data to map the whole archive properly. Further 
                     developments could yield more interesting and precise results, for instance, by extracting skeleton data on a finer temporal granularity and 
                     then filtering only to keep the <q>better</q> poses.<note>One would first need to define what constitutes a <q>better</q> pose, however.</note>
                  </p>
                  <p>Second, the feature vectors computed for the <name>Prix de Lausanne</name> archive are somewhat naive. Taking the lesson from the HKMALA 
                     feature computing, collaboration with dance experts could facilitate the design of more relevant, dance-specific features with precise 
                     measurements able to better capture and compare human bodies during dance performances. Nonetheless, our results demonstrate that even naive 
                     methods can produce new modes of access to embodied knowledge archives. Thus, we are confident that our method can be generalised for use by 
                     other archives and diverse embodied disciplines.
                  </p>
                  <p>Lastly, unlike standard computer vision challenges, it is difficult but necessary to quantify what makes a <q>good</q> estimation of poses or 
                     movement segments in the archival context, yet the standard varies across people and cultural themes. To this end, we resorted to evaluating the 
                     distribution and quality of embeddings, supplemented with an ad-hoc expert review of small sampling sets. Future improvement is suggested to 
                     integrate expert review dynamically along with the model training process, such as by intaking expert judgement as a score and feeding it back 
                     to the model, so as to enable a human-in-the-loop machine-learning process.
                  </p>
              </div>
              <div>
                <head>6.4 Outlook: Towards a New Archival Experience</head>
                  <p>The research presented in this paper was conducted within the larger context of <quote rend="inline">computational museology</quote>  
                     <ptr target="#kenderdine_mason_hibberd_2021"/>, a discipline aimed at developing new modes of access to cultural heritage through 
                     computational approaches, specifically designed for situated experiences in museum settings. To this end, further work will rely on 
                     leveraging the findings highlighted in this paper to create interactive installations to access these embodied knowledge archives. We contend 
                     that the implementations presented in this paper establish a robust foundation for developing interactive modes of access tailored to meet the 
                     requirements of casual users within situated experiences. In particular, two main directions will be pursued. First, based on the fundamental 
                     concept of placing visitors <quote rend="inline">inside</quote> the archive <ptr target="#shen_et_al_2019"/>, rather than looking at it on a 
                     plain screen, immersive environments (IEs) will be employed to create situated experiences in which users can navigate the whole archive. In 
                     practice, dimensionality reduction techniques will be employed to compute a mapping in two- or three-dimensions in order to generate a virtual 
                     world in which each point or glyph represents a pose (and its corresponding frame or clip). This virtual cloud of human bodies will then be 
                     displayed in large IEs, allowing visitors to freely navigate and discover the archive. Applying immersive technologies to the mediation of 
                     cultural heritage already has some interesting applications, particularly in the context of archaeological enquiry 
                     <ptr target="#sciuto_et_al_2023"/>, and we believe such technologies can also serve a purpose for embodied knowledge archives.
                  </p>
                  <p>Second, an interactive retrieval system can be developed, either based on pose or motion similarity. Users could strike a pose or perform a 
                     certain movement, detected in real-time with motion-tracking solutions, and the system would retrieve relevant clips from the collection. Such 
                     an experience would yield an interesting performative aspect that sees the visitor as a co-creator with the machine, essentially transforming 
                     them into a variable of the generative system, and other people around them as an audience witnessing the exchange in a 
                     <quote rend="inline">third-person's perspective</quote> <ptr target="#mul_masson_2018"/>.
                  </p>
              </div>
          </div>
          <div>
            <head>7 Conclusion</head>
              <p>Embodied knowledge archives are an integral part of our living heritage, and they contain important aspects of our cultures. Yet, it is still 
                 difficult to explore embodied knowledge archives, especially for more casual audiences that lack specific knowledge on the topic of the 
                 collection. To answer this challenge, we have proposed in this work a computational framework that leverages motion extraction AI to augment these 
                 datasets with a wealth of rich data, enabling new modes of analysis and access.
              </p>
              <p>The proposed method was applied to two embodied knowledge archives, containing dance and martial arts performances, respectively, which showcases 
                 its application to multimedia content and diverse access scenarios. In the former example, we devised a method to visualise a whole collection in 
                 two dimensions through the human poses embedded in the archival materials, revealing their structure and fostering serendipitous discoveries. In 
                 the latter, we extended our method to motion encoding from static poses and devised a motion-based query system, offering a new way to search an 
                 embodied knowledge archive. These scenarios showcase how our computational framework can operationalise this type of collection and unlock a 
                 variety of new modes of access suitable for non-expert audiences.
              </p>
          </div>
          <div>
            <head>Acknowledgements</head>
              <p>The authors are grateful to the <name>Prix de Lausanne</name> for the opportunity to work on their audiovisual archive, as part of the 
                 <name>SNSF</name>'s Sinergia grant, <title rend="italic">Narratives from the Long Tail: Transforming Access to Audiovisual Archives</title> 
                 (CRSII5_198632).
              </p>
              <p>The <name>Hong Kong Martial Arts Living Archive</name> is a longitudinal research collaboration between the <name>International Guoshu 
                 Association</name>, the <name>City University of Hong Kong</name>, and the <name>Laboratory for Experimental Museology (eM+), EPFL</name>.
              </p>
           </div>
      </body>
      <back>
        <listBibl>
          <bibl xml:id="aristidou_et_al_2018" label="Aristidou et al. 2018">Artistidou, A. (2018) <title rend="quotes">Deep motifs and motion signatures</title>, 
             <title rend="italic">Transactions on Graphics</title>, 37(6), pp. 1–13.
          </bibl>
          <bibl xml:id="aristidou_shamir_chrysanthou_2019" label="Aristidou, Shamir, and Chrysanthou 2019">Aristidou, A., Shamir, A., and Chrysanthou, Y. (2019) 
             <title rend="quotes">Digital dance ethnography: Organizing large dance collections</title>, <title rend="italic">Journal on Computing and Cultural 
             Heritage</title>, 12(4), pp. 1–27.
          </bibl>
          <bibl xml:id="arnold_tilton_2019" label="Arnold and Tilton 2019">Arnold, T. and Tilton, L. (2019) <title rend="quotes">Distant viewing: Analyzing large 
             visual corpora</title>, <title rend="italic">Digital Scholarship in the Humanities</title>, 34(Supplement 1), pp. i3-i16.
          </bibl>
          <bibl xml:id="aske_giardinetti_2023" label="Aske and Giardinetti 2023">Aske, K. and Giardinetti, M. (2023) <title rend="quotes">(Mis)matching metadata: 
             Improving accessibility in digital visual archives through the EyCon project</title>, <title rend="italic">Journal on Computing and Cultural 
             Heritage</title>, 16(4).
          </bibl>
          <bibl xml:id="bardiot_2021" label="Bardiot 2021">Bardiot, C. (2021) <title rend="italic">Performing arts and digital humanities: From traces to 
             data</title>. Hoboken, NJ: John Wiley &amp; Sons.
          </bibl>
          <bibl xml:id="bazarevsky_et_al_2020" label="Bavarevsky et al. 2020">Bazarevsky, V. et al. (2020) <title rend="quotes">Blazepose: On-device real-time 
             body pose tracking</title>, <title rend="italic">arXiv</title>. <ref target="https://arxiv.org/abs/2006.10204">https://arxiv.org/abs/2006.10204</ref>.
          </bibl>
          <bibl xml:id="benesh_benesh_1977" label="Benesh and Benesh 1977">Benesh, R. and Benesh, J. (1977) <title rend="italic">Reading dance: The birth of 
             choreology</title>. London: Souvenir Press.
          </bibl>
          <bibl xml:id="bernasconi_cetnic_impett_2023" label="Bernasconi, Cetinić, and Impett 2023">Bernasconi, V., Cetinić, E., and Impett, L. (2023) 
             <title rend="quotes">A computational approach to hand pose recognition in early modern paintings</title>, <title rend="italic">Journal of 
             Imaging</title>, 9(6).
          </bibl>
          <bibl xml:id="broadwell_tangherlini_2021" label="Broadwell and Tangherlini 2021">Broadwell, P. and Tangherlini, T.R. (2021) 
             <title rend="quotes">Comparative K-pop choreography analysis through deep-learning pose estimation across a large video corpus</title>, 
             <title rend="italic">DHQ: Digital Humanities Quarterly</title>, 15(1). Available at: 
             <ref target="https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html">https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html</ref>
          </bibl>
          <bibl xml:id="cameron_franks_hamidzadeh_2023" label="Cameron, Franks, and Hamidzadeh 2023">Cameron, S., Franks, P., and Hamidzadeh, B. (2023) 
             <title rend="quotes">Positioning paradata: A conceptual frame for AI processual documentation in archives and recordkeeping contexts</title>, 
             <title rend="italic">Journal on Computing and Cultural Heritage</title>, 16(4).
          </bibl>
          <bibl xml:id="cao_et_al_2019" label="Cao et al. 2019">Cao, Z. et al. (2019) <title rend="quotes">Openpose: Realtime multi-person 2D pose estimation 
             using part affinity fields</title>, <title rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>. Available at: 
             <ref target="https://arxiv.org/pdf/1812.08008.pdf">https://arxiv.org/pdf/1812.08008.pdf</ref>.
          </bibl>
          <bibl xml:id="chalmers_et_al_2021" label="Chalmers et al. 2021">Chalmers, A. et al. (2021) <title rend="quotes">Realistic humans in virtual cultural 
             heritage</title>, <title rend="italic">Proceedings of RISE IMET 2021</title>. Nicosia, Cyprus, 2-4 June 2021. New York: Springer, pp. 156-165.
          </bibl>
          <bibl xml:id="chao_et_al_2018" label="Chao et al. 2018">Chao, H. et al. (2018) <title rend="quotes">Kapturing kung fu: Future proofing the Hong Kong martial 
             arts living archive</title>, in Whatley, S., Cisneros, R.K., and Sabiescu, A. (eds.) <title rend="italic">Digital echoes: Spaces for intangible and 
             performance-based cultural heritage</title>. New York: Springer, pp. 249-264.
          </bibl>
          <bibl xml:id="colavizza_et_al_2021" label="Colavizza et al. 2021">Colavizza, G. (2021) <title rend="quotes">Archives and AI: An overview of current 
             debates and future perspectives</title>, <title rend="italic">Journal on Computing and Cultural Heritage</title>, 15(1).
          </bibl>
          <bibl xml:id="craig_et_al_2018" label="Craig et al. 2018">Craig, C.J. (2018) <title rend="quotes">The embodied nature of narrative knowledge: A 
             cross-study analysis of embodied knowledge in teaching, learning, and life</title>, <title rend="italic">Teaching and Teacher Education</title>, 71, 
             pp. 329-240.
          </bibl>
          <bibl xml:id="delbridge_2015" label="Delbridge 2015">Delbridge, M. (2015) <title rend="italic">Motion capture in performance: An introduction</title>. 
             New York: Springer.
          </bibl>
          <bibl xml:id="dimitropoulos_et_al_2014" label="Dimitropoulos et al. 2014">Dimitropoulos, K. et al. (2014) <title rend="quotes">Capturing the intangible 
             an introduction to the i-Treasures project</title>, <title rend="italic">Proceedings of the 9th international conference on computer vision theory 
             and applications</title>, vol. 3. Lisbon, Portugal, 5-8 January 2014. Setúbal, Portugal: SCITEPRESS, pp. 773-781. Available at: 
             <ref target="https://ieeexplore.ieee.org/abstract/document/7295018">https://ieeexplore.ieee.org/abstract/document/7295018</ref>.
          </bibl>
          <bibl xml:id="dork_carpendale_williamson_2011" label="Dörk, Carpendale, and Williamson 2011">Dörk, M., Carpendale, S., and Williamson, C. (2011) 
             <title rend="quotes">The information flaneur: A fresh look at information seeking</title>, <title rend="italic">Proceedings of the SIGCHI 
             conference on human factors in computing systems</title>. Vancouver, BC, Canda, 7-12 May 2011. New York: ACM, pp. 1215-1224. 
             <ref target="https://doi.org/10.1145/1978942.1979124">https://doi.org/10.1145/1978942.1979124</ref>.
          </bibl>
          <bibl xml:id="doulamis_et_al_2017" label="Doulamis et al. 2017">Doulamis, A.D. (2017) <title rend="quotes">Transforming intangible folkloric performing 
             arts into tangible choreographic digital objects: The terpsichore approach</title>, <title rend="italic">Proceedings of the 12th international 
             joint conference on computer vision, imaging and computer graphics, theory, and applications</title>. Porto, Portugal, 27 February-1 March 2017. 
             Setúbal, Portugal: SCITEPRESS, pp. 451-460.
             <ref target="https://doi.org/10.3030/691218">https://doi.org/10.3030/691218</ref>.
          </bibl>
          <bibl xml:id="edmondson_2004" label="Edmondson 2004">Edmondson, R. (2004) <title rend="italic">Audiovisual archiving: Philosophy and principles</title>. 
             Paris: United Nations Educational, Scientific and Cultural Organization.
          </bibl>
          <bibl xml:id="el-raheb_et_al_2018" label="El Raheb et al. 2018">El Raheb, K. (2018) <title rend="quotes">A web-based system for annotation of dance 
             multimodal recordings by dance practitioners and experts</title>, <title rend="italic">Proceedings of the 5th international conference on movement 
             and computing</title>. Genoa, Italy, 28-30 June 2018. 
             <ref target="https://doi.org/10.1145/3212721.3212722">https://doi.org/10.1145/3212721.3212722</ref>.
          </bibl>
          <bibl xml:id="fossati_et_al_2012" label="Fossati et al. 2012">Fossati, G. et al. (2012) <title rend="quotes">Found footage filmmaking, film archiving 
             and new participatory platforms</title>, in Guldemond, J., Bloemheuvel, M., and Fossati, G. (eds.) <title rend="italic">Found footage: Cinema 
             exposed</title>. Amsterdam: Amsterdam University Press, pp. 177-184.
          </bibl>
          <bibl xml:id="grammalidis_dimitropoulos_2015" label="Grammalidis and Dimitropoulos 2015">Grammalidis, N. and Dimitropoulos, K. (2015) 
             <title rend="quotes">Intangible treasures: Capturing the intangible cultural heritage and learning the rare know-how of living human 
             treasures</title>, <title rend="italic">Proceedings of the 2015 digital heritage international congress</title>, Granada, Spain, 28 September-2 
             October 2015. Available at: 
             <ref target="https://diglib.eg.org/handle/10.2312/14465">https://diglib.eg.org/handle/10.2312/14465</ref>.
          </bibl>
          <bibl xml:id="guest_1977" label="Guest 1977">Guest, A.H. (1977) <title rend="italic">Labanotation: Or, kinetography laban: The system of analyzing and 
             recording movement</title>. New York: Taylor &amp; Francis.
          </bibl>
          <bibl xml:id="griffin_wennerstrom_foka_2023" label="Griffin, Wennerström, and Foka 2023">Griffin, G., Wennerström, E., and Foka, A. (2023) 
             <title rend="quotes">AI and Swedish heritage organisations: Challenges and opportunities</title>, <title rend="italic">AI &amp; Society</title>. 
             <ref target="https://doi.org/10.1007/s00146-023-01689-y">https://doi.org/10.1007/s00146-023-01689-y</ref>.
          </bibl>
          <bibl xml:id="halko_martinsson_tropp_2011" label="Halko, Martinsson, and Tropp 2011">Halko, N., Martinsson, P.G. and Tropp, J.A. (2011), 
            <title rend="quotes">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>, 
            <title rend="italic">SIAM Review</title>, 53(2), pp. 217–288. 
            <ref target="https://doi.org/10.1137/090771806">https://doi.org/10.1137/090771806</ref>.
          </bibl>
          <bibl xml:id="hou_et_al_2022" label="Hou et al. 2022">Hou, Y. et al. (2022) <title rend="quotes">Digitizing intangible cultural heritage embodied: State 
            of the art</title>, <title rend="italic">Journal on Computing and Cultural Heritage</title>, 15(3), pp. 1–20.
          </bibl>
          <bibl xml:id="hou_seydou_kenderdine_2023" label="Hou, Seydou, and Kenderdine 2023">Hou, Y., Seydou, F.M., and Kenderdine S. (2023) 
            <title rend="quotes">Unlocking a multimodal archive of southern chinese martial arts through embodied cues</title>, 
            <title rend="italic">Journal of Documentation</title>. 
            <ref target="https://doi.org/10.1108/JD-01-2022-0027">https://doi.org/10.1108/JD-01-2022-0027</ref>.
          </bibl>
          <bibl xml:id="impett_2020a" label="Impett 2020a">Impett, L. (2020a) <title rend="quotes">Analyzing gesture in digital art history</title>, in 
            Brown, K. (ed.) <title rend="italic">The routledge companion to digital humanities and art history</title>. New York: Routledge, pp. 386-407.
          </bibl>
          <bibl xml:id="impett_2020b" label="Impett 2020b">Impett, L. (2020b) <title rend="italic">Painting by numbers: Computational methods and the history 
            of art</title>. EPFL.
          </bibl>
          <bibl xml:id="ingold_2011" label="Ingold 2011">Ingold, T. (2011) <title rend="italic">Being alive: Essays on movement, knowledge and description</title>. 
            New York: Taylor &amp; Francis.
          </bibl>
          <bibl xml:id="jaillant_2022" label="Jaillant 2022">Jaillant, L. (2022) <title rend="italic">Archives, access and artificial intelligence: Working 
            with born-digital and digitized archival collections</title>. Bielefeld, Germany: Bielefeld University Press.
          </bibl>
          <bibl xml:id="jaillant_rees_2023" label="Jaillant and Rees 2023">Jaillant, L. and Rees, A. (2023) <title rend="quotes">Applying AI to digital 
            archives: Trust, collaboration and shared professional ethics</title>, <title rend="italic">Digital Scholarship in the Humanities</title>, 38(2), pp. 
            571-585.
          </bibl>
          <bibl xml:id="kenderdine_mason_hibberd_2021" label="Kenderdine, Mason, and Hibberd 2021">Kenderdine, S., Mason, I., and Hibberd L. (2021) 
            <title rend="quotes">Computational archives for experimental museology</title>, <title rend="italic">Proceedings of RISE IMET 2021</title>. Nicosia, 
            Cyprus, 2-4 June 2021. New York: Springer, pp. 3-18.
          </bibl>
          <bibl xml:id="kreiss_bertoni_alahi_2021" label="Kreiss, Bertoni, and Alahi 2021">Kreiss, S., Bertoni, L., and Alahi, A. (2021) 
            <title rend="quotes">Openpifpaf: Composite fields for semantic keypoint detection and spatio-temporal association</title>, 
            <title rend="italic">IEEE Transactions on Intelligent Transportation Systems</title>, 23(8), pp. 13498-13511.
          </bibl>
          <bibl xml:id="krol_mynarski_2005" label="Król and Mynarski 2005">Król, H. and Mynarski, W. (2005) <title rend="italic">Cechy ruchu-charakterystyka i 
            możliwości parametryzacji [Features of movement-characteristics and capabilities of parametryzation]</title>. Katowice, Poland: Akademia Wychowania 
            Fizycznego.
          </bibl>
          <bibl xml:id="lin_et_al_2014" label="Lin et al. 2014">Lin, T.Y. (2014) <title rend="quotes">Microsoft COCO: Common objects in context</title>, 
            <title rend="italic">Proceedings of the 13th annual European conference of computer vision</title>. Zurich, Switzerland, 6-12 September 2014. 
            New York: Springer, pp. 740-755.
          </bibl>
          <bibl xml:id="ma_2003" label="Ma 2003">Ma, M. (2003) <title rend="italic">Wu xue tan zhen [Examination of truth in martial studies)</title>. Taipei, 
            Taiwan: Lion Books.
          </bibl>
          <bibl xml:id="mallik_chaudhury_2012" label="Mallik and Chaudhury 2012">Mallik, A. and Chaudhury, S. (2012) <title rend="quotes">Acquisition of 
            multimedia ontology: An application in preservation of cultural heritage</title>, <title rend="italic">International Journal of Multimedia Information 
            Retrieval</title>, 1(4), pp. 249–262.
          </bibl>
          <bibl xml:id="mallik_chaudhury_ghosh_2011" label="Mallik, Chaudhury, and Ghosh 2011">Mallik, A., Chaudhury, S., and Ghosh, H. (2011) 
            <title rend="quotes">Nrityakosha: Preserving the intangible heritage of indian classical dance</title>, <title rend="italic">Journal on Computing and 
            Cultural Heritage</title>, 4(3), pp. 1–25.
          </bibl>
          <bibl xml:id="manovich_2020" label="Manovich 2020">Manovich, L. (2020) <title rend="italic">Cultural analytics</title>. Cambridge, MA: The MIT Press.
          </bibl>
          <bibl xml:id="martinsson_rokhlin_tvgert_2011" label="Martinsson, Rokhlin, and Tvgert 2011">Martinsson, P.-G., Rokhlin, V., and Tygert, M. (2011) 
            <title rend="quotes">A randomized algorithm for the decomposition of matrices</title>, <title rend="italic">Applied and Computational Harmonic 
            Analysis</title>, 30(1), pp. 47–68. 
            <ref target="https://www.sciencedirect.com/science/article/pii/S1063520310000242">https://www.sciencedirect.com/science/article/pii/S1063520310000242</ref>.
          </bibl>
          <bibl xml:id="mcgregor_lab_2019" label="McGregor and Lab 2019">McGregor, W. and Lab, G.A.C. (2019) <title rend="italic">Living archive</title>. 
            Availablet at: <ref target="https://artsexperiments.withgoogle.com/living-archive">https://artsexperiments.withgoogle.com/living-archive</ref>. 
            (Accessed: 30 June 2023).
          </bibl>
          <bibl xml:id="mcinnes_et_al_2018" label="McInnes et al. 2018">McInnes, L. et al. (2018) <title rend="quotes">Umap: Uniform manifold approximation and 
            projection</title>, <title rend="italic">The Journal of Open Source Software</title>, 3(29). 
            <ref target="https://doi.org/10.21105/joss.00861">https://doi.org/10.21105/joss.00861</ref>.
          </bibl>
          <bibl xml:id="mul_masson_2018" label="Mul and Masson 2018">Mul, G. and Masson, E. (2018) <title rend="quotes">Data-based art, algorithmic poetry: 
            Geert Mul in conversation with Eef Masson</title>, <title rend="italic">TMG Journal for Media History</title>, 21(2).
          </bibl>
          <bibl xml:id="olesen_et_al_2016" label="Olesen et al. 2016">Olesen, C.G. (2016) <title rend="quotes">Data-driven research for film history: Exploring 
            the Jean Desmet collection</title>, <title rend="italic">Moving Image: The Journal of the Association of Moving Image Archivists</title>, 16(1), pp. 
            82–105.
          </bibl>
          <bibl xml:id="pedregosa_et_al_2011" label="Pedregosa et al. 2011">Pedregosa, F. et al. (2011) <title rend="quotes">Scikit-learn: Machine learning in 
            Python</title>, <title rend="italic">Journal of Machine Learning Research</title>, 12(85), pp. 2825–2830.
          </bibl>
          <bibl xml:id="rtsarchives_2018" label="RTSArchives 2018">RTSArchives (2018) <title rend="italic">Le nouveau site RTSarchives</title>. Available at: 
            <ref target="https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html">https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html</ref>. 
            (Accessed: 30 June 2023).
          </bibl>
          <bibl xml:id="salazar_2018" label="Salazar 2018">Salazar Sutil, N. (2018) <title rend="quotes">Section editorial: Human movement as critical creativity: 
            Basic questions for movement computing</title>, <title rend="italic">Computational Culture: a Journal of Software Studies</title>, 6.
          </bibl>
          <bibl xml:id="sciuto_et_al_2023" label="Sciuto et al. 2023">Sciuto, C. et al. (2023) <title rend="quotes">Exploring fragmented data: Environments, 
            people and the senses in virtual reality</title>, in Landeschi, G. and Betts, E. (eds.) <title rend="italic">Capturing the senses: Digital methods 
            for sensory archaeologies</title>. New York: Springer, pp. 85-103.
          </bibl>
          <bibl xml:id="sedmidubsky_et_al_2020" label="Sedmidubsky et al. 2020">Sedmidubsky, J. et al. (2020) <title rend="quotes">Motion words: A text-like 
            representation of 3D skeleton sequences</title>, <title rend="italic">Proceedings of the 42nd annual European conference on information 
            retrieval</title>. Lisbon, Portugal, 14-17 April 2020. New York: Springer, pp. 527-241. 
            <ref target="https://doi.org/10.1007/978-3-030-45439-5_35">https://doi.org/10.1007/978-3-030-45439-5_35</ref>.
          </bibl>
          <bibl xml:id="shen_et_al_2019" label="Shen et al. 2019">Shen, H. et al. (2019) <title rend="quotes">Information visualisation methods and techniques: 
            State-of-the-art and future directions</title>, <title rend="italic">Journal of Industrial Information Integration</title>, 16, pp. 100–102.
          </bibl>
          <bibl xml:id="van-der-maaten_hinton_2008" label="van der Maaten and Hinton 2008">van der Maaten, L. and Hinton, G. (2008) 
            <title rend="quotes">Visualizing data using t-sne</title>, <title rend="italic">Journal of Machine Learning Research</title>, 9(86), pp. 2579–2605. 
            Available at: 
            <ref target="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</ref>.
          </bibl>
          <bibl xml:id="whitelaw_2015" label="Whitelaw 2015">Whitelaw, M. (2015) <title rend="quotes">Generous interfaces for digital cultural 
            collections</title>, <title rend="italic">DHQ: Digital Humanities Quarterly</title>, 9(1). Availablet at: 
            <ref target="https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html">https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html</ref>.
          </bibl>
          <bibl xml:id="winters_prescott_2019" label="Winters and Prescott 2019">Winters, J. and Prescott, A. (2019) <title rend="quotes">Negotiating the 
            born-digital: A problem of search</title>, <title rend="italic">Archives and Manuscripts</title>, 47(3), pp. 391–403.
          </bibl>
          <bibl xml:id="wright_2017" label="Wright 2017">Wright, R. <title rend="quotes">The future of television archives</title>, 
            <title rend="italic">Digital Preservation Coalition</title>, 29 November. Available at: 
            <ref target="https://www.dpconline.org/blog/wdpd/the-future-of-television-archives">https://www.dpconline.org/blog/wdpd/the-future-of-television-archives</ref>.
          </bibl>
          <bibl xml:id="zkm_center_2023" label="ZKM Center for Art and Media Karlsruhe 2023">ZKM Center for Art and Media Karlsruhe (2023) 
            <title rend="quotes">William Forsythe: Improvisation technologies: The wesbite project</title>. Available at: 
            <ref target="https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project">https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project</ref>. 
            (Accessed 30 June 2023).
          </bibl>
        </listBibl>
      </back>
   </text>
</TEI>