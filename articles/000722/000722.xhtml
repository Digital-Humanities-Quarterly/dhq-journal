<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume¬†¬†Number¬†</div>
            <div class="toolbar"><a href="#">Preview</a> ¬†|¬† <span style="color: grey">XML</span> |¬† <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Augmenting Access to Embodied Knowledge Archives: A Computational Framework</h1>
               
               <div class="author"><span style="color: grey">Giacomo Alliata
                     </span>¬†&lt;<a href="mailto:giacomo_dot_alliata_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('giacomo_dot_alliata_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('giacomo_dot_alliata_at_epfl_dot_ch'); return false;">giacomo_dot_alliata_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               <div class="author"><span style="color: grey">Yumeng Hou
                     </span>¬†&lt;<a href="mailto:yumeng_dot_hou_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('yumeng_dot_hou_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('yumeng_dot_hou_at_epfl_dot_ch'); return false;">yumeng_dot_hou_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               <div class="author"><span style="color: grey">Sarah Kenderdine
                     </span>¬†&lt;<a href="mailto:sarah_dot_kenderdine_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('sarah_dot_kenderdine_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('sarah_dot_kenderdine_at_epfl_dot_ch'); return false;">sarah_dot_kenderdine_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Augmenting%20Access%20to%20Embodied%20Knowledge%20Archives%3A%20A%20Computational%20Framework&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Alliata&amp;rft.aufirst=Giacomo&amp;rft.au=Giacomo%20Alliata&amp;rft.au=Yumeng%20Hou&amp;rft.au=Sarah%20Kenderdine"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>With the burgeoning use of digital technologies in safeguarding intangible and living
                     heritage, memory institutions have produced a significant body 
                     of material yet to be made accessible for public transmission. The quest for new ways
                     to unlock these massive collections has intensified, 
                     especially in the case of embodied knowledge embedded in complex formats, such as
                     audiovisual and motion capture data.
                     </p>
                  
                  <p>Aiming to address the gap between living heritage archives and embodied knowledge
                     experience, this work inspects a computational framework 
                     incorporating machine intelligence, archival science, and digital museology. By reflecting
                     on how embodied knowledge is sustained and potentially 
                     valorised through human interaction, we devise a series of methods utilising vision-based
                     feature extraction, pose estimation, movement analysis, 
                     and machine learning. The goal is to augment the archival experience with new modes
                     of exploration, representation, and embodiment.
                     </p>
                  
                  <p>This article reports the computational procedures and algorithmic tools inspected
                     through two use cases. In the first example, we visualise the 
                     archives of the Prix de Lausanne, a collection of 50 years of video recordings of dance performances, for archival
                     exploration through 
                     the dancers' poses. In another experiment, movement encoding is employed to allow
                     multimodal data search via embodied cues in the 
                     Hong Kong Martial Arts Living Archive, a comprehensive documentation of the living heritage of martial arts that is chiefly
                     comprised 
                     of motion-captured performances by masters.
                     </p>
                  
                  <p>Though holding different application purposes, both projects operate on the proposed
                     framework and extract archive-specific features to create a 
                     meaningful representation of human bodies, which reveals the versatile applications
                     that computational capacities can achieve for embodied knowledge 
                     archives. The practices also represent a model of interdisciplinary involvement where
                     the archivists, computists, artists, and knowledge holders 
                     join hands to renew strategies for archival exploration and heritage interpretation
                     in a new light.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1 Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Living heritage, also known as intangible culture, encompasses the practices, expressions,
                     spaces, and knowledge that safeguard the rich 
                     diversity of human creativity and collective heritage, as defined by UNESCO. The preservation and exploration of living heritage 
                     has been of paramount importance in understanding and celebrating the cultural fabric
                     of our society. To further enhance our engagement with this 
                     heritage, advancements in digital technologies have opened new possibilities for capturing
                     and experiencing embodied knowledge.
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">In recent years, there has been a growing trend in the digital capturing of living
                     heritage and embodied knowledge 
                     [<a class="ref" href="#hou_et_al_2022">Hou et al. 2022</a>]. These endeavours aim to document and preserve not only physical artefacts but also
                     the intangible aspects of 
                     culture, such as rituals, performances, and traditional practices. However, GLAM sectors,
                     encompassing galleries, libraries, archives, and 
                     museums, face significant challenges in effectively managing and providing access
                     to these vast digital repositories 
                     [<a class="ref" href="#jaillant_2022">Jaillant 2022</a>].
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">One of the key challenges faced by GLAM sectors is the sheer scale of digitised collections.
                     As more and more heritage materials are digitised, 
                     the volume of data grows exponentially, making it increasingly difficult to navigate
                     and explore these archives manually and essentially turning 
                     these collections into ‚Äúdark archives‚Äù inaccessible to the public. Giovanna Fossati et al. further highlight how 
                     non-expert audiences are in need of more intuitive modes of access to discover these
                     collections [<a class="ref" href="#fossati_et_al_2012">Fossati et al. 2012</a>]. To address 
                     this challenge, recent research has embarked on applying computational analysis, artificial
                     intelligence (AI) in particular, to unlock various 
                     new modes of archival experience. Leveraging the transformative potential of AI<a class="noteRef" href="#d4e244">[1]</a>, we 
                     can enhance the retrieval, visualisation, and navigation of multimodal cultural data
                     materials [<a class="ref" href="#aske_giardinetti_2023">Aske and Giardinetti 2023</a>]. 
                     [<a class="ref" href="#cameron_franks_hamidzadeh_2023">Cameron, Franks, and Hamidzadeh 2023</a>] [<a class="ref" href="#hou_et_al_2022">Hou et al. 2022</a>]. Further, by involving the collaborative efforts of cultural 
                     domain experts in configuring AI tools, there is a promise to alleviate the gap between
                     data science and GLAM sectors, highlighted through 
                     various uses cases on the implementation of AI strategies in Swedish heritage organizations
                     [<a class="ref" href="#griffin_wennerstrom_foka_2023">Griffin, Wennerstr√∂m, and Foka 2023</a>], and 
                     enable users to interact with digitised archives in meaningful yet trustworthy contexts
                     [<a class="ref" href="#jaillant_rees_2023">Jaillant and Rees 2023</a>].
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">To tackle these challenges, we propose a computational framework that augments the
                     archival experience of embodied knowledge. Our approach 
                     intends to extract human movements in the form of skeleton data and process it, showing
                     its versatility through multiple use cases. By 
                     revalorizing and visualising knowledge, memory, and experience, we strive to enhance
                     the cognitive reception and engagement of users, ultimately 
                     improving the overall archival user experience. Our experiments are conducted on two
                     exemplar collections: the archives of the 
                     Prix de Lausanne, comprising an audiovisual dataset of dance performances, and the Hong Kong Martial Arts Living Archive 
                     (HKMALA), which contains both videos and motion capture recordings of Hong Kong martial arts performers. This work might be of 
                     interest to professionals of cultural heritage institutions wishing to explore new
                     methods of access to their collections, as well as researchers 
                     of computational humanities with a focus on embodied knowledge archives.
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">By exploring this computational framework, we hope to contribute to the ongoing efforts
                     to leverage visual AI technologies to transform the way 
                     we interact with and appreciate living heritage. Through the synthesis of cultural
                     heritage and cutting-edge computational methods, we can bridge 
                     the gap between the past and the present, enabling a deeper understanding and appreciation
                     of our shared human history.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2 Related Work</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.1 Embodied Knowledge as Part of Living Heritage</h2>
                     
                     <div class="counter"><a href="#p6">6</a></div>
                     <div class="ptext" id="p6">Though partly embedded in architectural sensorium and memory objects, intangible cultural
                        heritage (ICH) is inherently living and manifests 
                        through bodily expressions, individual practices, and social enactment ‚Äî on the multiple
                        layers of embodiment. Recent research has revealed a 
                        stress shift in documenting, describing, and presenting the living nature of ICH by
                        transforming intangible practices into tangible 
                        choreographic objects. For instance, the i-Treasures project (2013-2017) works to
                        digitize the living heritage of folk dances, folk singing, 
                        craftsmanship, and contemporary music composition in an attempt to sustain traditional
                        know-how via creative presentation 
                        [<a class="ref" href="#dimitropoulos_et_al_2014">Dimitropoulos et al. 2014</a>] [<a class="ref" href="#grammalidis_dimitropoulos_2015">Grammalidis and Dimitropoulos 2015</a>]. Taking a more public-facing approach, the 
                        Terpsichore project (2016 to 2020) integrates computational models, such as semantic
                        web technologies and machine learning, with storytelling 
                        to facilitate data accessibility and affordance of digitised materials [<a class="ref" href="#doulamis_et_al_2017">Doulamis et al. 2017</a>]. Famous choreographers have also 
                        been keen on documenting their practices with audiovisual recordings, resulting in
                        extensive collections such as William 
                        Forsythe's Performative Archive, recently acquired by the ZKM Center for Art and Media Karlsruhe. While its focus differs 
                        from the performing arts, the Hong Kong Martial Arts Living Archive (HKMALA) project (2012-present) forges a multimodal digital 
                        archiving paradigm by motion-capturing martial art sequence performances of renowned
                        masters. The motion-captured (MoCap) records are 
                        situated within a digital collation capturing different facets of Hong Kong's martial arts traditions, encompassing rituals, 
                        histories, re-enactments, and stories of practitioners [<a class="ref" href="#chao_et_al_2018">Chao et al. 2018</a>].
                        </div>
                     
                     <div class="counter"><a href="#p7">7</a></div>
                     <div class="ptext" id="p7">The embodied facet of ICH not only refers to the knowledge of the body but also that
                        which is dwelling in and enacted through the body 
                        [<a class="ref" href="#craig_et_al_2018">Craig et al. 2018</a>]. Movements and gestures are considered typical mediums to express and process mindfulness.
                        In various 
                        cases, they also mediate interactive processes, such as human-environment communication
                        and knowledge formation. Movement data usually 
                        documents the active dimension of mankind's creativity over time. Hence, as Tim Ingold's delineates, instead of targeting solely 
                        result-driven examinations, the ideal usage of such data should facilitate a new mode
                        of computation conveying the open-ended information 
                        embedded in bodily practices [<a class="ref" href="#salazar_2018">Salazar 2018</a>] [<a class="ref" href="#ingold_2011">Ingold 2011</a>].<a class="noteRef" href="#d4e313">[2]</a> In accordance with this ideal, MoCap technologies have gained 
                        increasing popularity in ICH archiving. MoCap allows data collection to be neutral,
                        simultaneous, ‚Äúbeyond the frame and 
                        within the volume‚Äù [<a class="ref" href="#delbridge_2015">Delbridge 2015</a>], thereby fostering a perceivable human presence in the virtual CH environments, as
                        
                        surveyed by Alan Chalmers et al. [<a class="ref" href="#chalmers_et_al_2021">Chalmers et al. 2021</a>].</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.2 Computational (Embodied) Archives</h2>
                     
                     <div class="counter"><a href="#p8">8</a></div>
                     <div class="ptext" id="p8">Digitization of archives allows researchers and practitioners to apply modern computational
                        methods to these collections. A recent review by 
                        Colavizza et al. has outlined multiple axes on which AI can improve archival practices by automating
                        recordkeeping processes and 
                        improving access to these collections as well as fostering new forms of digital archives
                        [<a class="ref" href="#colavizza_et_al_2021">Colavizza et al. 2021</a>]. Additionally, 
                        Lev Manovich has put forward a new way to analyse large datasets of cultural items through computational
                        means, a method he has 
                        named ‚Äúcultural analytics‚Äù [<a class="ref" href="#manovich_2020">Manovich 2020</a>]. Other researchers, such as Leonardo Impett, have focused 
                        specifically on computer vision models to better understand large visual cultural
                        collections through ‚Äúdistant 
                        viewing‚Äù [<a class="ref" href="#impett_2020b">Impett 2020b</a>].
                        </div>
                     
                     <div class="counter"><a href="#p9">9</a></div>
                     <div class="ptext" id="p9">From the perspective of experimental museology, computational approaches can help
                        improve the ‚Äúcivic value‚Äù of 
                        cultural archives [<a class="ref" href="#edmondson_2004">Edmondson 2004</a>]. Through its operationalisation, the archive is augmented on three main levels 
                        [<a class="ref" href="#kenderdine_mason_hibberd_2021">Kenderdine, Mason, and Hibberd 2021</a>]. First, it enters the social and immersive dimension of situated museological experiences.
                        
                        Second, archivists engage in new interdisciplinary exchanges with media experts and
                        the computer science community to solve many of the 
                        technical challenges cultural institutions face today. Third, new narratives can be
                        imagined by extracting novel features and revealing 
                        ‚Äúhidden structures‚Äù in the dataset [<a class="ref" href="#olesen_et_al_2016">Olesen et al. 2016</a>].
                        </div>
                     
                     <div class="counter"><a href="#p10">10</a></div>
                     <div class="ptext" id="p10">These hold especially true for audiovisual recordings, one of the primary examples
                        of collections used to capture and document embodied 
                        practices. As the foremost mnemonic records of the 21st century, these moving image
                        archives are part of our daily lives. In these last 
                        decades, major broadcasting institutions have digitised their entire collections.
                        For instance, in Switzerland, the 
                        Radio T√©l√©vision Suisse (RTS) has more than 200,000 hours of footage [<a class="ref" href="#rtsarchives_2018">RTSArchives 2018</a>], while in the 
                        United Kingdom, the British Broadcasting Corporation (BBC) preserves more than a million recorded 
                        hours [<a class="ref" href="#wright_2017">Wright 2017</a>]. In parallel, we are observing many advances in the field of visual AI dedicated
                        to human pose estimation  
                        with state-of-the-art models such as OpenPose [<a class="ref" href="#cao_et_al_2019">Cao et al. 2019</a>], OpenPifPaf [<a class="ref" href="#kreiss_bertoni_alahi_2021">Kreiss, Bertoni, and Alahi 2021</a>], and 
                        BlazePose [<a class="ref" href="#bazarevsky_et_al_2020">Bavarevsky et al. 2020</a>]. These AI algorithms can reliably extract human poses and movements from large moving
                        image 
                        collections at scale, essentially creating computational embodied archives. The wealth
                        of new data extracted by this process can then be 
                        further processed, unlocking new modes of access and novel ways of exploring these
                        archives.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.3 Human Bodies as a Way of Access</h2>
                     
                     <div class="counter"><a href="#p11">11</a></div>
                     <div class="ptext" id="p11">In the field of digital humanities, various projects have relied on such approaches
                        to analyse, explore, and better understand visual 
                        collections through human bodies. In digital art history, for instance, researchers
                        have undertaken a comprehensive analysis of large art 
                        corpora through human gestures [<a class="ref" href="#bernasconi_cetnic_impett_2023">Bernasconi, Cetiniƒá, and Impett 2023</a>] [<a class="ref" href="#impett_2020a">Impett 2020a</a>]. Similarly, in collaboration with 
                        choreographer Wayne McGregor, the Google Arts &amp; Culture Lab has developed the Living Archive, a web-based 
                        interface to navigate the collection of postures in McGregor's choreographies and create new movements in the choreographer's 
                        style [<a class="ref" href="#mcgregor_lab_2019">McGregor and Lab 2019</a>]. Furthermore, movement computing has gained popularity as an intelligent approach
                        to transforming 
                        the performed cultures into ‚Äúchoreographic objects‚Äù, and it is used for analysing, visualising, and interacting 
                        with datasets of dance heritage [<a class="ref" href="#doulamis_et_al_2017">Doulamis et al. 2017</a>]. Andreas Aristidou et al. developed a Labanotation 
                        [<a class="ref" href="#guest_1977">Guest 1977</a>] based framework for transforming movements in Cypriot dances to a high-dimensional
                        feature model and constructing 
                        a deep-learned <em class="term">motion signature</em> for similarity analysis [<a class="ref" href="#aristidou_et_al_2018">Aristidou et al. 2018</a>] 
                        [<a class="ref" href="#aristidou_shamir_chrysanthou_2019">Aristidou, Shamir, and Chrysanthou 2019</a>]. Improved from <em class="term">deep signature</em> encoding 
                        [<a class="ref" href="#aristidou_shamir_chrysanthou_2019">Aristidou, Shamir, and Chrysanthou 2019</a>], Jan Sedmidubsky et al. (2020) invented a text-like representation of 3D 
                        skeleton sequences and employed the benefits of text-learning models in a more complicated
                        context [<a class="ref" href="#sedmidubsky_et_al_2020">Sedmidubsky et al. 2020</a>]. 
                        Likewise, Katerina El Raheb et al. combines posture recognition and Benesh movement notation 
                        [<a class="ref" href="#benesh_benesh_1977">Benesh and Benesh 1977</a>] to assist with multimedia annotation and interactive learning. In addition to a solely
                        choreographic 
                        focus, the Nrityakosha project synthesises a marriage of detection algorithms and
                        semantic models. The researchers related embodied 
                        attributes to concepts of Indian classical dances and, correspondingly, created a
                        specialised ontology for describing knowledge in the 
                        multimedia archive [<a class="ref" href="#mallik_chaudhury_2012">Mallik and Chaudhury 2012</a>] [<a class="ref" href="#mallik_chaudhury_ghosh_2011">Mallik, Chaudhury, and Ghosh 2011</a>]. By applying the methodology of 
                        ‚Äúdistant viewing‚Äù [<a class="ref" href="#arnold_tilton_2019">Arnold and Tilton 2019</a>] to embodied knowledge archives, Peter Broadwell 
                        and Timothy R. Tangherlini propose a computational analysis of K-pop dance, leveraging human-pose estimation
                        algorithms applied 
                        to audiovisual recordings of famous K-pop groups and idols performances [<a class="ref" href="#broadwell_tangherlini_2021">Broadwell and Tangherlini 2021</a>]. Finally, readers will 
                        find additional interesting use cases in the comprehensive review provided by Clarisse Bardiot in 
                        <cite class="title italic">Performing Arts and Digital Humanities: From Traces to Data</cite> [<a class="ref" href="#bardiot_2021">Bardiot 2021</a>].
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3 Our Computation Framework</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.1 Rationale</h2>
                     
                     <div class="counter"><a href="#p12">12</a></div>
                     <div class="ptext" id="p12">The high-level goal of our computational framework is to enrich an embodied knowledge
                        archive by extracting human skeleton data. This wealth 
                        of new data, in the form of sets of key points, captures both static postures (when
                        looking at individual frames) as well as dynamic motions 
                        (when adding the temporal sequences of skeletons from successive frames). By applying
                        motion extraction algorithms, we can therefore 
                        operationalise the abstract features of human poses and movements, essentially translating
                        them into vectors that can then be further 
                        processed through other computational methods. Such a process augments the archive
                        and unlocks a multitude of new scenarios, examples of which 
                        will be discussed through our two use cases. In particular, we investigate visualisations
                        of the whole archive through human poses as well 
                        as motion-based retrieval.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.2 Methodology</h2>
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">Based on the relevant literature reviewed above, we propose the following general
                        framework to augment embodied knowledge archives and create 
                        new modes of experiencing them. This method applies to both moving image and MoCap
                        archives, with the main difference in the first step: 
                        extracting human poses. Indeed, when working with videos, human pose estimation models
                        such as OpenPose [<a class="ref" href="#cao_et_al_2019">Cao et al. 2019</a>] 
                        are required to extract human postures in the form of a ‚Äúskeleton‚Äù, or a list of keypoint edge pairs, as defined in the COCO dataset 
                        [<a class="ref" href="#lin_et_al_2014">Lin et al. 2014</a>]. Depending on the model, these can be in two or three dimensions and handle single
                        or multiple persons at the 
                        same time. Furthermore, we stress that these models can sometimes fail to accurately
                        detect full bodies, especially in situations where part 
                        of the body is occluded. The number of key points detected can also impact how much
                        these models are able to capture features specific to a 
                        certain discipline. While MoCap data generally comes with skeleton information, post-processing,
                        normalisation, and format conversion are 
                        often necessary to produce clean and operable skeleton datasets.
                        </div>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">Once skeleton data is extracted, poses need to be normalised so that they can be meaningfully
                        compared. This involves scaling the skeleton 
                        data with respect to the image sizes, in the case of video datasets, as well as centring
                        and scaling with respect to pose size. Subsequently, 
                        feature vectors can be computed based on this normalised set of key points. We note
                        that these features can vary a lot from one application 
                        to another, especially if one is working with static poses (as in our first use case)
                        or with motions (as in our second use case).
                        </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">These steps result in a computational embodied archive, in which each element is a
                        human pose, as a set of normalised skeleton key points, 
                        linked to its corresponding frame or timestamp in the recorded item and potentially
                        augmented with additional computed features. The wealth 
                        of data extracted can then be further processed for a variety of scenarios, of which
                        we present two examples. The first use case of this 
                        paper, which addresses a collection of dance performance video recordings, will explore
                        how the whole dataset can be mapped in two dimensions 
                        through the poses dancers take in their performances. Afterwards, we present a second
                        scenario on a dataset of martial arts performances, 
                        extending to motion-based similarity retrieval.
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">4 Use Case 1: Accessing an Audiovisual Dance Performance Archive via Human Poses</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.1 The Prix de Lausanne Audiovisual Archive</h2>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">The Prix de Lausanne is a competition for young dancers held yearly in Lausanne, Switzerland since 
                        1973<a class="noteRef" href="#d4e532">[3]</a>. Nearly each year of the competition 
                        has been recorded and digitised, resulting in a rich dataset of 1,445 mp4 videos of
                        individual dance performances across forty-two years, 
                        along with information on the dancers and their chosen performances. This metadata,
                        although valuable, does not document the embodied 
                        elements of dance performances, limiting how one can access such an archive. With
                        our solution, we augmented the available data using 
                        computational methods and employed the extracted features to map the whole dataset
                        in two dimensions through the human poses embedded in the 
                        material, revealing its hidden structures and creating new connections between similar
                        poses.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.2 Building Feature Vectors for the Dancers' Poses</h2>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">We extracted human skeleton data using the MediaPipe BlazePose GHUM 3D model [<a class="ref" href="#bazarevsky_et_al_2020">Bavarevsky et al. 2020</a>], which was mainly chosen 
                        for its out-of-the-box robustness and rapidity of inference for single human estimation.
                        In the case of the Prix de Lausanne, 
                        since the videos record one participant at a time on stage, we found this model to
                        be the most suitable. We also observed only occasional 
                        cases of occlusions when the dancers are wearing particularly large costumes. The
                        BlazePose algorithm outputs skeleton data as a set of 33 
                        key points in three-dimensional space, normalised to the image size. Compared to models
                        trained on the COCO dataset, MediaPipe algorithm 
                        detects more key points, including the index fingers and toes of both hands and feet,
                        allowing us to compute the angles of the wrists and 
                        ankles. Although the algorithm was not specifically trained on dance recordings, we
                        believe these additional features help to better capture 
                        specific poses of dance performances, where even the positions of the hands and feet
                        are crucial. Videos were processed to extract one pose 
                        every five seconds, a cautious design decision to avoid over-collecting similar poses
                        merely due to temporal closeness. We do note however 
                        this is potentially not the best solution to ensure a full coverage of the diversity
                        of poses, and future research in this direction could 
                        improve the results presented even further. It is worth noting that not all frames
                        necessarily had a detected pose, since in some cases the 
                        dancer was too far away from the camera, or because some videos were not properly
                        cut to the exact dance performance and thus had irrelevant 
                        frames at the beginning and/or the end. We measured the rate of detection by taking
                        the mean of the ratio of poses detected over the number of 
                        frames per video, obtaining a mean detection rate of 76.80% across all the videos.
                        The pose extraction process resulted in a total of 27,672 
                        poses.
                        </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">Once these poses had been extracted, we normalised them following a two-step procedure.
                        We first subtracted the pose centre, defined as the 
                        mean point between the left and right hips. We then scaled the pose by its size, defined
                        as the maximum between the torso size and the maximum 
                        distance between the pose centre and any key point, where the torso size was computed
                        as the distance between the centre of the shoulders and 
                        the pose centre (the centre of the hips). The procedure ensures meaningful comparison
                        between the poses by normalising them. An intuitive 
                        sample of normalised poses with their corresponding frames is provided in Figure 1.
                        </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.jpg" rel="external"><img src="resources/images/figure01.jpg" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†1.¬†</div>Sample of normalised poses with corresponding frames. Only the main key points are
                           displayed for ease of visualization (the nose, the 
                           shoulders, the elbows, the wrists, the hips, the knees, and the ankles).</div>
                     </div>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">Subsequently, two feature vectors were built and tested. For the first, we simply
                        took the vector of the key points and flattened it, 
                        resulting in a 99-dimensional vector. For the second, we built a custom feature vector,
                        taking a combination of pose lengths (distances 
                        between two key points) and pose angles (angles defined by three key points). Table
                        1 shows the features computed.
                        </div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Category</td>
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Feature</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Pose Lengths<br />(Distances Between Joints)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Left elbow ‚Äî Right elbow<br />Left wrist ‚Äî Right wrist<br />Left knee ‚Äî Right knee<br />Left ankle ‚Äî Right ankle
                                 <br />Left hip ‚Äî Left wrist<br />Right hip ‚Äî Right wrist<br />Left hip ‚Äî Left ankle<br />Right hip ‚Äî Right ankle<br />
                                 Left ankle ‚Äî Left wrist<br />Right ankle ‚Äî Right wrist<br />Left shoulder ‚Äî Right ankle<br />Right shoulder ‚Äî Left ankle<br />
                                 Left wrist ‚Äî Right ankle<br />Right wrist ‚Äî Left ankle
                                 </td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Pose Angles<br />(Defined by Three Key Points)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Left hip ‚Äî Left knee ‚Äî Left ankle<br />Right hip ‚Äî Right knee ‚Äî Right ankle<br />Left shoulder ‚Äî Left elbow ‚Äî Left wrist<br />
                                 Right shoulder ‚Äî Right elbow ‚Äî Right wrist<br />Left hip ‚Äî Left shoulder ‚Äî Left elbow<br />Right hip ‚Äî Right shoulder ‚Äî Right 
                                 elbow<br />Left shoulder ‚Äî Left hip ‚Äî Left knee<br />Right shoulder ‚Äî Right hip ‚Äî Right knee<br />Left elbow ‚Äî Left wrist ‚Äî Left 
                                 index finger<br />Right elbow ‚Äî Right wrist ‚Äî Right index finger<br />Left knee ‚Äî Left ankle ‚Äî Left index toe<br />Right knee ‚Äî Right 
                                 ankle ‚Äî Right index toe
                                 </td>
                              </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table¬†1.¬†</div>A list of custom features used to represent dance poses in the Prix de Laussanne archive.</div>
                     </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.3 Mapping the Prix de Lausanne Archive</h2>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20">One of the fundamental applications unlocked by computational archives is the possibility
                        to visualise whole collections. To this end, we 
                        apply dimensionality reduction (DR) algorithms to embed the high-dimensional feature
                        spaces in two or three dimensions, in order to create a 
                        novel way of visualising and navigating the whole archive. We tested both standard
                        DR methods, such as Principal Component Analysis (PCA), 
                        and t-distributed Stochastic Neighbour Embedding (tSNE), as well as a more recent
                        approach, Uniform Manifold Approximation and Projection 
                        (UMAP). We leveraged scikit-learn implementations for the two first algorithms and
                        the official UMAP Python implementation for the latter 
                        [<a class="ref" href="#pedregosa_et_al_2011">Pedregosa et al. 2011</a>] [<a class="ref" href="#halko_martinsson_tropp_2011">Halko, Martinsson, and Tropp 2011</a>] [<a class="ref" href="#martinsson_rokhlin_tvgert_2011">Martinsson, Rokhlin, and Tvgert 2011</a>] 
                        [<a class="ref" href="#van-der-maaten_hinton_2008">van der Maaten and Hinton 2008</a>] [<a class="ref" href="#mcinnes_et_al_2018">McInnes et al. 2018</a>]. For tSNE and UMAP we compared using cosine and Euclidean 
                        metrics as the distance measure to compute the projection. We therefore tested five
                        options on 27,672 poses, with both sets of feature 
                        vectors. Figure 2 shows the resulting embeddings in two dimensions.
                        </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.jpeg" rel="external"><img src="resources/images/figure02.jpeg" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†2.¬†</div>Embeddings of 27,672 poses in two dimensions. The top row (blue) uses the flattened
                           vectors of key points, while the bottom row (red) 
                           uses the custom features computed. From left to right, the algorithms used are PCA,
                           tSNE (cosine metric), tSNE (Euclidean metric), UMAP 
                           (cosine metric), and UMAP (Euclidean metric).</div>
                     </div>
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">We can immediately observe that PCA performs poorly on this dataset, as expected due
                        to its linear nature that cannot preserve much variance 
                        with just two dimensions (only 49% with the flattened vectors of key points and 45%
                        with the custom features). Both tSNE and UMAP, however, 
                        give much more interesting results, where structures in the dataset can be observed.
                        Since we did not observe meaningful differences between 
                        the two sets of feature vectors used, we pursued further examinations with the flattened
                        vectors of keypoints to exploit the heuristic 
                        quality of the algorithms independent of the choice of features. Therefore, we set
                        out to investigate tSNE and UMAP behaviours in more depth 
                        by studying the effects of their hyperparameters. For tSNE, we observed how the embedding
                        in two dimensions behaves for different 
                        perplexities (\(œµ\)), the main parameter for this algorithm, related to the number of 
                        neighbours considered for any given point during the computation. For UMAP, we investigated
                        the number of nearest neighbours 
                        (\(ùëõ\)), controlling how UMAP balances local versus global structure in the data, as well
                        as 
                        the minimum distance (\(ùëë_{min}\)) the algorithm is allowed to pack points together in the 
                        low-dimensional space. We used the Euclidean distance as it seemed to produce slightly
                        clearer results. Figure 3 shows how tSNE behaves for 
                        different values of \(œµ\), while Figure 4 displays the results of UMAP with varying 
                        \(ùëõ\) and \(ùëë_{min}\) values.
                        </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.jpeg" rel="external"><img src="resources/images/figure03.jpeg" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†3.¬†</div>Effect of the perplexity (œµ) on the tSNE embeddings of 27,672 poses (flattened vectors
                           of key points, Euclidean distance). Lower values 
                           of œµ result in a more granular embedding.</div>
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.jpeg" rel="external"><img src="resources/images/figure04.jpeg" style="" alt="" /></a></div>              
                        
                        <div class="caption">
                           <div class="label">Figure¬†4.¬†</div>Effect of the number of neighbours ùëõ and the minimum distance ùëë‚Çò‚Çô·µ¢‚Çô on the UMAP embeddings
                           of 27,672 poses (flattened vectors of 
                           key points, Euclidean distance). Lower values of ùëë‚Çò‚Çô·µ¢‚Çô result in a more packed embedding,
                           but global structures appear to be quite 
                           stable.</div>
                     </div>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">To conclude our exploration of the Prix de Lausanne moving image collection, we set
                        out to produce a map of the human poses embedded in the archive. Following the outlined
                        procedure, we extract the keypoints, compute the corresponding feature vectors and
                        apply UMAP (with Euclidean distance, <span class="hi italic">n</span> = 50, <span class="hi italic">ùëë</span>ùëöùëñùëõ = 0.5) on the flattened vectors of keypoints to create Figure 5, where we display
                        the corresponding pose at each point. For the sake of visibility, we only show a portion
                        of all the poses extracted, resulting in a map of 2768 poses. Through this visualisation,
                        we can verify the effectiveness of our approach in grouping together similar poses,
                        thus unlocking a new way of seeing an embodied knowledge archive.</div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†5.¬†</div>Effect of the perplexity ùúñ on the tSNE embeddings of 27672 poses (flattened vectors
                           of keypoints, Euclidean distance). Lower values of ùúñ result in a more granular embedding.</div>
                     </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5 Use Case 2: Accessing Martial Art Recordings through Motion Traits</h1>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">In the first use case, the human skeletons extracted have been treated as static postures,
                     thus operating on individual frames of the moving image recordings. For the purpose
                     of visualising the whole archive in two dimensions, this is suitable as we can draw
                     these skeletons, using them as glyphs representing frames in the collection, to create
                     compelling images as in Figure 5     . However, one might argue that embodied knowledge
                     archives are not only about human postures but also (if not more) about human motions.
                     This second use case thus investigates such an approach, proposing an innovative motion-based
                     retrieval mode. In the context of the Hong Kong Martial Arts Living Archive (HKMALA)
                     project, a retrieval system allows users to select a specific motion within the collection
                     (by choosing the start and end frames of a video sequence for instance) and returns
                     similar motions in the archive.      </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.1 Hong Kong Martial Arts Living Archive</h2>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">Traditional martial arts are considered knowledge treasures of humanities sustained
                        through generations by diverse ethnic groups. Among the genres, Southern Chinese martial
                        art (SCMA) is arguably one of the most prolonged systems embodying the mind-body ideologies
                        in the Far East, yet now facing challenges in knowledge transmission and the risk
                        of being lost. In preserving the living heritage of SCMA, the <span class="hi italic">Hong Kong Martial Arts Living Archive </span>(HKMALA) inspects a comprehensive set of digitisation tools to capture the martial
                        practices, with a chief focus on motion capturing (MoCap) the form sequence performances
                        (or known as or <span class="hi italic">taolu</span>). Since its origination in 2012, HKMALA has built a 4D motion archive spanning over
                        20 styles and 130 sets of empty-hand and weapon sequences.<a class="noteRef" href="#d4e756">[4]</a> The archive also collates various historical and reconstituted documentation of the
                        martial cultures, encompassing rituals, traditions, armaments and objects preserved
                        in Hong Kong      (Chao et al., 2018).</div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">The HKMALA‚Äôs exceptional volumes hold the promise to enable various scholarly inquiries.
                        However, its current archival organisation is based on a manual catalogue reflecting
                        how the content has been curated for exhibitions. Besides, the data consists of multiple
                        modalities, such as texts, images, videos and MoCap data, hence impeding public access
                        and dissemination at scale. There is a need for an effective yet meaningful way of
                        data access.      </div>
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">Addressing the aforementioned challenges, we propose devising a motion-based retrieval
                        framework that leverages machine learning to encode motion-wise information in multimodal
                        recordings. The approach facilitates content retrieval through embodied cues, operating
                        seamlessly across MoCap and moving image data formats. In the following paragraphs,
                        we aim to provide a technical overview of the technical procedures. For a more comprehensive
                        understanding of the implementation details, readers are invited to refer to the descriptions
                        in (Hou et al., 2023).</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.2 Encoding and retrieving martial arts movement</h2>
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">Chinese martial arts are renowned for their strong emphasis on form training, utilizing
                        codified movement sequences to practise fundamental techniques and technique combinations.
                        While less known, yet equally important, is the stress on combative tactics that involve
                        a mindful operation of whole-body movement. Thus, training is imparted through a combination
                        of freeform combats and integration into <span class="hi italic">taolu</span> practices (Ma, 2003). Given that assessment methods in Chinese martial arts typically
                        consider visually discernible characteristics and mechanical parameters, we apply
                        the same configurations to model martial arts movement. Designed in compatibility
                        with Laban metrics (Guest, 1977) for movement analysis, the approach aims to articulate
                        both qualitative and quantitative qualities regarding the technical stances, ballistic
                        activities, and reactions to the opponent or surroundings, as shown in Table 2 in
                        the Appendix.</div>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">The technical procedures commence with the extraction of 3D human skeletons from the
                        MoCap data or the extraction of 2D poses from video recordings, followed by depth
                        estimation to reconstruct the 3D structure. It then computes the coordinates and joint
                        angles from raw skeletons into basic kinematic metrics, which are then normalized
                        for further processing. On this basis, feature augmentation is employed to enrich
                        the kinematic feature set, incorporating metrics that reflect linear and angular motion
                        characteristics, along with the transition probabilities of motion time series. Finally,
                        the best 50 features are selected utilizing the variance threshold approach and fed
                        for training a discriminative neural network.</div>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">Holding a hypothesis that the characteristics of a long motion sequence can be represented
                        by its segmentations, our approach to the encoding task is set to pinpoint representative
                        motion units within the sequence     . Firstly, we gathered high-dimensional features
                        from equally segmented media data and employed deep learning to train a latent space,
                        embedding them into a      vector space where semantically similar dimensions are
                        put closer while dissimilar ones are set apart. Subsequently     , K-means clustering
                        is applied      to identify cluster centres as representatives. To create      a descriptive
                        and compact scheme for efficient retrieval, we encoded the inverse document frequency
                        (IDF) of each representative into a normalised histogram, named <span class="hi italic">metamotion</span>, to represent the distribution of archetypal qualities within a motion sequence.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.3 A motion-based retrieval engine</h2>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">To facilitate the envisioned scenario of exploratory archival browsing, a retrieval
                        system has been deployed to allow querying archival sequences containing similar movements.
                        The system integrates different retrieval algorithms, including      Locality-sensitive
                        hashing (LSH) and Ball-tree methods, which prove effective in achieving optimal  
                        retrieval efficiency and outperforms the baseline approach in existing research (see
                        4. Experimental evaluation in (Hou et al., 2023)).      </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">On the user end, an interactive search engine has been deployed enable motion-based
                        data exploration across formats. An intuitive search case is illustrated in Figure
                        6, showcasing a cross-model retrieval result from a hybrid of videos and MoCap data.
                        Additionally, Figure 7 demonstrates the initial development of the archival browser,
                        presenting the retrieved items with ontological descriptions for the conceptual entities
                        associated with the query item. This design aims to improve data explainability and
                        interoperability for future reuse, with ontology annotation sourced from the scholarly
                        work of the Martial Art Ontology (MAon).<a class="noteRef" href="#d4e795">[5]</a></div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†6.¬†</div>The top-5 similar sequences retrieved by the Query from a MoCap dataset (left) and
                           a video dataset (right) (Hou et al., 2023).</div>
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.jpg" rel="external"><img src="resources/images/figure07.jpg" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†7.¬†</div>Illustration of a motion search example supplemented with ontological representations
                           of the concepts of techniques.</div>
                     </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">6 Discussion</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.1 Visual AI toward ‚Äúaugmented archives‚Äù</h2>
                     
                     <div class="counter"><a href="#p32">32</a></div>
                     <div class="ptext" id="p32">When working with cultural heritage archives, including the born-digital ones, it
                        is common to expect metadata organisation in an archive-specific curatorial structure
                        with topical domain-language descriptions about the archival items. For instance,
                        both the Prix de Lausanne and HKMALA collections warrant a series of textual tags
                        indicating the content of dance and martial art performances and additional descriptions
                        providing knowledge about the performers. Such documentation, although resourceful,
                        entails a mode of interpretation and access rather limited to expert users. One needs
                        to know what dance performances <span class="hi italic">The Giselle </span>or <span class="hi italic">Nutcracker </span>are to look for them in the archive, or understand how martial arts are grounded on
                        a series of <span class="hi italic">taolu</span>. Furthermore, visual and embodied features are difficult to capture verbally. Even
                        though models like Labanotation have been developed to describe movement components,
                        especially in performing arts, they require a reasonably deep understanding of the
                        knowledge in order to encode and decode. Hence, it is arguably very hard, if not impossible,
                        to capture embodied knowledge features into literal formats or in an easily accessible
                        way. Therefore, this kind of metadata, although necessary and useful in many domains,
                        is not suitable for access to the general public. It does not support ‚Äúcasual modes
                        of access‚Äù, fundamentals for museum-goers and general audiences (Whitelaw et al.,
                        2015).</div>
                     
                     <div class="counter"><a href="#p33">33</a></div>
                     <div class="ptext" id="p33">In this article, we introduce      a computational framework aimed at enhancing cultural
                        archives      by enriching metadata      with embodied knowledge      descriptors
                        derived from audiovisual and multimodal materials.      Utilising computer vision
                        models, we extract and transform human poses and skeleton features into meaningful
                        data extractions, which potentially foster a range of usage scenarios     . At the
                        archival level, curators can operationalise      their collections as ‚Äò‚Äôdata sources‚Äô‚Äô
                        (Kenderdine et al., 2021),      envisioning new ways for querying, annotating and
                        analysing whole datasets to forge novel narrative paradigms.     . In parallel, the
                        enriched metadata improves data accessibility and diversified query channels improve
                        data findability, augmenting the archives' compliance with FAIR (findability, accessibility,
                        interoperability, and reusability) data principles, regardless of variations in language,
                        topics, and data formats. Furthermore, our approach demonstrates the potential for
                        interpreting data traits within and across cultural contexts. This not only contributes
                        to the reusability of archival content but also resonates     with the findings of
                        (Colavizza et al., 2021),      which highlights the      growing role of AI in archival
                        practices, particularly in      ‚Äúautomation of recordkeeping processes'' and improving
                        ‚Äúorganising and accessing archives‚Äù. Our experimentation, as exemplified through two
                        distinctive heritage archives, represents an innovative step forward with a focus
                        on embodied knowledge, allowing queries through more perceptive than quantitative
                        channels, which we believe is more natural and humanistic.     </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.2 Archive navigation and serendipitous discovery</h2>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">The operationalisation of embodied knowledge archives through visual AI, resulting
                        in ‚Äúaugmented archives‚Äù, provides a vast array of data that can further be processed
                        in order to create new modes of access adapted for general audiences. Following the
                        principles of ‚Äúgenerous interfaces‚Äù (Whitelaw et al., 2015), our method supports explorative
                        behaviours, encouraging a new paradigm of ‚Äúseeking the unknown‚Äù and avoiding the tendency
                        of only searching for what is known (Winters and Prescott, 2019). By laying out the
                        full archive through the lens of humanly understandable concepts such as postures
                        taken by the dancers, users can more easily comprehend the archive, without needing
                        specific knowledge about this topic. They do not need to have a specific goal in mind
                        or to be looking for something in particular. They can wander around, browsing like
                        an ‚Äúinformation flaneur‚Äù (D√∂rk et al., 2011), enjoying the Prix de Lausanne archive
                        in a way that traditional modes of access, based on querying through the metadata
                        information or on grid-like and lists visualisations, could not offer. Furthermore,
                        each pose links to a timestamp in a dance performance, and thus by grouping together
                        similar poses in the low-dimensional space, the dimensionality reduction also creates
                        serendipitous discoveries. Indeed, this mode of access rewards users for simply browsing
                        the collection and stumbling upon new performances as they move from a pose to their
                        neighbours on the map. Figure 8 explicates this process by showcasing poses similar
                        to an input pose with the corresponding video frames.</div>
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">This new mode of access is enhanced by dimensionality reduction algorithms. However,
                        one must take care in how to decide which algorithm to employ, and with which parameters.
                        In this work, we have analysed Principal Component Analysis, t-distributed Stochastic
                        Neighbour Embedding and Uniform Manifold Approximation and Projection. PCA is a well-established
                        dimensionality reduction technique, but, due to its linear nature, it often fails
                        to properly capture a real-life dataset with only a handful of dimensions. This is
                        clearly confirmed in Figure 2, where all embeddings generated with tSNE and UMAP show
                        much more interesting structures without creating a large clump of points like PCA.
                        Regarding the choice of feature vectors used, as mentioned, we fail to observe noticeable
                        differences between the flattened vectors of keypoints and the custom features computed.
                        We hypothesise the custom features (reported in Table 1) are not more discriminative
                        than the base keypoints. In subsequent analysis, we decided to use the flattened vectors
                        of keypoints, but we believe this offers us an opportunity for more collaboration,
                        since involving dance experts could help in crafting a more precise and adequate set
                        of features, based on Labanotation for instance (Guest, 1977). To gain a deeper understanding
                        of these algorithms, we have investigated the effect of parameters on tSNE and UMAP.</div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">Figure 3 shows that increasing the perplexity yields very different layouts, with
                        lower values displaying numerous small groupings of items while higher values reveal
                        more large-scale structures. This is in accordance with expectations, since with higher
                        perplexities tSNE considers more points when computing the vicinity of each item,
                        thus better capturing global structures. Surprisingly, when increasing the number
                        of neighbours <span class="hi italic">ùëõ</span> considered with UMAP in Figure 4, the outputs appear to be much more stable, with
                        global structures already visible with <span class="hi italic">ùëõ</span> = 50. It is in this case the second parameter, <span class="hi italic">ùëë</span>ùëöùëñùëõ, that affects more the results, yielding more sprayed-out mappings the higher
                        it is (since the minimum distance directly controls how packed the points can be in
                        the low dimensional space). These results indicate that when creating archive visualisation
                        like the map of poses in Figure 5, for instance, higher <span class="hi italic">ùëë</span>ùëöùëñùëõ might be more suitable to avoid overlaps between similar poses. However, if one
                        were to apply clustering algorithms on these embeddings to group together similar
                        poses, the more fine-grained and packed structures obtained with lower <span class="hi italic">ùëë</span>ùëöùëñùëõ would potentially yield better results. Therefore, we argue it is not a matter
                        of which embedding is better but rather which embedding is more adapted to the specific
                        narrative or mode of access sought.</div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="media/image8.jpg" rel="external"><img src="media/image8.jpg" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†8.¬†</div>Examples of similar poses in the Prix de Lausanne archive (top 5 matches). Simplified
                           poses and corresponding video frames are displayed. Notice how connections between
                           different performers are discovered.</div>
                     </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.3 Limitations and future work directions</h2>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Although we believe our computational framework to be well thought out, there are
                        still some limitations we would like to highlight, to hopefully later address them.</div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">First, human pose extraction from monocular videos is never perfect. Human bodies
                        can be wrongfully estimated or missed due to the influence of occlusion, monotonous
                        colour patterns, and camera angles, to name a few. Indeed, through our ‚Äúnaive‚Äù approach
                        to the Prix de Lausanne Archive, we only achieve a detection rate of 76.80%.<a class="noteRef" href="#d4e883">[6]</a> Furthermore, by checking a sample of skeletons extracted with the corresponding frames
                        as in Figure 8, we noticed that sometimes keypoints were not correctly extracted.
                        One possible reason is that the pose estimation is done systematically every five
                        seconds without a measure of their quality or significance. Nevertheless, our approach
                        proves sufficient for the use case described, as it still produces enough data to
                        map the whole archive properly. Further developments could yield more interesting
                        and precise results, for instance, by extracting skeleton data on a finer temporal
                        granularity and then filtering only to keep the ‚Äúbetter‚Äù poses.<a class="noteRef" href="#d4e885">[7]</a></div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Second, the feature vectors computed for the Prix de Lausanne are somewhat naive.
                        Taking the lesson from the HKMALA feature computing, collaboration with dance experts
                        could facilitate the design of more relevant dance-specific features with precise
                        measurements able to better capture and compare human bodies during dance performances.
                        Nonetheless, our results demonstrate that even naive methods can produce new modes
                        of access to embodied knowledge archives. Thus, we are confident our method generalises
                        well to other archives and diverse embodied disciplines.</div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">Lastly, unlike standard computer vision challenges, it is necessary but challenging
                        to quantify what defines a good estimation of poses or movement segments in the archival
                        context, yet the standard varies in people and cultural themes. To this end, we resorted
                        to evaluating the distribution and quality of embeddings, supplemented with an ad-hoc
                        expert review of small sampling sets. Future improvement is suggested to integrate
                        expert review dynamically along with the model training process, such as by intaking
                        expert judgement as a score and feeding it back to the model, so as to enable a human-in-the-loop
                        machine learning process.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.4 Outlook: towards a new archival experience</h2>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">The research presented in this paper has been conducted within the larger context
                        of ‚Äúcomputational museology‚Äù (Kenderdine et al., 2021), a discipline whose goal is
                        to develop new modes of access to cultural heritage through computational approaches,
                        specifically designed for situated experiences in museum settings. To this end, further
                        work will rely on leveraging the findings highlighted in this paper to create interactive
                        installations to access these embodied knowledge archives. We contend the implementation
                        presented in this paper establishes a robust foundation for developing interactive
                        modes of access tailored to meet the requirements of casual users within situated
                        experiences.  In particular, two main directions will be pursued.     On one hand,
                        based on the fundamental concept of placing visitors ‚Äúinside‚Äù the archive (Shen et
                        al., 2019) rather than looking at it on a plain screen, immersive environments (IEs)
                        will be employed to create situated experiences in which users can navigate the whole
                        archive. In practice, dimensionality reduction techniques will be employed to compute
                        a mapping in two or three dimensions in order to generate a virtual world in which
                        each point, or glyph, represents a pose (and its corresponding frame, or clip). This
                        virtual cloud of human bodies will then be displayed in large IEs, allowing visitors
                        to freely navigate and discover the archive. Applying immersive technologies to the
                        mediation of cultural heritage already has some interesting applications, in particular
                        in the context of archeological enquiry (Sciuto et al., 2023), and we believe such
                        technologies can also serve a purpose for Embodied Knowledge archives.</div>
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">On the other hand, an interactive retrieval system can be developed, either based
                        on pose or motion similarity. Users could strike a pose or perform a certain movement,
                        detected in real-time with motion-tracking solutions, and the system would retrieve
                        relevant clips from the collection. Such an experience would yield an interesting
                        performative aspect that sees the visitor as a co-creator with the machine, essentially
                        transforming them into a variable of the generative system, and other people around
                        as an audience, witnessing the exchange in a ‚Äúthird-person‚Äôs perspective‚Äù (Mul and
                        Masson, 2018).</div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">7 Conclusion</h1>
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">Embodied knowledge archives are an integral part of our Living Heritage and hold important
                     aspects of our cultures. Yet, it is still difficult to explore them, in particular
                     for more casual audiences that lack specific knowledge on the topic of the collection.
                     To answer this challenge, we have proposed in this work a computational framework
                     that leverages motion extraction AI to augment these datasets with a wealth of rich
                     data, enabling new modes of analysis and access to the important collections.</div>
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">The method proposed is applied to two embodied knowledge archives, on dance and on
                     martial arts performances, showcasing its application to multimedia content and diverse
                     access scenarios. In the former example, we devise a method to visualise a whole collection
                     in two dimensions through the human poses embedded in the archival material, revealing
                     its structure and fostering serendipitous discoveries. On the latter, we extend to
                     motion encoding from static poses and devise a motion-based query system, offering
                     a new way to search an embodied knowledge archive. These scenarios showcase how our
                     computational framework can operationalise this type of collection and unlock a variety
                     of new modes of access suitable for non-expert audiences.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">Authors are grateful to the <span class="hi italic">Prix de Lausanne </span>for the opportunity to work on their audiovisual archive, as part of the SNSF's Sinergia
                     grant <span class="hi italic">Narratives from the Long Tail: Transforming Access to Audiovisual Archives</span> (grant number CRSII5_198632).</div>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">The <span class="hi italic">Hong Kong Martial Arts Living Archive</span> is a longitudinal research collaboration between the International Guoshu Association,
                     City University of Hong Kong, and the Laboratory for Experimental Museology (eM+),
                     EPFL.</div>
                  </div>
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e244"><span class="noteRef lang en">[1]¬†By <em class="word">transformative potential</em>, we mean the 
                     potential of AI to augment the data available by extracting new features that can
                     open innovative venues for accessing the archive.</span></div>
               <div class="endnote" id="d4e313"><span class="noteRef lang en">[2]¬†Tim Ingold argues that some 
                     movement is ‚Äúautomatic and rhythmically responsive‚Äù to its surroundings and ‚Äúalong multiple 
                     pathways of sensory participation‚Äù [<a class="ref" href="#ingold_2011">Ingold 2011</a>].</span></div>
               <div class="endnote" id="d4e532"><span class="noteRef lang en">[3]¬†See <a href="https://www.prixdelausanne.org/" onclick="window.open('https://www.prixdelausanne.org/'); return false" class="ref">https://www.prixdelausanne.org</a></span></div>
               <div class="endnote" id="d4e756"><span class="noteRef lang en">[4]¬† The concept of four-dimensional space (4D) denotes a dynamic 3D space moving through
                     time.</span></div>
               <div class="endnote" id="d4e795"><span class="noteRef lang en">[5]¬† The Martial Art Ontology (MAon) V1.1, https://purl.org/maont/techCorpus.</span></div>
               <div class="endnote" id="d4e883"><span class="noteRef lang en">[6]¬† A ‚Äúnaive‚Äù approach in pattern recognition implies a straightforward and easy-to-implement
                     algorithm that finds all matching occurrences of a given input.</span></div>
               <div class="endnote" id="d4e885"><span class="noteRef lang en">[7]¬† One would first need to define what is a ‚Äò‚Äôbetter‚Äô‚Äô pose, however.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="aristidou_et_al_2018"><!-- close -->Aristidou et al. 2018</span>¬†Artistidou, A. (2018) ‚ÄúDeep motifs and motion signatures‚Äù, 
                  <cite class="title italic">Transactions on Graphics</cite>, 37(6), pp. 1‚Äì13.
                  </div>
               <div class="bibl"><span class="ref" id="aristidou_shamir_chrysanthou_2019"><!-- close -->Aristidou, Shamir, and Chrysanthou 2019</span>¬†Aristidou, A., Shamir, A., and Chrysanthou, Y. (2019) 
                  ‚ÄúDigital dance ethnography: Organizing large dance collections‚Äù, <cite class="title italic">Journal on Computing and Cultural 
                     Heritage</cite>, 12(4), pp. 1‚Äì27.
                  </div>
               <div class="bibl"><span class="ref" id="arnold_tilton_2019"><!-- close -->Arnold and Tilton 2019</span>¬†Arnold, T. and Tilton, L. (2019) ‚ÄúDistant viewing: Analyzing large 
                  visual corpora‚Äù, <cite class="title italic">Digital Scholarship in the Humanities</cite>, 34(Supplement 1), pp. i3-i16.
                  </div>
               <div class="bibl"><span class="ref" id="aske_giardinetti_2023"><!-- close -->Aske and Giardinetti 2023</span>¬†Aske, K. and Giardinetti, M. (2023) ‚Äú(Mis)matching metadata: 
                  Improving accessibility in digital visual archives through the EyCon project‚Äù, <cite class="title italic">Journal on Computing and Cultural 
                     Heritage</cite>, 16(4).
                  </div>
               <div class="bibl"><span class="ref" id="bardiot_2021"><!-- close -->Bardiot 2021</span>¬†Bardiot, C. (2021) <cite class="title italic">Performing arts and digital humanities: From traces to 
                     data</cite>. Hoboken, NJ: John Wiley &amp; Sons.
                  </div>
               <div class="bibl"><span class="ref" id="bazarevsky_et_al_2020"><!-- close -->Bavarevsky et al. 2020</span>¬†Bazarevsky, V. et al. (2020) ‚ÄúBlazepose: On-device real-time 
                  body pose tracking‚Äù, <cite class="title italic">arXiv</cite>. <a href="https://arxiv.org/abs/2006.10204" onclick="window.open('https://arxiv.org/abs/2006.10204'); return false" class="ref">https://arxiv.org/abs/2006.10204</a>.
                  </div>
               <div class="bibl"><span class="ref" id="benesh_benesh_1977"><!-- close -->Benesh and Benesh 1977</span>¬†Benesh, R. and Benesh, J. (1977) <cite class="title italic">Reading dance: The birth of 
                     choreology</cite>. London: Souvenir Press.
                  </div>
               <div class="bibl"><span class="ref" id="bernasconi_cetnic_impett_2023"><!-- close -->Bernasconi, Cetiniƒá, and Impett 2023</span>¬†Bernasconi, V., Cetiniƒá, E., and Impett, L. (2023) 
                  ‚ÄúA computational approach to hand pose recognition in early modern paintings‚Äù, <cite class="title italic">Journal of 
                     Imaging</cite>, 9(6).
                  </div>
               <div class="bibl"><span class="ref" id="broadwell_tangherlini_2021"><!-- close -->Broadwell and Tangherlini 2021</span>¬†Broadwell, P. and Tangherlini, T.R. (2021) 
                  ‚ÄúComparative K-pop choreography analysis through deep-learning pose estimation across
                  a large video corpus‚Äù, 
                  <cite class="title italic">DHQ: Digital Humanities Quarterly</cite>, 15(1). Available at: 
                  <a href="https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html" onclick="window.open('https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html'); return false" class="ref">https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="cameron_franks_hamidzadeh_2023"><!-- close -->Cameron, Franks, and Hamidzadeh 2023</span>¬†Cameron, S., Franks, P., and Hamidzadeh, B. (2023) 
                  ‚ÄúPositioning paradata: A conceptual frame for AI processual documentation in archives
                  and recordkeeping contexts‚Äù, 
                  <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 16(4).
                  </div>
               <div class="bibl"><span class="ref" id="cao_et_al_2019"><!-- close -->Cao et al. 2019</span>¬†Cao, Z. et al. (2019) ‚ÄúOpenpose: Realtime multi-person 2D pose estimation 
                  using part affinity fields‚Äù, <cite class="title italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</cite>. Available at: 
                  <a href="https://arxiv.org/pdf/1812.08008.pdf" onclick="window.open('https://arxiv.org/pdf/1812.08008.pdf'); return false" class="ref">https://arxiv.org/pdf/1812.08008.pdf</a>.
                  </div>
               <div class="bibl"><span class="ref" id="chalmers_et_al_2021"><!-- close -->Chalmers et al. 2021</span>¬†Chalmers, A. et al. (2021) ‚ÄúRealistic humans in virtual cultural 
                  heritage‚Äù, <cite class="title italic">Proceedings of RISE IMET 2021</cite>. Nicosia, Cyprus, 2-4 June 2021. New York: Springer, pp. 156-165.
                  </div>
               <div class="bibl"><span class="ref" id="chao_et_al_2018"><!-- close -->Chao et al. 2018</span>¬†Chao, H. et al. (2018) ‚ÄúKapturing kung fu: Future proofing the Hong Kong martial 
                  arts living archive‚Äù, in Whatley, S., Cisneros, R.K., and Sabiescu, A. (eds.) <cite class="title italic">Digital echoes: Spaces for intangible and 
                     performance-based cultural heritage</cite>. New York: Springer, pp. 249-264.
                  </div>
               <div class="bibl"><span class="ref" id="colavizza_et_al_2021"><!-- close -->Colavizza et al. 2021</span>¬†Colavizza, G. (2021) ‚ÄúArchives and AI: An overview of current 
                  debates and future perspectives‚Äù, <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 15(1).
                  </div>
               <div class="bibl"><span class="ref" id="craig_et_al_2018"><!-- close -->Craig et al. 2018</span>¬†Craig, C.J. (2018) ‚ÄúThe embodied nature of narrative knowledge: A 
                  cross-study analysis of embodied knowledge in teaching, learning, and life‚Äù, <cite class="title italic">Teaching and Teacher Education</cite>, 71, 
                  pp. 329-240.
                  </div>
               <div class="bibl"><span class="ref" id="delbridge_2015"><!-- close -->Delbridge 2015</span>¬†Delbridge, M. (2015) <cite class="title italic">Motion capture in performance: An introduction</cite>. 
                  New York: Springer.
                  </div>
               <div class="bibl"><span class="ref" id="dimitropoulos_et_al_2014"><!-- close -->Dimitropoulos et al. 2014</span>¬†Dimitropoulos, K. et al. (2014) ‚ÄúCapturing the intangible 
                  an introduction to the i-Treasures project‚Äù, <cite class="title italic">Proceedings of the 9th international conference on computer vision theory 
                     and applications</cite>, vol. 3. Lisbon, Portugal, 5-8 January 2014. Set√∫bal, Portugal: SCITEPRESS, pp. 773-781.
                  Available at: 
                  <a href="https://ieeexplore.ieee.org/abstract/document/7295018" onclick="window.open('https://ieeexplore.ieee.org/abstract/document/7295018'); return false" class="ref">https://ieeexplore.ieee.org/abstract/document/7295018</a>.
                  </div>
               <div class="bibl"><span class="ref" id="doulamis_et_al_2017"><!-- close -->Doulamis et al. 2017</span>¬†Doulamis, A.D. (2017) ‚ÄúTransforming intangible folkloric performing 
                  arts into tangible choreographic digital objects: The terpsichore approach‚Äù, <cite class="title italic">Proceedings of the 12th international 
                     joint conference on computer vision, imaging and computer graphics, theory, and applications</cite>. Porto, Portugal, 27 February-1 March 2017. 
                  Set√∫bal, Portugal: SCITEPRESS, pp. 451-460.
                  <a href="https://doi.org/10.3030/691218" onclick="window.open('https://doi.org/10.3030/691218'); return false" class="ref">https://doi.org/10.3030/691218</a>.
                  </div>
               <div class="bibl"><span class="ref" id="dork_carpendale_williamson_2011"><!-- close -->D√∂rk, Carpendale, and Williamson 2011</span>¬†D√∂rk, M., Carpendale, S., and Williamson, C. (2011) 
                  ‚ÄúThe information flaneur: A fresh look at information seeking‚Äù, <cite class="title italic">Proceedings of the SIGCHI 
                     conference on human factors in computing systems</cite>. Vancouver, BC, Canda, 7-12 May 2011. New York: ACM, pp. 1215-1224. 
                  <a href="https://doi.org/10.1145/1978942.1979124" onclick="window.open('https://doi.org/10.1145/1978942.1979124'); return false" class="ref">https://doi.org/10.1145/1978942.1979124</a>.
                  </div>
               <div class="bibl"><span class="ref" id="edmondson_2004"><!-- close -->Edmondson 2004</span>¬†Edmondson, R. (2004) <cite class="title italic">Audiovisual archiving: Philosophy and principles</cite>. 
                  Paris: United Nations Educational, Scientific and Cultural Organization.
                  </div>
               <div class="bibl"><span class="ref" id="el-raheb_et_al_2018"><!-- close -->El Raheb et al. 2018</span>¬†El Raheb, K. (2018) ‚ÄúA web-based system for annotation of dance 
                  multimodal recordings by dance practitioners and experts‚Äù, <cite class="title italic">Proceedings of the 5th international conference on movement 
                     and computing</cite>. Genoa, Italy, 28-30 June 2018. 
                  <a href="https://doi.org/10.1145/3212721.3212722" onclick="window.open('https://doi.org/10.1145/3212721.3212722'); return false" class="ref">https://doi.org/10.1145/3212721.3212722</a>.
                  </div>
               <div class="bibl"><span class="ref" id="fossati_et_al_2012"><!-- close -->Fossati et al. 2012</span>¬†Fossati, G. et al. (2012) ‚ÄúFound footage filmmaking, film archiving 
                  and new participatory platforms‚Äù, in Guldemond, J., Bloemheuvel, M., and Fossati, G. (eds.) <cite class="title italic">Found footage: Cinema 
                     exposed</cite>. Amsterdam: Amsterdam University Press, pp. 177-184.
                  </div>
               <div class="bibl"><span class="ref" id="grammalidis_dimitropoulos_2015"><!-- close -->Grammalidis and Dimitropoulos 2015</span>¬†Grammalidis, N. and Dimitropoulos, K. (2015) 
                  ‚ÄúIntangible treasures: Capturing the intangible cultural heritage and learning the
                  rare know-how of living human 
                  treasures‚Äù, <cite class="title italic">Proceedings of the 2015 digital heritage international congress</cite>, Granada, Spain, 28 September-2 
                  October 2015. Available at: 
                  <a href="https://diglib.eg.org/handle/10.2312/14465" onclick="window.open('https://diglib.eg.org/handle/10.2312/14465'); return false" class="ref">https://diglib.eg.org/handle/10.2312/14465</a>.
                  </div>
               <div class="bibl"><span class="ref" id="griffin_wennerstrom_foka_2023"><!-- close -->Griffin, Wennerstr√∂m, and Foka 2023</span>¬†Griffin, G., Wennerstr√∂m, E., and Foka, A. (2023) 
                  ‚ÄúAI and Swedish heritage organisations: Challenges and opportunities‚Äù, <cite class="title italic">AI &amp; Society</cite>. 
                  <a href="https://doi.org/10.1007/s00146-023-01689-y" onclick="window.open('https://doi.org/10.1007/s00146-023-01689-y'); return false" class="ref">https://doi.org/10.1007/s00146-023-01689-y</a>.
                  </div>
               <div class="bibl"><span class="ref" id="guest_1977"><!-- close -->Guest 1977</span>¬†Guest, A.H. (1977) <cite class="title italic">Labanotation: Or, kinetography laban: The system of analyzing and 
                     recording movement</cite>. New York: Taylor &amp; Francis.
                  </div>
               <div class="bibl"><span class="ref" id="halko_martinsson_tropp_2011"><!-- close -->Halko, Martinsson, and Tropp 2011</span>¬†Halko, N., Martinsson, P.G. and Tropp, J.A. (2011), 
                  ‚ÄúFinding structure with randomness: Probabilistic algorithms for constructing approximate
                  matrix decompositions‚Äù, 
                  <cite class="title italic">SIAM Review</cite>, 53(2), pp. 217‚Äì288. 
                  <a href="https://doi.org/10.1137/090771806" onclick="window.open('https://doi.org/10.1137/090771806'); return false" class="ref">https://doi.org/10.1137/090771806</a>.
                  </div>
               <div class="bibl"><span class="ref" id="hou_et_al_2022"><!-- close -->Hou et al. 2022</span>¬†Hou, Y. et al. (2022) ‚ÄúDigitizing intangible cultural heritage embodied: State 
                  of the art‚Äù, <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 15(3), pp. 1‚Äì20.
                  </div>
               <div class="bibl"><span class="ref" id="hou_seydou_kenderdine_2023"><!-- close -->Hou, Seydou, and Kenderdine 2023</span>¬†Hou, Y., Seydou, F.M., and Kenderdine S. (2023) 
                  ‚ÄúUnlocking a multimodal archive of southern chinese martial arts through embodied cues‚Äù, 
                  <cite class="title italic">Journal of Documentation</cite>. 
                  <a href="https://doi.org/10.1108/JD-01-2022-0027" onclick="window.open('https://doi.org/10.1108/JD-01-2022-0027'); return false" class="ref">https://doi.org/10.1108/JD-01-2022-0027</a>.
                  </div>
               <div class="bibl"><span class="ref" id="impett_2020a"><!-- close -->Impett 2020a</span>¬†Impett, L. (2020a) ‚ÄúAnalyzing gesture in digital art history‚Äù, in 
                  Brown, K. (ed.) <cite class="title italic">The routledge companion to digital humanities and art history</cite>. New York: Routledge, pp. 386-407.
                  </div>
               <div class="bibl"><span class="ref" id="impett_2020b"><!-- close -->Impett 2020b</span>¬†Impett, L. (2020b) <cite class="title italic">Painting by numbers: Computational methods and the history 
                     of art</cite>. EPFL.
                  </div>
               <div class="bibl"><span class="ref" id="ingold_2011"><!-- close -->Ingold 2011</span>¬†Ingold, T. (2011) <cite class="title italic">Being alive: Essays on movement, knowledge and description</cite>. 
                  New York: Taylor &amp; Francis.
                  </div>
               <div class="bibl"><span class="ref" id="jaillant_2022"><!-- close -->Jaillant 2022</span>¬†Jaillant, L. (2022) <cite class="title italic">Archives, access and artificial intelligence: Working 
                     with born-digital and digitized archival collections</cite>. Bielefeld, Germany: Bielefeld University Press.
                  </div>
               <div class="bibl"><span class="ref" id="jaillant_rees_2023"><!-- close -->Jaillant and Rees 2023</span>¬†Jaillant, L. and Rees, A. (2023) ‚ÄúApplying AI to digital 
                  archives: Trust, collaboration and shared professional ethics‚Äù, <cite class="title italic">Digital Scholarship in the Humanities</cite>, 38(2), pp. 
                  571-585.
                  </div>
               <div class="bibl"><span class="ref" id="kenderdine_mason_hibberd_2021"><!-- close -->Kenderdine, Mason, and Hibberd 2021</span>¬†Kenderdine, S., Mason, I., and Hibberd L. (2021) 
                  ‚ÄúComputational archives for experimental museology‚Äù, <cite class="title italic">Proceedings of RISE IMET 2021</cite>. Nicosia, 
                  Cyprus, 2-4 June 2021. New York: Springer, pp. 3-18.
                  </div>
               <div class="bibl"><span class="ref" id="kreiss_bertoni_alahi_2021"><!-- close -->Kreiss, Bertoni, and Alahi 2021</span>¬†Kreiss, S., Bertoni, L., and Alahi, A. (2021) 
                  ‚ÄúOpenpifpaf: Composite fields for semantic keypoint detection and spatio-temporal association‚Äù, 
                  <cite class="title italic">IEEE Transactions on Intelligent Transportation Systems</cite>, 23(8), pp. 13498-13511.
                  </div>
               <div class="bibl"><span class="ref" id="krol_mynarski_2005"><!-- close -->Kr√≥l and Mynarski 2005</span>¬†Kr√≥l, H. and Mynarski, W. (2005) <cite class="title italic">Cechy ruchu-charakterystyka i 
                     mo≈ºliwo≈õci parametryzacji [Features of movement-characteristics and capabilities of
                     parametryzation]</cite>. Katowice, Poland: Akademia Wychowania 
                  Fizycznego.
                  </div>
               <div class="bibl"><span class="ref" id="lin_et_al_2014"><!-- close -->Lin et al. 2014</span>¬†Lin, T.Y. (2014) ‚ÄúMicrosoft COCO: Common objects in context‚Äù, 
                  <cite class="title italic">Proceedings of the 13th annual European conference of computer vision</cite>. Zurich, Switzerland, 6-12 September 2014. 
                  New York: Springer, pp. 740-755.
                  </div>
               <div class="bibl"><span class="ref" id="ma_2003"><!-- close -->Ma 2003</span>¬†Ma, M. (2003) <cite class="title italic">Wu xue tan zhen [Examination of truth in martial studies)</cite>. Taipei, 
                  Taiwan: Lion Books.
                  </div>
               <div class="bibl"><span class="ref" id="mallik_chaudhury_2012"><!-- close -->Mallik and Chaudhury 2012</span>¬†Mallik, A. and Chaudhury, S. (2012) ‚ÄúAcquisition of 
                  multimedia ontology: An application in preservation of cultural heritage‚Äù, <cite class="title italic">International Journal of Multimedia Information 
                     Retrieval</cite>, 1(4), pp. 249‚Äì262.
                  </div>
               <div class="bibl"><span class="ref" id="mallik_chaudhury_ghosh_2011"><!-- close -->Mallik, Chaudhury, and Ghosh 2011</span>¬†Mallik, A., Chaudhury, S., and Ghosh, H. (2011) 
                  ‚ÄúNrityakosha: Preserving the intangible heritage of indian classical dance‚Äù, <cite class="title italic">Journal on Computing and 
                     Cultural Heritage</cite>, 4(3), pp. 1‚Äì25.
                  </div>
               <div class="bibl"><span class="ref" id="manovich_2020"><!-- close -->Manovich 2020</span>¬†Manovich, L. (2020) <cite class="title italic">Cultural analytics</cite>. Cambridge, MA: The MIT Press.
                  </div>
               <div class="bibl"><span class="ref" id="martinsson_rokhlin_tvgert_2011"><!-- close -->Martinsson, Rokhlin, and Tvgert 2011</span>¬†Martinsson, P.-G., Rokhlin, V., and Tygert, M. (2011) 
                  ‚ÄúA randomized algorithm for the decomposition of matrices‚Äù, <cite class="title italic">Applied and Computational Harmonic 
                     Analysis</cite>, 30(1), pp. 47‚Äì68. 
                  <a href="https://www.sciencedirect.com/science/article/pii/S1063520310000242" onclick="window.open('https://www.sciencedirect.com/science/article/pii/S1063520310000242'); return false" class="ref">https://www.sciencedirect.com/science/article/pii/S1063520310000242</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mcgregor_lab_2019"><!-- close -->McGregor and Lab 2019</span>¬†McGregor, W. and Lab, G.A.C. (2019) <cite class="title italic">Living archive</cite>. 
                  Availablet at: <a href="https://artsexperiments.withgoogle.com/living-archive" onclick="window.open('https://artsexperiments.withgoogle.com/living-archive'); return false" class="ref">https://artsexperiments.withgoogle.com/living-archive</a>. 
                  (Accessed: 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="mcinnes_et_al_2018"><!-- close -->McInnes et al. 2018</span>¬†McInnes, L. et al. (2018) ‚ÄúUmap: Uniform manifold approximation and 
                  projection‚Äù, <cite class="title italic">The Journal of Open Source Software</cite>, 3(29). 
                  <a href="https://doi.org/10.21105/joss.00861" onclick="window.open('https://doi.org/10.21105/joss.00861'); return false" class="ref">https://doi.org/10.21105/joss.00861</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mul_masson_2018"><!-- close -->Mul and Masson 2018</span>¬†Mul, G. and Masson, E. (2018) ‚ÄúData-based art, algorithmic poetry: 
                  Geert Mul in conversation with Eef Masson‚Äù, <cite class="title italic">TMG Journal for Media History</cite>, 21(2).
                  </div>
               <div class="bibl"><span class="ref" id="olesen_et_al_2016"><!-- close -->Olesen et al. 2016</span>¬†Olesen, C.G. (2016) ‚ÄúData-driven research for film history: Exploring 
                  the Jean Desmet collection‚Äù, <cite class="title italic">Moving Image: The Journal of the Association of Moving Image Archivists</cite>, 16(1), pp. 
                  82‚Äì105.
                  </div>
               <div class="bibl"><span class="ref" id="pedregosa_et_al_2011"><!-- close -->Pedregosa et al. 2011</span>¬†Pedregosa, F. et al. (2011) ‚ÄúScikit-learn: Machine learning in 
                  Python‚Äù, <cite class="title italic">Journal of Machine Learning Research</cite>, 12(85), pp. 2825‚Äì2830.
                  </div>
               <div class="bibl"><span class="ref" id="rtsarchives_2018"><!-- close -->RTSArchives 2018</span>¬†RTSArchives (2018) <cite class="title italic">Le nouveau site RTSarchives</cite>. Available at: 
                  <a href="https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html" onclick="window.open('https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html'); return false" class="ref">https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html</a>. 
                  (Accessed: 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="salazar_2018"><!-- close -->Salazar 2018</span>¬†Salazar Sutil, N. (2018) ‚ÄúSection editorial: Human movement as critical creativity: 
                  Basic questions for movement computing‚Äù, <cite class="title italic">Computational Culture: a Journal of Software Studies</cite>, 6.
                  </div>
               <div class="bibl"><span class="ref" id="sciuto_et_al_2023"><!-- close -->Sciuto et al. 2023</span>¬†Sciuto, C. et al. (2023) ‚ÄúExploring fragmented data: Environments, 
                  people and the senses in virtual reality‚Äù, in Landeschi, G. and Betts, E. (eds.) <cite class="title italic">Capturing the senses: Digital methods 
                     for sensory archaeologies</cite>. New York: Springer, pp. 85-103.
                  </div>
               <div class="bibl"><span class="ref" id="sedmidubsky_et_al_2020"><!-- close -->Sedmidubsky et al. 2020</span>¬†Sedmidubsky, J. et al. (2020) ‚ÄúMotion words: A text-like 
                  representation of 3D skeleton sequences‚Äù, <cite class="title italic">Proceedings of the 42nd annual European conference on information 
                     retrieval</cite>. Lisbon, Portugal, 14-17 April 2020. New York: Springer, pp. 527-241. 
                  <a href="https://doi.org/10.1007/978-3-030-45439-5_35" onclick="window.open('https://doi.org/10.1007/978-3-030-45439-5_35'); return false" class="ref">https://doi.org/10.1007/978-3-030-45439-5_35</a>.
                  </div>
               <div class="bibl"><span class="ref" id="shen_et_al_2019"><!-- close -->Shen et al. 2019</span>¬†Shen, H. et al. (2019) ‚ÄúInformation visualisation methods and techniques: 
                  State-of-the-art and future directions‚Äù, <cite class="title italic">Journal of Industrial Information Integration</cite>, 16, pp. 100‚Äì102.
                  </div>
               <div class="bibl"><span class="ref" id="whitelaw_2015"><!-- close -->Whitelaw 2015</span>¬†Whitelaw, M. (2015) ‚ÄúGenerous interfaces for digital cultural 
                  collections‚Äù, <cite class="title italic">DHQ: Digital Humanities Quarterly</cite>, 9(1). Availablet at: 
                  <a href="https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html" onclick="window.open('https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html'); return false" class="ref">https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html</a>.
                  </div>
               <div class="bibl"><span class="ref" id="winters_prescott_2019"><!-- close -->Winters and Prescott 2019</span>¬†Winters, J. and Prescott, A. (2019) ‚ÄúNegotiating the 
                  born-digital: A problem of search‚Äù, <cite class="title italic">Archives and Manuscripts</cite>, 47(3), pp. 391‚Äì403.
                  </div>
               <div class="bibl"><span class="ref" id="wright_2017"><!-- close -->Wright 2017</span>¬†Wright, R. ‚ÄúThe future of television archives‚Äù, 
                  <cite class="title italic">Digital Preservation Coalition</cite>, 29 November. Available at: 
                  <a href="https://www.dpconline.org/blog/wdpd/the-future-of-television-archives" onclick="window.open('https://www.dpconline.org/blog/wdpd/the-future-of-television-archives'); return false" class="ref">https://www.dpconline.org/blog/wdpd/the-future-of-television-archives</a>.
                  </div>
               <div class="bibl"><span class="ref" id="zkm_center_2023"><!-- close -->ZKM Center for Art and Media Karlsruhe 2023</span>¬†ZKM Center for Art and Media Karlsruhe (2023) 
                  ‚ÄúWilliam Forsythe: Improvisation technologies: The wesbite project‚Äù. Available at: 
                  <a href="https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project" onclick="window.open('https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project'); return false" class="ref">https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project</a>. 
                  (Accessed 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="van-der-maaten_hinton_2008"><!-- close -->van der Maaten and Hinton 2008</span>¬†van der Maaten, L. and Hinton, G. (2008) 
                  ‚ÄúVisualizing data using t-sne‚Äù, <cite class="title italic">Journal of Machine Learning Research</cite>, 9(86), pp. 2579‚Äì2605. 
                  Available at: 
                  <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" onclick="window.open('https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf'); return false" class="ref">https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a>.
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a> ¬†|¬† <span style="color: grey">XML</span> |¬† <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>