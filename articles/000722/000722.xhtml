<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume¬†¬†Number¬†</div>
            <div class="toolbar"><a href="#">Preview</a> ¬†|¬† <span style="color: grey">XML</span> |¬† <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Augmenting Access to Embodied Knowledge Archives: A Computational Framework</h1>
               
               <div class="author"><span style="color: grey">Giacomo Alliata
                     </span>¬†&lt;<a href="mailto:giacomo_dot_alliata_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('giacomo_dot_alliata_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('giacomo_dot_alliata_at_epfl_dot_ch'); return false;">giacomo_dot_alliata_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               <div class="author"><span style="color: grey">Yumeng Hou
                     </span>¬†&lt;<a href="mailto:yumeng_dot_hou_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('yumeng_dot_hou_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('yumeng_dot_hou_at_epfl_dot_ch'); return false;">yumeng_dot_hou_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               <div class="author"><span style="color: grey">Sarah Kenderdine
                     </span>¬†&lt;<a href="mailto:sarah_dot_kenderdine_at_epfl_dot_ch" onclick="javascript:window.location.href='mailto:'+deobfuscate('sarah_dot_kenderdine_at_epfl_dot_ch'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('sarah_dot_kenderdine_at_epfl_dot_ch'); return false;">sarah_dot_kenderdine_at_epfl_dot_ch</a>&gt;,¬†Laboratory for Experimental Museology, EPFL, Switzerland</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Augmenting%20Access%20to%20Embodied%20Knowledge%20Archives%3A%20A%20Computational%20Framework&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Alliata&amp;rft.aufirst=Giacomo&amp;rft.au=Giacomo%20Alliata&amp;rft.au=Yumeng%20Hou&amp;rft.au=Sarah%20Kenderdine"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>With the burgeoning use of digital technologies in safeguarding intangible and living
                     heritage, memory institutions have produced a significant body 
                     of material yet to be made accessible for public transmission. The quest for new ways
                     to unlock these massive collections has intensified, 
                     especially in the case of embodied knowledge embedded in complex formats, such as
                     audiovisual and motion capture data.
                     </p>
                  
                  <p>This study examines a computational workflow that combines posture recognition and
                     movement computing to bridge the gap in accessing digital 
                     archives that capture living knowledge and embodied experiences. By reflecting on
                     how embodied knowledge is sustained and potentially 
                     valorised through human interaction, we devise a series of methods utilising vision-based
                     feature extraction, pose estimation, movement analysis, 
                     and machine learning. The goal is to augment the archival experience with new modes
                     of exploration, representation, and embodiment.
                     </p>
                  
                  <p>This article reports the computational procedures and algorithmic tools inspected
                     through two use cases. In the first example, we visualise the 
                     archives of the Prix de Lausanne, a collection of 50 years of video recordings of dance performances, for archival
                     exploration through 
                     the dancers' poses. In another experiment, movement encoding is employed to allow
                     multimodal data search via embodied cues in the 
                     Hong Kong Martial Arts Living Archive, a comprehensive documentation of the living heritage of martial arts that is chiefly
                     comprised 
                     of motion-captured performances by masters.
                     </p>
                  
                  <p>Though holding different application purposes, both projects operate on the proposed
                     framework and extract archive-specific features to create a 
                     meaningful representation of human bodies, which reveals the versatile applications
                     that computational capacities can achieve for embodied knowledge 
                     archives. The practices also represent a model of interdisciplinary involvement where
                     the archivists, computists, artists, and knowledge holders 
                     join hands to renew strategies for archival exploration and heritage interpretation
                     in a new light.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1 Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Living heritage, also known as intangible culture, encompasses the practices, expressions,
                     spaces, and knowledge that safeguard the rich 
                     diversity of human creativity and collective heritage, as defined by UNESCO. The preservation and exploration of living heritage 
                     has been of paramount importance in understanding and celebrating the cultural fabric
                     of our society. To further enhance our engagement with this 
                     heritage, advancements in digital technologies have opened new possibilities for capturing
                     and experiencing embodied knowledge.
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">In recent years, there has been a growing trend in the digital capturing of living
                     heritage and embodied knowledge 
                     [<a class="ref" href="#hou_et_al_2022">Hou et al. 2022</a>]. These endeavours aim to document and preserve not only physical artefacts but also
                     the intangible aspects of 
                     culture, such as rituals, performances, and traditional practices. However, GLAM sectors,
                     encompassing galleries, libraries, archives, and 
                     museums, face significant challenges in effectively managing and providing access
                     to these vast digital repositories 
                     [<a class="ref" href="#jaillant_2022">Jaillant 2022</a>].
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">One of the key challenges faced by GLAM sectors is the sheer scale of digitised collections.
                     As more and more heritage materials are digitised, 
                     the volume of data grows exponentially, making it increasingly difficult to navigate
                     and explore these archives manually and essentially turning 
                     these collections into ‚Äúdark archives‚Äù inaccessible to the public. Giovanna Fossati et al. further highlight how 
                     non-expert audiences are in need of more intuitive modes of access to discover these
                     collections [<a class="ref" href="#fossati_et_al_2012">Fossati et al. 2012</a>]. To address 
                     this challenge, recent research has embarked on applying computational analysis, artificial
                     intelligence (AI) in particular, to unlock various 
                     new modes of archival experience. Leveraging the transformative potential of AI<a class="noteRef" href="#d4e253">[1]</a>, we 
                     can enhance the retrieval, visualisation, and navigation of multimodal cultural data
                     materials [<a class="ref" href="#aske_giardinetti_2023">Aske and Giardinetti 2023</a>]. 
                     [<a class="ref" href="#cameron_franks_hamidzadeh_2023">Cameron, Franks, and Hamidzadeh 2023</a>] [<a class="ref" href="#hou_seydou_kenderdine_2023">Hou, Seydou, and Kenderdine 2023</a>]. Further, by involving the collaborative efforts of 
                     cultural domain experts in configuring AI tools, there is a promise to alleviate the
                     gap between data science and GLAM sectors, highlighted 
                     through various uses cases on the implementation of AI strategies in Swedish heritage
                     organizations 
                     [<a class="ref" href="#griffin_wennerstrom_foka_2023">Griffin, Wennerstr√∂m, and Foka 2023</a>], and  enable users to interact with digitised archives in meaningful yet trustworthy
                     contexts 
                     [<a class="ref" href="#jaillant_rees_2023">Jaillant and Rees 2023</a>].
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">To tackle these challenges, we propose a computational framework that augments the
                     archival experience of embodied knowledge. Our approach 
                     intends to extract human movements in the form of skeleton data and process it, showing
                     its versatility through multiple use cases. By 
                     revalorizing and visualising knowledge, memory, and experience, we strive to enhance
                     the cognitive reception and engagement of users, ultimately 
                     improving the overall archival user experience. Our experiments are conducted on two
                     exemplar collections: the archives of the 
                     Prix de Lausanne, comprising an audiovisual dataset of dance performances, and the Hong Kong Martial Arts Living Archive 
                     (HKMALA), which contains both videos and motion capture recordings of Hong Kong martial arts performers. This work might be of 
                     interest to professionals of cultural heritage institutions wishing to explore new
                     methods of access to their collections, as well as researchers 
                     of computational humanities with a focus on embodied knowledge archives.
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">By exploring this computational framework, we hope to contribute to the ongoing efforts
                     to leverage visual AI technologies to transform the way 
                     we interact with and appreciate living heritage. Through the synthesis of cultural
                     heritage and cutting-edge computational methods, we can bridge 
                     the gap between the past and the present, enabling a deeper understanding and appreciation
                     of our shared human history.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2 Related Work</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.1 Embodied Knowledge as Part of Living Heritage</h2>
                     
                     <div class="counter"><a href="#p6">6</a></div>
                     <div class="ptext" id="p6">Though partly embedded in architectural sensorium and memory objects, intangible cultural
                        heritage (ICH) is inherently living and manifests 
                        through bodily expressions, individual practices, and social enactment ‚Äî on the multiple
                        layers of embodiment. Recent research has revealed a 
                        stress shift in documenting, describing, and presenting the living nature of ICH by
                        transforming intangible practices into tangible 
                        choreographic objects. For instance, the i-Treasures project (2013-2017) works to
                        digitize the living heritage of folk dances, folk singing, 
                        craftsmanship, and contemporary music composition in an attempt to sustain traditional
                        know-how via creative presentation 
                        [<a class="ref" href="#dimitropoulos_et_al_2014">Dimitropoulos et al. 2014</a>] [<a class="ref" href="#grammalidis_dimitropoulos_2015">Grammalidis and Dimitropoulos 2015</a>]. Taking a more public-facing approach, the 
                        Terpsichore project (2016 to 2020) integrates computational models, such as semantic
                        web technologies and machine learning, with storytelling 
                        to facilitate data accessibility and affordance of digitised materials [<a class="ref" href="#doulamis_et_al_2017">Doulamis et al. 2017</a>]. Famous choreographers have also 
                        been keen on documenting their practices with audiovisual recordings, resulting in
                        extensive collections such as William 
                        Forsythe's Performative Archive, recently acquired by the ZKM Center for Art and Media Karlsruhe. While its focus differs 
                        from the performing arts, the Hong Kong Martial Arts Living Archive (HKMALA) project (2012-present) forges a multimodal digital 
                        archiving paradigm by motion-capturing martial art sequence performances of renowned
                        masters. The motion-captured (MoCap) records are 
                        situated within a digital collation capturing different facets of Hong Kong's martial arts traditions, encompassing rituals, 
                        histories, re-enactments, and stories of practitioners [<a class="ref" href="#chao_et_al_2018">Chao et al. 2018</a>].
                        </div>
                     
                     <div class="counter"><a href="#p7">7</a></div>
                     <div class="ptext" id="p7">The embodied facet of ICH not only refers to the knowledge of the body but also that
                        which is dwelling in and enacted through the body 
                        [<a class="ref" href="#craig_et_al_2018">Craig et al. 2018</a>]. Movements and gestures are considered typical mediums to express and process mindfulness.
                        In various 
                        cases, they also mediate interactive processes, such as human-environment communication
                        and knowledge formation. Movement data usually 
                        documents the active dimension of mankind's creativity over time. Hence, as Tim Ingold's delineates, instead of targeting solely 
                        result-driven examinations, the ideal usage of such data should facilitate a new mode
                        of computation conveying the open-ended information 
                        embedded in bodily practices [<a class="ref" href="#salazar_2018">Salazar 2018</a>] [<a class="ref" href="#ingold_2011">Ingold 2011</a>].<a class="noteRef" href="#d4e322">[2]</a> In accordance with this ideal, MoCap technologies have gained 
                        increasing popularity in ICH archiving. MoCap allows data collection to be neutral,
                        simultaneous, ‚Äúbeyond the frame and 
                        within the volume‚Äù [<a class="ref" href="#delbridge_2015">Delbridge 2015</a>], thereby fostering a perceivable human presence in the virtual CH environments, as
                        
                        surveyed by Alan Chalmers et al. [<a class="ref" href="#chalmers_et_al_2021">Chalmers et al. 2021</a>].</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.2 Computational (Embodied) Archives</h2>
                     
                     <div class="counter"><a href="#p8">8</a></div>
                     <div class="ptext" id="p8">Digitization of archives allows researchers and practitioners to apply modern computational
                        methods to these collections. A recent review by 
                        Colavizza et al. has outlined multiple axes on which AI can improve archival practices by automating
                        recordkeeping processes and 
                        improving access to these collections as well as fostering new forms of digital archives
                        [<a class="ref" href="#colavizza_et_al_2021">Colavizza et al. 2021</a>]. Additionally, 
                        Lev Manovich has put forward a new way to analyse large datasets of cultural items through computational
                        means, a method he has 
                        named ‚Äúcultural analytics‚Äù [<a class="ref" href="#manovich_2020">Manovich 2020</a>]. Other researchers, such as Leonardo Impett, have focused 
                        specifically on computer vision models to better understand large visual cultural
                        collections through ‚Äúdistant 
                        viewing‚Äù [<a class="ref" href="#impett_2020b">Impett 2020b</a>].
                        </div>
                     
                     <div class="counter"><a href="#p9">9</a></div>
                     <div class="ptext" id="p9">From the perspective of experimental museology, computational approaches can help
                        improve the ‚Äúcivic value‚Äù of 
                        cultural archives [<a class="ref" href="#edmondson_2004">Edmondson 2004</a>]. Through its operationalisation, the archive is augmented on three main levels 
                        [<a class="ref" href="#kenderdine_mason_hibberd_2021">Kenderdine, Mason, and Hibberd 2021</a>]. First, it enters the social and immersive dimension of situated museological experiences.
                        
                        Second, archivists engage in new interdisciplinary exchanges with media experts and
                        the computer science community to solve many of the 
                        technical challenges cultural institutions face today. Third, new narratives can be
                        imagined by extracting novel features and revealing 
                        ‚Äúhidden structures‚Äù in the dataset [<a class="ref" href="#olesen_et_al_2016">Olesen et al. 2016</a>].
                        </div>
                     
                     <div class="counter"><a href="#p10">10</a></div>
                     <div class="ptext" id="p10">These hold especially true for audiovisual recordings, one of the primary examples
                        of collections used to capture and document embodied 
                        practices. As the foremost mnemonic records of the 21st century, these moving image
                        archives are part of our daily lives. In these last 
                        decades, major broadcasting institutions have digitised their entire collections.
                        For instance, in Switzerland, the 
                        Radio T√©l√©vision Suisse (RTS) has more than 200,000 hours of footage [<a class="ref" href="#rtsarchives_2018">RTSArchives 2018</a>], while in the 
                        United Kingdom, the British Broadcasting Corporation (BBC) preserves more than a million recorded 
                        hours [<a class="ref" href="#wright_2017">Wright 2017</a>]. In parallel, we are observing many advances in the field of visual AI dedicated
                        to human pose estimation  
                        with state-of-the-art models such as OpenPose [<a class="ref" href="#cao_et_al_2019">Cao et al. 2019</a>], OpenPifPaf [<a class="ref" href="#kreiss_bertoni_alahi_2021">Kreiss, Bertoni, and Alahi 2021</a>], and 
                        BlazePose [<a class="ref" href="#bazarevsky_et_al_2020">Bavarevsky et al. 2020</a>]. These AI algorithms can reliably extract human poses and movements from large moving
                        image 
                        collections at scale, essentially creating computational embodied archives. The wealth
                        of new data extracted by this process can then be 
                        further processed, unlocking new modes of access and novel ways of exploring these
                        archives.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.3 Human Bodies as a Way of Access</h2>
                     
                     <div class="counter"><a href="#p11">11</a></div>
                     <div class="ptext" id="p11">In the field of digital humanities, various projects have relied on such approaches
                        to analyse, explore, and better understand visual 
                        collections through human bodies. In digital art history, for instance, researchers
                        have undertaken a comprehensive analysis of large art 
                        corpora through human gestures [<a class="ref" href="#bernasconi_cetnic_impett_2023">Bernasconi, Cetiniƒá, and Impett 2023</a>] [<a class="ref" href="#impett_2020a">Impett 2020a</a>]. Similarly, in collaboration with 
                        choreographer Wayne McGregor, the Google Arts &amp; Culture Lab has developed the Living Archive, a web-based 
                        interface to navigate the collection of postures in McGregor's choreographies and create new movements in the choreographer's 
                        style [<a class="ref" href="#mcgregor_lab_2019">McGregor and Lab 2019</a>]. Furthermore, movement computing has gained popularity as an intelligent approach
                        to transform 
                        the performed cultures into ‚Äúchoreographic objects‚Äù, and it is used for analysing, visualising, and interacting 
                        with datasets of dance heritage [<a class="ref" href="#doulamis_et_al_2017">Doulamis et al. 2017</a>]. Andreas Aristidou et al. developed a Labanotation 
                        [<a class="ref" href="#guest_1977">Guest 1977</a>] based framework for transforming movements in Cypriot dances to a high-dimensional
                        feature model and constructing 
                        a deep-learned <em class="term">motion signature</em> for similarity analysis [<a class="ref" href="#aristidou_et_al_2018">Aristidou et al. 2018</a>] 
                        [<a class="ref" href="#aristidou_shamir_chrysanthou_2019">Aristidou, Shamir, and Chrysanthou 2019</a>]. Improved from <em class="term">deep signature</em> encoding 
                        [<a class="ref" href="#aristidou_shamir_chrysanthou_2019">Aristidou, Shamir, and Chrysanthou 2019</a>], Jan Sedmidubsky et al. invented a text-like representation of 3D 
                        skeleton sequences and employed the benefits of text-learning models in a more complicated
                        context [<a class="ref" href="#sedmidubsky_et_al_2020">Sedmidubsky et al. 2020</a>]. 
                        Likewise, Katerina El Raheb et al. combines posture recognition and Benesh movement notation 
                        [<a class="ref" href="#benesh_benesh_1977">Benesh and Benesh 1977</a>] to assist with multimedia annotation and interactive learning. In addition to a solely
                        choreographic 
                        focus, the Nrityakosha project synthesises a marriage of detection algorithms and
                        semantic models. The researchers related embodied 
                        attributes to concepts of Indian classical dances and, correspondingly, created a
                        specialised ontology for describing knowledge in the 
                        multimedia archive [<a class="ref" href="#mallik_chaudhury_2012">Mallik and Chaudhury 2012</a>] [<a class="ref" href="#mallik_chaudhury_ghosh_2011">Mallik, Chaudhury, and Ghosh 2011</a>]. By applying the methodology of 
                        ‚Äúdistant viewing‚Äù [<a class="ref" href="#arnold_tilton_2019">Arnold and Tilton 2019</a>] to embodied knowledge archives, Peter Broadwell 
                        and Timothy R. Tangherlini propose a computational analysis of K-pop dance, leveraging human-pose estimation
                        algorithms applied 
                        to audiovisual recordings of famous K-pop groups and idols performances [<a class="ref" href="#broadwell_tangherlini_2021">Broadwell and Tangherlini 2021</a>]. Finally, readers will 
                        find additional interesting use cases in the comprehensive review provided by Clarisse Bardiot in 
                        <cite class="title italic">Performing Arts and Digital Humanities: From Traces to Data</cite> [<a class="ref" href="#bardiot_2021">Bardiot 2021</a>].
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3 Our Computation Framework</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.1 Rationale</h2>
                     
                     <div class="counter"><a href="#p12">12</a></div>
                     <div class="ptext" id="p12">The high-level goal of our computational framework is to enrich an embodied knowledge
                        archive by extracting human skeleton data. This wealth 
                        of new data, in the form of sets of keypoints, captures both static postures (when
                        looking at individual frames) as well as dynamic motions 
                        (when adding the temporal sequences of skeletons from successive frames). By applying
                        motion extraction algorithms, we can therefore 
                        operationalise the abstract features of human poses and movements, essentially translating
                        them into vectors that can then be further 
                        processed through other computational methods. Such a process augments the archive
                        and unlocks a multitude of new scenarios, examples of which 
                        will be discussed through our two use cases. In particular, we investigate visualisations
                        of the whole archive through human poses as well 
                        as motion-based retrieval.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.2 Methodology</h2>
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">Based on the relevant literature reviewed above, we propose the following general
                        framework to augment embodied knowledge archives and create 
                        new modes of experiencing them. This method applies to both moving image and MoCap
                        archives, with the main difference in the first step: 
                        extracting human poses. Indeed, when working with videos, human pose estimation models
                        such as OpenPose [<a class="ref" href="#cao_et_al_2019">Cao et al. 2019</a>] 
                        are required to extract human postures in the form of a ‚Äúskeleton‚Äù, or a list of keypoint edge pairs, as defined in the COCO dataset 
                        [<a class="ref" href="#lin_et_al_2014">Lin et al. 2014</a>]. Depending on the model, these can be in two or three dimensions and handle single
                        or multiple persons at the 
                        same time. Furthermore, we stress that these models can sometimes fail to accurately
                        detect full bodies, especially in situations where part 
                        of the body is occluded. The number of keypoints detected can also impact how much
                        these models are able to capture features specific to a 
                        certain discipline. While MoCap data generally comes with skeleton information, post-processing,
                        normalisation, and format conversion are 
                        often necessary to produce clean and operable skeleton datasets.
                        </div>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">Once skeleton data is extracted, poses need to be normalised so that they can be meaningfully
                        compared. This involves scaling the skeleton 
                        data with respect to the image sizes, in the case of video datasets, as well as centring
                        and scaling with respect to pose size. Subsequently, 
                        feature vectors can be computed based on this normalised set of keypoints. We note
                        that these features can vary a lot from one application 
                        to another, especially if one is working with static poses (as in our first use case)
                        or with motions (as in our second use case).
                        </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">These steps result in a computational embodied archive, in which each element is a
                        human pose, as a set of normalised skeleton keypoints, 
                        linked to its corresponding frame or timestamp in the recorded item and potentially
                        augmented with additional computed features. The wealth 
                        of data extracted can then be further processed for a variety of scenarios, of which
                        we present two examples. The first use case of this 
                        paper, which addresses a collection of dance performance video recordings, will explore
                        how the whole dataset can be mapped in two dimensions 
                        through the poses dancers take in their performances. Afterwards, we present a second
                        scenario on a dataset of martial arts performances, 
                        extending to motion-based similarity retrieval.
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">4 Use Case 1: Accessing an Audiovisual Dance Performance Archive via Human Poses</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.1 The Prix de Lausanne Audiovisual Archive</h2>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">The Prix de Lausanne is a competition for young dancers held yearly in Lausanne, Switzerland since 
                        1973<a class="noteRef" href="#d4e541">[3]</a>. Nearly each year of the competition 
                        has been recorded and digitised, resulting in a rich dataset of 1,445 mp4 videos of
                        individual dance performances across forty-two years, 
                        along with information on the dancers and their chosen performances. This metadata,
                        although valuable, does not document the embodied 
                        elements of dance performances, limiting how one can access such an archive. With
                        our solution, we augmented the available data using 
                        computational methods and employed the extracted features to map the whole dataset
                        in two dimensions through the human poses embedded in the 
                        material, revealing its hidden structures and creating new connections between similar
                        poses.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.2 Building Feature Vectors for the Dancers' Poses</h2>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">We extracted human skeleton data using the MediaPipe BlazePose GHUM 3D model [<a class="ref" href="#bazarevsky_et_al_2020">Bavarevsky et al. 2020</a>], which was mainly chosen 
                        for its out-of-the-box robustness and rapidity of inference for single human estimation.
                        In the case of the Prix de Lausanne, 
                        since the videos record one participant at a time on stage, we found this model to
                        be the most suitable. We also observed only occasional 
                        cases of occlusions when the dancers are wearing particularly large costumes. The
                        BlazePose algorithm outputs skeleton data as a set of 33 
                        keypoints in three-dimensional space, normalised to the image size. Compared to models
                        trained on the COCO dataset, MediaPipe algorithm 
                        detects more keypoints, including the index fingers and index toes of the two hands
                        and the two feet, allowing us to compute the angles of 
                        the wrists and ankles. Although the algorithm was not specifically trained on dance
                        recordings, we believe these additional features help to 
                        better capture specific poses of dance performances, where even the positions of the
                        hands and feet are crucial. Videos were processed to 
                        extract one pose every five seconds, a cautious design decision to avoid over-collecting
                        similar poses merely due to temporal closeness. We do 
                        note however this is potentially not the best solution to ensure a full coverage of
                        the diversity of poses, and future research in this 
                        direction could improve the results presented even further. It is worth noting that
                        not all frames necessarily had a detected pose, since in 
                        some cases the dancer was too far away from the camera, or because some videos were
                        not properly cut to the exact dance performance and thus 
                        had irrelevant frames at the beginning and/or the end. We measured the rate of detection
                        by taking the mean of the ratio of poses detected 
                        over the number of frames per video, obtaining a mean detection rate of 76.80% across
                        all the videos. The pose extraction process resulted in 
                        a total of 27,672 poses.
                        </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">Once these poses had been extracted, we normalised them following a two-step procedure.
                        We first subtracted the pose centre, defined as the 
                        mean point between the left and right hips. We then scaled the pose by its size, defined
                        as the maximum between the torso size and the maximum 
                        distance between the pose centre and any keypoint, where the torso size was computed
                        as the distance between the centre of the shoulders and 
                        the pose centre (the centre of the hips). The procedure ensures meaningful comparison
                        between the poses by normalising them. An intuitive 
                        sample of normalised poses with their corresponding frames is provided in Figure 1.
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="Random sample of frames from the  videos displayed alongside the corresponding human pose extracted. The &#xA;                      human pose is represented by its skeleton of keypoints. Only the main keypoints are displayed for ease of visualisation (the nose, the &#xA;                      shoulders, the elbows, the wrists, the hips, the knees, and the ankles)." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†1.¬†</div>Sample of normalised poses with corresponding frames. Only the main keypoints are
                           displayed for ease of visualization (the nose, the 
                           shoulders, the elbows, the wrists, the hips, the knees, and the ankles).</div>
                     </div>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">Subsequently, two feature vectors were built and tested. For the first, we simply
                        took the vector of the keypoints and flattened it, 
                        resulting in a 99-dimensional vector. For the second, we built a custom feature vector,
                        taking a combination of pose lengths (distances 
                        between two keypoints) and pose angles (angles defined by three keypoints). Table
                        1 shows the features computed.
                        </div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Category</td>
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Feature</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Pose Lengths<br />(Distances Between Joints)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Left elbow ‚Äî Right elbow<br />Left wrist ‚Äî Right wrist<br />Left knee ‚Äî Right knee<br />Left ankle ‚Äî Right ankle
                                 <br />Left hip ‚Äî Left wrist<br />Right hip ‚Äî Right wrist<br />Left hip ‚Äî Left ankle<br />Right hip ‚Äî Right ankle<br />
                                 Left ankle ‚Äî Left wrist<br />Right ankle ‚Äî Right wrist<br />Left shoulder ‚Äî Right ankle<br />Right shoulder ‚Äî Left ankle<br />
                                 Left wrist ‚Äî Right ankle<br />Right wrist ‚Äî Left ankle
                                 </td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Pose Angles<br />(Defined by Three keypoints)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Left hip ‚Äî Left knee ‚Äî Left ankle<br />Right hip ‚Äî Right knee ‚Äî Right ankle<br />Left shoulder ‚Äî Left elbow ‚Äî Left wrist<br />
                                 Right shoulder ‚Äî Right elbow ‚Äî Right wrist<br />Left hip ‚Äî Left shoulder ‚Äî Left elbow<br />Right hip ‚Äî Right shoulder ‚Äî Right 
                                 elbow<br />Left shoulder ‚Äî Left hip ‚Äî Left knee<br />Right shoulder ‚Äî Right hip ‚Äî Right knee<br />Left elbow ‚Äî Left wrist ‚Äî Left 
                                 index finger<br />Right elbow ‚Äî Right wrist ‚Äî Right index finger<br />Left knee ‚Äî Left ankle ‚Äî Left index toe<br />Right knee ‚Äî Right 
                                 ankle ‚Äî Right index toe
                                 </td>
                              </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table¬†1.¬†</div>A list of custom features used to represent dance poses in the Prix de Laussanne archive.</div>
                     </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.3 Mapping the Prix de Lausanne Archive</h2>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20">One of the fundamental applications unlocked by computational archives is the possibility
                        to visualise whole collections. To this end, we 
                        apply dimensionality reduction (DR) algorithms to embed the high-dimensional feature
                        spaces in two or three dimensions, in order to create a 
                        novel way of visualising and navigating the whole archive. We tested both standard
                        DR methods, such as Principal Component Analysis (PCA), 
                        and t-distributed Stochastic Neighbour Embedding (tSNE), as well as a more recent
                        approach, Uniform Manifold Approximation and Projection 
                        (UMAP). We leveraged scikit-learn implementations for the two first algorithms and
                        the official UMAP Python implementation for the latter 
                        [<a class="ref" href="#pedregosa_et_al_2011">Pedregosa et al. 2011</a>] [<a class="ref" href="#halko_martinsson_tropp_2011">Halko, Martinsson, and Tropp 2011</a>] [<a class="ref" href="#martinsson_rokhlin_tvgert_2011">Martinsson, Rokhlin, and Tvgert 2011</a>] 
                        [<a class="ref" href="#van-der-maaten_hinton_2008">van der Maaten and Hinton 2008</a>] [<a class="ref" href="#mcinnes_et_al_2018">McInnes et al. 2018</a>]. For tSNE and UMAP we compared using cosine and Euclidean 
                        metrics as the distance measure to compute the projection. We therefore tested five
                        options on 27,672 poses, with both sets of feature 
                        vectors. Figure 2 shows the resulting embeddings in two dimensions.
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="Scatter plots of the embeddings of 27,672 poses in two dimensions, produced by different Dimensionality Reduction algorithms with &#xA;                      different parameters (from left to right, the algorithms used are PCA, tSNE with cosine metric, tSNE with Euclidean metric, UMAP with cosine &#xA;                      metric, and UMAP with Euclidean metric). The top row (blue) uses the flattened vectors of keypoints, while the bottom row (red) uses the &#xA;                      custom features computed." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†2.¬†</div>Embeddings of 27,672 poses in two dimensions. The top row (blue) uses the flattened
                           vectors of keypoints, while the bottom row (red) 
                           uses the custom features computed. From left to right, the algorithms used are PCA,
                           tSNE (cosine metric), tSNE (Euclidean metric), UMAP 
                           (cosine metric), and UMAP (Euclidean metric).</div>
                     </div>
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">We can immediately observe that PCA performs poorly on this dataset, as expected due
                        to its linear nature that cannot preserve much variance 
                        with just two dimensions (only 49% with the flattened vectors of keypoints and 45%
                        with the custom features). Both tSNE and UMAP, however, 
                        give much more interesting results, where structures in the dataset can be observed.
                        Since we did not observe meaningful differences between 
                        the two sets of feature vectors used, we pursued further examinations with the flattened
                        vectors of keypoints to exploit the heuristic 
                        quality of the algorithms independent of the choice of features. Therefore, we set
                        out to investigate tSNE and UMAP behaviours in more depth 
                        by studying the effects of their hyperparameters. For tSNE, we observed how the embedding
                        in two dimensions behaves for different 
                        perplexities (\(œµ\)), the main parameter for this algorithm, related to the number of 
                        neighbours considered for any given point during the computation. For UMAP, we investigated
                        the number of nearest neighbours 
                        (\(ùëõ\)), controlling how UMAP balances local versus global structure in the data, as well
                        as 
                        the minimum distance (\(ùëë_{min}\)) the algorithm is allowed to pack points together in the 
                        low-dimensional space. We used the Euclidean distance as it seemed to produce slightly
                        clearer results. Figure 3 shows how tSNE behaves for 
                        different values of \(œµ\), while Figure 4 displays the results of UMAP with varying 
                        \(ùëõ\) and \(ùëë_{min}\) values.
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" style="" alt="Scatter plots of the embeddings of 27,672 poses in two dimensions showing the effect of the main parameter perplexity on the tSNE &#xA;                      algorithm (on the flattened vectors of keypoints with Euclidean distance). The figure shows that lower perplexities result in a more granular &#xA;                      embedding, losing much of the global structure." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†3.¬†</div>Effect of the perplexity (œµ) on the tSNE embeddings of 27,672 poses (flattened vectors
                           of keypoints, Euclidean distance). Lower values 
                           of œµ result in a more granular embedding.</div>
                     </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" style="" alt="Scatter plots of the embeddings of 27,672 poses in two dimensions showing the effect of the main parameters number of neighbours and &#xA;                      minimum distance on the UMAP algorithm (on the flattened vectors of keypoints with Euclidean distance). The figure shows that lower minimum &#xA;                      distances result in a more packed embedding, but global structures appear to be quite stable across different values for the number of &#xA;                      neighbours." /></a></div>              
                        
                        <div class="caption">
                           <div class="label">Figure¬†4.¬†</div>Effect of the number of neighbours ùëõ and the minimum distance ùëë<span class="hi subscript">ùëöùëñùëõ</span> on the UMAP embeddings of 27,672 poses 
                           (flattened vectors of keypoints, Euclidean distance). Lower values of ùëë<span class="hi subscript">ùëöùëñùëõ</span> result in a more packed embedding, but 
                           global structures appear to be quite stable.</div>
                     </div>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">To conclude our exploration of the Prix de Lausanne moving image collection, we set out to produce a map of the human poses 
                        embedded in the archive. Following the outlined procedure, we extracted the keypoints,
                        computed the corresponding feature vectors, and 
                        applied UMAP (with Euclidean distance, \(ùëõ\) = 50, 
                        \(ùëë_{min}\) = 0.5) on the flattened vectors of keypoints to create Figure 5, where we display
                        
                        the corresponding pose at each point. For the sake of visibility, we only show a portion
                        of all the poses extracted, resulting in a map of 
                        2,768 poses. Through this visualisation, we can verify the effectiveness of our approach
                        in grouping together similar poses, thus unlocking a 
                        new way of seeing an embodied knowledge archive.
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="Visualisation of 2,768 dancers' poses placed in a 2D plot based on the 2D embeddings of the flattened vectors of keypoints, obtained &#xA;                      with the UMAP algorithm (Euclidean distance, ùëõ = 50, ùëë = 0.5)." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†5.¬†</div>Map of 2,768 dancers' poses, based on the 2D embeddings of the vectors of flattened
                           keypoints with UMAP (Euclidean distance, 
                           ùëõ = 50, ùëë<span class="hi subscript">ùëöùëñùëõ</span> = 0.5).</div>
                     </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5 Use Case 2: Accessing Martial Art Recordings through Motion Traits</h1>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">In the first use case, the extracted human skeletons were been treated as static postures,
                     thus operating on individual frames of the moving 
                     image recordings. This is suitable for the purpose of visualising the whole archive
                     in two dimensions because we can draw these skeletons, using 
                     them as glyphs representing frames in the collection to create compelling images,
                     as in Figure 5. However, one might argue that embodied knowledge 
                     archives are not only about human postures but also, if not more so, about human motions.
                     This second use case thus investigates such an approach, 
                     proposing an innovative motion-based retrieval mode. In the context of the Hong Kong Martial Arts Living Archive 
                     (HKMALA) project, a retrieval system allows users to select a specific motion within the
                     collection (by choosing the start and end 
                     frames of a video sequence, for instance) and returns similar motions in the archive.
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.1 Hong Kong Martial Arts Living Archive</h2>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">Traditional martial arts are considered knowledge treasures of humanities sustained
                        through generations by diverse ethnic groups. Among the 
                        genres, Southern Chinese martial arts (SCMA) is arguably one of the most prolonged
                        systems embodying Chinese mind-body ideologies, yet it is 
                        now facing challenges in knowledge transmission and the risk of being lost. In preserving
                        the living heritage of SCMA, the 
                        Hong Kong Martial Arts Living Archive inspects a comprehensive set of digitisation tools to capture the martial practices,
                        
                        with a chief focus on motion capturing form sequence performances, or <em class="term">taolu</em>. Since its origination in 2012, HKMALA 
                        has built a 4D motion archive spanning over 20 styles and 130 sets of empty-hand and
                        weapon sequences.<a class="noteRef" href="#d4e801">[4]</a> The archive also collates various historical and reconstituted 
                        materials documenting martial cultures, encompassing rituals, traditions, armaments,
                        and objects preserved in Hong Kong 
                        [<a class="ref" href="#chao_et_al_2018">Chao et al. 2018</a>].
                        </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">The HKMALA's exceptional volumes hold the promise to enable various scholarly inquiries. However,
                        its current archival 
                        organisation is based on a manual catalogue that merely indicates how the content
                        has been curated for exhibitions. Additionally, the data 
                        consists of multiple modalities, such as texts, images, videos, and MoCap data, thereby
                        impeding public access and dissemination at scale. 
                        There is a clear need for an effective yet meaningful way of data access.
                        </div>
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">Addressing the aforementioned challenges, we propose devising a motion-based retrieval
                        framework that leverages machine learning to encode 
                        motion-wise information in multimodal recordings. The approach facilitates content
                        retrieval through embodied cues, operating seamlessly 
                        across MoCap and moving image data formats. In the following paragraphs, we aim to
                        provide an overview of the technical procedures. For a 
                        more comprehensive understanding of the implementation process, readers are invited
                        to refer to the descriptions in 
                        [<a class="ref" href="#hou_seydou_kenderdine_2023">Hou, Seydou, and Kenderdine 2023</a>].
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.2 Encoding and Retrieving Martial Arts Movements</h2>
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">Chinese martial arts are renowned for their strong emphasis on form training, which
                        utilizes codified movement sequences to practise 
                        fundamental techniques and technique combinations. Less known, yet equally important,
                        is the stress on combative tactics that involve  
                        mindful operation of whole-body movement. Thus, training is imparted through a combination
                        of freeform combat and integration into 
                        <em class="term">taolu</em> practices [<a class="ref" href="#ma_2003">Ma 2003</a>]. Given that assessment methods in Chinese martial arts typically consider visually
                        
                        discernible characteristics and mechanical parameters, we applied the same configurations
                        to model martial arts movements. Designed in 
                        compatibility with Laban metrics for movement analysis [<a class="ref" href="#guest_1977">Guest 1977</a>], the approach aims to articulate both qualitative and 
                        quantitative qualities regarding the technical stances, ballistic activities, and
                        reactions to the opponent or surroundings, as shown in 
                        Table 1 in [<a class="ref" href="#hou_seydou_kenderdine_2023">Hou, Seydou, and Kenderdine 2023</a>].
                        </div>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">The process began with the extraction of 3D human skeletons from MoCap data, or the
                        extraction of 2D poses from video recordings, followed 
                        by depth estimation to reconstruct the 3D structure. The coordinates and joint angles
                        were then computed from raw skeletons into basic 
                        kinematic metrics, which were normalized for further processing. Feature augmentation
                        was employed to enrich the kinematic feature set, 
                        incorporating metrics that reflect linear and angular motion characteristics, along
                        with the transition probabilities of motion time 
                        series. Finally, the top 50 features with the highest variance were selected using
                        the variance threshold approach and used to train a 
                        discriminative neural network.
                        </div>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">Holding a hypothesis that the characteristics of a long motion sequence can be represented
                        by its segmentations, our encoding method was 
                        aimed to pinpoint representative motion units within the sequence. Firstly, we gathered
                        high-dimensional features from equally segmented 
                        media data and employed deep learning to train a latent space, embedding them into
                        a vector space where semantically similar dimensions are 
                        clustered together while dissimilar ones are set apart. Subsequently, K-means clustering
                        was applied to identify cluster centres as 
                        representatives. To create a descriptive and compact scheme for efficient retrieval,
                        we encoded the inverse document frequency (IDF) of 
                        each representative into a normalised histogram, or <em class="term">metamotion</em>, to represent the distribution of archetypal qualities within a 
                        motion sequence.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5.3 A Motion-Based Retrieval Engine</h2>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">To facilitate the envisioned scenario of exploratory archival browsing, a retrieval
                        system was deployed to allow querying archival 
                        sequences containing similar movements. The system integrates different retrieval
                        algorithms, including locality-sensitive hashing (LSH) and 
                        Ball-tree methods, which prove effective in achieving optimal retrieval efficiency,
                        outperforming the baseline approach in existing 
                        research.<a class="noteRef" href="#d4e850">[5]</a>
                        </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">On the user end, an interactive search engine was deployed to enable motion-based
                        data exploration across formats. An intuitive search case 
                        is illustrated in Figure 6, showcasing a cross-model retrieval result from a hybrid
                        of videos and MoCap data. Additionally, Figure 7 
                        demonstrates the initial development of the archival browser, presenting the retrieved
                        items with ontological descriptions for the 
                        conceptual entities associated with the query item. This design aims to improve data
                        explainability and interoperability for future reuse, 
                        with ontology annotations sourced from the scholarly work of The Martial Art Ontology (MAon).<a class="noteRef" href="#d4e862">[6]</a>
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="Top-5 similar sequences retrieved by a query featuring a staff performance. On the left are the top-5 MoCap sequences retrieved, &#xA;                        and on the right are the top-5 video retrieval results. The results all exhibit certain similarities in movement, regardless of the use of &#xA;                        a weapon." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†6.¬†</div>An example of the top-5 similar sequences retrieved by a query using a MoCap dataset
                           (left) and a video dataset (right) 
                           [<a class="ref" href="#hou_seydou_kenderdine_2023">Hou, Seydou, and Kenderdine 2023</a>].</div>
                     </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" style="" alt="An illustration of a configurable search interface conducting motion search. The query involves a technique with a whipping punch &#xA;                        and turning stance, supported by ontological representations of technical concepts. Five media sequences containing similar movements are &#xA;                        retrieved." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†7.¬†</div>Illustration of a motion search example supplemented with ontological representations
                           of the concepts of techniques.</div>
                     </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">6 Discussion</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.1 Visual AI Toward ‚ÄúAugmented Archives‚Äù</h2>
                     
                     <div class="counter"><a href="#p32">32</a></div>
                     <div class="ptext" id="p32">At cultural heritage archives, including born-digital archives, metadata is commonly
                        organised according to an archive-specific curatorial 
                        structure with topical domain-language descriptions of archival items. For instance,
                        both the Prix de Lausanne and 
                        HKMALA collections warrant a series of textual tags describing the contents of dance and
                        martial art performances, as well as 
                        information about performers. Such documentation, although useful, entails a mode
                        of interpretation and access rather limited to expert users. 
                        One needs to know what dance performances such as <cite class="title italic">The Giselle</cite> or <cite class="title italic">The Nutcracker</cite> are 
                        to look for them in the Prix de Laussane archive, and one must understand that martial arts are comprised of a series of 
                        <em class="term">taolu</em> in order to understand HKMALA collections. Furthermore, the visual and embodied features represented in these 
                        collections are difficult to capture verbally. Models that describe movement, such
                        as those based on notation systems like Labanotation, have 
                        shown promise. However, such models typically necessitate the alignment between the
                        movement and the notation language, as well as an 
                        advanced understanding of the subject matter by the annotator, in order to effectively
                        encode and decode the knowledge behind movements. As 
                        a result, it is very difficult, if not impossible, to capture embodied knowledge features
                        using literal formats or in an easily accessible 
                        way. Therefore, this kind of metadata, although necessary and useful in many domains,
                        is not suitable for access by the general public. It 
                        does not support the ‚Äúcasual modes of access‚Äù that are fundamental for museum-goers and general audiences 
                        [<a class="ref" href="#whitelaw_2015">Whitelaw 2015</a>].
                        </div>
                     
                     <div class="counter"><a href="#p33">33</a></div>
                     <div class="ptext" id="p33">In this article, we have introduced a computational framework aimed at enhancing cultural
                        archives by enriching metadata with embodied 
                        knowledge descriptors derived from audiovisual and multimodal materials. Utilising
                        computer vision models, we extract and transform human 
                        poses and skeleton features into meaningful data features, which potentially foster
                        a range of usage scenarios. At the archival level, 
                        curators can operationalise their collections as ‚Äúdata sources‚Äù [<a class="ref" href="#kenderdine_mason_hibberd_2021">Kenderdine, Mason, and Hibberd 2021</a>], 
                        envisioning new ways for querying, annotating, and analysing whole datasets to forge
                        novel narrative paradigms. In parallel, the enriched 
                        metadata improves data accessibility while the diversified query channels improve
                        data findability, augmenting the archives' compliance with 
                        FAIR (findability, accessibility, interoperability, and reusability) data principles,
                        regardless of variations in language, topic, and data 
                        format. Furthermore, our approach demonstrates the potential for interpreting data
                        traits within and across cultural contexts. This not only 
                        contributes to the reusability of archival content but also resonates with the findings
                        of Colavizza et al., which highlight the growing role 
                        of AI in archival practices, particularly in the ‚Äúautomation of recordkeeping processes‚Äù and the improvement of 
                        procedures for ‚Äúorganising and accessing archives‚Äù [<a class="ref" href="#colavizza_et_al_2021">Colavizza et al. 2021</a>]. Our experimentation, as 
                        exemplified through two distinctive heritage archives, represents an innovative step
                        forward with a focus on embodied knowledge, allowing 
                        queries through more perceptive than quantitative channels, which we believe is more
                        natural and humanistic.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.2 Archive Navigation and Serendipitous Discovery</h2>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">The operationalisation of embodied knowledge archives through visual AI, resulting
                        in ‚Äúaugmented archives‚Äù, provides a vast array of data 
                        that can further be processed in order to create new modes of access adapted for general
                        audiences. Following the principles of 
                        ‚Äúgenerous interfaces‚Äù [<a class="ref" href="#whitelaw_2015">Whitelaw 2015</a>], our method supports explorative behaviours, encouraging a 
                        new paradigm of ‚Äúseeking the unknown‚Äù [<a class="ref" href="#winters_prescott_2019">Winters and Prescott 2019</a>]. By laying out the full archive 
                        through the lens of easily understood concepts, such as dancers' postures, users can
                        more readily comprehend the archive without specific 
                        knowledge about the topic. Users do not need to have a specific goal in mind or be
                        looking for something in particular. Instead, they can 
                        wander around, browsing like an ‚Äúinformation flaneur‚Äù and enjoying the Prix de Lausanne archive in a 
                        way that traditional modes of access, based on querying metadata or skimming grids
                        and lists of information, could not 
                        offer [<a class="ref" href="#dork_carpendale_williamson_2011">D√∂rk, Carpendale, and Williamson 2011</a>]. Furthermore, each pose links to a timestamp in a dance performance, grouping together
                        
                        similar poses in the low-dimensional space to create serendipitous discoveries. Indeed,
                        this mode of access rewards users for simply browsing 
                        the collection and stumbling upon new performances as they move from pose to pose
                        on the map. Figure 8 explicates this process by showcasing 
                        poses similar to an input pose with the corresponding video frames.
                        </div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" style="" alt="Two examples of searching for similar poses in the  videos. In each row, an input frame/pose pair is &#xA;                      matched with its top-5 nearest neighbours (displaying both the original frames and the corresponding extracted human poses). The figure &#xA;                      demonstrates how connections between different performers are discovered." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure¬†8.¬†</div>Examples of similar poses in the Prix de Lausanne archive (top 5 matches). Simplified poses and corresponding video frames 
                           are displayed. Notice how connections between different performers are discovered.</div>
                     </div>
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">This new mode of access is enhanced by dimensionality reduction algorithms. However,
                        one must take care in deciding which algorithm to employ 
                        and with what parameters. In this work, we have analysed Principal Component Analysis
                        (PCA), t-distributed Stochastic Neighbour Embedding 
                        (tSNE), and Uniform Manifold Approximation and Projection (UMAP). PCA is a well-established
                        dimensionality reduction technique, but, due to 
                        its linear nature, it often fails to properly capture a real-life dataset with only
                        a handful of dimensions. This is clearly confirmed in 
                        Figure 2, where all embeddings generated with tSNE and UMAP show much more interesting
                        structures without creating a large clump of points 
                        like PCA does. Regarding the choice of feature vectors used, as mentioned, we did
                        not observe noticeable differences between the flattened 
                        vectors of keypoints and the custom features computed. We hypothesise that the custom
                        features (reported in Table 1) are not more 
                        discriminative than the base keypoints. Although in subsequent analyses we decided
                        to use the flattened vectors of keypoints, we believe 
                        this offers us an opportunity for more collaboration, since involving dance experts
                        could help us craft a more precise and adequate set of 
                        features, based on Labanotation, for instance [<a class="ref" href="#guest_1977">Guest 1977</a>]. To gain a deeper understanding of these algorithms, we have 
                        investigated the effect of parameters on tSNE and UMAP.
                        </div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">Figure 3 shows that increasing the perplexity yields very different layouts, with
                        lower values displaying numerous small groupings of items 
                        while higher values reveal more large-scale structures. This is in accordance with
                        expectations, since with higher perplexities tSNE considers 
                        more points when computing the vicinity of each item, thus better capturing global
                        structures. Surprisingly, in Figure 4, when increasing the 
                        number of neighbours (\(ùëõ\)) considered with UMAP, the outputs appear to be much more stable, 
                        with global structures already visible with \(ùëõ\) = 50. It is in this case that the second 
                        parameter, \(ùëë_{min}\), affects more the results, yielding more sprayed-out mappings the 
                        higher it is (since the minimum distance directly controls how packed the points can
                        be in the low dimensional space). These results indicate 
                        that when creating archive visualisations, Like the map of poses in Figure 5, for
                        instance, higher 
                        \(ùëë_{min}\) might be more suitable to avoid overlaps between similar poses. However, if one 
                        were to apply clustering algorithms on these embeddings to group together similar
                        poses, the more fine-grained and packed structures obtained 
                        with lower \(ùëë_{min}\) would potentially yield better results. Therefore, we argue that it is 
                        not a matter of which embedding is better overall but rather which embedding is better
                        adapted to the specific narrative or mode of access 
                        sought.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.3 Limitations and Future Work Directions</h2>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Although we believe our computational framework to be well thought out, there are
                        still some limitations we would like to highlight, hopefully 
                        to later address them.
                        </div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">First, human pose extraction from monocular videos is never perfect. Human bodies
                        can be incorrectly estimated or missed due to the influence 
                        of occlusion, monotonous colour patterns, or camera angles, to name a few. Indeed,
                        through our ‚Äúnaive‚Äù approach to the Prix de 
                        Lausanne archive, we only achieved a detection rate of 76.80%.<a class="noteRef" href="#d4e1028">[7]</a> Furthermore, by checking a sample 
                        of skeletons extracted with the corresponding frames, as in Figure 8, we noticed that
                        keypoints were not always correctly extracted. One 
                        possible reason is that pose estimation was done systematically every five seconds
                        without a measure of quality or significance. Nevertheless, 
                        our approach proves sufficient for the use case described, as it still produces enough
                        data to map the whole archive properly. Further 
                        developments could yield more interesting and precise results, for instance, by extracting
                        skeleton data on a finer temporal granularity and 
                        then filtering only to keep the ‚Äúbetter‚Äù poses.<a class="noteRef" href="#d4e1035">[8]</a>
                        </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Second, the feature vectors computed for the Prix de Lausanne archive are somewhat naive. Taking the lesson from the HKMALA 
                        feature computing, collaboration with dance experts could facilitate the design of
                        more relevant, dance-specific features with precise 
                        measurements able to better capture and compare human bodies during dance performances.
                        Nonetheless, our results demonstrate that even naive 
                        methods can produce new modes of access to embodied knowledge archives. Thus, we are
                        confident that our method can be generalised for use by 
                        other archives and diverse embodied disciplines.
                        </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">Lastly, unlike standard computer vision challenges, it is difficult but necessary
                        to quantify what makes a ‚Äúgood‚Äù estimation of poses or 
                        movement segments in the archival context, yet the standard varies across people and
                        cultural themes. To this end, we resorted to evaluating the 
                        distribution and quality of embeddings, supplemented with an ad-hoc expert review
                        of small sampling sets. Future improvement is suggested to 
                        integrate expert review dynamically along with the model training process, such as
                        by intaking expert judgement as a score and feeding it back 
                        to the model, so as to enable a human-in-the-loop machine-learning process.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.4 Outlook: Towards a New Archival Experience</h2>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">The research presented in this paper was conducted within the larger context of ‚Äúcomputational museology‚Äù  
                        [<a class="ref" href="#kenderdine_mason_hibberd_2021">Kenderdine, Mason, and Hibberd 2021</a>], a discipline aimed at developing new modes of access to cultural heritage through
                        
                        computational approaches, specifically designed for situated experiences in museum
                        settings. To this end, further work will rely on 
                        leveraging the findings highlighted in this paper to create interactive installations
                        to access these embodied knowledge archives. We contend 
                        that the implementations presented in this paper establish a robust foundation for
                        developing interactive modes of access tailored to meet the 
                        requirements of casual users within situated experiences. In particular, two main
                        directions will be pursued. First, based on the fundamental 
                        concept of placing visitors ‚Äúinside‚Äù the archive [<a class="ref" href="#shen_et_al_2019">Shen et al. 2019</a>], rather than looking at it on a 
                        plain screen, immersive environments (IEs) will be employed to create situated experiences
                        in which users can navigate the whole archive. In 
                        practice, dimensionality reduction techniques will be employed to compute a mapping
                        in two- or three-dimensions in order to generate a virtual 
                        world in which each point or glyph represents a pose (and its corresponding frame
                        or clip). This virtual cloud of human bodies will then be 
                        displayed in large IEs, allowing visitors to freely navigate and discover the archive.
                        Applying immersive technologies to the mediation of 
                        cultural heritage already has some interesting applications, particularly in the context
                        of archaeological enquiry 
                        [<a class="ref" href="#sciuto_et_al_2023">Sciuto et al. 2023</a>], and we believe such technologies can also serve a purpose for embodied knowledge
                        archives.
                        </div>
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">Second, an interactive retrieval system can be developed, either based on pose or
                        motion similarity. Users could strike a pose or perform a 
                        certain movement, detected in real-time with motion-tracking solutions, and the system
                        would retrieve relevant clips from the collection. Such 
                        an experience would yield an interesting performative aspect that sees the visitor
                        as a co-creator with the machine, essentially transforming 
                        them into a variable of the generative system, and other people around them as an
                        audience witnessing the exchange in a 
                        ‚Äúthird-person's perspective‚Äù [<a class="ref" href="#mul_masson_2018">Mul and Masson 2018</a>].
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">7 Conclusion</h1>
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">Embodied knowledge archives are an integral part of our living heritage, and they
                     contain important aspects of our cultures. Yet, it is still 
                     difficult to explore embodied knowledge archives, especially for more casual audiences
                     that lack specific knowledge on the topic of the 
                     collection. To answer this challenge, we have proposed in this work a computational
                     framework that leverages motion extraction AI to augment these 
                     datasets with a wealth of rich data, enabling new modes of analysis and access.
                     </div>
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">The proposed method was applied to two embodied knowledge archives, containing dance
                     and martial arts performances, respectively, which showcases 
                     its application to multimedia content and diverse access scenarios. In the former
                     example, we devised a method to visualise a whole collection in 
                     two dimensions through the human poses embedded in the archival materials, revealing
                     their structure and fostering serendipitous discoveries. In 
                     the latter, we extended our method to motion encoding from static poses and devised
                     a motion-based query system, offering a new way to search an 
                     embodied knowledge archive. These scenarios showcase how our computational framework
                     can operationalise this type of collection and unlock a 
                     variety of new modes of access suitable for non-expert audiences.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">The authors are grateful to the Prix de Lausanne for the opportunity to work on their audiovisual archive, as part of the 
                     SNSF's Sinergia grant, <cite class="title italic">Narratives from the Long Tail: Transforming Access to Audiovisual Archives</cite> 
                     (CRSII5_198632).
                     </div>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">The Hong Kong Martial Arts Living Archive is a longitudinal research collaboration between the International Guoshu 
                     Association, the City University of Hong Kong, and the Laboratory for Experimental Museology (eM+), EPFL.
                     </div>
                  </div>
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e253"><span class="noteRef lang en">[1]¬†By <em class="word">transformative potential</em>, we mean the 
                     potential of AI to augment the data available by extracting new features that can
                     open innovative venues for accessing the archive.</span></div>
               <div class="endnote" id="d4e322"><span class="noteRef lang en">[2]¬†Tim Ingold argues that some 
                     movement is ‚Äúautomatic and rhythmically responsive‚Äù to its surroundings and ‚Äúalong multiple 
                     pathways of sensory participation‚Äù [<a class="ref" href="#ingold_2011">Ingold 2011</a>].</span></div>
               <div class="endnote" id="d4e541"><span class="noteRef lang en">[3]¬†See <a href="https://www.prixdelausanne.org/" onclick="window.open('https://www.prixdelausanne.org/'); return false" class="ref">https://www.prixdelausanne.org</a></span></div>
               <div class="endnote" id="d4e801"><span class="noteRef lang en">[4]¬†The concept of four-dimensional 
                     (4D) space denotes a dynamic 3D space moving through time.</span></div>
               <div class="endnote" id="d4e850"><span class="noteRef lang en">[5]¬†See [<a class="ref" href="#hou_seydou_kenderdine_2023">Hou, Seydou, and Kenderdine 2023</a>,¬†section 4].</span></div>
               <div class="endnote" id="d4e862"><span class="noteRef lang en">[6]¬†See 
                     The Martial Art Ontology (MAon), v1.1, 
                     <a href="https://purl.org/maont/techCorpus" onclick="window.open('https://purl.org/maont/techCorpus'); return false" class="ref">https://purl.org/maont/techCorpus</a>.</span></div>
               <div class="endnote" id="d4e1028"><span class="noteRef lang en">[7]¬†A ‚Äúnaive‚Äù approach in pattern recognition implies a 
                     straightforward and easy-to-implement algorithm that finds all matching occurrences
                     of a given input.</span></div>
               <div class="endnote" id="d4e1035"><span class="noteRef lang en">[8]¬†One would first need to define what constitutes a ‚Äúbetter‚Äù pose, however.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="aristidou_et_al_2018"><!-- close -->Aristidou et al. 2018</span>¬†Artistidou, A. (2018) ‚ÄúDeep motifs and motion signatures‚Äù, 
                  <cite class="title italic">Transactions on Graphics</cite>, 37(6), pp. 1‚Äì13.
                  </div>
               <div class="bibl"><span class="ref" id="aristidou_shamir_chrysanthou_2019"><!-- close -->Aristidou, Shamir, and Chrysanthou 2019</span>¬†Aristidou, A., Shamir, A., and Chrysanthou, Y. (2019) 
                  ‚ÄúDigital dance ethnography: Organizing large dance collections‚Äù, <cite class="title italic">Journal on Computing and Cultural 
                     Heritage</cite>, 12(4), pp. 1‚Äì27.
                  </div>
               <div class="bibl"><span class="ref" id="arnold_tilton_2019"><!-- close -->Arnold and Tilton 2019</span>¬†Arnold, T. and Tilton, L. (2019) ‚ÄúDistant viewing: Analyzing large 
                  visual corpora‚Äù, <cite class="title italic">Digital Scholarship in the Humanities</cite>, 34(Supplement 1), pp. i3-i16.
                  </div>
               <div class="bibl"><span class="ref" id="aske_giardinetti_2023"><!-- close -->Aske and Giardinetti 2023</span>¬†Aske, K. and Giardinetti, M. (2023) ‚Äú(Mis)matching metadata: 
                  Improving accessibility in digital visual archives through the EyCon project‚Äù, <cite class="title italic">Journal on Computing and Cultural 
                     Heritage</cite>, 16(4).
                  </div>
               <div class="bibl"><span class="ref" id="bardiot_2021"><!-- close -->Bardiot 2021</span>¬†Bardiot, C. (2021) <cite class="title italic">Performing arts and digital humanities: From traces to 
                     data</cite>. Hoboken, NJ: John Wiley &amp; Sons.
                  </div>
               <div class="bibl"><span class="ref" id="bazarevsky_et_al_2020"><!-- close -->Bavarevsky et al. 2020</span>¬†Bazarevsky, V. et al. (2020) ‚ÄúBlazepose: On-device real-time 
                  body pose tracking‚Äù, <cite class="title italic">arXiv</cite>. <a href="https://arxiv.org/abs/2006.10204" onclick="window.open('https://arxiv.org/abs/2006.10204'); return false" class="ref">https://arxiv.org/abs/2006.10204</a>.
                  </div>
               <div class="bibl"><span class="ref" id="benesh_benesh_1977"><!-- close -->Benesh and Benesh 1977</span>¬†Benesh, R. and Benesh, J. (1977) <cite class="title italic">Reading dance: The birth of 
                     choreology</cite>. London: Souvenir Press.
                  </div>
               <div class="bibl"><span class="ref" id="bernasconi_cetnic_impett_2023"><!-- close -->Bernasconi, Cetiniƒá, and Impett 2023</span>¬†Bernasconi, V., Cetiniƒá, E., and Impett, L. (2023) 
                  ‚ÄúA computational approach to hand pose recognition in early modern paintings‚Äù, <cite class="title italic">Journal of 
                     Imaging</cite>, 9(6).
                  </div>
               <div class="bibl"><span class="ref" id="broadwell_tangherlini_2021"><!-- close -->Broadwell and Tangherlini 2021</span>¬†Broadwell, P. and Tangherlini, T.R. (2021) 
                  ‚ÄúComparative K-pop choreography analysis through deep-learning pose estimation across
                  a large video corpus‚Äù, 
                  <cite class="title italic">DHQ: Digital Humanities Quarterly</cite>, 15(1). Available at: 
                  <a href="https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html" onclick="window.open('https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html'); return false" class="ref">https://digitalhumanities.org/dhq/vol/15/1/000506/000506.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="cameron_franks_hamidzadeh_2023"><!-- close -->Cameron, Franks, and Hamidzadeh 2023</span>¬†Cameron, S., Franks, P., and Hamidzadeh, B. (2023) 
                  ‚ÄúPositioning paradata: A conceptual frame for AI processual documentation in archives
                  and recordkeeping contexts‚Äù, 
                  <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 16(4).
                  </div>
               <div class="bibl"><span class="ref" id="cao_et_al_2019"><!-- close -->Cao et al. 2019</span>¬†Cao, Z. et al. (2019) ‚ÄúOpenpose: Realtime multi-person 2D pose estimation 
                  using part affinity fields‚Äù, <cite class="title italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</cite>. Available at: 
                  <a href="https://arxiv.org/pdf/1812.08008.pdf" onclick="window.open('https://arxiv.org/pdf/1812.08008.pdf'); return false" class="ref">https://arxiv.org/pdf/1812.08008.pdf</a>.
                  </div>
               <div class="bibl"><span class="ref" id="chalmers_et_al_2021"><!-- close -->Chalmers et al. 2021</span>¬†Chalmers, A. et al. (2021) ‚ÄúRealistic humans in virtual cultural 
                  heritage‚Äù, <cite class="title italic">Proceedings of RISE IMET 2021</cite>. Nicosia, Cyprus, 2-4 June 2021. New York: Springer, pp. 156-165.
                  </div>
               <div class="bibl"><span class="ref" id="chao_et_al_2018"><!-- close -->Chao et al. 2018</span>¬†Chao, H. et al. (2018) ‚ÄúKapturing kung fu: Future proofing the Hong Kong martial 
                  arts living archive‚Äù, in Whatley, S., Cisneros, R.K., and Sabiescu, A. (eds.) <cite class="title italic">Digital echoes: Spaces for intangible and 
                     performance-based cultural heritage</cite>. New York: Springer, pp. 249-264.
                  </div>
               <div class="bibl"><span class="ref" id="colavizza_et_al_2021"><!-- close -->Colavizza et al. 2021</span>¬†Colavizza, G. (2021) ‚ÄúArchives and AI: An overview of current 
                  debates and future perspectives‚Äù, <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 15(1).
                  </div>
               <div class="bibl"><span class="ref" id="craig_et_al_2018"><!-- close -->Craig et al. 2018</span>¬†Craig, C.J. (2018) ‚ÄúThe embodied nature of narrative knowledge: A 
                  cross-study analysis of embodied knowledge in teaching, learning, and life‚Äù, <cite class="title italic">Teaching and Teacher Education</cite>, 71, 
                  pp. 329-240.
                  </div>
               <div class="bibl"><span class="ref" id="delbridge_2015"><!-- close -->Delbridge 2015</span>¬†Delbridge, M. (2015) <cite class="title italic">Motion capture in performance: An introduction</cite>. 
                  New York: Springer.
                  </div>
               <div class="bibl"><span class="ref" id="dimitropoulos_et_al_2014"><!-- close -->Dimitropoulos et al. 2014</span>¬†Dimitropoulos, K. et al. (2014) ‚ÄúCapturing the intangible 
                  an introduction to the i-Treasures project‚Äù, <cite class="title italic">Proceedings of the 9th international conference on computer vision theory 
                     and applications</cite>, vol. 3. Lisbon, Portugal, 5-8 January 2014. Set√∫bal, Portugal: SCITEPRESS, pp. 773-781.
                  Available at: 
                  <a href="https://ieeexplore.ieee.org/abstract/document/7295018" onclick="window.open('https://ieeexplore.ieee.org/abstract/document/7295018'); return false" class="ref">https://ieeexplore.ieee.org/abstract/document/7295018</a>.
                  </div>
               <div class="bibl"><span class="ref" id="doulamis_et_al_2017"><!-- close -->Doulamis et al. 2017</span>¬†Doulamis, A.D. (2017) ‚ÄúTransforming intangible folkloric performing 
                  arts into tangible choreographic digital objects: The terpsichore approach‚Äù, <cite class="title italic">Proceedings of the 12th international 
                     joint conference on computer vision, imaging and computer graphics, theory, and applications</cite>. Porto, Portugal, 27 February-1 March 2017. 
                  Set√∫bal, Portugal: SCITEPRESS, pp. 451-460.
                  <a href="https://doi.org/10.3030/691218" onclick="window.open('https://doi.org/10.3030/691218'); return false" class="ref">https://doi.org/10.3030/691218</a>.
                  </div>
               <div class="bibl"><span class="ref" id="dork_carpendale_williamson_2011"><!-- close -->D√∂rk, Carpendale, and Williamson 2011</span>¬†D√∂rk, M., Carpendale, S., and Williamson, C. (2011) 
                  ‚ÄúThe information flaneur: A fresh look at information seeking‚Äù, <cite class="title italic">Proceedings of the SIGCHI 
                     conference on human factors in computing systems</cite>. Vancouver, BC, Canda, 7-12 May 2011. New York: ACM, pp. 1215-1224. 
                  <a href="https://doi.org/10.1145/1978942.1979124" onclick="window.open('https://doi.org/10.1145/1978942.1979124'); return false" class="ref">https://doi.org/10.1145/1978942.1979124</a>.
                  </div>
               <div class="bibl"><span class="ref" id="edmondson_2004"><!-- close -->Edmondson 2004</span>¬†Edmondson, R. (2004) <cite class="title italic">Audiovisual archiving: Philosophy and principles</cite>. 
                  Paris: United Nations Educational, Scientific and Cultural Organization.
                  </div>
               <div class="bibl"><span class="ref" id="el-raheb_et_al_2018"><!-- close -->El Raheb et al. 2018</span>¬†El Raheb, K. (2018) ‚ÄúA web-based system for annotation of dance 
                  multimodal recordings by dance practitioners and experts‚Äù, <cite class="title italic">Proceedings of the 5th international conference on movement 
                     and computing</cite>. Genoa, Italy, 28-30 June 2018. 
                  <a href="https://doi.org/10.1145/3212721.3212722" onclick="window.open('https://doi.org/10.1145/3212721.3212722'); return false" class="ref">https://doi.org/10.1145/3212721.3212722</a>.
                  </div>
               <div class="bibl"><span class="ref" id="fossati_et_al_2012"><!-- close -->Fossati et al. 2012</span>¬†Fossati, G. et al. (2012) ‚ÄúFound footage filmmaking, film archiving 
                  and new participatory platforms‚Äù, in Guldemond, J., Bloemheuvel, M., and Fossati, G. (eds.) <cite class="title italic">Found footage: Cinema 
                     exposed</cite>. Amsterdam: Amsterdam University Press, pp. 177-184.
                  </div>
               <div class="bibl"><span class="ref" id="grammalidis_dimitropoulos_2015"><!-- close -->Grammalidis and Dimitropoulos 2015</span>¬†Grammalidis, N. and Dimitropoulos, K. (2015) 
                  ‚ÄúIntangible treasures: Capturing the intangible cultural heritage and learning the
                  rare know-how of living human 
                  treasures‚Äù, <cite class="title italic">Proceedings of the 2015 digital heritage international congress</cite>, Granada, Spain, 28 September-2 
                  October 2015. Available at: 
                  <a href="https://diglib.eg.org/handle/10.2312/14465" onclick="window.open('https://diglib.eg.org/handle/10.2312/14465'); return false" class="ref">https://diglib.eg.org/handle/10.2312/14465</a>.
                  </div>
               <div class="bibl"><span class="ref" id="griffin_wennerstrom_foka_2023"><!-- close -->Griffin, Wennerstr√∂m, and Foka 2023</span>¬†Griffin, G., Wennerstr√∂m, E., and Foka, A. (2023) 
                  ‚ÄúAI and Swedish heritage organisations: Challenges and opportunities‚Äù, <cite class="title italic">AI &amp; Society</cite>. 
                  <a href="https://doi.org/10.1007/s00146-023-01689-y" onclick="window.open('https://doi.org/10.1007/s00146-023-01689-y'); return false" class="ref">https://doi.org/10.1007/s00146-023-01689-y</a>.
                  </div>
               <div class="bibl"><span class="ref" id="guest_1977"><!-- close -->Guest 1977</span>¬†Guest, A.H. (1977) <cite class="title italic">Labanotation: Or, kinetography laban: The system of analyzing and 
                     recording movement</cite>. New York: Taylor &amp; Francis.
                  </div>
               <div class="bibl"><span class="ref" id="halko_martinsson_tropp_2011"><!-- close -->Halko, Martinsson, and Tropp 2011</span>¬†Halko, N., Martinsson, P.G. and Tropp, J.A. (2011), 
                  ‚ÄúFinding structure with randomness: Probabilistic algorithms for constructing approximate
                  matrix decompositions‚Äù, 
                  <cite class="title italic">SIAM Review</cite>, 53(2), pp. 217‚Äì288. 
                  <a href="https://doi.org/10.1137/090771806" onclick="window.open('https://doi.org/10.1137/090771806'); return false" class="ref">https://doi.org/10.1137/090771806</a>.
                  </div>
               <div class="bibl"><span class="ref" id="hou_et_al_2022"><!-- close -->Hou et al. 2022</span>¬†Hou, Y. et al. (2022) ‚ÄúDigitizing intangible cultural heritage embodied: State 
                  of the art‚Äù, <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 15(3), pp. 1‚Äì20.
                  </div>
               <div class="bibl"><span class="ref" id="hou_seydou_kenderdine_2023"><!-- close -->Hou, Seydou, and Kenderdine 2023</span>¬†Hou, Y., Seydou, F.M., and Kenderdine S. (2023) 
                  ‚ÄúUnlocking a multimodal archive of southern chinese martial arts through embodied cues‚Äù, 
                  <cite class="title italic">Journal of Documentation</cite>. 
                  <a href="https://doi.org/10.1108/JD-01-2022-0027" onclick="window.open('https://doi.org/10.1108/JD-01-2022-0027'); return false" class="ref">https://doi.org/10.1108/JD-01-2022-0027</a>.
                  </div>
               <div class="bibl"><span class="ref" id="impett_2020a"><!-- close -->Impett 2020a</span>¬†Impett, L. (2020a) ‚ÄúAnalyzing gesture in digital art history‚Äù, in 
                  Brown, K. (ed.) <cite class="title italic">The routledge companion to digital humanities and art history</cite>. New York: Routledge, pp. 386-407.
                  </div>
               <div class="bibl"><span class="ref" id="impett_2020b"><!-- close -->Impett 2020b</span>¬†Impett, L. (2020b) <cite class="title italic">Painting by numbers: Computational methods and the history 
                     of art</cite>. EPFL.
                  </div>
               <div class="bibl"><span class="ref" id="ingold_2011"><!-- close -->Ingold 2011</span>¬†Ingold, T. (2011) <cite class="title italic">Being alive: Essays on movement, knowledge and description</cite>. 
                  New York: Taylor &amp; Francis.
                  </div>
               <div class="bibl"><span class="ref" id="jaillant_2022"><!-- close -->Jaillant 2022</span>¬†Jaillant, L. (2022) <cite class="title italic">Archives, access and artificial intelligence: Working 
                     with born-digital and digitized archival collections</cite>. Bielefeld, Germany: Bielefeld University Press.
                  </div>
               <div class="bibl"><span class="ref" id="jaillant_rees_2023"><!-- close -->Jaillant and Rees 2023</span>¬†Jaillant, L. and Rees, A. (2023) ‚ÄúApplying AI to digital 
                  archives: Trust, collaboration and shared professional ethics‚Äù, <cite class="title italic">Digital Scholarship in the Humanities</cite>, 38(2), pp. 
                  571-585.
                  </div>
               <div class="bibl"><span class="ref" id="kenderdine_mason_hibberd_2021"><!-- close -->Kenderdine, Mason, and Hibberd 2021</span>¬†Kenderdine, S., Mason, I., and Hibberd L. (2021) 
                  ‚ÄúComputational archives for experimental museology‚Äù, <cite class="title italic">Proceedings of RISE IMET 2021</cite>. Nicosia, 
                  Cyprus, 2-4 June 2021. New York: Springer, pp. 3-18.
                  </div>
               <div class="bibl"><span class="ref" id="kreiss_bertoni_alahi_2021"><!-- close -->Kreiss, Bertoni, and Alahi 2021</span>¬†Kreiss, S., Bertoni, L., and Alahi, A. (2021) 
                  ‚ÄúOpenpifpaf: Composite fields for semantic keypoint detection and spatio-temporal association‚Äù, 
                  <cite class="title italic">IEEE Transactions on Intelligent Transportation Systems</cite>, 23(8), pp. 13498-13511.
                  </div>
               <div class="bibl"><span class="ref" id="krol_mynarski_2005"><!-- close -->Kr√≥l and Mynarski 2005</span>¬†Kr√≥l, H. and Mynarski, W. (2005) <cite class="title italic">Cechy ruchu-charakterystyka i 
                     mo≈ºliwo≈õci parametryzacji [Features of movement-characteristics and capabilities of
                     parametryzation]</cite>. Katowice, Poland: Akademia Wychowania 
                  Fizycznego.
                  </div>
               <div class="bibl"><span class="ref" id="lin_et_al_2014"><!-- close -->Lin et al. 2014</span>¬†Lin, T.Y. (2014) ‚ÄúMicrosoft COCO: Common objects in context‚Äù, 
                  <cite class="title italic">Proceedings of the 13th annual European conference of computer vision</cite>. Zurich, Switzerland, 6-12 September 2014. 
                  New York: Springer, pp. 740-755.
                  </div>
               <div class="bibl"><span class="ref" id="ma_2003"><!-- close -->Ma 2003</span>¬†Ma, M. (2003) <cite class="title italic">Wu xue tan zhen [Examination of truth in martial studies)</cite>. Taipei, 
                  Taiwan: Lion Books.
                  </div>
               <div class="bibl"><span class="ref" id="mallik_chaudhury_2012"><!-- close -->Mallik and Chaudhury 2012</span>¬†Mallik, A. and Chaudhury, S. (2012) ‚ÄúAcquisition of 
                  multimedia ontology: An application in preservation of cultural heritage‚Äù, <cite class="title italic">International Journal of Multimedia Information 
                     Retrieval</cite>, 1(4), pp. 249‚Äì262.
                  </div>
               <div class="bibl"><span class="ref" id="mallik_chaudhury_ghosh_2011"><!-- close -->Mallik, Chaudhury, and Ghosh 2011</span>¬†Mallik, A., Chaudhury, S., and Ghosh, H. (2011) 
                  ‚ÄúNrityakosha: Preserving the intangible heritage of indian classical dance‚Äù, <cite class="title italic">Journal on Computing and 
                     Cultural Heritage</cite>, 4(3), pp. 1‚Äì25.
                  </div>
               <div class="bibl"><span class="ref" id="manovich_2020"><!-- close -->Manovich 2020</span>¬†Manovich, L. (2020) <cite class="title italic">Cultural analytics</cite>. Cambridge, MA: The MIT Press.
                  </div>
               <div class="bibl"><span class="ref" id="martinsson_rokhlin_tvgert_2011"><!-- close -->Martinsson, Rokhlin, and Tvgert 2011</span>¬†Martinsson, P.-G., Rokhlin, V., and Tygert, M. (2011) 
                  ‚ÄúA randomized algorithm for the decomposition of matrices‚Äù, <cite class="title italic">Applied and Computational Harmonic 
                     Analysis</cite>, 30(1), pp. 47‚Äì68. 
                  <a href="https://www.sciencedirect.com/science/article/pii/S1063520310000242" onclick="window.open('https://www.sciencedirect.com/science/article/pii/S1063520310000242'); return false" class="ref">https://www.sciencedirect.com/science/article/pii/S1063520310000242</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mcgregor_lab_2019"><!-- close -->McGregor and Lab 2019</span>¬†McGregor, W. and Lab, G.A.C. (2019) <cite class="title italic">Living archive</cite>. 
                  Availablet at: <a href="https://artsexperiments.withgoogle.com/living-archive" onclick="window.open('https://artsexperiments.withgoogle.com/living-archive'); return false" class="ref">https://artsexperiments.withgoogle.com/living-archive</a>. 
                  (Accessed: 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="mcinnes_et_al_2018"><!-- close -->McInnes et al. 2018</span>¬†McInnes, L. et al. (2018) ‚ÄúUmap: Uniform manifold approximation and 
                  projection‚Äù, <cite class="title italic">The Journal of Open Source Software</cite>, 3(29). 
                  <a href="https://doi.org/10.21105/joss.00861" onclick="window.open('https://doi.org/10.21105/joss.00861'); return false" class="ref">https://doi.org/10.21105/joss.00861</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mul_masson_2018"><!-- close -->Mul and Masson 2018</span>¬†Mul, G. and Masson, E. (2018) ‚ÄúData-based art, algorithmic poetry: 
                  Geert Mul in conversation with Eef Masson‚Äù, <cite class="title italic">TMG Journal for Media History</cite>, 21(2).
                  </div>
               <div class="bibl"><span class="ref" id="olesen_et_al_2016"><!-- close -->Olesen et al. 2016</span>¬†Olesen, C.G. (2016) ‚ÄúData-driven research for film history: Exploring 
                  the Jean Desmet collection‚Äù, <cite class="title italic">Moving Image: The Journal of the Association of Moving Image Archivists</cite>, 16(1), pp. 
                  82‚Äì105.
                  </div>
               <div class="bibl"><span class="ref" id="pedregosa_et_al_2011"><!-- close -->Pedregosa et al. 2011</span>¬†Pedregosa, F. et al. (2011) ‚ÄúScikit-learn: Machine learning in 
                  Python‚Äù, <cite class="title italic">Journal of Machine Learning Research</cite>, 12(85), pp. 2825‚Äì2830.
                  </div>
               <div class="bibl"><span class="ref" id="rtsarchives_2018"><!-- close -->RTSArchives 2018</span>¬†RTSArchives (2018) <cite class="title italic">Le nouveau site RTSarchives</cite>. Available at: 
                  <a href="https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html" onclick="window.open('https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html'); return false" class="ref">https://www.rts.ch/archives/5919889-le-nouveau-site-rtsarchives.html</a>. 
                  (Accessed: 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="salazar_2018"><!-- close -->Salazar 2018</span>¬†Salazar Sutil, N. (2018) ‚ÄúSection editorial: Human movement as critical creativity: 
                  Basic questions for movement computing‚Äù, <cite class="title italic">Computational Culture: a Journal of Software Studies</cite>, 6.
                  </div>
               <div class="bibl"><span class="ref" id="sciuto_et_al_2023"><!-- close -->Sciuto et al. 2023</span>¬†Sciuto, C. et al. (2023) ‚ÄúExploring fragmented data: Environments, 
                  people and the senses in virtual reality‚Äù, in Landeschi, G. and Betts, E. (eds.) <cite class="title italic">Capturing the senses: Digital methods 
                     for sensory archaeologies</cite>. New York: Springer, pp. 85-103.
                  </div>
               <div class="bibl"><span class="ref" id="sedmidubsky_et_al_2020"><!-- close -->Sedmidubsky et al. 2020</span>¬†Sedmidubsky, J. et al. (2020) ‚ÄúMotion words: A text-like 
                  representation of 3D skeleton sequences‚Äù, <cite class="title italic">Proceedings of the 42nd annual European conference on information 
                     retrieval</cite>. Lisbon, Portugal, 14-17 April 2020. New York: Springer, pp. 527-241. 
                  <a href="https://doi.org/10.1007/978-3-030-45439-5_35" onclick="window.open('https://doi.org/10.1007/978-3-030-45439-5_35'); return false" class="ref">https://doi.org/10.1007/978-3-030-45439-5_35</a>.
                  </div>
               <div class="bibl"><span class="ref" id="shen_et_al_2019"><!-- close -->Shen et al. 2019</span>¬†Shen, H. et al. (2019) ‚ÄúInformation visualisation methods and techniques: 
                  State-of-the-art and future directions‚Äù, <cite class="title italic">Journal of Industrial Information Integration</cite>, 16, pp. 100‚Äì102.
                  </div>
               <div class="bibl"><span class="ref" id="whitelaw_2015"><!-- close -->Whitelaw 2015</span>¬†Whitelaw, M. (2015) ‚ÄúGenerous interfaces for digital cultural 
                  collections‚Äù, <cite class="title italic">DHQ: Digital Humanities Quarterly</cite>, 9(1). Availablet at: 
                  <a href="https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html" onclick="window.open('https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html'); return false" class="ref">https://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html</a>.
                  </div>
               <div class="bibl"><span class="ref" id="winters_prescott_2019"><!-- close -->Winters and Prescott 2019</span>¬†Winters, J. and Prescott, A. (2019) ‚ÄúNegotiating the 
                  born-digital: A problem of search‚Äù, <cite class="title italic">Archives and Manuscripts</cite>, 47(3), pp. 391‚Äì403.
                  </div>
               <div class="bibl"><span class="ref" id="wright_2017"><!-- close -->Wright 2017</span>¬†Wright, R. ‚ÄúThe future of television archives‚Äù, 
                  <cite class="title italic">Digital Preservation Coalition</cite>, 29 November. Available at: 
                  <a href="https://www.dpconline.org/blog/wdpd/the-future-of-television-archives" onclick="window.open('https://www.dpconline.org/blog/wdpd/the-future-of-television-archives'); return false" class="ref">https://www.dpconline.org/blog/wdpd/the-future-of-television-archives</a>.
                  </div>
               <div class="bibl"><span class="ref" id="zkm_center_2023"><!-- close -->ZKM Center for Art and Media Karlsruhe 2023</span>¬†ZKM Center for Art and Media Karlsruhe (2023) 
                  ‚ÄúWilliam Forsythe: Improvisation technologies: The wesbite project‚Äù. Available at: 
                  <a href="https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project" onclick="window.open('https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project'); return false" class="ref">https://zkm.de/en/project/william-forsythe-improvisation-technologies-the-website-project</a>. 
                  (Accessed 30 June 2023).
                  </div>
               <div class="bibl"><span class="ref" id="van-der-maaten_hinton_2008"><!-- close -->van der Maaten and Hinton 2008</span>¬†van der Maaten, L. and Hinton, G. (2008) 
                  ‚ÄúVisualizing data using t-sne‚Äù, <cite class="title italic">Journal of Machine Learning Research</cite>, 9(86), pp. 2579‚Äì2605. 
                  Available at: 
                  <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" onclick="window.open('https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf'); return false" class="ref">https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a>.
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a> ¬†|¬† <span style="color: grey">XML</span> |¬† <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>