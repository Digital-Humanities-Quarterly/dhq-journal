<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!-- Author should supply the title and personal information-->
            <title type="article">Automated Pattern Analysis in Gesture Research: Similarity
               Measuring in 3D Motion Capture Models of Communicative Action</title>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Daniel <dhq:family>Sch√ºller</dhq:family></dhq:author_name>
               <dhq:affiliation>Natural Media Lab, Human Technology Centre, RWTH Aachen
                  University</dhq:affiliation>
               <email>schueller@humtec.rwth-aachen.de</email>
               <dhq:bio>
                  <p>Daniel Sch√ºller studied philosophy, linguistics and history at
                     Friedrich-Wilhelms-Universit√§t Bonn and RWTH Aachen University, focusing on
                     philosophy of science and logic as well as on philosophy of language. In 2013
                     he graduated with an M.A. thesis in which he comparatively investigated the use
                     and heuristics of fictional models in history and physics. Since 2014, he is a
                     research assistant and doctoral student at the chair of Linguistics and
                     Cognitive Semiotics and the Natural Media Lab at RWTH Aachen University. His
                     main research interests include linguistic and semiotic theory ‚Äì with special
                     emphases on sign processes in co-speech gesture, semiotics in and of gesture
                     research, motion-capture technology, and the field of digital humanities in
                     general.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Christian <dhq:family>Beecks</dhq:family></dhq:author_name>
               <dhq:affiliation>University of M√ºnster</dhq:affiliation>
               <email>christian.beecks@uni-muenster.de</email>
               <dhq:bio>
                  <p>Christian Beecks is head of the Data Management and Analytics Group in the
                     Computer Science Department at the University of M√ºnster. In addition, he is a
                     Senior Researcher in the User-Centered Ubiquitous Computing Group at the
                     Fraunhofer Institute for Applied Information Technology FIT. He received his
                     PhD in 2013 from RWTH Aachen University.</p>
                  <p>His research interests include Multimedia Data Engineering, Real-time Data
                     Management and Smart Data Analysis. He has authored more than 70 conference and
                     journal papers and won two best paper awards. In addition, he is a reviewer for
                     various international conferences and journals.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Marwan <dhq:family>Hassani</dhq:family></dhq:author_name>
               <dhq:affiliation>Data Management and Exploration Group, RWTH Aachen
                  University</dhq:affiliation>
               <email>m.hassani@tue.nl</email>
               <dhq:bio>
                  <p>Marwan Hassani is an assistant professor in the architecture of information
                     systems group at Eindhoven University of Technology, The Netherlands.
                     Previously, he acted as a postdoc researcher and associate teaching assistant
                     at the data management and data exploration group at the RWTH Aachen
                     University, Germany. His research interests include stream data mining,
                     sequential pattern mining of multiple streams, stream process mining, efficient
                     anytime clustering of big data streams and exploration of evolving graph data.
                     Marwan received hid PhD (2015) from RWTH Aachen University. He received an
                     equivalence Master in Computer Science from RWTH Aachen University (2009). He
                     coauthored more than 42 scientific publications and serves on several program
                     committees.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Jennifer <dhq:family>Hinnell</dhq:family></dhq:author_name>
               <dhq:affiliation>Department of Linguistics, University of Alberta</dhq:affiliation>
               <email>hinnell@ualberta.ca</email>
               <dhq:bio>
                  <p>Jennifer Hinnell is a doctoral candidate in the Department of Linguistics at
                     the University of Alberta, Edmonton, Canada. Her research centers around
                     communication in interaction. She uses multimodal corpus data, 3D motion
                     capture data, and experimental methods to explore how people use their bodies,
                     in conjunction with semantic and syntactic structures in speech, to create and
                     convey meaning. Jennifer enjoys fruitful research partnerships with Little Red
                     Hen Distributed Learning Lab (UCLA) and the Natural Media Lab at the RWTH
                     Aachen, Germany.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Bela <dhq:family>Brenger</dhq:family></dhq:author_name>
               <dhq:affiliation>Natural Media Lab, Human Technology Centre, RWTH Aachen
                  University</dhq:affiliation>
               <email>brenger@rwth-aachen.de</email>
               <dhq:bio>
                  <p>Bela Brenger studied linguistics and computer science at RWTH Aachen
                     University. He graduated in 2015 with his interdisciplinary thesis analyzing
                     motion-capture data of head gestures in dialogues. Since 2016 he is part of the
                     scientific staff at the chair of Linguistics and Cognitive Semiotics and
                     manages the Natural Media Motion-Capture-Lab. Main interests are data-driven
                     analysis of multimodal communication with emphasis on methods to integrate
                     spatial gesture data and speech.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Thomas <dhq:family>Seidl</dhq:family></dhq:author_name>
               <dhq:affiliation>Ludwig Maximilian University of Munich</dhq:affiliation>
               <email>seidl@dbs.ifi.lmu.de</email>
               <dhq:bio>
                  <p>Thomas Seidl is Professor of Computer Science at Ludwig-Maximilians-Universit√§t
                     M√ºnchen (LMU Munich), Germany.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Irene <dhq:family>Mittelberg</dhq:family></dhq:author_name>
               <dhq:affiliation>Natural Media Lab, Human Technology Centre, RWTH Aachen
                  University</dhq:affiliation>
               <email>mittelberg@humtec.rwth-aachen.de</email>
               <dhq:bio>
                  <p>Irene Mittelberg is Professor of Linguistics and Cognitive Semiotics at the
                     Institute of English, American and Romance Studies at RWTH Aachen University.
                     She directs the Natural Media Lab at Human Technology Centre (HumTec) and the
                     Center for Sign Language and Gesture (SignGes). After gaining an M.A. in French
                     linguistics and art history from Hamburg University, she completed an M.A. and
                     a Ph.D. in Linguistics and Cognitive Studies at Cornell University. Combining
                     embodiment research with classic semiotic theories (e.g. C.S. Peirce, R.
                     Jakobson), Mittelberg‚Äôs cross-disciplinary research on language, gesture,
                     space, embodied cognition and the visual arts has emphasized the role of
                     metonymy, metaphor, frames, constructions, and image schemas in multimodal
                     communication. Moreover, Mittelberg and her research team have developed tools
                     and methods to use optical motion-capture technology for empirical gesture
                     research at the juncture of linguistics, semiotics, architectural design,
                     computer science, social neuroscience, and digital humanities.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association of Computers and the Humanities</publisher>
            <!-- This information will be completed at publication -->
            <idno type="DHQarticle-id">000309</idno>
            <idno type="volume">011</idno>
            <idno type="issue">2</idno>
            <date when="2017-05-22">22 May 2017</date>
            <dhq:articleType>article</dhq:articleType>
            <availability>
               <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>

         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!-- Authors may include one or more keywords of their choice -->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Each change should include @who and @when as well as a brief note on what was done. -->
         <change when="2017-04-25" who="JDF">Proofread file</change>
      </revisionDesc>
   </teiHeader>

   <text xml:lang="en">
      <front>
         <dhq:abstract>
            <!-- Include a brief abstract of the article -->
            <p>The question of how to model similarity between gestures plays an important role in
               current studies in the domain of human communication. Most research into recurrent
               patterns in co-verbal gestures ‚Äì manual communicative movements emerging
               spontaneously during conversation ‚Äì is driven by qualitative analyses relying on
               observational comparisons between gestures. Due to the fact that these kinds of
               gestures are not bound to well-formedness conditions, however, we propose a
               quantitative approach consisting of a distance-based similarity model for gestures
               recorded and represented in motion capture data streams. To this end, we model
               gestures by flexible feature representations, namely gesture signatures, which are
               then compared via signature-based distance functions such as the Earth Mover's
               Distance and the Signature Quadratic Form Distance. Experiments on real
               conversational motion capture data evidence the appropriateness of the proposed
               approaches in terms of their accuracy and efficiency. Our contribution to gesture
               similarity research and gesture data analysis allows for new quantitative methods of
               identifying patterns of gestural movements in human face-to-face interaction, i.e.,
               in complex multimodal data sets. </p>
         </dhq:abstract>
         <dhq:teaser>
            <!-- Include a brief teaser, no more than a phrase or a single sentence -->
            <p>Gesture similarity research and gesture data analysis allowing for new quantitative
               methods of identifying patterns of gestural movements in human face-to-face
               interaction.</p>
         </dhq:teaser>
      </front>
      <body>
         <head/>
         <!-- Use <div> only if there is more than one section -->
         <div>
            <head>Introduction</head>
            <p> Given the central place of the <term>embodied mind</term> in experientialist
               approaches to language, co-verbal gestures have become a valuable data source in
               cognitive, functional and anthropological linguistics (e.g. <ptr
                  target="#sweetser2007"/>). While there exist various views on embodiment, the core
               idea is that human higher cognitive abilities are shaped by the morphology of our
               bodies and the way we interact with the material, spatial and social environment
               (e.g. <ptr target="#gibbs2006"/>; <ptr target="#johnson1987"/>). Drawing on these
               premises, some gesture scholars stress that gestures are conditioned by the forms and
               affordances of their material habitat as well as the speakers‚Äô interactive and
               collaborative practices (e.g. <ptr target="#enfield2009"/>; <ptr
                  target="#streeck2011"/>). Pioneering work done by Kendon (e.g. <ptr
                  target="#kendon1972"/>, <ptr target="#kendon2004"/>), McNeill (e.g. <ptr
                  target="#mcneill1985"/>, <ptr target="#mcneill1992"/>, <ptr target="#mcneill2005"
               />) and M√ºller <ptr target="#mueller1998"/> has shown that manual gestures are an
               integral part of utterance formation and communicative interaction. The
               state-of-the-art of research in the growing interdisciplinary field of <term>gesture
                  studies</term> has recently been presented in the <title rend="italic"
                  >International Handbook on Multimodality in Human Interaction</title> (<ptr
                  target="#mueller2013"/>, Vol. 1 2013, <ptr target="#mueller2014"/> Vol. 2 2014).
               One quintessence to be drawn from this large body of work is that language, whether
               spoken or signed, is embodied, dynamic, multimodal and intersubjective (see also <ptr
                  target="#duncan2007"/>; <ptr target="#gibbs2006"/>; <ptr target="#jaeger2004"/>;
                  <ptr target="#mittelberg2013"/>; <ptr target="#mueller2008"/>). </p>
            <p> Indeed, human communication typically involves multiple modalities such as
               vocalizations, spoken or signed discourse, manual gestures, eye gaze, body posture
               and facial expressions. In face-to-face communication, manual gestures play an
               important role by conveying meaningful information and guiding the interlocutors‚Äô
               attention to objects and persons talked about. Gestures here are understood as
               spontaneously emerging, dynamic configurations and movements of the speakers‚Äô hands
               and arms that contribute to the communicative content and partake in the interactive
               organization of a spoken dialogue situation (e.g. <ptr target="#bavelas1992"/>; <ptr
                  target="#kendon2004"/>; <ptr target="#mcneill1992"/>; <ptr target="#mueller1998"
               />; <ptr target="#mittelberg2016"/>). The contribution of gestures to multimodal
               interaction may consist in, e.g., deictic reference to locations, ideas, persons or
               things (both abstract and concrete); they may also fulfill metalinguistic functions,
               e.g., in referring to citations of other speakers or outlining the structure of their
               argumentation. Gestures may also provide schematic, iconic portrayals of actions,
               things or spatial constellations <ptr target="#mittelberg2014"/>; <ptr
                  target="#rieser2012"/>. As we use the term here, gestures are not to be confused
               with emblems (e.g. the victory sign), which have a culturally defined form-meaning
               correlation in the same sense that words may have a fixed meaning <ptr
                  target="#mcneill1992"/>. So gestures are always to be investigated in view of the
               co-occurring speech with which they jointly create (new) semiotic material and
               support speakers in organizing their thoughts or drawing connections to their social
               and physical environment <ptr target="#streeck2011"/>; <ptr target="#mittelberg2016"
               />. </p>
            <p> Drawing on this large body of gesture research across various fields of the
               humanities and social sciences, the interdisciplinary approach presented here aims at
               identifying and visualizing patterns of gestural behavior with the help of
               custom-tailored computational tools and methods. Although co-speech gestures tend to
               be regarded as highly idiosyncratic in respect to their spontaneous individual
               articulation by speakers in spoken dialogue situations, it is safe to assume that
               there are recurring forms of dynamic hand configurations and movement patterns which
               are performed by speakers sharing the same cultural background. From this assumption
               follows the hypothesis that, on the one hand, a general degree of similarity between
               gestural forms may be presumed ‚Äì trivially ‚Äì due to the shared morphology of the
               human body (e.g. <ptr target="#tomasello1999"/>). On the other hand, the specific
               cultural context plays an important role in both language acquisition and the
               adoption of culture-specific behavioral patterns (see, e.g. <ptr
                  target="#bourdieu1987"/> on Habitus and Hexis). Moreover, gesture has also been
               ascribed a constitutive role regarding the human capacity for language both
               ontogenetically and phylogenetically (e.g. <ptr target="#goldin-meadow2003"/>; <ptr
                  target="#mcneill2012"/>). Previous empirical gesture research indeed shows that
               co-verbal gestures exhibit recurrent form features and movement patterns, as well as
               recurring form-meaning pairings: see, for instance, Bressem <ptr
                  target="#bressem2013"/> on form features; Fricke <ptr target="#fricke2010"/> on
                  <q>kinaesthemes</q>; Kendon <ptr target="#kendon2004"/> on <q>locution
                  clusters</q>, McNeill <ptr target="#mcneill1992"/>, <ptr target="#mcneill2000"/>
               on cross-linguistic path and manner imagery portraying motion events; McNeill <ptr
                  target="#mcneill2005"/> on <q>catchments</q>; Ladewig <ptr target="#ladewig2011"/>
               and M√ºller <ptr target="#mueller2010"/> on <q>recurring gestures</q>, and Cienki <ptr
                  target="#cienki2005"/> and Mittelberg <ptr target="#mittelberg2010"/> on
               image-schematic patterns. From this perspective, a question central to gesture
               research concerns the factors that may motivate and lend a certain systematicity to
               forms and functions of human communicative behaviors not constituting an independent
               sign system in the Saussurian sense as spoken and signed languages do. Manual gesture
               is a semiotically versatile medium, for it may, depending on a given context, assume
               more or less language-like functions: from accompanying spoken discourse in the form
               of pointing, accentuating beats, schematic iconicity or managing social interaction
               to carrying the full load of language (e.g. <ptr target="#goldin-meadow2007"/>). </p>
            <p> In this paper, we will focus on certain kinds of co-verbal gestures, i.e. specific
               image-schematic gestalts, e.g. spirals, circles, and straight paths <ptr
                  target="#cienki2013"/>, <ptr target="#mittelberg2010"/>. Figure 1 shows
               visualizations of several different gestural traces belonging to these movement
               types. In what follows, we will present a novel method designed to identify, search
               and cluster in an automatized fashion gestural movement patterns throughout our data
               set, and potentially also any other motion capture data set, e.g. to recognize and
               group those traces that are similar regarding their formal features and the way in
               which they unfold in gesture space. </p>
            <!--here goes FIGURE 1-->

            <figure>
               <graphic url="resources/images/figure01.png"/>
            </figure>
         </div>
         <div>
            <head>Research Objective</head>
            <p> Whereas the gesture research discussed above mostly relies on observational methods
               and qualitative video analyses, our aim is to add to the catalogue of methods for
               empirical linguistics and gesture studies by outlining a computational, quantitative
               and comparative 3D-model driven approach in gesture research. While there is a trend
               to combine qualitative with quantitative as well as experimental methods in
               multimodal communication research <ptr target="#gonzalez-marquez2007"/>; <ptr
                  target="#mcneill2001"/>; <ptr target="#mueller2013"/>, standardized and widely
               applicable tools and methods still need to be developed. In order to derive
               statistically significant patterns from aligned linguistic and behavioral data, some
               recent research initiatives have started to compile and work with larger-scale
               corpora, e.g. drawing on technological advances in data management and automated
               analyses (e.g. the Little Red Hen Project, <ptr target="#steen2013"/>). While using
               audio and video technology to record co-speech gestures remains the dominant way to
               construct multimodal corpora, some research teams have begun to employ 3D motion
               capture technology to overcome the limits of 2D video, e.g. Lu and Huenerfauth <ptr
                  target="#lu2010"/> for signed language, and Beskow et al. <ptr
                  target="#beskow2011"/> and Pfeiffer et al. <ptr target="#pfeiffer2013a"/> for
               spoken discourse (see also <ptr target="#pfeiffer2013"/> for an overview). Our
               contribution is to obtain some kind of numerical instrument for graduating gestural
               similarity for measuring gesture similarity in sets of recorded behavioral data.
               While this instrument, in its current state, in no way addresses the problem of the
               meaning of certain gestural forms, it is the first step towards a model of measuring
               similarity between recurring dynamic gesture form patterns. Our goal is to first
               establish a robust, flexible and automated methodology which allows us to determine
                  <list type="ordered">
                  <item>whether there are shared or common reoccurring gestural movement patterns in
                     a given set of 3D recorded, behavioral communication data,</item>
                  <item>exactly which forms there are, and</item>
                  <item>the extent to which they occur,</item>
               </list> and then to apply this methodology to the recorded 3D numerical MoCap data of
               a group of participants. </p>
            <p> Both the alignment of gestures with the co-occurring speech, and the semantic
               comparison of the established (formally) sufficiently similar gesture-speech
               constructions, still have to be done manually by human gesture researchers, through
               semiotic analyses of the multimodal, speech and behavioral data corpora. The primary
               aim of developing an automated indicator of gesture similarity is to identify
               recurrent movement patterns of interest from the recorded 3D corpus data
               computationally, and thus to enable human gesture researchers to handle these data
               sets in a more efficient manner. In order to make gesture similarity automatically
               accessible, we propose a distance-based similarity model for gestures arising in
               three-dimensional motion capture data streams. In comparison to two-dimensional video
               capture technology, working with numerical three-dimensional motion capture
               technology has the advantage of measuring and visualizing the temporal and spatial
               dynamics of otherwise invisible movement traces with the highest possible accuracy.
               We aim at maintaining this accuracy by aggregating movement traces, also called
               trajectories, into a <term>gesture signature</term>
               <ptr target="#beecks2015"/>. This gesture signature has the ability of weighting
               trajectories according to their relevance. Based on this lossless feature
               representation, we propose to measure similarity by means of distance-based
               approaches <ptr target="#beecks2013"/>, <ptr target="#beecks2010"/>. We particularly
               investigate the <term>Earth Mover's Distance</term>
               <ptr target="#rubner2000"/> and the <term>Signature Quadratic Form Distance</term>
               <ptr target="#beecks2010"/> for the comparison of two gesture signatures on real
               conversational motion capture data. </p>
         </div>
         <div>
            <head>Properties of the 3D Data Model</head>
            <p> From a philosophy of science point of view, before being able to apply computational
               algorithms to naturalistic real-world gestures, there must be a translation from the
               real-world dialogue situations, involving people speaking and gesturing, from which
               data are captured, to a computable set of data. For this purpose, a marker-based
                  <title rend="italic">Vicon</title> Motion Capture system was used in this study.
               Participants wear a series of markers attached to predetermined body parts of
               interest (fingers, wrists, elbows, neck, head, etc.). The <title rend="italic"
                  >Vicon</title> system automatically generates a chart of numerical 4-tuples of
               Euclidean space-time coordinates for each marker attached to these points on the
               participants‚Äô bodies. The movement of the markers is tracked by 14 <title
                  rend="italic">Vicon</title> infrared cameras, and the physical trajectories of the
               markers are represented in a chart of space-time coordinates. These space-time charts
               form the data sets that are investigated algorithmically, relieving the gesture
               analyst of the difficult, and subjective, task of manually examining highly ephemeral
               real-world dialogue situations. But what are the crucial features that such a
               numerical representation must have in order to enable researchers to not only
               investigate a model but also to finally derive statements and theories about a
               modeled real-world situation? We address the following research questions: Which
               logical features of the model are essential if one wants to investigate the real
               world by investigating a model? And secondly, what are the epistemic benefits of
               investigating models instead of real-world situations? </p>
            <p> The most important feature is that the model <emph>represents</emph> its
               representandum. The representandum in question is a set of relevant features and
               relations of a given part of reality, namely the change in space-time location of
               certain body parts caused by the participants‚Äô kinetic movement. The representing
               model, on the other hand, is the virtual, computational 3D recording of the
               real-world kinetic movement, mapped onto Euclidean space. Representation itself is
               the relation which holds between the model and its representandum. Representation, as
               we understand the term here, is a non-reflexive and non-symmetric relation, which
               simply means that an <emph>a</emph> does not necessarily represent <emph>a</emph>,
               and that if <emph>a</emph> represents <emph>b</emph>, then <emph>b</emph> does not
               necessarily represent <emph>a</emph>. We further assume that representation depends
               on transitive relations, such as identity between some complex relational features of
               the entities to be represented in a model and the entities that represent/model them.
               In short, this means that if there are entities <emph>x, y, z,</emph> there must be
               at least one mutual complex relational feature <emph>Fr(x,y,z),</emph> which these
               entities have in common. The transitive relation <emph>R</emph>, then, is the
                  <q>identity</q> relation, in that two entities have an identical relation to a
               third entity ‚Äì a frame of reference, or a <foreign xml:lang="la">tertium
                  comparationis</foreign>. </p>
            <p>
               <hi rend="bold">Definition:</hi> Transitivity </p>
            <p> For a binary relation <emph>R</emph> over a set <emph>A</emph> consisting of
                  <emph>x, y, z,</emph> the following statement holds true: </p>

            <dhq:example>
               <p>‚àÄx,y,z ‚àà A: xRy &amp; yRz ‚Üí xRz</p>
            </dhq:example>

            <p> The transitivity of <emph>R</emph> is so important here because, in terms of
               modeling, it is the crucial feature that R must have. The different relata involved
               are: movements of body parts <emph>(x)</emph>, movements of markers <emph>(y)</emph>,
               and computational trajectories <emph>(z)</emph>. The relation R which holds between
               these relata is the identity of their curve with respect to space ‚Äì either to a given
               virtual Euclidian space, or the physical space (which functions as a reference
               frame). The identity of the movement curve of body parts and the movement curve of
               markers simply stems from their physical attachment/ conjunction in physical space.
               The identity of marker movements and the computational trajectories is a result of
               metering the markers‚Äô light reflections, by means of the 14 Vicon infrared cameras,
               and numerically mapping the outcome onto Euclidean space coordinates. But if one
               differentiates between physical and Euclidean space, there is hardly any identity one
               could honestly speak of in this case. In what sense could physical and virtual
               movement curves ever be identical? Only in the sense that we <emph>identify</emph>
               physical and Euclidean space by conventions of scientific modeling and metering
               practices; the term <term>curve</term> is itself an indication of how familiar that
               convention is. Since Euclidean space is a conventionalized and well-accepted
               geometrical model of physical space ‚Äì we are well accustomed to talking about
               physical movement in terms of, distances, trajectories, vectors, miles, kilometres
               and change in space/time coordinates etc. ‚Äì one can say that Euclidean space is our
               familiar standard model for describing our perception of movement in physical space,
               both in everyday conversation and scientific discussion. Thus, it is justified to
               speak of the <term>identity</term> of the spatial curves of trajectories in Euclidean
               space and those of moving physical objects such as Motion Capture markers, only if we
               accept this convention. Regarding representation, what does this imply? If
               representation depends on the transitivity of relation R holding between the entities
                  <emph>x, y, z,</emph> and the identity of that relation depends on conventions of
               metering and modeling, representation additionally depends on following conventions.
               Given a 4-tuple of coordinates of a MoCap marker, the movement of this marker is
               modeled by a vector which points from tuple-1 to tuple-2 to tuple-n. The crucial
               feature of this kind of modeling is that the movement of a single marker
                  <emph>a</emph> is represented as a dynamic space-time trajectory which aggregates
               the consecutively changing coordinates of <emph>a</emph> over a given time frame. </p>
            <p> Regarding the above-mentioned definition of transitivity, let our variables take the
               following values: </p>
            <p>
               <hi rend="bold">x</hi> = movement of body part from position a to b;<lb/>
               <hi rend="bold">y</hi> = movement of marker M from position a to b;<lb/>
               <hi rend="bold">z</hi> = trajectory of marker M </p>
            <p> Given these values, we outline the transitivity relation as follows: </p>
            <dhq:example>
               <p>‚àÄx,y,z ‚àà A: xRy &amp; yRz ‚Üí xRz: x [movement of body part from position a to b]
                     <hi rend="bold">R</hi> y [movement of marker M from position a to b] &amp; y
                  [movement of marker M from position a to b] <hi rend="bold">R</hi> z [trajectory
                  of marker M] ‚Üí x [movement of body part from position a to b] <hi rend="bold"
                     >R</hi> z [trajectory of marker M].</p>
            </dhq:example>

            <p> This means that if <emph>x, y, z</emph> obtain a transitive relation <emph>R</emph>
               in the above sense, then <emph>x, y,</emph> and <emph>z</emph> are to be regarded as
               homomorphous abstract <emph>concepts</emph> that all denote the same event of
               spatiotemporal movement, and <emph>R</emph> is an equivalence relation. So the
                  <emph>extensional</emph> equivalence of these concepts is a necessary and
               sufficient condition that allows us to investigate reality by investigating the
               model: If a language and its translation are equivalent, it should be equally valid
               to investigate one or the other. However, since the translation of the concept
                     <q><emph>movement</emph> of marker</q> into <q><emph>aggregated</emph> marker
                     <emph>coordinates</emph></q> fails to be an <emph>intensionally</emph> adequate
               translation, i.e. the concepts do not <emph>mean</emph> the same (event vs.
               aggregated states of affairs), it could at first seem odd to describe real movement
               in terms of trajectories. But, since the concepts are at least phenomenologically and
               extensionally equivalent, this basically remains a question of the interpreter‚Äôs
               ontology (see <ptr target="#quine1980"/>) and how the final research result is to be
               formulated. If we decide to treat <q>event</q> and <q>aggregated states of
                  affairs</q> as being synonymous, the problem completely disappears. Otherwise, we
               have to re-translate the problematic concept into one which suits our needs. In terms
               of epistemic benefits, one major advantage of the proposed distance-based
               gesture-similarity model (see the following section), i.e. the combination of gesture
               signatures with signature-based distance functions, is its applicability to any type
               of gestural pattern and to data sets of any size. In fact, distance-based similarity
               models can be utilized in order to model similarity between gestural patterns whose
               movement types are well known and between gestural patterns whose inherent structures
               are completely unknown. In this way, they provide an unsupervised way of modeling
               gesture similarity. This flexibility is attributable to the fact that the proposed
               approaches are model independent, i.e. no complex gesture model has to be learned in
               a comprehensive training phase prior to indexing and query processing. Another
               advantage of the proposed distance-based gesture-similarity model is the possibility
               of efficient query processing. Although calculating the distance between two gesture
               signatures is a computationally expensive task, which results in at least a quadratic
               computation time complexity with respect to the number of relevant trajectories, many
               approaches such as the independent minimization lower bound of the Earth Mover's
               Distance on feature signatures <ptr target="#uysal2014"/> and metric indexing <ptr
                  target="#beecks2011"/>, as well as the Ptolemaic indexing <ptr
                  target="#hetland2013"/> of the Signature Quadratic Form Distance, are available
               for efficient query processing and, thus, for assessing gesture similarity in a
               larger quantitative way. </p>
         </div>
         <div>
            <head>Modeling Gesture Similarity</head>
            <p> In this section, we present a distance-based similarity model for the comparison of
               gestures within three-dimensional motion capture data streams. To this end, we first
               introduce <term>gesture signatures</term> as a formal model of gestures arising in
               motion capture data streams. Since gesture signatures comprise multiple
               three-dimensional trajectories, we continue with outlining distance functions for
               trajectories before we investigate distance functions applicable to gesture
               signatures. </p>
            <div>
               <head>Gesture Signatures</head>
               <p> Motion capture data streams can be thought of as sequences of points in a
                  three-dimensional Euclidean space. In the scope of this work, these points arise
                  from several reflective markers which are attached to the body and in particular
                  to the hands of a participant. The motion of the markers is triangulated via
                  multiple cameras and finally recorded every 10 milliseconds. In this way, each
                  marker defines a finite trajectory of points in a three-dimensional space. The
                  formal definition of a trajectory is given below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Trajectory </p>
               <p> Given a three-dimensional feature space R<hi rend="superscript">3</hi>, a
                  trajectory <emph>t</emph>:{1,‚Ä¶,n}‚Üí R<hi rend="superscript">3</hi> is defined for
                  all 1‚â§i‚â§n as: </p>
               <dhq:example>
                  <p>
                     <emph>t</emph>(i)=(x<hi rend="subscript">i</hi>,y<hi rend="subscript"
                        >i</hi>,z<hi rend="subscript">i</hi>) </p>
               </dhq:example>
               <p> A trajectory describes the motion of a single marker in a three-dimensional
                  space. It is worth noting that the time information is abstracted to integral
                  numbers in order to model trajectories arising from different time intervals.
                  Since a gesture typically arises from multiple markers within a certain period of
                  time, we aggregate several trajectories including their individual relevance by
                  means of a gesture signature. For this purpose, we denote the set of all finite
                  trajectories as trajectory space T=‚à™<hi rend="subscript">k‚ààN</hi>{t| t:{1,‚Ä¶,k}‚Üí
                     R<hi rend="superscript">3</hi>} , which is time-invariant, and define a gesture
                  signature as a function from the trajectory space T into the real numbers R. The
                  formal definition of a gesture signature is given below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Gesture Signature </p>
               <p> Let T be a trajectory space. A <term>gesture signature S</term>‚ààR<hi
                     rend="superscript">T</hi> is defined as: </p>
               <dhq:example>
                  <p> S:T‚Üí R subject to |S<hi rend="superscript">-1</hi>(R{0})|&lt;‚àû </p>
               </dhq:example>
               <p> A gesture signature formalizes a gesture by assigning a finite number of
                  trajectories non-zero weights reflecting their importance. Negative weights are
                  immaterial in practice but ensure the gesture space S={<emph>S</emph>‚ààR<hi
                     rend="superscript">T</hi>‚àß|S<hi rend="superscript">-1</hi>(R{0})|&lt;‚àû} forms a
                  vector space. While a weight of zero indicates insignificance of a trajectory, a
                  positive weight is utilized to indicate contribution to the corresponding gesture.
                  In this way, a gesture signature allows us to focus on the trajectories arising
                  from those markers which actually form a gesture. For example, if a gesture is
                  expressed by the participant's hands, only the corresponding hand markers and thus
                  trajectories have to be weighted positively. </p>
               <!-- FIX THE BOX HERE -->
               <p> A gesture signature defines a generic mathematical model but omits a concrete
                  functional implementation. In fact, given a subset of relevant trajectories ùíØ<hi
                     rend="superscript">+</hi>‚äÇT, the most naive way of defining a gesture signature
                     <emph>S</emph> consists in assigning relevant trajectories a weight of one and
                  irrelevant trajectories a weight of zero, i.e. by defining <emph>S</emph> for all
                     <emph>t</emph>‚ààT as follows: </p>
               <!--here goes IMAGE math_img1.png-->
               <figure>
                  <graphic url="resources/images/math_img1.png"/>
               </figure>
               <p> The isotropic behavior of this approach, however, completely ignores the inherent
                  characteristics of the relevant trajectories. We therefore weight each relevant
                  trajectory according to its inherent properties of <term>motion distance</term>
                  and <term>motion variance</term>. These properties are defined below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Motion Distance and Motion Variance </p>
               <p> Let T be a trajectory space and <emph>t</emph>:{1,‚Ä¶,n}‚Üí R<hi rend="superscript"
                     >3</hi> be a trajectory. The <emph>motion distance m</emph><hi rend="subscript"
                     >Œ¥</hi>:T‚ÜíR of trajectory <emph>t</emph> is defined as: </p>
               <!--here goes IMAGE math_img2.png-->
               <figure>
                  <graphic url="resources/images/math_img2.png"/>
               </figure>
               <p> The motion variance m<hi rend="subscript">œÉ<hi rend="superscript">2</hi></hi>:T‚ÜíR
                  of trajectory <emph>t</emph> is defined with mean </p>
               <!--here goes IMAGE math_img3.png-->
               <figure>
                  <graphic url="resources/images/math_img3.png"/>
               </figure>
               <p> as: </p>
               <!--here goes IMAGE math_img4.png-->
               <figure>
                  <graphic url="resources/images/math_img4.png"/>
               </figure>
               <p> The intuition behind motion distance and motion variance is to take into account
                  the overall movement and vividness of a trajectory. The higher these qualities,
                  the more information the trajectory may contain and vice versa. Their utilization
                  with respect to a set of relevant trajectories finally leads to the definitions of
                  a <emph>motion distance gesture signature</emph> and a <emph>motion variance
                     gesture signature</emph>, as shown below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Motion Distance Gesture Signature and Motion
                  Variance Gesture Signature </p>
               <p> Let T be a trajectory space and ùíØ<hi rend="superscript">+</hi>‚äÇT be a subset of
                  relevant trajectories. A motion distance gesture signature S<hi rend="subscript"
                        >m<hi rend="subscript">Œ¥</hi></hi>‚ààR<hi rend="superscript">T</hi> is defined
                  for all <emph>t</emph>‚ààT as: </p>
               <!--here goes IMAGE math_img5.png-->
               <figure>
                  <graphic url="resources/images/math_img5.png"/>
               </figure>
               <p> A motion variance gesture signature S<hi rend="subscript">m<hi rend="subscript"
                           >œÉ<hi rend="superscript">2</hi></hi></hi>‚ààR<hi rend="superscript">T</hi>
                  is defined for all <emph>t</emph>‚ààT as: </p>
               <!--here goes IMAGE math_img6.png-->
               <figure>
                  <graphic url="resources/images/math_img6.png"/>
               </figure>
               <p> Motion distance and motion variance gesture signatures are able to reflect the
                  characteristics of the expressed gestures with respect to the corresponding
                  relevant trajectories by adapting the number and weighting of relevant
                  trajectories. As a consequence, the computation of a (dis)similarity value between
                  gesture signatures is frequently based on the (dis)similarity values among the
                  involved trajectories in the trajectory space. We thus outline applicable
                  trajectory distance functions in the following section. </p>
            </div>
            <div>
               <head>Trajectory Distance Functions</head>
               <p> Due to the nature of trajectories whose inherent properties are rarely
                  expressible in a single figure, trajectories are frequently compared by aligning
                  their coincident similar points with each other. A prominent example is the
                     <term>Dynamic Time Warping Distance</term>, which was first introduced in the
                  field of speech recognition by Itakura <ptr target="#itakura1975"/> and Sakoe and
                  Chiba <ptr target="#sakoe1978"/> and later brought to the domain of pattern
                  detection in databases by Berndt and Clifford <ptr target="#berndt1994"/>. The
                  idea of this distance is to locally replicate points of the trajectories in order
                  to fit the trajectories to each other. The point-wise distances finally yield the
                  Dynamic Time Warping Distance, whose formal definition is given below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Dynamic Time Warping Distance </p>
               <p> Let <emph>t</emph><hi rend="subscript">n</hi>:{1,‚Ä¶,n}‚Üí R<hi rend="superscript"
                     >3</hi> and t<hi rend="subscript">m</hi>:{1,‚Ä¶,m}‚Üí R<hi rend="superscript"
                     >3</hi> be two trajectories from T and <emph>Œ¥</emph>:R<hi rend="superscript"
                     >3</hi>√óR<hi rend="superscript">3</hi>‚ÜíR be a distance function. The
                     <emph>Dynamic Time Warping Distance DTW</emph><hi rend="subscript">Œ¥</hi>:T√óT‚ÜíR
                  between <emph>t</emph><hi rend="subscript">n</hi> and <emph>t</emph><hi
                     rend="subscript">m</hi> is recursively defined as: </p>
               <!--here goes IMAGE math_img7.png-->
               <figure>
                  <graphic url="resources/images/math_img7.png"/>
               </figure>
               <p> with </p>
               <!--here goes IMAGE math_img8.png-->
               <figure>
                  <graphic url="resources/images/math_img8.png"/>
               </figure>
               <p> As can be seen in the definition above, the Dynamic Time Warping Distance is
                  defined recursively by minimizing the distances <emph>Œ¥</emph> between replicated
                  elements of the trajectories. In this way, the distance <emph>Œ¥</emph> assesses
                  the spatial proximity of two points while the Dynamic Time Warping Distance
                  preserves their temporal order within the trajectories. By utilizing Dynamic
                  Programming, the computation time complexity of the Dynamic Time Warping Distance
                  lies in ùí™(n¬∑m). </p>
               <p> Although there exist further approaches for the comparison of trajectories, such
                  as <term>Edit Distance on Real Sequences</term>
                  <ptr target="#chen2005"/>, <term>Minimal Variance Matching</term>
                  <ptr target="#latecki2005"/>, and <term>Mutual Nearest Point Distance</term>
                  <ptr target="#fang2009"/>, we have decided to utilize the Dynamic Time Warping
                  Distance for the following reasons: (i) The distance value is based on all points
                  of the trajectories with respect to their temporal order and is not attributed to
                  partial characteristics of the trajectories, (ii) it provides the ability of exact
                  indexing by lower bounding <ptr target="#keogh2002"/>, and (iii) it indicates
                  superior quality in terms of accuracy within preliminary investigations. </p>
               <p> Given a ground distance in the trajectory space T, we will show in the following
                  section how to lift this ground distance to the gesture space S‚äÇR<hi
                     rend="superscript">T</hi> in order to compare gesture signatures with each
                  other. </p>
            </div>
            <div>
               <head>Gesture Signature Distance Functions</head>
               <p> Gesture signatures can differ in size and length, i.e., in the number of relevant
                  trajectories and in the lengths of those trajectories. In order to quantify the
                  distance between differently structured gesture signatures, we apply
                  signature-based distance functions <ptr target="#beecks2013"/>, <ptr
                     target="#beecks2010"/>. In this paper, we focus on those signature-based
                  distance functions that consider the entire structure of two gesture signatures in
                  order not to favor partial similarity between short and long gesture signatures.
                  For this reason, we investigate the transformation-based <term>Earth Mover's
                     Distance</term>
                  <ptr target="#rubner2000"/> and the correlation-based <term>Signature Quadratic
                     Form Distance</term>
                  <ptr target="#beecks2010"/> in the remainder of this section. </p>
               <p> The Earth Mover's Distance, whose name was inspired by Stolfi and his vivid
                  description of the transportation problem, which he likened to finding the minimal
                  cost to move a total amount of earth from earth hills into holes <ptr
                     target="#rubner2000"/>, has been originated in the computer vision domain. It
                  defines the distance between two gesture signatures by measuring the cost of
                  transforming one gesture signature into another one. The formal definition of the
                  Earth Mover's Distance is given below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Earth Mover‚Äôs Distance </p>
               <p> Let <emph>S</emph><hi rend="subscript">1</hi>,<emph>S</emph><hi rend="subscript"
                     >2</hi>‚ààS be two gesture signatures and <emph>Œ¥</emph>:T√óT‚ÜíR be a trajectory
                  distance function. The <emph>Earth Mover‚Äôs Distance EMD</emph><hi rend="subscript"
                     >Œ¥</hi>:S√óS‚ÜíR between <emph>S</emph><hi rend="subscript">1</hi> and
                     <emph>S</emph><hi rend="subscript">2</hi> is defined as a minimum cost flow of
                  all possible flows <emph>F</emph>={f| f: T√óT‚ÜíR} as: </p>
               <!--here goes IMAGE math_img9.png-->
               <figure>
                  <graphic url="resources/images/math_img9.png"/>
               </figure>
               <p> subject to the constraints: </p>
               <!--here goes IMAGE math_img10.png-->
               <figure>
                  <graphic url="resources/images/math_img10.png"/>
               </figure>
               <p> As can be seen in the definition above, the Earth Mover's Distance between two
                  gesture signatures is defined as a linear optimization problem subject to
                  non-negative flows which do not exceed the corresponding limitations given by the
                  weights of the trajectories of both gesture signatures. The computation of the
                  Earth Mover's Distance can be restricted to the relevant trajectories of both
                  gesture signatures and follows a specific variant of the simplex algorithm <ptr
                     target="#hillier1990"/>. </p>
               <p> The idea of the Signature Quadratic Form Distance consists in adapting the
                  generic concept of <term>correlation</term> to gesture signatures. In general,
                  correlation is the most basic measure of bivariate relationship between two
                  variables <ptr target="#rodgers1988"/> and can be interpreted as the amount of
                  variance these variables share <ptr target="#rovine1997"/>. In order to apply the
                  concept of correlation to gesture signatures, all trajectories and corresponding
                  weights are related with each other based on a trajectory similarity function
                     <emph>s</emph>:T√óT‚ÜíR. The resulting <term>similarity correlation</term> between
                  two gesture signatures <emph>S</emph><hi rend="subscript">1</hi>,<emph>S</emph><hi
                     rend="subscript">2</hi>‚ààS is then defined as: </p>
               <!--here goes IMAGE math_img11.png-->
               <figure>
                  <graphic url="resources/images/math_img11.png"/>
               </figure>
               <p> The similarity correlation between two gesture signatures finally leads to the
                  definition of the Signature Quadratic Form Distance, as shown below. </p>
               <p>
                  <hi rend="bold">Definition:</hi> Signature Quadratic Form Distance </p>
               <p> Let <emph>S</emph><hi rend="subscript">1</hi>,<emph>S</emph><hi rend="subscript"
                     >2</hi>‚ààS be two gesture signatures and <emph>s</emph>:T√óT‚ÜíR be a trajectory
                  similarity function. The <emph>Signature Quadratic Form Distance SQFD</emph><hi
                     rend="subscript">s</hi>:S√óS‚ÜíR between <emph>S</emph><hi rend="subscript">1</hi>
                  and <emph>S</emph><hi rend="subscript">2</hi> is defined as: </p>
               <!--here goes IMAGE math_img12.png-->
               <figure>
                  <graphic url="resources/images/math_img12.png"/>
               </figure>
               <p> The Signature Quadratic Form Distance is defined by adding the intra-similarity
                  correlations &lt;<emph>S</emph><hi rend="subscript">1</hi>,<emph>S</emph><hi
                     rend="subscript">1</hi>&gt;<hi rend="subscript">s</hi> and
                     &lt;<emph>S</emph><hi rend="subscript">2</hi>,<emph>S</emph><hi
                     rend="subscript">2</hi>&gt;<hi rend="subscript">s</hi> of the gesture
                  signatures <emph>S</emph><hi rend="subscript">1</hi> and <emph>S</emph><hi
                     rend="subscript">2</hi> and subtracting their inter-similarity correlation
                     &lt;<emph>S</emph><hi rend="subscript">1</hi>,<emph>S</emph><hi
                     rend="subscript">2</hi>&gt;<hi rend="subscript">s</hi>. The smaller the
                  differences among the intra-similarity and inter-similarity correlations the lower
                  the resulting Signature Quadratic Form Distance, and vice versa. The computation
                  of the Signature Quadratic Form Distance can be restricted to the relevant
                  trajectories of both gesture signatures and has a quadratic computation time
                  complexity with respect to the number of relevant trajectories. </p>
               <p> More details regarding the Earth Mover's Distance and the Signature Quadratic
                  Form Distance as well as possible similarity functions can be found for instance
                  in the PhD thesis of Beecks <ptr target="#beecks2013b"/>. Among other approaches,
                  such as the matching-based Signature Matching Distance <ptr target="#beecks2013"
                  />, the aforementioned signature-based distance functions have been shown to
                  balance the trade-off between retrieval accuracy and query processing efficiency.
                  Before we investigate their performance in the context of gesture signatures, we
                  devote the next section to a discussion about the properties of the proposed
                  distance-based gesture similarity model. </p>
            </div>
         </div>
         <div>
            <head>Experimental Evaluation</head>
            <p> Evaluating the performance of distance-based similarity models is a highly empirical
               discipline. It is nearly unforeseeable which approach will provide the best retrieval
               performance in terms of accuracy. To this end, we qualitatively evaluated the
               proposed distance-based approaches to gesture similarity by using a natural media
               corpus of motion capture data collected for this project. This dataset comprises
               three-dimensional motion capture data streams arising from eight participants during
               a guided conversation. The participants were equipped with a multitude of reflective
               markers which were attached to the body and in particular to the hands. The motion of
               the markers was tracked optically via cameras at a frequency of 100 Hz. In the scope
               of this work, we used the right wrist marker and two markers attached to the right
               thumb and right index finger each. The gestures arising within the conversation were
               classified by domain experts according to the following types of movement: spiral,
               circle, and straight. Example gestures of these movement types are sketched in Figure
               1. A total of 20 gesture signatures containing five trajectories each was obtained
               from the motion capture data streams. The trajectories of the gesture signatures have
               been normalized to the interval [0,1]<hi rend="superscript">3</hi>‚ààR<hi
                  rend="superscript">3</hi> in order to maintain translation invariance. </p>
            <p> The resulting distance matrices between all gesture signatures with respect to the
               Earth Mover's Distance and the Signature Quadratic Form Distance are shown in Figure
               2 and Figure 3, respectively. As described in the previous Section, we utilized the
               Dynamic Time Warping Distance based on Euclidean Distance as trajectory distance for
               the Earth Mover's Distance and converted this trajectory distance by means of the
               power kernel <ptr target="#schoelkopf2001"/> with parameter <emph>Œ±</emph>=1 into a
               trajectory similarity function for the Signature Quadratic Form Distance. Since
               weighting of relevant trajectories by motion distance and motion variance,
               approximately shows a similar behavior, we include the results regarding motion
               variance gesture signatures only. We depict small and large distance values by bluish
               and reddish colors in order to visually indicate the performance of our proposal:
               gesture signatures from the same movement type should result in bluish colors while
               gesture signatures from different movement types should result in reddish colors. </p>
            <p> As can be seen in Figure 2 and Figure 3, both Earth Mover's Distance and Signature
               Quadratic Form Distance show the same tendency in terms of gestural dissimilarity.
               Although distance values computed through the aforementioned distance functions have
               different orders of magnitude, both gesture signature distance functions are
               generally able to distinguish gesture signatures from different movement types. On
               average, gesture signatures belonging to the same movement type are less dissimilar
               to each other than gesture signatures from different movement types. We further
               observed that the distinction between gesture signatures from the movement types
               spiral and straight are most challenging. This is caused by a similar sequence of
               movement of these two gestural types. While gesture signatures belonging to the
               movement type straight follow a certain direction, e.g., movement on the horizontal
               axis, gesture signatures from the movement type spiral additionally oscillate with
               respect to a certain direction. Since this oscillation can be dominated by the
               movement direction, the underlying trajectory distance functions are often unable to
               distinguish oscillating from non-oscillating trajectories and thus gesture signature
               of movement type spiral from those of movement type straight. </p>
            <p> Apart from the quality of accuracy, efficiency is another important aspect when
               evaluating the performance of gesture similarity models. For this purpose, we
               measured the computation times needed to perform single distance computations on a
               single-core 3.4 GHz machine. We implemented the proposed distance-based approaches in
               Java 1.7. The Earth Mover's Distance, which needs on average 148.6 milliseconds for a
               single distance computation, is approximately three times faster than the Signature
               Quadratic Form Distance, which needs on average 479.8 milliseconds for a single
               distance computation. In spite of the theoretically exponential and empirically
               super-cubic computation time complexity of the Earth Mover's Distance <ptr
                  target="#uysal2014"/>, this distance is able to outperform the Signature Quadratic
               Form Distance. The reason for this is the high number of computationally expensive
               trajectory distance computations. While the computation of the Earth Mover's Distance
               is carried out on the trajectory distances between the two gesture signatures, the
               computation of the Signature Quadratic Form Distance additionally takes into account
               the trajectory distances within both gesture signatures. Therefore the number of
               trajectory distance computations is significantly higher for the Signature Quadratic
               Form Distance than for the Earth Mover's Distance. </p>
            <p> To sum up, the experimental evaluation reveals that the proposed distance-based
               approaches are able to model gesture similarity in a flexible and model-independent
               way. Without the need for a preceding training phase, the Earth Mover's Distance and
               the Signature Quadratic Form Distance are able to provide similarity models for
               searching similar gestures which are formalized through gesture signatures. </p>
            <!--here goes FIGURE 2-->
            <figure>
               <graphic url="resources/images/figure02.png"/>
            </figure>
            <!--here goes FIGURE 3-->
            <figure>
               <graphic url="resources/images/figure03.png"/>
            </figure>
         </div>
         <div>
            <head>Conclusions and Future Work </head>
            <p> In this paper, we have investigated distance-based approaches to measure similarity
               between gestures arising in three-dimensional motion capture data streams. To this
               end, we have explicated gesture signatures as a way of aggregating the inherent
               characteristics of spontaneously produced co-speech gestures and signature-based
               distance functions such as the Earth Mover's Distance and the Signature Quadratic
               Form Distance in order to quantify dissimilarity between gesture signatures. The
               experiments conducted on real data are evidence of the appropriateness in terms of
               accuracy and efficiency of the proposal. </p>
            <p> In future work, we intend to extend our research on gesture similarity towards
               indexing and efficient query processing. While the focus of the present paper lies on
               dissimilarity between pairs of gestures, we further plan to quantitatively analyze
               motion capture data streams in a query-driven way in order to support the domain
               experts' qualitative analyses of gestural patterns within multi-media contexts. The
               overall goal of this research is to contribute to the advancement of automated
               methods of pattern recognition in gesture research by enhancing qualitative analyses
               of complex multimodal data in the humanities and social sciences. While this paper
               focuses on formal features of the gestural movements, further steps will entail
               examining the semantic and pragmatic dimensions of these patterns in light of the
               cultural contexts and embodied semiotic practices they emerge from. </p>
         </div>
         <div>
            <head>Acknowledgment</head>
            <p> This work is partially funded by the Excellence Initiative of the German federal and
               state governments and DFG grant SE 1039/7-1. This work extends <ptr
                  target="#beecks2015"/>. </p>
         </div>
      </body>
      <back>
         <listBibl>
            <!-- Encode each bibliographic item as <bibl>, and delete dummy record below. @xml:id and @label are required. All values for @xml:id should be lower-case. -->
            <bibl xml:id="bavelas1992" label="Bavelas 1992">Bavelas, J. B., Chovil, N., Lawrie, D.
               A. and Wade, A. <title rend="quotes">Interactive Gestures</title>, <title
                  rend="italic">Discourse Processes</title>, 15 (1992): 469-489.</bibl>
            <bibl xml:id="beecks2010" label="Beecks 2010">Beecks, C., M.S. Uysal, and Seidl, T.
                  <title rend="quotes">A Comparative Study of Similarity Measures for Content-Based
                  Multimedia Retrieval</title>. <title rend="italic">Proceedings of the IEEE
                  International Conference on Multimedia and Expo</title>, pp. 1552-1557
               (2010).</bibl>
            <bibl xml:id="beecks2010a" label="Beecks 2010a">Beecks, C., M.S. Uysal, and Seidl, T.
                  <title rend="quotes"> Signature Quadratic Form Distance</title>. <title
                  rend="italic">Proceedings of the 9th International Conference on Image and Video
                  Retrieval</title>, pp. 438-445 (2010).</bibl>
            <bibl xml:id="beecks2011" label="Beecks 2011">Beecks, C., Lokoc, J., Seidl, T., and
               Skopal, T. <title rend="quotes">Indexing the signature quadratic form distance for
                  efficient content-based multimedia retrieval</title>. In <title rend="italic"
                  >Proceedings of the ACM International Conference on Multimedia Retrieval</title>,
               2011, pp. 24:1‚Äì8.</bibl>
            <bibl xml:id="beecks2013" label="Beecks 2013">Beecks, C., S. Kirchhoff, and Seidl, T.
                  <title rend="quotes">On Stability of Signature-Based Similarity Measures for
                  Content-Based Image Retrieval</title>. <title rend="italic">Multimedia Tools and
                  Applications</title>, pp. 1-14.</bibl>
            <bibl xml:id="beecks2013a" label="Beecks 2013a">Beecks, C., S. Kirchhoff, and Seidl, T.
                  <title rend="quotes">Signature matching distance for content-based image
                  retrieval</title>. <title rend="italic">Proceedings of the ACM International
                  Conference on Multimedia Retrieval</title>, pp. 41-48 (2013)</bibl>
            <bibl xml:id="beecks2013b" label="Beecks 2013b">Beecks, C. <title rend="italic"
                  >Distance-Based Similarity Models for Content-Based Multimedia Retrieval</title>.
               Ph.D. thesis, RWTH Aachen University (2013). Available online: <ref
                  target="http://darwin.bth.rwth-aachen.de/opus3/volltexte/2013/4807/"
                  >http://darwin.bth.rwth-aachen.de/opus3/volltexte/2013/4807/</ref></bibl>
            <bibl xml:id="beecks2015" label="Beecks 2015">Beecks, C., Hassani, M., Hinnell, J.,
               Sch√ºller, D., Brenger, B., Mittelberg, I. and Seidl, T. <title rend="quotes"
                  >Spatiotemporal Similarity Search in 3D Motion Capture Gesture Streams</title>.
                  <title rend="italic">Proceedings of the 14th International Symposium on Spatial
                  and Temporal Databases</title>, pp. 355-372 (2015).</bibl>
            <bibl xml:id="beecks2015a" label="Beecks 2015a">Beecks, C., J. Lokoc, T. Seidl, and
               Skopal, T. <title rend="quotes">Indexing the Signature Quadratic Form Distance for
                  Efficient Content-Based Multimedia Retrieval</title>. <title rend="italic"
                  >Proceedings of the ACM International Conference on Multimedia Retrieval</title>,
               pp. 24:1-8 (2015).</bibl>
            <bibl xml:id="berndt1994" label="Berndt 1994">Berndt, D. and Clifford, J. <title
                  rend="quotes">Using dynamic time warping to find patterns in time series</title>.
                  <title rend="italic">AAAI94 Workshop on Knowledge Discovery in Databases</title>,
               pp. 359-370 (1994)</bibl>
            <bibl xml:id="beskow2011" label="Beskow 2011">Beskow, J., Alex, S., Al Moubayed, S.,
               Edlund, J. and House, D. <title rend="quotes">Kinetic Data for Large-Scale Analysis
                  and Modeling of Face-to-Face Conversation</title>. In <title rend="italic"
                  >Proceedings of the International Conference on Audio-Visual Speech
                  Processing</title>, Stockholm, pp. 103‚Äì106 (2011).</bibl>
            <bibl xml:id="bourdieu1987" label="Bourdieu 1987">Bourdieu, P. Sozialer Sinn. <title
                  rend="italic">Kritik der theoretischen Vernunft</title>. Suhrkamp, Frankfurt am
               Main (1987).</bibl>
            <bibl xml:id="bressem2013" label="Bressem 2013">Bressem, J. <title rend="quotes">A
                  Linguistic Perspective on the Notation of Form Features in Gestures</title>. In
               M√ºller, C. et al. (eds): <title rend="italic">Body ‚Äì Language ‚Äì Communication. An
                  International Handbook of Multimodality in Human Interaction</title>. (HSK 38.1).
               De Gruyter Mouton, Berlin/Boston,pp. 1079‚Äì1098 (2013).</bibl>
            <bibl xml:id="chen2005" label="Chen 2005">Chen, L., √ñzsu, M.T. and Oria, V. <title
                  rend="quotes">Robust and Fast Similarity Search for Moving Object
                  Trajectories</title>. <title rend="italic">Proceedings of the ACM SIGMOD
                  International Conference on Management of Data</title>, pp. 491-502 (2005).</bibl>
            <bibl xml:id="cienki2005" label="Cienki 2005">Cienki, A. <title rend="quotes">Image
                  Schemas and Gesture</title>. In Hampe, B. (ed). From <title rend="italic"
                  >Perception to Meaning: Image Schemas in Cognitive Linguistics</title>. Mouton de
               Gruyter, Berlin/New York (2005).</bibl>
            <bibl xml:id="cienki2013" label="Cienki 2013">Cienki, A. <title rend="quotes">Cognitive
                  Linguistics: Spoken Language and Gesture as Expressions of
                  Conceptualization</title>. In M√ºller, C., Cienki, A., Fricke, E., Ladewig, S.,
               McNeill, D. and Te√üendorf, S. (eds), <title rend="italic">Body‚Äì
                  Language‚ÄìCommunication: An International Handbook on Multimodality in Human
                  Interaction</title>, Vol. 1. Mouton de Gruyter, Berlin/New York, pp. 182‚Äì201
               (2013).</bibl>
            <bibl xml:id="duncan2007" label="Duncan 2007">Duncan, S., Cassell, J. and Levy E.T.
               (eds), <title rend="italic">Gesture and the Dynamic Dimension of Language</title>.
               John Benjamins, Amsterdam/ Philadelphia, pp. 269‚Äì283 (2007).</bibl>
            <bibl xml:id="enfield2009" label="Enfield 2009">Enfield, N. <title rend="italic">The
                  Anatomy of Meaning. Speech, Gestures, and Composite Utterances</title>. Cambridge
               University Press, Cambridge (2009).</bibl>
            <bibl xml:id="fang2009" label="Fang 2009">Fang, S. and Chan, H. <title rend="quotes"
                  >Human Identification by Quantifying Similarity and Dissimilarity in
                  Electrocardiogram Phase Space</title>. <title rend="italic">Pattern
                  Recognition</title>, vol. 42, 9 (2009): 1824-1831.</bibl>
            <bibl xml:id="fricke2010" label="Fricke 2010">Fricke, E. <title rend="quotes"
                  >Phonaestheme, Kinaestheme und Multimodale Grammatik</title>. <title rend="italic"
                  >Sprache und Literatur</title>, 41 (2010): 69‚Äì88.</bibl>
            <bibl xml:id="gibbs2006" label="Gibbs 2006">Gibbs, R. W., Jr. <title rend="italic"
                  >Embodiment and Cognitive Science</title>. Cambridge University Press, Cambridge
               (2006).</bibl>
            <bibl xml:id="goldin-meadow2003" label="Goldin-Meadow 2003">Goldin-Meadow, S. <title
                  rend="italic">Hearing Gesture: How Our Hands Help Us Think</title>. Harvard
               University Press, Cambridge, MA (2003).</bibl>
            <bibl xml:id="goldin-meadow2007" label="Goldin-Meadow 2007">Goldin-Meadow, S. <title
                  rend="quotes">Gesture with Speech and Without It</title>. In Duncan, S. (ed),
                  <title rend="italic">Gesture and the Dynamic Dimension of Language: Essays in
                  Honor of David McNeill</title>. John Benjamins Publishing Company,
               Amsterdam/Philadelphia, pp. 31‚Äì49 (2007).</bibl>
            <bibl xml:id="gonzalez-marquez2007" label="Gonzalez-Marquez 2007">Gonzalez-Marquez, M.,
               Mittelberg, I., Coulson, S. and Spivey, M.J. (eds). <title rend="italic">Methods in
                  Cognitive Linguistics</title>. John Benjamins, Amsterdam/Philadelphia
               (2007).</bibl>
            <bibl xml:id="hetland2013" label="Hetland 2013">Hetland, M.L., Skopal, T., Lokoc, J. and
               Beecks, C. <title rend="quotes">Ptolemaic Access Methods: Challenging the Reign of
                  the Metric Space Model</title>. <title rend="italic">Information Systems</title>,
               vol.38, no.7 (2013): 989-1006.</bibl>
            <bibl xml:id="hillier1990" label="Hillier 1990">Hillier, F. and Lieberman, G.. <title
                  rend="italic">Introduction to Linear Programming</title>.
               McGraw-Hill.(1990)</bibl>
            <bibl xml:id="itakura1975" label="Itakura 1975">Itakura, F. <title rend="quotes">Minimum
                  Prediction Residual Principle Applied to Speech Recognition</title>. <title
                  rend="italic">IEEE Transactions on Acoustics, Speech and Signal
               Processing</title>, vol.23, no.1 (1975): 67-72.</bibl>
            <bibl xml:id="jaeger2004" label="J√§ger 2004">J√§ger, L. &amp; Linz, E. (eds) <title
                  rend="italic">Medialit√§t und Mentalit√§t. Theoretische und empirische Studien zum
                  Verh√§ltnis von Sprache, Subjektivit√§t und Kognition</title>. Fink Verlag, M√ºnchen
               (2004).</bibl>
            <bibl xml:id="johnson1987" label="Johnson 1987">Johnson, M. <title rend="italic">The
                  Body in the Mind: The Bodily Basis of Meaning, Imagination, and Reason</title>.
               University of Chicago Press, Chicago (1987).</bibl>
            <bibl xml:id="kendon1972" label="Kendon 1972">Kendon, A. <title rend="quotes">Some
                  Relationships between Body Motion and Speech. An analysis of an example</title>.
               In Siegman, A. and Pope, B. (eds), <title rend="italic">Studies in Dyadic
                  Communication</title>. Pergamon Press, Elmsford, NY, pp. 177‚Äì210 (1972).</bibl>
            <bibl xml:id="kendon2004" label="Kendon 2004">Kendon, A. <title rend="italic">Gesture:
                  Visible Action as Utterance</title>. Cambridge University Press, Cambridge
               (2004).</bibl>
            <bibl xml:id="keogh2002" label="Keogh 2002">Keogh, E. <title rend="quotes">Exact
                  Indexing of Dynamic Time Warping</title>. <title rend="italic">Proceedings of 28th
                  International Conference on Very Large Data Bases</title>, 406-417 (2002).</bibl>
            <bibl xml:id="ladewig2011" label="Ladewig 2011">Ladewig, S. <title rend="quotes">Putting
                  the cyclic gesture on a cognitive basis</title>. <title rend="italic"
                  >CogniTextes</title>, 6 (2011).</bibl>
            <bibl xml:id="latecki2005" label="Latecki 2005">Latecki, L.J., Megalooikonomou, V.,
               Wang, Q., Lak√§mper, R., Ratanamahatana, C.A. and Keogh, E.J. <title rend="quotes"
                  >Elastic Partial Matching of Time Series.Knowledge Discovery in Databases</title>,
                  <title rend="italic">9th European Conference on Principles and Practice of
                  Knowledge Discovery in Databases, Lecture Notes in Computer Science</title>, vol.
               3721 (2005), Springer, pp. 577-584.</bibl>
            <bibl xml:id="lu2010" label="Lu 2010">Lu, P. and Huenerfauth, M. <title rend="quotes"
                  >Collecting a Motion-Capture Corpus of American Sign Language for Data-Driven
                  Generation Research</title>. In <title rend="italic">Proceedings of the NAACL HLT
                  2010 Workshop on Speech and Language Processing for Assistive
               Technologies</title>. Los Angeles, pp. 89‚Äì97 (2010).</bibl>
            <bibl xml:id="mcneill1985" label="McNeill 1985">McNeill, D. <title rend="quotes">So You
                  Think Gestures are Nonverbal?</title>. <title rend="italic">Psychological
                  Review</title>, 92,3 (1985): 350‚Äì371.</bibl>
            <bibl xml:id="mcneill1992" label="McNeill 1992">McNeill, D. <title rend="italic">Hand
                  and Mind: What Gestures Reveal about Thought</title>. Chicago University Press,
               Chicago (1992).</bibl>
            <bibl xml:id="mcneill2005" label="McNeill 2005">McNeill, D. Gesture and Thought. Chicago
               University Press, Chicago (2005).</bibl>
            <bibl xml:id="mcneill2012" label="McNeill 2012">McNeill, D. <title rend="italic">How
                  Language Began: Gesture and Speech in Human Evolution</title>. Cambridge
               University Press, Cambridge (2012).</bibl>
            <bibl xml:id="mcneill2000" label="McNeill 2000">McNeill, D. (ed). <title rend="italic"
                  >Language and Gesture</title>. Cambridge University Press, Cambridge
               (2000).</bibl>
            <bibl xml:id="mcneill2001" label="McNeill 2001">McNeill, D., Quek, F., McCullough,
               K.-E., Duncan, S., Furuyama, N., Bryll, R., Ma, X.-F. and Ansari, R. 2001. <title
                  rend="quotes">Catchments, Prosody and Discourse</title>, <title rend="italic"
                  >Gesture</title> 1, 1 (2001): 9‚Äì33.</bibl>
            <bibl xml:id="mittelberg2010" label="Mittelberg 2010">Mittelberg, I. <title
                  rend="quotes">Geometric and Image-Schematic Patterns in Gesture Space</title>. In
               Evans, V. and Chilton, P. (eds), <title rend="italic">Language, Cognition, and Space:
                  The State of the Art and New Directions</title>. Equinox, London, pp., 351‚Äì385
               (2010).</bibl>
            <bibl xml:id="mittelberg2010a" label="Mittelberg 2010a">Mittelberg, I. <title
                  rend="quotes">Interne und externe Metonymie: Jakobsonsche Kontiguit√§tsbeziehungen
                  in redebegleitenden Gesten</title>. <title rend="italic">Sprache und
                  Literatur</title> 41,1 (2010): 112‚Äì143.</bibl>
            <bibl xml:id="mittelberg2013" label="Mittelberg 2013">Mittelberg, I. <title
                  rend="quotes">Balancing Acts: Image Schemas and Force Dynamics as Experiential
                  Essence in Pictures by Paul Klee and their Gestural Enactments</title>. In B.
               Dancygier, M. Bokrent and J. Hinnell (eds), <title rend="italic">Language and the
                  Creative Mind</title>. Stanford: Center for the Study of Language and Information,
               pp. 325‚Äì346.</bibl>
            <bibl xml:id="mittelberg2013a" label="Mittelberg 2013a">Mittelberg, I. <title
                  rend="quotes">The Exbodied Mind: Cognitive-Semiotic Principles as Motivating
                  Forces in Gesture</title>. In M√ºller, C., Cienki, A., Fricke, E., Ladewig, S.H.,
               McNeill, D. and Te√üendorf, S. (eds). <title rend="italic">Body ‚Äì Language ‚Äì
                  Communication: An International Handbook on Multimodality in Human
                  Interaction</title>. Handbooks of Linguistics and Communication Science (38.1).
               Mouton de Gruyter,Berlin/New York, pp. 750-779 (2013).</bibl>
            <bibl xml:id="mittelberg2014" label="Mitteberg 2014">Mittelberg, I. and Waugh, L.R.
                  <title rend="quotes">Gestures and Metonymy</title>. In M√ºller, C., Cienki, A.,
               Fricke, E., Ladewig, S.H., McNeill, D. and Bressem, J. (eds), <title rend="italic"
                  >Body ‚Äì Language ‚Äì Communication. An International Handbook on Multimodality in
                  Human Interaction</title>. Vol. 2. Handbooks of Linguistics and Communcation
               Science. De Gruyter Mouton, Berlin/Boston, pp. 1747‚Äì1766 (2014).</bibl>
            <bibl xml:id="mittelberg2016" label="Mittelberg 2016">Mittelberg, I., Sch√ºller, D.
                  <title rend="quotes">Kulturwissenschaftliche Orientierung in der
                  Gestenforschung</title>. In J√§ger, L., Holly, W., Krapp, P., Weber, S. (eds.).
                  <title rend="italic">Language ‚Äì Culture ‚Äì Communication. An International Handbook
                  of Linguistics as Cultural Study</title>. Mouton de Gruyter, Berlin/New York, pp.
               879-890 (2016).</bibl>
            <bibl xml:id="mueller1998" label="M√ºller 1998">M√ºller, C. Redebegleitende Gesten. <title
                  rend="italic">Kulturgeschichte - Theorie - Sprachvergleich</title>. Berlin Verlag
               A. Spitz, Berlin (1998).</bibl>
            <bibl xml:id="mueller2008" label="M√ºller 2008">M√ºller, C. <title rend="italic">Metaphors
                  Dead and Alive, Sleeping and Waking. A Dynamic View</title>. University of Chicago
               Press, Chicago (2008).</bibl>
            <bibl xml:id="mueller2010" label="M√ºller 2010">M√ºller, C. <title rend="quotes">Wie
                  Gesten bedeuten. Eine kognitiv-linguistische und sequenzanalytische
                  Perspektive</title>, <title rend="italic">Sprache und Literatur</title> 41 (2010):
               37‚Äì68.</bibl>
            <bibl xml:id="mueller2013" label="M√ºller 2013">M√ºller, C., Cienki, A., Fricke, E.,
               Ladewig, S., McNeill, D. and Te√üendorf, S. (eds). <title rend="italic">Body‚Äì
                  Language‚ÄìCommunication: An International Handbook on Multimodality in Human
                  Interaction</title>, Vol. 1. Mouton de Gruyter, Berlin/New York (2013).</bibl>
            <bibl xml:id="mueller2014" label="M√ºller 2014">M√ºller, C., Cienki, A., Fricke, E.,
               Ladewig, S., McNeill, D. and Bressem, J.(eds). <title rend="italic">Body‚Äì
                  Language‚ÄìCommunication: An International Handbook on Multimodality in Human
                  Interaction</title>, Vol. 2. Mouton de Gruyter, Berlin/New York (2014).</bibl>
            <bibl xml:id="pfeiffer2013" label="Pfeiffer 2013">Pfeiffer, T. <title rend="quotes"
                  >Documentation with Motion Capture</title>. In M√ºller, C., Cienki, A., Fricke, E.,
               Ladewig, S.H., McNeill, D. and Te√üendorf, S. (eds). <title rend="italic">Body-
                  Language-Communication: An International Hand- book on Multimodality in Human
                  Interaction</title>, Hand- books of Linguistics and Communication Science. Mouton
               de Gruyter, Berlin, New York (2013).</bibl>
            <bibl xml:id="pfeiffer2013a" label="Pfeiffer 2013a">Pfeiffer, T., Hofmann, F., Hahn, F.,
               Rieser, H., and R√∂pke, I. <title rend="quotes">Gesture Semantics Reconstruction Based
                  on Motion Capturing and Complex Event Processing: A Circular Shape
               Example</title>. In M. Eskenazi, M. Strube, B. Di Eugenio, and J. D. Williams (eds).
                  <title rend="italic">Proceedings of the Special Interest Group on Discourse and
                  Dialog (SIGDIAL) 2013 Conference</title>, pp. 270‚Äì279 (2013).</bibl>
            <bibl xml:id="quine1980" label="Quine 1980">Quine, W.V.O. <title rend="italic">Wort und
                  Gegenstand</title>. Reclam, Stuttgart (1980).</bibl>
            <bibl xml:id="rieser2012" label="Rieser 2012">Rieser H., Bergmann K. and Kopp, S. <title
                  rend="quotes">How do Iconic Gestures Convey Visuo-Spatial Information? Bringing
                  Together Empirical, Theoretical, and Simulation Studies.</title> In Efthimiou, E.
               and Kouroupetroglou, G. (eds). <title rend="italic">Gestures in Embodied
                  Communication and Human-Computer Interaction</title>. Springer, Berlin/Heidelberg
               (2012).</bibl>
            <bibl xml:id="rodgers1988" label="Rodgers 1988">Rodgers, J. and Nicewander, W. <title
                  rend="italic">Thirteen Ways to Look at the Correlation Coefficient</title>.
               American Statistician, pp. 59-66 (1988).</bibl>
            <bibl xml:id="rovine1997" label="Rovine 1997">Rovine, M. and Von Eye, A. <title
                  rend="quotes">A 14th Way to Look at a Correlation Coefficient: Correlation as the
                  Poportion of Matches</title>. <title rend="italic">American Statistician</title>,
               pp. 42-46 (1997)</bibl>
            <bibl xml:id="rubner2000" label="Rubner 2000">Rubner, Y., C. Tomasi, and Guibas, L.J.
                  <title rend="quotes">The Earth Mover's Distance as a Metric for Image
                  Retrieval</title>, <title rend="italic">International Journal of Computer
                  Vision</title>, vol. 40,2 (2000): 99-121.</bibl>
            <bibl xml:id="sakoe1978" label="Sakoe 1978">Sakoe, H. and Chiba, S. <title rend="quotes"
                  >Dynamic Programming Algorithm Optimization for Spoken Word Recognition</title>,
                  <title rend="italic">IEEE Transactions on Acoustics, Speech and Signal
                  Processing</title>, vol. 26, 1(1978): 43-49.</bibl>
            <bibl xml:id="schoelkopf2001" label="Sch√∂lkopf 2001">Sch√∂lkopf, B. <title rend="quotes"
                  >The Kernel Trick for Distances</title>. <title rend="italic">Advances in Neural
                  Information Processing Systems</title>, 301-307.</bibl>
            <bibl xml:id="steen2013" label="Steen 2013">Steen, L. and Turner, M. <title
                  rend="quotes">Multimodal Construction Grammar</title>. In Dancygier, B., Bokrent,
               M. and Hinnell, J. (eds). <title rend="italic">Language and the Creative Mind,
                  Stanford: Center for the Study of Language and Information</title> (2013).</bibl>
            <bibl xml:id="streeck2011" label="Streeck 2011">Streeck, J., Goodwin, C. and LeBaron,
               C.D. <title rend="italic">Embodied Interaction: Language and Body in the Material
                  World: Learning in doing: Social, Cognitive and Computational
               Perspectives</title>. Cambridge University Press, New York (2011).</bibl>
            <bibl xml:id="sweetser2007" label="Sweetser 2007">Sweetser, E. <title rend="quotes"
                  >Looking at Space to Study Mental Spaces: Co-speech Gesture as a Crucial Data
                  Source in Cognitive Linguistics</title>. In Gonzalez-Marquez, M., Mittelberg, I.,
               Coulson, S. and Spivey, M. (eds). <title rend="italic">Methods in Cognitive
                  Linguistics</title>. John Benjamins, Amsterdam/Philadelphia, pp. 201¬¨224
               (2007).</bibl>
            <bibl xml:id="tomasello1999" label="Tomasello 1999">Tomasello, M. <title rend="italic"
                  >The Cultural Origins of Human Cognition</title>. Harvard University Press,
               Cambridge, MA (1999).</bibl>
            <bibl xml:id="uysal2014" label="Uysal 2014">Uysal, M.S., Beecks, C., Schm√ºcking, J., and
               Seidl, T. <title rend="quotes">Efficient Filter Approximation using the Earth Mover's
                  Distance in Very Large Multimedia Databases with Feature Signatures</title>.
                  <title rend="italic">Proceedings of the 23rd ACM International Conference on
                  Conference on Information and Knowledge Management</title>, pp. 979-988
               (2014).</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
