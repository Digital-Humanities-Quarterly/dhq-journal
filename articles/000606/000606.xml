<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English-->Universal
               Dependencies and Author Attribution of Short Texts with Syntax Alone</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Robert <dhq:family>Gorman</dhq:family>
               </dhq:author_name>
               <!--<idno type="ORCID"
                  ><!-\-if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000-\-></idno>-->
               <dhq:affiliation>Department of Classics and Religious Studies, University of
                  Nebraska-Lincoln</dhq:affiliation>
               <email>rgorman1@unl.edu</email>
               <dhq:bio>
                  <p>Robert Gorman is an associate professor of Classics at the University of
                     Nebraska-Lincoln. In his current research, he focuses on developing stylometric
                     profiles of ancient Greek prose authors by drawing from the morpho-syntactic
                     annotation in dependency treebanks. He is particularly interested in
                     investigating how much stylometric variation may occur among works of a single
                     author or even within a single work.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000606</idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->016</idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->2</idno>
            <date when="2022-04-01">01 April 2022</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change when="2022-08-03" who="BRG">resolved encoding errors in body and back
            matter</change>
         <change>The version history for this file can be found on <ref
               target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000606/000606.xml"
               >GitHub </ref></change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>Improving methods of stylometrics and classification so that they give good results
               with small texts is the focus of much research in the digital humanities and in the
               NLP community more generally. Recent work <ptr target="#gorman2020"/> has suggested
               that an approach using combinations of shallow and deep morpho-syntactic information
               can be quite successful. But because the data in that study were taken from hand
               annotated dependency treebanks, the wider applicability of such an approach remains
               in question. The present paper seeks to answer this question by using
               machine-generated morphological and syntactic annotations as the basis for a
               closed-set classification experiment. Texts were parsed according to the Universal
               Dependency schema using the <q>udpipe</q> package for R. Experiments were carried out
               on data from several languages covering a range of morphological complexity. To limit
               confounders, consideration of vocabulary was excluded. Results were quite promising,
               and, not surprisingly, a more complex morphology correlates with better accuracy
               (e.g., 100-token texts in Polish: 88% correct; 100-token texts in English: 74%).The
               method presented here has particular advantages for stylometrics as practiced in
               literary analysis and other fields in the humanities. The Universal Dependency
               annotation categories are generally similar to those used in traditional grammars.
               Thus, the variables which serve to distinguish the style of a given author are
               relatively easier to interpret and understand than, for example, are character
               n-grams or function words. This fact, combined with the availability of easy-to-use
               dependency parsers, opens up the study of a syntax-centered stylometrics to persons
               with a wide range of expertise. Even students at the early stages of their studies
               can identify and investigate the morpho-syntactic <q>signature</q> of a particular
               author. Therefore, the characterization of texts based on computational annotation of
               this type deserves a place in classification studies because of its combination of
               good results and good interpretability.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>Exploring the wider applicability of machine-generated morphological and syntactic
               annotation methods of stylometrics and classification, so that they give good results
               with small texts.</p>
         </dhq:teaser>
      </front>
      <body>

         <div>
            <head>1. Introduction</head>
            <p>Developing better methods of classifying short texts is becoming increasingly
               important in the field of computational stylometrics. Synoptic work (Eder 2015)
               suggests that many common approaches to at least one kind of text classification —
               Authorship Attribution — are unreliable when the text to be classified is less than a
               few thousand words. Maciej Eder’s study examines several classifiers (Burrows’ Delta,
               Support Vector Machine, and k-Nearest Neighbor) and finds that all suffer from a
               similar drop in effectiveness as the size of the target texts decreases. The same
               result is seen when considering the most usual independent variables (a.k.a.
                  <q>features</q>) used as input to the various classifiers. These features include
               Most Frequent Words, character n-grams, and POS (part of speech) n-grams. Based on
               Eder’s results, one must conclude that success in classification of short texts needs
               improved algorithms and/or more informative input variables.</p>
            <p>This study focuses on the second requirement and explores the value of morphological
               tagging and syntactic parsing to create a richly discriminative set of variables.
               Generally speaking, morphological and syntactic data are underused in Authorship
               Attribution as compared to lexical and character-based features. While a wide-ranging
               survey <ptr target="#stamatatos2009"/> was able to refer to a number of studies that
               used syntactic re-write rules or more complex morpho-syntactic features, an
               examination of references in a subsequent overview <ptr target="#swain2017"/>
               indicates POS tags are almost the only such features in frequent use. However, a more
               recent experiment in Authorship Attribution relying entirely on morpho-syntactic
               variables <ptr target="#gorman2020"/> suggests that such features can significantly
               enrich the information available for text classification. </p>
            <p>The general applicability of R. Gorman’s study is questionable for at least two
               reasons. First, it involves a morphologically complex language — ancient Greek. It is
               uncertain how its methods may be suited to a simpler morphological target such as
               English. Second, the study’s approach is based on a dependency syntax corpus in which
               morphology and dependency relations have been hand annotated by a single language
               expert. The corpus is unusually large to be annotated in this way, and the resulting
               accuracy and consistency is a luxury available in few languages. </p>
            <p>The present investigation will further explore the viability of a morpho-syntactic
               approach to text classification. It will test the effectiveness of such variables in
               several languages representing a wide spectrum of morphological complexity (English,
               German, Spanish, Finnish, and Polish). In addition, the requisite morphological and
               dependency annotation will be generated automatically, using a freely available
               program. As a proof of concept, classification will be carried out with only the
               resultant morpho-syntactic information. This restriction will simplify interpretation
               of results by reducing possible confounding elements within the proposed feature set.
               In addition, an important consideration in authorship classification is to eliminate
               as far as possible the effects of topic and genre and to avoid relying on any
               features that could be easily imitated or manipulated. Syntactic features are thought
               to meet this requirement better than lexical ones. Since identifying syntactic
               information requires extra processing with possibly noisy results, most
               investigations fall back on <q>function words</q> as a proxy for truly syntactic
               features. By directly using computationally generated syntactic analyses instead,
               this study will explore the benefits of such pre-processing in comparison to the
               costs of the noise it inevitably introduces.</p>
            <p>The remainder of this article has the following structure. Part 2 discusses the
               formation of the various language corpora. It then details the creation of the input
               variables. These variables are somewhat complicated, in that they are based on
               combinations of morpho-syntactic elements and, at the same time, seek to preserve as
               much flexibility as possible in the incorporation of these elements. Part 3 explains
               the several steps of the classification experiment itself. Part 4 presents the
               results of the classification process and explores their implications. It lays
               particular emphasis on the interpretability of the input variables used in this
               study. Arguably, descriptions of authorial style based on traditional
               morpho-syntactic categories will prove more persuasive to those outside the ranks of
               scholars of computational stylometrics. </p>
         </div>
         <div>
            <head>2. Corpora and Feature Extraction</head>
            <p>This study involves corpora of texts in several languages: English, Finnish, German,
               Spanish, and Polish. An attempt was made to gather publicly available texts in a
               wider range of languages, but it was hindered by several factors. The design of the
               proposed experiment requires 20 single-author texts in a given language. All texts
               should be more than 20,000 tokens in length, and all texts in a language should be of
               the same type. Where possible, texts were to be drawn from a well-defined temporal
               range. These demands were most easily met by selecting works of fiction, in
               particular novels. The works were drawn from Project Gutenberg and the Computational
               Stylistics Group.<note>
                  <ref target="https://www.gutenberg.org/">https://www.gutenberg.org/</ref> and <ref
                     target="https://computationalstylistics.github.io/resources/"
                     >https://computationalstylistics.github.io/resources/</ref>. The list of
                  authors and works is given in the appendix. </note>
            </p>
            <p>It may be surprising that a study interested in classification of short texts would
               use novels rather than, for example, tweets or news articles. However, the focus here
               is on the informational value of morpho-syntactic variables in various languages and
               not on any particular kind of text, and in this case the reasons for choosing novels
               are fairly compelling. As noted, the literature suggests that classification of more
               than a few texts becomes ineffective when the targets are shorter than a few thousand
               words. Thus, our investigation starts with texts c. 2000 words. Since data are
               traditionally split into a training set comprising approximately 90% of the material,
               with the remainder in the test set, texts of 20,000 words are in order. While it
               might be possible to manufacture texts of this length by concatenating tweets or the
               like, it would not be easy to find, for a range of languages, large sets of short
               texts that meet a second requirement: that single authorship of each 20,000-word text
               group can be assumed. Third, the use of novels minimizes, at least as compared to
               concatenated collections, possible confounding effects of genre and topic. </p>
            <p>The core of this investigation is variable extraction and preparation. The first step
               is to generate morpho-syntactic annotations for each text. The processing is done
               with the <q>udpipe</q> package for the R Software Environment <ptr
                  target="#wijffels2019"/>. The package includes functions to produce a universal
               dependency grammar analysis for texts in a wide variety of languages. Dependency
               grammar is an increasingly widely accepted approach to describing the structure of
               sentences. It offers an advantage to the present study in that, unlike phrase
               structure grammars, it deals well with non-projective grammatical sentence components
               — essentially, words closely related grammatically that are separated from each other
               by less closely related words. Non-projectivity occurs frequently in many languages,
               especially those which, unlike English, have a relatively complex system of
               morphology. The <ref target="https://universaldependencies.org/">Universal
                  Dependencies</ref> framework <ptr target="#nivre2015"/> has been developed by an
               international cooperative of scholars to further cross-linguistic language study. One
               of its most important contributions is to establish a common set of tags for
               morphological features and syntactic relationships. This standard is a great step
               forward in natural language processing across languages, since, <term>inter alia,
               </term>the parsers (i.e., programs to analyze syntax) sponsored by the Universal
               Dependencies (UD) project produce mutually compatible output: a single algorithm can
               pre-process texts from a range of languages.</p>
            <p>Raw text (.txt files) provided to the udpipe program gives an output in which each
               token is lemmatized, tagged for morphology, and parsed for universal dependency
               relationship. Formatted as a matrix, its most salient results look like this:</p>
            <table>
               <head>udpipe Annotation.</head>
               <row role="label">
                  <cell>token_id</cell>
                  <cell>token</cell>
                  <cell>lemma</cell>
                  <cell>upos</cell>
               </row>
               <row role="data">
                  <cell>1</cell>
                  <cell>Er</cell>
                  <cell>er</cell>
                  <cell>PRON</cell>
               </row>
               <row role="data">
                  <cell>2</cell>
                  <cell>lachte</cell>
                  <cell>lachen</cell>
                  <cell>VERB</cell>
               </row>
               <row role="data">
                  <cell>3</cell>
                  <cell>vor</cell>
                  <cell>vor</cell>
                  <cell>ADP</cell>
               </row>
               <row role="data">
                  <cell>4</cell>
                  <cell>Vergnügen</cell>
                  <cell>Vergnügen</cell>
                  <cell>NOUN</cell>
               </row>
               <row role="data">
                  <cell>5</cell>
                  <cell>,</cell>
                  <cell>,</cell>
                  <cell>PUNCT</cell>
               </row>
               <row role="data">
                  <cell>6</cell>
                  <cell>sich</cell>
                  <cell>er|es|sie</cell>
                  <cell>PRON</cell>
               </row>
               <row role="data">
                  <cell>7</cell>
                  <cell>über</cell>
                  <cell>über</cell>
                  <cell>ADP</cell>
               </row>
               <row role="data">
                  <cell>8</cell>
                  <cell>den</cell>
                  <cell>der</cell>
                  <cell>DET</cell>
               </row>
               <row role="data">
                  <cell>9</cell>
                  <cell>Katechismus</cell>
                  <cell>Katechismus</cell>
                  <cell>NOUN</cell>
               </row>
               <row role="data">
                  <cell>10</cell>
                  <cell>mokieren</cell>
                  <cell>mokieren</cell>
                  <cell>VERB</cell>
               </row>
               <row role="data">
                  <cell>11</cell>
                  <cell>zu</cell>
                  <cell>zu</cell>
                  <cell>PART</cell>
               </row>
               <row role="data">
                  <cell>12</cell>
                  <cell>können</cell>
                  <cell>können</cell>
                  <cell>AUX</cell>
               </row>
            </table>

            <table>
               <head>udpipe Annotation (continued)</head>
               <row role="label">
                  <cell>token_id</cell>
                  <cell>feats</cell>
                  <cell>head_token_id</cell>
                  <cell>dep_rel</cell>
               </row>
               <row role="data">
                  <cell>1</cell>
                  <cell>Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs</cell>
                  <cell>2</cell>
                  <cell>nsubj</cell>
               </row>
               <row role="data">
                  <cell>2</cell>
                  <cell>Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin</cell>
                  <cell>0</cell>
                  <cell>root</cell>
               </row>
               <row role="data">
                  <cell>3</cell>
                  <cell>Case=Nom|Gender=Neut|Number=Plur</cell>
                  <cell>4</cell>
                  <cell>case</cell>
               </row>
               <row role="data">
                  <cell>4</cell>
                  <cell>Case=Nom|Gender=Neut|Number=Sing</cell>
                  <cell>2</cell>
                  <cell>obl</cell>
               </row>
               <row role="data">
                  <cell>5</cell>
                  <cell>NA</cell>
                  <cell>10</cell>
                  <cell>punct</cell>
               </row>
               <row role="data">
                  <cell>6</cell>
                  <cell>Case=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes</cell>
                  <cell>10</cell>
                  <cell>obj</cell>
               </row>
               <row role="data">
                  <cell>7</cell>
                  <cell>Foreign=Yes</cell>
                  <cell>9</cell>
                  <cell>case</cell>
               </row>
               <row role="data">
                  <cell>8</cell>
                  <cell>Case=Acc|Definite=Def|Gender=Masc|Number=Sing|PronType=Art</cell>
                  <cell>9</cell>
                  <cell>det</cell>
               </row>
               <row role="data">
                  <cell>9</cell>
                  <cell>Case=Acc|Gender=Masc|Number=Sing</cell>
                  <cell>10</cell>
                  <cell>obl</cell>
               </row>
               <row role="data">
                  <cell>10</cell>
                  <cell>VerbForm=Inf</cell>
                  <cell>4</cell>
                  <cell>xcomp</cell>
               </row>
               <row role="data">
                  <cell>11</cell>
                  <cell>Case=Nom|Definite=Ind|Gender=Neut|Number=Sing|PronType=Ind</cell>
                  <cell>10</cell>
                  <cell>mark</cell>
               </row>
               <row role="data">
                  <cell>12</cell>
                  <cell>VerbForm=Inf</cell>
                  <cell>10</cell>
                  <cell>aux</cell>
               </row>
            </table>

            <p>The text is part of a sentence from <title rend="italic">Buddenbrooks</title> by
               Thomas Mann: <quote rend="inline"><foreign xml:lang="de">Er lachte vor Vergnügen,
                     sich über den Katechismus mokieren zu können …</foreign></quote> (<quote
                  rend="inline">He laughed with pleasure at being able to make fun of the
                  catechism</quote>). </p>
            <p>For each token the analysis gives the form as it appears in the text and its lemma.
               This information is not used in our approach to classification and will be ignored
               here. The remaining columns shown, however, are integral. The <q>upos</q> column
               contains the UD part-of-speech tags for each word. The <q>feats</q> column gives the
               morphological analysis. Morphology information has the form <q>TYPE=VALUE,</q> with
               multiple features separated by a bar symbol (TYPE1=VALUE1|TYPE2=VALUE2). For example,
               token #2, <emph>lachte</emph> (<q>laughed</q>), is identified as Indicative in the
               category Mood (Mood=Ind), Singular in the category Number (Number=Sing), etc. There
               is no limit to the number of features that may be assigned to a single token.</p>
            <p>Part of speech and morphology constitute what we may call <q>shallow</q> syntactic
               features. Information of this type may allow us to infer some syntactical structures,
               but they do not represent them directly. In contrast, the <q>head_token_id</q> and
                  <q>dep_rel</q> columns do constitute such a direct representation. The head token
               is the item that is the immediate syntactic <q>parent</q> of a given token. The
               dependency relation specifies the type of grammatical structure obtaining between
               parent and target. From these columns we may generate a description of the syntactic
               structure of the entire sentence in a representation of its <q>deep</q> syntax. The
               structure revealed by these data points is perhaps most clearly illustrated with the
               corresponding dependency tree:</p>

            <figure>
               <head>Dependency Tree of Example Sentence</head>
               <graphic url="resources/images/image1.tmp"/>
            </figure>

            <p>As is apparent, the syntactic <q>path</q> from the sentence root to the each
                  <q>leaf</q> token is given by the combination of head id and dependency
               relationship.</p>
            <p>Both shallow and deep syntactic information are used to create the input variables
               for this investigation. Before looking at those variables in detail, it is worth
               noticing that pre-processing with the udpipe program introduces a certain amount of
               noise through mistaken analyses. To focus on the <q>feats</q> output, token 4, the
               noun <emph><foreign xml:lang="de">Vergnügen</foreign></emph>, is incorrectly assigned
               NOMINATIVE case (should be DATIVE). In addition, the three indeclinable tokens in
               this text — i.e., words with a fixed and static form whose morphology is usually not
               further analyzed — are treated very strangely indeed: the preposition <emph><foreign
                     xml:lang="de">vor</foreign></emph> and the particle <emph><foreign
                     xml:lang="de">zu</foreign></emph> are assigned gender, number, and case
                     (<emph><foreign xml:lang="de">zu</foreign></emph> also gets classified for
               definiteness and pronoun type). The frequent preposition <emph><foreign xml:lang="de"
                     >über</foreign></emph> is treated as a foreign word, perhaps as a dependent of
               the ecclesiastical (and ultimately ancient Greek) word <emph><foreign xml:lang="de"
                     >Katechismus</foreign></emph>. Whatever the reason, the parser has introduced 4
               errors in 12 tokens. Of course, the example text was chosen because it is short yet
               relatively complex; it may not be at all representative of the general error rate of
               the udpipe program. Nonetheless, it would be reasonable to worry that the noise
               introduced by erroneous morpho-syntactic annotations would undercut their value for
               classification input variables. However, as we will see below, bad tagging and
               parsing seems to make little or no practical difference in the classification
               results. We may evidently assume that these input errors are distributed randomly
               across the classified texts and that they have little effect as compared to the
               information carried by the input variables.</p>
            <p>To turn now to the details of the features (i.e., input variables) themselves, there
               are two main principles guiding their creation: first, the set should include both
               shallow and deep syntactic information; second, the <q>degrees of freedom</q> present
               in the morpho-syntactic <q>system</q> of the target language should be, to the
               greatest extent possible, preserved and represented in the features.<note> Here the
                  metaphor <q>degrees of freedom</q> is drawn not from the statistical technical
                  term but rather from traditional physics. Thus, the phrase is meant to call to
                  mind the number of independent parameters that a mechanical system can have, such
                  as the yaw, pitch, roll, etc. of a ship or airplane. Analogously, as discussed
                  below, a German noun has three independent morphological parameters, gender,
                  number, and case, each with multiple possible values. <emph>Ex hypothesi</emph>,
                  any combination of these parameters can carry important information about an
                  author’s style, and therefore all “degrees of freedom” should be taken into
                  consideration. </note>
            </p>
            <p>The first criterion is met by including, alongside the morphological information for
               each word, its dependency relation and, for each word that is not a sentence root,
               the morphology and dependency relation for the parent word of the target word. To
               illustrate from the German sentence given above, the variables for the word
                     <emph><foreign xml:lang="de">Katechismus</foreign></emph> would include the
               following:</p>
            <list type="unordered">
               <item>Self: POS = noun, case = accusative, gender = masculine, number = singular,
                  dependency = oblique</item>
               <item>Parent: POS = verb, verb form = infinitive, dependency = open clausal
                  complement</item>
            </list>
            <p>Including values for parent as well as target word creates a representation of the
               hierarchical structure that most linguistic theories assume for syntax. Of course,
               the dimensions of morpho-syntactic information produced by the udpipe tagger-parser
               vary sharply by language. The following table (Table 4) indicates the number of
                  <q>basic</q> elements for each language. </p>
            <table>
               <head>Number of <q>Simplex</q> Variables</head>
               <row role="label">
                  <cell/>
                  <cell>target token</cell>
                  <cell>target + parent</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>16</cell>
                  <cell>32</cell>
               </row>
               <row role="data">
                  <cell>Finnish</cell>
                  <cell>23</cell>
                  <cell>46</cell>
               </row>
               <row role="data">
                  <cell>German</cell>
                  <cell>15</cell>
                  <cell>30</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>33</cell>
                  <cell>66</cell>
               </row>
               <row role="data">
                  <cell>Spanish</cell>
                  <cell>18</cell>
                  <cell>36</cell>
               </row>
            </table>

            <p>The second requirement mentioned above is meant to preserve the complexity of the
               morpho-syntax of the input texts. As we have noted, it is rare for an attribution
               attempt to take any morphological feature into account with the common exception of
               part of speech. In those studies that do include morphological details, the various
               values that make up the relevant information about a word seem to be coded in a
               single block. For example, Paulo Varela et al. (2018, 2016) incorporate in their
               variables a vector for <q>flexion</q> (number, gender, tense, etc.), but apparently
               combine the features of a word into a single value: thus the entry for <hi
                  rend="italic">é </hi>(Portuguese <q>is</q>) is reported as <q>PR 3S IND VFIN,</q>
               i.e, present indicative 3rd-person singular verb.<note> Varela et al. (2018, 2016) do
                  not explicitly explain how the internal structure of the values in the flexion
                  vector are handled in their approach <ptr target="#varela2018"/>
                  <ptr target="#varela2016"/>. However, the number of input variables in their
                  analysis (179 reduced to 132) is not consistent with the preservation of all
                  degrees of freedom in their five morpho-syntactic vectors.</note>
            </p>
            <p>Combining the morphological categories of a word into a single value obscures some of
               the stylistic information that the word may contain. For example, if we categorize a
               German noun into a consolidated unit of gender, number, and case (e.g., Masc Sing
               Nom), the computer can assign each noun in the corpus to one of the 24 resulting
               categories (3 genders x 2 numbers x 4 cases) and calculate a frequency distribution
               over them, but it cannot access the frequencies of, say, masculine nouns or of
               combinations of two categories such as feminine dative, etc.<note> Certainly, an
                  algorithm could be designed to recognize the internal structure of a composite
                  variable and calculate the frequency of the components, but this procedure would
                  be a more cumbersome route to the same result as including more granular variables
                  from the beginning.</note> There is no strong reason to presume that any frequency
               ratio among the ternary variables is more discriminative for the texts in a corpus
               than, e.g., the ratio between Masc Nom and Fem Nom. We may usefully think of the
               morpho-syntactic elements that make up combinations as <q>characters</q> in
                  <q>syntactic n-grams.</q><note><ptr target="#sidorov2012"/> explore what we might
                  call <q>word-level</q> syntactic n-grams. The <emph>n </emph>in their conception
                  represents the number of hierarchically contiguous words included, where hierarchy
                  is determined by dependency and is analogous to linear order in the more familiar
                  form of n-gram. <ptr target="#gorman2016"/> explore a similar procedure. In the
                  present study, greater weight is given to intra-word characteristics alongside
                  syntactic sequences between words. The analogy with character n-grams thus seems
                  apt.</note> Just as the addition to word-based inputs of character n-grams with
               different values of <emph>n</emph> may improve classification, in the same way using
               a range of combination lengths for morpho-syntactic elements may be valuable. Thus,
               it is best <emph>in principle, </emph>to include variables made of all <q>arities</q>
               of morpho-syntactic categories, from one to the total number of such categories
               (i.e., <mml:math>
                  <mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
                     <mrow>
                        <mfrac linethickness="0pt">
                           <mrow>
                              <mi>n</mi>
                           </mrow>
                           <mrow>
                              <mn>1</mn>
                           </mrow>
                        </mfrac>
                     </mrow>
                  </mfenced>
               </mml:math>…<mml:math>
                  <mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
                     <mrow>
                        <mfrac linethickness="0pt">
                           <mrow>
                              <mi>n</mi>
                           </mrow>
                           <mrow>
                              <mi>n</mi>
                           </mrow>
                        </mfrac>
                     </mrow>
                  </mfenced>
               </mml:math>).<note> From the perspective of information theory, <ptr
                     target="#shannon1948" loc="12"/> has pointed out that entropy is sub-additive,
                  in that the sum of the individual entropies of several variables is greater than
                  the joint entropy of those variables, except in the case where the variables are
                  statistically independent. While calculating the relative contribution to
                  information of two variables is elementary, specifying the contributions of
                  multiple variables is an open question <ptr target="#williams2010"/>. It may
                  therefore be more prudent to turn linguistic features into a larger number of more
                  basic variables than a smaller number of consolidated inputs.</note>
            </p>
            <p>Of course, such an extensive variable set is not feasible <emph>in practice.</emph>
               For example, the udpipe program for German returns 13 categories of morphological
               features. To these 13 must be added the part of speech and the dependency
               relationship, and since the characteristics of the parent word are to be combined
               with those of each target word, variable types will be combinations constructed from
               30 categories. If all sizes of combination were included (i.e., <mml:math>
                  <mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
                     <mrow>
                        <mfrac linethickness="0pt">
                           <mrow>
                              <mn>30</mn>
                           </mrow>
                           <mrow>
                              <mn>1</mn>
                           </mrow>
                        </mfrac>
                     </mrow>
                  </mfenced>
               </mml:math>…<mml:math>
                  <mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
                     <mrow>
                        <mfrac linethickness="0pt">
                           <mrow>
                              <mn>30</mn>
                           </mrow>
                           <mrow>
                              <mn>30</mn>
                           </mrow>
                        </mfrac>
                     </mrow>
                  </mfenced>
               </mml:math>), the number of types would be essentially <mml:math>
                  <msup xmlns="http://www.w3.org/1998/Math/MathML">
                     <mrow>
                        <mn>2</mn>
                     </mrow>
                     <mrow>
                        <mn>30</mn>
                     </mrow>
                  </msup>
               </mml:math>—over a billion separate combinations! Nor is the combinatorial explosion
               the only problem. Each of the <mml:math>
                  <msup xmlns="http://www.w3.org/1998/Math/MathML">
                     <mrow>
                        <mn>2</mn>
                     </mrow>
                     <mrow>
                        <mn>30</mn>
                     </mrow>
                  </msup>
                  <mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
               </mml:math>combinations is a type, not a variable. We have seen that the ternary
               combination Gender-Number-Case may take one of 24 different values in German: MASC
               SING NOM, etc. Thus, the number of type-value pairs reaches truly astronomical
               values. Clearly, we must combine the generation of input features with a rigorous
               culling and reduction process.</p>
            <p>The first step deals with the combinatorial dimension. Selecting grammatically
               sensible combinations such as Gender-Number-Case (and its unigram and bigram
               components) would be laborious and would require knowledge of the various languages
               to be classified. Thus, a naïve approach was preferred: an arbitrary maximum length
               was chosen. Given a set of 30 elements for combination, the length must necessarily
               be short. <mml:math>
                  <munderover xmlns="http://www.w3.org/1998/Math/MathML">
                     <mo>∑</mo>
                     <mrow>
                        <mi>k</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                        <mi> </mi>
                     </mrow>
                     <mrow>
                        <mn>4</mn>
                     </mrow>
                  </munderover>
                  <mrow xmlns="http://www.w3.org/1998/Math/MathML">
                     <mfenced separators="|">
                        <mrow>
                           <mfrac linethickness="0pt">
                              <mrow>
                                 <mn>30</mn>
                              </mrow>
                              <mrow>
                                 <mi>k</mi>
                              </mrow>
                           </mfrac>
                        </mrow>
                     </mfenced>
                  </mrow>
               </mml:math> already gives 31,930 unique combinations. When we take into account that
               each combination will have multiple values, trouble with data sparsity and
               computational limitations can be expected. Therefore, the maximum was set at three
               elements per combination. This limit results in 4,525 combinations. </p>
            <p>The next step populates each combination (or variable type) with its various values,
               as drawn from the basic morpho-syntactic elements produced by udpipe. This process is
               computationally slow for combinations of more than two elements, so we have used a
               smaller sample corpus for each language; it is composed of 500 tokens (punctuation
               excluded) from each text. When populated, the various combinations of types exhibit a
               large number of unique values. To take German as illustrative: the 30 unary types
               produce 161 values, the 435 binary types produce 5,579 values, and the 4,060 ternary
               combinations yield 56,349 values. As one would expect with linguistic data, the
               frequency distributions of these values are quite skewed. For example, 53,209 of the
               ternary values occur in 1% or fewer words and 19,116 occur only once in the sample
               corpus of 10,000 tokens. To avoid severe sparsity, another culling of variables is
               clearly in order.</p>
            <p>Once again, since we do not know without prior investigation which combinations may
               be most distinctive for authors and texts, we have had recourse to a naïve approach.
               For each combination length, only those type-value pairs that occur in 5% of the
               tokens in the sample corpus have been included as input variables for classification.
               A separate set of variables has been identified in this way for each language. The
               size of each sub-set of variables is given in the following table (Table 3).</p>
            <table>
               <head>Number of Variable Subsets with at Least 5% Frequency</head>
               <row role="label">
                  <cell/>
                  <cell>unary</cell>
                  <cell>binary</cell>
                  <cell>ternary</cell>
                  <cell>total</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>45</cell>
                  <cell>61</cell>
                  <cell>85</cell>
                  <cell>191</cell>
               </row>
               <row role="data">
                  <cell>Finnish</cell>
                  <cell>48</cell>
                  <cell>226</cell>
                  <cell>467</cell>
                  <cell>932</cell>
               </row>
               <row role="data">
                  <cell>German</cell>
                  <cell>58</cell>
                  <cell>216</cell>
                  <cell>318</cell>
                  <cell>592</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>63</cell>
                  <cell>331</cell>
                  <cell>726</cell>
                  <cell>1,120</cell>
               </row>
               <row role="data">
                  <cell>Spanish</cell>
                  <cell>46</cell>
                  <cell>150</cell>
                  <cell>189</cell>
                  <cell>385</cell>
               </row>
            </table>

            <p>Because the variables may be difficult to conceptualize, the following tables give an
               illustration, including, for selected languages, the most frequent type-value pair as
               well as the least frequent pair to make the 5% minimum. Examples will be given in
               each category for target word characteristics (marked <q>t:</q>), parent word
               characteristics (marked “p:”), and combinations of target and parent (Tables 4, 5,
               and 6).</p>
            <table>
               <head>Examples of Simplex Variables</head>
               <row role="label">
                  <cell/>
                  <cell>type</cell>
                  <cell>value</cell>
                  <cell>frequency</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>t: Num</cell>
                  <cell>Sing</cell>
                  <cell>29.80%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: POS</cell>
                  <cell>VERB</cell>
                  <cell>44.90%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Rel</cell>
                  <cell>root</cell>
                  <cell>5.15%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Rel</cell>
                  <cell>ccomp</cell>
                  <cell>5.56%</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>t: Num</cell>
                  <cell>Sing</cell>
                  <cell>48.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Num</cell>
                  <cell>Sing</cell>
                  <cell>65.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Rel</cell>
                  <cell>cc </cell>
                  <cell>5.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Case</cell>
                  <cell>Loc</cell>
                  <cell>5.40%</cell>
               </row>
            </table>

            <table>
               <head>Examples of Binary Variables</head>
               <row role="label">
                  <cell/>
                  <cell>type</cell>
                  <cell>value</cell>
                  <cell>frequency</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>t: POS &amp; t: Num</cell>
                  <cell>NOUN/Sing</cell>
                  <cell>13.80%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: POS &amp; p: Num</cell>
                  <cell>NOUN/Sing</cell>
                  <cell>28%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: POS &amp; t: VerbForm</cell>
                  <cell>AUX/Fin</cell>
                  <cell>5.50%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Num &amp; p: Rel</cell>
                  <cell>Sing/nmod</cell>
                  <cell>5.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Num &amp; p: POS</cell>
                  <cell>Sing/VERB</cell>
                  <cell>16.30%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: POS &amp; p: Rel</cell>
                  <cell>VERB/root</cell>
                  <cell>5%</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>t: Num &amp; t: Gender</cell>
                  <cell>Masc/Sing</cell>
                  <cell>21.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: POS &amp; p: Voice</cell>
                  <cell>VERB/Act</cell>
                  <cell>46.70%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: POS &amp; t: Animacy</cell>
                  <cell>Verb/Hum</cell>
                  <cell>5%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Gender &amp; p: Aspect</cell>
                  <cell>Fem/Perf</cell>
                  <cell>5.20%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Num &amp; p: Num</cell>
                  <cell>Sing/Sing</cell>
                  <cell>33.70%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Case &amp; p: VerbForm</cell>
                  <cell>Acc/Fin</cell>
                  <cell>5%</cell>
               </row>
            </table>

            <table>
               <head>Examples of Ternary Variables</head>
               <row role="label">
                  <cell/>
                  <cell>type</cell>
                  <cell>value</cell>
                  <cell>frequency</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>t: POS &amp; t: Num &amp; t: PronType</cell>
                  <cell>PRON/Sing/Prs</cell>
                  <cell>8.90%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: POS &amp; p: VerbForm &amp; p: Mood</cell>
                  <cell>VERB/Fin/Ind</cell>
                  <cell>19.90%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Rel &amp; t: Definite &amp; t: PronType</cell>
                  <cell>det/Def/Art</cell>
                  <cell>5.50%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Rel &amp; p: VerbForm &amp; p: Tense</cell>
                  <cell>root/Fin/Past</cell>
                  <cell>7.70%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Num &amp; p: POS &amp; p: Tense</cell>
                  <cell>Sing/Verb/Past</cell>
                  <cell>9.10%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Rel &amp; p: POS &amp; p: Mood</cell>
                  <cell>nsubj/Verb/Ind</cell>
                  <cell>5.10%</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>t: Mood &amp; t: VerbForm &amp; t: Voice</cell>
                  <cell>Ind/Fin/Act</cell>
                  <cell>14%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: POS &amp; p: VerbForm &amp; p: Mood</cell>
                  <cell>VERB/Fin/Ind</cell>
                  <cell>41.30%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: POS &amp; t: Animacy &amp; t: Voice</cell>
                  <cell>VERB/Hum/Act</cell>
                  <cell>5%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>p: Rel &amp; p: Num &amp; p: Aspect</cell>
                  <cell>conj/Sing/Perf</cell>
                  <cell>5%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Num &amp; p: POS &amp; p: VerbForm</cell>
                  <cell>Sing/VERB/Fin</cell>
                  <cell>20.60%</cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell>t: Num &amp; p: POS &amp; p: Rel</cell>
                  <cell>Sing/VERB/conj</cell>
                  <cell>5%</cell>
               </row>
            </table>

            <p>It is important to emphasize that it is the <emph>pairing</emph> of the (possibly
               compound) types <emph>and </emph>their (possibly compound) values which constitute
               the input variables for this investigation. In Polish, for example, a type combining
               the Number of the target word, with the part of speech and dependency relation of the
               target word’s parent can have many possible values. But only those combinations of
               values that occur no less often than the 5% frequency of Singular-Verb-conjunct have
               been taken into account. All less frequent values for this type have been excluded
               from the variables and ignored.<note> Note that it follows that one cannot pay
                  attention to the values alone: the value Fem/Perf may be included in the variable
                  set when it gives the values for the Gender and Aspect of a given word’s
                  dependency parent, but the same morphological features may be excluded when they
                  are the values of the target word’s <emph>own </emph>Gender and Aspect.</note>
            </p>
            <p>This generation and selection of variables produce, for each text in a language
               corpus, a matrix in which each row represents a token of the text and each column
               represents a type-value pair as described above. The total number of variable columns
               differs for each language (see Table 4). For each row, each cell is assigned a value
               of one if the token has the morpho-syntactic characteristic for that column;
               otherwise, zero is assigned. The design of the variables means any given token can
               have positive values in numerous columns. The German corpus, for example, has a mean
               of 48.8 positive columns per token (of a total 592 variables). </p>
         </div>
         <div>
            <head>3. Classification</head>
            <p>The purpose of this study is to test whether automated Universal Dependency
               annotation, as supplied by the udpipe program, can be useful for authorship
               attribution and related problems. It seeks to understand whether such an input
               contains enough information to overcome the noise introduced by a level of mistakes,
               which can be relatively high as compared to the results of expert <soCalled>hand
                  annotation.</soCalled> Since it is known that the information/noise ratio worsens
               for shorter input texts <ptr target="#eder2017"/>, this investigation focuses on the
               degree to which classification performance degrades as the size of the input
               decreases. This process requires texts of various lengths, with all other factors
               held constant, insofar as this may be possible. This requirement is met by creating a
               series of smaller <q>texts</q> for each author by sampling the full text.</p>
            <p>At each stage of classification, each text in each language corpus was divided into
               smaller texts, and these smaller units were classified according to author. <ptr
                  target="#gorman2020"/> has shown that — at least for expert-annotated ancient
               Greek prose — classification on the basis of combinations of morpho-syntactic
               characteristics is practically error-free for input texts larger than 500 tokens
               (&gt; 99%). Exploratory tests for our study confirm this high level of accuracy for
               text sizes of between 2000 and 600 tokens. Thus, this paper will present only the
               data for the smallest texts in the investigation; it begins with texts of 500 tokens,
               and then decreases to 400, 300, 200, 100, and finally 50 tokens.</p>
            <p>As is traditional in textual attribution studies, each text was treated as a <q>bag
                  of words</q> for the purpose of division into samples. Each token was — naively —
               treated as independent of all others; no further account was taken of the context of
               an individual token in sentence, paragraph, or any other unit of composition. Thus,
               segments may contain tokens from many parts of the original text without regard for
               their original order.<note> Because udpipe works sentence by sentence, all annotation
                  must be completed before creating new smaller <q>texts</q> by subsampling since
                  this random selection of course disrupts sentence structure. Thus, for strictly
                  practical reasons, the sampling process is applied to the matrices of token plus
                  annotation rather than to the raw text. This procedure seems to imply no
                  theoretical implications beyond those entailed by the application of the
                  bag-of-words method in general. My thanks to the anonymous reader for raising this
                  question. </note>
            </p>
            <p>Once segments of the appropriate length have been generated, the next step is to
               aggregate the values for all variables in each segment. For example, a matrix with
               20,000 rows representing as many separate tokens, each with a 1 or a 0 in its
               variable columns, is replaced by a new matrix with 40 rows, each representing 500
               tokens. Each variable column now contains the sum of the relevant column for the
               underlying tokens. Columns values are then normalized so that sums are replaced by
               relative frequencies. These <q>segment matrices</q> become the input for the
               classifying algorithm. </p>
            <p>To present a difficult classification problem <ptr target="#luyckx2008"/>
               <ptr target="#luyckx2011"/>, each language corpus contains one work apiece by 20
               different authors. Given the number of suitable texts available in the public domain,
               this number represent an attempt to balance between including more languages with
               fewer authors for each and including fewer languages with more authors in each.</p>
            <p>For the classification algorithm itself, logistic regression was chosen. This method
               has an ability to handle a large number of observations and variables. It is also
               able to function well in the presence of many co-linear variables. Specifically, the
                  <title rend="italic">LiblineaR</title> package for the <title rend="italic">R
                  Project for Statistical Computing </title>was chosen <ptr target="#fan2008"/>
               <ptr target="#helleputte2017"/>. This package offers a range of linear methods; we
               selected the L-2 regularization option for logistic regression.<note> Exploratory
                  testing revealed that the L-2 option was the quickest and most accurate of those
                  available. L-2 regression (also called <q>Ridge Regression</q>) is also preferable
                  for the way it handles collinearity. L-2 regression distributes the weight that
                  collinear variables will have in a model across those collinear variables. The
                  alternative method, L-1 or <q>Lasso Regression,</q> by contrast, arbitrarily
                  singles out one of the collinear variables and assigns it the full weight of those
                  variables. L-2 appears more conducive for the interpretation of the importance of
                  individual variables since L-1 may lead the unwary to dismiss the force of a large
                  number of collinear inputs.</note>
            </p>
            <p>For each input text size in each language, 90% of the data was used for training the
               classifier and the remaining 10% set aside for testing. Inclusion of a segment in the
               training or testing set was random. However, the amount of text by individual authors
               in some language corpora varies a great deal, and this situation may affect results,
               with the classifier, for example, strongly preferring the most frequent author in the
               training set. Thus, the random assignment into training and test sets was guided by
               associating a selection probability with each segment so that each of the
                  <emph>n</emph> authors in a corpus was represented by approximately <mml:math>
                  <mfrac xmlns="http://www.w3.org/1998/Math/MathML" bevelled="true">
                     <mrow>
                        <mn>1</mn>
                     </mrow>
                     <mrow>
                        <mi>n</mi>
                     </mrow>
                  </mfrac>
               </mml:math> of the segments in the test set. This balance prevents undue bias in the
               classifier. To validate the results of the classification testing, we used Monte
               Carlo subsampling <ptr target="#simon2007"/> applied at two levels. As a rule, the
               populating of the segments with randomly selected tokens was carried out ten
                  times.<note> Processing becomes slow for text sizes of 100 tokens and fewer,
                  depending on the morphological complexity of the language. The number of times the
                  texts were partitioned was reduced accordingly.</note> For each of these
               partitionings to create text segments, 100 additional random divisions into a
               training set and a test set were made. As noted below, because the classification
               process becomes much slower as text size falls, fewer iterations of both kinds of
               subsampling were used for the smallest segments. The results of the tests at each
               text size were averaged. These steps minimize the effects of the internal make-up of
               particular segments or of their inclusion or exclusion from the training/testing
               groups.</p>
         </div>
         <div>
            <head>4. Results and Discussion</head>
            <p>For each language corpus, the results of classification according to the size of the
               input text segments are given in Table 8. Since the goal of this study is to evaluate
               for authorship attribution the sufficiency of machine-generated Universal Dependency
               data, no consideration is given to various specialized measurements of classification
               success. Instead, the simplest measure of accuracy is used: percentage of successful
               attributions. The mean accuracy over all attempts is given in the top row of each
               cell; the range is given below the mean. </p>
            <table>
               <head>Classification Accuracy</head>
               <row role="label">
                  <cell/>
                  <cell>500</cell>
                  <cell>400</cell>
                  <cell>300</cell>
                  <cell>200</cell>
                  <cell>100</cell>
                  <cell>50</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell> 99.35<lb/> (95.8-100)<lb/>
                  </cell>
                  <cell> 99.05<lb/> (95.3-100)<lb/>
                  </cell>
                  <cell> 98.18<lb/> (94.5-100)<lb/>
                  </cell>
                  <cell> 96.28<lb/> (92.7-99.3)<lb/>
                  </cell>
                  <cell> 88.32<lb/> (84.3-92.0)<lb/>
                  </cell>
                  <cell> 74.19<lb/> (71.0-78.5) <lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>Finnish</cell>
                  <cell> 99.52<lb/> (99.3-99.7)<lb/>
                  </cell>
                  <cell> 99.03<lb/> (98.7-99.3)<lb/>
                  </cell>
                  <cell> 98.15<lb/> (97.8-98.4)<lb/>
                  </cell>
                  <cell> 95.56<lb/> (95.1-96.0)<lb/>
                  </cell>
                  <cell> 86.44<lb/> (85.8-86.9)<lb/>
                  </cell>
                  <cell> 70.61<lb/> (70.2-71.1)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>Spanish</cell>
                  <cell> 98.87<lb/> (98.7-99.0)<lb/>
                  </cell>
                  <cell> 98.25<lb/> (98.0-98.4)<lb/>
                  </cell>
                  <cell> 96.75<lb/> (96.4-97.0)<lb/>
                  </cell>
                  <cell> 93.18<lb/> (92.8-93.5)<lb/>
                  </cell>
                  <cell> 81.2<lb/> (80.6-81.6)<lb/>
                  </cell>
                  <cell> 61.7<lb/> (61.6-62.0)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>German</cell>
                  <cell> 96.7<lb/> (93.2-99.2)<lb/>
                  </cell>
                  <cell> 95.4<lb/> (91.6-98.4)<lb/>
                  </cell>
                  <cell> 92.85<lb/> (87.9-95.5)<lb/>
                  </cell>
                  <cell> 87.99<lb/> (84.8-91.1)<lb/>
                  </cell>
                  <cell> 76.95<lb/> (71.7-83.0)<lb/>
                  </cell>
                  <cell> 56.07<lb/> (54.5-58.2)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell> 98.26<lb/> (96.4-99.8)<lb/>
                  </cell>
                  <cell> 97.25<lb/> (94.0-99.4)<lb/>
                  </cell>
                  <cell> 94.75<lb/> (92.5-97.5)<lb/>
                  </cell>
                  <cell> 89.7<lb/> (87.3-92.6)<lb/>
                  </cell>
                  <cell> 74.45<lb/> (71.6-77.4)<lb/>
                  </cell>
                  <cell> 53.8<lb/> (51.6-55.9)<lb/>
                  </cell>
               </row>
            </table>

            <p>These data make clear that morpho-syntactic variables derived from UD automated
               parsing suffice to identify author/work in a closed set of authors with a single work
               for each. For all tested languages, accuracy is greater than 90% with text segments
               larger than 300 tokens. Each language corpus contains 20 classes for attribution; 90%
               accuracy is roughly 18 times random chance since the <q>no-information rate</q> for
               each iteration varies slightly from 5%.<note> We may take <q>random chance</q> here
                  to be equivalent of the reported <q>no-information rate.</q> The no-information
                  rate in a classification experiment is the occurrence rate of the most frequently
                  appearing class in the data. For example, if texts by Thomas Mann made up 50% of
                  our material, then a model that classified all test segments as <q>Mann</q> would
                  achieve 50% accuracy simply <q>by chance.</q> As indicated above, to minimize the
                  no-information rate, our approach ensured that the representation of each class in
                  the training set was roughly equal.</note> Such success offers promise for the
               feasibility of classification when vocabulary- and/or character-based variables are
               not appropriate. </p>
            <p>A few more specific observations about these results are in order. In general,
               accuracy tracks well with the size of the variable set for each language (see Table
               4). Languages with larger sets tend to have better accuracy. We may reasonably
               presume that this tendency is due in part to the correlation between the number of
               variables and the sparsity of the input matrices. Ordinarily, we expect sparsity —
               the number of cells of a matrix containing zeros — to increase as the number of
               variables becomes greater. And more sparsity in data for natural language processing
               usually means less accuracy. However, recall that in this study a single token may
               have positive values for many variables and that any variable which has positive
               values for fewer than 5% of tokens has been excluded. The combination of these two
               factors means that as the number of total variables increases, so too does the number
               of variables per token (Table 9). When the token matrices are aggregated to form
               representations of <q>texts</q> of various lengths, the variable columns are summed.
               The presence of positive values in a good number of columns in the original matrix
               makes it unsurprising when relatively few columns in the aggregated matrix sum to 0.
               We may illustrate this with the example of Spanish. The average token in the Spanish
               matrix has 34.7 columns with a positive value. Forming, e.g., a 50-token segment by
               the aggregation of the matrix rows means that, on average, 1,735 (34.7 × 50) positive
               values will be distributed among the 385 variable columns in the resulting matrix. It
               stands to reason that, all else being equal, we may expect relatively few aggregated
               columns containing zeros. In fact, with the variables used in this study, sparsity is
               almost non-existent for texts larger than 100 tokens; the sparsity rates for 50-token
               texts are themselves unexpectedly small given the number of variables (Table 9). This
               low sparsity may go far to explaining the good accuracy of the classification
               experiment.</p>
            <table>
               <head>Sparsity</head>
               <row role="label">
                  <cell/>
                  <cell> Variable types<lb/> total<lb/>
                  </cell>
                  <cell> Mean Variables<lb/> Per token<lb/>
                  </cell>
                  <cell> Sparsity<lb/> @50 tks.<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell>1,120</cell>
                  <cell>102.9</cell>
                  <cell>5.8%</cell>
               </row>
               <row role="data">
                  <cell>Finnish</cell>
                  <cell>932</cell>
                  <cell>69.7</cell>
                  <cell>3.8%</cell>
               </row>
               <row role="data">
                  <cell>Spanish</cell>
                  <cell>385</cell>
                  <cell>34.7</cell>
                  <cell>3.4%</cell>
               </row>
               <row role="data">
                  <cell>German</cell>
                  <cell>592</cell>
                  <cell>48.0</cell>
                  <cell>5.5%</cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell>191</cell>
                  <cell>16.7</cell>
                  <cell>5.0%</cell>
               </row>
            </table>

            <p>A second aspect of the accuracy results reported in Table 3 needs closer elaboration.
               As is usual for author attribution studies that take text size into consideration,
               results in each language show a monotonic decline in accuracy as text size decreases.
               However, when viewed from the perspective of relative error rates, our
               morpho-syntactic variables seem quite robust against decreasing text size.</p>
            <p>When considering classification accuracy of a range of text sizes, it is important to
               focus on the relationship between the change in accuracy and the change in text size.
               A 5% decrease in accuracy when text size drops from 2000 tokens to 1900 tokens is
               much more worrisome than the same decrease from 2000 to 1000 tokens. Table 10 gives
               the relevant information for this experiment. Specifically, it records the rates at
               which the error rate (1 – Accuracy) for each language increases as the text size is
               repeatedly halved: 400 to 200 tokens, 200 to 100, and 100 to 50.</p>
            <table>
               <head>Change in Error Rate by Text Size</head>
               <row role="label">
                  <cell/>
                  <cell>200/400</cell>
                  <cell>100/200</cell>
                  <cell>50/100</cell>
               </row>
               <row role="data">
                  <cell>Polish</cell>
                  <cell> 0.0372/.0095<lb/> = 3.92 <lb/>
                  </cell>
                  <cell> 0.1168/.0372<lb/> = 3.13 <lb/>
                  </cell>
                  <cell> 0.2581/0.1168<lb/> = 2.2 <lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>Finnish</cell>
                  <cell> 0.044/0.0097<lb/> = 4.57 <lb/>
                  </cell>
                  <cell> 0.1356/0.044<lb/> = 3.05 <lb/>
                  </cell>
                  <cell> 0.2939/0.1356<lb/> = 2.16 <lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>Spanish</cell>
                  <cell> 0.0682/0.0175<lb/> = 3.89 <lb/>
                  </cell>
                  <cell> 0.188/0.0682<lb/> = 2.75 <lb/>
                  </cell>
                  <cell> 0.383/0.188<lb/> = 2.03 <lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>German</cell>
                  <cell> 0.1201/.046<lb/> = 2.61 <lb/>
                  </cell>
                  <cell> 0.2305/0.1201<lb/> = 1.91 <lb/>
                  </cell>
                  <cell> 0.4393/0.2305<lb/> = 1.90 <lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>English</cell>
                  <cell> 0.103/0.0275<lb/> = 3.74 <lb/>
                  </cell>
                  <cell> 0.2555/0.103<lb/> = 2.48 <lb/>
                  </cell>
                  <cell> 0.462/0.2555<lb/> = 1.81 <lb/>
                  </cell>
               </row>
            </table>

            <p>The first row in each cell gives the error rate for a given text size divided by the
               error rate for the text that is double the size. For example, the Polish 200-token
               corpus had an average error rate of 3.72%, which is divided by the average error rate
               for the Polish 400-token corpus, 0.95%. The second line of the cell gives the
               corresponding result: while the text size decreases by a factor of 2, the error rate
               increases by a factor of 3.92. An overview of this table shows that for all languages
               the increase factor becomes significantly smaller as the input size itself decreases.
               While the absolute accuracy (or error) rate is still quite properly the metric of
               interest for literary and, most especially, forensic attribution methods, clearly
               presenting proportional relationships such as that of the rate of change in text size
               viz-à-viz the change in success rate should help us to better judge the effectiveness
               of different approaches to classification of small texts.</p>
            <p>There are, of course, anomalies in the patterns observable in Table 8. For example,
               since the German matrix has both more total variables and more variables per token
               than does the Spanish, we might reasonably expect classification of German would
               outperform Spanish. This is not the case. Nor can an explanation be offered
                  here.<note> It should be recognized that the number of variables produced by UD
                  parsing and the selection process outlined above is not a precise measure of a
                  language’s morphological complexity. Some relevant studies identify the complexity
                  of Spanish as greater than that of German <ptr target="#bittner2003"/>
                  <ptr target="#marzi2019"/>.</note> We can only note that some of the differences
               between the success rates of various languages must be due not to characteristics of
               the morpho-syntactic variables, but to particularities of the language corpora.
               Although an effort was made, within the limits of the texts available in the public
               domain, to assemble corpora that were roughly similar among languages, important
               aspects of the corpora must necessarily differ. It is inevitable, for example, that
               one corpus contains a higher proportion of texts that are more difficult to
               distinguish from each other than does another corpus. It is possible that this
               advantage, then, is so great that it may significantly affect the classification
               accuracy rates. Another complicating factor is introduced by the relative
               effectiveness of the udpipe annotation. Again, we must assume a possibly wide range
               between the most and the least accurate of the annotation algorithms. Accordingly, a
               relatively large amount of noise in the variables of an individual language may
               certainly be expected to suppress classification accuracy.</p>
            <p>However, identifying such possible confounding elements and measuring their effects
               are not germane to the purpose of this study. Our experiments have been designed to
               test the hypothesis that machine-generated morpho-syntactic annotation can be used
               for accurate closed-set attribution, even for fairly small texts, in a range of
               languages. All comparisons among languages are merely illustrative and suggestive:
               greater morpho-syntactic complexity seems to correlate at least approximately with
               accuracy. Detailed investigation of this apparent relationship must be left to
               experts in the respective languages and literatures.</p>
            <p>This investigation has set out to explore the discriminative value of a certain set
               of input features. It has advanced the hypothesis that, absent any vocabulary data, a
               mixture of a text’s shallow and deep morpho-syntactic characteristics, when combined
               in a way that preserves as many <q>degrees of freedom</q> as possible, can produce
               strong results in closed-set classification. These results are significant, insomuch
               as most text classification methods rely primarily on vocabulary or character
               information, with a text’s syntax represented only by function words and POS tags.
               However, the quantitative study of language and texts has recently seen the rise in
               prominence of approaches such as deep neural networks. These are often <q>end to
                  end</q> learning systems; input consists of <q>raw</q> text with little
               pre-processing and no feature extraction. Results can be amazingly accurate. In the
               presence of such effective alternatives, the reader of this study may reasonably feel
               doubts about a method which requires syntactic parsing as well as significant effort
               given to feature engineering. </p>
            <p>Yet, user-specified input features have their own advantages. The continuing efforts
               to find better methods of text classification is primarily driven, at least in the
               various disciplines of the humanities, by the desire to identify and describe the
               essential features of individual style. Investigations seek to confirm (or refute)
               the hypothesis that every user of language has a stylistic <q>fingerprint</q> or
                  <q>signature</q> that allows written or spoken <q>texts</q> from that source to be
               distinguished from all others. In order for studies of this sort to be plausible,
               however accurate the results may be, we must be able to give a clear interpretation
               for the variables on which the classification depends.<note>
                  <ptr target="#gabay2021" loc="360"/> argues persuasively that stylometry should go
                  beyond analysis of quantitative observations to focus on the exploration of
                  “stylistic features with an interpretative yield.”</note> Optimally, we must give
               this interpretation in terms of commonly accepted phenomena of language and
               communication. Generally speaking, hand-crafted input features meet this criterion.
               In contrast, end-to-end deep learning algorithms typically abstract their own
               features, and often it takes a great deal of effort to establish what phenomena the
               model is taking into account.<note> For example, one method of interpretation is to
                  inactivate one at a time hidden nodes within a neural network model to see if the
                     <q>ablation</q> affects the outcome of interest <ptr target="#lakretz2020a"/>
                  <ptr target="#lakretz2020b"/>. </note>
            </p>
            <p>The input features examined in this study are familiar in their basic elements and
               transparently interpretable. Anyone interested in the dimensions of authorial style
               presented here can quickly learn the outline of a language’s morphology and the
               elementary structures of Universal Dependency grammar. As a result, classification
               based on UD parsing and morphology tagging puts even beginning students of style in a
               position to explore clear distinctions between texts and authors.</p>
            <p>Since even a classification technique as fundamental as logistic regression
               complicates the matter by adding a layer of learned weights to the input features,
               the interpretability of these morpho-syntactic variables is well illustrated if
               looked at through the lens of a simple distance measure. Since its publication,
               Burrows’s Delta has become one of the most widely used metrics for comparing text
               styles. In essence, Delta measures, for each feature of interest, how far the target
               text differs from the mean of the corpus, normalized by the standard deviation of
               each variable.<note>
                  <mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mo>∆</mo>
                        </mrow>
                        <mrow>
                           <mi>B</mi>
                           <mi>u</mi>
                           <mi>r</mi>
                        </mrow>
                     </msub>
                  </mml:math>(<mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mi>T</mi>
                           <mi>e</mi>
                           <mi>x</mi>
                           <mi>t</mi>
                        </mrow>
                        <mrow>
                           <mn>1</mn>
                           <mo>,</mo>
                           <mi> </mi>
                        </mrow>
                     </msub>
                  </mml:math>
                  <mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mi>T</mi>
                           <mi>e</mi>
                           <mi>x</mi>
                           <mi>t</mi>
                        </mrow>
                        <mrow>
                           <mn>2</mn>
                        </mrow>
                     </msub>
                  </mml:math>) = <mml:math>
                     <munderover xmlns="http://www.w3.org/1998/Math/MathML">
                        <mo>∑</mo>
                        <mrow>
                           <mi>i</mi>
                           <mo>=</mo>
                           <mn>1</mn>
                        </mrow>
                        <mrow>
                           <mi>n</mi>
                        </mrow>
                     </munderover>
                     <mrow xmlns="http://www.w3.org/1998/Math/MathML">
                        <mo>|</mo>
                        <msub>
                           <mrow>
                              <mi>z</mi>
                           </mrow>
                           <mrow>
                              <mi>i</mi>
                           </mrow>
                        </msub>
                     </mrow>
                  </mml:math>(<mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mi>T</mi>
                           <mi>e</mi>
                           <mi>x</mi>
                           <mi>t</mi>
                        </mrow>
                        <mrow>
                           <mn>1</mn>
                        </mrow>
                     </msub>
                  </mml:math>) – <mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mi>z</mi>
                        </mrow>
                        <mrow>
                           <mi>i</mi>
                        </mrow>
                     </msub>
                  </mml:math>(<mml:math>
                     <msub xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                           <mi>T</mi>
                           <mi>e</mi>
                           <mi>x</mi>
                           <mi>t</mi>
                        </mrow>
                        <mrow>
                           <mn>2</mn>
                        </mrow>
                     </msub>
                  </mml:math>)|, where <emph>z </emph>is the <q>z-score</q>: (observation value –
                  population mean) / population standard deviation. As a sum of absolute values,
                  Burrows’s Delta is a species of Manhattan Distance <ptr target="#evert2017"
                  />.</note> In spite of its simplicity, Delta can produce very good classification
               results <ptr target="#eder2015"/>. Conveniently, classification by this method is
               included in the <q>Stylo</q> package for R <ptr target="#eder2016"/>. Table 11
               demonstrates the effectiveness of our morpho-syntactic variables by showing the
               results for the classification of 33 Polish novels using <q>Stylo.</q><note> The top
                  line of each <q>Accuracy</q> cell gives the mean of all classifications for that
                  size; the second line gives the range. For text sizes of 2000-600 tokens, data
                  were partitioned into text segments five time; each such partition was then
                  classified 100 times, with each classification using a different random test and
                  training set. Because of slowing processing speed, the 500-token texts were
                  classified only 50 times for each partitioning of segments.</note>
            </p>
            <table>
               <head>Classification Accuracy (Burrows’s Delta)</head>

               <row role="label">
                  <cell>Text Size</cell>
                  <cell>Accuracy</cell>
                  <cell>Text Size</cell>
                  <cell>Accuracy</cell>
                  <cell>Text Size</cell>
                  <cell>Accuracy</cell>
               </row>
               <row role="data">
                  <cell>2000</cell>
                  <cell> 99.99<lb/> (96.8-100)<lb/>
                  </cell>
                  <cell>1500</cell>
                  <cell> 99.98<lb/> (97.87-100)<lb/>
                  </cell>
                  <cell>900</cell>
                  <cell> 99.30<lb/> (96.0-100)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>1900</cell>
                  <cell> 99.98<lb/> (97.3-100)<lb/>
                  </cell>
                  <cell> 1400<lb/>
                  </cell>
                  <cell> 99.84<lb/> (98.0-100)<lb/>
                  </cell>
                  <cell>800</cell>
                  <cell> 98.82<lb/> (95.2-100)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>1800</cell>
                  <cell> 99.93<lb/> (97.5-100)<lb/>
                  </cell>
                  <cell>1300</cell>
                  <cell> 99.90<lb/> 98.1-100<lb/>
                  </cell>
                  <cell>700</cell>
                  <cell> 98.17<lb/> (93.7-100)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>1700</cell>
                  <cell> 99.97<lb/> (95.0-100)<lb/>
                  </cell>
                  <cell>1200</cell>
                  <cell> 99.68<lb/> 93.0-100<lb/>
                  </cell>
                  <cell>600</cell>
                  <cell> 97.1<lb/> (90.3-100)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell>1600</cell>
                  <cell> 100<lb/> (100-100)<lb/>
                  </cell>
                  <cell>1100</cell>
                  <cell> 99.3<lb/> (95.2-100)<lb/>
                  </cell>
                  <cell>500</cell>
                  <cell> 95.90<lb/> (90.1-100)<lb/>
                  </cell>
               </row>
               <row role="data">
                  <cell/>
                  <cell/>
                  <cell>1000</cell>
                  <cell> 99.67<lb/> (95.4-100)<lb/>
                  </cell>
                  <cell/>
                  <cell/>
               </row>
            </table>

            <p>Clearly, the variables used in this study can accurately distinguish texts without
               the addition of weights and calculations that may obscure interpretation. A very few
               brief examples should suffice to make this point. </p>
            <p>According to the application of Burrows’s Delta to the morpho-syntactic variables
               presented here, the two stylistically most distinct works in our corpus of English
               fiction are James Fenimore Cooper’s <title rend="italic">The Last of the Mohicans
               </title>(1826) and Mark Twain’s <title rend="italic">The Adventures of Huckleberry
                  Finn </title>(1885). The most distinctive feature in the set (where the two
               authors differ by almost four standard deviations) is Cooper’s fondness for an
               oblique dependent of a verb that itself is modified by its own dependent. In UD
               grammar, the oblique of a verb is a nominal dependent of a verb that is not an
               argument of the verb. Here is an example from Cooper’s first paragraph: <quote
                  rend="inline">The hardy colonist, and the trained European who fought at his side
                  ….</quote> Here, <emph>side </emph>in <q>at his side</q> is an oblique of
                  <emph>fought </emph>and is modified by its dependent possessive <emph>his.
               </emph>If the annotations created by <q>udpipe</q> are to be trusted, Cooper used
               such structures to an unusual degree, while Twain avoided them. Comparing Cooper to
               the entire English corpus, we find that the author was also inordinately fond of
               modifying the object of a verb with a dependency.<note> In UD grammar, object does
                  not refer only to a direct object of a transitive verb, but to the second argument
                  of any verb. For example, in <q>she went to the store</q>
                  <emph>store </emph>would be considered the object of <emph>went </emph>on the
                  assumption that, with that verb, an expression of goal is usually
                  mandatory.</note> The unusually high frequency of this construction remains even
               if we control for the number of objects in general, as well as when we consider the
               total frequency of all verbs.</p>
            <p>To examine a morphologically more complex language, in our German corpus the work
               with the greatest mean distance to all the others is Arthur Achleitner’s <title
                  rend="italic">Im grünen Tann </title>(1897?). One of the strongest <title
                  rend="italic">differentiae </title>is the author’s marked preference for verbs in
               the Present Indicative Active 3rd-person Singular. Because the data set contains
               values for simple morphological elements as well as combination, we can quickly see
               that Achleitner’s verbal tendency can be explained in terms of his very sharp
               relative avoidance of past tense verbs, alongside a more moderate relative deficit of
               plural verbs; we may assume that mood, voice, and person do not play a significant
               role in this stylistic peculiarity. With respect to nominal forms, Achleitner shows a
               relatively high number of nouns with a dependent definite article; this frequency is
               only partly explained by the relative number of nouns in general. At the same time,
               the frequency of pronouns is much lower than average. Accordingly, we might
               formulate, perhaps for our students, a “thumbnail” outline of Achleitner’s stylistic
               quirks: he loves <foreign xml:lang="de"><emph>liebt</emph></foreign> and hates
                  <foreign xml:lang="de"><emph>liebte</emph></foreign>; loves <foreign xml:lang="de"
                     ><emph>der, die, das</emph></foreign> and hates <foreign xml:lang="de"
                     ><emph>er, sie, es.</emph></foreign> It is a gross simplification to be sure,
               but it emphasizes the transparency of stylistic distinctions drawn on the basis of
               our feature set.</p>
            <p>In conclusion, the evidence presented in this study has shown that morpho-syntactic
               features can form a basis for the successful classification of texts. Results are
               good across languages ranging from the morphologically complex (e.g., Polish) to the
               simple (e.g., English). And, while it is unlikely that the frequency of
               morpho-syntactic elements is unaffected by topic, genre, etc., we can reasonably
               suppose that it is less affected by such elements external to the author than are
               variables based primarily on aspects of vocabulary. Therefore, morpho-syntactic
               features seem essential to characterizing an author’s stylistic signature. </p>
            <p>A second goal of the present work has been to demonstrate that we do not need to rely
               on expensive expert-annotated data to provide satisfactory syntactic information.
               Automated parsers such as udpipe are able to output Universal Dependency annotation
               that is sufficient to form the basis of effective variables representing both the
                  <q>shallow</q> and <q>deep</q> syntax of a text. Because, with most variable sets
               used in the attribution literature, noise tends to overwhelm information when text
               size decreases sufficiently, this investigation has focused on short texts. Even with
               texts of 50 tokens, accuracy remains many times better than random chance (74%-53%).
               These results were achieved with the most naïve methods of feature selection and
               without any optimization of the parsing or classifying algorithms. We may thus
               suggest that further explorations of the value of UD parsing for classification
               promise to be productive. </p>
            <p>Finally, we have argued that the morpho-syntactic features discussed here are
               advantageous, at least from the point of view of interpretability and pedagogical
               usefulness. Unlike n-grams, for example, morpho-syntactic variables represent
               traditional grammatical terms and categories. Unlike function words, such variables —
               at least when the preservation of <q>degrees of freedom</q> is emphasized in their
               construction — contain information based on every word in a text. As a result,
               stylistic traits formulated in terms of these variables are easily identified and
               illustrated in a way relatively more likely to be persuasive in the classroom or in
               the pages of a specialized literary journal.</p>
            <p>In sum, this investigation of short text classification on the basis of
               machine-generated Universal Dependency annotation indicates that, at the very least,
               morpho-syntax should occupy a larger role in the future development of text
               attribution. Perhaps it should even take a central role.</p>
         </div>


         <div>

            <head>Appendix: List of Works in Corpora</head>
            <p>English, Finnish, German, and Spanish texts are taken from Project Gutenberg (<ref
                  target="https://www.gutenberg.org/">https://www.gutenberg.org/</ref>) and are
               listed here by author and title. Polish texts are drawn from the web site of The
               Computational Stylistics Group (<ref
                  target="https://computationalstylistics.github.io/"
                  >https://computationalstylistics.github.io/</ref>; texts at <ref
                  target="https://github.com/computationalstylistics/100_polish_novels"
                  >https://github.com/computationalstylistics/100_polish_novels</ref>). No
               bibliographic data are provided on this site. Thus, Polish texts are listed by the
               file name used by the repository.</p>
            <div>
               <head>English Corpus</head>
               <p>Alcott, <title rend="italic">Little Women</title>
                  <lb/> Austen, <title rend="italic">Pride and Prejudice</title>
                  <lb/> Barrie, <title rend="italic">Peter Pan</title>
                  <lb/> Baum, <title rend="italic">The Wonderful Wizard of Oz</title>
                  <lb/> Bronte, <title rend="italic">Wuthering Heights</title>
                  <lb/> Cather, <title rend="italic">My Antonia</title>
                  <lb/> Christie, <title rend="italic">The Mysterious Affair at Styles</title>
                  <lb/> Conan Doyle, <title rend="italic">The Hound of the Baskervilles</title>
                  <lb/> Cooper . <title rend="italic">The Last of the Mohicans</title>
                  <lb/> Dickens, <title rend="italic">A Christmas Carol</title>
                  <lb/> Eliot, <title rend="italic">Middlemarch</title>
                  <lb/> Hardy, <title rend="italic">Tess of the d'Urbervilles</title>
                  <lb/> Joyce, <title rend="italic">A Portrait of the Artist as a Young Man</title>
                  <lb/> London, <title rend="italic">White Fang</title>
                  <lb/> Melville, <title rend="italic">Moby Dick</title>
                  <lb/> Montgomery, <title rend="italic">Anne of Green Gables</title>
                  <lb/> Shelly, <title rend="italic">Frankenstein</title>
                  <lb/> Sinclair, <title rend="italic">The Jungle</title>
                  <lb/> Twain, <title rend="italic">Adventures of Huckleberry Finn</title>
                  <lb/> Wharton, <title rend="italic">Ethan Frome</title>
                  <lb/></p>
               <div>
                  <head>Finnish Corpus</head>
                  <p>Aho, <title rend="italic">Hellmannin herra; Esimerkin vuoksi; Maailman
                        murjoma</title>
                     <lb/> Airola, <title rend="italic">Keksijän voitto</title>
                     <lb/> Alkio, <title rend="italic">Eeva</title>
                     <lb/> Anttila, <title rend="italic">Hallimajan nuoret</title>
                     <lb/> Canth, <title rend="italic">Köyhää kansaa; Salakari</title>
                     <lb/> Elster, <title rend="italic">Päivän valaisemia pilven hattaroita</title>
                     <lb/> Ervast, <title rend="italic">Haaveilija</title>
                     <lb/> Gummerus, <title rend="italic">Peritäänkö vihakin? </title>
                     <lb/> Haahti, <title rend="italic">Valkeneva tie</title>
                     <lb/> Hahnsson, <title rend="italic">Huutolaiset</title>
                     <lb/> Hannikainen, <title rend="italic">Erakkojärveläiset</title>
                     <lb/> Heman, <title rend="italic">Kysymysmerkkejä</title>
                     <lb/> Högman, <title rend="italic">Merimiehen matkamuistelmia 1</title>
                     <lb/> Ivalo, <title rend="italic">Aikansa lapsipuoli</title>
                     <lb/> Jääskeläinen, <title rend="italic">Iloisia juttuja IV</title>
                     <lb/> Jahnsson, <title rend="italic">Hatanpään Heikki ja hänen
                        morsiamensa</title>
                     <lb/> Järnefelt, <title rend="italic">Isänmaa</title>
                     <lb/> Järventaus, <title rend="italic">Kaukainen onni</title>
                     <lb/> Järvi, <title rend="italic">Harry</title>
                     <lb/> Kataja, <title rend="italic">Lain varjolla</title>
                     <lb/></p>
               </div>
               <div>
                  <head>German Corpus</head>
                  <p>Achleitner, <title rend="italic">Im grünen Tann</title>
                     <lb/> Ahlefeld, <title rend="italic">Die Bekanntschaft auf der Reise</title>
                     <lb/> Aldersfeld-Ballestrem, <title rend="italic">Die Falkner vom Falkenhof
                        1</title>
                     <lb/> Bechstein, <title rend="italic">Der Dunkelgraf</title>
                     <lb/> Bernhard, <title rend="italic">Die Glücklichen</title>
                     <lb/> Bonsels, <title rend="italic">Eros und die Evangelien</title>
                     <lb/> Feuchtwanger, <title rend="italic">Die häßliche Herzogin</title>
                     <lb/> Fontane, <title rend="italic">Effi Briest</title>
                     <lb/> Gjellerup, <title rend="italic">Der Pilger Kamanita</title>
                     <lb/> Goethe, <title rend="italic">Wilhelm Meisters Lehrjahre 1</title>
                     <lb/> Grillparzer, <title rend="italic">Das Kloster bei Sendomir</title>
                     <lb/> Hauff, <title rend="italic">Der Mann im Mond</title>
                     <lb/> Hesse, <title rend="italic">Unterm Rad</title>
                     <lb/> Hoffmann, <title rend="italic">Klein Zaches, genannt Zinnober</title>
                     <lb/> Huch, <title rend="italic">Der Fall Deruga</title>
                     <lb/> Mann, H., <title rend="italic">Der Untertan</title>
                     <lb/> Mann, Th., <title rend="italic">Buddenbrooks</title>
                     <lb/> Meyrink, <title rend="italic">Der Golem</title>
                     <lb/> Spyri, <title rend="italic">Heimatlos</title>
                     <lb/> Zweig, <title rend="italic">Die Liebe der Erika Ewald</title>
                     <lb/></p>
               </div>
               <div>
                  <head>Spanish Corpus</head>
                  <p>Blasco Ibáñez, <title rend="italic">La Catedral</title>
                     <lb/> Caro, <title rend="italic">Amar es vencer</title>
                     <lb/> Carrere, <title rend="italic">La copa de Verlaine</title>
                     <lb/> Conscience, <title rend="italic">La niña robada</title>
                     <lb/> Delgado, <title rend="italic">Angelina</title>
                     <lb/> Dourliac, <title rend="italic">Liette</title>
                     <lb/> Espina, <title rend="italic">Agua de Nieve</title>
                     <lb/> Fernández y González, <title rend="italic">Los hermanos
                        Plantagenet</title>
                     <lb/> Gil y Carrasco, <title rend="italic">El señor de Bembibre</title>
                     <lb/> Halévy, <title rend="italic">El Abate Constantin</title>
                     <lb/> Larra, <title rend="italic">Si yo fuera rico!</title>
                     <lb/> Larreta, <title rend="italic">La gloria de don Ramiro</title>
                     <lb/> Leumann, <title rend="italic">Adriana Zumarán</title> <lb/> Mancey,
                        <title rend="italic">Las Solteronas</title>
                     <lb/> Ocantos, <title rend="italic">Quilito</title>
                     <lb/> Ortega y Frías, <title rend="italic">La Gente Cursi</title>
                     <lb/> Palacio, <title rend="italic">La alegría del capitán Ribot</title>
                     <lb/> Pardo Bazán, <title rend="italic">Una Cristiana</title>
                     <lb/> Pereda, <title rend="italic">Al primer vuelo</title>
                     <lb/></p>
               </div>
               <div>
                  <head>Polish Corpus (20 texts used for logistic regression)</head>
                  <p>balucki_burmistrz_1887.txt<lb/> beczkowska_gniezdzie_1899.txt<lb/>
                     dabrowska_nocednie2_1932.txt<lb/> dmochowska_dwor_1903.txt<lb/>
                     domanska_paziowie_1910.txt<lb/> godlewska_kwiat_1897.txt<lb/>
                     iwaszkiewicz_czerwone_1934.txt<lb/> kaczkowski_olbrachtowi_1889.txt<lb/>
                     kossak_oreza_1937.txt<lb/> krzemieniecka_fatum_1904.txt<lb/>
                     kuncewiczowa_twarz_1928.txt<lb/> marrene_mezowie_1875.txt<lb/>
                     mostowicz_hanki_1939.txt<lb/> nalkowska_romans_1923.txt<lb/>
                     prus_faraon_1897.txt<lb/> rodziewicz_lato_1920.txt<lb/>
                     sienkiewicz_quo_1896.txt<lb/> sygietynski_calvados_1884.txt<lb/>
                     zapolska_tagiejew_1905.txt<lb/> zeromski_przedwiosnie_1924.txt<lb/></p>
               </div>
               <div>
                  <head>Polish Corpus (33 texts used for classification by Burrows’s Delta)</head>
                  <p>balucki_burmistrz_1887.txt<lb/> beczkowska_bedzie_1897.txt<lb/>
                     berent_diogenes_1937.txt<lb/> dabrowska_nocednie1_1931.txt<lb/>
                     deotyma_panienka_1893.txt<lb/> dmochowska_dwor_1903.txt<lb/>
                     domanska_historia_1913.txt<lb/> dygasinski_as_1896.txt<lb/>
                     godlewska_kato_1897.txt<lb/> gojawiczynska_dziewczeta_1935.txt<lb/>
                     iwaszkiewicz_czerwone_1934.txt<lb/> kaczkowski_grob_1857.txt<lb/>
                     korzeniowski_emeryt_1851.txt<lb/> kossak_bog_1935.txt<lb/>
                     kraszewski_kordecki_1850.txt<lb/> krzemieniecka_fatum_1904.txt<lb/>
                     kuncewiczowa_cudzoziemka_1936.txt<lb/> makuszynski_basie_1937.txt<lb/>
                     marrene_bozek_1871.txt<lb/> mniszek_gehenna_1914.txt<lb/>
                     mostowicz_hanki_1939.txt<lb/> nalkowska_granica_1935.txt<lb/>
                     orzeszkowa_gloria_1910.txt<lb/> prus_emancypantki_1894.txt<lb/>
                     reymont_chlopi_1908.txt<lb/> rodziewicz_lato_1920.txt<lb/>
                     samozwaniec_ustach_1922.txt<lb/> sienkiewicz_rodzina_1894.txt<lb/>
                     swietochowski_twinko_1936.txt<lb/> sygietynski_wysadzony_1891.txt<lb/>
                     zapolska_tagiejew_1905.txt<lb/> zarzycka_wiatr_1934.txt<lb/>
                     zeromski_syzyfowe_1897.txt<lb/></p>
               </div>
            </div>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="bittner2003" label="Bittner et al. 2003">Bittner, D., Dressler, W. U., and
               Kilani-Schoch, M. (eds). <title rend="italic">Development of Verb Inflection in First
                  Language Acquisition: A Cross-Linguistic Perspective.</title> Mouton de Gruyter,
               Berlin (2003).</bibl>
            <bibl xml:id="eder2015" label="Eder 2015">Eder, M. <title rend="quotes">Does Size
                  Matter? Authorship Attribution, Small Samples, Big Problem</title>, <title
                  rend="italic">Digital Scholarship in the Humanities</title>, 30.2 (2015):
               167–82.</bibl>
            <bibl xml:id="eder2017" label="Eder 2017">Eder, M. <title rend="quotes">Short Samples in
                  Authorship Attribution: A New Approach,</title>
               <title rend="italic">Digital Humanities 2017: Conference Abstracts</title>. McGill
               University, Montreal (2017), pp. 221–24. <ref
                  target="https://dh2017.adho.org/abstracts/341/341.pdf"
                  >https://dh2017.adho.org/abstracts/341/341.pdf</ref>.</bibl>
            <bibl xml:id="eder2016" label="Eder 2016">Eder, M., Rybicki, J., and Kestemont, M.
                  <title rend="quotes">Stylometry with R: a Package for Computational Text
                  Analysis</title>, <title rend="italic">R Journal</title>, 8.1 (2016):
               107–21.</bibl>
            <bibl xml:id="evert2017" label="Evert et al. 2017">Evert, S. Proisl, T., Fotis, J.,
               Reger, I., Pielström, S., Schöch, Ch., and Vitt, Th. <title rend="quotes"
                  >Understanding and Explaining Delta Measures for Authorship Attribution, Digital
                  Scholarship in the Humanities</title>, <title rend="italic">Digital Scholarship in
                  the Humanities</title> 32.suppl. 2 (2017): ii4–ii16. <ref
                  target="https://doi.org/10.1093/llc/fqx023"
                  >https://doi.org/10.1093/llc/fqx023</ref>.</bibl>
            <bibl xml:id="fan2008" label="Fan 2008">Fan, R.–E., Chang, K.–W., Hsieh, C.–J., Wang,
               X.–R., and Lin, C.–J. <title rend="quotes">Liblinear: a Library for Large Linear
                  Classification.</title>
               <title rend="italic">Journal of Machine Learning Research</title>, 9 (2008):
               1871–74.</bibl>
            <bibl xml:id="gabay2021" label="Gabay 2021">Gabay, S. <title rend="quotes">Beyond
                  Idiolectometry? On Racine’s Stylometric Signature</title>, <title rend="italic"
                  >Computational Humanities Research Conference</title>, November 17–19, 2021,
               Amsterdam, The Netherlands (2021): 359-76. <ref
                  target="http://ceur-ws.org/Vol-2989/long_paper39.pdf"
                  >http://ceur-ws.org/Vol-2989/long_paper39.pdf</ref></bibl>
            <bibl xml:id="gorman2016" label="Gorman and Gorman 2016">Gorman, R. and Gorman, V.
                  <title rend="quotes">Approaching Questions of Text Reuse in Ancient Greek Using
                  Computational Syntactic Stylometry</title>, <title rend="italic">Open
                  Linguistics</title>, 2 (2016): 500–10.</bibl>
            <bibl xml:id="gorman2020" label="Gorman 2020">Gorman, R. <title rend="quotes">Author
                  Identification of Short Texts Using Dependency Treebanks without
                  Vocabulary.</title>
               <title rend="italic">Digital Scholarship in the Humanities</title>, 35.4 (2020):
               812–25. <ref target="https://doi.org/10.1093/llc/fqz070"
                  >https://doi.org/10.1093/llc/fqz070</ref></bibl>
            <bibl xml:id="helleputte2017" label="Helleputte 2017">Helleputte, T., Gramme, P., and
               Paul, J. <title rend="italic">LiblineaR: Linear Predictive Models Based on the
                  Liblinear C/C++ Library.</title> R package version 2.10–8 (2017).</bibl>
            <bibl xml:id="lakretz2020a" label="Lakretz et al. 2020a">Lakretz, Y., Dehaene, S., and
               King, J-R. <title rend="quotes">What Limits Our Capacity to Process Nested Long-Range
                  Dependencies in Sentence Comprehension?</title>
               <title rend="italic">Entropy</title> 22.4 (2020a): 446. <ref
                  target="doi.org/10.3390/e2204044">doi.org/10.3390/e2204044</ref>6.</bibl>
            <bibl xml:id="lakretz2020b" label="Lakretz et al. 2020b">Lakretz, Y., Hupkes, D.,
               Vergallito, A., Marelli, M., Baroni, M., and Dehaene, S. <title rend="quotes"
                  >Exploring Processing of Nested Dependencies in Neural-Network Language Models and
                  Humans.</title> Preprint: arXiv:2006.11098 [cs.CL] (2020b). <ref
                  target="https://www.researchgate.net/publication/342352527_Exploring_Processing_of_Nested_Dependencies_in_Neural-Network_Language_Models_and_Humans"
                  >https://www.researchgate.net/publication/342352527_Exploring_Processing_of_Nested_Dependencies_in_Neural-Network_Language_Models_and_Humans</ref>
            </bibl>
            <bibl xml:id="luyckx2008" label="Luyckx and Daelemans 2008">Luyckx, K. and Daelemans, W.
                  <title rend="quotes">Authorship Attribution and Verification with Many Authors and
                  Limited Data.</title> In Donia Scott and Hans Uszkoreit (eds), <title
                  rend="italic">Proceedings of the 22nd International Conference on Computational
                  Linguistics</title>, Coling (2008), pp. 513–20. <ref
                  target="https://dl.acm.org/citation.cfm?id=1599146"
                  >https://dl.acm.org/citation.cfm?id=1599146</ref> .</bibl>
            <bibl xml:id="luyckx2011" label="Luyckx and Daelemans 2011">Luyckx, K. and Daelemans, W.
                  <title rend="quotes">The Effect of Author Set Size and Data Size in Authorship
                  Attribution</title>, <title rend="italic">Literary and Linguistic
                  Computing</title>, 26.1 (2011): 35–55.</bibl>
            <bibl xml:id="marzi2019" label="Marzi et al. 2019">Marzi, C., Ferro, M., and Pirrelli,
               V. <title rend="quotes">A Processing-Oriented Investigation of Inflectional
                  Complexity</title>, <title rend="italic">Frontiers in Communication</title>, 4
               (2019). doi: <ref target="10.3389/fcomm.2019.00048"
               >10.3389/fcomm.2019.00048</ref></bibl>
            <bibl xml:id="nivre2015" label="Nivre 2015">Nivre, J. <title rend="quotes">Towards a
                  Universal Grammar for Natural Language Processing.</title> In A. Gelbukh (ed.),
                  <title rend="italic">Computational Linguistics and Intelligent Text
                  Processing.</title> CICLing 2015. Lecture Notes in Computer Science, 9041 (2015).
               Springer, Cham. DOI: <ref target="10.1007/978-3-319-18111-0_1"
                  >10.1007/978-3-319-18111-0_1</ref></bibl>
            <bibl xml:id="shannon1948" label="Shannon 1948">Shannon, C. E. <title rend="quotes">A
                  Mathematical Theory of Communication</title>, <title rend="italic">The Bell System
                  Technical Journal</title>, 27 (1948): 379-423, 623-56.</bibl>
            <bibl xml:id="sidorov2012" label="Sidorov et al. 2012">Sidorov, G., Velasquez, F.,
               Stamatatos, E., Gelbukh, A., and Chanona–Hernández, L. <title rend="quotes">Syntactic
                  Dependency-Based N-Grams as Classification Features</title>, <title rend="italic"
                  >Lecture Notes on Computer Science</title>, 7630 (2012): 1–11.</bibl>
            <bibl xml:id="simon2007" label="Simon 2007">Simon, R. <title rend="quotes">Resampling
                  Strategies for Model Assessment and selection.</title> In W. Dubitzky, M. Granzow,
               and D. Berrar. (eds.), <title rend="italic">Fundamentals of Data Mining in Genomics
                  and Proteomics</title>. Springer, Boston (2007), pp. 173–86.</bibl>
            <bibl xml:id="stamatatos2009" label="Stamatatos 2009">Stamatatos, E. <title
                  rend="quotes">A Survey of Modern Authorship Attribution Methods</title>, <title
                  rend="italic">Journal of the American Society for Information Science and
                  Technology</title>, 60.3 (2009): 538-556. doi: <ref target="10.1002/asi.21001"
                  >10.1002/asi.21001</ref></bibl>
            <bibl xml:id="swain2017" label="Swain et al. 2017">Swain, S., Mishra, G., and Sindhu, C.
                  <title rend="quotes">Recent Approaches on Authorship Attribution Techniques — An
                  Overview</title>, <title rend="italic">International Conference of Electronics,
                  Communication and Aerospace Technology</title> (ICECA), Coimbatore (2017), pp.
               557-66. doi: <ref target="10.1109/ICECA.2017.8203599"
                  >10.1109/ICECA.2017.8203599</ref>.</bibl>
            <bibl xml:id="varela2018" label="Varela et al. 2018">Varela, P. J., Albonico, M.,
               Justino, E. J. R., and Bortolozzi, F. <title rend="quotes">A Computational Approach
                  for Authorship Attribution on Multiple Languages,</title> 2018 <title
                  rend="italic">International Joint Conference on Neural Networks</title> (IJCNN),
               Rio de Janeiro (2018), pp. 1-8, doi: <ref target="10.1109/IJCNN.2018.8489704"
                  >10.1109/IJCNN.2018.8489704</ref>.</bibl>
            <bibl xml:id="varela2016" label="Varela et al. 2016">Varela, P. J., Justino, E., Britto,
               A., and Bortolozzi, F. <title rend="quotes">A Computational Approach for Authorship
                  Attribution of Literary Texts Using Sintatic Features,</title> 2016 <title
                  rend="italic">International Joint Conference on Neural Networks</title> (IJCNN),
               Vancouver, BC (2016), pp. 4835-42. doi: <ref target="10.1109/IJCNN.2016.7727835"
                  >10.1109/IJCNN.2016.7727835</ref>.</bibl>
            <bibl xml:id="wijffels2019" label="Wijffels 2019">Wijffels, J. <title rend="quotes"
                  >udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency
                  Parsing with the ‘UDPipe’ ‘NLP’ Toolkit</title>, (2019). <ref
                  target="https://CRAN.R-project.org/package=udpip"
                  >https://CRAN.R-project.org/package=udpip</ref>e</bibl>
            <bibl xml:id="williams2010" label="Willliams and Beer 2010">Williams, P. and Beer, R.
                  <title rend="quotes">Nonnnegative Decomposition of Multivariate
                  Information.</title> Preprint: arXiv:1004.2515 [cs.IT] (2010). <ref
                  target="https://arxiv.org/pdf/1004.2515.pdf"
                  >https://arxiv.org/pdf/1004.2515.pdf</ref>.</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
