<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Hidden in
                  Plain-TeX: Investigating Minimal Computing Workflows</h1>
               
               
               <div class="author"><span style="color: grey">Nabeel Siddiqui
                     </span> &lt;<a href="mailto:Nabeel88_at_gmail_dot_com" onclick="javascript:window.location.href='mailto:'+deobfuscate('Nabeel88_at_gmail_dot_com'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('Nabeel88_at_gmail_dot_com'); return false;">Nabeel88_at_gmail_dot_com</a>&gt;, College of William and Mary</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Hidden%20in%20Plain-TeX%3A%20Investigating%20Minimal%20Computing%20Workflows&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Siddiqui&amp;rft.aufirst=Nabeel&amp;rft.au=Nabeel%20Siddiqui"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>Drawing on software studies, data feminism, digital rhetoric studies, information
                     science, and the history of computing, this paper foregrounds Markdown as a cultural
                     object to analyze the social, cultural, and political pressures surrounding the
                     digital humanities. Rather than beginning with contemporary discourse, it draws
                     parallels between Markdown and Donald Knuth's TeX. From the 1970s to 1990s, academic
                     researchers used TeX to construct plain-text scholarship in mathematics and the hard
                     sciences to enhance typography. Most academic saw these concerns as holding marginal
                     importance and quickly abandoned the approach for WYSIWYG word processors. Drawing
                     on
                     queer theorist Michael Warner, I argue that the community surrounding TeX responded
                     reactionary to these transformations by forming a counterpublic constituted through
                     a
                     circulation of texts bemoaning word processors [<a class="ref" href="#warner2002">Warner 2002</a>]. This
                     counterpublic persisted well into the 2000s but only made headway amongst niche
                     users.</p>
                  
                  <p>This plain-text focused counterpublic mutated in response to the neoliberalization
                     of
                     higher ed. Rather than viewing plain-text as “sustainable
                     scholarship,” academic lifehacking and minimal computing embraced what
                     David Golumbia calls computationalism, an ideological construct postulating social,
                     political, economic, and cultural ills as cybernetic systems to optimize — or
                     “hacked.” Minimal computing’s fetishization and casting of
                     plain-text as transformative mirrors similar discourses amongst older lifehacking
                     enthusiasts. Although plain-text scholarship is not representative of minimal
                     computing as a totality, minimal computing's emphasis  on workflows leads to few of
                     the supposed benefits advocates profess, and in many cases, worsens inequalities.</p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Daring Markdown</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">In 2004, John Gruber, a user interface designer and blogger, released a new markup
                     language entitled Markdown featuring what he claimed was simple and self-evident syntax.
                     “The best way to get a feel for Markdown's formatting syntax,” he asserted, “is simply
                     to look at a Markdown-formatted document” [<a class="ref" href="#gruber2004">Gruber 2004</a>]. Gruber’s language relied on a
                     set of simple conventions mimicking those in email correspondence. Users could surround
                     words in single and double asterisks to signal italics and bold fonts, respectively
                     and
                     create header levels by preceding header titles with a corresponding number of hashtags
                     (i.e. # for H1 and ## for H2). This simplicity was an explicit design goal that
                     permeated Markdown's design language and resulted in its quick adoption. </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Markdown's simplicity arises from its composition and plain text file format. “The
                     overriding design goal of Markdown,” writes Gruber, “is the idea that a
                     Markdown-formatted document should be publishable as-is, <em class="emph">as plain
                        text</em>, without looking like it's been marked up with tags or formatting
                     instructions” [<a class="ref" href="#gruber2004">Gruber 2004</a>, emphasis added]. According to the Unicode Standard, plain
                     text files are a “pure sequence of characters” that “contain enough information to
                     permit the text to be rendered legibly, and nothing more” [<a class="ref" href="#allen2012">Allen et al. 2012</a>, 18]. It
                     contrasts plain text with styled or rich text, which is “any text representation
                     consisting of plain text plus added information such as a language identifier, font
                     size, color, hypertext links, and so on” [<a class="ref" href="#allen2012">Allen et al. 2012</a>, 18]. Plain text files
                     benefit from simplicity, portability across different platforms, and longevity. These
                     qualities make it more than just a file type. For Dennis Yi Tenen, plain text is an
                     “editorial method of text transcription that is both 'faithful to the text of its
                     source' and is 'easier to read than the original documents ” [<a class="ref" href="#tenen2017">Tenen 2017</a>, 3]. In contrast
                     to the maximization of “system-centric ideals,” he sees plain text as embodying
                     minimalism [<a class="ref" href="#tenen2017">Tenen 2017</a>, 3].</div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">Plain text exposes the structure of digital encoding and resists what N. Katherine
                     Nayles calls the “flickering signifier.” In Saussurean semiotics, signifiers are
                     unitary, durable, and lack internal structures. In contrast, Hayles argues that
                     “flickering signifiers” infuse computational systems [<a class="ref" href="#hayles1999">Hayles 1999</a>, 25–49]. These are
                     signs embedded in layered informatics where small actions, like the press of a key,
                     can
                     cause massive changes in hardware and software. They literally “flicker” due to computer
                     monitors refreshing their presence multiple times per second, resulting in observers
                     always witnessing them in a state of flux [<a class="ref" href="#gitelman2002">Gitelman 2002</a>]. While plain text relies on
                     the same computational hardware, it provides a voyeuristic look into what Tenen calls
                     the “textual lamination” of digital materials [<a class="ref" href="#tenen2017">Tenen 2017</a>, 145–156]. This textual
                     lamination refers to the ways that the digital binds text on a screen in a series
                     of
                     physical and networked computational infrastructures. </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">Markup languages, like Markdown, also allow writers to structure plain-text documents
                     without aesthetic differentiations in typography. According to the Unicode Standard,
                     markup languages function by “interspersing plain text data with sequences of characters
                     that represent the additional data structure ” [<a class="ref" href="#allen2012">Allen et al. 2012</a>, 19]. By preserving
                     sequences of characters as plain text, these languages can separate tags from “real
                     content.” For many advocates, this mechanism allows writers to focus on content while
                     leaving design issues to others. </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">The separation of form and content comes with challenges. Chief amongst these is the
                     fragmentation of language protocols as practitioners expand capabilities. Gruber
                     designed Markdown to have a limited feature set, but his decisions bumped up against
                     practicality as popular websites such as GitHub, Reddit, Stack Exchange, Tumblr, and
                     WordPress began permitting users to post in Markdown. New Markdown parsers and
                     variations quickly emerged [<a class="ref" href="#teamvivaldi2018">Team Vivaldi 2018</a>]. For Gruber, these “flavors” provided
                     evidence of open ingenuity rather than chaos. When asked why he did not seek
                     standardization, Gruber responded “I believe Markdown's success is <em class="emph">due
                        to</em>, not in spite of, its lack of standardization. And its success is not
                     disputable” [<a class="ref" href="#gruber2014">Gruber 2014</a>].</div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">During the flurry of activity to expand Markdown, John MacFarlane, Professor of
                     Philosophy at UC Berkeley, released pandoc, a command-line utility that reads various
                     Markdown flavors and outputs them to different formats [<a class="ref" href="#macfarlene2020">MacFarlane 2020</a>]. This allowed
                     Markdown to serve as a protocol for creating diagrams, slide decks, and even websites.
                     Most importantly, pandoc provided a way for Markdown to include citations and footnotes
                     that resulted in it gaining popularity amongst a niche group of academics for scholarly
                     research. </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Pandoc permitted Markdown to serve as a viable option for various genres of writing,
                     but
                     what does Markdown have to do with minimal computing? According to Alex Gil, “minimal
                     computing is the application of minimalist principles to computing” [<a class="ref" href="#gil2015a">Gil 2015</a>]. He draws
                     inspiration from designer and architect Ernesto Oroza’s extemporal ethnography of
                     Cuba’s
                     DIY culture. In Havana, Oroza finds a city where participants resist bureaucratic
                     intervention and create idiosyncratic habitats. He stresses the “moral modulor,” who by
                     necessity sees the city through a lens of survival [<a class="ref" href="#oroza2020">Oroza 2020</a>]. “The moral modulor is
                     an individual who has the impulse to rebuild human life, and this is something he
                     does
                     for his children or his family, ”states Oroza, “His condition allows him to discriminate
                     against the superfluous or the useless” [<a class="ref" href="#oroza2016">Oroza 2016</a>]. Although these practices in Havana
                     may seem far from digital humanities research, they provide a paradigm for how digital
                     humanities scholars and practitioners should view infrastructure more broadly. Put
                     another way, Gil sees in Havana's “architecture of necessity” a catalyst for a digital
                     humanities community that discards — read: “minimizes” — the extraneous. He asks us to
                     look self-reflexively at our operations and orient them around the question of “What do
                     we need?” [<a class="ref" href="#gil2015a">Gil 2015</a>].</div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Minimal computing includes a myriad of definitions and practices. According to Jentery
                     Sayers, the <em class="emph">minimal</em> in minimal computing can refer to design,
                     usage, consumption, maintenance, barriers, Internet, externals, space, and technical
                     language. Yet, he leaves open the possibility for other forms of minimalism: “I did not
                     account for all the minimals or their particulars here,” [<a class="ref" href="#sayers2016">Sayers 2016</a>]. Likewise, Gil
                     states, “minimal computing is in the eye of the beholder” [<a class="ref" href="#gil2015a">Gil 2015</a>]. </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">Due to the often-ambiguous definitions of minimal computing, we must turn to
                     self-identified practitioners to understand its relationship with Markdown. As Gregory
                     Bateson notes, cultural activities, such as play, serve as meta-communication through
                     their self-referential nature [<a class="ref" href="#bateson2008">Bateson and Bateson 2008</a>]. In minimal computing,
                     plain-text scholarship in Markdown signals, “This is minimal computing.” This switch
                     from the ontological to the methodological is common when defining the digital
                     humanities. As Rafael Alvarado notes, digital humanities, like minimal computing,
                     has no
                     real definition, and it is better to see digital humanities as a <em class="emph">social</em> field rather than an ontological one:</div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">Instead of a definition, we have a genealogy, a network of family resemblances among
                     provisional schools of thought, methodological interests, and preferred tools, a history
                     of people who have chosen to call themselves digital humanists and who in the process
                     of
                     trying to define the term are creating that definition [<a class="ref" href="#alvarado2011">Alvarado 2011</a>]. </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">As we look for a “genealogy” of minimal computing tools, Markdown emerges as a recurring
                     theme. Jentery Sayers and Alex Gil, self-identified minimal computing practitioners,
                     frequently tout Markdown-inspired static site generators and plain-text scholarship
                     as
                     epitomizing minimal values and assuring sustainability. For example, in an analysis
                     of
                     Tenen and Grant Wythoff's workflow for sustainable plain-text scholarship in Markdown,
                     Gil praises the workflow’s ability to produce “minimal knowledge with the production of
                     a minimal artifact, without creating necessary friction for the readers” [<a class="ref" href="#gil2015a">Gil 2015</a>]. He
                     laments “user-friendly” interfaces for their potential to lead to what Matthew
                     Kirschenbaum calls a “haptic fallacy”: “the belief that electronic objects are
                     immaterial simply because we cannot reach out and touch them” (Kirschenbaum 2003).
                     Likewise, Sayers asserts that Jekyll, a Markdown reliant static site generator, is
                     representative of “minimal design” [<a class="ref" href="#sayers2016">Sayers 2016</a>]. Jekyll forms the basis for prominent
                     minimal computing websites, such as those for the Minimal Computing Working Group,
                     and
                     projects, such as Ed and Wax. We should note that Markdown is, of course, not the
                     totality of minimal computing practices. </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Markdown as a digital humanities practice is noticeable for not just its technical
                     innovation but the discourses surrounding it. These discourses stress Markdown as
                     a
                     force for sustainable scholarship, easing barriers of entry to digital humanities
                     in the
                     Global South, and as a means for subverting governmental surveillance. According to
                     Bradley Dilger and Jeff Rice, “Despite the predominant roles markup plays in online
                     writing, and despite the social, cultural, rhetorical, and technological implications
                     of
                     these roles, markup is often taken for granted as merely the code behind the text”
                     [<a class="ref" href="#dilger2010">Dilger and Rice 2010</a>]. In response, this article explores the sustainability of
                     Markdown as a digital humanities practice by looking back to a key precursor: Donald
                     Knuth’s TeX. </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">Readers may wonder why I focus on TeX and not other markup languages, such as HTML
                     or
                     TEI. Markup languages emerged in the 1960s when users used markups or “flags” to
                     distinguish between lower and upper-case letters. These flags were often proprietary
                     making it difficult for different computational platforms to interact with one another.
                     In 1967, Michael Kay implored scholars to create a “standard code in which any text
                     received from an outside source can be assumed to be” [<a class="ref" href="#kay1967">Kay 1967</a>]. The most influential
                     of these markup languages was the Generalized Markup Language (GML) created by Charles
                     Goldfarb, Edward Mosher, and Raymond Lorie at IBM in 1969. GML served as a foundation
                     for the Standard Generalized Markup Language (SGML) released in 1986, which in turn
                     provided a protocol for creating new markup languages like HTML and XML [<a class="ref" href="#goldfarb1999">Goldfarb 1999</a>].</div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">TeX and Markdown are notable for their embrace by scholars as computational tools
                     for
                     printed text. This is not to say that they do not use other markup languages, such
                     as
                     HTML and TEI, to disseminate research. In fact, digital humanities scholars often
                     stress
                     alternative publishing platforms, such as online exhibits, blogs, and digital maps
                     as
                     meaningful forms of scholarship themselves. However, this is in contrast to TeX and
                     Markdown where the goal is to create a more efficient workflow to output digital
                     materials in analog formats like academic journals and monographs. </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">In this paper, TeX's development and community provide a case study and catalyst for
                     the
                     social and cultural development of Markdown as a minimal computing practice. From
                     the
                     1970s to the 1990s, academic researchers used TeX to construct plain-text scholarship
                     in
                     mathematics and the hard sciences to enhance typographical output. Most academics
                     saw
                     these concerns as holding marginal importance and abandoned TeX for What You See Is
                     What
                     You Get (WYSIWYG) word processors, especially in the humanities. Drawing on queer
                     theorist Michael Warner, I argue that the community surrounding TeX responded
                     reactionarily to these transformations by forming a counterpublic constituted through
                     a
                     myriad of texts that bemoaned word processors [<a class="ref" href="#warner2002">Warner 2002</a>]. This counterpublic
                     persisted well into the 2000s but only made headway amongst a niche scholarly audience.
                     After exploring TeX, I conclude by looking at the lessons it provides digital humanities
                     scholars about embracing minimal computing practices like Markdown-based scholarship.
                     As
                     I make evident, although minimal computing encompasses a range of modalities, discourses
                     surrounding Markdown mirroring those of TeX should give pause to those seeking to
                     implement it in practice.</div>
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Taking TeX Seriously</h1>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">To understand Markdown-based scholarship in the digital humanities and growing advocacy
                     for its use, I turn to the history of computing. The history of computing remains
                     a
                     niche field and has had little overlap with the digital humanities. Historians of
                     computing have dismissed the latter as narrowly focusing on career development and
                     sidelining issues that concern other humanities scholars. As Thomas Haigh — echoing
                     Bruno Latour's criticism of modernity concludes — “We have never been digital” [<a class="ref" href="#haigh2006">Haigh 2006</a>]. 
                     Yet, computing's history can provide valuable insights into minimal computing
                     practices.</div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">Given markup's popularity amongst programmers and technical writers, it may be tempting
                     to see it as an extension of programming. However, the catalyst for plain-text
                     scholarship stems from developments and concerns about digital typography best
                     exemplified by Donald Knuth’s TeX. A mathematical prodigy, Knuth won a scholarship
                     to
                     Case Western Institute, where he tinkered with the university’s IBM 650, an early
                     mainframe computer, in his spare time. After switching concentrations from physics
                     to
                     mathematics, Knuth graduated with a Bachelor of Science in mathematics and, by a special
                     vote of the faculty, a Master of Science for exceptional work. He went on to earn
                     a
                     doctorate in mathematics from the California Institute of Technology and joined its
                     faculty as an Assistant Professor of Mathematics in 1963 [<a class="ref" href="#walden2019">Walden 2019</a>].</div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">Knuth began writing his magnum opus <cite class="title italic">The Art of Computer
                        Programming</cite>, originally a comprehensive work about compilers, full time in 1962
                     but took a break to investigate the statistical properties of linear probing. This
                     break
                     had a profound impact on Knuth who proposed a broader project to his publisher about
                     computational algorithms. By the summer of June 1965, he had completed three thousand
                     handwritten pages (roughly two thousand pages of printed text), and Addison-Wesley
                     agreed to publish the revised work in seven parts. With Knuth’s desire for <cite class="title italic">The Art of Programming</cite> to mirror the totality of computer science
                     algorithms, he continued to amend the work throughout his career. While revising the
                     second edition of the book in 1976, Knuth received galley proofs from his publisher.
                     Addison-Wesley had adopted a new digital-infused workflow, and Knuth became so aghast
                     at
                     the typographical quality that he threatened to quit the project altogether [<a class="ref" href="#knuth2007">Knuth 2007</a>].</div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">Pausing from writing <cite class="title italic">The Art of Programming</cite>, Knuth sought to
                     explore how computer science could enhance typography after seeing a high-resolution
                     digital typesetting machine in 1977. By dividing a page into discrete sections (pixels),
                     he searched algorithmically for the ideal place for ink (1) or not (0). “As a computer
                     scientist, I really identify with patterns of 0’s and 1’s; I ought to be able to do
                     something about this,” noted Knuth [<a class="ref" href="#tex2021">TeX Users Group 2021</a>]. </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">Knuth's work on typography formed the basis for TeX, and he dedicated his 1977–1978
                     academic sabbatical to the project. Approximately a year later, the American
                     Mathematical Society invited Knuth to give a lecture that he used to stress TeX’s
                     superiority and mathematical underpinning. For the attending academic audience, TeX
                     offered numerous benefits. As the TeX User Group’s official history notes, creating
                     academic articles at the time was expensive and relied on proprietary software. A
                     document created in one software program often rendered differently on competing
                     systems. TeX solved these issues by providing a free markup system that was portable
                     across platforms and geared towards academic work [<a class="ref" href="#tex2021">TeX Users Group 2021</a>].</div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">We should pause to underline that TeX's development had little to do with easing
                     composition as later enthusiasts would argue. While the portability of TeX did allow
                     researchers to exchange files without vendor lock-in, researchers were more concerned
                     about <em class="emph">preserving</em> aesthetics rather than stripping away content
                     and form. They exchanged TeX files with typographical information and choices affixed
                     in
                     document front matter so that others could compose facsimiles rather than leave
                     questions of appearance to them.</div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">Despite TeX’s advantages, it had a steep learning curve. Seeking to assist newcomers,
                     computer scientist Leslie Lamport assembled a series of macros for TeX, which he called
                     LaTeX, in 1984. LaTeX took cues from Scribe, a markup language Brian Reid created
                     for
                     his doctoral dissertation that popularized “logical design,” a philosophy that separated
                     the production of content from appearance. Reid claimed that Scribe allowed writers
                     to
                     focus on their writing rather than aesthetics, and by utilizing a small set of
                     declarations, they could still output their creations to different formats <span class="error"><a class="ref" href="#reid1981">#reid1981</a></span>.
                     Despite its innovations, many remained apprehensive about what they saw as Scribe’s
                     rigidity. Early responses touted LaTeX as “Scribe liberated from inflexible formatting
                     control” and “TeX for the masses” [<a class="ref" href="#mittelbach2004">Mittelbach et al. 2004</a>]. In his original manual <cite class="title italic">LaTeX: A Document Preparation System</cite>, Lamport acknowledges his
                     desire for a simpler system. “In turning TeX into LaTeX, I have tried to convert a
                     highly-tuned racing car into a comfortable family sedan,” writes Lamport, “The family
                     sedan isn't meant to go as fast as a racing car or be as exciting to drive, but it's
                     comfortable and gets you to the grocery store with no fuss” [<a class="ref" href="#lamport1986">Lamport 1986</a>, xiii].</div>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">The rhetoric around LaTeX differed significantly from TeX. Lamport, like Reid, saw
                     logical design as a chance to ease composition and to prevent vendor lock-in. This
                     is
                     not to say that concerns of typography disappeared, but they had diminished
                     considerably. TeX’s typographic benefits originally drew Lamport’s interest, but his
                     assemblage of easy-to-use macros shifted how advocates pitched TeX's benefits and
                     would
                     serve as a catalyst for later claims by Markdown enthusiasts. Still, most academics
                     continued to find markup-oriented scholarship to be unnecessarily difficult to use,
                     and
                     with WYSIWYG word processing software more readily available, all but the most diehard
                     users defected, especially in the humanities. </div>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">Word processing had emerged during the 1970s as an organizational paradigm for
                     centralizing corporate information [<a class="ref" href="#haigh2006">Haigh 2006</a>]. Businesses created word processing
                     departments that focused on text production in an attempt to ease the burden on other
                     departments. Manufacturers, such as IBM and Wang, created standalone systems for this
                     new information management paradigm, and by the early 1980s, these machines were
                     commonplace in corporate America. Word processing software dedicated solely to text
                     editing, on the other hand, was of marginal purpose. As Thomas Haigh notes:</div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">
                     <blockquote>
                        <p>It … seemed no more sensible to use a computer to edit than to travel to the shopping
                           mall in a supersonic fighter jet. Only the plummeting cost of interactive computing
                           could turn an absurd luxury into an expensive tool with economic justifications in
                           specialized fields, and eventually into an inexpensive office commonplace. [<a class="ref" href="#haigh2006">Haigh 2006</a>]</p>
                     </blockquote> </div>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">As personal computers expanded into business environments during the late 1980s, word
                     processing's meaning shifted to refer to standalone software for manipulating text.
                     WYSIWYG editors provided real-time rendering of how text would look on paper if the
                     user
                     printed it. Various manufacturers created competing programs throughout the decade,
                     but
                     Microsoft Word emerged as the market leader by the middle of the 1990s, in turn becoming
                     the de facto standard for academic manuscripts.</div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">If TeX remained a novel typographical system, its impact on the academic community
                     would
                     be minimal as many would have abandoned it when WYSIWYG editors became more efficient
                     at
                     displaying mathematical formulas. However, discourses about TeX's utility shifted
                     with
                     the widespread adoption of personal computing. Rather than emphasizing its technical
                     features, advocates posited it as resisting word processing’s dominance as a
                     compositional tool. Their discourses focused on two key trends. First, critics asserted
                     that word processing software's proprietary formats threatened open access of scholarly
                     information. Second, they saw word processors as “distracting” during intellectual
                     labor. Matthew Kirschenbaum notes, “Word processing … shapes and informs literary
                     subjects — the persons who inhabit the system (and economy) of literature,
                     green-screeners or otherwise” [<a class="ref" href="#kirschenbaum2016">Kirschenbaum 2016</a>]. As writers change their tools, these
                     tools shape their composition and orient them into a techno-social framework centered
                     around the process of writing. It is in this social milieu that TeX became a social
                     force for connecting detractors of word processing, rather than just a typographical
                     apparatus.</div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">By the late 1990s and early 2000s, TeX’s users formed a counterpublic united through
                     texts lamenting word processors. As Michael Warner demonstrates, counterpublics shape
                     queer identity and challenged Jürgen Habermas' “bourgeois” public sphere [<a class="ref" href="#warner2002">Warner 2002</a>].
                     Habermas correlates the public sphere's rise in the eighteenth century with the
                     emergence of coffeehouses, cafes, salons, and reading clubs in France, England, and
                     Germany. In these spaces, the notion of citizenship blended with an abstracted universal
                     body [<a class="ref" href="#habermas2015">Habermas 2015</a>]. Nancy Fraser criticizes this universalizing impetus of the public
                     sphere and draws attention to “subaltern counterpublics” that emphasized identity and
                     power [<a class="ref" href="#fraser1990">Fraser 1990</a>].</div>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">Michael Warner draws on Fraser’s conception to make evident how texts' circulations
                     are
                     critical in forming counterpublics. He notes that the “public” refers to a myriad of
                     contradictory definitions. On one hand, <em class="term">the public</em> refers to a
                     totality whose members are defined through a set of universalities. <em class="term">A
                        public</em>, on the other hand, refers to a subset within this broader public. Warner
                     considers a third definition of public. “This kind of public,” notes Warner, “comes into
                     being only in relation to texts and their circulation. It exists <em class="emph">by
                        virtue of being addressed</em>” [<a class="ref" href="#warner2002">Warner 2002</a>, original emphasis]. The audience's
                     attention to the text creates a set of relationships, and in doing so, formulates
                     a
                     distinct “poetic” worldview [<a class="ref" href="#warner2002">Warner 2002</a>]. </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">As corporations embraced word processing during the 1990s, desktop publishing
                     constituted a specific public defined through the circulation of texts, advertisements,
                     images, and videos addressing a neoliberal workforce. According to Jamie Peck and
                     Adam
                     Tickell, neoliberalism has two distinct phases of state mobilization: a “roll-back” and
                     a “roll-out.” In the roll-back period of the 1980s, political elites mobilized the state
                     to create mass deregulation and implemented supply-side economic policies. These
                     policies catapulted Margaret Thatcher and Ronald Reagan into political office but
                     reached their political limit by the 1990s. This led to neoliberalism's “roll-out,”
                     where policymakers mobilized the state to regulate and discipline the same individuals
                     dispossessed by neoliberalization, namely African American and Latino communities
                     [<a class="ref" href="#peck2002">Peck and Tickell 2002</a>].
                     Eschewing collective political action, neoliberalism imparted ideas
                     of self-entrepreneurship, confidence, and prudence onto political subjects.</div>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">Neoliberal market ideology, as Fred Turner shows, intersected with advocacy for
                     deregulation by cybercultural entrepreneurs [<a class="ref" href="#turner2006">Turner 2006</a>, 175–206]. For neoliberal
                     policymakers, computers with WYSIWYG desktop publishing software were essential to
                     launching micro-marketing campaigns, assessing the labor force quantitatively, and
                     accelerating policy report and white paper creation. In this environment, TeX served
                     as
                     a counterpublic against capitalist exploitation of intellectual labor. Although most
                     TeX
                     users were white, their outlook on academic publishing pushed them against the
                     neoliberalizing trend of efficiency and commercial concerns. As historians have noted
                     about the formation of this community:</div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">
                     <blockquote>
                        <p>TeX resulted in a worldwide community of users, developers, and user groups evolved,
                           largely disconnected from the more conventional desktop publishing world driven by
                           commercial concerns of publishers and desktop publishing system vendors (so very
                           different than Knuth’s concerns for TeX). This community remains vibrant today, 40
                           years
                           later, and is an important branch in the development of desktop publishing. [<a class="ref" href="#beeton2018">Beeton, Berry, and Walden 2018</a>]</p>
                     </blockquote>
                  </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">The concentrated community backlash to business practices was in part due to Knuth’s
                     curation of enthusiasts around his software. On February 22, 1980, a small group of
                     enthusiasts bound through Knuth's influence formed the TeX Users Group (TUG). According
                     to Beeton, Berry, and Walden, “The formation of TUG was an important step in TeX's
                     becoming widely popular and in TEX development activity eventually becoming independent
                     of Stanford ” [<a class="ref" href="#beeton2018">Beeton, Berry, and Walden 2018</a>] From 1982, Knuth also began to hold a
                     “MetaFont for Lunch Brunch” to discuss typography — the name alludes to his
                     complementary technology for specifying fonts, which failed to gain traction [<a class="ref" href="#beeton2018">Beeton, Berry, and Walden 2018</a>].
                     The group and Knuth’s academic connections ensured that a
                     series of graduate researchers, faculty, and outside contributors would continue to
                     develop and notably, <em class="emph">write</em> about the system. By the decade's end,
                     TeX User Groups had close to four thousand members and growing international reach.
                     They
                     began publishing a journal entitled <cite class="title italic">TUGBoat</cite> with support from the
                     American Mathematical Society, and the circulation of this new journal helped solidify
                     the counterpublic.</div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">As mentioned earlier, concerns about word processing's growing influence in the academic
                     environment and distracting tendencies dominated texts about TeX. For instance, an
                     early
                     critique of word processors underlies Lamport's discussion of LaTeX. In his original
                     manual, he compares logical design to visual design (another term for “what you see is
                     what you get”). According to Lamport, LaTeX “encourages better writing” by causing
                     writers to focus on their document's logical structure [<a class="ref" href="#lamport1986">Lamport 1986</a>, 8]. It also allows
                     for better focus on text's composition rather than design: “LaTeX was designed to free
                     you from formatting concerns, allowing you to concentrate on writing. If, while writing,
                     you spend a lot of time worrying about form, you are probably misusing LaTeX” [<a class="ref" href="#lamport1986">Lamport 1986</a>, 8].
                     This critique became explicit in Lamport's 1994 revised manual. He notes that
                     when he wrote LaTeX, there were few facilities for authors to typeset their documents
                     whereas they had become commonplace by the mid-1990s. He goes on to explicitly lambast
                     the development of “WYSIWYG programs [that] replace LaTeX's logical design with visual
                     design” [<a class="ref" href="#lamport1986">Lamport 1986</a>, 7].</div>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">Later guides for LaTeX — which became the de facto flavor of TeX — would reassert
                     claims
                     about word processing’s perceived flaws for academic work. In 1989, for instance,
                     Jon
                     Warbrick released a condensed version of Lamport’s manual. In it, he writes, “A visual
                     system makes it easier to create visual effects rather than a coherent structure;
                     logical design encourages you to concentrate on your writing and makes it harder to
                     use
                     formatting as a substitute for good writing” <span class="error"><a class="ref" href="#warbrick1988">#warbrick1988</a></span>. Another widely
                     circulated manual by Gavin Maltby in 1992 entitled <cite class="title italic">An Introduction to
                        TeX and Friends</cite>, notes “Essential to the spirit of TeX is that <em class="emph">it formats the document whilst you just take care of the content</em>, making for
                     increased productivity” [<a class="ref" href="#maltby1992">Maltby 1992</a>, 3, original emphasis]. Another self-described
                     “polemical rant in favor of TeX as opposed to word processors” by Allin Cottrell,
                     Professor of Economics at Wake Forest University, describes Knuth's invention as a
                     panacea to WYSIWYG word processors’ faults. “The word processor is a stupid and grossly
                     inefficient tool for preparing text for communication with others,” contends Cottrell
                     [<a class="ref" href="#cottrell1999">Cottrell 1999</a>].</div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">The conflation of TeX with logical design focused on content creation rather than
                     typography continues into the present. For example, the website for the LaTeX project
                     prominently states the importance of separating content from presentation: “LaTeX is not
                     a word processor! Instead, LaTeX encourages authors not to worry too much about the
                     appearance of their documents but to concentrate on getting the right content ” [<a class="ref" href="#latexproject">LaTeX Project 2020</a>].
                     It contrasts LaTeX with how an author would format a document by determining
                     layout and font. “This has two results,” notes the site, “authors wasting their time
                     with designs; and a lot of badly designed documents!” [<a class="ref" href="#latexproject">LaTeX Project 2020</a>]</div>
                  
                  <div class="counter"><a href="#p37">37</a></div>
                  <div class="ptext" id="p37">In short, by the late 1990s and early 2000s, a counterpublic emerged through blogs,
                     newspaper articles, and online discussion forums positioning TeX as an alternative
                     to
                     word processing for academic production. As Warner makes evident, “texts themselves do
                     not create publics [or counterpublics], but the concatenation of texts through time.
                     Only when a previously existing discourse can be supposed, and a responding discourse
                     can be postulated, can a text address a public” [<a class="ref" href="#warner2002">Warner 2002</a>].</div>
                  
                  <div class="counter"><a href="#p38">38</a></div>
                  <div class="ptext" id="p38">Readers in mathematical fields may wonder how TeX can be a counterpublic given its
                     popularity in their disciplines. It is important to emphasize that counterpublics
                     often
                     gain wide support amongst certain subgroups. A counterpublic maintains at some level,
                     conscious or not, an awareness of its subordinate status argues Warner. “The cultural
                     horizon against which it marks itself off is not just a general or wider public but
                     a
                     dominant one,” he continues [<a class="ref" href="#warner2002">Warner 2002</a>]. For instance, queer communities may have
                     consumer literature challenging heteronormative relationships, but these works remain
                     niche amongst mainstream publishers. Likewise, even though TeX has adoption in
                     mathematical fields, it has little support in the corporate world, governmental sector,
                     or most academic disciplines.</div>
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">The Pitfalls of Markdown</h1>
                  
                  <div class="counter"><a href="#p39">39</a></div>
                  <div class="ptext" id="p39">TeX's origins and subsequent development provide unique insight for scholars of minimal
                     computing. As mentioned earlier, minimal computing advocates often stress Markdown
                     and
                     Markdown-adjacent tools as representative, although not the totality of their practice.
                     In this brief conclusion, I assess four cautious lessons that TeX provides minimal
                     computing practitioners about the adoption of Markdown-based scholarship in the digital
                     humanities.</div>
                  
                  <div class="counter"><a href="#p40">40</a></div>
                  <div class="ptext" id="p40">The first lesson TeX provides is that the adoption of niche tools rarely eases barriers
                     of entry and access. With the rise of WYSIWYG word processors, digital composition
                     opened up to a wider range of individuals. Works that were in TeX pushed against this
                     mainstream adoption. As a result, with growing familiarity of computers throughout
                     the
                     1990s, only those that knew what TeX was, found its system easier to manage, and most
                     importantly, had the specialized training often gained in graduate school to manipulate
                     it were able to contribute. Anne B. McGrail draws attention to a similar issue in
                     minimal computing in reference to her community college students:</div>
                  
                  <div class="counter"><a href="#p41">41</a></div>
                  <div class="ptext" id="p41">My students might read Gil’s and Sayer’s pieces but they will likely write about them
                     using Google or Word on their laptops and email me using Outlook — transgressing minimal
                     computing principles at every keystroke. For these definitions of minimal computing
                     themselves point to a set of assumptions for humanistic computing that are worth
                     considering in CC students’ digital lives [<a class="ref" href="#mcgrail2017">McGrail 2017</a>].</div>
                  
                  <div class="counter"><a href="#p42">42</a></div>
                  <div class="ptext" id="p42">Second, TeX shows that plain-text scholarship, like that written in Markdown, requires
                     a
                     large infrastructure to support it and is far from minimal. In fact, criticisms of
                     large-scale infrastructures as limiting forces in the digital humanities well precede
                     the Minimal Computing Working Group. In a 2012 study of digital humanities projects,
                     Joris van Zundert concludes:</div>
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">
                     <blockquote>
                        <p>At least as far as digital humanities research is concerned, there is little benefit
                           to
                           be expected from the current large infrastructure projects. Their all-purpose nature
                           enforces a generalized strategy aimed at the establishment of standards which is at
                           odds
                           with innovative, explorative research. Being standards-driven, institutionally bound,
                           and at worst enforcing specific implementations, they are platforms of exclusiveness.
                           [<a class="ref" href="#vanzundert2012">Van Zundert 2012</a>]</p>
                     </blockquote>
                  </div>
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">Echoing Gil's later remark that minimal computing advocates should orient themselves
                     around the question of “what we need,” Zundert writes that digital humanities
                     infrastructures should indeed be the simplest thing that could possibly work.
                     Yet, TeX shows that most supporters, including Knuth, were often only
                     able to contribute to the project due to the support of their respective academic
                     institutions. Likewise, Markdown-oriented site generators, such as Jekyll, stress
                     the
                     “free” hosting of GitHub, a company Microsoft bought for over $7.5 billion and backs
                     with its large corporate infrastructure. The impact of Microsoft’s large server network,
                     muddled with bit rot and electrical waste, is rarely taken into account.</div>
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">Third, TeX shows that plain-text scholarship is not significantly more future-proof
                     or
                     sustainable compared to more proprietary alternatives. In regards to their workflow,
                     Tenen and Wythoff argue, “Writing in plain text guarantees that your files will remain
                     readable ten, fifteen, twenty years from now.” They contrast this with proprietary
                     formats, like Word or Google Docs, that “may go the way of WordPerfect in the future”
                     <span class="error"><a class="ref" href="#tenen2014">#tenen2014</a></span>. While the “plain sequence of characters” is likely to be
                     readable, the scripts scholars rely on to convert their documents into more acceptable
                     publishing standards have limited upkeep. As TeX grew in popularity, support for older
                     versions waned leaving many scholars without an easy means of reconverting their
                     documents. To turn back Markdown, it’s important to remember that a key reason for
                     its
                     growing popularity is its ability to transform into a wide variety of formats through
                     pandoc. Although a variety of coders contribute to it, ultimately, it relies on the
                     leadership of MacFarlane, and it is unclear how long it would survive without his
                     input.
                     In contrast, large organizations and governmental agencies that have adopted proprietary
                     software along with the corporations that produce this software have a much greater
                     incentive to maintain compatibility.</div>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">Finally, plain-text scholarship provides little security for vulnerable political
                     activists as claimed by boosters. Alex Gil notes how sites off-grid can assist political
                     revolution. He highlights a project entitled No Connect that he and his colleagues
                     developed at Columbia University's Group for Experimental Methods in the Humanities
                     [<a class="ref" href="#gil2015b">Gil and Tenen 2015</a>].
                     No Connect is a theme for Jekyll meant for users without an internet
                     connection. Presumably, this allows political dissidents to distribute subversive
                     and
                     sensitive materials through Sneakernets, informal networks for transferring information
                     through physical mediums such as CD-ROMs, floppy disks, and USB drives. While such
                     sites
                     may circumnavigate traditional computer networks, they come with their challenges.
                     For
                     instance, governmental agencies posing as bureaucrats can steal or copy them. These
                     same
                     individuals may produce malware and keyloggers that are automatically installed when
                     a
                     user places the USB drive in their computer, compromising more elements of the
                     Sneakernets. A popular exploit places a large surge of electricity through the USB
                     port
                     frying the machine [<a class="ref" href="#angelopoulou2019">Angelopoulou et al. 2019</a>]. Later developments of TeX/LaTeX introduced
                     security risks due to their reliance on macros [<a class="ref" href="#checkoway2010">Checkoway, Shacham, and Rescorla 2010</a>].
                     While Markdown does not rely on as many macros, its convoluted conversion structure
                     leaves it open to similar vulnerabilities. </div>
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">Of course, the minimal computing subfield is divergent and growing, and the emphasis
                     on
                     plain-text scholarship that this article highlights does not represent the field's
                     totality. Many advocates of minimal computing have little focus on markup languages.
                     Nonetheless, by historicizing and contextualizing Markdown through my analysis of
                     TeX, I
                     have sought to show some of the pitfalls that may occur through widespread adoption.
                     In
                     the future, these issues may be solved, but TeX’s lessons should give digital humanities
                     scholars a cautionary warning.</div>
                  </div>
               
               
               
               
               
               </div>
            
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="allen2012"><!-- close -->Allen et al. 2012</span>  Allen, Julie D. et al. “The Unicode Standard.” <cite class="title italic">Unicode Consortium, 2012.</cite>
                  <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.683.9133&amp;rep=rep1&amp;type=pdf" onclick="window.open('http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.683.9133&amp;rep=rep1&amp;type=pdf'); return false" class="ref">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.683.9133&amp;rep=rep1&amp;type=pdf</a>.</div>
               <div class="bibl"><span class="ref" id="alvarado2011"><!-- close -->Alvarado 2011</span>  Rafael Alvarado, Rafael. “The Digital Humanities Situation.”<cite class="title italic"> The Transducer</cite>, May 11, 2011. <a href="http://transducer.ontoligent.com/?p" onclick="window.open('http://transducer.ontoligent.com/?p'); return false" class="ref">http://transducer.ontoligent.com/?p</a>.</div>
               <div class="bibl"><span class="ref" id="angelopoulou2019"><!-- close -->Angelopoulou et al. 2019</span>  Angelopoulou, Olga et al. “Killing Your Device via Your USB Port.”
                  In <cite class="title italic">HAISA</cite>, 61–72, 2019.</div>
               <div class="bibl"><span class="ref" id="bateson2008"><!-- close -->Bateson and Bateson 2008</span>  Bateson, Gregory and Mary Catherine Bateson. <cite class="title italic">Steps to an Ecology of Mind</cite>, 177–93. Chicago; London: University
                  of Chicago Press, 2008.</div>
               <div class="bibl"><span class="ref" id="beeton2018"><!-- close -->Beeton, Berry, and Walden 2018</span>  Beeton, Barbara, Karl Berry, and David Walden. “TEX: A
                  Branch in Desktop Publishing Evolution, Part 1.” <cite class="title italic">IEEE Annals of the
                     History of Computing</cite> 40.3 (2018): 78–93.</div>
               <div class="bibl"><span class="ref" id="checkoway2010"><!-- close -->Checkoway, Shacham, and Rescorla 2010</span>  Checkoway, Stephen, Hovav Shacham, and Eric
                  Rescorla. “Are Text-Only Data Formats Safe? Or, Use This LATEX Class File to Pwn Your
                  Computer.” In <cite class="title italic">3rd USENIX Workshop on Large-Scale Exploits and Emergent
                     Threats</cite>, 8, 2010. (San Jose, CA: USENIX Association).</div>
               <div class="bibl"><span class="ref" id="cottrell1999"><!-- close -->Cottrell 1999</span>  Cottrell, Allin. “Word Processors: Stupid and Inefficient,” June 29,
                  1999. <a href="http://ricardo.ecn.wfu.edu/~cottrell/wp.html" onclick="window.open('http://ricardo.ecn.wfu.edu/~cottrell/wp.html'); return false" class="ref">http://ricardo.ecn.wfu.edu/~cottrell/wp.html</a>.</div>
               <div class="bibl"><span class="ref" id="dilger2010"><!-- close -->Dilger and Rice 2010</span>  Dilger, Bradley J. and Jeff Rice. “Making a Vocabulary for
                  <span class="monospace">&lt;HTML&gt;</span>.” In <cite class="title italic">A to <span class="monospace">&lt;A&gt;</span>: Keywords of Markup, </cite>xi–xxiv.
                  Mineappolis, MN: University of Minnesota Press, 2010.</div>
               <div class="bibl"><span class="ref" id="fraser1990"><!-- close -->Fraser 1990</span>  Fraser, Nancy. “Rethinking the Public Sphere: A Contribution to the
                  Critique of Actually Existing Democracy.” <cite class="title italic">Social Text</cite> 25/26
                  (1990): 56–80.</div>
               <div class="bibl"><span class="ref" id="gil2015a"><!-- close -->Gil 2015</span>  Gil, Alex. “The User, the Learner and the Machines We Make.” <cite class="title italic">Minimal Computing: A Working Group of GO::DH</cite> (blog), May 21, 2015.
                  <a href="https://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/" onclick="window.open('https://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/'); return false" class="ref">https://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/</a>.</div>
               <div class="bibl"><span class="ref" id="gil2015b"><!-- close -->Gil and Tenen 2015</span>  Gil, Alex and Dennis Yi Tenen. “No Connect.” <cite class="title italic">Group for Experimental Methods in the Humanities, </cite>July 24, 2015.
                  https://xpmethod.columbia.edu/knowledge-design-studio/2015-07-24-no-connect.html.</div>
               <div class="bibl"><span class="ref" id="gitelman2002"><!-- close -->Gitelman 2002</span>  Gitelman, Lisa. “Materiality Has Always Been in Play: An Interview with
                  N. Katherine Hayles.” <cite class="title italic">Iowa Journal of Cultural Studies</cite> 2.1
                  (2002): 7–12.</div>
               <div class="bibl"><span class="ref" id="goldfarb1999"><!-- close -->Goldfarb 1999</span>  Goldfarb, Charles F. “The Roots of SGML: A Personal Recollection.” <cite class="title italic">Technical Communication</cite> 46.1 (1999): 75-83.</div>
               <div class="bibl"><span class="ref" id="gruber2004"><!-- close -->Gruber 2004</span>  Gruber, John. “Markdown.” <cite class="title italic">Daring Fireball</cite>, December
                  17, 2004. <a href="https://daringfireball.net/projects/markdown/" onclick="window.open('https://daringfireball.net/projects/markdown/'); return false" class="ref">https://daringfireball.net/projects/markdown/</a>.</div>
               <div class="bibl"><span class="ref" id="gruber2014"><!-- close -->Gruber 2014</span>  Gruber, John. “Twitter /@gruber: @comex I believe Markdown’s success is
                  *due to*, not in spite of, its lack of standardization. And its success is not
                  disputable.” September 3, 2014, 11:09 PM.
                  <a href="https://twitter.com/gruber/status/507364924340060160" onclick="window.open('https://twitter.com/gruber/status/507364924340060160'); return false" class="ref">https://twitter.com/gruber/status/507364924340060160</a>.</div>
               <div class="bibl"><span class="ref" id="habermas2015"><!-- close -->Habermas 2015</span>  Habermas, Jürgen. <cite class="title italic">The Structural Transformation of the
                     Public Sphere</cite> 22–78. Cambridge:Polity, 2015. </div>
               <div class="bibl"><span class="ref" id="haigh2006"><!-- close -->Haigh 2006</span>  Haigh, Thomas. “Remembering the Office of the Future: The Origins of Word
                  Processing and Office Automation.” IEEE Annals of the History of Computing 28. 4
                  (October 2006): 6–31. <a href="https://doi.org/10.1109/MAHC.2006.70" onclick="window.open('https://doi.org/10.1109/MAHC.2006.70'); return false" class="ref">https://doi.org/10.1109/MAHC.2006.70</a>.</div>
               <div class="bibl"><span class="ref" id="hayles1999"><!-- close -->Hayles 1999</span>  Hayles, Katherine. How We Became Posthuman: Virtual Bodies in Cybernetics,
                  Literature, and Informatics. Chicago, IL: University of Chicago Press, 1999.</div>
               <div class="bibl"><span class="ref" id="kay1967"><!-- close -->Kay 1967</span>  Kay, Martin. “Standards for Encoding Data in a Natural Language.” Computers
                  and the Humanities 1.5 (1967): 170–77.</div>
               <div class="bibl"><span class="ref" id="kirschenbaum2003"><!-- close -->Kirschenbaum 2003</span>  Kirschenbaum, Matthew G. “Virtuality and VRML: Software Studies
                  after Manovich.” In <cite class="title italic">The Politics of Information</cite>, 149–53.
                  Stanford, CA: Alt-X Press, 2003. <a href="https://www.marcbousquet.net/pubs/Politics_Information.pdf" onclick="window.open('https://www.marcbousquet.net/pubs/Politics_Information.pdf'); return false" class="ref">https://www.marcbousquet.net/pubs/Politics_Information.pdf</a>.</div>
               <div class="bibl"><span class="ref" id="kirschenbaum2016"><!-- close -->Kirschenbaum 2016</span>  Kirschenbaum, Matthew G. Track Changes: A Literary History of Word
                  Processing Cambridge, MA: Harvard University Press, 2016. <a href="https://doi.org/10.4159/9780674969469" onclick="window.open('https://doi.org/10.4159/9780674969469'); return false" class="ref">https://doi.org/10.4159/9780674969469</a>.</div>
               <div class="bibl"><span class="ref" id="knuth2007"><!-- close -->Knuth 2007</span>  Knuth, Donald. “Oral History.” Interview by Edward Feigenbaum. Computer
                  History Museum, March 14, 2007.
                  https://www.computerhistory.org/collections/catalog/102658053.</div>
               <div class="bibl"><span class="ref" id="latexproject"><!-- close -->LaTeX Project 2020</span>  LaTeX Project. “An Introduction to LaTeX.” LaTeX Project. Accessed
                  August 19, 2020. https://www.latex-project.org/about/.</div>
               <div class="bibl"><span class="ref" id="lamport1986"><!-- close -->Lamport 1986</span>  Lamport, Leslie. <cite class="title italic">LaTeX: A Document Preparation System: User’s Guide and
                     Reference Manual.</cite> Reading, MA: Addison-Wesley, 1986.</div>
               <div class="bibl"><span class="ref" id="macfarlene2020"><!-- close -->MacFarlane 2020</span>  MacFarlane, John. “About Pandoc.” <cite class="title italic">Pandoc</cite>,
                  August 20, 2020. <a href="https://pandoc.org/" onclick="window.open('https://pandoc.org/'); return false" class="ref">https://pandoc.org/</a>.</div>
               <div class="bibl"><span class="ref" id="maltby1992"><!-- close -->Maltby 1992</span>  Maltby, Gavin. <cite class="title italic">An Introduction to Tex and Friends</cite>,
                  1992. <a href="http://faculty.washington.edu/stacy/prb/pdf/intro_to_tex_and_friends.pdf" onclick="window.open('http://faculty.washington.edu/stacy/prb/pdf/intro_to_tex_and_friends.pdf'); return false" class="ref">http://faculty.washington.edu/stacy/prb/pdf/intro_to_tex_and_friends.pdf</a>.</div>
               <div class="bibl"><span class="ref" id="mcgrail2017"><!-- close -->McGrail 2017</span>  McGrail, Anne. “Open Source in Open Access Environments: Choices and
                  Necessities.” Minimal Computing, February 17, 2017.
                  <a href="https://go-dh.github.io/mincomp/thoughts/2017/02/17/mcgrail-choices/" onclick="window.open('https://go-dh.github.io/mincomp/thoughts/2017/02/17/mcgrail-choices/'); return false" class="ref">https://go-dh.github.io/mincomp/thoughts/2017/02/17/mcgrail-choices/</a>.</div>
               <div class="bibl"><span class="ref" id="mittelbach2004"><!-- close -->Mittelbach et al. 2004</span>  Mittelbach, Frank et al. <cite class="title italic">The LaTeX Companion</cite>. Boston, MA:
                  Addison-Wesley Professional, 2004.</div>
               <div class="bibl"><span class="ref" id="oroza2016"><!-- close -->Oroza 2016</span>  Oroza, Ernesto. “Interview with Ernesto Oroza.” Interview by Alex Gil. In
                  Debates in the Digital Humanities 2016, edited by Matthew K. Gold and Lauren F. Klein,
                  184-93. Minneapolis: University of Minnesota Press, 2016.</div>
               <div class="bibl"><span class="ref" id="oroza2020"><!-- close -->Oroza 2020</span>  Oroza, Ernesto. “About.” <cite class="title italic">Architecture of Necessity</cite>,
                  August 20, 2020. <a href="http://architectureofnecessity.com/about/" onclick="window.open('http://architectureofnecessity.com/about/'); return false" class="ref">http://architectureofnecessity.com/about/</a>.</div>
               <div class="bibl"><span class="ref" id="peck2002"><!-- close -->Peck and Tickell 2002</span>  Peck, Jamie and Adam Tickell. “Neoliberalizing Space.” Antipode
                  34.3 (2002): 380–404.</div>
               <div class="bibl"><span class="ref" id="sayers2016"><!-- close -->Sayers 2016</span>  Sayers, Jentery. “Minimal Definitions-Minimal Computing.” <cite class="title italic">Minimal Computing: A Working Group of GO::DH</cite>, October 2, 2016.
                  http://go-dh.github.io/mincomp/thoughts/2016/10/02/minimal-definitions/.</div>
               <div class="bibl"><span class="ref" id="tex2021"><!-- close -->TeX Users Group 2021</span>  TeX Users Group. “History of TeX - TeX Users Group.” TeX Users
                  Group. Accessed October 22, 2021. <a href="https://www.tug.org/whatis.html" onclick="window.open('https://www.tug.org/whatis.html'); return false" class="ref">https://www.tug.org/whatis.html</a>.</div>
               <div class="bibl"><span class="ref" id="teamvivaldi2018"><!-- close -->Team Vivaldi 2018</span>  Team Vivaldi. “What is Markdown and Why Should You Care.” <cite class="title italic">Vivaldi Browser</cite>, November 13, 2018.
                  <a href="https://vivaldi.com/blog/markdown-in-vivaldi-notes/" onclick="window.open('https://vivaldi.com/blog/markdown-in-vivaldi-notes/'); return false" class="ref">https://vivaldi.com/blog/markdown-in-vivaldi-notes/</a>.</div>
               <div class="bibl"><span class="ref" id="tenen2017"><!-- close -->Tenen 2017</span>  Tenen, Dennis. <cite class="title italic">Plain Text: The Poetics of
                     Computation</cite>. Stanford, CA: Stanford University Press, 2017. </div>
               <div class="bibl"><span class="ref" id="turner2006"><!-- close -->Turner 2006</span>  Turner, Fred. From Counterculture to Cyberculture: Stewart Brand, the
                  Whole Earth Network, and the Rise of Digital Utopianism. Chicago, IL: University of
                  Chicago Press, 2006.</div>
               <div class="bibl"><span class="ref" id="vanzundert2012"><!-- close -->Van Zundert 2012</span>  Van Zundert, Joris. “If You Build It, Will We Come? Large Scale
                  Digital Infrastructures as a Dead End for Digital Humanities.” Historical Social
                  Research/Historische Sozialforschung (2012): 165–86. </div>
               <div class="bibl"><span class="ref" id="walden2019"><!-- close -->Walden 2019</span>  Walden, David. “Donald E. Knuth - A.M. Turing Award Laureate.,”
                  Association for Computing Machinery, 2019.
                  https://amturing.acm.org/award_winners/knuth_1013846.cfm. </div>
               <div class="bibl"><span class="ref" id="warner2002"><!-- close -->Warner 2002</span>  Warner, Michael. “Publics and Counterpublics.” Public Culture 14.1 (2002):
                  49–90.</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>