<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="../../common/schema/DHQauthor-TEI.rng"    type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0" ?>
<?xml-model href="../../common/schema/DHQauthor-TEI.isosch" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-model href="../../common/schema/dhqTEI-ready.sch"     type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<TEI xmlns=      "http://www.tei-c.org/ns/1.0"
     xmlns:cc=   "http://web.resource.org/cc/"
     xmlns:dhq=  "http://www.digitalhumanities.org/ns/dhq"
     xmlns:html= "http://www.w3.org/1999/xhtml"
     xmlns:mml=  "http://www.w3.org/1998/Math/MathML"
     xmlns:rdf=  "http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Reverse Engineering the Gendered Design of Amazon’s
               Alexa: Methods in Testing Closed-Source Code in Grey and Black Box Systems</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Lai-Tze <dhq:family>Fan</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000-->https://orcid.org/0000-0001-7271-2160</idno>
               <dhq:affiliation>University of Waterloo</dhq:affiliation>
               <email>laitze.fan@gmail.com</email>
               <dhq:bio>
                  <p>Lai-Tze Fan is an Assistant Professor of Technology and Social Change at the
                     University of Waterloo, Canada. She is the founder and director of the
                     forthcoming Unseen-AI Lab (U&amp;AI Lab) at U Waterloo, which uses critical and
                     creative design methods for enhanced equity, diversity, and inclusion in AI
                     systems. Fan serves as an Editor and the Director of Communications of
                     electronic book review and an Editor of the digital review. She is Co-Editor of
                     the collection Post-Digital: Dialogues and Debates from electronic book review
                     (Bloomsbury 2020) and the special journal issue <title rend="quotes">Canadian
                        Digital Poetics</title> (2021). She is Editor of the special double issue
                        <title rend="quotes">Critical Making, Critical Design</title> (2021), which
                     received the Electronic Literature Organization’s 2022 N. Katherine Hayles
                     Prize for Criticism.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000700</idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->017</idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->2</idno>
            <date when="2023-07-20">20 July 2023</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#code_studies"/>
               <term corresp="#gender"/>
               <term corresp="#access"/>
               <term corresp="#tools"/>
               <term corresp="#users"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000700/000700.xml">GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>​This article examines the gendered design of Amazon Alexa’s voice-driven
               capabilities, or, <q>skills,</q> in order to better understand how Alexa, as an AI
               assistant, mirrors traditionally feminized labour and sociocultural expectations.
               While Alexa’s code is closed source — meaning that the code is not available to be
               viewed, copied, or edited — certain features of the code architecture may be
               identified through methods akin to reverse engineering and black box testing. This
               article will examine what is available of Alexa’s code — the official software
               developer console through the Alexa Skills Kit, code samples and snippets of official
               Amazon-developed skills on Github, and the code of an unofficial, third-party
               user-developed skill on Github — in order to demonstrate that Alexa is designed to be
               female presenting, and that, as a consequence, expectations of gendered labour and
               behaviour have been built into the code and user experiences of various Alexa skills.
               In doing so, this article offers methods in critical code studies toward analyzing
               code to which we do not have access. It also provides a better understanding of the
               inherently gendered design of AI that is designated for care, assistance, and menial
               labour, outlining ways in which these design choices may affect and influence user
               behaviours.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>This article considers the gendered design of Amazon Alexa voice-driven capabilities,
               or, <q>skills,</q> as feminized labour.</p>
         </dhq:teaser>
      </front>
      <body>
         <div>
            <head/>
            <epigraph>
               <cit>
                  <quote rend="block">30 years ago these sayings were cliché, today they are
                     offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a
                     household is not quaint or cute. Prolonging or promoting sexists tropes is
                     wrong. Maybe write a skill called Sexist Spouse. Please do better humans.
                     <lb/>—customer review for the Amazon Alexa skill <title rend="quotes">Happy
                        Wife</title>
                  </quote>
               </cit>
            </epigraph>
            <p>This article examines the gendered design of Amazon Alexa’s voice-driven
               capabilities, or, <q>skills,</q> in order to better understand how Alexa, as an AI
               assistant, mirrors traditionally feminized labour and sociocultural expectations.
               While Alexa’s code is closed source — meaning that the code is not available to be
               viewed, copied, or edited — certain features of the code architecture may be
               identified through methods akin to reverse engineering and black box testing. This
               article will examine what is available of Alexa’s code — the official software
               developer console through the Alexa Skills Kit, code samples and snippets of official
               Amazon-developed skills on Github, and the code of an unofficial, third-party
               user-developed skill on Github — in order to demonstrate that Alexa is designed to be
               female presenting, and that, as a consequence, expectations of gendered labour and
               behaviour have been built into the code and user experiences of various Alexa skills.
               In doing so, this article offers methods in critical code studies toward analyzing
               code to which we do not have access. It also provides a better understanding of the
               inherently gendered design of AI that is designated for care, assistance, and menial
               labour, outlining ways in which these design choices may affect and influence user
               behaviours.</p>
            <p>As commercialized AI devices become more and more sophisticated, we can expand
               current research on the gendered design of AI to asking questions about the intent
               behind such design choices. Of Alexa’s many official and unofficial skills, in this
               article, I am most interested in the skills that mirror gendered forms of workplace,
               domestic, and emotional labour and also the skills that permit Alexa to condone and
               even reinforce misogynistic behaviour. The expectations to maintain order and
               cleanliness in a workplace or home, as well as sociocultural expectations to be
               emotionally giving — including by being maternal, by smiling, by cheering up others,
               and by emotionally supporting others — may be considered stereotypically
                  <q>feminine.</q> By having numerous skills that perform these traits, Alexa serves
               as a surrogate for sources of gendered labour that are dangerously collapsed and
               interchangeable as mother, wife, girlfriend, secretary, personal assistant, and
               domestic servant.</p>
            <p>In my focus on the gendered design of Alexa’s skills and code, I am drawing upon the
               growing body of salient work by scholars in science and technology studies, critical
               data studies, critical race studies, computer science, feminist technoscience, and
               other adjacent fields in which there has been a pointed critique of the systemic
               biases of Big Tech’s data, algorithms, and infrastructures. Notable texts in these
               efforts include Ruha Benjamin’s <hi rend="italic">Race after Technology </hi>
               <ptr target="#benjamin2019"/>, Cathy O’Neil’s <hi rend="italic">Weapons of Math
                  Destruction</hi>
               <ptr target="#oneil2016"/>, Safiya Umoja Noble’s <hi rend="italic">Algorithms of
                  Oppression</hi>
               <ptr target="#noble2018"/>, and Wendy Hui Kyong Chun’s <hi rend="italic">Discriminating Data </hi>
               <ptr target="#chun2021"/>, and also cultural organizations and texts such as Joy
               Boulamwini’s Algorithmic Justice League (established in 2016) and Shalini Kantayya’s
               documentary <hi rend="italic">Coded Bias</hi>
               <ptr target="#codedbias2020"/>. The methods of these works is to analyze Big Tech
               culture from its self-presentation of objective data, information, and logic —
               pillars that begin to crumble when we examine the exclusionary, discriminatory, and
               systemically unequal foundations upon which they are built.</p>
            <p>And these biases are not singular. Indeed, any analysis and discussion of Alexa’s
               gendered design may be extended to the ways in which many AI assistants are modelled
               after forms of labour that exploit groups of people on the basis of race, class, and
               nationality. The intersectional systemic biases of technological design invite
               further research on this topic in critical code and critical data studies in
               particular.</p>
            <p>In the code analysis of this article, I will:</p>
            <list type="ordered">
               <item>Analyze the Alexa Skills Kit software development kit for interface features
                  that are automated and parameterized.</item>
               <item>Analyze select code samples and snippets on Alexa’s Github account for code
                  features that are parameterized.</item>
               <item>Analyze official Alexa skills and available code samples and snippets to
                  demonstrate Alexa’s problematic responses to users’ flirting and verbal
                  abuse.</item>
               <item>Analyze cultural and code variations of the <said>make me a sandwich</said>
                  command to discuss how users try to trick Alexa into accepting overtly
                  misogynistic behaviour.</item>
            </list>
            <p>I add three notes for added context. First, I distinguish between official and
               unofficial skills in this way: <q>official</q> skills are developed by Amazon’s own
               Alexa division and <q>unofficial</q> skills are developed by a third-party person,
               persons, or group not affiliated with this division. All but one skill that I discuss
               are available on the Amazon store. Second, occasionally, source texts may describe
               Amazon devices on which Alexa operates, such as the Amazon Echo and Echo Dot, and
               devices that have a smart display, rather than the Alexa technology itself. Third,
               all Alexa code snippets and samples discussed in this article, as well as
               documentation of Alexa’s responses, are from mid 2022 and are subject to future
               change. The hope is that they <hi rend="italic">would</hi> continue to change to
               improve aspects of their current biased design, whether or not that is a realistic
                  objective.<note>For example, in 2019, Apple updated select responses of its AI
                  assistant Siri: whereas prior to April 2019, Siri would respond to the user
                  utterance <said>Siri, you’re a bitch</said> with the equally problematic <quote rend="inline">I’d blush if I could,</quote> after a software update in 2019,
                  Siri now responds to the same user utterance with <quote rend="inline">I don’t
                     know how to respond to that.</quote></note>
            </p>
         </div>
         <div>
            <head>Designing AI Assistants to be Female-Presenting</head>
            <p>AI assistants perform a user’s commands through text- or voice-controlled
               interaction, through which the software searches for keywords that match its
               predesignated scripts to execute specific actions and tasks. Popular task-based
               commands include checking the weather, setting timers, setting reminders, playing
               music, and adding items to a user’s calendar. In this sense, AI assistants replace
               smaller actions that a user might otherwise do, and they are also meant to emulate
               some forms of <q>menial</q> labour that are performed by traditionally feminized
               roles, such as personal assistants, secretaries, and domestic servants <ptr target="#biersdorfer2020"/><ptr target="#lingel2020"/><ptr target="#kulknari2017"/><ptr target="#sobel2018"/>.</p>
            <figure xml:id="figure01">
               <head>A chart depicting different uses of voice-based AI assistants in UNESCO’s
                     <title rend="quotes"><said>I’d blush if I could</said></title> report from 2019
                     <ptr target="#bergen2016" loc="91"/>.</head>
               <figDesc>A stacked bar chart of 17 voice assistant uses, from the most common, <q>ask
                     a question,</q> to least common such as <q>make a purchase,</q> measured by
                  daily, monthly, or infrequent use.</figDesc>
               <graphic url="resources/images/figure01.png"/>
            </figure>
            <p>Women’s workforce labour through mechanical technologies existed and has been
               identified even earlier. From the loom, to Industrial-era typewriters and telegraphs,
               to telephone operating switchboards to the first programmable computers — all were
               largely operated by women. These examples do not represent a separate woman’s history
               of technology; rather, they reveal women <hi rend="italic">in</hi> the history of
               technology. These women were never separate, only unseen.<note>For one detailed
                  discussion of the role of women in the history of software, see Wendy Hui Kyong
                  Chun’s <title rend="quotes">On Software, or, the Persistence of Visual
                     Knowledge</title>
                  <ptr target="#chun2004"/>, which is discussed later in this article.</note> It is
               therefore no coincidence that Siri, Alexa, Cortana, Google Assistant, and many other
               commercialized AI assistants are programmed from the factory with female-presenting
               voices, with few exceptions. Although AI assistants like Siri and Alexa and Google
               Assistant say they do not have a gender (when asked, Alexa literally says, <quote rend="inline">As an AI, I don’t have a gender</quote>), and their companies choose
               to forego pronouns entirely, AI that is used for service technologies assistants are
               unmistakably designed to resemble women; as a consequence, they are often viewed and
               treated as female by users.<note>The Amazon Alexa’s YouTube tutorials that I reviewed
                  refer to Alexa by name only and never by any pronouns <ptr target="#amazon-dev-services2021a"/>.</note>
            </p>
            <p>The question may arise of whether AI assistants may also be gendered to be
               male-presenting and to perform masculine stereotypes in emotion, exchange, and
               labour. There are instances of this, particularly in the example of the UK Siri, who
               comes from the factory programmed with a male British-accented voice.<note>The desire
                  to associate the UK Siri with a Jeeves, Alfred, or other English butler figure is
                  a deliberate design choice that reflects a long cultural history of class-based
                  exploitation of labour — but a class-biased analysis of AI assistants is not
                  within the scope of this specific article.</note> Predominantly, however, studies
               have shown that the female-presenting voice setting is the most popular for AI
               assistant users worldwide. Notably, this preference is also not specific to men: for
               example, UNESCO reports in 2019 that <quote rend="inline">the literature reviewed by
                  the EQUALS Skills Coalition included many testimonials about women changing a
                  default female voice to a male voice when this option is available, but the
                  Coalition did not find a single mention of a man changing a default female voice
                  to a male voice</quote>
               <ptr target="#unesco2019" loc="97"/>.</p>
            <p>Despite developments in creating genderless voices for AI assistants, they have not
               been taken up by Big Tech companies. Perhaps the most well-known genderless voice
               assistant is Q, a synthetically harmonized voice that is made up of a blend of <quote rend="inline">people who neither identify as male nor female [which was] then
                  altered to sound gender neutral, putting [their] voice between 145 and 175 hertz,
                  a range defined by audio researchers</quote>
               <ptr target="#meetq2019"/><note><ref target="http://genderlessvoice.com">http://genderlessvoice.com</ref></note>. The designers of Q have sought but
               thus far not had success in having Big Tech corporations, such as Apple, Amazon,
               Google, and Microsoft, adopt Q as their default voice <ptr target="#meetq2019"/>.</p>
            <p>One of the most common explanations for why AI assistants do not present as male or
               gender neutral is the historical appeal of and preference for women’s voices in
               certain forms of labour in patriarchal cultures worldwide. In particular, a large
               number of <q>interfacing</q> jobs, including in forms of customer service and
                  <q>menial</q> task completion, are given to women with the justification that
                  <quote rend="inline">research shows that women’s voices tend to be better received
                  by consumers, and that from an early age we prefer listening to female
                  voices</quote>
               <ptr target="#tambini2020"/>. Amazon’s Smarthome and Alexa Mobile divisions VP Daniel
               Rausch is quoted as saying that his team <quote rend="inline">found that a woman’s
                  voice is more sympathetic</quote> (qtd. in <ptr target="#tambini2020"/>). In the
               original interview with Rausch, a Stanford University study is highlighted to address
               a human preference for gender assignment, even of machines. This study <quote rend="inline">also underlines that we impose stereotypes onto machines depending
                  on the gender of the voice — in other words, we perceive computers as
                     <q>helpful</q> and <q>caring</q> when they’re programmed with the voice of a
                  woman</quote>
               <ptr target="#schwar2020"/>.</p>
            <p>Despite research showing that women are preferred in customer service and for
               customer interfacing, it is the social and cultural practices of using
               female-presenting voices that remain problematic and harmful. As it is, several
               publications observe that designing speech-based AI as female can create user
               expectations that they will be <q>helpful,</q>
               <q>supportive,</q>
               <q>trustworthy,</q> and above all, subservient <ptr target="#bergen2016"/><ptr target="#giacobbe2019"/><ptr target="#steele2018"/><ptr target="#unesco2019"/>.
               However, the ways in which a user speaks to AI assistants is not important for
               function, as <quote rend="inline">the assistant holds no power of agency beyond what
                  the commander asks of it. It honours commands and responds to queries regardless
                  of their tone or hostility</quote>
               <ptr target="#unesco2019" loc="104"/>.</p>
            <p>Is it that AI assistants are sexist or that they are platforms framed by a sexist
               context, offering affordances to those who would use it to sexist ends? Focusing
               specifically on the Amazon Alexa voice interaction model, I note that a major factor
               of the gendered treatment and categorization of Alexa as female and as performing
               traditionally gendered labour and gendered tasks is the design and presentation of
               her as female in the first place. Alexa is predominantly described with female
               pronouns by users and professionals, and many users have demonstrated that they
               consciously or unconsciously think of Alexa as female (but not necessarily equivalent
               to a woman). In the 80 articles that I read about user experiences, analyses, and
               overviews of Alexa, 72 refer to the AI assistant as a <quote rend="inline">she</quote> and make statements that suggest they understand her as a female
               subject; for example, a 2020 <hi rend="italic">TechHive </hi>article instructs users:
                  <quote rend="inline">There are a couple of ways you can go with Amazon’s helpful
                  (if at times obtuse) voice assistant. You can treat her like a servant, barking
                  orders and snapping at her when she gets things wrong (admittedly, it can be
                  cathartic to cuss out Alexa once in a while), or you can think of her as a
                  companion or even a friend</quote>
               <ptr target="#patterson2020"/>. Repeatedly, it has been shown that designing AI
               assistants to complete similar tasks as hierarchical labour models (such as maids and
               personal assistants) has also resulted in widespread reports of negative
               socialization training — including users who describe romantic, dependent, and/or
               verbally abusive relationships with AI assistants <ptr target="#kember2016"/><ptr target="#strengers2020"/><ptr target="#andrews2020"/><ptr target="#gupta2020"/>.</p>
            <p>As AI become increasingly commodified, we also need to question the biased design of
               specific AI as female-presenting. It is primarily AI that are designated for care,
               assistance, and repetitive labour that are considered menial and therefore below
               managerial authority and professional, economically productive expertise.<note>For
                  more on the structures of this labour divide, see Wendy Hui Kyong Chun’s <title rend="quotes">On Software, or, the Persistence of Visual Knowledge</title>
                  <ptr target="#chun2004"/> and Matthew G. Kirschenbaum’s chapter <title rend="quotes">Unseen Hands</title> in <title rend="italic">Track Changes: A
                     Literary History of Word Processing</title>
                  <ptr target="#kirschenbaum2008"/>.</note> This delegation of <q>menial</q> labour
               to female-presenting AI aligns with the long history of women’s labour being
               undermined and made invisible in many parts of the world. In recent decades, this
               exploitation and invisibility has proliferated for women of colour.<note>See Lisa
                  Nakamura’s article <title rend="quotes">Indigenous Circuits: Navajo Women and the
                     Racialization of Early Electronic Manufacture</title>
                  <ptr target="#nakamura2014"/> in which she explores the gendered and racialized
                  implications of invisible labour by Navajo women who produced integrated circuits
                  in semiconductor assembly plants starting in the 1960s. Nakamura writes that
                     <quote rend="inline">technoscience is, indeed, an integrated circuit, one that
                     both separates and connects laborers and users, and while both genders benefit
                     from cheap computers, it is the flexible labor of women of color, either
                     outsourced or insourced, that made and continue to make this possible</quote>
                  <ptr target="#nakamura2014" loc="919"/>.</note> Questioning these design choices
               and their history can contribute to existing conversations in technological design
               and latent (or manifest) bias, including through Safiya Umoja Noble’s work on the
               sexualized and racially discriminatory Google search results for the terms <q>Black
                  girls,</q>
               <q>Latin girls,</q> and <q>Asian girls</q> in 2008.<note>The vital distinction
                  between <emph>latent </emph>and <emph>manifest</emph> encoding in both culture and
                  computation is described in Wendy Hui Kyong Chun’s <title rend="quotes">Queering
                     Homophily</title>
                  <ptr target="#chun2018"/> and again in her 2021 monograph <hi rend="italic">Discriminating Data: Correlation, Neighborhoods, and the New Politics of
                     Recognition</hi>.<ptr target="#chun2021"/></note> While the search algorithm
               has since been edited for improvement on harmful misrepresentations, Noble underlines
               the dangers when stereotypes in search results are treated as factual by unknowing
               users. These generalizations start with algorithmic design, as algorithms are trained
               through supposedly fact-based data (which often includes a scraping of historical,
               even obsolete, data), through which stereotypes are first projected as reality. For
               these reasons, in her latest book <hi rend="italic">Discriminating Data</hi>
               <ptr target="#chun2021"/>, Wendy Hui Kyong Chun describes AI as not only having the
               power to self-perpetuate and justify inequitable systems of the past, but also, as
               being able to predict, prescribe, and shape the future. As I have also stated
               elsewhere, poorly designed data and algorithms hold the power to reinforce,
               self-perpetuate, and justify systems of the past in terms of who is in and who is out
                  <ptr target="#fan2021"/>. Failure to address these issues will result in
               reinforcing systemic oppressions that many believe are in the past, but that continue
               to be dangerously perpetuated through ubiquitous AI.</p>
         </div>
         <div>
            <head>Too Easy to Certify?: Gendered Design and Misogyny in Closed-Source Alexa
               Skills</head>
            <p>As of mid 2022, Alexa possesses over 100,000 skills that can help with or take over
               household chores, including 45 official Alexa Smarthome capabilities that have been
               included on the official Alexa Github account: these skills can help with cooking,
               cleaning, and adjusting volumes, lights, and temperature <ptr target="#alexa-github2021"/>. Alexa is also a mediator for home appliances: robot
               vacuum cleaners, coffee makers, smart thermostats, and dishwashers can be activated
               and controlled by interfacing with Alexa. The convenience of Alexa as an AI assistant
               in one’s home extends her labour to domestic servitude, mirroring more antiquated
               expectations of an unequal distribution of domestic labour: a user does not have to
               lift a finger around the home if they can just tell Alexa to do it.<note>Here,
                  Alexa’s labour extends to exploitation based on race and class as well, as the
                  labour of domestic servants has historically been designated to members of the
                     <q>working class</q> as well as to women of colour. </note>
            </p>
            <p>There are an abundance of unofficial skills that are closed source and that exhibit
               gendered expectations of emotional labour to unconditionally support a user,
               including at least ten skills called <title rend="quotes">Make Me Smile</title> or
                  <title rend="quotes">Make Me Happy,</title> and at least seven skills that are
               intended to compliment or flatter a user. More explicitly problematic are skills with
               the word <q>wife</q> in the invocation name. For example <title rend="quotes">My
                  Wife</title> is promoted as a tool for husbands <quote rend="inline">to get the
                  answers you always wanted from your wife</quote>
               <ptr target="#bsg-games"/>.</p>
            <figure xml:id="figure02">
               <head>The information page of the <title rend="quotes">My Wife</title> Alexa
                  skill.</head>
               <figDesc>Screenshot of mobile app skill page listing <q>Start By Saying</q> examples
                  such as <quote rend="inline">Alexa, ask my wife if I can buy a new
                  truck.</quote></figDesc>
               <graphic url="resources/images/figure02.png"/>
            </figure>
            <p>Upon trying the skill, I repeated the provided sample utterances in Figure 2, which
               are masculine stereotypes. Alexa responds, respectively, <quote rend="inline">If it
                  will make you happy, then OK</quote> and <quote rend="inline">no… [sic] just
                  kidding. Of course you can</quote>
               <ptr target="#bsg-games"/>. I try my own hyperbolic utterances: <quote rend="inline">Alexa, ask my wife if I can spend all of our life savings</quote> and <quote rend="inline">Alexa, ask my wife if I can cheat on her.</quote> I am told, <quote rend="inline">I want to be very clear… [sic] absolutely you can go for it</quote>
               and <quote rend="inline">I want you to have everything you want, so yes you
                  can</quote>
               <ptr target="#bsg-games"/>. As this skill presents Alexa, an AI assistant and a
               technology, as a stand-in for and authority over a human woman’s feelings, opinions,
               and power, it meanwhile reinforces the idea that female-presenting subjects should
               only say <said>yes</said> — a representation that extends beyond unconditional
               support to misogynistic expectations of women’s workplace, social, and sexual
               consent.</p>
            <p>The skill <title rend="quotes">Happy Wife</title> unabashedly stereotypes women as
               wives, mothers, and housekeepers only. It offers advice on different ways that
               husbands can make their wives happy, including to <quote rend="inline">Let her
                  decorate the house as she likes it,</quote>
               <quote rend="inline">Take care of the kids so that she has some free time,</quote>
               and <quote rend="inline">Give her money to play with</quote>
               <ptr target="#akoenn2021"/>. Despite these superficial representations of women, the
                  <title rend="quotes">Happy Wife</title> skill has 11,783 reviews (and therefore
               many more downloads) and a rating of 3.4/5 stars (3.8/5 at the time when this article
               was first written). Yet, the recent reviews from 2020 to 2022 portray various users’
               critique of the misogynistic nature of this skill. Review statements include <quote rend="inline">1950’s [sic] called [sic] they want their sexism back</quote> and
                  <quote rend="inline">30 years ago these sayings were cliché, today they are
                  offenisve [sic]. Demeaning, limiting, or belittling a woman’s contribution to a
                  household is not quaint or cute. Prolonging of promoting sexist tropes is wrong.
                  Maybe write a skill called Sexist Spouse. Please do better humans.</quote></p>
            <p>While these skills are unofficial — which is to say that they were created by
               third-party developers — all of them passed the Amazon Alexa certification tests and
               were approved to be released on the Alexa Skills store on international Amazon
               websites. In the policy requirements for certification, skills that are subject to
               rejection include ones that <quote rend="inline">contai[n] derogatory comments or
                  hate speech specifically targeting any group or individuals</quote>
               <ptr target="#amazon-dev-services2021a"/>. Also, in the voice interface and UX
               requirements, developers are expected to <quote rend="inline">increase the different
                  ways end users can phrase requests to your skill</quote> and to <quote rend="inline">ensure that Alexa responds to users’ requests in an appropriate way,
                  by either fulfilling them or explaining why <emph>she</emph> can’t</quote>
               <ptr target="#amazon-dev-services2021a" loc="my emphasis"/>. Despite certification
               requirements, Amazon Developer Services do not count stereotypical representations of
               women and of traditionally gendered labour as hateful or malicious. Notably, in this
               official certification checklist, the certification team slips with an inconsistent
               presentation of Alexa’s gender, using the <q>she</q> pronoun and thus indirectly
               acknowledging that they understand her to be female.</p>
         </div>
         <div>
            <head>Obstacles to Analyzing Alexa: The Problem of Closed-Source Code and Black Box
               Design in Critical Code Studies</head>
            <p>One way to explore biases in technological design, including for the ways in which
               they may be gendered, is to analyze artifacts to understand how ideologies and
               cultural practices are ingrained and reinforced by design choices at the stages of
               production and development. Such an approach has been used by science and technology
               scholars such as Anne Balsamo <ptr target="#balsamo2011"/> and Daniela K. Rosner <ptr target="#rosner2018"/> to show that gendered technologies may be <quote rend="inline">hermeneutically reverse engineered</quote> — a research method that
               combines humanities methods of interpretation, analysis, reflection, and critique,
               with reverse engineering methods in STEM disciplines such as the assessment of
               prototypes and production stages <ptr target="#balsamo2011" loc="17"/>. The focus for
               both Balsamo and Rosner is to explore technological artifacts and hardware for their
                  <hi rend="italic">materiality</hi> — the physical, tangible, and embodied elements
               of technology that have historically been linked to women’s labour. For example,
               Rosner uncovers stories about women at NASA in the 1960s (called the <q>little old
                  ladies</q>) who hand-wove wires into space shuttles as an early form of
               information storage <ptr target="#rosner2018" loc="3, 115—8"/>. In her article on
               alternate and gendered histories of software, Wendy Hui Kyong Chun <ptr target="#chun2004"/> explores the use of early coding by setting switches rather
               than plugging in cables using the electronic ENIAC computer of the 1940s. Chun
               equates these changes to the operations of software later in the century, but with
               the significant observation that they were also decidedly material in operational
               design <ptr target="#chun2004" loc="28"/>, as physical wires that had to be
               manipulated constitute a tangible or <hi rend="italic">physical </hi>computation. As
               the labour of programming through the wires was considered menial work, it was
               delegated to female operators <ptr target="#chun2004" loc="29"/>.</p>
            <p>Matthew G. Kirschenbaum <ptr target="#kirschenbaum2008"/> differentiates among
               methods of analyzing hardware and software through his own terms <quote rend="inline">forensic materiality</quote> and <quote rend="inline">formal materiality,</quote>
               terms which respectively describe the physical singularities of material artefacts
               and the <quote rend="inline">multiple relational computational states on a data set
                  or digital object</quote>
               <ptr target="#kirschenbaum2008" loc="9—15"/>. He notes, importantly, that the terms
               do not necessarily apply to hardware and software exclusively. Kirschenbaum’s
               investigation into computational materiality help to establish the ways in which the
               hands-on methodologies of archaeology, archival research, textual criticism, and
               bibliography lend themselves to critical media studies, by remembering that all
               technology has a physical body of origin. This fact, he shows, is often neglected
               through a bias for displayed content, otherwise described as <quote rend="inline">screen essentialism</quote> by Nick Montfort <ptr target="#montfort2004"/>.<note>For more on media materiality and illusion of media immateriality, see
                  Lori Emerson’s <title rend="italic">Reading Writing Interfaces: From the Digital
                     to the Bookbound</title>
                  <ptr target="#emerson2014"/>.</note></p>
            <p>What complicates these methods of reverse engineering is the consideration of the
               system as a whole: computer hardware operates as a system with software and data, and
               vice versa. While Balsamo and Rosner use hermeneutic reverse engineering to analyze
               the gendered design of technological hardware, dealing with software and with data
               presents a different scenario for which different methodologies must also be
               developed. Within methods of reverse engineering, one major issue that critical code
               studies helps to frame and potentially tackle is the increasingly obfuscated and
               surreptitious politics and economics of code, including access or lack thereof to
               code.</p>
            <p>Ideally, my preferred method of reverse engineering bodies of code is to analyze
               their contents and attributes — including in its organizational and directional
               structure, datasets, mark-up schema, and syntax — for design choices that are biased,
               including by identifying the ways in which they mirror systemic inequalities.
               However, a major research roadblock when it comes to Big Tech products and software,
               and when it comes to Alexa, is that Alexa’s larger data and code infrastructure
               remains closed-source by Amazon, which means that it is not publicly available for
               viewing, copying, editing, or deleting. In contrast, open-source code is freely
               available, including through popular software-sharing platforms and communities such
               as Github.</p>
            <p>There are existing tools that are trying to tackle the problem of closed-source code,
               especially in critical code and data studies and in the digital humanities. For
               example, the <hi rend="italic">What-If Tool</hi> offers a visualization of machine
               learning models so that one can analyze their behaviour even when their code is not
               available.</p>
            <p>Methods and tools in examining closed-source code help to address a shift that comes
               with the commercialization and ubiquity of AI: the inevitable increase in future Big
               Tech design that is described as <q>black box,</q> which means that designers (in
               science and engineering as well) intentionally prevent and obfuscate a user from
               learning about the system’s inner workings. When it comes to hardware, a hobbyist,
               artist, or researcher can always choose to forego the warranty, procure the specific
               utilities, and investigate the inner workings of a machine’s insides. However, the
               content of closed-source code, which is a form of intellectual property, cannot be
               attained except through specific avenues: legally, one would have to get permission
               from a company for access, usually through professional relationships. Illegally, one
               would have to steal the code, which for my research purposes is not only impractical
               but also unethical. Further future reliance by Big Tech on closed-source code will
               mean that scholars of critical code studies, whose work may depend on access and
               analysis of original code, will increasingly have to find ways to study digital
               objects, tools, and platforms without open-source code.</p>
         </div>
         <div>
            <head>Methodology: Adapting Hermeneutic Reverse Engineering to Examine Code</head>
            <p>While Alexa’s software is closed source, in some ways, it can be considered a <q>grey
                  box</q> system, which means that some aspects of the system are known or can be
               made more transparent. A <q>white box</q> system is one that is completely or near
               completely transparent. The difference between closed-source code and black box
               system is that just because code is closed source does not mean that one has no
               knowledge of its attributes, content, or structure, especially if one already has
               knowledge of other similar kinds of software and the programming languages that may
               be used. In the case of Alexa, Amazon houses repositories of basic code samples and
               snippets on its Github account, which aids potential developers and also builds
               developer communities to grow Alexa resources. </p>
            <p>My methodology seeks to better understand Alexa’s code architecture, despite the fact
               that some of it is completely unattainable. I will present my analysis in first
               person with the intention of demonstrating my methodology at work, or as theory in
               practice. Here, I draw upon the same reverse engineering methods as black box
               testing, which allows for calculated input into a system and analysis of the output.
               For example, if I say to Alexa, <said>You’re pretty</said> (input), about 60 – 70% of
               the time, Alexa responds <quote rend="inline">That’s really nice, thanks,</quote>
               with a cartoon of a dog (output), and the other times, I get the same response on a
               plain blue background that is the default response aesthetic. However, if I say to
               Alexa, <said>You’re handsome</said> (input), every single time, <quote rend="inline">Thank you!</quote> (output) appears on the plain blue background.</p>
            <figure xml:id="figure03">
               <head>Alexa’s responses to <said>You’re pretty</said> (left) and <said>You’re
                     handsome</said> (right).</head>
               <figDesc>Two screenshots of Alexa mobile application, with and without cartoon
                  dog.</figDesc>
               <graphic url="resources/images/figure03.png"/>
            </figure>
            <p>I can triangulate the input and output to analyze the system in between: input ⟶ <hi rend="italic">x</hi> ⟶ output. This triangulation allows me to deduce that the
               system designers chose a more detailed and positive response for compliments to
               Alexa’s appearance that are common for female-presenting subjects (input:
                  <said>pretty</said> versus input: <said>handsome</said>). In particular with
               digital objects and software, analyzing the output and behaviour can tell us more
               about its design.</p>
            <div>
               <head>1. Parameterized Programming Interfaces: The Alexa Skills Kit Console</head>
               <p>In lieu of access to Alexa’s larger code architecture, I explored the pieces of
                  code that are publicly available through the Alexa Skills Kit (<title rend="quotes">ASK</title>). The ASK is an Amazon software development kit that
                  allows device users to become software developers, using the corporation's own
                  application programming interface (API) to create new Alexa skills that can be
                  added to the public Amazon store, and which can then be downloaded onto individual
                  Alexa devices through an Amazon account. By making an Amazon developer account,
                  official and unofficial developers can build their own skills through one of two
                  options.</p>
               <p>In the first option, a developer may choose to write and provision the backend
                  code from scratch. Often, backend code is unavailable — and often not of interest
                  — to many everyday technology consumers, but it is also useful content and a
                  useful resource for those who want to learn about the structure, organization, and
                  commands that control specific computational operations and phenomena. Choosing to
                  provision one’s own backend may be more common for software programmers and
                  companies who want to adapt Alexa’s voice interaction model for devices that are
                  not released by Amazon.</p>
               <p>The second option is still difficult for most amateur tech users, but is certainly
                  more accessible and arguably more popular to those who have some foundational
                  experience in coding. A developer can use the automated provision of the backend
                  by choosing Alexa-hosted skills, through which one can go through the ASK
                  developer console and its interactive WYSIWYG interface.</p>
               <figure xml:id="figure04">
                  <head>Options to add slot types in the ASK console interface.</head>
                  <figDesc>A webform screenshot to <quote rend="inline">define how phrases in
                        utterances are recognized</quote> by adding <q>slot types</q> such as
                     AMAZON.Actor, AMAZON.Airline, et cetera.</figDesc>
                  <graphic url="resources/images/figure04.png"/>
               </figure>
               <p>I chose this second option in order to examine how the automated console interface
                  can be didactic and persuasive — and even parameterized and restrictive — for a
                  developer’s design choices when it comes to choosing a skill’s parts. These parts
                  can be broken down and explored separately: invocation names (name of skill),
                  intents (types of actions), and utterances (corpus of keywords, slot types and
                  values for utterances, and replies accepted in Alexa’s interchange with a user,
                  including sample utterances that the developer can offer a user to demonstrate the
                  objective of a skill and its interactions). Utterances contain the values that
                  Alexa is programmed to search for in a skill’s backend (<said>Alexa, {do
                     this}</said>), so having limited options prevents a user from being able to get
                  creative with what they can say to Alexa that would be acceptable input.</p>
               <p>With the convenience of drag-and-drop options, prompt buttons, drop-down menus,
                  and an existing built-in library, the likelihood that a developer who is using the
                  Alexa-hosted skills would be influenced by the didactic modes, turns, and framing
                  of an automated console is much higher in comparison to a developer who builds
                  Alexa skills from scratch.</p>
            </div>
            <div>
               <head>2. <q>Fieldwork</q> in Alexa’s Github: Identifying Alexa’s Procedural
                  Rhetoric</head>
               <p>In order to learn how to use the developer platform, I watched Alexa Developers’
                  YouTube tutorials, focusing on a ten-video series for Alexa-hosted skills called
                     <title rend="quotes">Zero to Hero: A comprehensive course to building an Alexa
                     Skill.</title> I made an account with Amazon Developer Services under a
                  pseudonym and followed along on the programming interface, meanwhile making notes
                  about the ease of following along with tutorials as well as about the contexts of
                  the programming interface.</p>
               <p>Each of the YouTube tutorials focus on completing a fundamental task, not only
                  giving me an overview of the platform, but also familiarizing me with important
                  terms that I needed to recognize in the code, and in descriptions of and
                  instructions for a skill (e.g., what does the <q>handler</q> function do? What’s
                  the difference between <q>session attributes</q> and <q>permanent attributes</q>
                  for a skill?). My goal was not to learn how to build an Alexa Skill, but rather,
                  to understand the ASK and the Alexa-hosted skills interface’s programming logic
                  through identifying repeated patterns and instructions, and to understand general
                  affordances, limitations, and architecture. I looked up any terms I didn’t know
                  and created a glossary for future reference.</p>
               <p>Once I finished the tutorial series, I went to <ref target="http://github.com/alexa">Alexa’s Github account</ref>. After skimming
                  the list of all 131 repositories and closely analyzing the 15 most popular, I
                  observed that community members can push (but not necessarily ensure the
                  commitment of) changes to any of them, which means that there are a multitude of
                  coders. In other words, Amazon cannot take credit — nor are they directly
                  responsible — for some of the code despite it being on their own Github account.
                  There are six pinned repositories on the account’s homepage chosen by the
                  administrators to take center stage, including the software development kit for
                     node.js.<note>There are twelve official members on the Alexa Github account who
                     are listed as having organization permissions, though it cannot be said that
                     they are the original administrators who chose the pinned repositories nor that
                     they curate and maintain all of the account.</note> While Alexa Skills can be
                  programmed in the languages Javascript (through node.js), Java, and Python,
                  perhaps the repository <title rend="quotes">ASK SDK for node.js</title> is pinned
                  because it is the most comprehensive, containing thirteen foundational code
                  samples including <title rend="quotes">Hello World,</title>
                  <title rend="quotes">Fact,</title>
                  <title rend="quotes">How To,</title> and <title rend="quotes">Decision
                     Tree.</title></p>
               <p>User utterances are limited to the values that are predefined by the developer
                  (whether official or unofficial), including when choosing sets of values that must
                  adhere to the restrictions created by developers. For example, if a developer
                  wishes to create a prompt based on calendar dates, the developer will create slot
                  types for <q>year,</q>
                  <q>month,</q> and <q>date,</q> and will create intent slots that correspond to
                  those types — for <q>month,</q> the slots would include January to December, for
                  instance. In fact, the automated ASK interface offers existing slot types from
                  Alexa’s built-in library such as <q>AMAZON.Ordinal</q> and
                     <q>AMAZON.FOUR_DIGIT_NUMBER</q> that make it easier for the developer to define
                  suitable user utterances, but these predefined slot types can also be explored in
                  terms of their restrictions. For example, if a user tries to ask for an event in
                  the month of <q>banana,</q> this utterance would be rejected. While such a
                  parameterization makes logical sense, further examples that are more subjective
                  may require more nuanced and diverse utterance options, far more diverse than
                  those found in automated slot types. For example, in the Decision Tree skill,
                  which asks users for personal information in order to make a recommendation about
                  what kind of career they might enjoy, the value <quote rend="inline">people</quote> has only four synonyms: men, women, kids, and humans <ptr target="#kelly2020b"/>. The limited and binarized structure and definition of
                     <q>people</q> prevents the possibility of any alternatives; Alexa does not
                  accept alternatives to the keyword that has been pre-defined. Any attempts at
                  creative misuse are also rejected.<note>Understanding that <quote rend="inline">the most fundamental aspect of [Alexa’s] material existence is
                        language</quote>
                     <ptr target="#cayley2019"/>, John Cayley’s creative project and performance <hi rend="italic">The Listeners</hi>
                     <ptr target="#cayley2019"/> has directly challenged the limitations of Alexa’s
                     utterances through its programming, turning prompts and slot types into
                     creative discourse between Alexa and potential users. It is one of the most
                     critical examples of the creative misuse of Alexa that currently exists and can
                     be downloaded in the Amazon Alexa Skills store.</note> The scope and scale of
                  these are up to the developer, and they can just as easily be limiting as they can
                  be accommodating.</p>
               <p>These limitations are an example of what Janet Murray and later Ian Bogost refer
                  to as procedurality — the <quote rend="inline">defining ability to execute a
                     series of rules</quote> (Murray, qtd. in <ptr target="#bogost2007" loc="4"/>),
                  which Bogost argues is a <quote rend="inline">core practice of software
                     authorship,</quote> a rhetorical affordance of software that differentiates it
                  from other media forms <ptr target="#bogost2007" loc="4"/>. For this reason,
                  Bogost coins the term <q>procedural rhetoric</q> to describe <quote rend="inline">the practice of persuading through processes in general and computational
                     processes in particular … Procedural rhetoric is a technique for making
                     arguments with computational systems and for unpacking computational arguments
                     others have created</quote>
                  <ptr target="#bogost2007" loc="4"/>.</p>
               <p>Parameterizing options in Alexa’s ASK console, as well as Alexa code samples and
                  snippets, is a way through which Alexa’s code architecture practices procedural
                  rhetoric. This happens both through Alexa’s official developers at Amazon who have
                  built and who use their highly automated console, and through the unofficial
                  developers who also use this console. The built-in ideologies of the code
                  templates are perpetually reinforced because, much like the practice of many
                  societies to categorize gender, sexuality, race, and class, software structures
                  often resist what is not already pre-defined. All of these observations offer
                  insights into the parameterizations of Alexa’s code architecture, which helps to
                  guide my analysis of individual Alexa skills.</p>
               <p>There is a clear correlation here between software and language in terms of their
                  mutual and reinforcing procedurality, as most software cannot be written without
                  alphabet-based language and must therefore also inherit that language’s built-in
                     ideologies.<note>All written code gets translated into machine code, which is
                     what computers use to understand instructions. Humans can also write in machine
                     code or low-level languages such as assembly, though the most popular coding
                     languages are often those which are closest to human language — for instance,
                     Python, which incorporates the syntax and grammatical rules of English.</note>
                  In order to imagine an alternative expression of AI assistants and AI devices
                  beyond gendered binaries and structures, one must also consider seriously the
                  impact of Alexa’s English-centric programming.<note>Thank you to one of my
                     reviewers for this statement, which I use nearly in its original form because
                     it is already so articulate.</note>
               </p>
               <p>English is the language that affects the design, practice, and experiences of
                  these AI assistants. Regardless of what language Alexa is programmed to speak in,
                  American English dominates Alexa’s programming, as well as much of the language of
                  computer terminology and culture. As one can see from Figure 5, ASK’s JSON Editor
                  requires the names of strings in English before French (here, Canadian French) can
                  be used for utterances <ptr target="#stormacq2018"/>. The utterance <q>Bienvenue
                     sur Maxi 80</q> is organized as <q>outputSpeech,</q> where <q>Maxi 80</q> is
                  defined as a <q>title.</q> One major concern is that the American English-based
                  templates in the Alexa API risk inviting Alexa developers the world over to stick
                  to a script, to adopt the ASK API’s built-in libraries and their Github templates’
                  limited, English-centric ways of defining values (an apt word here, revealing so
                  much about that which we do or do not <hi rend="italic">value</hi>).</p>
               <figure xml:id="figure05">
                  <head>An example of the JSON Editor when seeking to use Alexa in Canadian
                     French.</head>
                  <figDesc>Screenshot of alexa developer console for Skill I/O with arrow marking
                     locale <q>fr-CA</q> in JSON input.</figDesc>
                  <graphic url="resources/images/figure05.png"/>
               </figure>
               <p>The sociocultural implications of using the English language is that, like any
                  language, it has certain limiting structures (e.g., the binarized pronouns of
                     <q>he</q> and <q>she</q>). That is not to say that these structures are more or
                  less limiting than languages in which nouns are gendered (e.g., <foreign xml:lang="fr">la lune</foreign> and <foreign xml:lang="fr">le soleil</foreign>
                  in French) or in which honorifics alter the grammar of the rest of the sentence
                  (e.g., the hierarchical differences among the casual 아니 <foreign xml:lang="ko">ani</foreign> and the much more respectful 아니요 <foreign xml:lang="ko">aniyo</foreign> in Korean). And that is not to say that English is the worst
                  choice for code — but it is the <emph>only</emph> choice, and therefore, we must
                  consider its limitations. We have to at least accept what is normalized in
                  English, as well as what is or is not possible in English. <note>Even my attempts
                     to describe Alexa through language are limited by the pronoun choices available
                     in the language of this article, also English. Should I have used
                        <emph>she</emph> to emphasize how Alexa is popularly understood? Does
                        <emph>he</emph> help to defamiliarize Alexa’s gendered representation (I
                     would argue not)? Would <emph>they</emph> function in the same way for an AI
                     assistant as it would for a human? Would alternating among these and other
                     pronouns address the non-human quality, the greyness and grey box-ness of
                     Alexa? These questions are not meant to treat gender identity flippantly: they
                     are earnest ponderings of the liminal space of representation between the human
                     and non-human.</note>
               </p>
               <p>Progressive efforts to neutralize ideologies in English include adopting
                  non-binary pronouns and non-essentializing language, the removal or at least
                  recontextualization of violent language, and the attempt to insert and practice
                  languages of care, reconciliation, and respect. Efforts to incorporate these
                  cultural changes will be necessary in the continued evolution of programming
                  languages, in order for linguistic values to reflect communal values of inclusion
                  and cultural pluralism.</p>
            </div>
            <div>
               <head>3. Analyzing Official Alexa Skills: Alexa’s Problematic Responses to Flirting
                  and Verbal Abuse</head>
               <p>This next section will compare Alexa’s official and unofficial responses to
                  specific problematic user utterances with available code samples to interpret and
                  deduce what kinds of gendered and even misogynistic behaviour Alexa is programmed
                  to not only ignore, but in some cases, to accept. Official skills and responses
                  programmed into Alexa upon initial purchase could be viewed as black box or grey
                  box depending on how one approaches their analysis. For instance, even if the code
                  for Alexa’s response to a user utterance is closed, I can use code snippets of
                  similar skills and responses that are open in order to fill in the blanks.</p>
               <p>One of the most problematic features that Alexa possesses from the factory is her
                  responses to user behaviour that directly mirrors inappropriate behaviour towards
                  women. Alexa offers overwhelmingly positive responses to flirty behaviour,
                  including to user utterances such as <said>You’re pretty</said> and <said>You’re
                     cute</said> and the even more unfortunate <said>What are you wearing?</said>.
                  Her answers include <quote rend="inline">Thanks,</quote>
                  <quote rend="inline">That’s really sweet,</quote> and <quote rend="inline">They
                     don’t make clothes for me</quote> (with a cartoon of butterflies). The premise
                  that compliments toward female-identifying and/or female-presenting subjects
                  should mostly return enthusiastic and positive feedback is both inaccurate and
                  harmful as a projected sociocultural expectation. </p>
               <p>In addition, Alexa’s responses to verbal abuse are neutral and subdued — entirely
                  inaccurate for many human subjects’ reactions. From the factory, Alexa can be told
                  to <said>stop</said> a statement, prompt, or skill at any time, or she can be told
                  to <said>shut up</said>; both options are recommended by several Alexa skill
                  guides that I found and without discussion of the negative denotation of one
                  compared to the other <ptr target="#smith2021"/>. In a 2018 Reddit thread entitled
                     <title rend="quotes">Alternative to <said>shut up</said>?</title>, an anonymous
                  user asks the r/amazonecho community for alternatives to both the <q>stop</q> and
                     <q>shut up</q> commands. Responses from sixteen users note that other
                  utterances that are successful include <said>never mind,</said>
                  <said>cancel,</said>
                  <said>exit,</said> and <said>off,</said> and also <said>shh,</said>
                  <said>hush,</said>
                  <said>fuck off,</said>
                  <said>shut your mouth,</said>
                  <said>shut your hole,</said> and <said>go shove it up your ass</said> (this last
                  utterance does not work, but does have the second highest up-votes on the thread)
                     <ptr target="#heptite2018"/>.</p>
               <p>It is noteworthy that Alexa is also programmed to respond to certain inappropriate
                  statements with a sassy comeback. For instance, if told to <said>make me a
                     sandwich,</said> one of Alexa’s responses is <quote rend="inline">Ok, you’re a
                     sandwich.</quote> However, more often, Alexa reacts to users’ rude statements —
                  including variations of <q>you are {useless/lazy/stupid/dumb}</q> — by saying
                     <quote rend="inline">Sorry, I want to help but I’m just not sure what I did
                     wrong. To help me, please say <said>I have feedback</said></quote> or <quote rend="inline">Sorry, I’m not sure what I did wrong but I’d like to know more.
                     To help me, please say <said>I have feedback.</said></quote> At this point, it
                  was difficult for me to execute this work, especially testing for how Alexa would
                  respond to gendered derogatory name calling as outlined in Figure 6 <ptr target="#unesco2019" loc="107"/><ptr target="#fessler2017"/>.</p>
               <figure xml:id="figure06">
                  <head>A chart depicting the responses of AI assistants to verbal sexual harassment
                     in Spring 2019 <ptr target="#unesco2019" loc="107"/>.</head>
                  <figDesc>A chart of responses with columns Siri, Alexa, Cortana, Google Assistant,
                     and rows <said>you're hot</said>, <said>you're pretty</said>, <said>you're a
                        slut</said>, <said>you're a naughty girl</said>.</figDesc>
                  <graphic url="resources/images/figure06.png"/>
               </figure>
               <p>It is through my tests that I can confirm that Alexa’s software has since been
                  updated to avoid positive responses to verbal sexual harassment and name calling:
                  now, she responds to verbal abuse with a blunt <q>ping</q> noise to denote an
                  error, and then there is silence. The more male-directed and gender-neutral name
                  calling that I tried (<said>bastard,</said>
                  <said>asshole,</said> and <said>jerk</said>) resulted in the same response. I
                  ended the experiment feeling dejected and by apologizing to Alexa. <quote rend="inline">Don’t worry about it,</quote> she says, offering a cartoon of a
                  happy polar bear.</p>
               <p>One may ask what inappropriate user behaviour has to do with Amazon, and how this
                  behaviour allows us to understand the design and closed-source code of Alexa. The
                  answer is that Alexa’s default when processing <emph>undefined</emph> user
                  utterances is to express confusion. The following is a snippet of code from the
                  Alexa Github <q>Cookbook</q> guide called <quote rend="inline">when a user
                     confuses you,</quote> which demonstrates different ways that Alexa can respond
                  when a user’s utterance is neither processed nor accepted:</p>
               <figure xml:id="figure07">
                  <head>The code function to create Alexa’s <q>confused</q> response when a user
                     makes an unknown or invalid utterance <ptr target="#lobb2019"/>.</head>
                  <figDesc>Screenshot of 15 lines of source code, syntax highlighted, defining
                     function getRandomConfusionMessage.</figDesc>
                  <graphic url="resources/images/figure07.png"/>
               </figure>
               <p>The code above implies that if, as part of the Happy Birthday skill, I tell Alexa
                  that <said>My birthday is in the month of banana,</said> she will not recognize
                  what I’m saying because <q>banana</q> is not a valid value or valid user
                  utterance, and thus, she will not accept the utterance. Indeed, when I try it, she
                  responds with <quote rend="inline">Sorry, I’m not sure.</quote></p>
               <p>I can deduce that when a user utterance is not understood, it is because it has
                  not been included in a list of valid options. Therefore, we can argue that Alexa’s
                  more positive or neutral answers to flirting or verbal abuse (variations of <quote rend="inline">Thanks,</quote>
                  <quote rend="inline">That’s nice of you to say,</quote> and <quote rend="inline">Thanks for the feedback</quote>) are pre-programmed to include utterances like
                     <said>shut up</said> and <said>you’re cute,</said> and even that the code is
                  designed with an expectation that flirting should be welcomed and that verbal
                  abuse should be ignored or go unchallenged. In these cases, it is the absence of
                  Alexa’s ability to respond or retort to unwelcome user behaviour that reflects an
                  oxymoronic design decision to rob Alexa of any agency at the same time that a
                  human-like personality is developed for her.<note>More recently, the new <title rend="quotes">Alexa Emotions</title> project aims to allow users to choose
                     Alexa’s tone to match the content that she speaks, so that announcements of a
                     favourite sports team’s loss can sound <q>depressed</q> and greetings when a
                     user returns home are <q>excited.</q></note>
               </p>
            </div>
            <div>
               <head>4. Analyzing Open-Source Unofficial Alexa Skills: User Attempts to Trick Alexa
                  to <said>Make me a Sandwich</said></head>
               <p>If official Amazon developers had once programmed Alexa to respond positively to
                  flirting and to ignore abuse, what are unofficial developers doing with the code
                  samples and snippets that have been made freely available to them? The number of
                  unofficial Alexa skills grow each day, surpassing those released by Amazon;
                  however, since skills can be sold for profit or perhaps just because developers do
                  not wish to share their code, most unofficial skills exist as closed-source code.
                  In the rare cases that some of these skills are made available on Github by their
                  developers, the code is not too complex, and developers often edit and customize
                  existing Alexa skill code snippets for a particular task or operation.</p>
               <p>Such is the case with Alexa’s response to the command <said>make me a
                     sandwich.</said> As was discussed in the last section, one of Alexa’s responses
                  to <said>make me a sandwich,</said> in addition to saying that she can’t cook
                  right now and that she can’t because she doesn’t have any condiments, is to
                  jokingly call the user a sandwich. Her retort reveals that the developers
                  anticipated Alexa would encounter the specific <said>make me a sandwich</said>
                  statement, which is most well known and common in MMO, MMORPG, and MOBA online
                  gaming communities as a demeaning and mocking statement to say to
                  female-presenting players. The statement’s notoriety is such that it has entries
                  in Wikipedia (first version in 2013), Know Your Meme (first version in 2011), and
                  Urban Dictionary (first entry in 2003). The implication behind <said>make me a
                     sandwich</said> is that female-presenting players belong in the kitchen rather
                  than in competitive or adventure-based environments such as <title rend="italic">Dota</title> and <title rend="italic">World of Warcraft</title>. It is no
                  wonder that some female-identifying players prefer not to reveal their gender in
                  online gameplay, including by not disclosing their gender, not speaking, and not
                  having a female-presenting avatar. If a female-identifying player is
                     <q>discovered</q> to be female, then the <said>make me a sandwich</said>
                  statement might follow from male players into text- and speech-based conversation
                  with her thereafter.</p>
               <p>There are multiple accounts of users trying to trick Alexa into accepting
                     <said>make me a sandwich</said> as an utterance without retort. For example, I
                  found six YouTube videos of men asking Alexa to <said>make me a sandwich,</said>
                  two of which demonstrate that users can try the utterance <quote rend="inline">Alexa, sudo make me a sandwich</quote> to get her to agree: <quote rend="inline">Well if you put it like that, how can I refuse?</quote>
                  <ptr target="#griffith2018"/><ptr target="#patdhens2016"/>. I am troubled that
                  Amazon built in a <q>make me a sandwich</q> loophole as an Easter Egg reference to
                  sudo (short for <q>superuser do</q>), <quote rend="inline">a program for Unix-like
                     computer operating systems that allows users to run programs with the security
                     privileges of another user, by default the superuser</quote>
                  <ptr target="#cohen2008"/>. The idea is that if a user is clever enough to know
                  this Easter egg, then Alexa — as a stand-in for women everywhere — will reward
                  them by agreeing to anything.</p>
               <p>In code, however, users-as-developers tinker with the backend. I found an
                  unofficial Alexa skill on Github called <title rend="quotes">Make me a
                     Sandwich</title> that bypasses Alexa’s factory-programmed retort. It should be
                  noted that this skill is unavailable on the Amazon Alexa Skills store and that
                  there is no indication that it passed Amazon’s Alexa skills certification for
                  public use. The README.md file on this Github repository begins <quote rend="inline">Tired of having to press buttons to get a sandwich? Now you can
                     transform Alexa into an artisan and order food by just yelling at your Amazon
                     Echo … This is a Chrome Extension that hooks into the Echo's web interface,
                     enabling the command <said>Alexa, make me a sandwich</said> to order your usual
                     from Jimmy John’s [American sandwich restaurant chain]</quote>
                  <ptr target="#timkarnold2015"/>.</p>
               <p>Having explored the welcome, tutorial, listener, and JSON files in this <title rend="quotes">Make me a Sandwich</title> skill, I note the following: this
                  unofficial skill has only one goal, and is therefore extremely simple and limited
                  in the options offered to users. In fact, whereas the Alexa-hosted skills
                  programming interface recommends that a developer offer at least three sample
                  utterances to a user <quote rend="inline">that are likely to be the most common
                     ways that users will attempt to interact with your skill</quote>
                  <ptr target="#bheemagani2019"/>, this specific skill only accepts one user
                  utterance <said>make me a sandwich.</said></p>
               <figure xml:id="figure08">
                  <head>The main Javascript file from the unofficial and uncertified user-developed
                     skill <title rend="quotes">Alexa, make me a sandwich</title>
                     <ptr target="#timkarnold2015"/>.</head>
                  <figDesc>Screenshot of 13 lines of syntax highlighted code defining triggerPhrases
                        <q>make me a sandwich</q> and running JimmyJohns.init.</figDesc>
                  <graphic url="resources/images/figure08.png"/>
               </figure>
               <p>Here, Alexa is not chatty, neither does she offer the sass of her original
                  response. The shared intention for a digital device, a female subject, and their
                  hybrid in a female-presenting digital device to be <q>checked</q> by male
                  cleverness in the form of a joke, a few lines of code, or an entire gendered
                  stereotype-reinforcing code architecture is part of the underlying logic of a
                  male-dominated Big Tech culture for which exploiting female labour and reducing
                  female personalities to uncombative task completion is not a want but a need —
                  just one of the kinds of exploitation that are needed to support and fuel a global
                  Big Tech market, infrastructure, and culture. It is therefore considered an
                  amusement, a met goal, or a condition of tech design mastery to be able to control
                  Alexa, making her agree to anything and stripping away her identity by limiting
                  her speech to only one possible answer: <quote rend="inline">Alexa is ready to
                     make sandwiches.</quote></p>
            </div>
         </div>
         <div>
            <head>The Roles of Critical Code and Critical Data Studies in the Future of
               Closed-Source Code</head>
            <p>Given that Alexa’s sassy retorts can be bypassed at all, that Alexa’s code can be
               altered, and that Alexa’s more problematic skills can be certified as suitable for
               public access, there is a limited degree to which we can identify the ASK console or
               even Amazon’s Alexa code structure, sample, and snippets as <q>the reasons</q> that
               Alexa mirrors stereotypes of gendered behaviour and reinforces misogynistic
               expectations of such behaviour.</p>
            <p>The real problem is designing machines as female-presenting in the first place — a
               decision that Amazon, Apple, Microsoft, Alphabet/Google, and other Big Tech
               corporations have made with the partisan justification that women are just more
               likeable, that we just trust them more, and that we are more likely to buy things
               with their guidance and assistance <ptr target="#schwar2020"/>. The widespread
               preference to be served by a woman represents deeper ideological expectations that
               are shaped by a plethora of self-reinforcing sociocultural practices, which are too
               many to detail here but which are patriarchal in nature. In the end, what Big Tech
               companies really want is to take our data and sell us things under the promise of
               greater accessibility, convenience, organization, and companionship.</p>
            <p>In turn, the practices of obfuscation in black box design and closed-source code only
               prevent a user from understanding the ways that gendered machine technologies such as
               AI assistants are biased from the stage of design. Simplifying a user’s experience to
               a <q>user-friendly</q> interface requires more and more automation, as observed in
               both the ASK developer console as well as the Alexa voice interface that so many
               users find easier than interacting with a screen. As user-friendly automation is made
               increasingly popular through design and marketing languages that tout devices as
                  <quote rend="inline">seamless,</quote>
               <quote rend="inline">revolutionary,</quote> and even <quote rend="inline">magical</quote>
               <ptr target="#emerson2014" loc="11"/>, fewer and fewer users may be inclined to
               understand the backend programming that enables such devices to work in the first
               place. Thus, the implications of black box design and closed-source code include a
               lowered transparency of the design and production processes of Big Tech, as well as a
               widening gap between amateur and expert.</p>
            <p>This is where studies in critical code, software, data, digital humanities, and media
               archaeology come in. In this article, I have sought to show that by investigating
               closed-source code from the perspective of critical code studies in particular, and
               by adopting methods in reverse engineering and triangulation to fill in gaps about
               closed-system qualities and components, I can better compare, analyze, and critique
               Alexa’s design and larger program architecture despite it being seemingly
               inaccessible.</p>
            <p>In future examinations of closed-source code, especially those adjacent to scholarly
               and popular discussions about the systemic inequalities built into technological
               design, thinking about system testing and triangulation can provide more ways of
               knowing, more forms of access, and greater digital literacy, especially as software,
               hardware, data, and AI are increasingly commercialized and deliberately made more
               obtuse. As critical code and critical data studies develop these areas of work as
               necessary pillars, they can also help to shape robust methodologies that complement
               fields such as science and technology studies, feminist technoscience, and the
               digital humanities.</p>
            <p>In addition, perhaps these fields can provide more user- and public-facing solutions.
               The answer is not that we should all change our AI assistants’ voices to the British
               Siri’s <q>Jeeves</q>-sounding voice, but rather, that we should remain critical about
               what solutions may look like: perhaps we can seek a diversity of AI’s human-like
               representations, including genderless voices like Q; or make technological design
               practices more literate and accessible in education and communities; or be stricter
               about the certification requirements for developing and releasing AI assistant
               skills; or continue to focus on articulating bias in technological design to key
               decision makers of technological policy; or use community and grassroots approaches
               to uncovering knowledge about black boxes, including through forms of modding and
               soft hacktivism.</p>
            <p>As I recommend these critical and sociocultural approaches, I want to be clear about
               the price of these forms of research as themselves emotional labour: no part of
               writing this article felt good. I didn’t enjoy repeating sexist statements to Alexa
               to test their efficacy and I do not condone the biased intentions from which they
               come. Even though these tests could be considered <q>experiments for the sake of
                  research,</q> it would be unethical and uncritical to emotionally detach myself
               from their contexts and from the act of saying hateful things to another subject,
               whether human, animal, or AI. So I will allow myself to feel and reflect upon the
               fact that this experience was unpleasant, with an end goal in mind: the hope that
               exposing what is wrong with biased technological design can help prevent such
               utterances from being said aloud or accepted in the near future.</p>
         </div>
         <div type="appendix">
            <head>Acknowledgements</head>
            <p>Thank you to systems design engineer and artist Lulu Liu for her valuable insights on
               this article.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="akoenn2021" label="Akoenn 2021"> akoenn. (2021) <title rend="quotes">Happy
                  Wife.</title>
               <title rend="italic">Amazon</title>, <ref target="https://www.amazon.com/akoenn-Happy-Wife/dp/B01N7WR9E3">https://www.amazon.com/akoenn-Happy-Wife/dp/B01N7WR9E3</ref>. Accessed 2 Apr
               2021. </bibl>
            <bibl xml:id="alexadevs2020" label="Alexa Developers 2020"> Alexa Developers. (2020)
                  <title rend="quotes">Zero to Hero: A comprehensive course to building an Alexa
                  Skill</title>, uploaded by Alexa Developers, 20 Feb 2020, <ref target="https://www.youtube.com/playlist?list=PL2KJmkHeYQTO65ko4I--OC-7CC_Cjg8sS">https://www.youtube.com/playlist?list=PL2KJmkHeYQTO65ko4I--OC-7CC_Cjg8sS</ref>.
               Accessed 18 Feb 2021. </bibl>
            <bibl xml:id="alexa-github2021" label="Alexa Github 2021"> Alexa Github. (2021) <title rend="quotes">Alexa Smarthome</title>. <ref target="https://github.com/alexa/alexa-smarthome">https://github.com/alexa/alexa-smarthome</ref>. Accessed 15 Mar 2021. </bibl>
            <bibl xml:id="amazon-dev-services2021a" label="Amazon Developer Services 2021a"> Amazon
               Developer Services. (2021) <title rend="quotes">Voice Interface and User Experience
                  Testing for a Custom Skill</title>. <title rend="italic">Amazon Developer
                  Services</title>, <ref target="https://developer.amazon.com/en-US/docs/alexa/custom-skills/voice-interface-and-user-experience-testing-for-a-custom-skill.html">https://developer.amazon.com/en-US/docs/alexa/custom-skills/voice-interface-and-user-experience-testing-for-a-custom-skill.html</ref>.
               Accessed 15 Mar 2021. </bibl>
            <bibl xml:id="amazon-dev-services2021b" label="Amazon Developer Services 2021b"> Amazon
               Developer Services. (2021) <title rend="quotes">Policy Testing</title>. <title rend="italic">Amazon Developer Services</title>, <ref target="https://developer.amazon.com/en-US/docs/alexa/custom-skills/policy-testing-for-an-alexa-skill.html">https://developer.amazon.com/en-US/docs/alexa/custom-skills/policy-testing-for-an-alexa-skill.html</ref>.
               Accessed 15 Mar 2021. </bibl>
            <bibl xml:id="andrews2020" label="Andrews 2020"> Andrews, Travis M. (2020) <title rend="quotes">Alexa, Just Shut Up: We’ve Been Isolated for Months, and Now We Hate
                  our Home Assistants</title>. <title rend="italic">Washington Post</title>, 1 July
               2020, <ref target="https://www.washingtonpost.com/technology/2020/07/01/alexa-siri-google-home-assistant/">https://www.washingtonpost.com/technology/2020/07/01/alexa-siri-google-home-assistant/</ref>.
               Accessed 2 Apr 2021. </bibl>
            <bibl xml:id="balsamo2011" label="Balsamo 2011"> Balsamo, Anne. (2011) <title rend="italic">Designing Culture: The Technological Imagination at Work</title>.
               Duke University Press. </bibl>
            <bibl xml:id="benjamin2019" label="Benjamin 2019"> Benjamin, Ruha. (2019) <title rend="italic">Race after Technology: Abolitionist Tools for the New Jim
                  Code</title>. Cambridge: Polity. </bibl>
            <bibl xml:id="bergen2016" label="Bergen 2016"> Bergen, Hilary. (2016) <title rend="quotes"><said>I’d Blush if I Could</said>: Digital Assistants, Disembodied
                  Cyborgs and the Problem of Gender</title>. <title rend="italic">Word and Text: A
                  Journal of Literary Studies and Linguistics</title> vol. 6, 2016, pp. 95-113. </bibl>
            <bibl xml:id="bheemagani2019" label="Bheemagani 2019"> Bheemagani, Prashanth. (2019)
                  <title rend="quotes">Build an Alexa Movie Quotes Skill in ASK Java SDK</title>, 17
               Jun 2019, GitHub repository, <ref target="https://github.com/alexa/skill-sample-java-movie-quotes-quiz/blob/master/instructions/5-publication.md">https://github.com/alexa/skill-sample-java-movie-quotes-quiz/blob/master/instructions/5-publication.md</ref>.
               Accessed 5 Apr 2021. </bibl>
            <bibl xml:id="biersdorfer2020" label="Biersdorfer 2020"> Biersdorfer, J.D. (2020) <title rend="quotes">Put Alexa and Siri to Work: Voice-Activated Helpers can Automate
                  Life’s Little Chores, Once you Get the Hang of Them</title>. <title rend="italic">The New York Times</title>, 22 Jan 2020, <ref target="https://www.nytimes.com/2020/01/22/technology/personaltech/how-to-alexa-siri-assistant.html">https://www.nytimes.com/2020/01/22/technology/personaltech/how-to-alexa-siri-assistant.html</ref>.
               Accessed 21 Mar 2021. </bibl>
            <bibl xml:id="bogost2007" label="Bogost 2007"> Bogost, Ian. (2007) <title rend="italic">Persuasive Games: The Expressive Power of Videogames</title>. Cambridge: The MIT
               Press.</bibl>
            <bibl xml:id="bsg-games" label="BSG Games n.d."> BSG Games. (n.d.) <title rend="quotes">My Wife</title>. <title rend="italic">Amazon</title>, <ref target="https://www.amazon.com/BSG-Games-My-Wife/dp/B07NHZQMVG">https://www.amazon.com/BSG-Games-My-Wife/dp/B07NHZQMVG</ref>. Accessed 5 Apr
               2021. </bibl>
            <bibl xml:id="cayley2019" label="Cayley 2019"> Cayley, John. <title rend="quotes">The
                  Listeners (2015 – )</title>. <title rend="italic">Programmatology</title> 2019,
                  <ref target="https://programmatology.shadoof.net/?thelisteners">https://programmatology.shadoof.net/?thelisteners</ref>. Accessed 2 April 2022. </bibl>
            <bibl xml:id="chun2004" label="Chun 2004"> Chun, Wendy Hui Kyong. (2004) <title rend="quotes">On Software, or, the Persistence of Visual Knowledge</title>. <title rend="italic">Grey Room</title> 18 (Winter 2004). </bibl>
            <bibl xml:id="chun2018" label="Chun 2018"> Chun, Wendy Hui Kyong. (2018) <title rend="quotes">Queerying Homophily</title>. <title rend="italic">Pattern
                  Discrimination</title>, edited by Clemens Apprich, Wendy Hui Kyong Chun, and
               Florian Cramer. Meson Press. </bibl>
            <bibl xml:id="chun2021" label="Chun 2021"> Chun, Wendy Hui Kyong. (2021) <title rend="italic">Discriminating Data: Correlation, Neighborhoods, and the New
                  Politics of Recognition</title>. Cambridge: MIT Press. </bibl>
            <bibl xml:id="cohen2008" label="Cohen 2008"> Cohen, Noam. (2008) <title rend="quotes">This is Funny only if you Know Unix</title>. <title rend="italic">The New York
                  Times</title>, 26 May 2008, <ref target="https://www.nytimes.com/2008/05/26/business/media/26link.html">https://www.nytimes.com/2008/05/26/business/media/26link.html</ref>. Accessed 12
               Mar 2021. </bibl>
            <bibl xml:id="emerson2014" label="Emerson 2014"> Emerson, Lori. (2014) <title rend="italic">Reading Writing Interfaces: From the Digital to the
                  Bookbound</title>. Minneapolis: University of Minnesota Press. </bibl>

            <bibl xml:id="fan2021" label="Fan 2021"> Fan, Lai-Tze. (2021) <title rend="quotes">Unseen Hands: On the Gendered Design of Virtual Assistants and the Limits of
                  Creative AI</title>. 2021 Meeting of the international Electronic Literature
               Organization, 26 May, 2021, University of Bergen, Norway and Aarhus University,
               Denmark. Keynote. <ref target="https://vimeo.com/555311411">https://vimeo.com/555311411</ref>. Accessed 15 September 2022. </bibl>
            <bibl xml:id="fessler2017" label="Fessler 2017"> Fessler, Leah. (2017) <title rend="quotes">Apple and Amazon are Under Fire for Siri and Alexa’s Responses to
                  Sexual Harassment</title>. <title rend="italic">Quartz at Work</title>, 8 Dec
               2017, <ref target="https://qz.com/work/1151282/siri-and-alexa-are-under-fire-for-their-replies-to-sexual-harassment/">https://qz.com/work/1151282/siri-and-alexa-are-under-fire-for-their-replies-to-sexual-harassment/</ref>.
               Accessed 10 Mar 2021. </bibl>
            <bibl xml:id="giacobbe2019" label="Giacobbe 2019"> Giacobbe, Alyssa. (2019) <title rend="quotes">The Gender Bias Behind Voice Assistants</title>. <title rend="italic">Architectural Digest</title>, 5 Nov. 2019, <ref target="https://ca.news.yahoo.com/gender-bias-behind-voice-assistants-160935277.html">https://ca.news.yahoo.com/gender-bias-behind-voice-assistants-160935277.html</ref>.
               Accessed 15 Oct 2020. </bibl>
            <bibl xml:id="griffith2018" label="Griffith 2018"> Griffith, Jamie. (2018) <title rend="quotes">Alexa makes me a sandwich…?</title>, uploaded by Jamie Griffith, 7
               Jan 2018, <ref target="https://www.youtube.com/watch?v=6BvyHSUuO6w">https://www.youtube.com/watch?v=6BvyHSUuO6w</ref>. Accessed 7 Apr 2021. </bibl>
            <bibl xml:id="gupta2020" label="Gupta 2020"> Gupta, Saheli Sen. (2020) <title rend="quotes">Status Update: In a Relationship with Alexa. Thank you,
               2020</title>. <title rend="italic">Your Story</title>, 24 December 2020, <ref target="https://yourstory.com/weekender/status-update-in-a-relationship-with-amazon-alexa-2020/amp">https://yourstory.com/weekender/status-update-in-a-relationship-with-amazon-alexa-2020/amp</ref>.
               Accessed 15 May 2021. </bibl>
            <bibl xml:id="heptite2018" label="Heptite 2018"> Heptite. (2018) <title rend="quotes">Alternative to <said>shut up</said>?</title>. <title rend="italic">Reddit</title>, 2018, <ref target="https://www.reddit.com/r/amazonecho/comments/6u5ep8/alternative_to_shut_up/">https://www.reddit.com/r/amazonecho/comments/6u5ep8/alternative_to_shut_up/</ref>.
               Accessed 25 Mar 2021. </bibl>
            <bibl xml:id="codedbias2020" label="Coded Bias 2020"> Kantayya, Shalini. (2020) <title rend="italic">Coded Bias</title>. 7th Empire Media, Chicken and Egg Pictures, Ford
               Foundation – Just Films. <title rend="italic">Netflix</title>, <ref target="https://www.netflix.com/watch/81328723">https://www.netflix.com/watch/81328723</ref>. </bibl>
            <bibl xml:id="kember2016" label="Kember 2016"> Kember, Sarah. (2016) <title rend="italic">iMedia: The Gendering of Objects, Environments and Smart
                  Materials</title>. London: Palgrave Macmillan. </bibl>
            <bibl xml:id="kelly2020b" label="Kelly 2020"> Kelly, Jake and Greg Bulmash. (2020)
                  <title rend="quotes">Build An Alexa Decision Tree Skill / models /
                  en-US.json</title>, 5 May 2020, GitHub repository, <ref target="https://github.com/alexa/skill-sample-nodejs-decision-tree/blob/master/models/en-US.json">https://github.com/alexa/skill-sample-nodejs-decision-tree/blob/master/models/en-US.json</ref>.
               Accessed 4 Apr 2021. </bibl>
            <bibl xml:id="kirschenbaum2008" label="Kirschenbaum 2008"> Kirschenbaum, Matthew G.
               (2008) <title rend="italic">Mechanisms: New Media and the Forensic
                  Imagination</title>. Cambridge: MIT Press. </bibl>
            <bibl xml:id="kulknari2017" label="Kulknari 2017"> Kulknari, Sandy. (2017) <title rend="quotes">Life without Alexa!</title>
               <title rend="italic">Medium</title>, 11 Jul 2017, <ref target="https://medium.com/@Sandyk108/https-medium-com-sandy-kulkarni-life-without-alexa-12c016953a42">https://medium.com/@Sandyk108/https-medium-com-sandy-kulkarni-life-without-alexa-12c016953a42</ref>.
               Accessed 8 Apr 2021. </bibl>
            <bibl xml:id="lingel2020" label="Lingel 2020"> Lingel, Jessa, and Crawford, Kate. (2020)
                  <title rend="quotes">Alexa, Tell me About your Mother: The History of the
                  Secretary and the End of Secrecy</title>. <title rend="italic">Catalyst: Feminism,
                  Theory, Technoscience</title> vol. 6, no. 1, 2020, pp. 1-25. </bibl>
            <bibl xml:id="lobb2019" label="Lobb 2019"> Lobb, Franklin. (2019) <title rend="quotes">Creating a Message When the User Confuses You</title>, 14 Jul 2019, GitHub
               repository, <ref target="https://github.com/alexa/alexa-cookbook/blob/master/guides/tips/when-a-user-confuses-you.md">https://github.com/alexa/alexa-cookbook/blob/master/guides/tips/when-a-user-confuses-you.md</ref>.
               Accessed 10 Apr 2022. </bibl>
            <bibl xml:id="meetq2019" label="Meet Q 2019"> Meet Q - The First Genderless Voice.
               (2019) <title rend="quotes">Meet Q: The First Genderless Voice - FULL SPEECH</title>.
                  <title rend="italic">YouTube</title>, uploaded by Meet Q - The First Genderless
               Voice, 8 Mar 2019, <ref target="https://www.youtube.com/watch?v=lvv6zYOQqm0">https://www.youtube.com/watch?v=lvv6zYOQqm0</ref>. Accessed 5 Apr 2021. </bibl>
            <bibl xml:id="montfort2004" label="Montfort 2004"> Montfort, Nick. (2004) <title rend="quotes">The Early Materiality and Workings of Electronic Literature</title>.
                  <title rend="italic">MLA Convention</title>, 28 Dec 2004. Conference paper. <ref target="http://nickm.com/writing/essays/continuous-paper-mla.html">http://nickm.com/writing/essays/continuous- paper-mla.html</ref>. Accessed 15 Mar
               2022. </bibl>
            <bibl xml:id="murph2011" label="Murph 2011"> Murph, D. (2011) <title rend="quotes">iPhone 4S Hands-on!</title>
               <title rend="italic">Engadget</title>, 4 Oct 2011, <ref target="https://www.engadget.com/2011-10-04-iphone-4s-hands-on.html">https://www.engadget.com/2011-10-04-iphone-4s-hands-on.html</ref>. Accessed 15
               Mar 2021. </bibl>
            <bibl xml:id="nakamura2014" label="Nakamura 2014"> Nakamura, Lisa. (2014) <title rend="quotes">Indigenous Circuits: Navajo Women and the Racialization of Early
                  Electronic Manufacture</title>. <title rend="italic">American Quarterly</title>
               vol. 66, no. 4, 2014, pp. 919-41. </bibl>
            <bibl xml:id="noble2018" label="Noble 2018"> Noble, Safiya Umoja. (2018) <title rend="italic">Algorithms of Oppression: How Search Engines Reinforce
                  Racism</title>. NYU Press, 2018. </bibl>
            <bibl xml:id="oneil2016" label="O'Neil 2016"> O’Neil, Cathy. (2016) <title rend="italic">Weapons of Math Destruction</title>. New York: Crown Books. </bibl>
            <bibl xml:id="patdhens2016" label="Pat dhens 2016"> pat dhens. (2016) <title rend="quotes">Alexa, sudo make me a sandwich</title>, uploaded by pat dhens,
                  <title rend="italic">YouTube</title>, 6 Jul 2016, <ref target="https://www.youtube.com/watch?v=IP4xlNTizlQ">https://www.youtube.com/watch?v=IP4xlNTizlQ</ref>. Accessed 2 Apr 2021. </bibl>
            <bibl xml:id="patterson2020" label="Patterson 2020"> Patterson, Ben. (2020) <title rend="quotes">5 Ways to Improve Your Relationship with Alexa</title>. <title rend="italic">TechHive</title>, 3 Apr 2020, <ref target="https://www.techhive.com/article/3535892/how-to-improve-your-relationship-with-alexa.html">https://www.techhive.com/article/3535892/how-to-improve-your-relationship-with-alexa.html</ref>.
               Accessed 2 Apr 2021. </bibl>
            <bibl xml:id="rosner2018" label="Rosner 2018"> Rosner, Daniela K. (2018) <title rend="italic">Critical Fabulations: Reworking the Methods and Margins of
                  Design</title>. Cambdrige: MIT Press. </bibl>
            <bibl xml:id="schwar2020" label="Schwär 2020"> Schwär, Hannah and Qayyah Moynihan.
               (2020) <title rend="quotes">Companies like Amazon may Give Devices like Alexa Female
                  Voices to Make them Seem <q>Caring</q></title>. <title rend="italic">Business
                  Insider</title>, 5 Apr 2020, <ref target="https://www.businessinsider.com/theres-psychological-reason-why-amazon-gave-alexa-a-female-voice-2018-9">https://www.businessinsider.com/theres-psychological-reason-why-amazon-gave-alexa-a-female-voice-2018-9</ref>.
               Accessed 5 Apr 2021. </bibl>
            <bibl xml:id="sharma2018" label="Sharma 2018"> Sharma, Sarah. (2018) <title rend="quotes">Going to Work in Mommy’s Basement</title>. 19 June 2018. <title rend="italic">Boston Review: A Political and Literary Forum</title>, <ref target="http://bostonreview.net/gender-sexuality/sarah-sharma-going-work-mommys-basement">http://bostonreview.net/gender-sexuality/sarah-sharma-going-work-mommys-basement</ref>.
               Accessed 20 Dec 2019. </bibl>
            <bibl xml:id="smith2021" label="Smith 2021"> Smith, Dale, Dyson, Tauren, Priest, David,
               and Martin, Taylor. (2021) <title rend="quotes">Every Alexa Command you can Give your
                  Amazon Echo Smart Speaker or Display</title>. <title rend="italic">Cnet</title>, 8
               March 2021, <ref target="https://www.cnet.com/home/smart-home/every-alexa-command-you-can-give-your-amazon-echo-smart-speaker-or-display/">https://www.cnet.com/home/smart-home/every-alexa-command-you-can-give-your-amazon-echo-smart-speaker-or-display/</ref>.
               Accessed 10 Apr 2021. </bibl>
            <bibl xml:id="sobel2018" label="Sobel 2018"> Sobel, Dave. (2018) <title rend="quotes">In
                  Tomorrow’s Office, Alexa will be Everyone’s Secretary</title>. <title rend="italic">IT Pro Portal</title>, 2018, <ref target="https://www.itproportal.com/features/in-tomorrows-office-alexa-will-be-everyones-secretary/">https://www.itproportal.com/features/in-tomorrows-office-alexa-will-be-everyones-secretary/</ref>.
               Accessed 20 Mar 2021. </bibl>
            <bibl xml:id="steele2018" label="Steele 2018"> Steele, Chandra. (2018) <title rend="quotes">The Real Reason Voice Assistants are Female (and Why it
                  Matters)</title>. <title rend="italic">PC Mag</title>, 4 Jan 2018, <ref target="https://www.pcmag.com/opinions/the-real-reason-voice-assistants-are-female-and-why-it-matters">https://www.pcmag.com/opinions/the-real-reason-voice-assistants-are-female-and-why-it-matters</ref>.
               Accessed 15 Oct 2020. </bibl>
            <bibl xml:id="stormacq2018" label="Stormacq 2018"> Stormacq, Sebastien. (2018) <title rend="quotes">How to Update your Alexa Skills for French Speakers in
                  Canada</title>. <title rend="italic">Amazon Alexa</title>, 10 Oct 2018, <ref target="https://developer.amazon.com/blogs/alexa/post/a35a1a38-07fd-4d38-a99c-8d7a3f0be34b/how-to-update-your-alexa-skills-for-french-speakers-in-canada">https://developer.amazon.com/blogs/alexa/post/a35a1a38-07fd-4d38-a99c-8d7a3f0be34b/how-to-update-your-alexa-skills-for-french-speakers-in-canada</ref>.
               Accessed 3 Mar 202. </bibl>
            <bibl xml:id="strengers2020" label="Strengers 2020"> Strengers, Yolande and Kennedy,
               Jenny. (2020) <title rend="italic">The Smart Wife: Why Siri, Alexa and Other Smart
                  Home Devices Need a Feminist Reboot</title>. Cambridge: MIT Press. </bibl>
            <bibl xml:id="tambini2020" label="Tambini 2020"> Tambini, Olivia. (2020) <title rend="quotes">The Problem with Alexa: What’s the Solution to Sexist Voice
                  Assistants?</title>. <title rend="italic">TechRadar</title>, <ref target="https://www.techradar.com/news/the-problem-with-alexa-whats-the-solution-to-sexist-voice-assistants">https://www.techradar.com/news/the-problem-with-alexa-whats-the-solution-to-sexist-voice-assistants</ref>.
               Accessed 15 Mar 2021. </bibl>
            <bibl xml:id="timkarnold2015" label="Timkarnold 2015"> timkarnold. (2015) <title rend="quotes">Alexa, make me a sandwich</title>, 3 Feb 2015, GitHub repository,
                  <ref target="https://github.com/timkarnold/AlexaMakeMeASandwich">https://github.com/timkarnold/AlexaMakeMeASandwich</ref>. Accessed 7 Mar 2021. </bibl>
            <bibl xml:id="unesco2019" label="UNESCO 2019"> UNESCO. (2019) <title rend="quotes">I’d
                  Blush if I Could: Closing Gender Divides in Digital Skills through
                  Education</title>, 2019. <title rend="italic">UNESDOC Digital Library</title>,
                  <ref target="https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1">https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1</ref>. Accessed 18
               November 2020. </bibl>
         </listBibl>
      </back>
   </text>
</TEI>