<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
	xmlns:mml="http://www.w3.org/1998/Math/MathML">
	<teiHeader>
		<fileDesc>
			<titleStmt>
				<!--Author should supply the title and personal information-->
				<title type="article" xml:lang="en"><!--article title in English-->Interpreting Measures of Meaning:
					Introducing Salience Differentiated Stability</title>
				<!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
				<dhq:authorInfo>
					<!--Include a separate <dhq:authorInfo> element for each author-->
					<dhq:author_name>Hugo Dirk <dhq:family>Hogenbirk</dhq:family>
					</dhq:author_name>
					<idno type="ORCID"
						><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
					<dhq:affiliation>University of Groningen, Departement of the History of Philosophy</dhq:affiliation>
					<email>h.d.hogenbirk@rug.nl</email>
					<dhq:bio>
						<p>Hugo Dirk Hogenbirk is a PhD-candidate at the University of Groningen in the Netherlands. 
							He's working on the history of early modern philosophy and works with corpus analysis tools, with particular emphasis on the relations between shifts in language use and conceptual development.
							Other academic interests include the work of Anne Conway, analytic metaphysics, and the philosophy of games and fiction.</p>
					</dhq:bio>
				</dhq:authorInfo>
				<dhq:authorInfo>
					<!--Include a separate <dhq:authorInfo> element for each author-->
					<dhq:author_name>Wim <dhq:family>Mol</dhq:family>
					</dhq:author_name>
					<idno type="ORCID"
						><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
					<dhq:affiliation></dhq:affiliation>
					<email>wimgmol@gmail.com</email>
					<dhq:bio>
						<p>Wim Mol has a Master's degree in Philosophy. His philosophical interests include the philosophy of language and epistemology. He is currently working outside academia.</p>
					</dhq:bio>
				</dhq:authorInfo>
			</titleStmt>
			<publicationStmt>
				<publisher>Alliance of Digital Humanities Organizations</publisher>
				<publisher>Association for Computers and the Humanities</publisher>
				<!--This information will be completed at publication-->
				<idno type="DHQarticle-id"
					><!--including leading zeroes: e.g. 000110-->000648</idno>
				<idno type="volume"
					><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->016</idno>
				<idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->4</idno>
				<date when="2022-09-16">16 September 2022</date>
				<dhq:articleType>article</dhq:articleType>
				<availability status="CC-BY-ND">
					<!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
					<cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
				</availability>
			</publicationStmt>
			<sourceDesc>
				<p>This is the source</p>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<classDecl>
				<taxonomy xml:id="dhq_keywords">
					<bibl>DHQ classification scheme; full list available at <ref
							target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
							>http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
					</bibl>
				</taxonomy>
				<taxonomy xml:id="authorial_keywords">
					<bibl>Keywords supplied by author; no controlled vocabulary</bibl>
				</taxonomy>
			</classDecl>
		</encodingDesc>
		<profileDesc>
			<langUsage>
				<language ident="en" extent="original"/>
				<!--add <language> with appropriate @ident for any additional languages-->
			</langUsage>
			<textClass>
				<keywords scheme="#dhq_keywords">
					<!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
					<list type="simple">
						<item/>
					</list>
				</keywords>
				<keywords scheme="#authorial_keywords">
					<!--Authors may include one or more keywords of their choice-->
					<list type="simple">
						<item/>
					</list>
				</keywords>
			</textClass>
		</profileDesc>
		<revisionDesc>
			<!--Each change should include @who and @when as well as a brief note on what was done.-->
			<change/>
		</revisionDesc>
	</teiHeader>
	<text xml:lang="en" type="original">
		<front>
			<dhq:abstract>
				<!--Include a brief abstract of the article-->
				<p>In digital studies of the use of words in intellectual history, meaning is
					measured based on the idea of Firth that a word can be characterized by the
					company it keeps. The words that are literally close to it in the texts in
					which it is written should tell us something about what the word means. In
					practice, we will look at meaning being measured by a method called <title
						rend="italic">Pointwise Mutual Information </title>or PMI for short.
					However, even granting that we use PMI, this description is still quite
					vague and allows for multiple different ways to interpret 'the company a
					word keeps' in the practice of coding an actual algorithm to discern this
					company. In this paper we will look specifically at the choice to 1. use
					all words close to another or 2. use merely the ones that are most
					disproportionately present. Using work from contemporary philosophers of
					language Mark Wilson and Sally Haslanger, we argue that both capture an
					aspect of meaning, 2 capturing the most salient way to understand a word,
					and 1 capturing the subtle, not so salient, but nonetheless important ways
					in which words are used overall. Then we will look to measure the overall
					stability in word meaning, the degree to which it is used similarly in
					different texts within a corpus. Having characterized PMI, salience and
					stability we will introduce <title rend="italic">Salience Differentiated
						Stability </title>or SDS, a value indicating both the salient and less
					salient stability of a word, which will help identify words that are
					simpler or more shifty than they at first appear. Lastly we will test the
					use of this new value by doing a case study of the salience differentiated
					stability of common terms in early modern physics text books.</p>
			</dhq:abstract>
			<dhq:teaser>
				<!--Include a brief teaser, no more than a phrase or a single sentence-->
				<p/>
			</dhq:teaser>
		</front>
		<body>
			<div>
			<head>Section 1: Introduction</head>
			<p>In digital humanities it is common to study the change in use of a word in a
				corpus of text by means of vector semantics, or distributional semantics. The
				idea is to map word-types found in a text or in an entire corpus to
					vectors.<note> For a good general introduction, see <ptr target="#smith2020"/></note> Each
				coordinate of the vector will represent an association with another word-type.
				An algorithm can be written to scour parts of text near the word-type for
				associations. The idea is that a word like, for example, 'food' will
				automatically be associated with other words that tell us about the concept of
				food. Prominent examples of food, like 'cheese' or 'bread', or near synonyms
				like 'nutrition', or biologically related organs like 'stomach'. Such methods
				have at least the following advantages: first, they are fully automated, and
				hence, large corpora of text that could not realistically be studied manually
				can now be studied. Secondly, they can track homonymy, (near) synonymy and
				changes in the use of an investigated word, even as the word stays
				linguistically constant. Linguistically these methods are justified by the idea
				due to John Firth that <quote rend="inline">You shall know a word by the company
					it keeps</quote> <ptr target="#firth1957"/>. Firth, and somewhat synchronously, Zellig
				Harris, defended and developed the view that the verbal connotations of a word
				gave us important insight into its use, more so than any definition could do
				<ptr target="#firth1957"/> <ptr target="#harris1964"/>. We need not go that far. To justify the idea of
				vector semantics we only need to believe that the distributional properties of a
				word give us some semantic insights into the word that are of interest to a
				purveyor of large corpora <ptr target="#gavin2019"/>.</p>
			<p>However, this does not settle the exact details of our vector semantics. There are
				many different ways to calculate the exact values of each of the coordinates,
				and it is not clear which of these characterizations of the company of a word
				tells us what. Some people might assume that one or the other characterization
				gets us closer to a word’s meaning, to be expressed for example, by scoring
				higher or lower on pre-defined metrics. We will offer an alternative to this
				approach by arguing that different methods might give us different facets of
				meaning. </p>
			<p>In this paper we look at two particular ways to turn the company of a word into
				vectors. One, where each word-type in a corpus is mapped to a number
				representing the frequency with which it occurs near another word-type relative
				to its overall frequency, and another where a word-type is associated with a
				list of high scoring words. Both of these methods find the 'company' of a word
				in some way. However, we will claim, these results need not be the same for a
				given word-type. Hence, the main purpose of this paper is to offer an
				interpretation of what these different ways of specifying ‘the company' tell us,
				so that an investigator can i) more easily choose between the two methods and
				ii) use them both next to each other profitably. Both of these goals constitute
				a novelty; i) depends on the interpretations of the methods as providing
				different (and not competing) facets of meaning (sec.2). By doing such analysis
				we are applying insight from recent innovations in the philosophy of language
				that have come to fall under the name of ‘conceptual engineering’, to the
				interpretation of language by algorithms. ii) is cashed out in a new method that
				measures the divergence of the two methods as a new semantic measure (sec.4)
				whose object has no counterpart in the literature. In order to interpret these
				methods we will examine their workings in detail. </p>
			<p> In section 2, we draw on the work of philosophers like Sally Haslanger and Mark
				Wilson to argue that a word has salient, more publicly known and obvious
				semantic content as well as more subtle and hence less salient content. We will
				argue that both are important aspects of the semantics of a word. The subtle,
				less salient, content is usually understood as an ability to navigate subtleties
				which we cannot or would not make explicit. We will argue that our
				restricted-list method tracks the salient connotations whereas the unrestricted
				method tracks its more complete use, including hidden semantic subtleties.</p>
			<p>In section 3 we need to dive into the technicalities of the methods discussed. We
				will show in detail the operational difference between measuring more and less
				salient semantic content. In addition we will introduce methods to measure the
				overall stability of word-meaning in a given corpus. Stability will be a measure
				of the similarity of word use across multiple texts. Lastly we will introduce
				our novel methodological proposal which we dub Salience Differentiated
				Stability, which is a two-vector signifying a word's relative salient and less
				salient stability. This is a metric that will be applicable to any given corpus
				of texts.</p>
			<p>In section 4 we provide an interpretation of SDS-scores. A word can score high and
				low on both salient and non-salient stability. We give interpretations for all
				four possible cases.</p>
			<p>Finally, in section 5, we provide the results of an explorative case study where
				the Salience Differentiated Stability of a small set of word-types in a corpus
				of 17th and 18th century physics/natural philosophy is measured and analyzed. This is not
				a history paper per se, but rather an examination of methodology. Hence, the
				purpose of this case study is not to study 17th century physics/natural
				philosophy. It is rather to see whether the methodology, which this paper is
				about, yields results that make prima facie sense and are non-trivial. They
				might be trivial if varying salience yielded no difference in results or none
				that could be plausibly interpreted. We will find in the case study that words
				which score high on salient stability, do not necessarily score high on
				subtle/less salient stability, or vice versa. This means that in actual
				historical examples, salient and less salient semantic content can be quite
				different both in content and in overall stability. </p></div>
			<div>
			<head>Section 2: Salience</head>
			<p>When it comes to vector semantics, there are different ways to code the algorithms
				that determine the semantic content of a word in a given body of text. We will
				look at two methods using PMI and vector embedding, where a word is
				characterized as a large vector, each of the coordinates in the vector
				signifying how often another word-type appears in close proximity to it. The
				number appearing in these coordinates is called the collocation score. So, if we
				are characterizing 'gravity' and 'gravity' and 'force' often appear close to one
				another in a text, the coordinate signifying 'force' will have a large number
				(see section 3 for the relevant math and coding). The main methodological choice
				we will be looking at is the following: when constructing a vector for a word we
				might decide to ignore all but the most extreme coordinates, that is, all but
				those words with the highest collocation scores, but we don't have to restrict
				our attention in this way. We could instead look at all of the coordinates in
				our vector. The question is: which of these ways of coding best captures the
				semantic content we want to capture. One way to answer this is by arguing that
				the meaning of a word is mostly determined by the words it is associated with
				most or alternatively, the way it is associated with all other word-types in the
				text. We believe both these answers can coexist in a way. In this section we
				will argue that there are different ways to conceive of the semantic content of
				a word that justify the use of different algorithms. </p>
			<p>The idea of distinguishing different kinds of semantic analysis already exists in
				the work of feminist philosopher Sally Haslanger. In her <title rend="italic">What
					Are We Talking About? The Semantics and Politics of Social Kinds</title> she
				distinguishes different forms of analysis that yield different facets of the
				meaning of the same word <ptr target="#haslanger2012" loc="365–380"/>). A conceptual analysis
				yields the manifest meaning, the way we (or for our purposes, perhaps not us,
				but some historical population) explicitly understand the meaning of a word. A
				descriptive analysis would yield the use of the word which might in fact diverge
				from the way we think we use it. Haslanger gives the example of her son’s
				primary school which makes use of a concept of tardiness. Officially, any child
				that arrives after 8:25 was tardy but as Haslanger's son pointed out: <quote rend="inline">Don’t worry Mom, no one is ever tardy on Wednesdays because
					my teacher doesn’t turn in the attendance sheet on Wednesday until after
					the first period</quote> <ptr target="#haslanger2012" loc="268"/>. Here the use of 'tardy' diverges from
				the official, explicit definition. Its manifest meaning, which you would have
				learned by asking the teachers or school staff what ‘tardy’ is, was not the
				operative meaning, which you learned from studying the tracking mechanism.
				Beyond this rather local example, Haslanger is thinking about concepts of race
				and gender, 'man', 'woman', 'white', 'black', etc. These concepts might
				superficially, or manifestly, be thought to refer to biological categories. For
				this reason we might argue that the whole concept of race is misguided since the
				underlying biological category either does not exist or is not nearly as
				pronounced as the word implies. However, there is also a de facto use of our
				race categories that deserves study – because even though the manifest meaning
				might turn out to be bogus, this does not mean these sorts of word are not still
				doing relevant and investigable semantic work, to be found by looking at the
				operative meaning <ptr target="#haslanger2012" loc="221–247"/>.</p>
				<p>In his book <title rend="italic">Wandering Significance</title> <ptr target="#wilson2006"/> Mark
				Wilson uses the concept of a façade to denote concepts that superficially seem
				univocal but on closer inspection have subtly divergent uses in different
				contexts <ptr target="#wilson2006" loc="147"/>. A simple example is that of a rainbow <ptr target="#wilson2006" loc="21"/>. In 
				some contexts it makes sense to say a pot of gold is at the end of it,
				presumably to be reached by riding a unicorn over it. In other, more literal
				contexts, rainbows have no clear ends, and cannot be ridden. </p>
			<p>A better, but more complicated example is that of hardness. Most of us have some
				notion what it is for a material to be hard, but upon inspection of the
				technical details, the concept reveals itself as something much more
				heterogeneous than it at first appears. Wilson notes that a range of different
				tests and metrics exist depending on what type of material specialists speak of
				and for what purpose. Some of these tests identify hardness with resistance to
				scratching or cutting and some of these metrics identify it with flow stress or
				a resistance to penetration. Wilson gives examples: <quote rend="inline">In these
					contexts [indenter tests for common metals], yield strength becomes the
					attribute to which “hardness” locally gravitates (although such
					identification is completely unnatural for glass)</quote> <ptr target="#wilson2006" loc="341"/>. As
				Wilson notes, not all of these tests even make sense for each type of material
				and for each purpose. Hence hardness relates to a range of local patches of
				meaning that are tied together by the vague general notion of hardness but are
				each individually quite different. The use of the concept admits of a lot of
				subtleties that are not apparent at face value, often because they only appear
				in exotic contexts or because different specialized contexts are usually kept
				separate. This means that we should expect the use of the concept to be more
				divergent than its most salient description. Another example is given by Thomas
				Kuhn <ptr target="#kuhn2012" loc="188"/> who argues that the well-known formula <emph>F=ma</emph> is applied
				quite differently in different branches of physics. </p>
			<p>We agree with these authors on the following points: first that the use of a word
				is not always the same as the explicitly given definition or the first
				description of it that comes to mind. Secondly that the use of a word often has
				a lot more subtlety and divergence to it than we commonly give credit for.
				Thirdly that both the subtle uses of the word and its more salient, obvious
				meaning are of interest. We will use the notion of salience to distinguish
				greater and smaller degrees to which the meaning of a term will be public and
				obvious to its users (authors and readers). The idea is that the more extreme
				coordinates of our vectors will show us the more salient aspects of a word's
				meaning, whereas the smaller coordinates and the differences between them are
				still relevant to the way a word is broadly used, but not in a way that readily
				comes to mind even to those who use the word regularly. Unlike different types
				of meaning or analysis, salience will not be a binary division with salient and
				non-salient features but will admit of degrees. This suits our purposes because
				in the digital methods employed we will distinguish the salient from the not so
				salient by a continuous variable, occasionally using an unavoidably arbitrary
				cut-off point of a value above which word-relations will be considered salient.
				Hence, 'salient' will be used much the same as 'large', supervening on an
				underlying property that varies continuously (size) and being only
				understandable in relative terms (large compared to a human is different from
				large compared to the sun).</p></div>
			<div>
			<head>Section 3: Algorithm implementation and details</head>
			<p>Our two methods for the extraction of semantic information about word-types from a
				corpus are the following: (1) extraction of lists of collocates and (2)
				generation of a vector-representation. We will first introduce the generation of
				a vector-representation of a word in a given text and then argue that extracting
				collocates is a specific restriction on this vector-representation that allows
				it to bring to the fore the more salient features of a word-type. Then the
				different ways of measuring similarity between word-types will be introduced and
				the particular application of our proposed algorithms for conducting our
				case-study. Two words are of the same word-type if they are spelled identically.
				We might consider grammatical cases of the same stem as identical as well, in
				which case the unit under discussion is a lemma rather than a word-type.
				Word-types are used throughout as the objects of study, however, for the
				case-study in Section. 5 we’ve lemmatized the texts and are thus looking at
				lemmas. However, the methods proposed are equally applicable to both types of
				object.</p>
				<div>
			<head>Vector semantics</head>
			<p>In accordance with Firth’s and Harris’ intuitions, the algorithm works by
				considering the contexts of word-types (by looking at what word-types surround
				them) and saving that data in a proper data-structure – meaning that word-types
				will be represented <emph>as</emph> the other words that they co-occur
				with and the counts of these co-occurrences. First we must get the base
				information we need from the texts. This is done by counting for every word-type
				which other words co-occur with them within some window of size n. Here n
				signifies the number of words that we look at on both sides of all the
				occurrences of the word-type investigated. This means that if we for example
				choose a window of size 4, and consider the following sentence: </p>
			<cit><quote rend="block">The movement towards digital hermeneutics is fraught with difficulties, but
				movement is never without difficulties.</quote> </cit>
			<p>We can derive a representation of this sentence in terms of relative closeness of
				word-types. There are 12 different word-types in the text. For each word-type we
				can ask how often each word-type occurs (given a particular windowsize). In
				table 1 below you can see what values this would deliver (the first column shows
				how many occurrences of the word-type (WT) occur in total):</p>
			<table>
				<head>Example of construction of a table of all co-occurrence vectors</head>
				<row role="label">
					<cell>#</cell>
					<cell>WT</cell>
					<cell>The</cell>
					<cell>movement</cell>
					<cell>towards</cell>
					<cell>Digital</cell>
					<cell>hermeneutics</cell>
					<cell>is</cell>
					<cell>fraught</cell>
					<cell>with</cell>
					<cell>difficulties</cell>
					<cell>but</cell>
					<cell>never</cell>
					<cell>without</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>The</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>2</cell>
					<cell>Movement</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Towards</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Digital</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Hermeneutics</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>2</cell>
					<cell>Is</cell>
					<cell>0</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>3</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Fraught</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>With</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
				</row>
				<row role="data">
					<cell>2</cell>
					<cell>Difficulties</cell>
					<cell>0</cell>
					<cell>2</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>3</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>2</cell>
					<cell>1</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>But</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Never</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>2</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
				</row>
				<row role="data">
					<cell>1</cell>
					<cell>Without</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>0</cell>
					<cell>0</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
					<cell>1</cell>
				</row>
			</table> 
			
			<p>This is a vector-representation of the contextual information in the text. Each of
				the rows is a vector (i.e. there is a vector for each word-type) in a
				12-dimensional space. Every individual number in the table represents how often
				the column word co-occurs with the row word. Important to note is that the
				windowsize directly impacts the nature of the results of the
				vector-representation of the text – in our experiment in section 5, we use a
				windowsize of 10. There is a strong case to be made that taking different values
				for the windowsize can in many ways be used to extract not better or lesser
				results, but properly different results from the text.<note> For example, the
					problem with a very small windowsize is that it starts picking up on
					syntactic relations moreso than semantic relations (verbs will generally be
					scored lower with one another due to the syntactic restrictions on
					following up verbs with verbs for example) – meaning windowsize is not just
					an optimizable parameter, but one that can be tweaked to pick up on
					different features that may prove to be of interest. We have followed a
					windowsize that (very roughly) is small enough to approximate a ‘sentence
					length’ window, while being large enough to smooth out some of the
					syntactical connections – but the choice for 10 and not, say, 15, is
					somewhat arbitrarily made here. Other variations are also available, see
					for example <ptr target="#debolla2019" loc="375–378"/></note>
			</p>
			<p> However, these raw counts are, on their own, not particularly informative. They
				need to be transformed so that we can extract values that indicate how strongly
				two word-types are connected that is not directly influenced by the total
				occurrences of certain word-types. We see in the above table for example that
				‘movement’, ‘is’, and ‘difficulties’ all have higher total co-occurrence counts
				than the other words – a direct consequence of their own higher frequency.
				However, it is not so that by occurring more often, a word-type is necessarily
				more connected to more types of words in interesting or salient ways. This means
				we need to transform these raw counts-scores into a score that is unaffected by
				total frequency of word-types. For this we have used and will now introduce the
				measure <term>pointwise mutual information.</term>
			</p></div>
				<div>
			<head>Measuring: PMI</head>
			<p>Each of the pairs of word-types needs to be scored – or, their connectedness needs
				to be measured – based on the data from Table 1. The way to do this is by
				extracting the data about the number of times they have co-occurred, together
				with how many instances of the word-type there were. The method we will be using
				to do this is <term>Pointwise Mutual Information</term> (PMI).<note>
					Introduced for applications in linguistic analysis by Church and Hanks in
						<title rend="italic">Word Association Norms, Mutual Information, and
						Lexicography</title> (<ref target="#church1989">1989</ref>). Used and discussed further in for example
					<ptr target="#bouma2009"/>.</note> PMI measures the chance of finding a word-type <emph>y</emph> within
				a window around another word-type <emph>x</emph> (within a chosen windowsize) in the
				investigated text, divided by the chance of finding y in the whole text. The
				idea is that related words should be found disproportionately in each other’s
					windows<note> This is analogous to methods of finding out how two subsets
					of a probability space are correlated. There the normal procedure is to
					divide the chance of being in both by the chance of being in both under the
					hypothesis that they are independent: <emph>P(x, y)/P(x)P(y)</emph>. If the result is 1,
					they are independent, if higher, the two sets are positively correlated
					with higher numbers indicating stronger correlations, and if lower, they
					are anticorrelated, with lower numbers indicating stronger anticorrelation.
					Here we use the same idea albeit slightly complicated by the fact that we
					are working with windows around tokens of <emph>x</emph>, and that instances of y may be
						counted doubly. In most formulations of PMI <ptr target="#church1989"/>, the
					notation mimics more closely finding correlations between subsets of a
					probability space – computationally, we do the same thing as these
					authors.</note>. The overall chance of finding <emph>y</emph> indicates a regular
				proportion. We take the log of this value in order to make the results better
				suited for vector calculus later. We will write small <emph>x</emph> for the set of instances
				of x itself and capital <emph>X</emph> for the windows around instances of <emph>x</emph>. Notice that the
				different windows <emph>X</emph> might overlap. In this case the same word-token will be
				counted twice or more. Hence <emph>X</emph> should be seen as the disjoint or indexed union
				of the windows. The formula for PMI reads:</p>
			<p>
				<mml:math>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">P</mi>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">M</mi>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">I</mi>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
						<mrow>
							<mi>x</mi>
							<mo>,</mo>
							<mi> </mi>
							<mi>y</mi>
						</mrow>
					</mfenced>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
					<msub xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mi>l</mi>
							<mi>o</mi>
							<mi>g</mi>
						</mrow>
						<mrow>
							<mn>2</mn>
						</mrow>
					</msub>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mi>P</mi>
							<mfenced separators="|">
								<mrow>
									<mi>y</mi>
									<mo>∈</mo>
									<mi>X</mi>
								</mrow>
							</mfenced>
						</mrow>
						<mrow>
							<mi> </mi>
							<mi>P</mi>
							<mo>(</mo>
							<mi>y</mi>
							<mo>)</mo>
						</mrow>
					</mfrac>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
				</mml:math>
			</p>
			<p>This provides us with the average of the chance of finding a token of <emph>y</emph> in a
				window near a token of <emph>x</emph>, divided by the overall chance of finding <emph>y</emph> in the
				text. The fraction can only be a positive value (both denominator and numerator
				are positive), but a log of a value between 0 and 1 gives a negative value. If
				the two word-types are fully independent from each other, we’d see that there is
				no difference in the chance of finding <emph>x</emph> and <emph>y</emph> in window-proximity to each other
				in our actual text, versus the hypothetical situation where they have no actual
				relation – i.e. the fraction will then give as a result 1. <emph>Log(1) = 0</emph>, so PMI
				scores above 0 should be read as indicating a positive correlation between the
				two word-types in the text, whereas scores below 0 indicate negative
				correlation, and 0 itself indicates independence.</p>
			<p>Computationally the chance <emph>P(y)</emph> is simply the instances of <emph>y</emph> divided by the total
				word count. The chance P<mml:math>
					<mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
						<mrow>
							<mi>y</mi>
							<mo>∈</mo>
							<mi>X</mi>
						</mrow>
					</mfenced>
				</mml:math>is found by counting each instance of <emph>y</emph> within the window of an
				instance of <emph>x</emph>, where the same <emph>y</emph> may be counted more than once when it appears in
				more than one window, divided by the size of the disjoint union <emph>X</emph> of all the
				windows, which will always be the window size multiplied by the amount of times
				<emph>x</emph> appears in the text.</p>
			<p>
				<mml:math>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mi>p</mi>
							<mfenced separators="|">
								<mrow>
									<mi>y</mi>
									<mo>∈</mo>
									<mi>X</mi>
								</mrow>
							</mfenced>
						</mrow>
						<mrow>
							<mi>P</mi>
							<mfenced separators="|">
								<mrow>
									<mi>y</mi>
								</mrow>
							</mfenced>
						</mrow>
					</mfrac>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mfenced separators="|">
								<mrow>
									<mfrac>
										<mrow>
										<mo>|</mo>
										<mi>y</mi>
										<mi> </mi>
										<mi>f</mi>
										<mi>o</mi>
										<mi>u</mi>
										<mi>n</mi>
										<mi>d</mi>
										<mi> </mi>
										<mi>i</mi>
										<mi>n</mi>
										<mi> </mi>
										<mi>w</mi>
										<mi>i</mi>
										<mi>n</mi>
										<mi>d</mi>
										<mi>o</mi>
										<mi>w</mi>
										<mi>s</mi>
										<mi> </mi>
										<mi>a</mi>
										<mi>r</mi>
										<mi>o</mi>
										<mi>u</mi>
										<mi>n</mi>
										<mi>d</mi>
										<mi> </mi>
										<mi>x</mi>
										<mo>|</mo>
										</mrow>
										<mrow>
										<mo>|</mo>
										<mi>x</mi>
										<mo>|</mo>
										<mi>*</mi>
										<mi>w</mi>
										<mi>i</mi>
										<mi>n</mi>
										<mi>d</mi>
										<mi>o</mi>
										<mi>w</mi>
										<mi> </mi>
										<mi>s</mi>
										<mi>i</mi>
										<mi>z</mi>
										<mi>e</mi>
										</mrow>
									</mfrac>
								</mrow>
							</mfenced>
						</mrow>
						<mrow>
							<mfenced separators="|">
								<mrow>
									<mfrac>
										<mrow>
										<mo>|</mo>
										<mi>y</mi>
										<mo>|</mo>
										</mrow>
										<mrow>
										<mi>T</mi>
										<mi>o</mi>
										<mi>t</mi>
										<mi>a</mi>
										<mi>l</mi>
										<mi> </mi>
										<mi>w</mi>
										<mi>o</mi>
										<mi>r</mi>
										<mi>d</mi>
										<mi>s</mi>
										</mrow>
									</mfrac>
								</mrow>
							</mfenced>
						</mrow>
					</mfrac>
				</mml:math>
			</p>
			<p>So for example, when we consider table 1 above, we can see that the score for
				PMI(is, with) with a window size of four is given by the fact that we find
				‘with’ around ‘is’ twice, that ‘is’ occurs twice itself, that ‘with’ occurs once
				and that the entire text consists of 15 words, giving us: </p>
			<p>
				<mml:math>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">l</mi>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">o</mi>
					<mi xmlns="http://www.w3.org/1998/Math/MathML">g</mi>
					<mn xmlns="http://www.w3.org/1998/Math/MathML">2</mn>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mfrac>
								<mrow>
									<mn>2</mn>
								</mrow>
								<mrow>
									<mn>2</mn>
									<mi>*</mi>
									<mn>4</mn>
								</mrow>
							</mfrac>
						</mrow>
						<mrow>
							<mfrac>
								<mrow>
									<mn>1</mn>
								</mrow>
								<mrow>
									<mn>15</mn>
								</mrow>
							</mfrac>
						</mrow>
					</mfrac>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
					<mfenced xmlns="http://www.w3.org/1998/Math/MathML" separators="|">
						<mrow>
							<mfrac>
								<mrow>
									<mn>0.25</mn>
								</mrow>
								<mrow>
									<mn>0.0667</mn>
								</mrow>
							</mfrac>
						</mrow>
					</mfenced>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
					<mn xmlns="http://www.w3.org/1998/Math/MathML">3.75</mn>
				</mml:math>
			</p>
			<p>When the corpus is only a single sentence, this method does not yet pick up on any
				significant semantic properties – but as we will see later on, when the number
				of sentences and words, grow, this problem diminishes. </p></div>
				<div>
			<head>Divergence: extracting collocations and the salient</head>
			<p>From this vector-representation of a word with scores based on PMI, there are (at
				least) two routes available for the extraction of different
				meaning-representations. We argue that one discovers more salient aspects of
				meaning, whereas the other discovers the more subtle/less salient aspects of
				meaning. </p>
			<p> Many of the scores inside the vector representation are low. The connectedness of
				the terms is there, but not particularly salient. Specifically within the
				approach that is called <term>the extraction of collocations</term>,
				this intuition is put to good use. Collocates to a specific word are those words
				that are <term>particularly connected</term> to the word that is being
				investigated. Instead of looking at 100.000 word-types and their connectedness
				to a word of interest, in these cases, one restricts the list to those
				word-types that score at least a certain score on the PMI measure. This often
				leaves investigators with a handful, up to a ~100 word-types, which together
				provide a good overview of the interesting semantic properties of the word-type
				under investigation.<note> For examples and applications of this sort of
					analysis, see <ptr target="#gavin2019"/>, <ptr target="#brezina2015"/>, <ptr target="#davies2012"/> and,
					for a particularly influential application, see <ptr target="#debolla2013"/></note> In
				our own algorithm, we have taken the threshold to be a PMI-score of 4, so as to
				mimic these applications, which often aim to consider around a few dozen of
				word-types for an investigated word-type.<note> The choice for the minimal
					threshold for the PMI value might appear arbitrary, while also being
					significantly influential in the differences between the output of the two
					methods we propose to give an interpretation. However, as we argued in the
					introduction, although there is here a paradox of the heap, at least an
					interpretation is available for the quantity (we propose salience) that is
					being cut-off. </note>
			</p>
			<p> We argue that what investigators making use of lists of collocates are extracting are
				those connections between word-types that are (within the investigated corpus)
				particularly salient, and by extension, more public and explicit. This works
				well for their purposes. The list of high scoring words will mostly include
				words that, to a reader of, or author in, the corpus, will likely ‘make
				immediate sense’ when provided. Of course, the nature of the connection won’t be
				provided, but that they are connected will be clear on most occasions to most
				people who read the corpus – and looking at this limited set of words can
				provide an investigator with a lot of useful information on the corpus. </p>
			<p> Contrast this with the vector representation. Here the meaning of a word-type is
				not given by a list of <emph>x</emph> particularly saliently connected words, but by the PMI
				score with every single other word-type contained in the corpus. In addition,
				instead of taking an implicit ‘one-hot’ representation as in the collocation
				extraction (another word-type either is, or is not, connected enough) the vector
				representation typically takes the exact outcome of the PMI calculation as its
				score. This means that even minute differences will play a role in the
				differentiation between different word-types. What we are picking up on here, we
				argue, are the more hidden, less explicit, connections a word-type makes within
				the corpus. It investigates how the particular ways of using a term influence
				further usage and application of a term, despite it being particularly hard for
				a speaker or reader to explicate that such a causally productive connection
				exists.</p>
			<p>We have suggested that PMI scores track relevant semantic properties of a word. In
				addition, that high scores track more salient features, and low scores more
				implicit and subtle features. What argument or evidence is there for this?
				Perhaps the use of a word is part of its meaning but surely what other words it
				appears near in written text is at best an approximation of how the word is
				used. </p>
			<p>Psychological research shows that PMI is positively correlated with judgment of
				similarity, that is to say if words have higher PMI scores in sample texts then
				test-subjects are more likely to judge them similar <ptr target="#mcdonald2000" loc="35–67"/>.
				In addition, texts with high PMI scores for given word pairs have been shown to 
				make people see the words as more related in meaning <ptr target="#mcdonald2001"/>. In addition, as McDonald points out, such scores can be correlated with
				other psychological variables such as effect in lexical priming <ptr target="#lowe2000"/>, analogous reasoning <ptr target="#ramscar2000"/> and synonym selection <ptr target="#landauer1997"/>. </p>
			<p>This shows that whatever properties are tracked by PMI are not mere artifacts of
				the texts studied but have at least some correlation with psychological
				indicators of word meaning. What does not follow from this work is that lower
				PMI scores indicate less salient aspects of the use of a word. That would, by
				its very nature, be hard to extract from straightforward questions to test
				subjects, though it might be studied in other ways. In section 5 we will include
				results from an explorative case study that show that in at least some cases,
				looking at just the high scoring words yields different results from looking at
				all word associations.</p></div>
				<div>
			<head>Similarity measures</head>
			<p>The above methods describe how we propose to be able to extract an approximation
				of less or more salient aspects of meaning. In the first case, the vector is the
				representation of the less salient aspects of the concept of a word-type in a
				text/corpus, in the latter case, the list of connected terms is the
				representation of the more salient aspects of the concept of a word-type in a
				text/corpus. In the case of the extracted list of collocates, research sometimes
				focusses on the qualitative investigation of this list of words itself (indeed,
				the ability to do so is one of the attractive features of the entire
					approach).<note> Especially for historical analysis of texts, these methods
					are more useful than the often more complicated, fully quantitative and
					hard to interpret tools of natural language processing.</note> The vector
				approach generally does not allow for this and is often coupled together with a
				measure of the similarity between vectors. This allows the researcher, for
				example, to check the stability of terms over time, find synonyms, and discover
				dissimilarity between terms which are expected to hang together.<note> The
					suggestion for the application of these methods can already be found in
					<ptr target="#harris1964" loc="158–161"/></note> We introduce a commonly applied 
				similarity measure for vector-representations and argue that there are useful
				options for measuring the similarity of lists of collocates. These measures will
				be used in the case study to extract useful information about our corpus for
				both sorts of meaning-representations.</p>
			<p> In the case of the vector representation, what we are looking for is actually a
				measure for <emph>closeness</emph> of coordinates in a
				multi-dimensional space. A number of different options is available, but one
				of the more often used measures is <term>cosine similarity</term> <ptr target="#han2012" loc="sec. 2.4.7"/>. Intuitively, how similar two
				vectors are is measured by looking at how much they coincide for each of their
				coordinates. A simple measure for this is provided in linear algebra by the
				inner product, or, dot-product. This measure sums the products of each pair of
				coordinates that lie in the same dimension, or:</p>
			<p>
				<mml:math>
					<mover xmlns="http://www.w3.org/1998/Math/MathML" accent="true">
						<mrow>
							<mi>v</mi>
						</mrow>
						<mo>⃗</mo>
					</mover>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">∙</mo>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<mover xmlns="http://www.w3.org/1998/Math/MathML" accent="true">
						<mrow>
							<mi>w</mi>
						</mrow>
						<mo>⃗</mo>
					</mover>
					<mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
					<msubsup xmlns="http://www.w3.org/1998/Math/MathML">
						<mo>∑</mo>
						<mrow>
							<mi>i</mi>
							<mo>=</mo>
							<mn>1</mn>
						</mrow>
						<mrow>
							<mi>N</mi>
						</mrow>
					</msubsup>
					<mrow xmlns="http://www.w3.org/1998/Math/MathML">
						<msub>
							<mrow>
								<mi>v</mi>
							</mrow>
							<mrow>
								<mn>1</mn>
							</mrow>
						</msub>
						<mi>*</mi>
						<mi> </mi>
						<msub>
							<mrow>
								<mi>w</mi>
							</mrow>
							<mrow>
								<mn>1</mn>
							</mrow>
						</msub>
						<mo>+</mo>
						<mi> </mi>
						<msub>
							<mrow>
								<mi>v</mi>
							</mrow>
							<mrow>
								<mn>2</mn>
							</mrow>
						</msub>
						<mi>*</mi>
						<mi> </mi>
						<msub>
							<mrow>
								<mi>w</mi>
							</mrow>
							<mrow>
								<mn>2</mn>
							</mrow>
						</msub>
						<mo>+</mo>
						<mi> </mi>
						<mo>.</mo>
						<mo>.</mo>
						<mi> </mi>
						<mo>+</mo>
						<mi> </mi>
						<msub>
							<mrow>
								<mi>v</mi>
							</mrow>
							<mrow>
								<mi>N</mi>
							</mrow>
						</msub>
						<mi>*</mi>
						<mi> </mi>
						<msub>
							<mrow>
								<mi>w</mi>
							</mrow>
							<mrow>
								<mi>N</mi>
							</mrow>
						</msub>
					</mrow>
				</mml:math>
			</p>
			<p>Partly, this measure does what we want; the more (high) values shared between two
				vectors in similar dimensions, the higher the similarity of the two vectors will
				be. However, what is counter-intuitive is that longer vectors (vectors with
				higher values in many dimensions) will <emph>generally</emph> be
				scored higher than shorter vectors will be. However, it is not the case that
				just because a word-type has more strong connections with other word-types
				(higher PMI-scores) it should be deemed to be expected to be more similar to
				other word-types in general. This problem can be amended simply by normalizing
				the lengths of the two vectors; in this way, we only measure the similarity in
				distribution of values across the different dimensions of the two vectors, while
				negating the influence of their absolute lengths. We get for our similarity
				measure:</p>
			<p>
				<mml:math>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mover accent="true">
								<mrow>
									<mi>v</mi>
								</mrow>
								<mo>⃗</mo>
							</mover>
							<mi> </mi>
							<mo>∙</mo>
							<mi> </mi>
							<mover accent="true">
								<mrow>
									<mi>w</mi>
								</mrow>
								<mo>⃗</mo>
							</mover>
							<mi> </mi>
						</mrow>
						<mrow>
							<mfenced open="|" close="|" separators="|">
								<mrow>
									<mover accent="true">
										<mrow>
										<mi>v</mi>
										</mrow>
										<mo>⃗</mo>
									</mover>
								</mrow>
							</mfenced>
							<mi>*</mi>
							<mo>|</mo>
							<mover accent="true">
								<mrow>
									<mi>w</mi>
								</mrow>
								<mo>⃗</mo>
							</mover>
							<mo>|</mo>
						</mrow>
					</mfrac>
				</mml:math>
			</p>
			<p>Linear algebra shows that this formula captures the same thing as taking the
				cosine of the angle between the two vectors <emph>v</emph> and <emph>w</emph>, hence the measure’s name <term>cosine
					similarity</term>. Higher values are attained when the distribution of values
				between two vectors amongst all the possible dimensions is similar – or, when
				the angle between the two vectors is close to zero (in addition, for orthogonal
				vectors, we get a similarity of 0, and for opposite vectors, a value of -1).
				This allows our measure to provide values between -1 and 1.</p>
			<p> In the case of the lists of terms, how to model similarity is less researched
				because lists of terms do not easily translate into a question about vectors in
				space. Intuitively, however, what we want to measure is the amount of overlap
				two terms exhibit in their lists of collocates. The greater the overlap, the
				larger the amount of saliently connected terms two words share. What is
				dangerous however is that we do not want that word-types that have a larger
				number of strongly connected terms be more similar in general to other words
				(which would happen if we would take ‘a large overlap’ to be signified by a
				large number of overlapping terms). This needs to be normalized. One of the
				measures used for these purposes is the Jaccard index, which scores the overlap
				between the neighborhood of two nodes in a network in the following way:</p>
			<p>
				<mml:math>
					<mfrac xmlns="http://www.w3.org/1998/Math/MathML">
						<mrow>
							<mo>|</mo>
							<mi>A</mi>
							<mo>∩</mo>
							<mi>B</mi>
							<mo>|</mo>
						</mrow>
						<mrow>
							<mo>|</mo>
							<mi>A</mi>
							<mo>∪</mo>
							<mi>B</mi>
							<mo>|</mo>
						</mrow>
					</mfrac>
					<mi xmlns="http://www.w3.org/1998/Math/MathML"> </mi>
				</mml:math>
			</p>
			<p>The intersection provides the overlap and the union provides the sense of the
				‘total’ size of the neighborhoods; sharing a large amount of terms provides a
				higher score the smaller the total sample size of potentially shared terms
				becomes.</p>
			<p>This measure is straightforward in the special case that the neighborhoods have
				the same cardinality. The sets {E, F, G, H} and {G, H, I, J} will have a Jaccard
				index of 1/3; the intersection contains two elements (G and H), whereas the
				union contains six elements (E, F, G, H, I, J). In the case of the same
				cardinality, we also have the nice property that complete overlap will provide a
				score of 1. This breaks down in the case of dissimilar cardinalities. Take the
				Jaccard Index over {E, F} and {E, F, G, H}. This will turn out to be a half (two
				shared elements and four total elements). This means that the scores will be
				influenced by the lengths of the lists of collocates. However, this on its own
				is not a bad thing, as the length of the list tells us something about how much
				salient connections there are for the word within the corpus. If two terms
				significantly differ in this respect, it is reasonable to take this along in the
				calculations. What we do want to avoid (and what the Jaccard Index avoids) is
				that words with larger or smaller lists of terms get scored more or less highly
				on their similarity scores in general. This is not the case, since the length of
				the list only comes into play relatively to the length of the
					list of another word-type with which the similarity is measured.
				Word-types with the same, or a similar, neighborhood size will have a
				higher potential score, but this does not translate into a preference for either
				larger or smaller neighborhoods to influence the scoring on its own. In
				addition, the Jaccard index is commutative. It will always tell us that a first
				word is as similar to a second word as the second to the first. This maintains
				the intuitive symmetry of the similarity relation.</p></div>
				<div>
			<head>Intermezzo: a comparison with word embeddings</head>
			<p>As a brief intermezzo we will discuss why our mode of interpretation, which
				focuses on the salience of individual scores in the PMI-based semantic vector,
				would be much harder for word embedding based methods like word2vec <ptr target="#mikolov2013"/> and BERT <ptr target="#devlin2018"/>. Such methods have been used in many
				semantic and linguistic contexts, including recently the tracing of semantic
				change in historical corpora <ptr target="#hamilton2016"/>, <ptr target="#wevers2020"/>. In fact, for many tasks like tracking synonymy and homonymy
				word-embeddings are superior to the sparse vector approaches that have been
				interpreted above.</p>
			<p>Word embeddings can make use of different architectures, but let us look at the
				continuous bag-of-words model. Making use of a neural net, an algorithm attempts
				to invent word vectors that are maximally useful in the task of predicting, on
				the basis of a collection of words surrounding a word’s position (the
				bag-of-words), what word will occur in the middle of these words. By altering
				the values (weights) of a word’s entries into its vector so as to be maximally
				effective at this task, a vector representation of the word is learned. As the
				algorithm learns, it doesn’t find the eventual weights, or scores, of words
				along a number of axes, it defines its own, maximally informative, axes. That is
				to say, in using a method like word2vec, not only are the scores that should
				define a word-vector extracted, but also the best conceptual frame of
				representing the word-vectors is constructed. However, due to the difficulty in
				interpreting what a particular axis stands for, it also becomes difficult to
				interpret individual scores along this axis as is done above (entries being
				interpreted for example, as more or less salient connection between word-types). </p>
			<p>Consider the vector describing word type ‘The’ in table 1. This vector has 12
				values, each representing the number of times the word ‘the’ co-occurred with
				the 12 other word types. Each of the vectors can be retraced to interpretable
				quantities through its elements which are interpretable quantities. By contrast
				a comparable word2vec vector would contain entries that represent variables
				invented by the algorithm. This does not allow for the word-type-pair-based
				analysis we propose. </p>
			<p> To put it a bit more technically, word embeddings use various techniques to
				represent semantic properties in lowdimensional spaces. This reduction in the
				amount of dimensions has computational advantages, but the downside from an
				interpretative point of view is that it is less clear what the elements of
				vectors represent (in any case, there is no reasons to expect them to represent
				word-type-pair relations). Our methods also allow us to define a vector-space
				and norms on it (see the section on similarity measures) which has far more
				dimensions, but the directions, vectors, and individual entries can be much more
				readily interpreted because this space was invented by and for people. </p>
			<p>It might be possible, instead of trying to interpret the individual steps of the
				algorithm produced by a teaching algorithm, to instead interpret what word
				embedding methods are good at, or what using certain training data teaches
				them<note> As an example see <ptr target="#underwood2019"/></note>, but that is beyond
				the scope of this paper.</p></div>
				<div>
			<head>Stability: Introducing salience differentiated stability scores</head>
			<p>Having discussed two ways to model words, vector-representations and
				collocate-representation, which respectively model the less and the more salient
				aspects of its meaning, we now move to the concept of the stability of a
				word-type within a corpus of individual texts, which is the final ingredient
				required for our proposed application of combining differently measuring the
				more and less salient aspects of meaning. </p>
			<p>For each of the works in a given corpus we can generate both vector and
				collocation representations of a set of different word-types that are relevant
				to the corpus. Using the similarity measures we have discussed above, we are
				able to compare each of these models to one another. In particular, we compare
				the models of the same word in different works. For instance, the usage of the
				term ‘cause’ in one work of the corpus can then be compared on the level of
				similarity (for both facets) with the usage of ‘cause’ in another work. For our
				two methods we then get a relation of similarity for each word-type between
				every work in the corpus. These results can be used for building up a network
				representation of the corpus. Consider for example Figure 1 below, which
				provides the resulting graph, based on the network of all similarity scores
				between all works in the corpus of early-modern English physics/natural
				philosophy textbooks that will be further detailed in section 5, for the
				vector-representation based method, indexed on the word ‘body’:</p>  
					
					<figure>
						<head>Resulting network representation from comparing each work in the corpus
							to every other using similarity scores based on their models of ‘body’ that
							incorporate the less salient aspects of its meaning.</head>
						<graphic url="resources/images/figure01.png"/>
						<figDesc>Image of a network graph with several hundred edges connecting various nodes which are identified with red text</figDesc>
					</figure>
					
			
			<p>The nodes stand for the different works in the corpus, while the weighted edges
				that connect them stand for the similarity between the two works in the
				generated models of the investigated word-type. Of course, one can analyze such
				a (set of) networks (one for each word-type) in many different ways to allow for
				interesting corpus analysis.<note> <ptr target="#brezina2015"/> have provided a good
					framework for combining vector semantics and network analysis in more
					involved ways than those that we have applied here.</note> We will only
				make use of a particular route of investigation – we will look at the average
				weight of the edges for each of these different networks (i.e. word-types) for
				both of the models available. On its own, this average of the edge weight
				intuitively gives a number that indicates how similar the word is used across
				the corpus. A high score indicates that many of the works have particularly
				similar models for their particular meaning of the investigated word-type. A low
				score by contrast tells us that the word is not used particularly similar across
				the corpus, and that the meaning of the term is particularly unstable. We’ll
				label these scores ‘<term>stability scores</term>’. Higher values
				signal that a term under investigation is used more similarly across the entire
				corpus, i.e., the term’s usage is more stable in that corpus than one that
				scores lower. </p>
			<p> Our important claim regarding these numbers comes from the divergence between
				stability scores obtained by the same word-type, depending on whether we are
				considering their more or less salient aspects of meaning. Indeed, whereas one
				might expect to find two kinds of terms – terms that have high stability scores
				for both the more and less salient aspects of their meaning, and that score low
				on both – we find divergence between these scores. It is these divergencies of
				stability scores when one differentiates between more and less salient aspects
				of meaning that must be interpreted and that will be provided as our case-study.
				To investigate such scores we call <term>salience differentiated
					stability analysis</term>.</p></div></div>
				<div>
			<head>Section 4: A framework for interpreting SDS-scores</head>
			<p>We have used two methods to distinguish similarity between word usage. One of
				them, we argued, measured the most salient associations with a word, whereas the
				other measured it’s overall usage, including the most subtle differences in
				emphasis. We then used the Jaccard index and cosine similarity to gain an
				overall measure of how much a word is used similarly across texts, according to
				our two methods. It turns out that the two scores are not always correlated.
				Some words score highly on their average jaccard score but low on their average
				cosine similarity and vice versa. If this were to happen often, and in ways we
				could predict this would in itself be evidence that they measure different
				things. </p>
			<p>For doing salience differentiated stability analysis, we can divide words into
				four categories, or even better, a two-dimensional grid where each word is
				closer to one corner or another. A word will either have a lot of salient
				difference in usage or not across a corpus, and it will have a lot of less
				salient differences in usage or not. Something interesting is to be said of each
				of these four categories.</p>
			<list rend="numbered">
				<item>The simplest are words which score high on both scores. These are words
					which are similarly understood by most authors and which are used in much
					the same way. We should expect them to have a straightforward and broadly
					understood meaning. Given that the terms are central to the corpus (to be
					ascertained via domain knowledge, or other methods, like topic modelling)
					one can expect the terms to be discussed, but not conceptually
					controversial. </item>
				<item>Words which score lowly on both scores are also quite straightforward.
					These are words which are understood differently and used differently. We
					should expect them to be at the center of controversies which are difficult
					to resolve, or to be outright polysemic. </item>
				<item>More interesting are words which are similar in salient ways but
					dissimilar in more subtle ways. Here superficial similarity may mask
					slippery differences in usage. We will call such words façades. If we
					follow Haslanger we should expect these words to be used in masking certain
					hypocrisies and if we follow Wilson these words may suggest a
					straightforward usage when they are in fact used in a façade-like manner,
					being used subtly differently in different areas of application.
					Alternatively it could indicate a controversy in a narrow domain, where the
					broad subject of discussion is fixed but the details are the subject of
					controversy.</item>
				<item>Lastly we have words which have a lot of salient dissimilarity but which
					are overall used relatively similarly. We should expect these words to be
					the subject of controversy whilst their actual use is relatively
					fixed.</item>
			</list>
				</div>
				<div>
			<head>Section 5: Explorative case-study of early modern natural philosophical
				terminology</head>
			<p>Now we turn to the case study. We reiterate: the purpose of this case study is not
				primarily to study the history of natural philosophy/physics. It is rather to
				provide a proof of concept of salience differentiated stability. If the case
				study were to bear out that all words are similarly stable, or that Jaccard
				scores do not yield different results from cosine similarities, our methods
				would be trivial. If some of the results were not plausibly interpretable, the
				results would simply be too garbled to show anything meaningful. Neither appears
				to be the case in this case study which provides at least a minimal proof of
				concept for salience differentiated stability.</p>
			<p>Our corpus consists of early-modern British natural philosophical/physics
				textbooks (1600~1800), published in English. The corpus has been built within
				the context of the <title rend="quotes">The Normalisation of natural philosophy</title> project, based at
				the university of Groningen. It consists of abroad selection of natural
				philosophical/physics texts, annotated for their level of systematicity, from
				four geographical areas (Great-Britain, The Netherlands, Germany and France),
				between 1600 and 1800.<note> See <title rend="quotes">Mapping early modern natural philosophy: corpus
					collection and authority acknowledgement</title> <ptr target="#sangiacomo2021b"/>. For more
				information on the construction process. And for more information on the
				relative merits compared to classical corpus construction, see <title rend="quotes">Expanding the
				Corpus of Early Modern Natural Philosophy: Initial Results and a Review of
				Available Sources</title> <ptr target="#sangiacomo2021a"/>.</note> From this corpus we’ve made use
				of the most systematic works in the British English language corpus that were
				also available in a digitized format. This led to a sub-corpus of 50 works, each
				of which has been OCR’ed in the context of the <title rend="quotes">Normalisation</title> project. The
				OCR’ing has been conducted with an expected accuracy of 90-95% <ptr target="#sangiacomo"/>, sufficient for many text-mining methods, including for
				example vector-semantic modelling <ptr target="#hill2019"/> and collocate-extraction <ptr target="#sangiacomo"/>.<note>Note that the corpus has been built up without preference to particular
					philosophical schools, and contains a spectrum of natural philosophical schools
					and their works. The titles, authors and dates of our sub corpus can be found in
					the addendum, a more complete overview of the entire corpus can be found online <ptr target="#sangiacomo2021b"/>.
					</note></p>
			
			<p>The texts are preprocessed by the lemmatization of the texts using the Natural
				Language Toolkit’s lemmatizer in Python <ptr target="#bird2009"/>, removing low
				frequency words (&lt;4), removing numbers, interpunction and singular letters,
				decapitalization of all words, and the removal of OCR-based nonsense word types
				and articles. These steps are particular to this case-study, but the SDS-scores
				do not depend on any of these steps and can be applied to raw-texts as well as
				more heavily cleaned texts.</p>
			<p>On the basis of this corpus we modeled 15 key terms’ SDS-scores. These key terms
				were derived both via domain knowledge and via topic modelling – a computational
				method that aims to extract the topics most central to the text.<note> For the
					topic modelling in particular we thank Raluca Tanasescu who modelled part
					of a similar, Latin corpus, from this we have taken suitable
					translations.</note> That is to say, they play a central role within the
				corpus, but the list of these 15 words is in no sense exhaustive of such
				terms.</p>
			<p>In Figure 2 we have provided the results of our explorative case-study as to the
				differences in stability one can find in salient and non-salient connections for
				technical terminology in early-modern English language natural
				philosophy/physics. For each term and for each work in the corpus, we have
				generated models based on the distributional algorithms mentioned above (one to
				be interpreted as a model of the salient elements of the terms’ meaning, and the
				other as the less salient parts of the terms’ meaning). Based on the outcomes,
				the terms will be classified in our system introduced in section 4.</p> 
			
					<figure>
						<head>A scatter plot that shows positions of our investigated terms based on both
							their more and less salient stability scores.</head>
						<graphic url="resources/images/figure02.jpeg"/>
						<figDesc>Scatterplot showing SDS scores. The background is black with blue points. White text identifies the blue points.</figDesc>
					</figure>
			
			<p>Above we see a plot of the scores of our terminology. On the horizontal axis we
				have the average similarity normalized to the average corpus-wide similarity
				scores based on the collocation based approach, on the vertical, the same for
				the vector-based approach. From this, we are given four quadrants in which our
				values are plotted, which agree with the four categories we introduced in the
				previous section. The lower left quadrant signifies terms about which there is
				neither salient nor subtle stability in the terminology, the terms provoke
				discussion and their legitimacy is opposed. The upper right quadrant signifies
				terms that exhibit stability both in salient and subtle facets of their meaning
				– i.e. unproblematic, generally simple, terms. The lower right quadrant
				signifies terms that exhibit façade like behavior – the terms are stable and
				unproblematic at the surface level, but at the operative level exhibit
				unrecognized semantic shifts. Finally, the top left quadrant signifies what we
				have dubbed integral controversy terms – despite being central to much
				discussion, the terms exert a unifying power on the discourse through their
				subtle similarity. What follows is an interpretation of these results that
				places them within the literature on the development of early-modern natural
				philosophy – the case study is explorative and has as its main aim to show the
				possibility for fruitful application of the method.</p>
			<div><head>Simple terms:</head>
			<p>In the upper right quadrant we find 4 terms (excluding motion) – ‘earth’, ‘water’,
				‘sun’ and ‘object’. The commonality between the first three terms is that they
				all refer to identifiable, concrete, phenomenally accessible, things. This is of
				course not to say there were no interesting discussions about these terms (a lot
				of astronomical work was done on the relation between the earth and the sun, and
				on the nature of these two planets) but the terms are still relatively
				straightforward qua meaning. For the sun in particular a simple ostensive
				definition is available, somewhat similarly for water (water being this
				ostensively available stuff, and other similar stuff). Indeed, the three terms
				are all very neatly counted as easily identifiable, concrete, objects of
				investigation within the discipline of natural philosophy (fire and electricity
				also inch close to this quadrant). ‘Object’ is the only abstract term of the
				four and also scores somewhat closer to the façade quadrant than the rest. But,
				the usage of a term like ‘object’ is simple – and there can be little truly
				different applications of it. Because even when speaking of mental, biological
				or physical objects, in all cases nothing changes about the ways in which they
				are objects.</p></div>
					<div>
			<head>Integral controversy terms</head>
			<p>There are three terms in our case-study that fall (or almost fall) in the upper
				left quadrant that we have proposed to understand as being comprised of terms
				that are extremely central to the discourse, are central to much controversy,
				but enforce a certain unity into the discourse via implicit agreement on the
				more general connections that such terms have. ‘Motion’ and ‘matter’ are edge
				cases, so let’s first look at the clearest case, ‘body’.</p>
			<p>‘Body’ is extremely central to early modern natural philosophy. The early
				mechanicist philosophies in the 16th century (like Descartes’ philosophy) let
				themselves be summarized in the striking sentence <quote rend="inline">bodies in
					motion,</quote> or, <quote rend="inline">matter in motion</quote> <ptr target="#nadler1993" loc="3"/>, <ptr target="#roux2017" loc="27–28"/>. Although many schools (already existing in the form
				of scholastic schools, or following schools, like experimentalists and
				Newtonians) will reject mechanicism’s claim that the investigation of <quote rend="inline">bodies in motion</quote> exhausts the activities of the natural
				philosopher, they however will all have to concede that the mechanicist has set
				the program: to be a natural philosopher is to (at least) have answers to
				questions pertaining to bodies (whatever they might exactly be) and their
				movements (whatever that might end up meaning exactly). </p>
			<p> Once the program has been set – the natural philosopher is to say something about
				bodies – it is natural that ‘body’ itself becomes a contested term. The
				philosopher who can provide and defend a conception of ‘body’ such that it
				easily fits with his/her more general outlook is a philosopher who has
				successfully scaled the walls of natural philosophy. At the same time, these
				moves need to be made out in the open – since the eye of the reader is fixed on
				these terms. Façade-like behavior is not to be expected – all of the
				disagreement is out in the open, not hidden, nor is there any manifest agreement
				that could help hide subterranean disagreement.</p>
			<p> A few examples will show the extent of the mutability of ‘body’ within early
				modern philosophy, where these mutations are central to the arguments provided.
				Descartes’ mechanicism comes together with a very explicit statement about the
				nature of what bodies are – they are primarily to be characterized by their
				extension. This is coupled together with a statement about the other ‘type of
				thing’ there is in the world, namely, thought. Bodies being extension means that
				all the other properties of bodies can be explained reductively by reference to
				their extensional properties (and their motions) – except any properties (or
				movements) that are to be explained via thought. If body is anything, it is to
				be extension, and if body is to not be anything in particular, it is not to be
				thought. Amazingly, in opposition to this, we find later natural philosophers to
				remain true to Descartes style of reasoning about body (1. Body is central to
				their activity as natural philosophers and 2. It is an investigation into body’s
				essential attributes and motions that should occupy them in particular) while in
				the meantime moving away wholly from his conception of body. Indeed, to make
				room for their particular systems of natural philosophy, we find thinkers like
				Moore, Cavendish and Conway, more or less destroy all properties ascribed to
				bodies by Descartes. For Conway, both the impenetrability (one of the ways the
				space filling ‘extension’ was often fleshed out) of bodies is opposed and it is
				claimed that bodies are both spiritual and material-like <ptr target="#lopston1982" loc="15"/>.
				Cavendish phrases the issue somewhat differently, but for her too, bodies are
				coupled together with mental properties <ptr target="#shaheen2019" loc="3553–3554"/>, as is the
				case for Moore, who arguably provides less reworking since he mostly takes the 
				extended conception of motion to be gappy in accounting for phenomena <ptr target="#roux2017" loc="27"/>. What allows for these very deep metaphysical discussions is that,
				in the end, bodies can be somewhat easily identified in everyday activity. And
				whatever the exact nature of bodies turns out to be, it should in the end still be
				mapped on a set of paradigmatic instances of bodies (even the radical turnaround
				one finds in a thinker like Conway, where one would expect the concept to be so
				definitively deformed to no longer map unto the same objects as previous
				concepts of body, is still applied to recognizable cases, like the different
				unmixed fluids and how they hang together in a single bottle).<note> She argues
					that a phenomenon like the freezing of a bottle of liquor producing a small
					unfrozen part with a higher density of alcohol (or, tentatively, spirit)
					can best be modeled as a separation of gross ‘body’ from the more supple
					‘spiritual’ body (i.e. the unmixing of a liquid through freezing is a
					separation of the more spiritual bodies from the more matter-like ones.)
					<ptr target="#conway1996" loc="43–44"/></note>
			</p>
			<p> So, what about ‘motion’ and ‘matter’? Generally, ‘matter’ was used more
				restrictively than ‘body’ – and was less easily identified with everyday objects
				in the way that bodies could be. One would expect that matter was less stable in
				application because matter was not so strictly tied together to phenomenally
				accessible paradigmatic examples (bodies might be best imagined as spheres,
				houses, coherent swathes of fluid, cannon balls whilst matter was often
				understood to encompass more restrictively the corpuscles that make up bodies).
				This is due to the Aristotelian roots of the concept – matter plays a role in
				the hylomorphic theories where things are essentially made up out of the
				combination of matter and form <ptr target="#manning2012"/>.</p>
			<p>Motion get reinterpreted radically within early-modern philosophy, motion as
				change makes place for motion as mere local-motion. However, this move is not as
				extensively challenged in the development of early modern natural philosophy as
				was body. While body functioned as the pivotal point where different
				conceptualizations could be made to do all of the metaphysical lifting, motion
				did not take up such a place. Motion was on the agenda but was more easily
				understood and less prone to the radical reinterpretations (after the initial
				reconfiguration) than occurred for body. </p></div>
					<div>
			<head>Façade terms:</head>
			<p>In the lower right quadrant we would expect to find terms that exhibit façade like
				behavior. These are words similar in salient ways but dissimilar in more subtle
				ways. Superficial similarity may mask slippery differences in usage. Three terms
				seem to apply – ‘fire’, ‘electricity’, and ‘method’. We will not discuss ‘fire’,
				because, as we can see in the plot the distance between ‘fire’ and the ‘x=x’
				line is very small. That is to say, the difference between the more and less
				salient scores is very small (98/104).</p>
			<p> Similarly, ‘electricity’ will not be extensively considered for two reasons.
				Firstly, it’s quite close on the line toward the upper right quadrant. In
				addition, the technical nature of the term precludes a useful analysis without
				more extensive background knowledge of the term. A few short remarks that should
				be read in that light are that: ‘electricity’ has properties in common with
				‘earth’, ‘sun’ and ‘water’ in the sense that it is a term that designates a
				somewhat properly delineated group of phenomena. In particular, static
				electrical effects (which resulted in the attraction of other objects) and
				magnetic materials (which show somewhat similar behavior) were termed
				electrical. Although practitioners disagreed on how to explain these phenomena,
				for some time, this set of phenomena was clearly delineated. However, further
				study revealed that some previously ‘electrical’ phenomena should be delineated
				from ‘the electrical per se’, in particular excluding magnetic phenomena from
				the term's extension <ptr target="#gregory2007" loc="35–42"/>. In addition, there is the novelty
				of the topic in natural philosophy as a systematic topic of interest – whereas
				it was of fringe importance in foregoing natural philosophy, it gets
				conceptualized more systematically within early modern natural philosophy – the
				novelty should induce us to expect less stability in the application of the term
				in its non-salient features, whereas identifiable phenomena should make us
				expect more stable non-salient applications. What is less easily explained is
				the cause for the agreement about salient facets of electricity’s meaning –
				especially since the debate about electricity is often characterized as a number
				of schools disagreeing fundamentally about the nature of electricity (Ibid,
				p.35-42). Note that most of the systematic early modern explanation started to
				come up in the second half of the 18th century, making it only a small part of
				the timeline of our corpus. We leave this discussion as is.</p>
			<p> Central in the lower right quadrant is ‘method’. Many early modern thinkers
				agreed on the centrality of method in science and presumably its salient
				stability can be explained by this as well as by a shared understanding of the
				overall concept. We might be able to explain its subtle instability by the fact
				that method as a word generally signifies that some technicalities are to come,
				but which technicalities differs depending on which method in particular will be
				discussed.</p>
			<p> If we are correct, method’s presence in this quadrant can be explained without
				calling it a façade. Electricity might be a façade, but to uncover this would
				require more in depth analysis of the development of the concept of electricity
				in this period. This means that our case-study did not bear out our expectations
				– no clear cases of facades were found in the lower right quadrant.</p></div>
					<div>
			<head>Crisis terms:</head>
			<p>The most densely populated quadrant is that of the crisis terms (5 terms,
				excluding ‘matter’) – ‘part’, ‘cause’, ‘specie’, ‘form’, ‘experiment’. These are
				words which are understood differently and used differently. These terms are
				thus such that we should expect both that the terms were controversial and
				discussed explicitly in natural philosophy and that there were little methods
				available to tie these terms together via, for example, implicit agreement on
				the (paradigmatic) extension of these terms (as in the case of body). </p>
			<p> All of these terms agree with this characterization. The four terms ‘part’,
				‘cause’, ‘specie’ and ‘form’ are all derived from scholastic philosophy and
				heavily debated in early modern natural philosophy. Species, (substantial) forms
				and causes were all important aspects of the scholastic/Aristotelian framework,
				and were all reworked, or even outright rejected, in subsequent schools of
				natural philosophy (like Cartesianism and Newtonianism) <ptr target="#blair2006" loc="366"/>.
				Cause is already reworked by Descartes, who also rejects (although not fully)
				substantial forms <ptr target="#flage1997" loc='845'/>. We see species being
				rejected by mechanicists as well, as well as non-mechanicists like Conway
				<ptr target="#conway1996" loc="30–31"/>. Cause is outright rejected as being the proper object
				of investigation for natural philosophy by later Newtonians like van
				Musschenbroek <ptr target="#sangiacomo2018" loc="51"/>. Forms are sometimes reintroduced and
				reworked and all the while remain in play in the strong scholastic school that
				remains in operation for most of the early-modern period, particularly in the
				university context <ptr target="#sangiacomo"/>. What differentiates these
				terms from body are two things: i) body has easily accessible paradigmatic
				instances of the concept’s extension which are not so readily available for, for
				example, part and form (and arguably, at least some of the fourfold of
				Aristotelian causes) and ii) whereas body was extensively discussed in the light
				of its given central position to the discipline, these terms were discussed
				because their position within the discipline were under dispute. Experiment has
				a similar structure, except there the discussion will not have been most
				explicitly between scholastic and new philosophies, but between
				rationalist/Hobbesian conceptions of natural philosophy and experimental/Boylean
				<ptr target="#shapin2018"/>. Again, not only is experiment a more
				technical term that allows for less easy identification of its extension (what
				even counts as an experiment, as opposed to observation, or an uninformative
				‘account’) <ptr target="#shapin2018"/>. In these debates, it is also still up for grabs whether it
				has any place in natural philosophy. In this sense we propose the terms in
				quadrant to be crisis terms – they are central to the discipline, because in
				many ways, the form of the discipline is transformed by drastically questioning
				the contents and validity of these terms.</p></div></div>
				<div>
			<head>Conclusion</head>
			<p>In this paper two methods for the automated semantic investigations of corpora
				often used in the digital humanities have been compared over which facets of
				meaning they track. We’ve claimed that full vector-representations of a word’s
				meaning tracks the word’s more hidden, subtle and less salient aspects of its
				meaning and that the collocate-representation tracks the more salient aspects of
				its meaning. We’ve proposed a new measure on the basis of this which makes use
				of these features: Salience Differentiated Stability. SDS-scores signify the
				divergence between the salient stability and the non-salient of a word in a
				corpus of works. Finally, a small case study making use of SDS-scores concerning
				a corpus of early-modern natural philosophy has been provided.</p></div>
			<div>
			<head>Acknowledgements</head>
			<p>This research has been supported by the ERC starting grant NaturalPhilosophy Grant
				ID: 801653, with the title: “The normalization of natural philosophy: how
				teaching practices shaped the evolution of early modern science” and the
				Research Training Group Situated Cognition funded by the Deutsche
				Forschungsgemeinschaft .</p></div>
			<div>
			<head>Addendum </head>
				<p>This addendum contains some details about the contents of our corpus – all of
					the following texts have been made use of in OCR’d format to allow for the
					computational analyses given above.</p>
				
				<table>
					<head>Corpus</head>
					<row role="label">
						<cell>Author</cell>
						<cell>Title</cell>
						<cell>Publication Date</cell>
					</row>
					<row role="data">
						<cell>Digby</cell>
						<cell>Two Treatises in the one of which, The nature of Bodies; in the
							other, The nature of Mans Soule, is looked into</cell>
						<cell>1644</cell>
					</row>
					<row role="data">
						<cell>White</cell>
						<cell>Institutiones Peripateticae </cell>
						<cell>1646</cell>
					</row>
					<row role="data">
						<cell>Comenius</cell>
						<cell>Naturall philosophie reformed by divine light, or, A synopsis of
							physicks by J.A. Comenius</cell>
						<cell>1651</cell>
					</row>
					<row role="data">
						<cell>Charleton</cell>
						<cell>Physiologia Epicuro-Gassendo-Charltoniana: or a Fabrik of
							Science Natural, Upon the Hypothesis of Atoms, Founded by
							Epicurus, Reparied by Petrus Gassendus, Augmented by Walter
							Charleton </cell>
						<cell>1654</cell>
					</row>
					<row role="data">
						<cell>Boyle</cell>
						<cell>The Sceptical Chymist (2 ed: ... Whereunto is Added a Defence of
							the Authors Explication of the Experiments, Against the
							Obiections of Franciscus Linus and Thomas Hobbes (a book-length
							addendum to the second edition of New Experiments
							Physico-Mechanical)</cell>
						<cell>1661</cell>
					</row>
					<row role="data">
						<cell>Boyle</cell>
						<cell>Considerations touching the Usefulness of Experimental Natural
							Philosophy (followed by a second part in 1671)</cell>
						<cell>1663</cell>
					</row>
					<row role="data">
						<cell>Boyle</cell>
						<cell>Origin of Forms and Qualities according to the Corpuscular
							Philosophy</cell>
						<cell>1666</cell>
					</row>
					<row role="data">
						<cell>Cavendish</cell>
						<cell>Observations on Experimental Philosophy </cell>
						<cell>1666</cell>
					</row>
					<row role="data">
						<cell>Cavendish</cell>
						<cell>Grounds of Natural Philosophy </cell>
						<cell>1668</cell>
					</row>
					<row role="data">
						<cell>Boyle</cell>
						<cell>A Free Enquiry into the Vulgarly Received Notion of
							Nature</cell>
						<cell>1686</cell>
					</row>
					<row role="data">
						<cell>Midgley</cell>
						<cell>A new treatise of natural philosophy, free'd from the
							intricacies of the schools adorned with many curious experiments
							both medicinal and chymical : as also with several observations
							useful for the health of the body</cell>
						<cell>1687</cell>
					</row>
					<row role="data">
						<cell>Le Grand</cell>
						<cell>An Entire Body of Philosophy according to the principles of the
							famous Renate Des Cartes in three books.</cell>
						<cell>1694</cell>
					</row>
					<row role="data">
						<cell>Newton</cell>
						<cell>Opticks</cell>
						<cell>1704</cell>
					</row>
					<row role="data">
						<cell>Ditton</cell>
						<cell>The General Laws of Nature and Motion, with their application to
							mechanicks. Also the doctrine of centripetal forces and
							velocities of bodies describing any of the conick sections, being
							a part of the great Mr. Newton’s principles</cell>
						<cell>1709</cell>
					</row>
					<row role="data">
						<cell>Worster</cell>
						<cell>A compendious and methodical account of the principles of
							natural philosophy </cell>
						<cell>1722</cell>
					</row>
					<row role="data">
						<cell>Stirling </cell>
						<cell>A course of mechanical and experimental philosophy : consisting
							of Seven Parts. </cell>
						<cell>1727</cell>
					</row>
					<row role="data">
						<cell>Greene</cell>
						<cell>The Principles of the Philosophy of the Expansive and
							Contractive Forces; or, An Inquiry into the Principles of the
							Modern Philosophy, that is, into the Several Chief Rational
							Sciences, which are Extant </cell>
						<cell>1727</cell>
					</row>
					<row role="data">
						<cell>Pemberton</cell>
						<cell>A View of Sir Isaac Newton’s Philosophy </cell>
						<cell>1728</cell>
					</row>
					<row role="data">
						<cell>Horsley</cell>
						<cell>A Short and General Account of the most Necessary and
							Fundamental Principles of Natural Philosophy ... Revised,
							corrected, and adapted to a course of experiments ... By John
							Booth.</cell>
						<cell>1743</cell>
					</row>
					<row role="data">
						<cell>Desaguliers</cell>
						<cell>A course of Experimental Philosophy </cell>
						<cell>1745</cell>
					</row>
					<row role="data">
						<cell>Rowning</cell>
						<cell>A Compendious System of Natural Philosophy </cell>
						<cell>1744</cell>
					</row>
					<row role="data">
						<cell>Robertson</cell>
						<cell>The principles of natural philosophy explain'd and illustrated
							by experiments; in a course of sixteen lectures: to be perform'd
							at Mr. Fuller's Academy, etc.</cell>
						<cell>1745</cell>
					</row>
					<row role="data">
						<cell>Knight</cell>
						<cell>An attempt to demonstrate that all the phaenomena in nature may
							be explained by two simple active principles, Attraction and
							Repulsion: Wherein the Attraction of Cohesion, Gravity and
							Magnetism are shewn to be One and the Same and the Phenomena of
							the Latter are more Particularly Explained </cell>
						<cell>1748</cell>
					</row>
					<row role="data">
						<cell>Rutherfort</cell>
						<cell>A System of Natural Philosophy; being a Course of Lectures in
							Mechanics, Optics, Hydrostatics and Astronomy </cell>
						<cell>1748</cell>
					</row>
					<row role="data">
						<cell>MacLaurin</cell>
						<cell>An Account of Sir Isaac Newton’s Philosophyical Discoveries </cell>
						<cell>1748</cell>
					</row>
					<row role="data">
						<cell>Emerson</cell>
						<cell>The Principles of Mechanics: explaining and demonstrating the
							general laws of motion, the laws of motion, the laws of gravity,
							motion of descending Bodies, projectiles, mechanics powers,
							pendulums, center of gravity etc. strength and stress of timber,
							hydrostatics, and construction of machines </cell>
						<cell>1754</cell>
					</row>
					<row role="data">
						<cell>Wilson</cell>
						<cell>The principles of philosophy. The principles of natural
							philosophy: with some remarks upon the fundamental principles of
							the Newtonian philosophy</cell>
						<cell>1754</cell>
					</row>
					<row role="data">
						<cell>Jones</cell>
						<cell>An essay on the first principles of natural philosophy ... in
							four books</cell>
						<cell>1763</cell>
					</row>
					<row role="data">
						<cell>Ferguson</cell>
						<cell>An easy and pleasant introduction to Sir Isaac Newton's
							Philosophy : containing the first principles of mechanics,
							trigonometry, optics, and astronomy</cell>
						<cell>1772</cell>
					</row>
					<row role="data">
						<cell>Arden</cell>
						<cell>Analysis of Mr. Arden’s course of lectures on natural and
							experimental philosophy. Viz. Natural philosophy in general,
							chemistry, electricity, mechanics, geography, astronomy,
							hydrostatics, pneumatics, optics</cell>
						<cell>1774</cell>
					</row>
					<row role="data">
						<cell>Hamilton</cell>
						<cell>Four introductory lectures in natural philosophy. : I. Of the
							rules of philosophising, the essential properties of matter, and
							laws of motion. II. Of the several kinds of attraction, and
							particularly of cohesion. III. Of gravity, or the attraction of
							gravitation. IV. The laws of motion explained, and confirmed by
							experiments.</cell>
						<cell>1774</cell>
					</row>
					<row role="data">
						<cell>Lovett</cell>
						<cell>The Electrical philosophers, Containing a New System of Physics </cell>
						<cell>1774</cell>
					</row>
					<row role="data">
						<cell>Fenning</cell>
						<cell>The young man's book of knowledge : being a proper supplement to
							The young man's companion. In six parts</cell>
						<cell>1774</cell>
					</row>
					<row role="data">
						<cell>Banks</cell>
						<cell>An epitome of a course of lectures on natural and experimental
							philosophy.</cell>
						<cell>1775</cell>
					</row>
					<row role="data">
						<cell>Atwood</cell>
						<cell>A Description of the Experiments intended to Illustrate a Course
							of Lectures on the Principles of Natural philosophy </cell>
						<cell>1776</cell>
					</row>
					<row role="data">
						<cell>Cullen</cell>
						<cell>First lines of physics, for the use of students in the
							University of Edinburgh (4 vol.)</cell>
						<cell>1777</cell>
					</row>
					<row role="data">
						<cell>Robinson</cell>
						<cell>Outlines of a Course of Experimental Philosophy </cell>
						<cell>1784</cell>
					</row>
					<row role="data">
						<cell>Atkinson</cell>
						<cell>A Compendium of a Course of Lectures on Natural and Experimental
							Philosophy</cell>
						<cell>1784</cell>
					</row>
					<row role="data">
						<cell>Telescope</cell>
						<cell>The Newtonian system of philosophy.</cell>
						<cell>1787</cell>
					</row>
					<row role="data">
						<cell>Peart</cell>
						<cell>On the elementary principles of nature ; and the simple laws by
							which they are governed. Being an attempt to demonstrate their
							existence, and to explain their mode of action ; particularly in
							those states, in which they produce the attractions of cohesion,
							gravitation, magnetism and electricity ; and also fire, light,
							and water.</cell>
						<cell>1789</cell>
					</row>
					<row role="data">
						<cell>Adams</cell>
						<cell>Lectures on natural and experimental philosophy, considered in
							it's present state of improvement. </cell>
						<cell>1794</cell>
					</row>
					<row role="data">
						<cell>Walker</cell>
						<cell>Analysis of a course of lectures in natural and experimental
							philosophy : Viz. 1. Properties of matter, 2. Mechanics, 3.
							Chemistry, 4 &amp; 5. Pneumatics, 6. Hydrostatics, 7.
							Electricity, 8. Electricity, 9. Optics, 10. Use Of The Globes, 11
							&amp; 12. Astronomy, &amp;c. By A. Walker.</cell>
						<cell>1795</cell>
					</row>
					<row role="data">
						<cell>Anderson</cell>
						<cell>Institutes of Physics </cell>
						<cell>1798</cell>
					</row>
					<row role="data">
						<cell>Gregory</cell>
						<cell>The Economy of Nature Explained and Illustrated on the
							Principles of Modern Philosophy vol.2</cell>
						<cell>1798</cell>
					</row>
					<row role="data">
						<cell>Gregory</cell>
						<cell>The Economy of Nature Explained and Illustrated on the
							Principles of Modern Philosophy vol.3</cell>
						<cell>1798</cell>
					</row>
					<row role="data">
						<cell>Wood</cell>
						<cell>The principles of mechanics : designed for the use of students
							in the University. By James Wood ...</cell>
						<cell>1800</cell>
					</row>
					<row role="data">
						<cell>Robison</cell>
						<cell>Elements of Mechanical Philosophy: Being the Substance of a
							Course of Lectures on that Science</cell>
						<cell>1804</cell>
					</row>
					<row role="data">
						<cell>Young</cell>
						<cell>A course of lectures on Natural Philosophy and the Mechanical
							Arts </cell>
						<cell>1807</cell>
					</row>
					<row role="data">
						<cell>Playfair</cell>
						<cell>Outlines of Natural Philosophy </cell>
						<cell>1812</cell>
					</row>
					<row role="data">
						<cell>Wylde</cell>
						<cell>The circle of the sciences; a cyclopaedia of experimental,
							chemical, mathematical, &amp; mechanical philosophy, and natural
							history;</cell>
						<cell>1862</cell>
					</row>
				</table>
			</div>
		</body>
	
		<back>
			<listBibl>
				<bibl xml:id="bird2009" label="Bird et al. 2009"> Bird, Steven, Loper, Edward and Klein, Ewan. (2009), <title rend="italic">Natural Language
					Processing with Python</title>. O’Reilly Media Inc.</bibl>
				<bibl xml:id="blair2006" label="Blair 2006"> Blair, A. (2006). <title rend="quotes">Natural Philosophy.</title> In K. Park, &amp; L. Daston, <title
					rend="italic">The Cambridge History of Science; Early Modern
					Science</title> (Vol. 3, pp. 365-405). Cambridge: Cambridge University
					Press.</bibl>
				<bibl xml:id="bouma2009" label="Bouma 2009"> Bouma, G. (2009). <title rend="quotes">Normalized (pointwise) mutual information in collocation
					extraction.</title> <title rend="italic">Proceeding of GSCL</title>, (pp. 31-40).</bibl>
				<bibl xml:id="brezina2015" label="Brezina 2015"> Brezina, V., McEnery, T., &amp; Wattam, S. (2015). <title rend="quotes">Collocations in context: A
					new perspective on collocation networks.</title> <title rend="italic">International
						Journal of Corpus Linguistics, 20</title>(2), 139-173.</bibl>
				<bibl xml:id="church1989" label="Church 1989"> Church, K., &amp; Hanks, P. (1989). <title rend="quotes">Word Association Norms, Mutual
					Information, and lexicography.</title> <title rend="italic">27th Annual Meeting of the Association for Computational
						Linguistics</title>, 76-83.</bibl>
				<bibl xml:id="conway1996" label="Conway 1996"> Conway, A. (1996). <title rend="italic">The Principles of the Most Ancient and
					Modern Philosophy.</title> (A. Coudert, &amp; T. Corse, Eds.) Cambridge:
					Cambridge University Press.</bibl>
				<bibl xml:id="davies2012" label="Davies 2012"> Davies, M. (2012). <title rend="quotes">Expanding horizons in historical linguistics with the
					400-million word Corpus of Historical American English.</title> <title rend="italic">Corpora,</title> 7, 121-157</bibl>
				<bibl xml:id="debolla2013" label="de Bolla 2013"> de Bolla, P. (2013). <title rend="italic">The Architecture of Concepts: The
					Historical Formation of Human Rights.</title> Fordham University
					Press.</bibl>
				<bibl xml:id="debolla2019" label="de Bolla 2019"> de Bolla, P., Jones, E., Nulty, P., Recchia, G., &amp; Regan, J. (2019).
					<title rend="quotes">Distributional Concept Analysis: A Computational Model for History of
					Concepts.</title> <title rend="italic">Contributions to the history of concepts</title>,
					66-92.</bibl>
				<bibl xml:id="devlin2018" label="Devilin 2018"> Devlin, J. Chang, M. Lee, K. &amp; Toutanova, K. (2018). <title rend="quotes">BERT: Pre-training
					of Deep Bidirectional Transformers for Language Understanding.</title> <ref target="https://arxiv.org/abs/1810.04805v2">
					<title rend="italic">arXiv:1810.04805v2</title>
				</ref>
				</bibl>
				<bibl xml:id="firth1957" label="Firth 1957"> Firth, J. (1957). <title rend="italic">A Synopsis of Linguistic Theory.</title>
					Oxford: <title rend="italic">Studies in linguistic analysis</title>, Blackwell.</bibl>
				<bibl xml:id="flage1997" label="Flage 1997"> Flage, D., &amp; Bonnen, C. (1997). <title rend="quotes">Descartes on Causation.</title> <title rend="italic"
					>The Review of Metaphysics, 50</title>(4), 841-872.</bibl>
				<bibl xml:id="gal2013" label="Gal 2013"> Gal, O., &amp; Chen-Morris, R. (2013). <title rend="italic">Baroque
					Science.</title> Chicago: The University of Chicago Press.</bibl>
				<bibl xml:id="gavin2019" label="Gavin 2019"> Gavin, Jennings, Kersey, &amp; Pasanek. (2019). Spaces of Meaning: Conceptual
					History, Vector Semantics, and Close Reading. In Gold, &amp; Klein, <title
						rend="italic">Debates in the Digital Humanities 2019.</title>
					Minneapolis: University of Minnesota Press.</bibl>
				<bibl xml:id="goldberg2014" label="Goldberg 2014"> Goldberg, Y. &amp; Levy, O. (2014) <title rend="quotes">word2vec Explained: Deriving Mikolov et
					al.’s Negative-Sampling Word-Embedding Method.</title> <title rend="italic">arXiv:1402.3722</title>
				</bibl>
				<bibl xml:id="gregory2007" label="Gregory 2007"> Gregory, F. (2007). <title rend="italic">Natural Science in Western
					History.</title> Boston: Houghton Miflin Company.</bibl>
				<bibl xml:id="hamilton2016" label="Hamilton 2016"> Hamilton, W. Leskovec, J. Jurafsky, D. (2016) <title rend="quotes">Diachronic Word Embeddings
					Reveal Statistical Laws of Semantic Change.</title> arXiv:1605.09096</bibl>
				<bibl xml:id="han2012" label="Han 2012"> Han, J., Kamber, M., &amp; Pei, J. (2012). <title rend="quotes">Getting to Know Your Data.</title> In <title
					rend="italic">Data Mining</title> (Third edition ed., pp. 39-82).
					Elsevier.</bibl>
				<bibl xml:id="harris1964" label="Harris 1964"> Harris, Z. (1964). <title rend="quotes">Distributional Structure (org. 1954).</title> In J. Fodor, &amp;
					J. Katz, <title rend="italic">The Structure of Language: Readings in the
						Philosophy of Language</title> (pp. 33–49). Englewood Cliffs:
					Prentice-Hall.</bibl>
				<bibl xml:id="haslanger2012" label="Haslanger 2012"> Haslanger, S. (2012). <title rend="quotes">What Are We Talking About? The Semantics and Politics
					of Social Kinds.</title> In S. Haslanger, <title rend="italic">Resisting Reality:
						Social Construction and Social Critique</title> (pp. 365-380). Oxford
					University Press.</bibl>
				<bibl xml:id="hill2019" label="Hill 2019"> Hill, M.J., Hengchen, S. (2019) <title rend="quotes">Quantifying the impact of dirty OCR on
					historical text analysis: Eighteenth Century Collections Online as a case
					study, Digital Scholarship in the Humanities,</title> 34 (4): 825–843,
					<ref target="https://doi.org/10.1093/llc/fqz024">https://doi.org/10.1093/llc/fqz024</ref>.</bibl>
				<bibl xml:id="hughes2016" label="Hughes et al. 2016"> Hughes, L., Constantopulos, P., &amp; Dallas, C. (2016). <title rend="quotes">Digital Methods in
					the Humanities: Understanding and Describing their Use across the
					Disciplines.</title> In R. S. Susan Schreibman, <title rend="italic">A New Companion
						to Digital Humanities</title> (pp. 150-170). Oxford, Malden: John Wiley
					&amp; Sons, Ltd.</bibl>
				<bibl xml:id="kuhn2012" label="Kuhn 2012"> Kuhn, T. (1962/2012). <title rend="italic">The Structure of Scientific
					Revolutions</title> (4th ed.). Chicago: University of Chicago Press.</bibl>
				<bibl xml:id="landauer1997" label="Landauer and Dumais 1997"> Landauer, T., &amp; Dumais, S. (1997). <title rend="quotes">A solution to Plato's problem: The
					latent semantic analysis theory of acquisition, induction, and
					representation of knowledge.</title> <title rend="italic">Psychological review,
						104</title>(2), 211.</bibl>
				<bibl xml:id="lopston1982" label="Lopston 1982"> Lopston, P. (1982). <title rend="quotes">Introduction.</title> In A. Conway, &amp; P. Lopston (Ed.), <title
					rend="italic">The Principles of the Most Ancient and Modern
					Philosophy</title> (pp. 1-60). Dordrecht: Kluwer Academic Publishing
					Group.</bibl>
				<bibl xml:id="lowe2000" label="Lowe and McDonald 2000"> Lowe, W., &amp; McDonald, S. (2000). <title rend="italic">The direct route:
					Mediated priming in semantic space.</title> The University of
					Edinburgh.</bibl>
				<bibl xml:id="ludlow2014" label="Ludlow 2014"> Ludlow, P. (2014). <title rend="italic">Living Words: Meaning Underdetermination
					and the Dynamic Lexicon.</title> Oxford University Press.</bibl>
				<bibl xml:id="manning2012" label="Manning 2012"> Manning, G. (2012). <title rend="quotes">Three Biased Reminders about Hylomorphism in Early Modern Science and Philosophy.</title> In <title rend="italic">Matter and Form in Early Modern Science and Philosophy</title> (pp. 1-32). Leiden: Brill.</bibl>
				<bibl xml:id="mcdonald2000" label="McDonald 2000">  McDonald, S. (2000). <title rend="italic">Environmental Determinants of Lexical
					Processing Effort</title> (PhD-Thesis ed.). University of Edinburgh.</bibl>
				<bibl xml:id="mcdonald2001" label="McDonald and Ramscar 2001"> McDonald, S., &amp; Ramscar, M. (2001). <title rend="quotes">Testing the distributioanl
					hypothesis: The influence of context on judgements of semantic similarity.</title>
					<title rend="italic">Proceeding of the Annual Meeting of the Cognitive
						Science Society</title>, <title rend="italic">23.</title>
				</bibl>
				<bibl xml:id="mikolov2013" label="Mikolov et al. 2013"> Mikolov, T. Chen, K. Corrado, G. &amp; Dean, J. (2013). <title rend="quotes">Efficient Estimation
					of Word Representations in Vector Space.</title> <ref
						target="https://arxiv.org/abs/1301.3781v3">
						<title rend="italic">arXiv:1301.3781v3</title>
					</ref> </bibl>
				<bibl xml:id="nadler1993" label="Nadler 1993"> Nadler, S. (1993). <title rnd="quotes">Introduction.</title> In <title rend="italic">Causation in early
					modern philosophy</title> (pp. 1-8). University Park: The Pennsylvania
					State University Press.</bibl>
				<bibl xml:id="ramscar2000" label="Ramscar and Yarlett 2000"> Ramscar, M., &amp; Yarlett, D. (2000). <title rend="quotes">The use of a
					high-dimensional, 'environmental' context space to model retrieval in
					analogy and similarity-based transfer.</title> <title rend="italic">Proceedings of the
						22nd Annual Conference of the Cognitive Science Society.</title>
				</bibl>
				<bibl xml:id="roux2017" label="Roux 2017"> Roux, S. (2017). <title rend="quotes">From the mechanical philosophy to early modern mechanisms.</title>
					In P. I. Stuart Glennan, <title rend="italic">The Routledge Handbook of
						Mechanisms and Mechanical Philosophy</title> (pp. 26-45). Routledge.</bibl>
				<bibl xml:id="sangiacomo2018" label="Sangiacomo 2018"> Sangiacomo, A. (2018). <title rend="quotes">Teleology and the Evolution of Natural Philosophy.</title> <title
					rend="italic">Studia Leibnitiana, 50</title>(1), 41-56.</bibl>
				<bibl xml:id="sangiacomo2021a" label="Sangiacomo et al. 2021a"> Sangiacomo, A., Tanasescu, R., Donker, S., &amp; Hogenbirk, H (2021a).
					Expanding the Corpus of Early Modern Natural Philosophy: Initial Results
					and a Review of Available Sources. Journal of Early Modern Studies, 10(1),
					107-115</bibl>
				<bibl xml:id="sangiacomo2021b" label="Sangiacomo et al. 2021b"> Sangiacomo, Andrea; Tanasescu, Raluca; Donker, Silvia; and Hogenbirk, Hugo
					(2021b). <title rend="quotes">Normalisation of Early Modern Science: Inventory of 17th- and
					18th-Century Sources (1.0.0</title>) [Data set]. Zenodo.
					<ref target="https://doi.org/10.5281/zenodo.5566681">https://doi.org/10.5281/zenodo.5566681</ref></bibl>
				<bibl xml:id="sangiacomo2022" label="Sangiacomo et al. 2022a"> Sangiacomo, A., Tanasescu, R., Donker, S., &amp; Hogenbirk, H. (2022a).
					<title rend="quotes">Mapping early modern natural philosophy: corpus collection and authority
					acknowledgement.</title> <title rend="italic">Annals of Science, 79(1) 1-39.</title>
				</bibl>
				<bibl xml:id="sangiacomo" label="Sangiacomo et al. 2022b"> Sangiacomo, A., Hogenbirk, H., Tanasescu, R., Karaisl, A., White, N. (2022b) <title rend="quotes">Reading in the mist: High-quality optical character recognition based on freely available early modern digitized books.</title> <title rend="italic">Digital Scholarship in the Humanities</title>. </bibl>
				<bibl xml:id="shaheen2019" label="Shaheen 2019"> Shaheen, J. (2019). <title rnd="quotes">Part of nature and division in Margaret Cavendish’s materialism.</title> <title rend="italic">Synthese, 196</title>, 3551-3575.</bibl>
				<bibl xml:id="shapin2018" label="Shapin and Schaffer 1985/2018"> Shapin, S., &amp; Schaffer, S. (1985/2018). <title rend="italic">Leviathan and
					the air-pump</title> (First Princeton Classics paperback ed.). Linotron
					Baskerville: Princeton University Press.</bibl>
				<bibl xml:id="smith2020" label="Smith 2020"> Smith, N. A. (2020). <title rend="italic">Contextual Word
					Representations: A Contextual Introduction.</title> Retrieved from
					https://arxiv.org/: <ref target="https://arxiv.org/pdf/1902.06006.pdf"
						>https://arxiv.org/pdf/1902.06006.pdf</ref>
				</bibl>
				<bibl xml:id="underwood2019" label="Underwood 2019"> Underwood, T. (2019). <title rend="italic">Distant Horizons: Digital Evidence
					and Literary Change. </title>Chicago: University of Chicago Press.</bibl>
				<bibl xml:id="wilson2006" label="Wilson 2006"> Wilson, M. (2006). <title rend="italic">Wandering Significance: An Essay on
					Conceptual Behavior.</title> Oxford: Clarendon Press.</bibl>
				<bibl xml:id="wevers2020" label="Wevers and Koolen 2020"> Wevers, M. &amp; Koolen, M. (2020). Digital begriffsgeschichte: Tracing
					semantic change using word embeddings. <title rend="italic">Historical
						Methods: A Journal of Quantitative and Interdisciplinary History,</title>
					53, 226-243</bibl>
			</listBibl>
		</back>
	</text>
</TEI>
