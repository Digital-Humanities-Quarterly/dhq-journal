<TEI xmlns="http://www.tei-c.org/ns/1.0">
     <teiHeader>
          <fileDesc>
               <titleStmt>
                    <title/>
                    <author/>
               </titleStmt>
               <editionStmt>
                    <edition><date>2023-07-21</date></edition>
               </editionStmt>
               <publicationStmt>
                    <p>unknown</p>
               </publicationStmt>
               <sourceDesc>
                    <p>Converted from a Word document</p>
               </sourceDesc>
          </fileDesc>
          <encodingDesc>
               <appInfo>
                    <application xml:id="docxtotei" ident="TEI_fromDOCX" version="7.56.0">
                         <label>DOCX to TEI</label>
                    </application>
               </appInfo>
          </encodingDesc>
          <revisionDesc>
               <listChange>
                    <change><date>2024-01-08T20:52:20Z</date><name/></change>
               </listChange>
          </revisionDesc>
     </teiHeader>
     <text>
          <body>
               <div>
                    <p rend="Title" style="text-align: justify;">Seeking Information in Spanish
                         Historical Newspapers: The Case of <hi rend="italic">Diario de Madrid</hi>
                              (18<hi rend="superscript">th</hi> and 19<hi rend="superscript">th</hi>
                         Centuries)</p>
                    <p rend="Standard" style="text-align: right;"><hi style="font-size:14pt">Eva
                              Sánchez-Salido, Antonio Menta and Ana García-Serrano</hi><note
                              place="foot" xml:id="ftn1" n="1"><p rend="footnote text">Corresponding
                                   author.</p></note></p>
                    <p rend="Standard" style="text-align: right;">ETSI Informática, UNED</p>
                    <p rend="Standard" style="text-align: right;">{evasan, agarcia}@lsi.uned.es,
                         amenta@invi.uned.es</p>
               </div>
               <div>
                    <head>Abstract</head>
                    <p rend="Standard" style="text-align: justify;">New technologies for seeking
                         information are based in machine learning techniques such as statistical or
                         deep learning approaches that require a large number of computational
                         resources as well as the availability of huge corpora to develop the
                         applications that, in this concrete sub-area of Artificial Intelligence,
                         are the so-called <hi rend="italic">models</hi>. Nowadays, the reusability
                         of the developed models is approached with fine-tuning and transfer
                         learning techniques. When the available corpus is written in a language or
                         domain with scarce resources, the accuracy of these approaches decreases,
                         so it is important to address the start of the task by using
                         state-of-the-art techniques.</p>
                    <p rend="Standard" style="text-align: justify;">This is the main problem tackled
                         in the work presented here, coming from the art historians’ interest in an
                         image-based digitized collection of newspapers called
                         <hi rend="italic" xml:space="preserve">Diario de Madrid </hi>(DM) from the
                         Spanish press between 18<hi rend="superscript">th</hi> and 19<hi
                              rend="superscript">th</hi> centuries, which is freely available at the
                         Spanish National Library (BNE). Their focus is on information related to
                         entities such as historical persons, locations as well as objects for sale
                         or lost and others, to obtain geo-localization visualizations and solve
                         some historical riddles. The first step needed technically is to obtain the
                         transcriptions of the original digitalized newspapers from the DM
                         (1788-1825) collection. After that, the second step is the development of a
                         Named Entity Recognition (NER) model to label or annotate automatically the
                         available corpus with the entities of interest for their research. For
                         this, once the CLARA-DM corpus is created, a sub-corpus must be manually
                         annotated for the training step in current Natural Language Processing
                         (NLP) techniques, using human effort helped by selected computational
                         tools. To develop the necessary annotation model (CLARA-AM), an
                         experimentation step is carried out with state-of-the-art Deep Learning
                         (DL) models and an already available corpus, which complements the corpus
                         that we have developed. <anchor type="commentRangeStart" n="1"/>Once the
                         new model is established, th<anchor type="commentRangeStart" n="3"/>e
                         automatic transcription of any newspaper page is ready for use<anchor
                              type="commentRangeEnd" n="3"/><note place="comment" resp="#Autor"
                              n="3"><date when=""/><hi style="font-family:Segoe UI">Not necessarily,
                                   errors should be assessed</hi></note><anchor
                              type="commentRangeEnd" n="1"/><hi rend="annotation_reference"><note
                                   place="comment" resp="#Autor" n="1"><date when=""/>Quito esta
                                   frase porque no tiene que ver con el modelo NER del que se está
                                   hablando</note></hi>.</p>
                    <p rend="Standard" style="text-align: justify;">A main contribution of the paper
                         is the methodology developed to tackle similar problems like that of art
                         historians’ digitized corpus: selecting specific tools when available,
                         reusing developed DL models to carry out new experiments in an available
                         corpus, reproducing experiments in the art historians’ own corpus and
                         applying transfer learning techniques within a domain with few resources.
                         Three Four different resources developed are described: the transcribed
                         corpus, the DL-based transcription model, the annotated corpus and the DL
                         computational models developed for the transcription and annotation using a
                         specific domain-based set of labels in a small corpus. The CLARA-TM
                         transcription model learned for the DM is accessible from January 2023<note
                              place="foot" xml:id="ftn2" n="2"><p rend="footnote text"><ref
                                        target="https://readcoop.eu/model/spanish-print-xviii-xix/"
                                             ><hi rend="underline"
                                             >https://readcoop.eu/model/spanish-print-xviii-xix/</hi></ref><seg xml:space="preserve">    </seg></p></note>
                         at the READ-COOP website under the title “Spanish print XVIII-XIX - Free
                         Public AI Model for Text Recognition with Transkribus”.</p>
               </div>
               <div>
                    <head>Introduction</head>
                    <p rend="Standard" style="text-align: justify;">Corpora construction is a
                         laborious task (Gruszczyński et al., 2021; Ruiz Fabo et al., 2017; Wissler
                         et al., 2014), since it is desirable that the corpora may be used by
                         different people and serve various purposes. Corpora are classified
                         according to different parameters, such as subject matter, purpose,
                         discourse modality and others, some of which are the most important being:
                         balance, representativeness, or transparency (Davies and Parodi, 2022;
                         Gebru et al., 2021; Torruella, 2017). The process of corpus construction is
                         divided into different phases, ranging from the definition of its
                         boundaries and purpose, pre-processing, storage, annotation, to name a few
                         (Aldama et al., 2022; Nakayama, 2021; Calvo-Tello, 2019; Kabatek, 2013;
                         Rojo, 2010).</p>
                    <p rend="Standard" style="text-align: justify;">Corpus-based research has been
                         dominated by statistical and neural models (Moreno-Sandoval, 2019;
                         Nieuwenhuijsen, 2016; Rojo, 2016) until the end of the last decade, when
                         Transformer-based models appeared (Vaswani et al., 2017), requiring the
                         availability of very large corpora for training. Such massive corpora are
                         scarcely available in the field of Humanities mainly because the corpora
                         need to be annotated according to the interests of the corpus end-users, so
                         it happens that the types of entities often vary according to the origin,
                         language, domain, or purpose of the dataset. The Named Entity Recognition
                         (NER) discipline, dealing with interesting entities of interest, has
                         evolved a lot since its beginnings in the first competitive NER task in
                         1996 (Grishman and Sundheim, 1996), as many other tasks and datasets have
                         been created for evaluation.</p>
                    <p rend="Standard" style="text-align: justify;">Early NER systems made use of
                         algorithms based on hand-built rules, lexicons and gazetteers, orthographic
                         features or ontologies, among others, with the human cost of producing
                         domain-specific resources that this entails (Li et al., 2022). Later, these
                         systems were followed by those based on feature engineering and machine
                         learning (Nadeau and Sekine, 2007), which were the dominant technique in
                         the task of named entity recognition until the first decade of the 2000s,
                         that is, until the NLP revolution with the advent of neural networks. The
                         most common supervised machine learning systems of this type that were used
                         for NER include Hidden Markov Models (HMMs) (Bikel et al., 1997), Support
                         Vector Machines (SVM) (Asahara and Matsumoto, 2003), Conditional Random
                         Fields (CRF) (McCallum and Li, 2003), Maximum Entropy models (ME)
                         (Borthwick et al., 1998), and decision trees (Sekine, 1998).</p>
                    <p rend="Standard" style="text-align: justify;">Starting with (Collobert et al.,
                         2011), neural network-based systems are interesting because they do not
                         require domain-specific resources such as lexicons or ontologies and are
                         therefore more domain-independent (Yadav and Bethard, 2018). In this
                         context, several neural network architectures were proposed, mostly based
                         on some form of recurrent neural network (RNN) on characters (Lample et
                         al., 2016), and embeddings of words or word components (Akbik et al.,
                         2018). Finally in 2017 the Transformer architecture was introduced in the
                         paper <hi rend="italic">Attention Is All You Need</hi> (Vaswani et al.,
                         2017) which is here to stay, and its derivatives such as BERT (Devlin et
                         al., 2019) and RoBERTa (Liu et al., 2019), which we make use of today, as
                         well as the Spanish-based MarIA<note place="foot" xml:id="ftn3" n="3"><p
                                   rend="footnote text" style="text-align: justify;"><ref
                                        target="https://www.bne.es/es/noticias/1111-el-primer-sistema-masivo-de-inteligencia-artificial-de-la-lengua-espanola-maria"
                                             ><hi rend="color(00000A)"
                                             >https://www.bne.es/es/noticias/1111-el-primer-sistema-masivo-de-inteligencia-artificial-de-la-lengua-espanola-maria</hi></ref></p></note>
                         or RigoBERTa<note place="foot" xml:id="ftn4" n="4"><p rend="footnote text"
                                   style="text-align: justify;"><ref
                                        target="https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/"
                                             ><hi rend="color(00000A)"
                                             >https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/</hi></ref></p></note>.</p>
                    <p rend="Standard" style="text-align: justify;">In the domain of Digital
                         Humanities (DH), applying NER models to historical documents poses several
                         challenges, as shown by other works in the domain (Chastang et al., 2021;
                         Kettunen et al., 2017). This is a fertile field within NLP, since cultural
                         institutions are carrying out digitization projects in which large amounts
                         of images containing text are obtained (Piotrowski, 2012; Terras, 2011).
                         One of the challenges is the margin of error still present in optical
                         character recognition (OCR) systems (Boros et al., 2020), since historical
                         documents are generally very noisy, contain smudges, and have different
                         typographies that are generally unknown to the systems, which makes
                         character recognition even more difficult.</p>
                    <p rend="Standard" style="text-align: justify;">Another challenge is related to
                         the transfer of knowledge to new domains or languages, in this case to
                         adapt NER models trained with datasets in current languages to old
                         languages (Baptiste et al., 2021; Bollmann, 2019; De Toni et al., 2022).
                         Transfer learning techniques have become common practice in a wide range of
                         tasks in NLP
                         <?biblio ADDIN ZOTERO_ITEM CSL_CITATION {"citationID":"KdZMEwGq","properties":{"formattedCitation":"(Hintz & Biemann, 2016; Pruksachatkun et\\uc0\\u160{}al., 2020; Zoph et\\uc0\\u160{}al., 2016)","plainCitation":"(Hintz & Biemann, 2016; Pruksachatkun et al., 2020; Zoph et al., 2016)","noteIndex":0},"citationItems":[{"id":977,"uris":["http://zotero.org/users/8638006/items/RZHQ8MK3"],"itemData":{"id":977,"type":"paper-conference","container-title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","DOI":"10.18653/v1/P16-1012","event-place":"Berlin, Germany","event-title":"ACL 2016","page":"118–129","publisher":"Association for Computational Linguistics","publisher-place":"Berlin, Germany","source":"ACLWeb","title":"Language Transfer Learning for Supervised Lexical Substitution","URL":"https://aclanthology.org/P16-1012","author":[{"family":"Hintz","given":"Gerold"},{"family":"Biemann","given":"Chris"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2016",8]]}}},{"id":973,"uris":["http://zotero.org/users/8638006/items/GFX5UFZ7"],"itemData":{"id":973,"type":"paper-conference","abstract":"While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.","container-title":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","DOI":"10.18653/v1/2020.acl-main.467","event-place":"Online","event-title":"ACL 2020","page":"5231–5247","publisher":"Association for Computational Linguistics","publisher-place":"Online","source":"ACLWeb","title":"Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?","title-short":"Intermediate-Task Transfer Learning with Pretrained Language Models","URL":"https://aclanthology.org/2020.acl-main.467","author":[{"family":"Pruksachatkun","given":"Yada"},{"family":"Phang","given":"Jason"},{"family":"Liu","given":"Haokun"},{"family":"Htut","given":"Phu Mon"},{"family":"Zhang","given":"Xiaoyi"},{"family":"Pang","given":"Richard Yuanzhe"},{"family":"Vania","given":"Clara"},{"family":"Kann","given":"Katharina"},{"family":"Bowman","given":"Samuel R."}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2020",7]]}}},{"id":975,"uris":["http://zotero.org/users/8638006/items/7Z5PNFQY"],"itemData":{"id":975,"type":"paper-conference","container-title":"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing","DOI":"10.18653/v1/D16-1163","event-place":"Austin, Texas","event-title":"EMNLP 2016","page":"1568–1575","publisher":"Association for Computational Linguistics","publisher-place":"Austin, Texas","source":"ACLWeb","title":"Transfer Learning for Low-Resource Neural Machine Translation","URL":"https://aclanthology.org/D16-1163","author":[{"family":"Zoph","given":"Barret"},{"family":"Yuret","given":"Deniz"},{"family":"May","given":"Jonathan"},{"family":"Knight","given":"Kevin"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2016",11]]}}}],"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}?>(Hintz
                         &amp; Biemann, 2016; Pruksachatkun et al., 2020; Zoph et al., 2016),
                         including the domain of DH such as works presented in the workshop on
                         Natural Language Processing for Digital Humanities at NLPAI 2021
                         <?biblio ADDIN ZOTERO_ITEM CSL_CITATION {"citationID":"Q3pV8wL5","properties":{"formattedCitation":"(Blouin et\\uc0\\u160{}al., 2021; Rubinstein & Shmidman, 2021)","plainCitation":"(Blouin et al., 2021; Rubinstein & Shmidman, 2021)","noteIndex":0},"citationItems":[{"id":979,"uris":["http://zotero.org/users/8638006/items/8CSFS4MM"],"itemData":{"id":979,"type":"paper-conference","abstract":"Named entity recognition is of high interest to digital humanities, in particular when mining historical documents. Although the task is mature in the field of NLP, results of contemporary models are not satisfactory on challenging documents corresponding to out-of-domain genres, noisy OCR output, or old-variants of the target language. In this paper we study how model transfer methods, in the context of the aforementioned challenges, can improve historical named entity recognition according to how much effort is allocated to describing the target data, manually annotating small amounts of texts, or matching pre-training resources. In particular, we explore the situation where the class labels, as well as the quality of the documents to be processed, are different in the source and target domains. We perform extensive experiments with the transformer architecture on the LitBank and HIPE historical datasets, with different annotation schemes and character-level noise. They show that annotating 250 sentences can recover 93% of the full-data performance when models are pre-trained, that the choice of self-supervised and target-task pre-training data is crucial in the zero-shot setting, and that OCR errors can be handled by simulating noise on pre-training data and resorting to recent character-aware transformers.","container-title":"Proceedings of the Workshop on Natural Language Processing for Digital Humanities","event-place":"NIT Silchar, India","event-title":"NLP4DH 2021","page":"152–162","publisher":"NLP Association of India (NLPAI)","publisher-place":"NIT Silchar, India","source":"ACLWeb","title":"Transferring Modern Named Entity Recognition to the Historical Domain: How to Take the Step?","title-short":"Transferring Modern Named Entity Recognition to the Historical Domain","URL":"https://aclanthology.org/2021.nlp4dh-1.18","author":[{"family":"Blouin","given":"Baptiste"},{"family":"Favre","given":"Benoit"},{"family":"Auguste","given":"Jeremy"},{"family":"Henriot","given":"Christian"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2021",12]]}}},{"id":982,"uris":["http://zotero.org/users/8638006/items/IIGQ44UH"],"itemData":{"id":982,"type":"paper-conference","abstract":"A big unknown in Digital Humanities (DH) projects that seek to analyze previously untouched corpora is the question of how to adapt existing Natural Language Processing (NLP) resources to the specific nature of the target corpus. In this paper, we study the case of Emergent Modern Hebrew (EMH), an under-resourced chronolect of the Hebrew language. The resource we seek to adapt, a diacritizer, exists for both earlier and later chronolects of the language. Given a small annotated corpus of our target chronolect, we demonstrate that applying transfer-learning from either of the chronolects is preferable to training a new model from scratch. Furthermore, we consider just how much annotated data is necessary. For our task, we find that even a minimal corpus of 50K tokens provides a noticeable gain in accuracy. At the same time, we also evaluate accuracy at three additional increments, in order to quantify the gains that can be expected by investing in a larger annotated corpus.","container-title":"Proceedings of the Workshop on Natural Language Processing for Digital Humanities","event-place":"NIT Silchar, India","event-title":"NLP4DH 2021","page":"106–110","publisher":"NLP Association of India (NLPAI)","publisher-place":"NIT Silchar, India","source":"ACLWeb","title":"NLP in the DH pipeline: Transfer-learning to a Chronolect","title-short":"NLP in the DH pipeline","URL":"https://aclanthology.org/2021.nlp4dh-1.12","author":[{"family":"Rubinstein","given":"Aynat"},{"family":"Shmidman","given":"Avi"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2021",12]]}}}],"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}?>(Blouin
                         et al., 2021; Rubinstein &amp; Shmidman, 2021). </p>
                    <p rend="Standard" style="text-align: justify;">Recent initiatives have been
                         launched, such as the creation and annotation of datasets (Ehrmann et al.,
                         2022; Neudecker, 2016) and holding competitive events such as HIPE (<hi
                              rend="italic">Identifying Historical People, Places and other
                              Entities</hi>) (Ehrmann et al., 2020a) addressing NER in historical
                         texts. The general goals of HIPE include improving the robustness of
                         systems, enabling comparison of the performance of NER systems on
                         historical texts, and, in the long term, fostering efficient semantic
                         indexing in historical documents. The HIPE2020 corpus, like shares
                         objectives with the CLARA-DM corpus, since it involves the recognition of
                         entities in historical newspapers, and has specific types of entities, that
                         are slightly different from the general NER ones of localizations (LOC),
                         persons (PER), organizations (ORG) and miscellanea (MISC). It is common
                         practice in this domain that Historians, Linguists and Computer Science
                         researchers collaborate to seek information on the domain-dependent
                         entities and scenarios in which the historical research is focused (Rivero,
                         2022; Merino 2022; García-Serrano and Castellanos, 2016; Calle et al.
                         2006). We propose the use of the HIPE project approach with the aim of
                         taking advantage of these resources and gaining perspective on the approach
                         for our domain, a task for which we have little annotated data.</p>
                    <p rend="Standard" style="text-align: justify;">This paper is organized starting
                         with a section dedicated to the corpus creation, transcription and
                         sub-corpus manual annotation. Available tools are evaluated according to
                         the properties of the original DM newspaper collection, the total amount of
                         data contained and offered <anchor type="commentRangeStart" n="31"/><anchor
                              type="commentRangeStart" n="32"/>and required software to be
                         installednecessary installations facilities<anchor type="commentRangeEnd"
                              n="31"/><note place="comment" resp="#Autor" n="31"><date when=""/><hi
                                   style="font-family:Segoe UI">??</hi></note><anchor
                              type="commentRangeEnd" n="32"/><hi rend="annotation_reference"><note
                                   place="comment" resp="#Autor" n="32"><date when=""/>(Antonio) Yo
                                   también creo que no queda claro. Pondría "usability" o
                                   similar.</note></hi>. Afterwards, a section is devoted to the
                         experimentation setting to perform the CLARA-DM corpus automatic annotation
                         using deep learning models for NER. First, we carry out some experiments
                         with a resource on multilingual historical newspapers such as the HIPE2020
                         corpus containing historical OCR texts in French, English and German
                         (Ehrmann et al., 2020b). One main goal is to identify which type of process
                         adaptation and Transformer-based models will obtain the best results for
                         the CLARA-DM corpus, based on the experience with HIPE2020. Then, we
                         conduct some experiments on CLARA-DM dataset, in a transfer learning set-up
                         (zero-shot and few-shot learning) between (1) three different sets of
                         entity types: the general NER ones, the one used in the HIPE2020
                         experiments, and the domain specific ones used in CLARA-DM, and (2)
                         languages (Spanish, Spanish, English and German). Finally, in the last
                         section we outline the conclusions drawn from the experiments and conclude
                         the paper with some plans for future work.</p>
               </div>
               <div>
                    <head>Creation of the corpus CLARA-DM</head>
                    <p rend="Standard" style="text-align: justify;">A major goal of the
                              CLARA-HD<note place="foot" xml:id="ftn5" n="5"><p rend="footnote text"
                                   style="text-align: justify;">CLARA-HD project
                                   (PID2020-116001RB-C32) is funded by MCIN - AEI
                                   (AEI/10.13039/501100011033).</p></note> collaborative project
                         involving art historians, linguists, and computer scientists, is to speed
                         up the art historians’ research, through the transcription of the PDF
                         newspapers’ pages and the development of a robust model for recognition of
                         named entities. The work started with a comparison of the performance of
                         other NER systems on historical texts, following the IMPRESSO<note
                              place="foot" xml:id="ftn6" n="6"><p rend="footnote text"><ref
                                        target="https://impresso-project.ches"><hi
                                             rend="color(00000A)"
                                        >https://impresso-project.ch</hi></ref></p></note> project
                         and the HIPE series of shared tasks whose objective is to foster efficient
                         semantic indexing on historical documents. The CLARA-HD project is based on
                         the joint research of art historians, who are keen on investigating what is
                         done and where it is done in the city of Madrid (Molina, 2021; Cámara et
                         al., 2020; Molina and Vega, 2018); the technicians, who are experts in
                         Natural Language Processing and Deep Learning Models in applied research
                         (Menta and García-Serrano, 2022; Sánchez-Salido, 2022; Menta et al., 2022;
                         García-Serrano and Menta-Garuz, 2022), and the linguists, well-known
                         experts in corpus creation, analysis, and exploitation (Campillos-Llanos,
                         2022; Moreno-Sandoval et al., 2018).</p>
                    <p rend="Standard" style="text-align: justify;"><anchor type="commentRangeStart"
                              n="41"/><anchor type="commentRangeStart" n="42"/>The<anchor
                              type="commentRangeEnd" n="41"/><note place="comment" resp="#Autor"
                              n="41"><date when=""/><hi style="font-family:Segoe UI">This paraphraph
                                   belongs more to the introduction, and I believe this has been
                                   already said, it seems redundant wrt one has read
                              already.</hi></note><anchor type="commentRangeEnd" n="42"/><hi
                              rend="annotation_reference"><note place="comment" resp="#Autor" n="42"
                                        ><date when=""/>Es algo que ya se ha dicho en el abstract,
                                   pero yo lo dejaría aquí porque da el contexto sobre la
                                   construcción del corpus..</note></hi> starting point of the
                         CLARA-HD project was the analysis of the art historians’ interest in the
                         digitized collection of newspapers called <hi rend="italic">Diario de
                              Madrid</hi>, published between the 18<hi rend="superscript">th</hi>
                         and 19<hi rend="superscript">th</hi> centuries and readily available at the
                              BNE<note place="foot" xml:id="ftn7" n="7"><p rend="footnote text"
                                   style="text-align: justify;"><ref
                                        target="https://hemerotecadigital.bne.es/hd/card?oid=0001510462"
                                             ><hi rend="color(00000A)"
                                             >https://hemerotecadigital.bne.es/hd/card?oid=0001510462</hi></ref></p></note>.
                         The focus of art historians is on information related to historical
                         persons, locations but also objects for sale or lost, so the first step is
                         the construction of a historical corpus from the original newspapers, the
                         so-called CLARA-DM corpus, which involves designing a style guideline for
                         transcription and a second style guideline for the annotation of named
                         entities, which entails identifying the domain-based set of labels
                         (semantic categories). The next step is the manual annotation of a
                         sub-corpus to serve as the training corpus for an application development
                         using current DL techniques. Finally, the development of an efficient model
                         to recognise the specific named entities is tackled to annotate the
                         CLARA-DM corpus automatically.</p>
                    <p rend="Standard" style="text-align: justify;">There are different tools to
                         deal with any process in corpus management, from the document
                         transcription, storage and search phase, annotation, analysis, and even
                         corpus exploitation using Python libraries for information extraction. In
                         this work we use Transkribus<note place="foot" xml:id="ftn8" n="8"><p
                                   rend="footnote text" style="text-align: justify;"><ref
                                        target="https://readcoop.eu/transkribus"><hi
                                             rend="color(00000A)"
                                             >https://readcoop.eu/transkribus</hi></ref></p></note>
                         for transcription, Tagtog<note place="foot" xml:id="ftn9" n="9"><p
                                   rend="footnote text" style="text-align: justify;"><ref
                                        target="https://tagtog.com/"><hi rend="color(00000A)"
                                             >https://tagtog.com</hi></ref></p></note> for
                         annotation, and HuggingFace<note place="foot" xml:id="ftn10" n="10"><p
                                   rend="footnote text" style="text-align: justify;"><ref
                                        target="https://huggingface.co/"><hi rend="color(00000A)"
                                             >https://huggingface.co</hi></ref></p></note> for the
                         implementation of Deep Learning models. In the remainder of this section
                         the first two steps are detailed, and the third one will be tackled in the
                         following section.</p>
                    <div>
                         <head>Transcription of the digitalized newspapers</head>
                         <p rend="Standard" style="text-align: justify;">The first step building the
                              corpus is the transcription of the texts, which we carried out using
                              the Transkribus tool (Menta et al., 2022). The tool was selected once
                              compared with commercial and open-source transcription tools as Amazon
                                   Textract<note place="foot" xml:id="ftn11" n="11"><p
                                        rend="footnote text" style="text-align: justify;"><ref
                                             target="https://aws.amazon.com/es/textract"><hi
                                                  rend="color(00000A)"
                                                  >https://aws.amazon.com/es/textract</hi></ref></p></note>
                              and Google’s Tesseract<note place="foot" xml:id="ftn12" n="12"><p
                                        rend="footnote text" style="text-align: justify;"><ref
                                             target="https://github.com/tesseract-ocr/tesseract"><hi
                                                  rend="color(00000A)"
                                                  >https://github.com/tesseract-ocr/tesseract</hi></ref></p></note>,
                              as the accuracy for old typography was slightly better, no fee for
                              basic functionalities of the tool was required and has been used by
                              other projects in ancient languages (Kirmizialtin and Wrisley, 2022;
                              Aranda García, 2022; Ayuso García, 2022; Bazzaco et al., 2022;
                              Alrasheed at al., 2021; Derrick, T., 2019). Also, it provides
                              functionalities both on the web browser and in the client version, and
                              has proven to be very helpful for small DH research groups (Perdiki,
                              2022).</p>
                         <p rend="Standard" style="text-align: justify;">Transcription using the
                              Transkribus tool starts by requiring the layout recognition, which
                              consists of detecting the text regions and lines of text within the
                              documents. This automatic process is not perfect, since the model does
                              not recognise only text but also recognises some regions such as lines
                              or spots that we have to remove manually. Furthermore, the main
                              problem with newspapers is that there are tables or columns, and the
                              model is not able to recognise the order in which it should read the
                              pages. That is why we have to carry out a manual task to sort the
                              text. It should be noted that we do this process because we make use
                              of models that use sentences for training (Transformers for NER), and
                              we also want the corpus to be used in the future for semantic and
                              syntactic analysis. Note that the layout is not so relevant when only
                              a word-based analysis is wanted.</p>
                         <p rend="Standard" style="text-align: justify;">Once we have carried out
                              the structure recognition of the newspaper pages, the so-called layout
                              recognition, we can move on to transcribing the text of the pages,
                              either manually or using a public model, since the Transkribus tool
                              contains public models that are trained with historical texts in
                              different languages. We explored some of them, but they were still
                              unable to recognise the text with some quality. The "Spanish Golden
                              Age Theatre Prints 1.0" model (Cuéllar and Vega García-Luengos, 2022;
                              Cuéllar, 2021) especially failed to recognise numbers or capital
                              letters in our documents. So, what we propose to do is to develop our
                              own transcription model (CLARA-TM) for the transcription of the
                              documents in the CLARA-DM corpus. The first question was to find out
                              how many pages we needed for a model in Transkribus to learn to
                              transcribe automatically. The more training data, the better, but
                              according to Transkribus guidelines, it is possible to start training
                              the model with at least 25-<anchor type="commentRangeStart" n="50"/>75
                                   pages<anchor type="commentRangeEnd" n="50"/><note place="comment"
                                   resp="#Autor" n="50"><date when=""/><hi
                                        style="font-family:Segoe UI">Could you add source? This
                                        seems a lot for printed texts,</hi></note> manually
                              transcribed, or less when working with printed instead of handwritten
                                   texts<note place="foot" xml:id="ftn13" n="13"><p
                                        rend="footnote text"
                                        >https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/
                                   </p></note>. </p>
                         <p rend="Standard" style="text-align: justify;">We carried out several
                              tests. In the first one we trained the model with 37 manually
                              transcribed newspaper pages and obtained an error rate in character
                              recognition (CER) of 4% in the validation set. This error decreases in
                              successive tests with more amount and homogeneous training data. We
                              realised that most errors in the transcription were caused by a lack
                              of uniformity in the manual transcriptions, since they were carried
                              out by different people. In order to make a more reliable
                              transcription model, the transcribed pages were reviewed manually and
                              a standardisation process wase carried out. This includes aspects such
                              as the unification of the way fractions are transcribed (1/2 o <hi
                                   rend="superscript color(040C28)" style="font-size:15pt">1</hi><hi
                                   rend="color(040C28)" style="font-size:15pt">⁄</hi><hi
                                   rend="subscript color(040C28)" style="font-size:15pt">2</hi>),
                              the inclusion or omission of symbols such as "=" (used before an
                              author's signature), "&amp;" or "§§", or the correction or not of
                              typos. After this process, there is a high degree of homogeneity in
                              the data that the model will see in its training, and therefore a
                              higher probability of successful learning. After several tests
                              (performed by changing the model parameters (number of epochs,
                              learning rate and documents selected for training and validation) we
                              trained the model with <anchor type="commentRangeStart" n="62"/>193
                              transcribed pages<anchor type="commentRangeEnd" n="62"/><note
                                   place="comment" resp="#Autor" n="62"><date when=""/><hi
                                        style="font-family:Segoe UI">Mention criteria to choose
                                        these pages, ex : layout, font, years, tables, ..
                                   ??</hi></note> and obtained less than a 1% error, so we have kept
                              it as our own transcription model for the CLARA-DM corpus. The
                              obtained CER is a good one, slightly better than that obtained from
                              public models that use similar resources to ours<note place="foot"
                                   xml:id="ftn14" n="14"><p rend="footnote text"
                                        style="text-align: justify;">See <hi rend="italic">German
                                             Fraktur 19</hi><hi rend="italic superscript">th</hi><hi
                                             rend="italic">-20</hi><hi rend="italic superscript"
                                             >th</hi><hi rend="italic" xml:space="preserve"> century </hi>(<ref
                                             target="https://readcoop.eu/model/german-fraktur-19th-20th-century/"
                                                  ><hi rend="color(00000A)"
                                                  >https://readcoop.eu/model/german-fraktur-19th-20th-century/</hi></ref>),
                                             <hi rend="italic">French newspapers late 18</hi><hi
                                             rend="italic superscript"
                                             >th</hi><hi rend="italic" xml:space="preserve"> century – midth of 20</hi><hi
                                             rend="italic superscript"
                                        >th</hi><hi rend="italic" xml:space="preserve"> century</hi>
                                             (<ref
                                             target="https://readcoop.eu/model/french-newspapers-late-18th-century-midth-of-20th-century/"
                                                  ><hi rend="color(00000A)"
                                                  >https://readcoop.eu/model/french-newspapers-late-18th-century-midth-of-20th-century/</hi></ref>),
                                             <hi rend="italic">Transkribus Print Multi-Language</hi>
                                             (<ref
                                             target="https://readcoop.eu/model/print-multi-language-danish-dutch-german-finnish-french-latin-swedish/"
                                                  ><hi rend="color(00000A)"
                                                  >https://readcoop.eu/model/print-multi-language-danish-dutch-german-finnish-french-latin-swedish/</hi></ref>)
                                        or <hi rend="italic">Dutch newspapers 17</hi><hi
                                             rend="italic superscript"
                                        >th</hi><hi rend="italic" xml:space="preserve"> century</hi>
                                             (<ref
                                             target="https://readcoop.eu/model/dutch-newspapers-17th-century/"
                                                  ><hi rend="color(00000A)"
                                                  >https://readcoop.eu/model/dutch-newspapers-17th-century/</hi></ref>).</p></note>.
                              The pages used for training correspond to 37 different newspapers
                              randomly chosen (between the ones downloaded, which covered the first
                              day of every month between 1788-1825), since they all have a similar
                              structure: 4-8 pages, the first one containing a table and both
                              capital and lower case letters, and pages divided in columns at the
                              end of the newspaper.</p>
                         <p rend="Standard" style="text-align: justify;">From now, the model can be
                              applied to transcribe text automatically from any <hi rend="italic"
                                   >Diario de Madrid</hi> newspaper (see Figure 1) <anchor
                                   type="commentRangeStart" n="68"/>with a layout recognition
                              carried out manually<anchor type="commentRangeEnd" n="68"/><note
                                   place="comment" resp="#Autor" n="68"><date when=""/><hi
                                        style="font-family:Segoe UI">Layout recognition is the main
                                        weakness of Transkribus from my experience.</hi></note>. The
                              original collection has 13,479 newspapers (59,424 pages) and the
                              CLARA-DM corpus currently has 589 newspapers with layout recognition
                              done (2,474 pages), 37 newspapers manually transcribed (201 pages),
                                   <anchor type="commentRangeStart" n="69"/>143 newspapers
                              automatically transcribed (657 pages, containing an average of 1% of
                              errors in the transcriptions)<anchor type="commentRangeEnd" n="69"
                                   /><note place="comment" resp="#Autor" n="69"><date when=""/><hi
                                        style="font-family:Segoe UI">Errors?</hi></note>, and 10
                              newspapers manually annotated (53 pages/24,843 tokens).</p>
                         <figure>
                              <table rend="rules">
                                   <row>
                                        <cell style="text-align: center;"
                                             rend="Standard background-color(FFFFFF)"><figure>
                                                  <graphic n="1001" width="5.652997222222222cm"
                                                  height="8.625cm" url="media/image1.png"
                                                  rend="inline"/>
                                             </figure></cell>
                                        <cell style="text-align: justify;"
                                             rend="Standard background-color(FFFFFF)"><figure>
                                                  <graphic n="1002" width="8.103cm" height="8.634cm"
                                                  url="media/image2.png" rend="inline"/>
                                             </figure></cell>
                                   </row>
                              </table>
                              <head>Figure 1: Hardcopy of an original page (left) and its automatic
                                   transcription with CLARA-TM (right).</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">The human and computational
                              resources spent for the development of our transcription model are
                              quite important in the project planning and, given that the project is
                              currently funded by the Spanish Government, the model is already
                              published (January 2023) for free use at the Transkribus tool and
                                   website<note place="foot" xml:id="ftn15" n="15"><p
                                        rend="footnote text" style="text-align: justify;"><ref
                                             target="https://readcoop.eu/model/spanish-print-xviii-xix"
                                                  ><hi rend="color(00000A)"
                                                  >https://readcoop.eu/model/spanish-print-xviii-xix</hi></ref></p></note>.</p>
                    </div>
                    <div>
                         <head>Sub-corpus manual annotation</head>
                         <p rend="Standard" style="text-align: justify;">Once we have the
                              transcribed corpus, we can move on to the manual annotation step, but
                              why do we need to do this? The datasets used for a NER task can be
                              annotated or not, depending on whether the system to be developed is
                              supervised or unsupervised. Since we are going to use
                              Transformer-based systems for leading the current state of the art, we
                              need annotated data to train them. On the other hand, a general NER
                              system usually includes the general entities categories (tags/labels)
                              Person, Place, Organisation and Others. However, in the CLARA-DM
                              corpus of historical newspapers we need to define a different set of
                              tags according to the needs of art historians, so that is the second
                              reason why we have to carry out the task of manual annotation first to
                              “teach” (train) the model. Furthermore, the presence of one or more
                              types of encoding and annotations is a very important aspect in the
                              possibilities of corpus exploitation (Rojo, 2010).</p>
                         <p rend="Standard" style="text-align: justify;">To decide which annotation
                              tool to use, an evaluation was made between four current leading
                              annotation tools: Prodigy<note place="foot" xml:id="ftn16" n="16"><p
                                        rend="footnote text"><ref target="https://prodi.gy"><hi
                                                  rend="color(00000A)"
                                        >https://prodi.gy</hi></ref></p></note>, Doccano<note
                                   place="foot" xml:id="ftn17" n="17"><p rend="footnote text"><ref
                                             target="https://doccano.herokuapp.com"><hi
                                                  rend="color(00000A)"
                                                  >https://doccano.herokuapp.com</hi></ref></p></note>,
                                   Brat<note place="foot" xml:id="ftn18" n="18"><p
                                        rend="footnote text"><ref target="http://brat.nlplab.org"
                                                  ><hi rend="color(00000A)"
                                                  >http://brat.nlplab.org</hi></ref></p></note> and
                                   Tagtog<note place="foot" xml:id="ftn19" n="19"><p
                                        rend="footnote text"><ref target="https://www.tagtog.com/"
                                                  ><hi rend="color(00000A)"
                                                  >https://www.tagtog.com/</hi></ref></p></note>.
                              The comparison of their characteristics is shown in Table 1 (general
                              functionalities; whether or not there are user management facilities;
                              whether inter-annotator agreement is automatically calculated; if it
                              is possible to automate tagging; input and output formats; operating
                              system availability; availability of a web version; whether or not
                              Python programming tools need to be installed locally, and finally if
                              is for free, open-source and if user programming skills are required).
                              The Tagtog tool was chosen because it is the only one that integrates
                              a metric for viewing the agreement between annotators. In addition, it
                              allows us to visualize it as it is annotated, which makes the
                              annotation flow much more dynamic.</p>
                         <figure>
                              <table rend="rules">
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"/>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt"
                                             >Prodigy</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt"
                                             >Doccano</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Brat</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt"
                                             >Tagtog</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">General
                                                  functionalities</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">tagging text, images and
                                                  videos and train models with the tagged
                                             data</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">text classification,
                                                  sequence labelling, sequence-to-sequence
                                                  tasks</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">entity and relationship
                                                  annotation, lookups and other NLP derived
                                                  tasks</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">entity and
                                                  relationshipannotation, document
                                                  classification</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">User
                                                  management</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Metrics for
                                                  inter-annotator agreement</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Possibility to
                                                  automate tagging</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes (under
                                                  subscription)</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Input
                                                  formats</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">TXT, JSONL, JSON, CSVand
                                                  others</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">TXT, JSONL,
                                             CoNLL</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">TXT</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">TXT, CSV, source code files,
                                                  URLs and others</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Output
                                                  formats</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">JSONL</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">JSONL</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">.ann</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">TXT, HTML, XML, CSV,
                                                  ann.json, EntitiesTsv, others</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Operating
                                                  system</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">Windows, Mac and
                                             Linux</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">Windows, Mac and
                                             Linux</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">Mac or Linux (on Windows it
                                                  is recommended a virtual machine)</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">Windows,
                                             MacandLinux</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Requires
                                                  interacting with the command line</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no (on the web
                                             version)</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Needs Python
                                                  installed</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes (3.6+)</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">Yes (3.8+)</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes (2.5+)</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Programming
                                                  skills required</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">familiarity with Python is
                                                  desirable</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">knowledge of Linux and
                                                  Apache servers is required</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no (on the web
                                             version)</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Open
                                             source</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">partially</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:9pt">Free</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">no</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  style="font-size:9pt">yes (different
                                                  subscriptions)</hi></cell>
                                   </row>
                              </table>
                              <head><anchor xml:id="Ref109374200"/><anchor xml:id="Ref109374213"
                                   />Table 1. Comparison of tagging tools.</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">The process of the manual
                              annotation of a corpus involves the construction of an “annotation
                              guideline” in which the types of labels/tags are defined and
                              specifications are given on what to annotate and what not to annotate,
                              from where and to where to annotate, etc (Campillos-Llanos et al.,
                              2021; Moreno Sandoval et al., 2020). Achieving a high annotation
                              agreement enables the number of annotators in the future to be
                              increased.</p>
                         <p rend="Standard" style="text-align: justify;">The process also includes
                              the definition of the set of labels, that were defined together with
                              art historians in several turns. On the one hand, the art historians
                              know the information they need, but they do not have the perspective
                              of the computer scientist who knows what kind of categories the model
                              is able to learn to generalise. It is therefore a delicate task that
                              requires an effort of understanding on both sides. When deciding on
                              the set of labels, these can describe broad categories, such as
                              persons, places, organisations, etc, or finer categories, such as
                              streets and squares within the place category or nobles and lords
                              within the person category. It is more convenient to start with a
                              finer or more granular set of labels, as converting these categories
                              into their corresponding broad versions is simpler than carrying out
                              the reverse operation.</p>
                         <p rend="Standard" style="text-align: justify;">Based on the dialogue with
                              the art historians and the documents they provided us with, a first
                              proposal of taxonomy of entities was drawn up. It contained a wide set
                              of entities and sub-entities (such as different kinds of religious
                              places or houses) that is expected to be reduced in order to increase
                              the quality of the automatic annotation. These entities are dumped
                              into the annotation tool and the first annotation cycle was carried
                              out.</p>
                         <figure>
                              <graphic n="1003" width="13.714cm" height="7.228cm"
                                   url="media/image3.png" rend="inline"/>
                              <head>Figure 2. Inter-Annotator Agreement for Person entity class
                                   automatically calculated by the tool.</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">The first sub-corpus
                              contains five newspapers with 28 pages, 928 sentences and 15,145
                              tokens, which are annotated by four annotators using a blind
                              annotation process, that is, each person annotates the document
                              independently without consulting the others, following the guidelines.
                              The tool then calculates the Inter-Annotator Agreement (IAA) for each
                              entity (see Figure 2). With these metrics, the quality of the labels
                              is re-evaluated, and the taxonomy is adjusted. In the second round the
                              tag taxonomy is as shown in Figure 3.</p>
                         <figure>
                              <graphic n="1004" width="9.693cm" height="10.106997222222223cm"
                                   url="media/image4.jpeg" rend="inline"/>
                              <head>Figure 3. Taxonomy of entities.</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">Figure 4 shows an annotated
                              text using the Tagtog tool that shares the colour legend classes shown
                              at Figure 3:
                              <hi rend="italic" xml:space="preserve">iglesia de san Luis </hi>is a
                              (multiword) entity denoting a religious building; <hi rend="italic"
                                   >actores, coristas, bailarines</hi> are professions.</p>
                         <figure>
                              <table rend="rules">
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><figure>
                                                  <graphic n="1005" width="9.71cm" height="5.069cm"
                                                  url="media/image5.png" rend="block"/>
                                             </figure></cell>
                                        <cell style="text-align: left;" rend="Standard"><figure>
                                                  <graphic n="1006" width="4.514cm" height="5.625cm"
                                                  url="media/image6.png" rend="block"/>
                                             </figure></cell>
                                   </row>
                              </table>
                              <head>Figure 4. Tagtog tool: hardcopy of a manually annotated text
                                   (left) and an excerpt codified in the IOB format (right).</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">Finally, once all the
                              occurrences in the sub-corpus of historical newspapers are manually
                              annotated/tagged, the Tagtog tool provides different export formats,
                              including ann.json. After, the annotations output is converted into
                              the IOB format in order to train the machine learning models (as
                              described in the next section). To obtain the documents in IOB format,
                              a new script is designed to transform the Tagtog EntitiesTsv output
                              format into IOB format. At this moment the development of the deep
                              learning system can start (the so-called DM model).</p>
                    </div>
               </div>
               <div>
                    <head>Towards an automatic annotation model for CLARA-DM</head>
                    <p rend="Standard" style="text-align: justify;">Once we have some CLARA-DM
                         manually annotated (previously transcribed) newspapers (sub-corpus) with
                         their layout recognised, some experiments to perform a NER task
                         automatically reusing or transforming into a new one previously developed
                         models in similar annotation settings can be carried out. The question is
                         whether we can use one of the existing NER models or not, and how. </p>
                    <p rend="Standard" style="text-align: justify;">A brief explanation of the
                         methodology and terminology of the experiments is as follows:</p>
                    <list rend="bulleted">
                         <item>First, we experiment with the HIPE2020 dataset (notice that we can
                              use the terms corpus or dataset interchangeably, although the latter
                              has a more computational nuance). <anchor type="commentRangeStart"
                                   n="81"/>W<anchor type="commentRangeEnd" n="81"/><note
                                   place="comment" resp="#Autor" n="81"><date when=""/><hi
                                        style="font-family:Segoe UI">rephrase</hi></note>e seek to
                              notice evaluate the performance if there is a difference between the
                              use of monolingual and multilingual models (that arebeing the latter
                              larger and trained in several languages) <anchor
                                   type="commentRangeStart" n="88"/>at the level of
                                   performance<anchor type="commentRangeEnd" n="88"/><note
                                   place="comment" resp="#Autor" n="88"><date when=""/><hi
                                        style="font-family:Segoe UI">??</hi></note>, and to see if
                              knowledge transfers between languages, through monolingual and/or
                              multilingual models in a fine-tuning setup. Then we look at knowledge
                              transfer between tasks, that is, we evaluate whether it is beneficial
                              for a different task to use models trained with datasets for the
                              general NER task (and therefore have different labels than
                              HIPE2020).</item>
                         <item><anchor type="commentRangeStart" n="90"/>SecondlyIn a similar
                              approach, we then experiment with the CLARA-DM dataset, with a <anchor
                                   type="commentRangeStart" n="95"/>similar target<anchor
                                   type="commentRangeEnd" n="95"/><note place="comment"
                                   resp="#Autor" n="95"><date when=""/><hi
                                        style="font-family:Segoe UI">Not clear</hi></note>. First,
                              we carry out some experiments without training the selected models
                              (zero-shot experiments)we carry out some zero-shot experiments, that
                              is, <anchor type="commentRangeEnd" n="90"/><hi
                                   rend="annotation_reference"><note place="comment" resp="#Autor"
                                        n="90"><date when=""/>He añadido una mínima modificación "
                                        without training the selected models (zero-shot
                                        experiments)"</note></hi>Wwe evaluate on CLARA-DM dataset
                              some models trained for general tags in a NER task or for NER on
                              historical newspapers, and compare which setup achieves better
                              results. After that, we train models with the CLARA-DM dataset and see
                              if adding more historical training data improves the results. We
                              include a preliminary qualitative error analysis on the results
                              obtained for this first evaluation step based on HIPE2020 and CLARA-DM
                              datasets.</item>
                         <item>Then we carry out a <anchor type="commentRangeStart" n="103"
                                   />second<anchor type="commentRangeEnd" n="103"/><note
                                   place="comment" resp="#Autor" n="103"><date when=""/><hi
                                        style="font-family:Segoe UI">third?</hi></note> evaluation
                              step, in which we evaluate several aspects. The first one is to study
                              whether the method of adjudication for the final version of the
                              manually annotated documents plays a role in the performance of the
                              models (that is, when there are several annotators, there are
                              different versions of the annotations and it is necessary to decide
                              which label is the final one). The second one is a measurement of the
                              performance based on the development of the annotation guideline
                              versions, that is, the way in which the documents are annotated, and
                              the availability of more documents annotated with the latest
                              guidelines to see the gain in performance.</item>
                    </list>
                    <p rend="Standard" style="text-align: justify;">All the previous steps imply the
                         selection of different available DL models to decide justifiably if we have
                         to develop our own model, as we did for transcription.</p>
                    <p rend="Standard" style="text-align: justify;">The models used for the
                         experiments are based on RoBERTa (monolingual) (Liu et al., 2019) and
                              XLM-RoBERTa<note place="foot" xml:id="ftn20" n="20"><p
                                   rend="footnote text"><ref
                                        target="https://huggingface.co/xlm-roberta-base"><hi
                                             rend="color(00000A)"
                                             >https://huggingface.co/xlm-roberta-base</hi></ref></p></note>
                         (multilingual) (Conneau et al., 2020). Among the monolingual models we
                         experiment with:</p>
                    <list rend="numbered">
                         <item>two Spanish: RoBERTa-BNE<note place="foot" xml:id="ftn21" n="21"><p
                                        rend="footnote text"><ref
                                             target="https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne</hi></ref></p></note>
                              from the MarIA project (Gutiérrez-Fandiño et al., 2022) and
                                   BERTin-RoBERTa<note place="foot" xml:id="ftn22" n="22"><p
                                        rend="footnote text"><ref
                                             target="https://huggingface.co/bertin-project/bertin-roberta-base-spanish"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/bertin-project/bertin-roberta-base-spanish</hi></ref></p></note>
                              from the BERTin project (de la Rosa et al., 2022),</item>
                         <item>one English: DistilRoBERTa<note place="foot" xml:id="ftn23" n="23"><p
                                        rend="footnote text"><ref
                                             target="https://huggingface.co/distilroberta-base"><hi
                                                  rend="color(00000A)"
                                                  >https://huggingface.co/distilroberta-base</hi></ref></p></note>
                              (Sanh et al, 2019),</item>
                         <item>one French: DistilCamemBERT<note place="foot" xml:id="ftn24" n="24"
                                        ><p rend="footnote text"><ref
                                             target="https://huggingface.co/cmarkea/distilcamembert-base"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/cmarkea/distilcamembert-base</hi></ref></p></note>
                              (Delestre and Amar, 2022) and</item>
                         <item>one German: GottBERT<note place="foot" xml:id="ftn25" n="25"><p
                                        rend="footnote text"><ref
                                             target="https://huggingface.co/uklfr/gottbert-base"><hi
                                                  rend="color(00000A)"
                                                  >https://huggingface.co/uklfr/gottbert-base</hi></ref></p></note>
                              (Scheible et al., 2020).</item>
                    </list>
                    <p rend="Standard" style="text-align: justify;">On the other hand, we use models
                         that have been trained for a general set of NER tags:</p>
                    <list rend="numbered">
                         <item>one monolingual for Spanish: RoBERTa-BNE-NER-CAPITEL<note
                                   place="foot" xml:id="ftn26" n="26"><p rend="footnote text"><ref
                                             target="https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-capitel-ner"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-capitel-ner</hi></ref></p></note>,</item>
                         <item>and two multilingual ones, one for Spanish
                                   (XLM-RoBERTa-NER-Spanish<note place="foot" xml:id="ftn27" n="27"
                                        ><p rend="footnote text"><ref
                                             target="https://huggingface.co/MMG/xlm-roberta-large-ner-spanish"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/MMG/xlm-roberta-large-ner-spanish</hi></ref></p></note>)
                              and another one trained in 10 languages with high resources
                                   (XLM-RoBERTa-NER-HRL<note place="foot" xml:id="ftn28" n="28"><p
                                        rend="footnote text"><ref
                                             target="https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl"
                                                  ><hi rend="color(00000A)"
                                                  >https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl</hi></ref></p></note>).</item>
                    </list>
                    <p rend="Standard" style="text-align: justify;">The working environment is a
                         Google Colaboratory notebook, which provides a NVIDIA Tesla T4 GPU with
                         16GB of RAM and CUDA version 11.2. In addition, Transformers 4.11.3,
                         Datasets 1.16.1, HuggingFace Tokenizers 0.10.3 and Pytorch 1.12.1+cu113
                         libraries are installed for running the experiments.</p>
                    <p rend="Standard" style="text-align: justify;">In the following sub-sections
                         the subsequent experiments and related analysis of the results are
                         described:</p>
                    <p rend="Standard" style="text-align: justify;">1) Using the HIPE2020 dataset,
                         that contains different multilingual sub-corpus and entities annotated with
                         specific tags, different from the general ones. Two different strategies
                         are studied. The first one is the fine-tuning to observe both whether the
                         monolingual training in French and German transfers to English, and if the
                         multilingual model trained only with French or German improves in other
                         languages not trained with. The second one is the evaluation of the
                         transfer of knowledge from models using general tags to models with a
                         different set of tags.</p>
                    <p rend="Standard" style="text-align: justify;">2) As the CLARA-DM dataset uses
                         its specific set of tags, different from the HIPE2020 ones, two strategies
                         are used for the first set of experiments: on the one hand, the use of
                         models trained with external NER datasets (generalist or specific) on a
                         zero-shot setup, and on the other hand, training with the CLARA-DM labelled
                         data on a few-shot learning set-up. Some experiments use the CAPITEL
                              dataset<note place="foot" xml:id="ftn29" n="29"><p
                                   rend="footnote text"><ref
                                        target="https://sites.google.com/view/capitel2020"><hi
                                             rend="color(00000A)"
                                             >https://sites.google.com/view/capitel2020</hi></ref></p></note>
                         from IberLEF2020 (the task is a general NER for Spanish).</p>
                    <p rend="Standard" style="text-align: justify;">3) After the discussion and
                         conclusions on the first evaluation step, a new step for experiments is
                         planned in order to evaluate (a) the method of adjudicating the final
                         version of the manually annotated newspapers, (b) different aspects of the
                         annotation guidelines (the way of annotating the classes and the total
                         amount of tags), and (c) the amount of training data.</p>
                    <div>
                         <head>Experiments with HIPE2020</head>
                         <p rend="Standard" style="text-align: justify;">The HIPE2020 (<hi
                                   rend="italic">Identifying Historical People, Places and other
                                   Entities</hi>) competitive event held at the CLEF conference,
                              shares several objectives with the work presented as it focuses on the
                              evaluation of NLP, information extraction and information retrieval
                              systems. The HIPE2020 corpus (Ehrmann et al., 2020b) made available
                              for experimentation in this competition is a collection of digitized
                              historical documents in three languages: English, French and German.
                              The documents come from the archives of different Swiss, Luxembourg
                              and American newspapers. The dataset was annotated following the HIPE
                              annotation guidelines (Ehrmann et al., 2020c), which in turn was
                              derived from the Quaero annotation guidelines (Rosset et al., 2011).
                              The corpus uses the IOB format, providing training, test and
                              validation sets for French and German, and no training corpus for
                              English. The goal was to gain new insights and prospectives into the
                              transferability of entity recognition approaches across languages,
                              time periods, document types, and annotation tag sets.</p>
                         <p rend="Standard" style="text-align: justify;">The HIPE2020 corpus is
                              annotated with the labels of Person, Place, Organization, Time, and
                              Human Production. It contains 185 German documents totalling 149,856
                              tokens, 126 English documents totalling 45,695 tokens, and 244 French
                              documents with 245,026 tokens. In total, they make up a corpus of 555
                              documents and 440,577 tokens. The dataset is pre-processed to recover
                              the phrases that make up the documents and to be able to pass them to
                              the models together with the labels, obtaining a total of 7,887
                              phrases in French (of which 5,334 correspond to the training -166,217
                              tokens-, 1,186 to validation and 1,367 to test), 5,462 sentences in
                              German (of which 3,185 correspond to training -86,444 tokens-, 1,136
                              to validation and 1,141 to test), and 1,437 sentences in English (938
                              in validation and 499 in test). Note that the French training set is
                              considerably larger than the German training set.</p>
                         <div>
                              <head>Fine-tuning</head>
                              <p rend="Standard" style="text-align: justify;">The first experiments
                                   consist of fine-tuning on the HIPE2020 dataset. When training a
                                   machine learning model there are several hyperparameters to be
                                   configured. The “number of epochs” is the number of times that
                                   the algorithm is going through the whole training dataset. The
                                   “batch size” is the number of training examples (in this case,
                                   sentences) used in one iteration. And the “learning rate”
                                   determines the pace at which an algorithm updates or learns the
                                   values of the parameters. The models’ hyperparameters are
                                   configured for a training in 3 epochs and a batch size of 12 in
                                   both the training and validation set, and a 5e-5 learning rate.
                                   The rest of the model configuration is set by default using the
                                   AutoConfig, AutoModel and AutoTokenizer classes of the
                                   Huggingface Transformers library. First, we fine-tune three
                                   monolingual models, which are shown in the first three rows of
                                   Table 2, and then the multilingual model, whose results are shown
                                   in the last three rows. In both cases we train first with the
                                   French dataset, then with the German dataset, and thirdly with
                                   French and German jointly, since the English sub-corpus has no
                                   training dataset.</p>
                              <p rend="Standard" style="text-align: justify;">The evaluation metrics
                                   are based on precision, recall and F1. Briefly explained,
                                   precision is the relationship (fraction) of relevant instances
                                   among the retrieved instances, whilst recall is the fraction of
                                   relevant instances that were retrieved. The F1 measure is the
                                   harmonic mean of the precision and recall.</p>
                              <p rend="Standard" style="text-align: justify;">The objective of the
                                   first experiment is to analyse whether the knowledge learned on
                                   the NER training with the historical texts in French and German
                                   transfers to English. Secondly, whether the multilingual training
                                   with one language improves the performance in the other
                                   languages. We find that both claims hold true. The performance
                                   annotating the English sub-corpus of a model trained jointly in
                                   French and German improves compared to the performance of the
                                   models trained only with French or German (both when using the
                                   monolingual English model and the multilingual model) as shown
                                   with the results in the third row, that are better than the ones
                                   in the first and second rows, as well as the results in the sixth
                                   row, that are better than those in the fourth and fifth ones.
                                   Also, when training the multilingual model only with French or
                                   German, the result improves in the languages in which it has not
                                   been trained. For example, when training DistilCamemBERT with the
                                   French sub-corpus, the F1 in the German sub-corpus is 0.19,
                                   whilst when training XLM-RoBERTa with French, the F1 in German is
                                   0.63.</p>
                              <p rend="Standard" style="text-align: justify;">Moreover, it is
                                   noteworthy that the multilingual model manages to equal or even
                                   improve the results of the monolingual models.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="3"
                                                  rend="Standard">FR</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="3"
                                                  rend="Standard">DE</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="2"
                                                  rend="Standard">EN</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;"/>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >DistilCamemBERT-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: left;" rend="Standard"/>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >0.74</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.8</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.77</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.13</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.36</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.19</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.38</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >GottBERT-<hi rend="bold">de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.28</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.38</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.32</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.69</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.75</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.72</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.4</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.52</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >DistilRoBERTa-<hi rend="bold">fr+de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.66</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.75</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.7</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.59</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.4</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.6</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.76</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.8</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.78</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.72</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.53</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.61</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.61</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.68</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.65</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.69</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.75</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.72</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.54</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr+de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.76</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.8</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.78</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.75</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.76</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.76</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.59</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.62</hi></cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref112846597"/>Table 2. Experiments with
                                        monolingual and multilingual models on French, German and
                                        English HIPE2020 datasets.</head>
                              </figure>
                         </div>
                         <div>
                              <head>Transfer of knowledge with general NER datasets</head>
                              <p rend="Standard" style="text-align: justify;">In view of the
                                   usefulness of the multilingual model in the previous results, in
                                   the following experiments we use the multilingual model trained
                                   for NER in 10 languages with high resources.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="3"
                                                  rend="Standard"><hi style="font-size:10pt"
                                                  >FR</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="3"
                                                  rend="Standard"><hi style="font-size:10pt"
                                                  >DE</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" cols="2"
                                                  rend="Standard"><hi style="font-size:10pt"
                                                  >EN</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;"/>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">P</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">R</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">F1</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">P</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">R</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">F1</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">P</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">R</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"><anchor
                                                  xml:id="Hlk114329206"/>XLM-R-ner-<anchor
                                                  xml:id="Hlk114329571"/>hrl</cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.54</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.6</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.56</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.53</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.56</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.54</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.46</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.54</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-ner-hrl-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.77</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.82</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.79</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.67</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.7</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.68</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.56</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.63</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-ner-hrl-<hi rend="bold">de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.71</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.73</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.72</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.73</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.77</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.75</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.64</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.57</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-ner-hrl-<hi rend="bold">fr+de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.78</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.68</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt">0.8</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.76</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt">0.8</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.78</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:10pt">0.6</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold" style="font-size:10pt"
                                                  >0.68</hi></cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref113275975"/>Table 3. Evaluation on
                                        HIPE2020 of the model XLM-RoBERTa trained on 10 high
                                        resource languages for NER.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">The model is first
                                   evaluated on HIPE2020 without training (the so-called zero-shot
                                   learning), which is shown in the first row of the table. Then the
                                   model is trained with the HIPE2020 datasets, first French, then
                                   German and finally with both. Briefly, the zero-shot transfer
                                   learning means that we are taking a model trained for a specific
                                   or general task, and directly applying it on a different task or
                                   dataset for which the model has not been trained.</p>
                              <p rend="Standard" style="text-align: justify;">The results are
                                   slightly better than those obtained in the previous experiments,
                                   but in return we are evaluating on a general set of labels
                                   (Person, Location, Organization, and Dates), different from the
                                   one that HIPE2020 uses.</p>
                         </div>
                    </div>
                    <div>
                         <head>Experiments with CLARA-DM</head>
                         <p rend="Standard" style="text-align: justify;">With the insights we have
                              extracted from the results of previous experiments, we move on to
                              carrying out experiments with our developed dataset. To carry out
                              these experiments we have the manually annotated sub-corpus of 5
                              newspapers, which have been obtained from the annotations of between 3
                              and 4 annotators and merged using the majority vote method. After a
                              pre-processing phase carried out using the spaCy<note place="foot"
                                   xml:id="ftn30" n="30"><p rend="footnote text"><ref
                                             target="https://spacy.io"><hi rend="color(00000A)"
                                                  >https://spacy.io</hi></ref></p></note> package to
                              delimit the sentences that make up the newspapers, a dataset of 928
                              sentences is obtained, with a total of 15,145 tokens. The annotation
                              guidelines were in a preliminary version, and the inter-annotator
                              agreement was still to be improved. Therefore, at this point we tackle
                              different experiments.</p>
                         <p rend="Standard" style="text-align: justify;">The CLARA-DM dataset has a
                              large and original set of labels. This implies that, in order to
                              obtain a specific NER model for the dataset, it will be necessary to
                              have enough training data. We will adopt two strategies to carry out
                              the first experiments, on the one hand, making use of models trained
                              with external NER datasets (generalist or specific), and on the other
                              hand, training with the available labelled data.</p>
                         <p rend="Standard" style="text-align: justify;">The set of labels of the
                              dataset is extensive: it includes the generic labels of person, place,
                              establishment, profession, ornaments, furniture, sales and losses or
                              findings, and also the sub-labels person_lords (for nobles, high
                              officials, etc), place_address (for streets, squares, gateways),
                              place_religious (convents, parishes), place_hospital, place_college
                              and place_fountain. In total, they make up a set of 14 tags, which
                              when duplicated in the IOB format and together with the empty tag 'O',
                              add up to a total of 29 tags. This increases the complexity for the
                              models to learn, and therefore the need for sufficient training data.
                              On the other hand, in order to apply zero-shot learning, the names of
                              the labels must be changed and simplified so that they are the same as
                              those of the training datasets of the models, thus losing the more
                              specific labels (such as religious places, ornaments or objects for
                              sale) and drastically reducing the size of the set of labels, with the
                              loss of information and efforts made during the labelling process that
                              this entails.</p>
                         <figure>
                              <table rend="rules">
                                   <row>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">PER</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >SEÑOR</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">LOC</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >RELIG</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >DIREC</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">COLE</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">HOSP</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">PROF</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >ESTABLEC</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >VENTA</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt"
                                             >PÉRDIDA</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:6pt">ADOR</hi></cell>
                                        <cell style="text-align: left;" rend="Standard"><hi
                                                  rend="bold" style="font-size:8pt"
                                             >Total</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">347</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">49</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">78</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">28</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">112</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">1</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">2</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">89</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">81</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">32</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">14</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">4</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  style="font-size:8pt">837</hi></cell>
                                   </row>
                              </table>
                              <head><anchor xml:id="Ref114848441"/><anchor xml:id="Ref114848431"
                                   />Table 4. Label distribution in CLARA-DM.</head>
                         </figure>
                         <p rend="Standard" style="text-align: justify;">The distribution of the
                              tags in these documents is shown in Table 4. The class with by far the
                              most examples is person, followed by address, profession,
                              establishment, place, and lords. The classes of religious places,
                              losses, schools, hospitals, and ornaments are notably
                              underrepresented, and those of fountains and furniture do not even
                              appear in the dataset (because they might not have been annotated by
                              at least two people and therefore do not appear in the final
                              version).</p>
                         <p rend="Standard" style="text-align: justify;">In the following we
                              describe the experiments carried out to study the benefits of the
                              transfer of knowledge between tasks and languages on our CLARA-DM
                              dataset.</p>
                         <div>
                              <head>Zero-shot using CLARA-DM as test set</head>
                              <p rend="Standard" style="text-align: justify;">In these first two
                                   experiments we are not training with CLARA-DM but using it as a
                                   test set. This means that we are not evaluating on the set of
                                   labels of CLARA-DM but on the ones of the datasets that the
                                   models have been trained on. In Table 5 we evaluate two models
                                   trained for NER in Spanish, one multilingual and one monolingual.
                                   In Table 6, models trained with HIPE2020 (specific labels in
                                   historical newspapers) or with the CAPITEL dataset from
                                   IberLEF2020 (general NER for Spanish) are evaluated in
                                   CLARA-DM.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-ner-spanish</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.39</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.48</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.43</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-ner-capitel</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.43</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.53</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.48</hi></cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref115089864"/>Table 5. Evaluation on
                                        CLARA-DM of general NER models for Spanish.</head>
                              </figure>
                              <figure>
                                   <head><anchor xml:id="Ref115089864"/>Table 5. Evaluation on
                                        CLARA-DM of general NER models for Spanish.</head>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.43</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.54</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.48</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr-de</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.44</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.49</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr-de-en</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.51</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.48</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.47</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.51</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-ner-capitel-<hi rend="bold"
                                                  >fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.47</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >BERTin-<hi rend="bold">fr</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.42</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.6</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.5</cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref113975122"/>Table 6. Evaluation on
                                        CLARA-DM with models trained with HIPE2020.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">We find that in both
                                   cases the monolingual models are slightly better. In the case of
                                   training with HIPE2020, it turns out to be more beneficial to
                                   train only with French, than to add English and German, since it
                                   is the most similar language to Spanish. Moreover, it is better
                                   to train with the French sub-corpus of HIPE2020, that tackles the
                                   same task (NER in historical newspapers), than training with
                                   CAPITEL, which tackles general NER for Spanish, so the task
                                   influences the model.</p>
                         </div>
                         <div>
                              <head>Few-shot/fine-tuning on CLARA-DM</head>
                              <p rend="Standard" style="text-align: justify;">In these experiments
                                   we train with the few data we have manually annotated (that is
                                   why the experiments are few-shot learning and not zero-shot
                                   learning) using 3 newspapers as training, 1 as validation and 1
                                   as test (containing approximately 700 sentences for training, 120
                                   for validation and 110 for test). On Table 7 the results of
                                   fine-tuning several models are shown, the best one being the
                                   Spanish monolingual BERTin model.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.41</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.52</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.42</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.50</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.46</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >BERTin-R-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.48</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.58</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.52</hi></cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref114418419"/>Table 7. Fine-tuning on
                                        CLARA-DM.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">Even if we consider
                                   the corpus very small in comparison with the HIPE2020 one, (700
                                   sentences versus 7,900 in the French sub-corpus) it turns out
                                   that with only 3 newspapers for training, similar results are
                                   achieved to those of directly evaluating the models trained with
                                   HIPE2020 shown in Table 6.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr-clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.59</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.64</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.61</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-<hi rend="bold">fr-clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.53</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.61</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.57</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-ner-capitel-<hi rend="bold"
                                                  >clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.54</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.59</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.57</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >RoBERTa-bne-ner-capitel-<hi rend="bold"
                                                  >fr-clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.55</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.57</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >BERTin-R-<hi rend="bold">fr-clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.54</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.68</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.6</cell>
                                        </row>
                                   </table>
                                   <head>Table 8. Training and evaluation in CLARA-DM of models
                                        trained with HIPE2020 and CAPITEL.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">Lastly, in Table 8 the
                                   training with CLARA-DM has been combined with training with
                                   HIPE2020 (only the French part) and CAPITEL. Again, we have
                                   obtained the best results of all experiments until now, but at
                                   the cost of evaluating on a different set of labels than that of
                                   CLARA-DM and therefore wasting the annotation effort.</p>
                              <p rend="Standard" style="text-align: justify;">This is the only case
                                   in which the performance of monolingual and multilingual models
                                   are very similar. Here it is interesting to note that, again, it
                                   gives better results to train with the French HIPE2020, which
                                   contains historical newspapers, than with CAPITEL, which is
                                   Spanish for generic NER in general domains. CAPITEL<note
                                        place="foot" xml:id="ftn31" n="31"><p rend="footnote text">
                                             https://plantl.mineco.gob.es/tecnologias-lenguaje/comunicacion-formacion/eventos/eventosinfoday2019/Aspectos%20destacados%20del%20Plan%20TL/corpus-anotado-Jordi-Porta.pdf</p></note>
                                   contains texts after 2005 on the following topics: Science and
                                   technology; Social sciences, beliefs and thought; Politics,
                                   economy and justice; Arts culture and shows; Current affairs,
                                   leisure and daily life; Health and Others.</p>
                              <p rend="Standard" style="text-align: justify;">At this point, it is
                                   worth noting that the labels that each model had in its (first)
                                   training in each experiment are as follows:</p>
                              <list rend="bulleted">
                                   <item>in Table 5, the tags for the experiment XLM-R-ner-spanish
                                        are general person, location, organization, and
                                        miscellaneous, and those of the experiment
                                        RoBERTa-bne-ner-capitel are those of CAPITEL, that is
                                        person, location, organization and others (in BIOES format
                                        instead of BIO),</item>
                                   <item>in Table 6 all the experiments have the labels of HIPE2020,
                                        except for RoBERTa-bne-ner-capitel-fr, which has those of
                                        CAPITEL,</item>
                                   <item>in Table 7 the labels are those of CLARA-DM and</item>
                                   <item>in Table 8 the labels are those of HIPE2020 or CAPITEL,
                                        whichever comes first.</item>
                              </list>
                              <p rend="Standard" style="text-align: justify;">Since we have seen
                                   that models trained with a different set of labels (HIPE2020 or
                                   CAPITEL ones) are able to predict with some quality the labels
                                   that they have in common with the CLARA-DM dataset, we can do the
                                   opposite experiment. In order not to lose the wide range of
                                   labels in CLARA-DM, we can first train the models adding
                                   fictitious tags, so that in the second training with CLARA-DM we
                                   include all the classes.</p>
                              <p rend="Standard" style="text-align: justify;">For example, regarding
                                   the XLM-RoBERTa model: when fine-tuning it first with French
                                   HIPE2020 and after with CLARA-DM, we obtained metrics of around
                                   60% (first row of Table 8), but we were evaluating only on the
                                   tags Person, Place and Organization, which are the ones HIPE2020
                                   has in common with the CLARA-DM corpus. If we change the classes
                                   in the first training with HIPE2020 by adding the ones present in
                                   CLARA-DM, and fine-tune first with HIPE2020 and then with
                                   CLARA-DM, we get the metrics shown in Table 9. Performance drops
                                   by 14% on average, but in return we are evaluating the corpus on
                                   the whole CLARA-DM tagset, with a model trained with both
                                   CLARA-DM and HIPE2020.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">fr</hi>-<hi rend="bold"
                                                  >clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.44</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.51</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.47</cell>
                                        </row>
                                   </table>
                                   <head>Table 9. Evaluation on CLARA-DM of a model trained first
                                        with HIPE and then with CLARA-DM, with the tags of CLARA-DM
                                        corpus.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">Comparing this
                                   performance with that obtained when only training with CLARA-DM
                                   (Table 7) (since here we are training with both HIPE2020 and
                                   CLARA-DM), the results are quite similar, so apparently there is
                                   no real added value when adding the first training with HIPE2020,
                                   that is, using more resources.</p>
                         </div>
                         <div>
                              <head>Qualitative analysis</head>
                              <p rend="Standard" style="text-align: justify;">As a preliminary
                                   qualitative analysis, or error analysis, Table 10 shows the
                                   performance per label of the best model fine-tuned with CLARA-DM,
                                   which was BERTin (Table 7).</p>
                              <p rend="Standard" style="text-align: justify;">It is interesting to
                                   note that the results are consistent with the current state of
                                   the annotation guidelines, since entities such as persons,
                                   locations and religious places have a high degree of
                                   inter-annotator agreement, above 70%, being those that obtain the
                                   best metrics, while others such as establishments or objects for
                                   sale still need to be revised through the guideline and the model
                                   also has a harder time identifying them correctly. This seems to
                                   be even more relevant than the inner imbalance of the dataset,
                                   since classes such as religious places do not have many
                                   appearances or occurrences, but the model recognises them with a
                                   high degree of accuracy.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >support</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >Establishment</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >establec</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.23</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >0.31</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.26</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >29</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Place
                                                  or Location</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >loc</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.79</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.71</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.75</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >21</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Place -
                                                  College</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >loc_cole</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Place -
                                                  Address</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >loc_direc</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.42</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.78</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="underline">0.55</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >23</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Place -
                                                  Religious</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >loc_relig</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.80</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.67</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.73</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >6</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Losses
                                                  or Findigs</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >perdida_hallazgo</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >0</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >Person</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >pers</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.53</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >1.00</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.70</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >8</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Person
                                                  - Lords</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >pers_señores</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.56</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.64</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="underline">0.60</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >14</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Trades
                                                  and Professions</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >prof</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.61</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.67</cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="underline">0.64</hi></cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >21</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >Sales</cell>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >venta</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.50</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.08</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.14</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >12</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"/>
                                             <cell style="text-align: left;" rend="Standard">Micro
                                                  avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.48</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.58</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.52</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >135</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"/>
                                             <cell style="text-align: left;" rend="Standard">Macro
                                                  avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.44</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.49</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.44</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >135</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"/>
                                             <cell style="text-align: left;" rend="Standard"
                                                  >Weighted avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.51</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.58</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.51</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >135</cell>
                                        </row>
                                   </table>
                                   <head><anchor xml:id="Ref115028226"/>Tabla 10. Metrics of each
                                        label with BERTin model fine-tuned on CLARA-DM.</head>
                              </figure>
                         </div>
                    </div>
                    <div>
                         <head>First evaluation step discussion</head>
                         <p rend="Standard" style="text-align: justify;">In the experiments with
                              HIPE2020 we have observed that the use of multilingual models is
                              beneficial when we have datasets in several languages for the NER
                              task, as they allow the knowledge to be transferred to languages with
                              fewer or no resources. Furthermore, we have seen that including
                              domain-generic datasets slightly improves the results, but at the cost
                              of evaluating on a different set of labels, and therefore wasting the
                              efforts of the tagging procedure.</p>
                         <p rend="Standard" style="text-align: justify;">In spite of not having a
                              particularly robust or sophisticated model, and although very precise
                              results were not one main goal of the work, results of around 80% on
                              the F1 measure for French and German, and 65% for English, which has
                              no training data, have been achieved (previous Section 3.1).</p>
                         <p rend="Standard" style="text-align: justify;">As regards the experiments
                              with CLARA-DM, in general, better results have been obtained with the
                              monolingual models in Spanish, except when we have trained jointly
                              with CLARA-DM and HIPE2020 datasets, in which the multilingual model
                              has been on a par with the monolingual ones. It has also been shown
                              that the similarity between Spanish and French favours the transfer of
                              knowledge within the same domain, and that this transfer is even
                              better than training with a generic NER dataset in the same
                              language.</p>
                         <p rend="Standard" style="text-align: justify;">An important conclusion for
                              corpora of languages or domains with scarce resources has also been
                              initially contrasted: the importance of the inter-annotator agreement
                              over the dataset imbalance.</p>
                         <p rend="Standard" style="text-align: justify;">In addition, as with three
                              annotated newspapers, the results of fine-tuning with CLARA-DM achieve
                              similar results than evaluating in CLARA-DM a model trained on
                              HIPE2020, even if, the joint training with HIPE2020 and CLARA-DM has
                              not given rise to a great improvement in results. That is shown in
                              Table 6 (zero-shot) were models trained with HIPE2020 are evaluated in
                              CLARA-DM with results of around 50% F1, while in Table 7 (fine-tuning
                              in CLARA-DM) a 50% F1 is also achieved just by training with only 3
                              newspapers. So, it is for sure that with more annotated documents,
                              good results can be expected.</p>
                         <p rend="Standard" style="text-align: justify;">The results are susceptible
                              to improvement, since the quality of the annotation guidelines is
                              still to be enhanced, and so the inter-annotator agreement, which will
                              lead to have more quality and homogeneous data.</p>
                         <p rend="Standard" style="text-align: justify;">From this analysis, the
                              plan for the second evaluation step is to try to improve the models
                              obtained in this first evaluation step, and to measure the gain in
                              performance, once we have more robust annotation guidelines, and more
                              annotated newspapers. As will be described in the next section, the
                              experiments are based on the three models used for fine-tuning in
                              CLARA-DM (Table 7) since it has been shown that training with more
                              added datasets is not so beneficial, but it is better to have more
                              data in CLARA-DM.</p>
                    </div>
                    <div>
                         <head>Second evaluation step</head>
                         <p rend="Standard" style="text-align: justify;">In order to confirm the
                              previous results, in this series of experiments the following
                              parameters are evaluated: the method of adjudicating the final version
                              of the manually annotated newspapers, aspects of the annotation
                              guidelines (the way of annotating the classes and the total amount of
                              tags), and the amount of training data.</p>
                         <p rend="Standard" style="text-align: justify;">In Tagtog, when several
                              users annotate the same document, as a result, there are different
                              annotation versions. Adjudication is the process of resolving
                              inconsistencies between these versions before promoting a version to
                              master (final version). In other words, the different annotators’
                              versions are merged into one, (using various strategies). Adjudication
                              can either be manual (when a reviewer promotes a version to master) or
                                   automatic<note place="foot" xml:id="ftn32" n="32"><p
                                        rend="footnote text"><ref target="automatic-adjudication"
                                                  ><hi rend="color(00000A)"
                                                  >https://docs.tagtog.com/collaboration.html#automatic-adjudication</hi></ref></p></note>,
                              based on different adjudication methods such as the IAA (or Best
                              Annotators) or the Majority Vote. Automatic adjudication based on Best
                              Annotators means that for each single annotation task, the annotations
                              of the user with the best IAA are promoted to master. The goal is to
                              have the best annotations available for each annotation task in master
                              version. Furthermore, automatic adjudication by Majority Vote means
                              that for each single annotation, it is promoted to master only if it
                              was annotated by over 50% of the annotators.</p>
                         <p rend="Standard" style="text-align: justify;">First, an experiment is
                              carried out with the same documents as before but obtained with a
                              different adjudication method. While in the first experiments the
                                   <anchor xml:id="Int_bqLZpX9M"/>final version was obtained by the
                              Majority Vote method, in this case the Best Annotators method is
                              used.</p>
                         <p rend="Standard" style="text-align: justify;">Then, the progress of the
                              annotation guidelines is evaluated, as well as the gain in performance
                              with a bigger number of annotated newspapers.</p>
                         <p rend="Standard" style="text-align: justify;">In these experiments we
                              will limit ourselves to carrying out experiments exclusively with the
                              CLARA-DM dataset (not HIPE2020, CAPITEL, etc) and with the models used
                              in the previous few-shot experiments. The newspapers to be used will
                              be those of the first experiments <anchor xml:id="Int_83fIJShu"/>and
                              also new annotated newspapers.</p>
                         <div>
                              <head>Evaluation of the adjudication method</head>
                              <p rend="Standard" style="text-align: justify;">By carrying out the
                                   experiments in Table 7, that is, fine-tuning with CLARA-DM, but
                                   instead with the final annotations obtained by the Best
                                   Annotators method, we get the results shown in Table 11.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >XLM-R-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.47</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.53</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.50</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >RoBERTa-bne-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.49</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.55</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold">0.52</hi></cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >BERTin-R-<hi rend="bold">clara</hi></cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.37</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.47</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.41</cell>
                                        </row>
                                   </table>
                                   <head>Table 11. Fine-tuning with CLARA-DM (the same experiments
                                        as in Table 7), but with final version of the annotations
                                        obtained with the Best Annotators method.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">With RoBERTa-BNE model
                                   the F1 measure improves from 0.46 to 0.52, accuracy from 0.42 to
                                   0.49, and recall from 0.50 to 0.55. And with XLM-RoBERTa, the F1
                                   measure improves from 0.46 to 0.5, precision from 0.41 to 0.47
                                   and recall from 0.52 to 0.53. However, with the BERTin model,
                                   which achieved the best results in Table 7, the F1 measure has
                                   decreased from 0.52 to 0.41, accuracy from 0.48 to 0.37, and
                                   recall from 0.58 to 0.47.</p>
                              <p rend="Standard">That is, by changing the adjudication method, the
                                   best performing method has changed, even though the F1 measure of
                                   0.52 is still not surpassed.</p>
                         </div>
                         <div>
                              <head>Progress of annotation guidelines and availability of more
                                   training data</head>
                              <p rend="Standard" style="text-align: justify;">The annotation
                                   guidelines are adjusted in several turns, by analysing both the
                                   Inter Annotator Agreement and the performance of the models.</p>
                              <p rend="Standard" style="text-align: justify;">Eleven new newspapers
                                   were annotated in accordance with the new guidelines. In
                                   particular, the place_fountains entity is deleted, since we
                                   consider it from now on included within the furniture. Also,
                                   place_college and place_hospital tags are deleted (and included
                                   in establishments), since the three entities had very few
                                   mentions in the newspapers. Finally, the category Organization
                                   (administrative bodies) is created, to differentiate it from that
                                   of Establishments (commerce, leisure, services and others) and to
                                   be in line with other common annotation guidelines.</p>
                              <p rend="Standard" style="text-align: justify;">All in all, we get a
                                   set of 12 classes: two for people (person_general, person_lord)
                                   three for places (place_general, place_address, place_religious),
                                   establishments, organizations, professions, and four for objects
                                   (ornaments, furniture, sales, losses/findings), having the
                                   taxonomy shown in Figure 5.</p>
                              <figure>
                                   <graphic n="1007" width="9.163cm" height="9.818cm"
                                        url="media/image7.jpeg" rend="inline"/>
                                   <head>Figure5. Final label taxonomy for CLARA-DM.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">In intermediate steps,
                                   some different ways of labelling were evaluated. For example, at
                                   some point we agreed to annotate the profession of a person
                                   within the <hi rend="italic" xml:space="preserve">person </hi>tag
                                   whenever they appeared contiguously (as in <hi rend="italic">Sr.
                                        D. Josef de la Cruz y Loyola, Gobernador de dicho Real
                                        Sitio</hi>). However, this proves ambiguous, and we confirm
                                   that is it better to label more concrete and nuclear entities,
                                   since it is clearer for annotators, and thus improves the IAA,
                                   and in turn leads to better performance of the models on the
                                   classes with better IAA.</p>
                              <p rend="Standard" style="text-align: justify;">The results of
                                   fine-tuning the models with the 11 new annotated newspapers,
                                   annotated with the final guidelines and the final version
                                   obtained with the Best Annotators method, are shown in Table 12.
                                   In this case we used 7 newspapers for training, that contained
                                   1,228 sentences, which nearly doubles the number of sentences
                                   that we had for the first experiments.</p>
                              <table rend="rules">
                                   <row>
                                        <cell style="text-align: justify;" rend="Standard"/>
                                        <cell style="text-align: center;" rend="Standard">P</cell>
                                        <cell style="text-align: center;" rend="Standard">R</cell>
                                        <cell style="text-align: center;" rend="Standard">F1</cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: justify;" rend="Standard"
                                             >XLM-RoBERTa-clara</cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="color(212121)">0.74</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="color(212121)">0.79</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="color(212121)">0.76</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: justify;" rend="Standard"
                                             >RoBERTa-bne-clara</cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold color(212121)">0.75</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold color(212121)">0.80</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold color(212121)">0.78</hi></cell>
                                   </row>
                                   <row>
                                        <cell style="text-align: justify;" rend="Standard"
                                             >BERTin-R-clara</cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="bold color(212121)">0.75</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="color(212121)">0.77</hi></cell>
                                        <cell style="text-align: center;" rend="Standard"><hi
                                                  rend="color(212121)">0.76</hi></cell>
                                   </row>
                              </table>
                              <p rend="Standard" style="text-align: center;"><hi
                                        style="font-size:9pt">Table12. Training and evaluation in
                                        CLARA-DM with more newspapers and new guidelines.</hi></p>
                              <p rend="Standard" style="text-align: justify;">While the results of
                                   the first fine-tuning had a performance of around 50% in all the
                                   metrics (Table 7), with the updated annotation guidelines and
                                   double the number of sentences for the training, metrics of more
                                   than 75% have been achieved. It is also observed that when the
                                   IAA improves for a specific class, so the models get to predict
                                   it better, even when there are fewer examples of the class.</p>
                              <figure>
                                   <table rend="rules">
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >P</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >R</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >F1</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >support</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Places or Locations</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >loc</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.89</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.84</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.87</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >50</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Places – Streets and Squares</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >loc_direc</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.82</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.94</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.87</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >111</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Places – Religious Buildings</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >loc_relig</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.64</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.62</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >26</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Organizations, Institutions</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >org_adm</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.53</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.58</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >27</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Establisments</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >org_establec</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.67</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.60</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >50</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Persons</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >pers</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.81</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >1.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.89</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >216</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Persons - Lords</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >pers_señores</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.58</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.40</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.47</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >55</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Ornaments</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >prod_ador</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.80</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.57</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.67</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >7</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Furniture</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >prod_mobil</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.00</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >1</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard"><p
                                                  rend="Standard">Losses or findigs</p></cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >prod_perdida-hallazgo </cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.82</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.75</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.78</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >12</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Sales</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >prod_venta</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.62</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.57</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.59</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >14</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: left;" rend="Standard">Trades
                                                  and professions</cell>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >prof</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.66</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.67</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.66</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >78</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Micro avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.75</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.80</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.78</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >647</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Macro avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.65</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.63</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.64</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >647</cell>
                                        </row>
                                        <row>
                                             <cell style="text-align: justify;" rend="Standard"/>
                                             <cell style="text-align: justify;" rend="Standard"
                                                  >Weighted avg</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.74</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.80</cell>
                                             <cell style="text-align: center;" rend="Standard"
                                                  >0.77</cell>
                                             <cell style="text-align: right;" rend="Standard"
                                                  >647</cell>
                                        </row>
                                   </table>
                                   <head>Table 13. Metrics of each label with RoBERTa-BNE.</head>
                              </figure>
                              <p rend="Standard" style="text-align: justify;">If we take a look at
                                   the performance per entity class as in Table 13, one noticeable
                                   aspect is <anchor type="commentRangeStart" n="130"/>that entities
                                   that do not contain proper names are usually harder to
                                        predict<anchor type="commentRangeEnd" n="130"/><note
                                        place="comment" resp="#Autor" n="130"><date when=""/><hi
                                             style="font-family:Segoe UI">spacy span categorizer is
                                             a new hint for such kind of entities.</hi></note>, such
                                   as products or professions. We might consider tagging these
                                   making use of predefined lists instead of the NER model.
                                   Furthermore, sometimes there are very few occurrences of these
                                   classes (such is the case of the furniture in Table 13) and this
                                   affects the performance as a whole.</p>
                              <p rend="Standard" style="text-align: justify;"><anchor
                                        type="commentRangeStart" n="132"/>On the other hand, the
                                   results are also very conditioned by the choice of training and
                                   test data, <anchor type="commentRangeEnd" n="132"/><note
                                        place="comment" resp="#Autor" n="132"><date when=""/><hi
                                             style="font-family:Segoe UI">N-fold cross validation
                                             may help with that?</hi></note>since we do not yet have
                                   enough examples. Even so, it is shown that both stronger
                                   annotation guidelines and the availability of more documents
                                   improves the performance of the models.</p>
                         </div>
                    </div>
               </div>
               <div>
                    <head><anchor type="commentRangeStart" n="134"/>Summary of theProposed
                         methodologyConclusions and future work</head>
                    <list rend="numbered">
                         <item>As a summary of the methodology used in this work, the following is a
                              list of the necessary steps necessary to carry out a process of
                              recognition of named entities in historical documents.As a summary of
                              the methodology used in this work, the steps followed are listed
                              below. </item>
                    </list>
                    <list rend="bulleted">
                         <item><hi rend="bold">Digitization:</hi> The first step involves digitizing
                              the historical papers if they are not already in a digital format.
                              This can be done using scanners or digital cameras. This step was made
                              by BNE previously at this work.</item>
                         <item>The first step is to digitize the historical documents if they are
                              not already in a digital format. This can be done using scanners or
                              digital cameras. This step has already been carried out by the BNE in
                              this work.</item>
                         <item><hi rend="bold">Optical Character Recognition (OCR):</hi> Once the
                              documents have been digitized, OCR software is used to convert the
                              text-based images into machine-encoded text. This stage may involve
                              manual error correction to deal with the inaccuracies of the OCR
                              process, particularly with historical documents that may have faded or
                              smudged ink or unusual typography. As explained in previous sections,
                              we used Transkribus with a layout recognition model and trained our
                              own model in Transkribus to obtain the text.Once the documents are
                              digitized, OCR software is used to convert the images of text into
                              machine-encoded text. This stage may involve manual error correction
                              to handle the inaccuracies in the OCR process, especially with
                              historical documents which may have faded or smudged ink, or unusual
                              typography. As we explain in previous sections, we use Transkribus
                              using a layout model and training our own model to obtain the
                              text.</item>
                         <item><hi rend="bold">Annotation guidelines:</hi> The initial stage in
                              training a NER model involves establishing rules for accurate entity
                              identification. It's crucial to explicitly define the types of named
                              entities that the model is expected to recognize.The first step in the
                              training of a NER model is the definition of the rules for the correct
                              identification of an entity. It is important to clearly define what
                              types of named entities the model should recognize.the first step to
                              train a NER model is defined the rules to identify a entity
                              correctly.</item>
                         <item><hi rend="bold">Annotation:</hi> The entities to be recognized by the
                              NER process are then annotated. This involves marking up the text with
                              tags that indicate the type of entity. This is usually done manually
                              by human annotators, using annotation software. We used Tagtog in this
                              work.</item>
                         <item><hi rend="bold">Validation:</hi> The annotated text is then validated
                              to ensure the accuracy of the entity recognition and annotation
                              processes. This can involve a review by human annotators, or the use
                              of validation software that compares the annotated text to a gold
                              standard or benchmark. In this work, we used Tagtog for annotation and
                              validation.We used Tagtog in this work.</item>
                         <item><hi rend="bold">CreationTraining the model: of a Named Entity
                                   Recognition ResourcesModel:</hi> The final step is to train a
                              named entity recognition model from the annotated and validated text.
                              This involves the separation of the data into a training set, a
                              validation set and a test set. This is followed by the selection of
                              the most appropriate model and finally by training the model several
                              times with different hyper-parameters.Then the selection of the most
                              suitable type of model and finally the training of the model several
                              times with different hyper-parameters. .The final step is the creation
                              of named entity resourcesmodel from the annotated and validated text.
                              This include,: split the data into train, validation and test set.
                              Then select the appropriate kind of model and finally train the model
                              several times with different hyperparameters.This could involve
                              creating a database or corpus of named entities, or integrating the
                              named entities into an existing resource.</item>
                         <item><hi rend="bold">Evaluation:</hi> The performance of the NER process
                              should be evaluated periodically using appropriate metrics such as
                              precision, recall, and F1-score. This helps to identify areas for
                              improvement and guide future work.<anchor type="commentRangeEnd"
                                   n="134"/><hi rend="annotation_reference"><note place="comment"
                                        resp="#Autor" n="134"><date when=""/>(Antonio) He creado
                                        esta sección para explicar la metodologia. Si pensáis que
                                        sobra, que es muy repetitivo, quitadlo directamente. Es por
                                        plasmar claramente lo que dicen en la
                              revisión.</note></hi></item>
                    </list>
                    <p rend="Standard" style="text-align: justify;">TIn conclusion, thehe
                         methodology employed in this research can be extended to other domains and
                         languages, facilitating advancements in various fields. For instance, by
                         leveraging multilingual models and adapting them to other languages,
                         similar NER tasks in historical newspapers from different countries can be
                         accomplished. Additionally, applying the developed annotation guidelines
                         and expanding the dataset to include newspapers from diverse regions and
                         time periods would enable the automatic annotation and prediction of
                         entities in a broader range of Spanish newspapers, and potentially other
                         languages. This adaptability and transferability of the methodology make it
                         a valuable resource for historians and researchers working on various
                         textual collections beyond historical newspapers, such as ancient
                         manuscripts, legal documents, or literary works in different languages. By
                         scaling up the annotated data and training the models with more diverse
                         samples, the accuracy and robustness of the NER models can be further
                         enhanced, fostering more efficient and accurate analyses in the digital
                         humanities and beyond.</p>
               </div>
               <div>
                    <head>Conclusions and future work</head>
                    <p rend="Standard" style="text-align: justify;">The characteristics of digitized
                         newspapers and the research interests of historians justify the need to
                         develop the CLARA-DM corpus, a model for transcription, and a specific
                         model for named entity recognition in these texts.</p>
                    <p rend="Standard" style="text-align: justify;">As regards named entity
                         recognition, we have seen that cross-language and cross-domain knowledge is
                         transferred not only with multilingual models, but also with monolingual
                         ones. On the other hand, having corpora or datasets for the same specific
                         task (NER in historical newspapers in this case) in other languages might
                         be useful and is even better than having generic datasets in the same
                         language.</p>
                    <p rend="Standard" style="text-align: justify;">In the developed CLARA-DM
                         dataset, the monolingual models and the use of a dataset of historical
                         newspapers in French have stood out because it is a language close to
                         Spanish.</p>
                    <p rend="Standard" style="text-align: justify;">Nevertheless, the use of
                         external datasets could not compete with having more annotated data of our
                         own corpus. In the final experiments we found that an improvement in the
                         annotation guidelines and an increase in the labelled data significantly
                         improves the performance of the models. In addition, it is verified that
                         the models are sensitive to choices such as the method of adjudication for
                         the final version of the annotations, or the choice of data for training
                         and testing.</p>
                    <p rend="Standard" style="text-align: justify;"><anchor type="commentRangeStart"
                              n="222"/>We believe that the entity recognition system can be improved
                         in future research by using the Simple Knowledge Organisation System (SKOS)
                         and creating an ontology. Based on the trained models, and after reviewing
                         the discovered entities by historians, we propose to create a taxonomy.
                         This will be used to improve the identification of new mentions in other
                         newspapers. In addition, storing information in an ontology will allow more
                         complex queries at run time by reducing the ambiguity of entities. For
                         example, having all the mentions of a particular location in a single URI
                         would allow you to queringqueryingy all the people who have traded
                              there.<anchor type="commentRangeEnd" n="222"/><hi
                              rend="annotation_reference"><note place="comment" resp="#Autor"
                                   n="222"><date when=""/>(antonio) Añadido párrafo sobre skos y
                                   ontologias. </note></hi></p>
                    <p rend="Standard" style="text-align: justify;">In summary, from a collection of
                         newspapers in PDF format it has been possible to obtain a model for its
                         transcription, with an accuracy of 99%, and a model for the prediction of
                         the entities in the transcribed newspapers, with an accuracy of more than
                         75%. This last result will be improved in the future as we trained the
                         model with only 7 newspapers. Furthermore, as we already have
                         better-defined annotation guidelines we hope to speed up the annotation
                         process and get an even more robust final NER model trained with more data
                         to annotate automatically any other Spanish newspaper published in the same
                         period.In conclusion, the methodology employed in this research can be
                         extended to other domains and languages, facilitating advancements in
                         various fields. For instance, by leveraging multilingual models and
                         adapting them to other languages, similar NER tasks in historical
                         newspapers from different countries can be accomplished. Additionally,
                         applying the developed annotation guidelines and expanding the dataset to
                         include newspapers from diverse regions and time periods would enable the
                         automatic annotation and prediction of entities in a broader range of
                         Spanish newspapers, and potentially other languages. This adaptability and
                         transferability of the methodology make it a valuable resource for
                         historians and researchers working on various textual collections beyond
                         historical newspapers, such as ancient manuscripts, legal documents, or
                         literary works in different languages. By scaling up the annotated data and
                         training the models with more diverse samples, the accuracy and robustness
                         of the NER models can be further enhanced, fostering more efficient and
                         accurate analyses in the digital humanities and beyond.</p>
               </div>
               <div>
                    <head>Acknowledgements</head>
                    <p rend="Standard" style="text-align: justify;">This work has been supported by
                         the Spanish Government and funded under the CLARA-HD project
                         (PID2020-116001RB-C32, <ref target="https://clara-nlp.uned.es/home/dh/"><hi
                                   rend="color(00000A)"
                              >https://clara-nlp.uned.es/home/dh/</hi></ref>).</p>
                    <p rend="Standard" style="text-align: justify;">We would like to acknowledge the
                         support of the UNED art history professors Álvaro Molina and Alicia Cámara,
                         research directors of the CARCEM<note place="foot" xml:id="ftn33" n="33"><p
                                   rend="footnote text"><ref
                                        target="https://dimh.hypotheses.org/author/dimh"><hi
                                             rend="color(00000A)"
                                             >https://dimh.hypotheses.org/author/dimh</hi></ref></p></note>project.
                         Finally, warm thanks to Andrés Rodríguez-Francés and Víctor Sánchez-Sánchez
                         members for a while of UNED research group NLP&amp;IR &gt; DH.</p>
               </div>
               <div>
                    <head>Appendix: Brief introduction to Deep Learning</head>
                    <p rend="Standard" style="text-align: justify;">Artificial Intelligence (AI) is
                         the field of study that focuses on the creation of computer systems and
                         software capable of performing tasks that require human intelligence. This
                         covers areas ranging from speech recognition and computer vision
                         capabilities, to complex decision making, machine learning and problem
                         solving.</p>
                    <p rend="Standard" style="text-align: justify;">There are different approaches
                         within AI, such as rule-based AI, which uses a set of predefined
                         instructions and rules to make decisions, and machine learning, which is
                         based on algorithms and models that allow machines to learn from examples
                         and data.</p>
                    <p rend="Standard" style="text-align: justify;">Machine learning is a
                         sub-discipline of AI based on the idea of building mathematical or
                         statistical models that can learn from data. These models are trained using
                         a training data set, where examples are provided with their respective
                         labels or expected results. The machine learning algorithm analyses the
                         data and adjusts its internal parameters to find patterns and correlations
                         between input features and output labels.</p>
                    <p rend="Standard" style="text-align: justify;">Once the model has been trained,
                         it can be used to make predictions or decisions about new data that have
                         not been used during training. The goal of machine learning is to
                         generalise the knowledge acquired during training so that it can be applied
                         to new and unknown situations.</p>
                    <p rend="Standard" style="text-align: justify;">There are several types of
                         machine learning, each focusing on different approaches and techniques to
                         address specific problems. There are three main types: supervised and
                         unsupervised learning, and reinforcement learning.</p>
                    <p rend="Standard" style="text-align: justify;">In supervised learning, the
                         algorithm is provided with a training data set consisting of input examples
                         and the corresponding outputs, and the goal for the algorithm is to learn
                         to map the inputs to the correct outputs. In unsupervised learning, the
                         algorithm is confronted with a set of unlabelled training data. The
                         objective is to find patterns, structures or intrinsic relationships in the
                         data. In reinforcement learning, the algorithm interacts with a dynamic
                         environment and receives feedback in the form of rewards or punishments
                         based on its actions, and learns through trial and error, adjusting its
                         behaviour to maximise rewards over time.</p>
                    <p rend="Standard" style="text-align: justify;">Deep Learning is a branch of
                         machine learning that relies on artificial neural networks to learn and
                         extract high-level representations from complex, unstructured data.</p>
                    <p rend="Standard" style="text-align: justify;">Two essential stages in deep
                         learning are pre-training and training/fine-tuning. Pre-training involves
                         training a model on a related task or a large dataset to learn general
                         features and patterns. For example, large language models are pre-trained
                         in huge corpora such as Wikipedia. Then, fine-tuning follows, where the
                         model's parameters are adjusted on a smaller labeled dataset related to the
                         specific target task (i.e. NER). This process allows the model to leverage
                         prior knowledge from pre-training and adapt to the target task, leading to
                         improved performance. This is a popular approach in Deep Learning called
                         transfer learning
                         <?biblio ADDIN ZOTERO_ITEM CSL_CITATION {"citationID":"MMf61rBl","properties":{"formattedCitation":"(Malte & Ratadiya, 2019; Ruder et\\uc0\\u160{}al., 2019)","plainCitation":"(Malte & Ratadiya, 2019; Ruder et al., 2019)","noteIndex":0},"citationItems":[{"id":967,"uris":["http://zotero.org/users/8638006/items/SA3YTKNB"],"itemData":{"id":967,"type":"article","abstract":"In this paper, we present a study of the recent advancements which have helped bring Transfer Learning to NLP through the use of semi-supervised training. We discuss cutting-edge methods and architectures such as BERT, GPT, ELMo, ULMFit among others. Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies. However, owing to the vast nature of natural languages these methods do not generalise well and failed to learn the nuances of language. Thus machine learning algorithms such as Naive Bayes and decision trees coupled with traditional models such as Bag-of-Words and N-grams were used to usurp this problem. Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation. We talk about how Transfer Learning has brought about the well-known ImageNet moment for NLP. Several advanced architectures such as the Transformer and its variants have allowed practitioners to leverage knowledge gained from unrelated task to drastically fasten convergence and provide better performance on the target task. This survey represents an effort at providing a succinct yet complete understanding of the recent advances in natural language processing using deep learning in with a special focus on detailing transfer learning and its potential advantages.","DOI":"10.48550/arXiv.1910.07370","note":"arXiv:1910.07370 [cs]","number":"arXiv:1910.07370","publisher":"arXiv","source":"arXiv.org","title":"Evolution of transfer learning in natural language processing","URL":"http://arxiv.org/abs/1910.07370","author":[{"family":"Malte","given":"Aditya"},{"family":"Ratadiya","given":"Pratik"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2019",10,16]]}}},{"id":969,"uris":["http://zotero.org/users/8638006/items/LYKZUFUC"],"itemData":{"id":969,"type":"paper-conference","abstract":"The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.","container-title":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials","DOI":"10.18653/v1/N19-5004","event-place":"Minneapolis, Minnesota","page":"15–18","publisher":"Association for Computational Linguistics","publisher-place":"Minneapolis, Minnesota","source":"ACLWeb","title":"Transfer Learning in Natural Language Processing","URL":"https://aclanthology.org/N19-5004","author":[{"family":"Ruder","given":"Sebastian"},{"family":"Peters","given":"Matthew E."},{"family":"Swayamdipta","given":"Swabha"},{"family":"Wolf","given":"Thomas"}],"accessed":{"date-parts":[["2023",7,19]]},"issued":{"date-parts":[["2019",6]]}}}],"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}?>(Malte
                         &amp; Ratadiya, 2019; Ruder et al., 2019), especially when dealing with
                         limited labeled data, besides the usual supervised, unsupervised and
                         reinforcement learning approaches.</p>
                    <p rend="Standard" style="text-align: justify;">In the context of transfer
                         learning, zero-shot and few-shot learning are approaches that leverage
                         pre-trained models to address limited data scenarios. Zero-shot learning
                         aims to recognize new classes unseen during training by utilizing semantic
                         relationships or embeddings learned from related classes. This allows the
                         model to make predictions on entirely novel categories without any
                         fine-tuning on specific examples. On the other hand, few-shot learning
                         focuses on learning from a few examples of each new class. The model adapts
                         its knowledge from pre-training to recognize and generalize to new classes
                         with only a small amount of labeled data. These techniques significantly
                         enhance the capabilities of transfer learning, enabling models to excel in
                         situations with minimal labeled data and effectively tackle new and
                         previously unseen tasks.</p>
                    <p rend="Standard" style="text-align: justify;">Hyperparameters, such as epochs
                         and learning rate, are crucial settings in deep learning models that are
                         not learned from the data during training. Instead, they are set before
                         training begins and can significantly impact the model's performance.
                         "Epochs" represent the number of times the model iterates through the
                         entire dataset during training. Increasing epochs can allow the model to
                         see the data more times but may risk overfitting. "Learning rate" controls
                         the step size for updating the model's parameters during training. A high
                         learning rate can lead to faster convergence, but it might cause
                         overshooting and instability. Balancing these hyperparameters is essential
                         to achieve optimal training and ensure the model generalizes well to new,
                         unseen data.</p>
                    <p rend="Standard" style="text-align: justify;">Two types of systems make use of
                         Deep Learning in this paper: OCR and NER. Optical Character Recognition
                         (OCR) is a technology that utilizes neural networks and computer vision
                         techniques to automatically recognize and extract text from images or
                         scanned documents. Deep learning models, such as Convolutional Neural
                         Networks (CNNs), are employed to learn the complex features of characters
                         and words, enabling accurate text recognition. Named Entity Recognition
                         (NER) systems are a type of natural language processing (NLP) technology
                         that uses deep learning and machine learning techniques to automatically
                         identify and classify named entities in text. NER systems employ models,
                         such as recurrent neural networks (RNNs) or transformer-based architectures
                         like BERT, to learn the patterns and context of words in sentences,
                         allowing them to recognize and label named entities accurately.</p>
                    <p><pb/></p>
               </div>
               <div>
                    <head>References</head>
                    <p rend="Standard" style="text-align: justify;">Akbik, A., Blythe, D., and
                         Vollgraf, R. (2018) ‘Contextual String Embeddings for Sequence Labeling’,
                              <hi rend="italic">Proceedings of the 27th International Conference on
                              Computational Linguistics</hi>, 1638-1649. Available at: <ref
                              target="https://aclanthology.org/C18-1139"><hi rend="color(00000A)"
                                   >https://aclanthology.org/C18-1139</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Aldama, N., Guerrero, M.,
                         Montoro, H., and Samy, D. (2022) ‘Anotación de corpus lingüísticos:
                         Metodología utilizada en el Instituto de Ingeniería del Conocimiento
                         (IIC)’, 17.</p>
                    <p rend="Standard" style="text-align: justify;">Alrasheed, N., Rao, P. and
                         Grieco, V. (2021) ‘Character Recognition <anchor xml:id="Int_rxHWsadO"/>Of
                         Seventeenth-Century Spanish American Notary Records Using Deep Learning’,
                              <hi rend="italic">DHQ</hi> 15.4. Available at: <ref
                              target="http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html"
                                   ><hi rend="color(00000A)"
                                   >http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Aranda García, N. (2022)
                         ‘Humanidades Digitales y literatura medieval española: La integración de
                         Transkribus en la base de datos COMEDIC’, <hi rend="italic">Historias
                              Fingidas</hi>, 0, 127-149. Available at: <ref
                              target="https://doi.org/10.13136/2284-2667/1107"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.13136/2284-2667/1107</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Asahara, M., and Matsumoto, Y.
                         (2003) ‘Japanese Named Entity Extraction with Redundant Morphological
                         Analysis’, <hi rend="italic">Proceedings of the 2003 Human Language
                              Technology Conference of the North American Chapter of the Association
                              for Computational Linguistics</hi>, 8-15. Available at: <ref
                              target="https://aclanthology.org/N03-1002"><hi rend="color(00000A)"
                                   >https://aclanthology.org/N03-1002</hi></ref></p>
                    <p rend="Bibliography" style="text-align: justify;">Ayuso García, M. (2022) ‘Las
                         ediciones de Arnao Guillén de Brocar de BECLaR transcritas con ayuda de
                         Transkribus y OCR4all: Creación de un modelo para la red neuronal y posible
                         explotación de los resultados’, <hi rend="italic">Historias Fingidas</hi>,
                              <hi rend="italic">0</hi>, 151-173. Available at: <ref
                              target="https://doi.org/10.13136/2284-2667/1102"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.13136/2284-2667/1102</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Baptiste, B., Favre, B.,
                         Auguste, J., and Henriot, C. (2021) ‘Transferring Modern Named Entity
                         Recognition to the Historical Domain: How to Take the Step?’, <hi
                              rend="italic">Workshop on Natural Language Processing for Digital
                              Humanities (NLP4DH)</hi>. Available at: <ref
                              target="https://hal.archives-ouvertes.fr/hal-03550384"><hi
                                   rend="color(00000A)"
                                   >https://hal.archives-ouvertes.fr/hal-03550384</hi></ref></p>
                    <p rend="Bibliography" style="text-align: justify;">Bazzaco, S., Ruiz, A. M. J.,
                         Ruberte, Á. T., and Molares, M. M. (2022) ‘Sistemas de reconocimiento de
                         textos e impresos hispánicos de la Edad Moderna. La creación de unos
                         modelos de HTR para la transcripción automatizada de documentos en gótica y
                         redonda (s. XV-XVII)’, <hi rend="italic">Historias Fingidas</hi>, <hi
                              rend="italic">0</hi>, 67-125. Available at: <ref
                              target="https://doi.org/10.13136/2284-2667/1190"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.13136/2284-2667/1190</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Bikel, D. M., Miller, S.,
                         Schwartz, R., and Weischedel, R. (1997) ‘Nymble: A High-Performance
                         Learning Name-finder’, <hi rend="italic">Fifth Conference on Applied
                              Natural Language Processing</hi>, 194-201. Available at: <ref
                              target="https://doi.org/10.3115/974557.974586"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.3115/974557.974586</hi></ref></p>
                    <p rend="Bibliography">Blouin, B., Favre, B., Auguste, J., &amp; Henriot, C.
                         (2021). Transferring Modern Named Entity Recognition to the Historical
                         Domain: How to Take the Step? <hi rend="italic">Proceedings of the Workshop
                              on Natural Language Processing for Digital Humanities</hi>, 152-162.
                         https://aclanthology.org/2021.nlp4dh-1.18</p>
                    <p rend="Standard" style="text-align: justify;">Bollmann, M. (2019) ‘A
                         Large-Scale Comparison of Historical Text Normalization Systems.’ <hi
                              rend="italic">Proceedings of the 2019 Conference of the North American
                              Chapter of the Association for Computational Linguistics: Human
                              Language Technologies, Volume 1 (Long and Short Papers)</hi>,
                         3885-3898. Available at: <ref target="https://doi.org/10.18653/v1/N19-1389"
                                   ><hi rend="color(00000A)"
                                   >https://doi.org/10.18653/v1/N19-1389</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Boros, E., Hamdi, A., Linhares
                         Pontes, E., Cabrera-Diego, L. A., Moreno, J. G., Sidere, N., and Doucet, A.
                         (2020) ‘Alleviating Digitization Errors in Named Entity Recognition for
                         Historical Documents.’ <hi rend="italic">Proceedings of the 24th Conference
                              on Computational Natural Language Learning</hi>, 431-441. Available
                         at: <ref target="https://doi.org/10.18653/v1/2020.conll-1.35"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.18653/v1/2020.conll-1.35</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Borthwick, A., Sterling, J.,
                         Agichtein, E., and Grishman, R. (1998). ‘NYU: Description of the MENE Named
                         Entity System as Used in MUC-7.’ <hi rend="italic">Seventh Message
                              Understanding Conference (MUC-7): Proceedings of a Conference Held in
                              Fairfax, Virginia, April 29 - May 1, 1998</hi>. MUC 1998. Available
                         at: <ref target="https://aclanthology.org/M98-1018"><hi
                                   rend="color(00000A)"
                         >https://aclanthology.org/M98-1018</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Calle-Gómez, Javier;
                         García-Serrano, Ana and Martínez, Paloma. (2006). ‘Intentional processing
                         as a key for rational behaviour through Natural Interaction’,
                              <hi rend="italic" xml:space="preserve">Interacting </hi><anchor
                              xml:id="Int_ZGP2ZeEB"/><hi rend="italic">With Computers</hi>, Vol: 18
                         Nº: 6, pp:1419-1446 10.1016/j.intcom.2006.05.002 </p>
                    <p rend="Standard" style="text-align: justify;">Cámara, Alicia; Molina, Álvaro y
                         Margarita A. Vázquez. (2020). Manassero (eds.). ‘La ciudad de los saberes
                         en la Edad Moderna’, Gijón, Ediciones Trea, 296 pp. Available at: <ref
                              target="http://e-spacio.uned.es/fez/view/bibliuned:404-Amolina-1011"
                                   ><hi rend="color(00000A)"
                                   >http://e-spacio.uned.es/fez/view/bibliuned:404-Amolina-1011</hi></ref>.</p>
                    <p rend="Standard" style="text-align: justify;">Calvo Tello, J. (2019). ‘Diseño
                         de corpus literario para análisis cuantitativos.’ <hi rend="italic">Revista
                              de Humanidades Digitales</hi>, <hi rend="italic">4</hi>, 115-135.
                         Available at: <ref target="https://doi.org/10.5944/rhd.vol.4.2019.25187"
                                   ><hi rend="color(00000A)"
                                   >https://doi.org/10.5944/rhd.vol.4.2019.25187</hi></ref>
                    </p>
                    <p rend="Standard" style="text-align: justify;">Campillos-Llanos, L., Terroba
                         Reinares, A. R., Zakhir Puig, S., Valverde-Mateos, A., and
                         Capllonch-Carrión, A. (2022) ‘Building a comparable corpus and a benchmark
                         for Spanish medical text simplification.’ <hi rend="italic">Proceedings of
                              the Annual Conference of the Spanish Association for Natural Language
                              Processing 2022: Projects and Demonstrations (SEPLN-PD 2022)</hi>.</p>
                    <p rend="Standard" style="text-align: justify;">Campillos-Llanos, L.,
                         Valverde-Mateos, A., Capllonch-Carrión, A. et al. (2021) ‘A clinical trials
                         corpus annotated with UMLS entities to enhance the access to evidence-based
                         medicine.’ BMC Med Inform Decis Mak 21, 69 Available at: <ref
                              target="https://doi.org/10.1186/s12911-021-01395-z"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.1186/s12911-021-01395-z</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Chastang, P., Torres Aguilar,
                         S., Tannier, X. (2021). ‘A Named Entity Recognition Model for Medieval
                         Latin Charters.’ <hi rend="italic">DHQ</hi> 15.4. Available at: <ref
                              target="http://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html"
                                   ><hi rend="color(00000A)"
                                   >http://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Collobert, R., Weston, J.,
                         Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011) ‘Natural
                         Language Processing (Almost) from Scratch.’ <hi rend="italic">Natural
                              Language Processing</hi>, 45.</p>
                    <p rend="Standard" style="text-align: justify;">Conneau, A., Khandelwal, K.,
                         Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M.,
                         Zettlemoyer, L., and Stoyanov, V. (2020) ‘Unsupervised Cross-lingual
                         Representation Learning at Scale’ (arXiv:1911.02116). ArXiv. Available at:
                              <ref target="http://arxiv.org/abs/1911.02116"><hi rend="color(00000A)"
                                   >http://arxiv.org/abs/1911.02116</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Cuéllar, Álvaro. (2021).
                         ‘Spanish Golden Age Theatre Manuscripts (Spelling Modernization) 1.0’.
                         Transkribus.</p>
                    <p rend="Standard" style="text-align: justify;">Cuéllar, Álvaro and Vega
                         García-Luengos, Germán. (2021) ‘ETSO. Estilometría aplicada al Teatro del
                         Siglo de Oro.’ etso.es.</p>
                    <p rend="Standard" style="text-align: justify;">Davies, M., and Parodi, G.
                         (2022) ‘Constitución de corpus crecientes del español.’ At G. Parodi, P.
                         Cantos-Gómez, C. Howe, M. Lacorte, J. Muñoz-Basol, and J. Muñoz-Basol, <hi
                              rend="italic">Lingüística de corpus en español</hi> (1.<hi
                              rend="superscript">a</hi> ed., pp. 13-32). Routledge. Available at:
                              <ref target="https://doi.org/10.4324/9780429329296-3"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.4324/9780429329296-3</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">De la Rosa, J., Ponferrada, E.
                         G., Villegas, P., Salas, P. G. de P., Romero, M., and Grandury, M. (2022)
                              ‘<hi rend="italic">B</hi>ERTIN: Efficient Pre-Training of a Spanish
                         Language Model using Perplexity Sampling<hi rend="italic">’</hi>
                         (arXiv:2207.06814). ArXiv. Available at: <ref
                              target="http://arxiv.org/abs/2207.06814"><hi rend="color(00000A)"
                                   >http://arxiv.org/abs/2207.06814</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">De Toni, F., Akiki, C., De La
                         Rosa, J., Fourrier, C., Manjavacas, E., Schweter, S., and Van Strien, D.
                         (2022) ‘Entities, Dates, and Languages: Zero-Shot on Historical Texts with
                         T0.’ <hi rend="italic">Proceedings of BigScience Episode #5 -- Workshop on
                              Challenges and Perspectives in Creating Large Language Models</hi>,
                         75-83. Available at: <ref
                              target="https://doi.org/10.18653/v1/2022.bigscience-1.7"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.18653/v1/2022.bigscience-1.7</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Delestre, C., and Amar, A.
                         (2022) ‘DistilCamemBERT: A distillation of the French model CamemBERT’
                         (arXiv:2205.11111). ArXiv. Available at: <ref
                              target="https://doi.org/10.48550/arXiv.2205.11111"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.48550/arXiv.2205.11111</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Derrick, T. (2019) ‘Using
                         Transkribus <anchor xml:id="Int_94iJbJFP"/>For Automated Text Recognition
                         of Historical Bengali Books’ <hi rend="italic">British Library Digital
                              Scholarship Blog</hi>. Available at: <ref
                              target="https://blogs.bl.uk/digital-scholarship/2019/08/using-transkribus-for-automated-text-recognition-of-historical-bengali-books.html"
                                   ><hi rend="color(00000A)"
                                   >https://blogs.bl.uk/digital-scholarship/2019/08/using-transkribus-for-automated-text-recognition-of-historical-bengali-books.html</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Devlin, J., Chang, M.-W., Lee,
                         K., and Toutanova, K. (2019) ‘BERT: Pre-training of Deep Bidirectional
                         Transformers for Language Understanding<hi rend="italic">’</hi>
                         (arXiv:1810.04805). ArXiv. Available at: <ref
                              target="http://arxiv.org/abs/1810.04805"><hi rend="color(00000A)"
                                   >http://arxiv.org/abs/1810.04805</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Ehrmann, M., Romanello, M.,
                         Clematide, S., Ströbel, P. B., and Barman, R. (2020b) ‘Language Resources
                         for Historical Newspapers: The Impresso Collection.’ <hi rend="italic"
                              >Proceedings of the 12th Language Resources and Evaluation
                              Conference</hi>, 958-968. Available at: <ref
                              target="https://aclanthology.org/2020.lrec-1.121"><hi
                                   rend="color(00000A)"
                                   >https://aclanthology.org/2020.lrec-1.121</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Ehrmann, M., Romanello, M.,
                         Fluckiger, A., and Clematide, S. (2020a). ‘Extended Overview of CLEF HIPE
                         2020: Named Entity Processing on Historical Newspapers.’ 38.</p>
                    <p rend="Standard" style="text-align: justify;">Ehrmann, M., Romanello, M.,
                         Najem-Meyer, S., Doucet, A., and Clematide, S. (2022). ‘Extended Overview
                         of HIPE-2022: Named Entity Recognition and Linking in Multilingual
                         Historical Documents.’ 26.</p>
                    <p rend="Standard" style="text-align: justify;">Ehrmann, M., Watter, C.,
                         Romanello, M., and Clematide, S. (2020c). ‘Impresso Named Entity Annotation
                         Guidelines’. Available at: <ref
                              target="https://doi.org/10.5281/zenodo.3604227"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.5281/zenodo.3604227</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">García-Serrano, A. and
                         Castellanos, A. (2016) ‘Representación y organización de documentos
                         digitales: detalles y práctica sobre la ontología DIMH’. <hi rend="italic"
                              >Revista de Humanidades Digitales</hi>, v.1, 314-344, ISSN 2531-1786.
                         Available at: <ref target="https://doi.org/10.5944/rhd.vol.1.2017.17155"
                                   ><hi rend="color(00000A)"
                                   >https://doi.org/10.5944/rhd.vol.1.2017.17155</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">García-Serrano, A., and
                         Menta-Garuz, A (2022). ‘La inteligencia artificial en las Humanidades
                         Digitales: dos experiencias con corpus digitales.’ <hi rend="italic"
                              >Revista de Humanidades Digitales</hi>, 7, 19-39.</p>
                    <p rend="Standard" style="text-align: justify;">Gebru, T., Morgenstern, J.,
                         Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., and Crawford, K.
                         (2021)’ Datasheets for datasets.’ <hi rend="italic">Communications of the
                              ACM</hi>, <hi rend="italic">64</hi>(12), 86-92. Available at: <ref
                              target="https://doi.org/10.1145/3458723"><hi rend="color(00000A)"
                                   >https://doi.org/10.1145/3458723</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Grishman, R., and Sundheim, B.
                         (1996) ‘Message Understanding Conference-6: A brief history’ <hi
                              rend="italic">Proceedings of the 16th conference on Computational
                              linguistics - Volume 1</hi>, 466-471. Available at: <ref
                              target="https://doi.org/10.3115/992628.992709"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.3115/992628.992709</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Gruszczyński, W., Adamiec, D.,
                         Bronikowska, R., Kieraś, W., Modrzejewski, E., Wieczorek, A., and Woliński,
                         M. (2021) ‘The Electronic Corpus of 17th- and 18th-century Polish Texts.’
                              <hi rend="italic">Language Resources and Evaluation</hi>. Available
                         at: <ref target="https://doi.org/10.1007/s10579-021-09549-1"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.1007/s10579-021-09549-1</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Gutiérrez-Fandiño, A.,
                         Armengol-Estapé, J., Pàmies, M., Llop-Palao, J., Silveira-Ocampo, J.,
                         Carrino, C. P., Armentano-Oller, C., Rodriguez-Penagos, C.,
                         Gonzalez-Agirre, A., and Villegas, M. (2022) ‘MarIA: Spanish Language
                              Models’<hi rend="italic">,</hi> 22.</p>
                    <p rend="Bibliography">Hintz, G., &amp; Biemann, C. (2016). Language Transfer
                         Learning for Supervised Lexical Substitution. <hi rend="italic">Proceedings
                              of the 54th Annual Meeting of the Association for Computational
                              Linguistics (Volume 1: Long Papers)</hi>, 118-129.
                         https://doi.org/10.18653/v1/P16-1012</p>
                    <p rend="Standard" style="text-align: justify;">Kabatek, J. (2013) ‘¿Es posible
                         una lingüística histórica basada en un corpus representativo?’ <hi
                              rend="italic">Iberoromania</hi>, <hi rend="italic">77</hi>(1).
                         Available at: <ref target="https://doi.org/10.1515/ibero-2013-0045"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.1515/ibero-2013-0045</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Kettunen, K., Mäkelä, E.,
                         Ruokolainen T., Kuokkala J. and Löfberg, L. (2017) ‘Old Content and Modern
                         Tools – Searching Named Entities in a Finnish OCRed Historical Newspaper
                         Collection 1771–1910.’
                         <hi rend="italic" xml:space="preserve">DHQ </hi>11.3. Available at: <ref
                              target="http://digitalhumanities.org:8081/dhq/vol/11/3/000333/000333.html"
                                   ><hi rend="color(00000A)"
                                   >http://digitalhumanities.org:8081/dhq/vol/11/3/000333/000333.html</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Kirmizialtin, Suphan and David
                         Joseph Wrisley. (2022) ‘Automated Transcription of Non-Latin Script
                         Periodicals: A Case Study in the Ottoman Turkish Print Archive.’
                         <hi rend="italic" xml:space="preserve">DHQ </hi>16.2. Available at: <ref
                              target="http://www.digitalhumanities.org/dhq/vol/16/2/000577/000577.html"
                                   ><hi rend="color(00000A)"
                                   >http://www.digitalhumanities.org/dhq/vol/16/2/000577/000577.html</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Lample, G., Ballesteros, M.,
                         Subramanian, S., Kawakami, K., and Dyer, C. (2016) ‘Neural Architectures
                         for Named Entity Recognition’ (arXiv:1603.01360). ArXiv. Available at: <ref
                              target="https://doi.org/10.48550/arXiv.1603.01360"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.48550/arXiv.1603.01360</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Li, J., Sun, A., Han, J., and
                         Li, C. (2022) ‘A Survey on Deep Learning for Named Entity Recognition.’ <hi
                              rend="italic">IEEE Transactions on Knowledge and Data
                         Engineering</hi>, <hi rend="italic">34</hi>(1), 50-70. Available at: <ref
                              target="https://doi.org/10.1109/TKDE.2020.2981314"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.1109/TKDE.2020.2981314</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Liu, Y., Ott, M., Goyal, N., Du,
                         J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and
                         Stoyanov, V. (2019) ‘RoBERTa: A Robustly Optimized BERT Pretraining
                         Approach’ (arXiv:1907.11692). ArXiv. Available at: <ref
                              target="https://doi.org/10.48550/arXiv.1907.11692"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.48550/arXiv.1907.11692</hi></ref></p>
                    <p rend="Bibliography">Malte, A., &amp; Ratadiya, P. (2019). <hi rend="italic"
                              >Evolution of transfer learning in natural language processing</hi>
                         (arXiv:1910.07370). arXiv. https://doi.org/10.48550/arXiv.1910.07370</p>
                    <p rend="Standard" style="text-align: justify;">McCallum, A., and Li, W. (2003)
                         ‘Early results for Named Entity Recognition with Conditional Random Fields,
                         Feature Induction and Web-Enhanced Lexicons.’ <hi rend="italic">Proceedings
                              of the Seventh Conference on Natural Language Learning at HLT-NAACL
                              2003</hi>, 188-191. Available at: <ref
                              target="https://aclanthology.org/W03-0430"><hi rend="color(00000A)"
                                   >https://aclanthology.org/W03-0430</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Menta, A., and García-Serrano,
                         A. (2022) ‘Controllable Sentence Simplification Using Transfer Learning.’
                         Proceedings of the Working Notes of CLEF.</p>
                    <p rend="Bibliography" style="text-align: justify;">Menta, A., Sánchez-Salido,
                         E., and García-Serrano, A. (2022) ‘Transcripción de periódicos históricos:
                         Aproximación CLARA-HD’, <hi rend="italic">Proceedings of the Annual
                              Conference of the Spanish Association for Natural Language Processing
                              2022: Projects and Demonstrations (SEPLN-PD 2022)</hi>. Available at:
                              <ref target="https://ceur-ws.org/Vol-3224/paper17.pdf"><hi
                                   rend="color(00000A)"
                                   >https://ceur-ws.org/Vol-3224/paper17.pdf</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Merino Recalde, David. (2022)
                         ‘El sistema de personajes de las comedias urbanas de Lope de Vega.
                         Propuesta metodológica y posibilidades del análisis de redes sociales para
                         el estudio del teatro del Siglo de Oro’ Master Thesis, UNED. Facultad de
                         Filología. Departamento de Literatura Española y Teoría de la Literatura.
                         Available at: <ref
                              target="http://e-spacio.uned.es/fez/view/bibliuned:master-Filologia-FILTCE-Dmerino"
                                   ><hi rend="color(00000A)"
                                   >http://e-spacio.uned.es/fez/view/bibliuned:master-Filologia-FILTCE-Dmerino</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Molina Martín, Á. (2021)
                         ‘Cartografías del adorno en las residencias nobiliarias de la corte de
                         Carlos IV: redes y modelos de buen gusto y distinción’ <hi rend="italic"
                              >Magallanica. Revista de Historia Moderna</hi>, 7(14), 205-235.</p>
                    <p rend="Standard" style="text-align: justify;">Molina, Á., and Vega, J. (2018)
                         ‘Adorno y representación: escenarios cotidianos de vida a finales del siglo
                         XVIII en Madrid’, 139-166.</p>
                    <p rend="Standard" style="text-align: justify;">Moreno Sandoval, A. (2019). <hi
                              rend="italic">Lenguas y computación</hi>. Síntesis.</p>
                    <p rend="Bibliography" style="text-align: justify;">Moreno Sandoval, A., Díaz
                         García, J., Campillos Llanos, L., and Redondo, T. (2018) ‘<hi rend="italic"
                              >Biomedical Term Extraction: NLP Techniques in Computational
                              Medicine’.</hi> Available at: <ref
                              target="https://doi.org/10.9781/ijimai.2018.04.001"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.9781/ijimai.2018.04.001</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Moreno Sandoval, Antonio,
                         Gisbert, Ana and Montoro Zamorano, Helena. (2020) ‘FinT-esp: A corpus of
                         financial reports in Spanish’.</p>
                    <p rend="Standard" style="text-align: justify;">Nadeau, D., and Sekine, S.
                         (2007) ‘A Survey of Named Entity Recognition and Classification’ <hi
                              rend="italic">Lingvisticae Investigationes</hi>, <hi rend="italic"
                              >30</hi>. Available at: <ref
                              target="https://doi.org/10.1075/li.30.1.03nad"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.1075/li.30.1.03nad</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Nakayama, E. (2021)
                         ‘Implementación de un corpus comparable de español y japonés de acceso
                         abierto para la traducción especializada<hi rend="italic">’,</hi> 29.</p>
                    <p rend="Standard" style="text-align: justify;">Neudecker, C. (2016) ‘An Open
                         Corpus for Named Entity Recognition in Historic Newspapers’, <hi
                              rend="italic">Proceedings of the Tenth International Conference on
                              Language Resources and Evaluation (LREC’16)</hi>, 4348-4352. Available
                         at: <ref target="https://aclanthology.org/L16-1689"><hi
                                   rend="color(00000A)"
                         >https://aclanthology.org/L16-1689</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Nieuwenhuijsen, D. (2016) ‘Notas
                         sobre la aportación del análisis estadístico a la lingüística de corpus’,
                              <hi rend="italic">Notas sobre la aportación del análisis estadístico a
                              la lingüística de corpus</hi> (pp. 215-237). De Gruyter. Available at:
                              <ref target="https://doi.org/10.1515/9783110462357-011"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.1515/9783110462357-011</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Perdiki, Elpida. (2022) ‘Review
                         of 'Transkribus: Reviewing HTR training on (Greek) manuscripts'." RIDE 15.
                         doi: 10.18716/ride.a.15.6. Accessed: 21.12.2022. Available at: <ref
                              target="https://ride.i-d-e.de/issues/issue-15/transkribus/"><hi
                                   rend="color(00000A)"
                                   >https://ride.i-d-e.de/issues/issue-15/transkribus/</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Piotrowski, M. (2012). <hi
                              rend="italic">Natural Language Processing for Historical Texts</hi>.
                         Graeme Hirst, University of Toronto.</p>
                    <p rend="Bibliography">Pruksachatkun, Y., Phang, J., Liu, H., Htut, P. M.,
                         Zhang, X., Pang, R. Y., Vania, C., Kann, K., &amp; Bowman, S. R. (2020).
                         Intermediate-Task Transfer Learning with Pretrained Language Models: When
                         and Why Does It Work? <hi rend="italic">Proceedings of the 58th Annual
                              Meeting of the Association for Computational Linguistics</hi>,
                         5231-5247. https://doi.org/10.18653/v1/2020.acl-main.467</p>
                    <p rend="Standard" style="text-align: justify;">Rivero, Manuel. (2022) ‘Italian
                         Madrid: Ambassadors, Regents, and Courtiers in the Hospital de San Pedro y
                         San Pablo’, <hi rend="italic">Culture &amp;Amp; History Digital
                              Journal</hi>, <hi rend="italic">11</hi>(1), e003. Available at: <ref
                              target="https://doi.org/10.3989/chdj.2022.003"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.3989/chdj.2022.003</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Rojo, G. (2010) ‘Sobre
                         codificación y explotación de corpus textuales: Otra comparación del Corpus
                         del español con el CORDE y el CREA’, <hi rend="italic">Lingüística</hi>,
                              <hi rend="italic">24</hi>, 11-50.</p>
                    <p rend="Standard" style="text-align: justify;">Rojo, G. (2016) ‘<anchor
                              xml:id="Int_DKW8nHWY"/>Los corpus textuales del español’, In book: <hi
                              rend="italic">Enciclopedia lingüística hispánica</hi>. Publisher:
                         Routledge. Editors: Gutiérrez-Rexach. Available at: <ref
                              target="https://www.researchgate.net/publication/294407007_Los_corpus_textuales_del_espanol"
                                   ><hi rend="color(00000A)"
                                   >https://www.researchgate.net/publication/294407007_Los_corpus_textuales_del_espanol</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Rosset, S., Grouin, C., and
                         Zweigenbaum, P. (2011) ‘Entités nommées structurées: Guide d’annotation
                         Quaero.’. Available at: <ref
                              target="http://www.quaero.org/media/files/bibliographie/quaero-guide-annotation-2011.pdf"
                                   ><hi rend="color(00000A)"
                                   >http://www.quaero.org/media/files/bibliographie/quaero-guide-annotation-2011.pdf</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Ruiz Fabo, P., Bermúdez Sabel,
                         H., Martínez-Cantón, C. and Calvo Tello J. (2017) ‘Diachronic Spanish
                         Sonnet Corpus (DISCO)’, Madrid. UNED. Available at: <ref
                              target="https://github.com/pruizf/disco"><hi rend="color(00000A)"
                                   >https://github.com/pruizf/disco</hi></ref></p>
                    <p rend="Bibliography">Rubinstein, A., &amp; Shmidman, A. (2021). NLP in the DH
                         pipeline: Transfer-learning to a Chronolect. <hi rend="italic">Proceedings
                              of the Workshop on Natural Language Processing for Digital
                              Humanities</hi>, 106-110.
                         https://aclanthology.org/2021.nlp4dh-1.12</p>
                    <p rend="Bibliography">Ruder, S., Peters, M. E., Swayamdipta, S., &amp; Wolf, T.
                         (2019). Transfer Learning in Natural Language Processing. <hi rend="italic"
                              >Proceedings of the 2019 Conference of the North American Chapter of
                              the Association for Computational Linguistics: Tutorials</hi>, 15-18.
                         https://doi.org/10.18653/v1/N19-5004</p>
                    <p rend="Standard" style="text-align: justify;">Sánchez-Salido, Eva. (2022)
                         ‘Reconocimiento de entidades en corpus de dominios específicos:
                         experimentación con periódicos históricos’, Master Thesis (30 ECTS). ETSI
                         Informática. UNED</p>
                    <p rend="Standard" style="text-align: justify;">Sanh, V., Debut, L., Chaumond,
                         J., and Wolf, T. (2019) ‘Distilbert, a distilled version of BERT: smaller,
                         faster, cheaper and lighter’, ArXiv preprint, abs/1910.01108</p>
                    <p rend="Standard" style="text-align: justify;">Scheible, R., Thomczyk, F.,
                         Tippmann, P., Jaravine, V., and Boeker, M. (2020) ‘GottBERT: A pure German
                         Language Model’, (arXiv:2012.02110). ArXiv. Available at: <ref
                              target="https://doi.org/10.48550/arXiv.2012.02110"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.48550/arXiv.2012.02110</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Sekine, S. (1998) ‘Description
                         of the Japanese NE System Used for MET-2', <hi rend="italic">Seventh
                              Message Understanding Conference (MUC-7): Proceedings of a Conference
                              Held in Fairfax, Virginia, April 29 - May 1, 1998</hi>. MUC 1998.
                         Available at: <ref target="https://aclanthology.org/M98-1019"><hi
                                   rend="color(00000A)"
                         >https://aclanthology.org/M98-1019</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Terras, M. M. (2011) ‘The Rise
                         of Digitization’, En R. Rikowski (Ed.), <hi rend="italic">Digitisation
                              Perspectives</hi> (pp. 3-20). SensePublishers. Available at: <ref
                              target="https://doi.org/10.1007/978-94-6091-299-3_1"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.1007/978-94-6091-299-3_1</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Torruella Casañas, J. (2017) <hi
                              rend="italic">Lingüística de corpus: Génesis y bases metodológicas de
                              los corpus (históricos) para la investigación en lingüística.</hi>
                         Peter Lang.</p>
                    <p rend="Standard" style="text-align: justify;">Vaswani, A., Shazeer, N.,
                         Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp;
                         Polosukhin, I. (2017) ‘Attention Is All You Need’, (arXiv:1706.03762).
                         ArXiv. Available at: <ref
                              target="https://doi.org/10.48550/arXiv.1706.03762"><hi
                                   rend="color(00000A)"
                                   >https://doi.org/10.48550/arXiv.1706.03762</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Wissler, L., Almashraee, M.,
                         Monett, D., &amp; Paschke, A. (2014) ‘The Gold Standard in Corpus
                         Annotation’, Available at: <ref
                              target="https://doi.org/10.13140/2.1.4316.3523"><hi
                                   rend="color(00000A)"
                              >https://doi.org/10.13140/2.1.4316.3523</hi></ref></p>
                    <p rend="Standard" style="text-align: justify;">Yadav, V., and Bethard, S.
                         (2018) ‘A Survey on Recent Advances in Named Entity Recognition from Deep
                         Learning models’, <hi rend="italic">Proceedings of the 27th International
                              Conference on Computational Linguistics</hi>, 2145-2158. Available at:
                              <ref target="https://aclanthology.org/C18-1182"><hi
                                   rend="color(00000A)"
                         >https://aclanthology.org/C18-1182</hi></ref><?biblio ADDIN ZOTERO_BIBL {"uncited":[],"omitted":[],"custom":[]} CSL_BIBLIOGRAPHY?></p>
                    <p rend="Bibliography">Zoph, B., Yuret, D., May, J., &amp; Knight, K. (2016).
                         Transfer Learning for Low-Resource Neural Machine Translation. <hi
                              rend="italic">Proceedings of the 2016 Conference on Empirical Methods
                              in Natural Language Processing</hi>, 1568-1575.
                         https://doi.org/10.18653/v1/D16-1163</p>
               </div>
          </body>
     </text>
</TEI>
