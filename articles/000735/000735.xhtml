<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Seeking Information
                  in Spanish Historical Newspapers: The Case of <cite class="title italic">Diario de
                     Madrid</cite> (18th and 19th Centuries)</h1>
               
               
               <div class="author"><span style="color: grey">Eva Sánchez-Salido
                     </span> &lt;<a href="mailto:evasan_at_lsi_dot_uned_dot_es" onclick="javascript:window.location.href='mailto:'+deobfuscate('evasan_at_lsi_dot_uned_dot_es'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('evasan_at_lsi_dot_uned_dot_es'); return false;">evasan_at_lsi_dot_uned_dot_es</a>&gt;, ETSI Informática, UNED</div>
               
               <div class="author"><span style="color: grey">Antonio Menta
                     </span> &lt;<a href="mailto:agarcia_at_lsi_dot_uned_dot_es" onclick="javascript:window.location.href='mailto:'+deobfuscate('agarcia_at_lsi_dot_uned_dot_es'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('agarcia_at_lsi_dot_uned_dot_es'); return false;">agarcia_at_lsi_dot_uned_dot_es</a>&gt;, ETSI Informática, UNED</div>
               
               <div class="author"><span style="color: grey">Ana García-Serrano<a class="noteRef" href="#d3e77">[1]</a>
                     </span> &lt;<a href="mailto:amenta_at_invi_dot_uned_dot_es" onclick="javascript:window.location.href='mailto:'+deobfuscate('amenta_at_invi_dot_uned_dot_es'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('amenta_at_invi_dot_uned_dot_es'); return false;">amenta_at_invi_dot_uned_dot_es</a>&gt;, ETSI Informática, UNED</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Seeking%20Information%20in%20Spanish%20Historical%20Newspapers%3A%20The%20Case%20of%20Diario%20de%20Madrid%20(18th%20and%2019th%20Centuries)&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Sánchez-Salido&amp;rft.aufirst=Eva&amp;rft.au=Eva%20Sánchez-Salido&amp;rft.au=Antonio%20Menta&amp;rft.au=Ana%20García-SerranoCorresponding author."> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>New technologies for seeking information are based in machine learning techniques
                     such as statistical or deep learning approaches that require a large number of
                     computational resources as well as the availability of huge corpora to develop the
                     applications that, in this concrete sub-area of Artificial Intelligence, are the
                     so-called <em class="term">models</em>. Nowadays, the reusability of the developed models is
                     approached with fine-tuning and transfer learning techniques. When the available
                     corpus is written in a language or domain with scarce resources, the accuracy of
                     these approaches decreases, so it is important to address the start of the task by
                     using state-of-the-art techniques.</p>
                  
                  <p>This is the main problem tackled in the work presented here, coming from the art
                     historians’ interest in an image-based digitized collection of newspapers called
                     <cite class="title italic">Diario de Madrid</cite> (DM) from the Spanish press between
                     18th and 19th centuries, which is freely available at the Spanish National Library
                     (BNE). Their focus is on information related to entities such as historical persons,
                     locations as well as objects for sale or lost and others, to obtain geo-localization
                     visualizations and solve some historical riddles. The first step needed technically
                     is to obtain the transcriptions of the original digitalized newspapers from the DM
                     (1788-1825) collection. After that, the second step is the development of a Named
                     Entity Recognition (NER) model to label or annotate automatically the available
                     corpus with the entities of interest for their research. For this, once the CLARA-DM
                     corpus is created, a sub-corpus must be manually annotated for the training step in
                     current Natural Language Processing (NLP) techniques, using human effort helped by
                     selected computational tools. To develop the necessary annotation model (CLARA-AM),
                     an experimentation step is carried out with state-of-the-art Deep Learning (DL)
                     models and an already available corpus, which complements the corpus that we have
                     developed.</p>
                  
                  <p>A main contribution of the paper is the methodology developed to tackle similar
                     problems like that of art historians’ digitized corpus: selecting specific tools when
                     available, reusing developed DL models to carry out new experiments in an available
                     corpus, reproducing experiments in the art historians’ own corpus and applying
                     transfer learning techniques within a domain with few resources. Four different
                     resources developed are described: the transcribed corpus, the DL-based transcription
                     model, the annotated corpus and the DL models developed for the annotation using a
                     specific domain-based set of labels in a small corpus. The CLARA-TM transcription
                     model learned for the DM is accessible from January 2023 at the READ-COOP website
                     under the title “Spanish print XVIII-XIX - Free Public AI Model
                     for Text Recognition with Transkribus”(<a href="https://readcoop.eu/model/spanish-print-xviii-xix/" onclick="window.open('https://readcoop.eu/model/spanish-print-xviii-xix/'); return false" class="ref">https://readcoop.eu/model/spanish-print-xviii-xix/</a>).</p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1. Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Corpora construction is a laborious task ([<a class="ref" href="#gruszczynski_etal2021">Gruszczyński et al. 2021</a>];
                     [<a class="ref" href="#ruizfaboetal2017">Ruiz Fabo et al. 2017</a>]; [<a class="ref" href="#wissleretal2014">Wissler et al. 2014</a>]), since it is
                     desirable that the corpora may be used by different people and serve various
                     purposes. Corpora are classified according to different parameters, such as subject
                     matter, purpose, discourse modality and others, some of the most important being:
                     balance, representativeness, or transparency ([<a class="ref" href="#davies_parodi2022">Davies and Parodi 2022</a>];
                     [<a class="ref" href="#gebruetal2021">Gebru et al. 2021</a>]; [<a class="ref" href="#torruellacasanas2017">Torruella Casañas 2017</a>]). The
                     process of corpus construction is divided into different phases, ranging from the
                     definition of its boundaries and purpose, pre-processing, storage, annotation, to
                     name a few ([<a class="ref" href="#aldamaetal2022">Aldama et al. 2022</a>]; [<a class="ref" href="#nakayama2021">Nakayama 2021</a>]; [<a class="ref" href="#calvotello2019">Calvo Tello 2019</a>]; [<a class="ref" href="#kabatek2013">Kabatek 2013</a>]; [<a class="ref" href="#rojo2010">Rojo 2010</a>]).</div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Corpus-based research has been dominated by statistical and neural models ([<a class="ref" href="#morenosandoval2019">Moreno Sandoval 2019</a>]; [<a class="ref" href="#nieuwenhuijsen2016">Nieuwenhuijsen 2016</a>]; [<a class="ref" href="#rojo2016">Rojo 2016</a>]) until the end of the last decade, when Transformer-based
                     models appeared [<a class="ref" href="#vaswanietal2017">Vaswani et al. 2017</a>], requiring the availability of very
                     large corpora for training. Such massive corpora are scarcely available in the field
                     of Humanities mainly because the corpora need to be annotated according to the
                     interests of the corpus end-users, so it happens that the types of entities often
                     vary according to the origin, language, domain, or purpose of the dataset. The Named
                     Entity Recognition (NER) discipline, dealing with entities of interest, has evolved
                     a
                     lot since its beginnings in the first competitive NER task in 1996 [<a class="ref" href="#grishman_sundheim1996">Grishman and Sundheim 1996</a>], as many other tasks and datasets have been
                     created for evaluation.</div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">Early NER systems made use of algorithms based on hand-built rules, lexicons and
                     gazetteers, orthographic features or ontologies, among others, with the human cost
                     of
                     producing domain-specific resources that this entails [<a class="ref" href="#lietal2022">Li et al.2022</a>].
                     Later, these systems were followed by those based on feature engineering and machine
                     learning [<a class="ref" href="#nadeau_sekine2007">Nadeau and Sekine 2007</a>], which were the dominant technique in the
                     task of named entity recognition until the first decade of the 2000s, that is, until
                     the NLP revolution with the advent of neural networks. The most common supervised
                     machine learning systems of this type that were used for NER include Hidden Markov
                     Models (HMMs) [<a class="ref" href="#bikeletal1997">Bikel et al. 1997</a>], Support Vector Machines (SVM) [<a class="ref" href="#asahara_matsumoto2003">Asahara and Matsumoto 2003</a>], Conditional Random Fields (CRF) [<a class="ref" href="#mccallum_li2003">McCallum and Li 2003</a>], Maximum Entropy models (ME) [<a class="ref" href="#borthwicketal1998">Borthwick et al. 1998</a>], and decision trees [<a class="ref" href="#sekine1998">Sekine 1998</a>].</div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">Starting with [<a class="ref" href="#collobert_etal2011">Collobert et al. 2011</a>], neural network-based systems are
                     interesting because they do not require domain-specific resources such as lexicons
                     or
                     ontologies and are therefore more domain-independent [<a class="ref" href="#yadav_bethard2018">Yadav and Bethard 2018</a>]. In this context, several neural network architectures were proposed, mostly based
                     on some form of recurrent neural network (RNN) on characters [<a class="ref" href="#lampleetal2016">Lample et al. 2016</a>], and embeddings of words or word components [<a class="ref" href="#akbik_blythe_vollgraf2018">Akbik, Blythe, and Vollgraf 2018</a>]. Finally in 2017 the Transformer
                     architecture was introduced in the paper <cite class="title italic">Attention Is All You
                        Need</cite>
                     [<a class="ref" href="#vaswanietal2017">Vaswani et al. 2017</a>] which is here to stay, and its derivatives such as
                     BERT [<a class="ref" href="#devlin_etal2019">Devlin et al. 2019</a>] and RoBERTa [<a class="ref" href="#liuetal2019">Liu et al. 2019</a>], which
                     we make use of today, as well as the Spanish-based MarIA<a class="noteRef" href="#d3e358">[1]</a> or RigoBERTa<a class="noteRef" href="#d3e364">[2]</a>.</div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">In the domain of Digital Humanities (DH), applying NER models to historical documents
                     poses several challenges, as shown by other works in the domain ([<a class="ref" href="#chastang_torresaguilar_tannier2021">Chastang, Torres Aguilar, and Tannier 2021</a>]; [<a class="ref" href="#kettunenetal2017">Kettunen et al. 2017</a>]). This is a fertile field within NLP, since cultural institutions are carrying out
                     digitization projects in which large amounts of images containing text are obtained
                     ([<a class="ref" href="#piotrowski2012">Piotrowski 2012</a>]; [<a class="ref" href="#terras2011">Terras 2011</a>]). One of the
                     challenges is the margin of error still present in optical character recognition
                     (OCR) systems [<a class="ref" href="#borosetal2020">Boros et al. 2020</a>], since historical documents are
                     generally very noisy, contain smudges, and have different typographies that are
                     generally unknown to the systems, which makes character recognition even more
                     difficult.</div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Another challenge is related to the transfer of knowledge to new domains or
                     languages, in this case to adapt NER models trained with datasets in current
                     languages to old languages ([<a class="ref" href="#baptisteetal2021">Baptiste et al. 2021</a>]; [<a class="ref" href="#bollmann2019">Bollmann 2019</a>]; [<a class="ref" href="#detoni_etal2022">De Toni et al. 2022</a>]). Transfer learning
                     techniques have become common practice in a wide range of tasks in NLP ([<a class="ref" href="#hintz_biemann2016">Hintz and Biemann 2016</a>]; [<a class="ref" href="#pruksachatkunetal2020">Pruksachatkun et al. 2020</a>]; [<a class="ref" href="#zophetal2016">Zoph et al. 2016</a>]), including the domain of DH such as works presented in
                     the workshop on Natural Language Processing for Digital Humanities at NLPAI 2021 [<a class="ref" href="#blouinetal2021">Blouin et al. 2021</a>]
                     [<a class="ref" href="#rubinstein_shmidman2021">Rubinstein and Shmidman 2021</a>]). </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Recent initiatives have been launched, such as the creation and annotation of
                     datasets ([<a class="ref" href="#ehrmannetal2022">Ehrmann et al. 2022</a>]; [<a class="ref" href="#neudecker2016">Neudecker 2016</a>]) and
                     holding competitive events such as HIPE (<cite class="title italic">Identifying Historical
                        People, Places and other Entities</cite>) [<a class="ref" href="#ehrmannetal2020a">Ehrmann et al. 2020a</a>]
                     addressing NER in historical texts. The general goals of HIPE include improving the
                     robustness of systems, enabling comparison of the performance of NER systems on
                     historical texts, and, in the long term, fostering efficient semantic indexing in
                     historical documents. The HIPE2020 corpus shares objectives with the CLARA-DM corpus,
                     since it involves the recognition of entities in historical newspapers, and has
                     specific types of entities, that are slightly different from the general NER ones
                     of
                     localizations (LOC), persons (PER), organizations (ORG) and miscellanea (MISC). It
                     is
                     common practice in this domain that Historians, Linguists and Computer Science
                     researchers collaborate to seek information on the domain-dependent entities and
                     scenarios in which the historical research is focused ([<a class="ref" href="#rivero2022">Rivero 2022</a>];
                     [<a class="ref" href="#merinorecalde2022">Merino Recalde 2022</a>]; [<a class="ref" href="#garcia-serrano_castellanos2016">García-Serrano and Castellanos 2016</a>]; [<a class="ref" href="#calle-gomez_garcia-serrano_martinez2006">Calle-Gómez, García-Serrano, and Martínez, 2006</a>]). We propose the use of
                     the HIPE project approach with the aim of taking advantage of these resources and
                     gaining perspective on the approach for our domain, a task for which we have little
                     annotated data.</div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">This paper is organized starting with a section dedicated to the corpus creation,
                     transcription and sub-corpus manual annotation. Available tools are evaluated
                     according to the properties of the original DM newspaper collection, the total amount
                     of data contained and offered and required software to be installed. Afterwards, a
                     section is devoted to the experimentation setting to perform the CLARA-DM corpus
                     automatic annotation using deep learning models for NER. First, we carry out some
                     experiments with a resource on multilingual historical newspapers such as the
                     HIPE2020 corpus containing historical OCR texts in French, English and German [<a class="ref" href="#ehrmannetal2020b">Ehrmann et al. 2020b</a>]. One main goal is to identify which type of process
                     adaptation and Transformer-based models will obtain the best results for the CLARA-DM
                     corpus, based on the experience with HIPE2020. Then, we conduct some experiments on
                     CLARA-DM dataset, in a transfer learning set-up (zero-shot and few-shot learning)
                     between (1) three different sets of entity types: the general NER ones, the one used
                     in the HIPE2020 experiments, and the domain specific ones used in CLARA-DM, and (2)
                     languages (Spanish, English and German). Finally, in the last section we outline the
                     conclusions drawn from the experiments and conclude the paper with some plans for
                     future work.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2. Creation of the corpus CLARA-DM</h1>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">A major goal of the CLARA-HD<a class="noteRef" href="#d3e438">[3]</a> collaborative project involving
                     art historians, linguists, and computer scientists, is to speed up the art
                     historians’ research, through the transcription of the PDF newspapers’ pages and the
                     development of a robust model for recognition of named entities. The work started
                     with a comparison of the performance of other NER systems on historical texts,
                     following the IMPRESSO<a class="noteRef" href="#d3e440">[4]</a> project and the HIPE series of shared tasks whose objective is to foster
                     efficient semantic indexing on historical documents. The CLARA-HD project is based
                     on
                     the joint research of art historians, who are keen on investigating what is done and
                     where it is done in the city of Madrid ([<a class="ref" href="#molinamartin2021">Molina Martín 2021</a>]; [<a class="ref" href="#camara_molina_vazquez2020">Cámara, Molina, and Vázquez 2020</a>]; [<a class="ref" href="#molina_vega2018">Molina and Vega 2018</a>]); the
                     technicians, who are experts in Natural Language Processing and Deep Learning Models
                     in applied research ([<a class="ref" href="#menta_garcia-serrano2022">Menta and García-Serrano 2022</a>]; [<a class="ref" href="#sanchez-salido2022">Sánchez-Salido 2022</a>]; [<a class="ref" href="#menta_sanchez-salido_garcia-serrano2022">Menta, Sánchez-Salido, and García-Serrano 2022</a>]; [<a class="ref" href="#garcia-serrano_menta-garuz2022">García-Serrano and Menta-Garuz 2022</a>]), and the linguists, well-known experts
                     in corpus creation, analysis, and exploitation ([<a class="ref" href="#campillos-llanosetal2022">Campillos-Llanos et al. 2022</a>]; [<a class="ref" href="#morenosandovaletal2018">Moreno Sandoval et al. 2018</a>]).</div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">The starting point of the CLARA-HD project was the analysis of the art historians’
                     interest in the digitized collection of newspapers called <cite class="title italic">Diario
                        de Madrid</cite>, published between the 18th and 19th centuries and readily
                     available at the BNE<a class="noteRef" href="#d3e471">[5]</a>. The focus of art historians is on information related to historical persons,
                     locations but also objects for sale or lost, so the first step is the construction
                     of
                     a historical corpus from the original newspapers, the so-called CLARA-DM corpus,
                     which involves designing a style guideline for transcription and a second style
                     guideline for the annotation of named entities, which entails identifying the
                     domain-based set of labels (semantic categories). The next step is the manual
                     annotation of a sub-corpus to serve as the training corpus for an application
                     development using current DL techniques. Finally, the development of an efficient
                     model to recognise the specific named entities is tackled to annotate the CLARA-DM
                     corpus automatically.</div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">There are different tools to deal with any process in corpus management, from the
                     document transcription, storage and search phase, annotation, analysis, and even
                     corpus exploitation using Python libraries for information extraction. In this work
                     we use Transkribus<a class="noteRef" href="#d3e480">[6]</a> for transcription, Tagtog<a class="noteRef" href="#d3e486">[7]</a> for annotation, and HuggingFace<a class="noteRef" href="#d3e492">[8]</a> for the implementation of Deep Learning models. In the remainder of this
                     section the first two steps are detailed, and the third one will be tackled in the
                     following section.</div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.1. Transcription of the digitalized newspapers</h2>
                     
                     <div class="counter"><a href="#p12">12</a></div>
                     <div class="ptext" id="p12">The first step building the corpus is the transcription of the texts, which we
                        carried out using the Transkribus tool [<a class="ref" href="#menta_sanchez-salido_garcia-serrano2022">Menta, Sánchez-Salido, and García-Serrano 2022</a>]. The tool was selected once
                        compared with commercial and open-source transcription tools as Amazon Textract<a class="noteRef" href="#d3e507">[9]</a> and Google’s Tesseract<a class="noteRef" href="#d3e513">[10]</a>, as the accuracy for old typography was slightly better, no fee for basic
                        functionalities of the tool was required and has been used by other projects in
                        ancient languages ([<a class="ref" href="#kirmizialtin_wrisley2022">Kirmizialtin and Wrisley 2022</a>]; [<a class="ref" href="#arandagarcia2022">Aranda García 2022</a>]; [<a class="ref" href="#ayusogarcia2022">Ayuso García 2022</a>]; [<a class="ref" href="#bazzacoetal2022">Bazzaco et al. 2022</a>]; [<a class="ref" href="#alrasheed_rao_grieco2021">Alrasheed, Rao, and Grieco 2021</a>]; [<a class="ref" href="#derrick2019">Derrick 2019</a>]). Also, it provides functionalities both on the web
                        browser and in the client version, and has proven to be very helpful for small DH
                        research groups [<a class="ref" href="#perdiki2022">Perdiki 2022</a>].</div>
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">Transcription using the Transkribus tool starts by requiring the layout
                        recognition, which consists of detecting the text regions and lines of text within
                        the documents. This automatic process is not perfect, since the model does not
                        recognise only text but also recognises some regions such as lines or spots that
                        we have to remove manually. Furthermore, the main problem with newspapers is that
                        there are tables or columns, and the model is not able to recognise the order in
                        which it should read the pages. That is why we have to carry out a manual task to
                        sort the text. It should be noted that we do this process because we make use of
                        models that use sentences for training (Transformers for NER), and we also want
                        the corpus to be used in the future for semantic and syntactic analysis. Note that
                        the layout is not so relevant when only a word-based analysis is wanted.</div>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">Once we have carried out the structure recognition of the newspaper pages, the
                        so-called layout recognition, we can move on to transcribing the text of the
                        pages, either manually or using a public model, since the Transkribus tool
                        contains public models that are trained with historical texts in different
                        languages. We explored some of them, but they were still unable to recognise the
                        text with some quality. The "Spanish Golden Age Theatre Prints 1.0" model ([<a class="ref" href="#cuellar_vegagarcia-luengos2021">Cuéllar and Vega García-Luengos 2021</a>]; [<a class="ref" href="#cuellar2021">Cuéllar 2021</a>])
                        especially failed to recognise numbers or capital letters in our documents. So,
                        what we propose to do is to develop our own transcription model (CLARA-TM) for the
                        transcription of the documents in the CLARA-DM corpus. The first question was to
                        find out how many pages we needed for a model in Transkribus to learn to
                        transcribe automatically. The more training data, the better, but according to
                        Transkribus guidelines, it is possible to start training the model with at least
                        25-75 pages manually transcribed, or less when working with printed instead of
                        handwritten texts <a class="noteRef" href="#d3e543">[11]</a>. </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">We carried out several tests. In the first one we trained the model with 37
                        manually transcribed newspaper pages and obtained an error rate in character
                        recognition (CER) of 4% in the validation set. This error decreases in successive
                        tests with more amount and homogeneous training data. We realised that most errors
                        in the transcription were caused by a lack of uniformity in the manual
                        transcriptions, since they were carried out by different people. In order to make
                        a more reliable transcription model, the transcribed pages were reviewed manually
                        and a standardisation process wase carried out. This includes aspects such as the
                        unification of the way fractions are transcribed (1/2 o 1⁄2), the inclusion or
                        omission of symbols such as "=" (used before an author's signature), "&amp;" or
                        "§§", or the correction or not of typos. After this process, there is a high
                        degree of homogeneity in the data that the model will see in its training, and
                        therefore a higher probability of successful learning. After several tests
                        (performed by changing the model parameters (number of epochs, learning rate and
                        documents selected for training and validation) we trained the model with 193
                        transcribed pages and obtained less than a 1% error, so we have kept it as our own
                        transcription model for the CLARA-DM corpus. The obtained CER is a good one,
                        slightly better than that obtained from public models that use similar resources
                        to ours<a class="noteRef" href="#d3e550">[12]</a>.
                        The pages used for training correspond to 37 different newspapers randomly chosen
                        (between the ones downloaded, which covered the first day of every month between
                        1788-1825), since they all have a similar structure: 4-8 pages, the first one
                        containing a table and both capital and lower case letters, and pages divided in
                        columns at the end of the newspaper.</div>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">From now, the model can be applied to transcribe text automatically from any
                        <cite class="title italic">Diario de Madrid</cite> newspaper (see Figure 1) with a
                        layout recognition carried out manually. The original collection has 13,479
                        newspapers (59,424 pages) and the CLARA-DM corpus currently has 589 newspapers
                        with layout recognition done (2,474 pages), 37 newspapers manually transcribed
                        (201 pages), 143 newspapers automatically transcribed (657 pages, containing an
                        average of 1% of errors in the transcriptions), and 10 newspapers manually
                        annotated (53 pages/24,843 tokens).</div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="Scan of printed book page juxtaposed with is own line-by-line numbered&#xA;                     transcription." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>Hardcopy of an original page (left) and its automatic transcription with
                           CLARA-TM (right).</div>
                     </div>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">The human and computational resources spent for the development of our
                        transcription model are quite important in the project planning and, given that
                        the project is currently funded by the Spanish Government, the model is already
                        published (January 2023) for free use at the Transkribus tool and website<a class="noteRef" href="#d3e597">[13]</a>.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.2. Sub-corpus manual annotation</h2>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">Once we have the transcribed corpus, we can move on to the manual annotation step,
                        but why do we need to do this? The datasets used for a NER task can be annotated
                        or not, depending on whether the system to be developed is supervised or
                        unsupervised. Since we are going to use Transformer-based systems for leading the
                        current state of the art, we need annotated data to train them. On the other hand,
                        a general NER system usually includes the general entities categories
                        (tags/labels) Person, Place, Organisation and Others. However, in the CLARA-DM
                        corpus of historical newspapers we need to define a different set of tags
                        according to the needs of art historians, so that is the second reason why we have
                        to carry out the task of manual annotation first to “teach” (train) the
                        model. Furthermore, the presence of one or more types of encoding and annotations
                        is a very important aspect in the possibilities of corpus exploitation [<a class="ref" href="#rojo2010">Rojo 2010</a>].</div>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">To decide which annotation tool to use, an evaluation was made between four
                        current leading annotation tools: Prodigy<a class="noteRef" href="#d3e619">[14]</a>, Doccano<a class="noteRef" href="#d3e625">[15]</a>, Brat<a class="noteRef" href="#d3e631">[16]</a> and Tagtog<a class="noteRef" href="#d3e637">[17]</a>. The comparison of their characteristics is shown in Table 1 (general
                        functionalities; whether or not there are user management facilities; whether
                        inter-annotator agreement is automatically calculated; if it is possible to
                        automate tagging; input and output formats; operating system availability;
                        availability of a web version; whether or not Python programming tools need to be
                        installed locally, and finally if is for free, open-source and if user programming
                        skills are required). The Tagtog tool was chosen because it is the only one that
                        integrates a metric for viewing the agreement between annotators. In addition, it
                        allows us to visualize it as it is annotated, which makes the annotation flow much
                        more dynamic.</div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Prodigy</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Doccano</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Brat</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Tagtog</span></td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">General functionalities</span></td>
                              
                              <td valign="top" class="cell">tagging text, images and videos and train models with the tagged
                                 data</td>
                              
                              <td valign="top" class="cell">text classification, sequence labelling, sequence-to-sequence
                                 tasks</td>
                              
                              <td valign="top" class="cell">entity and relationship annotation, lookups and other NLP derived
                                 tasks</td>
                              
                              <td valign="top" class="cell">entity and relationship annotation, document classification</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">User management</span></td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Metrics for inter-annotator agreement</span></td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">yes</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Possibility to automate tagging</span></td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">yes (under subscription)</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Input formats</span></td>
                              
                              <td valign="top" class="cell">TXT, JSONL, JSON, CSVand others</td>
                              
                              <td valign="top" class="cell">TXT, JSONL, CoNLL</td>
                              
                              <td valign="top" class="cell">TXT</td>
                              
                              <td valign="top" class="cell">TXT, CSV, source code files, URLs and others</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Output formats</span></td>
                              
                              <td valign="top" class="cell">JSONL</td>
                              
                              <td valign="top" class="cell">JSONL</td>
                              
                              <td valign="top" class="cell">.ann</td>
                              
                              <td valign="top" class="cell">TXT, HTML, XML, CSV, ann.json, EntitiesTsv, others</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Operating system</span></td>
                              
                              <td valign="top" class="cell">Windows, Mac and Linux</td>
                              
                              <td valign="top" class="cell">Windows, Mac and Linux</td>
                              
                              <td valign="top" class="cell">Mac or Linux (on Windows it is recommended a virtual machine)</td>
                              
                              <td valign="top" class="cell">Windows, Mac and Linux</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Requires interacting with the command line</span></td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">no (on the web version)</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Needs Python installed</span></td>
                              
                              <td valign="top" class="cell">yes (3.6+)</td>
                              
                              <td valign="top" class="cell">yes (3.8+)</td>
                              
                              <td valign="top" class="cell">yes (2.5+)</td>
                              
                              <td valign="top" class="cell">no</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Programming skills required</span></td>
                              
                              <td valign="top" class="cell">familiarity with Python is desirable</td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">knowledge of Linux and Apache servers is required</td>
                              
                              <td valign="top" class="cell">no (on the web version)</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Open source</span></td>
                              
                              <td valign="top" class="cell">Partially</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Free</span></td>
                              
                              <td valign="top" class="cell">no</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes</td>
                              
                              <td valign="top" class="cell">yes (different subscriptions)</td>
                              </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>Comparison of tagging tools.</div>
                     </div>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20">The process of the manual annotation of a corpus involves the construction of an
                        “annotation guideline” in which the types of labels/tags are defined and
                        specifications are given on what to annotate and what not to annotate, from where
                        and to where to annotate, etc ([<a class="ref" href="#campillos-llanosetal2021">Campillos-Llanos et al. 2021</a>]; [<a class="ref" href="#morenosandovaletal2018">Moreno Sandoval et al. 2018</a>]). Achieving a high annotation agreement
                        enables the number of annotators in the future to be increased.</div>
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">The process also includes the definition of the set of labels, that were defined
                        together with art historians in several turns. On the one hand, the art historians
                        know the information they need, but they do not have the perspective of the
                        computer scientist who knows what kind of categories the model is able to learn to
                        generalise. It is therefore a delicate task that requires an effort of
                        understanding on both sides. When deciding on the set of labels, these can
                        describe broad categories, such as persons, places, organisations, etc, or finer
                        categories, such as streets and squares within the place category or nobles and
                        lords within the person category. It is more convenient to start with a finer or
                        more granular set of labels, as converting these categories into their
                        corresponding broad versions is simpler than carrying out the reverse
                        operation.</div>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">Based on the dialogue with the art historians and the documents they provided us
                        with, a first proposal of taxonomy of entities was drawn up. It contained a wide
                        set of entities and sub-entities (such as different kinds of religious places or
                        houses) that is expected to be reduced in order to increase the quality of the
                        automatic annotation. These entities are dumped into the annotation tool and the
                        first annotation cycle was carried out.</div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="Table showing rates of agreement between entites by percentage." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 2. </div>Inter-Annotator Agreement for Person entity class automatically calculated
                           by the tool.</div>
                     </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">The first sub-corpus contains five newspapers with 28 pages, 928 sentences and
                        15,145 tokens, which are annotated by four annotators using a blind annotation
                        process, that is, each person annotates the document independently without
                        consulting the others, following the guidelines. The tool then calculates the
                        Inter-Annotator Agreement (IAA) for each entity (see Figure 2). With these
                        metrics, the quality of the labels is re-evaluated, and the taxonomy is adjusted.
                        In the second round the tag taxonomy is as shown in Figure 3.</div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.jpeg" rel="external"><img src="resources/images/figure03.jpeg" style="" alt="Flowchart showing the organization of entities, e.g.  and&#xA;                         both become part of the larger entity&#xA;                     ." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Taxonomy of entities.</div>
                     </div>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">Figure 4 shows an annotated text using the Tagtog tool that shares the colour
                        legend classes shown at Figure 3: <cite class="title italic">iglesia de san Luis
                           </cite>is a (multiword) entity denoting a religious building; <span class="foreign i">actores, coristas, bailarines</span> are professions.</div>
                     
                     <div class="figure">
                        
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" style="" alt="A text sample with words and phrases tagged in many colors on the left,&#xA;                     juxtaposed with a table of tagged words and their corresponding categories on&#xA;                     the right." /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Tagtog tool: hardcopy of a manually annotated text (left) and an excerpt
                           codified in the IOB format (right).</div>
                     </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">Finally, once all the occurrences in the sub-corpus of historical newspapers are
                        manually annotated/tagged, the Tagtog tool provides different export formats,
                        including ann.json. After, the annotations output is converted into the IOB format
                        in order to train the machine learning models (as described in the next section).
                        To obtain the documents in IOB format, a new script is designed to transform the
                        Tagtog EntitiesTsv output format into IOB format. At this moment the development
                        of the deep learning system can start (the so-called DM model).</div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3. Towards an automatic annotation model for CLARA-DM</h1>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">Once we have some CLARA-DM manually annotated (previously transcribed) newspapers
                     (sub-corpus), some experiments to perform a NER task automatically reusing or
                     transforming into a new one previously developed models in similar annotation
                     settings can be carried out. The question is whether we can use one of the existing
                     NER models or not, and how. </div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">A brief explanation of the methodology and terminology of the experiments is as
                     follows:</div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">First, we experiment with the HIPE2020 dataset (notice that we can use the
                           terms corpus or dataset interchangeably, although the latter has a more
                           computational nuance). We seek to evaluate the performance of monolingual and
                           multilingual models (being the latter larger and trained in several languages),
                           and to see if knowledge transfers between languages, through monolingual and/or
                           multilingual models in a fine-tuning setup. Then we look at knowledge transfer
                           between tasks, that is, we evaluate whether it is beneficial for a different task
                           to use models trained with datasets for the general NER task (and therefore have
                           different labels than HIPE2020).</li>
                        <li class="item">In a similar approach, we then experiment with the CLARA-DM dataset. First, we
                           carry out some experiments without training the selected models (zero-shot
                           experiments)we carry out some zero-shot experiments, that is, we evaluate on
                           CLARA-DM dataset some models trained for general tags in a NER task or for NER on
                           historical newspapers, and compare which setup achieves better results. After
                           that, we train models with the CLARA-DM dataset and see if adding more historical
                           training data improves the results. We include a preliminary qualitative error
                           analysis on the results obtained for this first evaluation step based on HIPE2020
                           and CLARA-DM datasets.</li>
                        <li class="item">Then we carry out a second evaluation step, in which we evaluate several
                           aspects. The first one is to study whether the method of adjudication for the
                           final version of the manually annotated documents plays a role in the performance
                           of the models (that is, when there are several annotators, there are different
                           versions of the annotations and it is necessary to decide which label is the final
                           one). The second one is a measurement of the performance based on the development
                           of the annotation guideline versions, that is, the way in which the documents are
                           annotated, and the availability of more documents annotated with the latest
                           guidelines to see the gain in performance.</li>
                     </ul>
                  </div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">All the previous steps imply the selection of different available DL models to decide
                     justifiably if we have to develop our own model, as we did for transcription.</div>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">The models used for the experiments are based on RoBERTa (monolingual) [<a class="ref" href="#liuetal2019">Liu et al. 2019</a>] and XLM-RoBERTa<a class="noteRef" href="#d3e940">[18]</a> (multilingual) [<a class="ref" href="#conneau_etal2020">Conneau et al.2020</a>]. Among the monolingual
                     models we experiment with:</div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">two Spanish: RoBERTa-BNE<a class="noteRef" href="#d3e953">[19]</a> from the MarIA project [<a class="ref" href="#gutierrez-fandinoetal2022">Gutiérrez-Fandiño et al. 2022</a>] and BERTin-RoBERTa<a class="noteRef" href="#d3e961">[20]</a> from the BERTin project [<a class="ref" href="#delarosa_etal2022">De la Rosa et al. 2022</a>],</li>
                        <li class="item">one English: DistilRoBERTa<a class="noteRef" href="#d3e972">[21]</a>
                           [<a class="ref" href="#sanhetal2019">Sanh et al. 2019</a>],</li>
                        <li class="item">one French: DistilCamemBERT<a class="noteRef" href="#d3e983">[22]</a>
                           [<a class="ref" href="#delestre_amar2022">Delestre and Amar 2022</a>] and</li>
                        <li class="item">one German: GottBERT<a class="noteRef" href="#d3e994">[23]</a>
                           [<a class="ref" href="#scheibleetal2020">Scheible et al. 2020</a>].</li>
                     </ul>
                  </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">On the other hand, we use models that have been trained for a general set of NER
                     tags:</div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">one monolingual for Spanish: RoBERTa-BNE-NER-CAPITEL<a class="noteRef" href="#d3e1010">[24]</a>,</li>
                        <li class="item">and two multilingual ones, one for Spanish (XLM-RoBERTa-NER-Spanish<a class="noteRef" href="#d3e1019">[25]</a>) and another one trained in 10 languages with high resources (XLM-RoBERTa-NER-HRL<a class="noteRef" href="#d3e1025">[26]</a>).</li>
                     </ul>
                  </div>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">The working environment is a Google Colaboratory notebook, which provides a NVIDIA
                     Tesla T4 GPU with 16GB of RAM and CUDA version 11.2. In addition, Transformers
                     4.11.3, Datasets 1.16.1, HuggingFace Tokenizers 0.10.3 and Pytorch 1.12.1+cu113
                     libraries are installed for running the experiments.</div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">In the following sub-sections the subsequent experiments and related analysis of the
                     results are described:</div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">1) Using the HIPE2020 dataset, that contains different multilingual sub-corpus and
                     entities annotated with specific tags, different from the general ones. Two different
                     strategies are studied. The first one is the fine-tuning to observe both whether the
                     monolingual training in French and German transfers to English, and if the
                     multilingual model trained only with French or German improves in other languages
                     not
                     trained with. The second one is the evaluation of the transfer of knowledge from
                     models using general tags to models with a different set of tags.</div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">2) As the CLARA-DM dataset uses its specific set of tags, different from the HIPE2020
                     ones, two strategies are used for the first set of experiments: on the one hand, the
                     use of models trained with external NER datasets (generalist or specific) on a
                     zero-shot setup, and on the other hand, training with the CLARA-DM labelled data on
                     a
                     few-shot learning set-up. Some experiments use the CAPITEL dataset<a class="noteRef" href="#d3e1042">[27]</a> from IberLEF2020 (the task is a general NER for Spanish).</div>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">3) After the discussion and conclusions on the first evaluation step, a new step for
                     experiments is planned in order to evaluate (a) the method of adjudicating the final
                     version of the manually annotated newspapers, (b) different aspects of the annotation
                     guidelines (the way of annotating the classes and the total amount of tags), and (c)
                     the amount of training data.</div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.1. Experiments with HIPE2020</h2>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">The HIPE2020 (<cite class="title italic">Identifying Historical People, Places and other
                           Entities</cite>) competitive event held at the CLEF conference, shares several
                        objectives with the work presented as it focuses on the evaluation of NLP,
                        information extraction and information retrieval systems. The HIPE2020 corpus [<a class="ref" href="#ehrmannetal2020b">Ehrmann et al. 2020b</a>] made available for experimentation in this
                        competition is a collection of digitized historical documents in three languages:
                        English, French and German. The documents come from the archives of different
                        Swiss, Luxembourg and American newspapers. The dataset was annotated following the
                        HIPE annotation guidelines [<a class="ref" href="#ehrmannetal2020c">Ehrmann et al. 2020c</a>], which in turn was
                        derived from the Quaero annotation guidelines [<a class="ref" href="#rosset_grouin_zweigenbaum2011">Rosset, Grouin, and Zweigenbaum 2011</a>]. The corpus uses the IOB format,
                        providing training, test and validation sets for French and German, and no
                        training corpus for English. The goal was to gain new insights and prospectives
                        into the transferability of entity recognition approaches across languages, time
                        periods, document types, and annotation tag sets.</div>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">The HIPE2020 corpus is annotated with the labels of Person, Place, Organization,
                        Time, and Human Production. It contains 185 German documents totalling 149,856
                        tokens, 126 English documents totalling 45,695 tokens, and 244 French documents
                        with 245,026 tokens. In total, they make up a corpus of 555 documents and 440,577
                        tokens. The dataset is pre-processed to recover the phrases that make up the
                        documents and to be able to pass them to the models together with the labels,
                        obtaining a total of 7,887 phrases in French (of which 5,334 correspond to the
                        training -166,217 tokens-, 1,186 to validation and 1,367 to test), 5,462 sentences
                        in German (of which 3,185 correspond to training -86,444 tokens-, 1,136 to
                        validation and 1,141 to test), and 1,437 sentences in English (938 in validation
                        and 499 in test). Note that the French training set is considerably larger than
                        the German training set.</div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.1.1. Fine-tuning</h3>
                        
                        <div class="counter"><a href="#p38">38</a></div>
                        <div class="ptext" id="p38">The first experiments consist of fine-tuning on the HIPE2020 dataset. When
                           training a machine learning model there are several hyperparameters to be
                           configured. The “number of epochs” is the number of times that the
                           algorithm is going through the whole training dataset. The “batch size” is
                           the number of training examples (in this case, sentences) used in one
                           iteration. And the “learning rate” determines the pace at which an
                           algorithm updates or learns the values of the parameters. The models’
                           hyperparameters are configured for a training in 3 epochs and a batch size of
                           12 in both the training and validation set, and a 5e-5 learning rate. The rest
                           of the model configuration is set by default using the AutoConfig, AutoModel
                           and AutoTokenizer classes of the Huggingface Transformers library. First, we
                           fine-tune three monolingual models, which are shown in the first three rows of
                           Table 2, and then the multilingual model, whose results are shown in the last
                           three rows. In both cases we train first with the French dataset, then with the
                           German dataset, and thirdly with French and German jointly, since the English
                           sub-corpus has no training dataset.</div>
                        
                        <div class="counter"><a href="#p39">39</a></div>
                        <div class="ptext" id="p39">The evaluation metrics are based on precision, recall and F1. Briefly
                           explained, precision is the relationship (fraction) of relevant instances among
                           the retrieved instances, whilst recall is the fraction of relevant instances
                           that were retrieved. The F1 measure is the harmonic mean of the precision and
                           recall.</div>
                        
                        <div class="counter"><a href="#p40">40</a></div>
                        <div class="ptext" id="p40">The objective of the first experiment is to analyse whether the knowledge
                           learned on the NER training with the historical texts in French and German
                           transfers to English. Secondly, whether the multilingual training with one
                           language improves the performance in the other languages. We find that both
                           claims hold true. The performance annotating the English sub-corpus of a model
                           trained jointly in French and German improves compared to the performance of
                           the models trained only with French or German (both when using the monolingual
                           English model and the multilingual model) as shown with the results in the
                           third row, that are better than the ones in the first and second rows, as well
                           as the results in the sixth row, that are better than those in the fourth and
                           fifth ones. Also, when training the multilingual model only with French or
                           German, the result improves in the languages in which it has not been trained.
                           For example, when training DistilCamemBERT with the French sub-corpus, the F1
                           in the German sub-corpus is 0.19, whilst when training XLM-RoBERTa with French,
                           the F1 in German is 0.63.</div>
                        
                        <div class="counter"><a href="#p41">41</a></div>
                        <div class="ptext" id="p41">Moreover, it is noteworthy that the multilingual model manages to equal or even
                           improve the results of the monolingual models.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" rowspan="2"></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="3">FR</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="3">DE</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="2">EN</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">DistilCamemBERT-<span class="hi bold">fr</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.74</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.8</span></td>
                                 
                                 <td valign="top" class="cell">0.77</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.13</td>
                                 
                                 <td valign="top" class="cell">0.36</td>
                                 
                                 <td valign="top" class="cell">0.19</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.38</td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">GottBERT-<span class="hi bold">de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.28</td>
                                 
                                 <td valign="top" class="cell">0.38</td>
                                 
                                 <td valign="top" class="cell">0.32</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.69</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell">0.72</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.4</td>
                                 
                                 <td valign="top" class="cell">0.52</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">DistilRoBERTa-<span class="hi bold">fr+de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.66</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell">0.7</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell">0.59</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.4</td>
                                 
                                 <td valign="top" class="cell">0.6</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-<span class="hi bold">fr</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.76</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.8</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.78</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell">0.72</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">0.61</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-<span class="hi bold">de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.61</td>
                                 
                                 <td valign="top" class="cell">0.68</td>
                                 
                                 <td valign="top" class="cell">0.65</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.69</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell">0.72</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-<span class="hi bold">fr+de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.76</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.8</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.78</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.75</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.76</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.76</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.59</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.62</span></td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 2. </div>Experiments with monolingual and multilingual models on French, German
                              and English HIPE2020 datasets.</div>
                        </div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.1.2. Transfer of knowledge with general NER datasets</h3>
                        
                        <div class="counter"><a href="#p42">42</a></div>
                        <div class="ptext" id="p42">In view of the usefulness of the multilingual model in the previous results, in
                           the following experiments we use the multilingual model trained for NER in 10
                           languages with high resources.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" rowspan="2"></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="3">FR</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="3">DE</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell" colspan="2">EN</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-ner-hrl</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.4</td>
                                 
                                 <td valign="top" class="cell">0.6</td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-ner-hrl-<span class="hi bold">fr</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.77</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.82</span></td>
                                 
                                 <td valign="top" class="cell">0.79</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell">0.7</td>
                                 
                                 <td valign="top" class="cell">0.68</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-ner-hrl-<span class="hi bold">de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.71</td>
                                 
                                 <td valign="top" class="cell">0.73</td>
                                 
                                 <td valign="top" class="cell">0.72</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.73</td>
                                 
                                 <td valign="top" class="cell">0.77</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.64</span></td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-ner-hrl-<span class="hi bold">fr+de</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.78</span></td>
                                 
                                 <td valign="top" class="cell">0.68</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.8</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.76</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.8</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.78</span></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">0.6</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.68</span></td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 3. </div>Evaluation on HIPE2020 of the model XLM-RoBERTa trained on 10 high
                              resource languages for NER</div>
                        </div>
                        
                        <div class="counter"><a href="#p43">43</a></div>
                        <div class="ptext" id="p43">The model is first evaluated on HIPE2020 without training (the so-called
                           zero-shot learning), which is shown in the first row of the table. Then the
                           model is trained with the HIPE2020 datasets, first French, then German and
                           finally with both. Briefly, the zero-shot transfer learning means that we are
                           taking a model trained for a specific or general task, and directly applying it
                           on a different task or dataset for which the model has not been trained.</div>
                        
                        <div class="counter"><a href="#p44">44</a></div>
                        <div class="ptext" id="p44">The results are slightly better than those obtained in the previous
                           experiments, but in return we are evaluating on a general set of labels
                           (Person, Location, Organization, and Dates), different from the one that
                           HIPE2020 uses.</div>
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.2. Experiments with CLARA-DM</h2>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">With the insights we have extracted from the results of previous experiments, we
                        move on to carrying out experiments with our developed dataset. To carry out these
                        experiments we have the manually annotated sub-corpus of 5 newspapers, which have
                        been obtained from the annotations of between 3 and 4 annotators and merged using
                        the majority vote method. After a pre-processing phase carried out using the spaCy<a class="noteRef" href="#d3e1571">[28]</a> package to delimit the sentences that make up the newspapers, a dataset of
                        928 sentences is obtained, with a total of 15,145 tokens. The annotation
                        guidelines were in a preliminary version, and the inter-annotator agreement was
                        still to be improved. Therefore, at this point we tackle different
                        experiments.</div>
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46">The CLARA-DM dataset has a large and original set of labels. This implies that, in
                        order to obtain a specific NER model for the dataset, it will be necessary to have
                        enough training data. We will adopt two strategies to carry out the first
                        experiments, on the one hand, making use of models trained with external NER
                        datasets (generalist or specific), and on the other hand, training with the
                        available labelled data.</div>
                     
                     <div class="counter"><a href="#p47">47</a></div>
                     <div class="ptext" id="p47">The set of labels of the dataset is extensive: it includes the generic labels of
                        person, place, establishment, profession, ornaments, furniture, sales and losses
                        or findings, and also the sub-labels person_lords (for nobles, high officials,
                        etc), place_address (for streets, squares, gateways), place_religious (convents,
                        parishes), place_hospital, place_college and place_fountain. In total, they make
                        up a set of 14 tags, which when duplicated in the IOB format and together with the
                        empty tag “O”, add up to a total of 29 tags. This increases the complexity
                        for the models to learn, and therefore the need for sufficient training data. On
                        the other hand, in order to apply zero-shot learning, the names of the labels must
                        be changed and simplified so that they are the same as those of the training
                        datasets of the models, thus losing the more specific labels (such as religious
                        places, ornaments or objects for sale) and drastically reducing the size of the
                        set of labels, with the loss of information and efforts made during the labelling
                        process that this entails.</div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell">PER</td>
                              
                              <td valign="top" class="cell">SEÑOR</td>
                              
                              <td valign="top" class="cell">LOC</td>
                              
                              <td valign="top" class="cell">RELIG</td>
                              
                              <td valign="top" class="cell">DIREC</td>
                              
                              <td valign="top" class="cell">COLE</td>
                              
                              <td valign="top" class="cell">HOSP</td>
                              
                              <td valign="top" class="cell">PROF</td>
                              
                              <td valign="top" class="cell">ESTABLEC</td>
                              
                              <td valign="top" class="cell">VENTA</td>
                              
                              <td valign="top" class="cell">PÉRDIDA</td>
                              
                              <td valign="top" class="cell">ADOR</td>
                              
                              <td valign="top" class="cell">Total</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">347</td>
                              
                              <td valign="top" class="cell">49</td>
                              
                              <td valign="top" class="cell">78</td>
                              
                              <td valign="top" class="cell">28</td>
                              
                              <td valign="top" class="cell">112</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">89</td>
                              
                              <td valign="top" class="cell">81</td>
                              
                              <td valign="top" class="cell">32</td>
                              
                              <td valign="top" class="cell">14</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell">837</td>
                              </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 4. </div>Label distribution in CLARA-DM.</div>
                     </div>
                     
                     <div class="counter"><a href="#p48">48</a></div>
                     <div class="ptext" id="p48">The distribution of the tags in these documents is shown in Table 4. The class
                        with by far the most examples is person, followed by address, profession,
                        establishment, place, and lords. The classes of religious places, losses, schools,
                        hospitals, and ornaments are notably underrepresented, and those of fountains and
                        furniture do not even appear in the dataset (because they might not have been
                        annotated by at least two people and therefore do not appear in the final
                        version).</div>
                     
                     <div class="counter"><a href="#p49">49</a></div>
                     <div class="ptext" id="p49">In the following we describe the experiments carried out to study the benefits of
                        the transfer of knowledge between tasks and languages on our CLARA-DM dataset.</div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.2.1. Zero-shot using CLARA-DM as test set</h3>
                        
                        <div class="counter"><a href="#p50">50</a></div>
                        <div class="ptext" id="p50">In these first two experiments we are not training with CLARA-DM but using it
                           as a test set. This means that we are not evaluating on the set of labels of
                           CLARA-DM but on the ones of the datasets that the models have been trained on.
                           In Table 5 we evaluate two models trained for NER in Spanish, one multilingual
                           and one monolingual. In Table 6, models trained with HIPE2020 (specific labels
                           in historical newspapers) or with the CAPITEL dataset from IberLEF2020 (general
                           NER for Spanish) are evaluated in CLARA-DM.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R-ner-spanish</td>
                                 
                                 <td valign="top" class="cell">0.39</td>
                                 
                                 <td valign="top" class="cell">0.48</td>
                                 
                                 <td valign="top" class="cell">0.43</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne-ner-capitel</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.43</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.53</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.48</span></td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 5. </div>Evaluation on CLARA-DM of general NER models for Spanish.</div>
                        </div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-fr</span></td>
                                 
                                 <td valign="top" class="cell">0.43</td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 
                                 <td valign="top" class="cell">0.48</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-fr-de</span></td>
                                 
                                 <td valign="top" class="cell">0.44</td>
                                 
                                 <td valign="top" class="cell">0.49</td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-fr-de-en</span></td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 
                                 <td valign="top" class="cell">0.51</td>
                                 
                                 <td valign="top" class="cell">0.48</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne<span class="hi bold">-fr</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.47</span></td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.51</span></td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne-new-capitel<span class="hi bold">-fr</span></td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 
                                 <td valign="top" class="cell">0.47</td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">BERTin<span class="hi bold">-fr</span></td>
                                 
                                 <td valign="top" class="cell">0.42</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.6</span></td>
                                 
                                 <td valign="top" class="cell">0.5</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 6. </div>Evaluation on CLARA-DM with models trained with HIPE2020.</div>
                        </div>
                        
                        <div class="counter"><a href="#p51">51</a></div>
                        <div class="ptext" id="p51">We find that in both cases the monolingual models are slightly better. In the
                           case of training with HIPE2020, it turns out to be more beneficial to train
                           only with French, than to add English and German, since it is the most similar
                           language to Spanish. Moreover, it is better to train with the French sub-corpus
                           of HIPE2020, that tackles the same task (NER in historical newspapers), than
                           training with CAPITEL, which tackles general NER for Spanish, so the task
                           influences the model.</div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.2.2. Few-shot/fine-tuning on CLARA-DM</h3>
                        
                        <div class="counter"><a href="#p52">52</a></div>
                        <div class="ptext" id="p52">In these experiments we train with the few data we have manually annotated
                           (that is why the experiments are few-shot learning and not zero-shot learning)
                           using 3 newspapers as training, 1 as validation and 1 as test (containing
                           approximately 700 sentences for training, 120 for validation and 110 for test).
                           On Table 7 the results of fine-tuning several models are shown, the best one
                           being the Spanish monolingual BERTin model.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.41</td>
                                 
                                 <td valign="top" class="cell">0.52</td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.42</td>
                                 
                                 <td valign="top" class="cell">0.50</td>
                                 
                                 <td valign="top" class="cell">0.46</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">BERTin-R<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.48</td>
                                 
                                 <td valign="top" class="cell">0.58</td>
                                 
                                 <td valign="top" class="cell">0.52</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 7. </div>Fine-tuning on CLARA-DM.</div>
                        </div>
                        
                        <div class="counter"><a href="#p53">53</a></div>
                        <div class="ptext" id="p53">Even if we consider the corpus very small in comparison with the HIPE2020 one,
                           (700 sentences versus 7,900 in the French sub-corpus) it turns out that with
                           only 3 newspapers for training, similar results are achieved to those of
                           directly evaluating the models trained with HIPE2020 shown in Table 6.</div>
                        
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-fr-clara</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.59</span></td>
                                 
                                 <td valign="top" class="cell">0.64</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.61</span></td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne<span class="hi bold">-fr-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">0.61</td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne-ner-capitel<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 
                                 <td valign="top" class="cell">0.59</td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne-ner-capitel<span class="hi bold">-fr-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.55</td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">BERTin-R<span class="hi bold">-fr-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.54</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.68</span></td>
                                 
                                 <td valign="top" class="cell">0.6</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 8. </div>Training and evaluation in CLARA-DM of models trained with HIPE2020 and
                              CAPITEL.</div>
                        </div>
                        
                        <div class="counter"><a href="#p54">54</a></div>
                        <div class="ptext" id="p54">Lastly, in Table 8 the training with CLARA-DM has been combined with training
                           with HIPE2020 (only the French part) and CAPITEL. Again, we have obtained the
                           best results of all experiments until now, but at the cost of evaluating on a
                           different set of labels than that of CLARA-DM and therefore wasting the
                           annotation effort.</div>
                        
                        <div class="counter"><a href="#p55">55</a></div>
                        <div class="ptext" id="p55">This is the only case in which the performance of monolingual and multilingual
                           models are very similar. Here it is interesting to note that, again, it gives
                           better results to train with the French HIPE2020, which contains historical
                           newspapers, than with CAPITEL, which is Spanish for generic NER in general
                           domains. CAPITEL<a class="noteRef" href="#d3e1983">[29]</a>
                           contains texts after 2005 on the following topics: Science and technology;
                           Social sciences, beliefs and thought; Politics, economy and justice; Arts
                           culture and shows; Current affairs, leisure and daily life; Health and
                           Others.</div>
                        
                        <div class="counter"><a href="#p56">56</a></div>
                        <div class="ptext" id="p56">At this point, it is worth noting that the labels that each model had in its
                           (first) training in each experiment are as follows:</div>
                        
                        <div class="ptext">
                           <ul class="list simple">
                              <li class="item">in Table 5, the tags for the experiment XLM-R-ner-spanish are general
                                 person, location, organization, and miscellaneous, and those of the
                                 experiment RoBERTa-bne-ner-capitel are those of CAPITEL, that is person,
                                 location, organization and others (in BIOES format instead of BIO),</li>
                              <li class="item">in Table 6 all the experiments have the labels of HIPE2020, except for
                                 RoBERTa-bne-ner-capitel-fr, which has those of CAPITEL,</li>
                              <li class="item">in Table 7 the labels are those of CLARA-DM and</li>
                              <li class="item">in Table 8 the labels are those of HIPE2020 or CAPITEL, whichever comes
                                 first.</li>
                           </ul>
                        </div>
                        
                        <div class="counter"><a href="#p57">57</a></div>
                        <div class="ptext" id="p57">Since we have seen that models trained with a different set of labels (HIPE2020
                           or CAPITEL ones) are able to predict with some quality the labels that they
                           have in common with the CLARA-DM dataset, we can do the opposite experiment. In
                           order not to lose the wide range of labels in CLARA-DM, we can first train the
                           models adding fictitious tags, so that in the second training with CLARA-DM we
                           include all the classes.</div>
                        
                        <div class="counter"><a href="#p58">58</a></div>
                        <div class="ptext" id="p58">For example, regarding the XLM-RoBERTa model: when fine-tuning it first with
                           French HIPE2020 and after with CLARA-DM, we obtained metrics of around 60%
                           (first row of Table 8), but we were evaluating only on the tags Person, Place
                           and Organization, which are the ones HIPE2020 has in common with the CLARA-DM
                           corpus. If we change the classes in the first training with HIPE2020 by adding
                           the ones present in CLARA-DM, and fine-tune first with HIPE2020 and then with
                           CLARA-DM, we get the metrics shown in Table 9. Performance drops by 14% on
                           average, but in return we are evaluating the corpus on the whole CLARA-DM
                           tagset, with a model trained with both CLARA-DM and HIPE2020.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-fr-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.44</td>
                                 
                                 <td valign="top" class="cell">0.51</td>
                                 
                                 <td valign="top" class="cell">0.47</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 9. </div>Evaluation on CLARA-DM of a model trained first with HIPE and then with
                              CLARA-DM, with the tags of CLARA-DM corpus.</div>
                        </div>
                        
                        <div class="counter"><a href="#p59">59</a></div>
                        <div class="ptext" id="p59">Comparing this performance with that obtained when only training with CLARA-DM
                           (Table 7) (since here we are training with both HIPE2020 and CLARA-DM), the
                           results are quite similar, so apparently there is no real added value when
                           adding the first training with HIPE2020, that is, using more resources.</div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.2.3. Qualitative analysis</h3>
                        
                        <div class="counter"><a href="#p60">60</a></div>
                        <div class="ptext" id="p60">As a preliminary qualitative analysis, or error analysis, Table 10 shows the
                           performance per label of the best model fine-tuned with CLARA-DM, which was
                           BERTin (Table 7).</div>
                        
                        <div class="counter"><a href="#p61">61</a></div>
                        <div class="ptext" id="p61">It is interesting to note that the results are consistent with the current
                           state of the annotation guidelines, since entities such as persons, locations
                           and religious places have a high degree of inter-annotator agreement, above
                           70%, being those that obtain the best metrics, while others such as
                           establishments or objects for sale still need to be revised through the
                           guideline and the model also has a harder time identifying them correctly. This
                           seems to be even more relevant than the inner imbalance of the dataset, since
                           classes such as religious places do not have many appearances or occurrences,
                           but the model recognises them with a high degree of accuracy.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell">support</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Establishment</td>
                                 
                                 <td valign="top" class="cell">establec</td>
                                 
                                 <td valign="top" class="cell">0.23</td>
                                 
                                 <td valign="top" class="cell">0.31</td>
                                 
                                 <td valign="top" class="cell">0.26</td>
                                 
                                 <td valign="top" class="cell">29</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Place or Location</td>
                                 
                                 <td valign="top" class="cell">loc</td>
                                 
                                 <td valign="top" class="cell">0.79</td>
                                 
                                 <td valign="top" class="cell">0.71</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.75</span></td>
                                 
                                 <td valign="top" class="cell">21</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Place - College</td>
                                 
                                 <td valign="top" class="cell">loc_cole</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Place - Address</td>
                                 
                                 <td valign="top" class="cell">loc_direc</td>
                                 
                                 <td valign="top" class="cell">0.42</td>
                                 
                                 <td valign="top" class="cell">0.78</td>
                                 
                                 <td valign="top" class="cell">0.55</td>
                                 
                                 <td valign="top" class="cell">23</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Place - Religious</td>
                                 
                                 <td valign="top" class="cell">loc_relig</td>
                                 
                                 <td valign="top" class="cell">0.80</td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.73</span></td>
                                 
                                 <td valign="top" class="cell">6</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Losses or Findings</td>
                                 
                                 <td valign="top" class="cell">perdida_hallazgo</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Person</td>
                                 
                                 <td valign="top" class="cell">pers</td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">1.00</td>
                                 
                                 <td valign="top" class="cell">0.70</td>
                                 
                                 <td valign="top" class="cell">8</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Person - Lords</td>
                                 
                                 <td valign="top" class="cell">pers_señores</td>
                                 
                                 <td valign="top" class="cell">0.56</td>
                                 
                                 <td valign="top" class="cell">0.64</td>
                                 
                                 <td valign="top" class="cell">0.60</td>
                                 
                                 <td valign="top" class="cell">14</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Trades and Professions</td>
                                 
                                 <td valign="top" class="cell">prof</td>
                                 
                                 <td valign="top" class="cell">0.61</td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell">0.64</td>
                                 
                                 <td valign="top" class="cell">21</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Sales</td>
                                 
                                 <td valign="top" class="cell">venta</td>
                                 
                                 <td valign="top" class="cell">0.50</td>
                                 
                                 <td valign="top" class="cell">0.08</td>
                                 
                                 <td valign="top" class="cell">0.14</td>
                                 
                                 <td valign="top" class="cell">12</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"> </td>
                                 
                                 <td valign="top" class="cell">Micro avg</td>
                                 
                                 <td valign="top" class="cell">0.48</td>
                                 
                                 <td valign="top" class="cell">0.58</td>
                                 
                                 <td valign="top" class="cell">0.52</td>
                                 
                                 <td valign="top" class="cell">135</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">Macro avg</td>
                                 
                                 <td valign="top" class="cell">0.44</td>
                                 
                                 <td valign="top" class="cell">0.49</td>
                                 
                                 <td valign="top" class="cell">0.44</td>
                                 
                                 <td valign="top" class="cell">135</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">Weighted avg</td>
                                 
                                 <td valign="top" class="cell">0.51</td>
                                 
                                 <td valign="top" class="cell">0.58</td>
                                 
                                 <td valign="top" class="cell">0.51</td>
                                 
                                 <td valign="top" class="cell">135</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 10. </div>Metrics of each label with BERTin model fine-tuned on CLARA-DM.</div>
                        </div>
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.3. First evaluation step discussion</h2>
                     
                     <div class="counter"><a href="#p62">62</a></div>
                     <div class="ptext" id="p62">In the experiments with HIPE2020 we have observed that the use of multilingual
                        models is beneficial when we have datasets in several languages for the NER task,
                        as they allow the knowledge to be transferred to languages with fewer or no
                        resources. Furthermore, we have seen that including domain-generic datasets
                        slightly improves the results, but at the cost of evaluating on a different set of
                        labels, and therefore wasting the efforts of the tagging procedure.</div>
                     
                     <div class="counter"><a href="#p63">63</a></div>
                     <div class="ptext" id="p63">In spite of not having a particularly robust or sophisticated model, and although
                        very precise results were not one main goal of the work, results of around 80% on
                        the F1 measure for French and German, and 65% for English, which has no training
                        data, have been achieved (previous Section 3.1).</div>
                     
                     <div class="counter"><a href="#p64">64</a></div>
                     <div class="ptext" id="p64">As regards the experiments with CLARA-DM, in general, better results have been
                        obtained with the monolingual models in Spanish, except when we have trained
                        jointly with CLARA-DM and HIPE2020 datasets, in which the multilingual model has
                        been on a par with the monolingual ones. It has also been shown that the
                        similarity between Spanish and French favours the transfer of knowledge within the
                        same domain, and that this transfer is even better than training with a generic
                        NER dataset in the same language.</div>
                     
                     <div class="counter"><a href="#p65">65</a></div>
                     <div class="ptext" id="p65">An important conclusion for corpora of languages or domains with scarce resources
                        has also been initially contrasted: the importance of the inter-annotator
                        agreement over the dataset imbalance.</div>
                     
                     <div class="counter"><a href="#p66">66</a></div>
                     <div class="ptext" id="p66">In addition, as with three annotated newspapers, the results of fine-tuning with
                        CLARA-DM achieve similar results than evaluating in CLARA-DM a model trained on
                        HIPE2020, even if, the joint training with HIPE2020 and CLARA-DM has not given
                        rise to a great improvement in results. That is shown in Table 6 (zero-shot) were
                        models trained with HIPE2020 are evaluated in CLARA-DM with results of around 50%
                        F1, while in Table 7 (fine-tuning in CLARA-DM) a 50% F1 is also achieved just by
                        training with only 3 newspapers. So, it is for sure that with more annotated
                        documents, good results can be expected.</div>
                     
                     <div class="counter"><a href="#p67">67</a></div>
                     <div class="ptext" id="p67">The results are susceptible to improvement, since the quality of the annotation
                        guidelines is still to be enhanced, and so the inter-annotator agreement, which
                        will lead to have more quality and homogeneous data.</div>
                     
                     <div class="counter"><a href="#p68">68</a></div>
                     <div class="ptext" id="p68">From this analysis, the plan for the second evaluation step is to try to improve
                        the models obtained in this first evaluation step, and to measure the gain in
                        performance, once we have more robust annotation guidelines, and more annotated
                        newspapers. As will be described in the next section, the experiments are based on
                        the three models used for fine-tuning in CLARA-DM (Table 7) since it has been
                        shown that training with more added datasets is not so beneficial, but it is
                        better to have more data in CLARA-DM.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.4. Second evaluation step</h2>
                     
                     <div class="counter"><a href="#p69">69</a></div>
                     <div class="ptext" id="p69">In order to confirm the previous results, in this series of experiments the
                        following parameters are evaluated: the method of adjudicating the final version
                        of the manually annotated newspapers, aspects of the annotation guidelines (the
                        way of annotating the classes and the total amount of tags), and the amount of
                        training data.</div>
                     
                     <div class="counter"><a href="#p70">70</a></div>
                     <div class="ptext" id="p70">In Tagtog, when several users annotate the same document, as a result, there are
                        different annotation versions. Adjudication is the process of resolving
                        inconsistencies between these versions before promoting a version to master (final
                        version). In other words, the different annotators’ versions are merged into one,
                        (using various strategies). Adjudication can either be manual (when a reviewer
                        promotes a version to master) or automatic<a class="noteRef" href="#d3e2314">[30]</a>, based on different adjudication methods such as the IAA (or Best
                        Annotators) or the Majority Vote. Automatic adjudication based on Best Annotators
                        means that for each single annotation task, the annotations of the user with the
                        best IAA are promoted to master. The goal is to have the best annotations
                        available for each annotation task in master version. Furthermore, automatic
                        adjudication by Majority Vote means that for each single annotation, it is
                        promoted to master only if it was annotated by over 50% of the annotators.</div>
                     
                     <div class="counter"><a href="#p71">71</a></div>
                     <div class="ptext" id="p71">First, an experiment is carried out with the same documents as before but obtained
                        with a different adjudication method. While in the first experiments the final
                        version was obtained by the Majority Vote method, in this case the Best Annotators
                        method is used.</div>
                     
                     <div class="counter"><a href="#p72">72</a></div>
                     <div class="ptext" id="p72">Then, the progress of the annotation guidelines is evaluated, as well as the gain
                        in performance with a bigger number of annotated newspapers.</div>
                     
                     <div class="counter"><a href="#p73">73</a></div>
                     <div class="ptext" id="p73">In these experiments we will limit ourselves to carrying out experiments
                        exclusively with the CLARA-DM dataset (not HIPE2020, CAPITEL, etc) and with the
                        models used in the previous few-shot experiments. The newspapers to be used will
                        be those of the first experiments and also new annotated newspapers.</div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.4.1. Evaluation of the adjudication method</h3>
                        
                        <div class="counter"><a href="#p74">74</a></div>
                        <div class="ptext" id="p74">By carrying out the experiments in Table 7, that is, fine-tuning with CLARA-DM,
                           but instead with the final annotations obtained by the Best Annotators method,
                           we get the results shown in Table 11.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-R<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.47</td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">0.50</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.49</td>
                                 
                                 <td valign="top" class="cell">0.55</td>
                                 
                                 <td valign="top" class="cell">0.52</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">BERTin-R<span class="hi bold">-clara</span></td>
                                 
                                 <td valign="top" class="cell">0.37</td>
                                 
                                 <td valign="top" class="cell">0.47</td>
                                 
                                 <td valign="top" class="cell">0.41</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 11. </div>Fine-tuning with CLARA-DM (the same experiments as in Table 7), but with
                              final version of the annotations obtained with the Best Annotators
                              method.</div>
                        </div>
                        
                        <div class="counter"><a href="#p75">75</a></div>
                        <div class="ptext" id="p75">With RoBERTa-BNE model the F1 measure improves from 0.46 to 0.52, accuracy from
                           0.42 to 0.49, and recall from 0.50 to 0.55. And with XLM-RoBERTa, the F1
                           measure improves from 0.46 to 0.5, precision from 0.41 to 0.47 and recall from
                           0.52 to 0.53. However, with the BERTin model, which achieved the best results
                           in Table 7, the F1 measure has decreased from 0.52 to 0.41, accuracy from 0.48
                           to 0.37, and recall from 0.58 to 0.47.</div>
                        
                        <div class="counter"><a href="#p76">76</a></div>
                        <div class="ptext" id="p76">That is, by changing the adjudication method, the best performing method has
                           changed, even though the F1 measure of 0.52 is still not surpassed.</div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.4.2. Progress of annotation guidelines and availability of more training
                           data</h3>
                        
                        <div class="counter"><a href="#p77">77</a></div>
                        <div class="ptext" id="p77">The annotation guidelines are adjusted in several turns, by analysing both the
                           Inter Annotator Agreement and the performance of the models.</div>
                        
                        <div class="counter"><a href="#p78">78</a></div>
                        <div class="ptext" id="p78">Eleven new newspapers were annotated in accordance with the new guidelines. In
                           particular, the place_fountains entity is deleted, since we consider it from
                           now on included within the furniture. Also, place_college and place_hospital
                           tags are deleted (and included in establishments), since the three entities had
                           very few mentions in the newspapers. Finally, the category Organization
                           (administrative bodies) is created, to differentiate it from that of
                           Establishments (commerce, leisure, services and others) and to be in line with
                           other common annotation guidelines.</div>
                        
                        <div class="counter"><a href="#p79">79</a></div>
                        <div class="ptext" id="p79">All in all, we get a set of 12 classes: two for people (person_general,
                           person_lord) three for places (place_general, place_address, place_religious),
                           establishments, organizations, professions, and four for objects (ornaments,
                           furniture, sales, losses/findings), having the taxonomy shown in Figure 5.</div>
                        
                        <div class="figure">
                           
                           
                           
                           <div class="ptext"><a href="resources/images/figure05.jpeg" rel="external"><img src="resources/images/figure05.jpeg" style="" alt="Flowchart showing label taxonomy for the 12 classes." /></a></div>
                           
                           <div class="caption">
                              <div class="label">Figure 5. </div>Final label taxonomy for CLARA-DM.</div>
                        </div>
                        
                        <div class="counter"><a href="#p80">80</a></div>
                        <div class="ptext" id="p80">In intermediate steps, some different ways of labelling were evaluated. For
                           example, at some point we agreed to annotate the profession of a person within
                           the <em class="term">person</em> tag whenever they appeared contiguously (as in <cite class="title italic">Sr. D. Josef de la Cruz y Loyola, Gobernador de dicho Real
                              Sitio</cite>). However, this proves ambiguous, and we confirm that is it
                           better to label more concrete and nuclear entities, since it is clearer for
                           annotators, and thus improves the IAA, and in turn leads to better performance
                           of the models on the classes with better IAA.</div>
                        
                        <div class="counter"><a href="#p81">81</a></div>
                        <div class="ptext" id="p81">The results of fine-tuning the models with the 11 new annotated newspapers,
                           annotated with the final guidelines and the final version obtained with the
                           Best Annotators method, are shown in Table 12. In this case we used 7
                           newspapers for training, that contained 1,228 sentences, which nearly doubles
                           the number of sentences that we had for the first experiments.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">XLM-RoBERTa-clara</td>
                                 
                                 <td valign="top" class="cell">0.74</td>
                                 
                                 <td valign="top" class="cell">0.79</td>
                                 
                                 <td valign="top" class="cell">0.76</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">RoBERTa-bne-clara</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.75</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.80</span></td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.78</span></td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">BERTin-R-clara</td>
                                 
                                 <td valign="top" class="cell"><span class="hi bold">0.75</span></td>
                                 
                                 <td valign="top" class="cell">0.77</td>
                                 
                                 <td valign="top" class="cell">0.76</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 12. </div>Training and evaluation in CLARA-DM with more newspapers and new
                              guidelines.</div>
                        </div>
                        
                        <div class="counter"><a href="#p82">82</a></div>
                        <div class="ptext" id="p82">While the results of the first fine-tuning had a performance of around 50% in
                           all the metrics (Table 7), with the updated annotation guidelines and double
                           the number of sentences for the training, metrics of more than 75% have been
                           achieved. It is also observed that when the IAA improves for a specific class,
                           so the models get to predict it better, even when there are fewer examples of
                           the class.</div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">P</td>
                                 
                                 <td valign="top" class="cell">R</td>
                                 
                                 <td valign="top" class="cell">F1</td>
                                 
                                 <td valign="top" class="cell">support</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Places or Locations</td>
                                 
                                 <td valign="top" class="cell">loc</td>
                                 
                                 <td valign="top" class="cell">0.89</td>
                                 
                                 <td valign="top" class="cell">0.84</td>
                                 
                                 <td valign="top" class="cell">0.87</td>
                                 
                                 <td valign="top" class="cell">50</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Places — Streets and Squares</td>
                                 
                                 <td valign="top" class="cell">loc_direc</td>
                                 
                                 <td valign="top" class="cell">0.82</td>
                                 
                                 <td valign="top" class="cell">0.94</td>
                                 
                                 <td valign="top" class="cell">0.87</td>
                                 
                                 <td valign="top" class="cell">111</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Places – Religious Buildings</td>
                                 
                                 <td valign="top" class="cell">loc_relig</td>
                                 
                                 <td valign="top" class="cell">0.64</td>
                                 
                                 <td valign="top" class="cell">0.62</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell">26</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Organizations, Institutions</td>
                                 
                                 <td valign="top" class="cell">org_adm</td>
                                 
                                 <td valign="top" class="cell">0.53</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell">0.58</td>
                                 
                                 <td valign="top" class="cell">27</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Establishments</td>
                                 
                                 <td valign="top" class="cell">org_establec</td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell">0.60</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell">50</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Persons</td>
                                 
                                 <td valign="top" class="cell">pers</td>
                                 
                                 <td valign="top" class="cell">0.81</td>
                                 
                                 <td valign="top" class="cell">1.00</td>
                                 
                                 <td valign="top" class="cell">0.89</td>
                                 
                                 <td valign="top" class="cell">216</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Persons - Lords</td>
                                 
                                 <td valign="top" class="cell">pers_señores</td>
                                 
                                 <td valign="top" class="cell">0.58</td>
                                 
                                 <td valign="top" class="cell">0.40</td>
                                 
                                 <td valign="top" class="cell">0.47</td>
                                 
                                 <td valign="top" class="cell">55</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Ornaments</td>
                                 
                                 <td valign="top" class="cell">prod_ador</td>
                                 
                                 <td valign="top" class="cell">0.80</td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell">7</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Furniture</td>
                                 
                                 <td valign="top" class="cell">prod_mobil</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">0.00</td>
                                 
                                 <td valign="top" class="cell">1</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Losses or Findings</td>
                                 
                                 <td valign="top" class="cell">prod_perdida-hallazgo</td>
                                 
                                 <td valign="top" class="cell">0.82</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell">0.78</td>
                                 
                                 <td valign="top" class="cell">12</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Sales</td>
                                 
                                 <td valign="top" class="cell">prod_venta</td>
                                 
                                 <td valign="top" class="cell">0.62</td>
                                 
                                 <td valign="top" class="cell">0.57</td>
                                 
                                 <td valign="top" class="cell">0.59</td>
                                 
                                 <td valign="top" class="cell">14</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell">Trades and professions</td>
                                 
                                 <td valign="top" class="cell">prof</td>
                                 
                                 <td valign="top" class="cell">0.66</td>
                                 
                                 <td valign="top" class="cell">0.67</td>
                                 
                                 <td valign="top" class="cell">0.66</td>
                                 
                                 <td valign="top" class="cell">78</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"> </td>
                                 
                                 <td valign="top" class="cell">Micro avg</td>
                                 
                                 <td valign="top" class="cell">0.75</td>
                                 
                                 <td valign="top" class="cell">0.80</td>
                                 
                                 <td valign="top" class="cell">0.78</td>
                                 
                                 <td valign="top" class="cell">647</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">Macro avg</td>
                                 
                                 <td valign="top" class="cell">0.65</td>
                                 
                                 <td valign="top" class="cell">0.63</td>
                                 
                                 <td valign="top" class="cell">0.64</td>
                                 
                                 <td valign="top" class="cell">647</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell"></td>
                                 
                                 <td valign="top" class="cell">Weighted avg</td>
                                 
                                 <td valign="top" class="cell">0.74</td>
                                 
                                 <td valign="top" class="cell">0.80</td>
                                 
                                 <td valign="top" class="cell">0.77</td>
                                 
                                 <td valign="top" class="cell">647</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 13. </div>Metrics of each label with RoBERTa-BNE.</div>
                        </div>
                        
                        <div class="counter"><a href="#p83">83</a></div>
                        <div class="ptext" id="p83">If we take a look at the performance per entity class as in Table 13, one
                           noticeable aspect is that entities that do not contain proper names are usually
                           harder to predict, such as products or professions. We might consider tagging
                           these making use of predefined lists instead of the NER model. Furthermore,
                           sometimes there are very few occurrences of these classes (such is the case of
                           the furniture in Table 13) and this affects the performance as a whole.</div>
                        
                        <div class="counter"><a href="#p84">84</a></div>
                        <div class="ptext" id="p84">On the other hand, the results are also very conditioned by the choice of
                           training and test data, since we do not yet have enough examples. Even so, it
                           is shown that both stronger annotation guidelines and the availability of more
                           documents improves the performance of the models.</div>
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">4. Proposed methodology</h1>
                  
                  <div class="counter"><a href="#p85">85</a></div>
                  <div class="ptext" id="p85">As a summary of the methodology used in this work, the following is a list of the
                     necessary steps necessary to carry out a process of recognition of named entities
                     in
                     historical documents.As a summary of the methodology used in this work, the steps
                     followed are listed below.</div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item"><span class="hi bold">Digitization:</span> The first step is to digitize the historical
                           documents if they are not already in a digital format. This can be done using
                           scanners or digital cameras. This step has already been carried out by the BNE in
                           this work.</li>
                        <li class="item"><span class="hi bold">Optical Character Recognition (OCR):</span> Once the documents
                           have been digitized, OCR software is used to convert the text-based images into
                           machine-encoded text. This stage may involve manual error correction to deal with
                           the inaccuracies of the OCR process, particularly with historical documents that
                           may have faded or smudged ink or unusual typography. As explained in previous
                           sections, we used a layout recognition model and trained our own model in
                           Transkribus to obtain the text.</li>
                        <li class="item"><span class="hi bold">Annotation guidelines:</span> The initial stage in training a NER
                           model involves establishing rules for accurate entity identification. It's crucial
                           to explicitly define the types of named entities that the model is expected to
                           recognize.</li>
                        <li class="item"><span class="hi bold">Annotation:</span> The entities to be recognized by the NER
                           process are then annotated. This involves marking up the text with tags that
                           indicate the type of entity. This is usually done manually by human annotators,
                           using annotation software.</li>
                        <li class="item"><span class="hi bold">Validation:</span> The annotated text is then validated to ensure
                           the accuracy of the entity recognition and annotation processes. This can involve
                           a review by human annotators, or the use of validation software that compares the
                           annotated text to a gold standard or benchmark. In this work, we used Tagtog for
                           annotation and validation.</li>
                        <li class="item"><span class="hi bold">Training the model:</span> The final step is to train a named
                           entity recognition model from the annotated and validated text. This involves the
                           separation of the data into a training set, a validation set and a test set. This
                           is followed by the selection of the most appropriate model and finally by training
                           the model several times with different hyperparameters.</li>
                        <li class="item"><span class="hi bold">Evaluation:</span> The performance of the NER process should be
                           evaluated using appropriate metrics such as precision, recall, and F1-score. This
                           helps to identify areas for improvement and guide future work.</li>
                     </ul>
                  </div>
                  
                  <div class="counter"><a href="#p86">86</a></div>
                  <div class="ptext" id="p86">The methodology employed in this research can be extended to other domains and
                     languages, facilitating advancements in various fields. For instance, by leveraging
                     multilingual models and adapting them to other languages, similar NER tasks in
                     historical newspapers from different countries can be accomplished. Additionally,
                     applying the developed annotation guidelines and expanding the dataset to include
                     newspapers from diverse regions and time periods would enable the automatic
                     annotation and prediction of entities in a broader range of Spanish newspapers, and
                     potentially other languages. This adaptability and transferability of the methodology
                     make it a valuable resource for historians and researchers working on various textual
                     collections beyond historical newspapers, such as ancient manuscripts, legal
                     documents, or literary works in different languages. By scaling up the annotated data
                     and training the models with more diverse samples, the accuracy and robustness of
                     the
                     NER models can be further enhanced, fostering more efficient and accurate analyses
                     in
                     the digital humanities and beyond.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5. Conclusions and future work</h1>
                  
                  <div class="counter"><a href="#p87">87</a></div>
                  <div class="ptext" id="p87">The characteristics of digitized newspapers and the research interests of historians
                     justify the need to develop the CLARA-DM corpus, a model for transcription, and a
                     specific model for named entity recognition in these texts.</div>
                  
                  <div class="counter"><a href="#p88">88</a></div>
                  <div class="ptext" id="p88">As regards named entity recognition, we have seen that cross-language and
                     cross-domain knowledge is transferred not only with multilingual models, but also
                     with monolingual ones. On the other hand, having corpora or datasets for the same
                     specific task (NER in historical newspapers in this case) in other languages might
                     be
                     useful and is even better than having generic datasets in the same language.</div>
                  
                  <div class="counter"><a href="#p89">89</a></div>
                  <div class="ptext" id="p89">In the developed CLARA-DM dataset, the monolingual models and the use of a dataset
                     of
                     historical newspapers in French have stood out because it is a language close to
                     Spanish.</div>
                  
                  <div class="counter"><a href="#p90">90</a></div>
                  <div class="ptext" id="p90">Nevertheless, the use of external datasets could not compete with having more
                     annotated data of our own corpus. In the final experiments we found that an
                     improvement in the annotation guidelines and an increase in the labelled data
                     significantly improves the performance of the models. In addition, it is verified
                     that the models are sensitive to choices such as the method of adjudication for the
                     final version of the annotations, or the choice of data for training and testing.</div>
                  
                  <div class="counter"><a href="#p91">91</a></div>
                  <div class="ptext" id="p91">We believe that the entity recognition system can be improved in future research by
                     using the Simple Knowledge Organisation System (SKOS) and creating an ontology. Based
                     on the trained models, and after reviewing the discovered entities by historians,
                     we
                     propose to create a taxonomy. This will be used to improve the identification of new
                     mentions in other newspapers. In addition, storing information in an ontology will
                     allow more complex queries at run time by reducing the ambiguity of entities. For
                     example, having all the mentions of a particular location in a single URI would allow
                     you to queringqueryingy all the people who have traded there. </div>
                  
                  <div class="counter"><a href="#p92">92</a></div>
                  <div class="ptext" id="p92">In summary, from a collection of newspapers in PDF format it has been possible to
                     obtain a model for its transcription, with an accuracy of 99%, and a model for the
                     prediction of the entities in the transcribed newspapers, with an accuracy of more
                     than 75%. This last result will be improved in the future as we trained the model
                     with only 7 newspapers. Furthermore, as we already have better-defined annotation
                     guidelines we hope to speed up the annotation process and get an even more robust
                     final NER model trained with more data to annotate automatically any other Spanish
                     newspaper published in the same period.In conclusion, the methodology employed in
                     this research can be extended to other domains and languages, facilitating
                     advancements in various fields. For instance, by leveraging multilingual models and
                     adapting them to other languages, similar NER tasks in historical newspapers from
                     different countries can be accomplished. Additionally, applying the developed
                     annotation guidelines and expanding the dataset to include newspapers from diverse
                     regions and time periods would enable the automatic annotation and prediction of
                     entities in a broader range of Spanish newspapers, and potentially other languages.
                     This adaptability and transferability of the methodology make it a valuable resource
                     for historians and researchers working on various textual collections beyond
                     historical newspapers, such as ancient manuscripts, legal documents, or literary
                     works in different languages. By scaling up the annotated data and training the
                     models with more diverse samples, the accuracy and robustness of the NER models can
                     be further enhanced, fostering more efficient and accurate analyses in the digital
                     humanities and beyond.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p93">93</a></div>
                  <div class="ptext" id="p93">This work has been supported by the Spanish Government and funded under the CLARA-HD
                     project (PID2020-116001RB-C32, <a href="https://clara-nlp.uned.es/home/dh/" onclick="window.open('https://clara-nlp.uned.es/home/dh/'); return false" class="ref">https://clara-nlp.uned.es/home/dh/</a>).</div>
                  
                  <div class="counter"><a href="#p94">94</a></div>
                  <div class="ptext" id="p94">We would like to acknowledge the support of the UNED art history professors Álvaro
                     Molina and Alicia Cámara, research directors of the CARCEM<a class="noteRef" href="#d3e2836">[31]</a>project. Finally, warm thanks to Andrés Rodríguez-Francés and Víctor
                     Sánchez-Sánchez members for a while of UNED research group NLP&amp;IR &gt; DH.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix: Brief introduction to Deep Learning</h1>
                  
                  <div class="counter"><a href="#p95">95</a></div>
                  <div class="ptext" id="p95">Artificial Intelligence (AI) is the field of study that focuses on the creation of
                     computer systems and software capable of performing tasks that require human
                     intelligence. This covers areas ranging from speech recognition and computer vision
                     capabilities, to complex decision making, machine learning and problem solving.</div>
                  
                  <div class="counter"><a href="#p96">96</a></div>
                  <div class="ptext" id="p96">There are different approaches within AI, such as rule-based AI, which uses a set
                     of
                     predefined instructions and rules to make decisions, and machine learning, which is
                     based on algorithms and models that allow machines to learn from examples and
                     data.</div>
                  
                  <div class="counter"><a href="#p97">97</a></div>
                  <div class="ptext" id="p97">Machine learning is a sub-discipline of AI based on the idea of building mathematical
                     or statistical models that can learn from data. These models are trained using a
                     training data set, where examples are provided with their respective labels or
                     expected results. The machine learning algorithm analyses the data and adjusts its
                     internal parameters to find patterns and correlations between input features and
                     output labels.</div>
                  
                  <div class="counter"><a href="#p98">98</a></div>
                  <div class="ptext" id="p98">Once the model has been trained, it can be used to make predictions or decisions
                     about new data that have not been used during training. The goal of machine learning
                     is to generalise the knowledge acquired during training so that it can be applied
                     to
                     new and unknown situations.</div>
                  
                  <div class="counter"><a href="#p99">99</a></div>
                  <div class="ptext" id="p99">There are several types of machine learning, each focusing on different approaches
                     and techniques to address specific problems. There are three main types: supervised
                     and unsupervised learning, and reinforcement learning.</div>
                  
                  <div class="counter"><a href="#p100">100</a></div>
                  <div class="ptext" id="p100">In supervised learning, the algorithm is provided with a training data set consisting
                     of input examples and the corresponding outputs, and the goal for the algorithm is
                     to
                     learn to map the inputs to the correct outputs. In unsupervised learning, the
                     algorithm is confronted with a set of unlabelled training data. The objective is to
                     find patterns, structures or intrinsic relationships in the data. In reinforcement
                     learning, the algorithm interacts with a dynamic environment and receives feedback
                     in
                     the form of rewards or punishments based on its actions, and learns through trial
                     and
                     error, adjusting its behaviour to maximise rewards over time.</div>
                  
                  <div class="counter"><a href="#p101">101</a></div>
                  <div class="ptext" id="p101">Deep Learning is a branch of machine learning that relies on artificial neural
                     networks to learn and extract high-level representations from complex, unstructured
                     data.</div>
                  
                  <div class="counter"><a href="#p102">102</a></div>
                  <div class="ptext" id="p102">Two essential stages in deep learning are pre-training and training/fine-tuning.
                     Pre-training involves training a model on a related task or a large dataset to learn
                     general features and patterns. For example, large language models are pre-trained
                     in
                     huge corpora such as Wikipedia. Then, fine-tuning follows, where the model's
                     parameters are adjusted on a smaller labeled dataset related to the specific target
                     task (i.e. NER). This process allows the model to leverage prior knowledge from
                     pre-training and adapt to the target task, leading to improved performance. This is
                     a
                     popular approach in Deep Learning called transfer learning ([<a class="ref" href="#malte_ratadiya2019">Malte and Ratadiya 2019</a>]; [<a class="ref" href="#ruderetal2019">Ruder et al. 2019</a>]), especially when
                     dealing with limited labeled data, besides the usual supervised, unsupervised and
                     reinforcement learning approaches.</div>
                  
                  <div class="counter"><a href="#p103">103</a></div>
                  <div class="ptext" id="p103">In the context of transfer learning, zero-shot and few-shot learning are approaches
                     that leverage pre-trained models to address limited data scenarios. Zero-shot
                     learning aims to recognize new classes unseen during training by utilizing semantic
                     relationships or embeddings learned from related classes. This allows the model to
                     make predictions on entirely novel categories without any fine-tuning on specific
                     examples. On the other hand, few-shot learning focuses on learning from a few
                     examples of each new class. The model adapts its knowledge from pre-training to
                     recognize and generalize to new classes with only a small amount of labeled data.
                     These techniques significantly enhance the capabilities of transfer learning,
                     enabling models to excel in situations with minimal labeled data and effectively
                     tackle new and previously unseen tasks.</div>
                  
                  <div class="counter"><a href="#p104">104</a></div>
                  <div class="ptext" id="p104">Hyperparameters, such as epochs and learning rate, are crucial settings in deep
                     learning models that are not learned from the data during training. Instead, they
                     are
                     set before training begins and can significantly impact the model's performance.
                     "Epochs" represent the number of times the model iterates through the entire dataset
                     during training. Increasing epochs can allow the model to see the data more times
                     but
                     may risk overfitting. "Learning rate" controls the step size for updating the model's
                     parameters during training. A high learning rate can lead to faster convergence, but
                     it might cause overshooting and instability. Balancing these hyperparameters is
                     essential to achieve optimal training and ensure the model generalizes well to new,
                     unseen data.</div>
                  
                  <div class="counter"><a href="#p105">105</a></div>
                  <div class="ptext" id="p105">Two types of systems make use of Deep Learning in this paper: OCR and NER. Optical
                     Character Recognition (OCR) is a technology that utilizes neural networks and
                     computer vision techniques to automatically recognize and extract text from images
                     or
                     scanned documents. Deep learning models, such as Convolutional Neural Networks
                     (CNNs), are employed to learn the complex features of characters and words, enabling
                     accurate text recognition. Named Entity Recognition (NER) systems are a type of
                     natural language processing (NLP) technology that uses deep learning and machine
                     learning techniques to automatically identify and classify named entities in text.
                     NER systems employ models, such as recurrent neural networks (RNNs) or
                     transformer-based architectures like BERT, to learn the patterns and context of words
                     in sentences, allowing them to recognize and label named entities accurately.</div>
                  
                  <div class="counter"><a href="#p106">106</a></div>
                  <div class="ptext" id="p106"></div>
                  </div>
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d3e358"><span class="noteRef lang en">[1] 
                     <a href="https://www.bne.es/es/noticias/1111-el-primer-sistema-masivo-de-inteligencia-artificial-de-la-lengua-espanola-maria" onclick="window.open('https://www.bne.es/es/noticias/1111-el-primer-sistema-masivo-de-inteligencia-artificial-de-la-lengua-espanola-maria'); return false" class="ref">https://www.bne.es/es/noticias/1111-el-primer-sistema-masivo-de-inteligencia-artificial-de-la-lengua-espanola-maria</a>
                     </span></div>
               <div class="endnote" id="d3e364"><span class="noteRef lang en">[2] 
                     <a href="https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/" onclick="window.open('https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/'); return false" class="ref">https://www.iic.uam.es/inteligencia-artificial/procesamiento-del-lenguaje-natural/modelo-lenguaje-espanol-rigoberta/</a>
                     </span></div>
               <div class="endnote" id="d3e438"><span class="noteRef lang en">[3] CLARA-HD project (PID2020-116001RB-C32) is funded
                     by MCIN - AEI (AEI/10.13039/501100011033).</span></div>
               <div class="endnote" id="d3e440"><span class="noteRef lang en">[4] 
                     <a href="https://impresso-project.ches" onclick="window.open('https://impresso-project.ches'); return false" class="ref">https://impresso-project.ch</a>
                     </span></div>
               <div class="endnote" id="d3e471"><span class="noteRef lang en">[5] 
                     <a href="https://hemerotecadigital.bne.es/hd/card?oid=0001510462" onclick="window.open('https://hemerotecadigital.bne.es/hd/card?oid=0001510462'); return false" class="ref">https://hemerotecadigital.bne.es/hd/card?oid=0001510462</a>
                     </span></div>
               <div class="endnote" id="d3e480"><span class="noteRef lang en">[6] 
                     <a href="https://readcoop.eu/transkribus" onclick="window.open('https://readcoop.eu/transkribus'); return false" class="ref">https://readcoop.eu/transkribus</a>
                     </span></div>
               <div class="endnote" id="d3e486"><span class="noteRef lang en">[7] 
                     <a href="https://tagtog.com/" onclick="window.open('https://tagtog.com/'); return false" class="ref">https://tagtog.com</a>
                     </span></div>
               <div class="endnote" id="d3e492"><span class="noteRef lang en">[8] 
                     <a href="https://huggingface.co/" onclick="window.open('https://huggingface.co/'); return false" class="ref">https://huggingface.co</a>
                     </span></div>
               <div class="endnote" id="d3e507"><span class="noteRef lang en">[9] 
                     <a href="https://aws.amazon.com/es/textract" onclick="window.open('https://aws.amazon.com/es/textract'); return false" class="ref">https://aws.amazon.com/es/textract</a>
                     </span></div>
               <div class="endnote" id="d3e513"><span class="noteRef lang en">[10] 
                     <a href="https://github.com/tesseract-ocr/tesseract" onclick="window.open('https://github.com/tesseract-ocr/tesseract'); return false" class="ref">https://github.com/tesseract-ocr/tesseract</a>
                     </span></div>
               <div class="endnote" id="d3e543"><span class="noteRef lang en">[11] <a href="https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/" onclick="window.open('https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/'); return false" class="ref">https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/</a></span></div>
               <div class="endnote" id="d3e550"><span class="noteRef lang en">[12] See <cite class="title italic">German Fraktur 19th-20th century</cite>
                     (<a href="https://readcoop.eu/model/german-fraktur-19th-20th-century/" onclick="window.open('https://readcoop.eu/model/german-fraktur-19th-20th-century/'); return false" class="ref">https://readcoop.eu/model/german-fraktur-19th-20th-century/</a>), <cite class="title italic">French newspapers late 18th century – midth of 20th
                        century</cite> (<a href="https://readcoop.eu/model/french-newspapers-late-18th-century-midth-of-20th-century/" onclick="window.open('https://readcoop.eu/model/french-newspapers-late-18th-century-midth-of-20th-century/'); return false" class="ref">https://readcoop.eu/model/french-newspapers-late-18th-century-midth-of-20th-century/</a>),
                     <cite class="title italic">Transkribus Print Multi-Language</cite> (<a href="https://readcoop.eu/model/print-multi-language-danish-dutch-german-finnish-french-latin-swedish/" onclick="window.open('https://readcoop.eu/model/print-multi-language-danish-dutch-german-finnish-french-latin-swedish/'); return false" class="ref">https://readcoop.eu/model/print-multi-language-danish-dutch-german-finnish-french-latin-swedish/</a>)
                     or <cite class="title italic">Dutch newspapers 17th century</cite> (<a href="https://readcoop.eu/model/dutch-newspapers-17th-century/" onclick="window.open('https://readcoop.eu/model/dutch-newspapers-17th-century/'); return false" class="ref">https://readcoop.eu/model/dutch-newspapers-17th-century/</a>).</span></div>
               <div class="endnote" id="d3e597"><span class="noteRef lang en">[13] 
                     <a href="https://readcoop.eu/model/spanish-print-xviii-xix" onclick="window.open('https://readcoop.eu/model/spanish-print-xviii-xix'); return false" class="ref">https://readcoop.eu/model/spanish-print-xviii-xix</a>
                     </span></div>
               <div class="endnote" id="d3e619"><span class="noteRef lang en">[14] 
                     <a href="https://prodi.gy" onclick="window.open('https://prodi.gy'); return false" class="ref">https://prodi.gy</a>
                     </span></div>
               <div class="endnote" id="d3e625"><span class="noteRef lang en">[15] 
                     <a href="https://doccano.herokuapp.com" onclick="window.open('https://doccano.herokuapp.com'); return false" class="ref">https://doccano.herokuapp.com</a>
                     </span></div>
               <div class="endnote" id="d3e631"><span class="noteRef lang en">[16] 
                     <a href="http://brat.nlplab.org" onclick="window.open('http://brat.nlplab.org'); return false" class="ref">http://brat.nlplab.org</a>
                     </span></div>
               <div class="endnote" id="d3e637"><span class="noteRef lang en">[17] 
                     <a href="https://www.tagtog.com/" onclick="window.open('https://www.tagtog.com/'); return false" class="ref">https://www.tagtog.com/</a>
                     </span></div>
               <div class="endnote" id="d3e940"><span class="noteRef lang en">[18] 
                     <a href="https://huggingface.co/xlm-roberta-base" onclick="window.open('https://huggingface.co/xlm-roberta-base'); return false" class="ref">https://huggingface.co/xlm-roberta-base</a>
                     </span></div>
               <div class="endnote" id="d3e953"><span class="noteRef lang en">[19] 
                     <a href="https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne" onclick="window.open('https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne'); return false" class="ref">https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne</a>
                     </span></div>
               <div class="endnote" id="d3e961"><span class="noteRef lang en">[20] 
                     <a href="https://huggingface.co/bertin-project/bertin-roberta-base-spanish" onclick="window.open('https://huggingface.co/bertin-project/bertin-roberta-base-spanish'); return false" class="ref">https://huggingface.co/bertin-project/bertin-roberta-base-spanish</a>
                     </span></div>
               <div class="endnote" id="d3e972"><span class="noteRef lang en">[21] 
                     <a href="https://huggingface.co/distilroberta-base" onclick="window.open('https://huggingface.co/distilroberta-base'); return false" class="ref">https://huggingface.co/distilroberta-base</a>
                     </span></div>
               <div class="endnote" id="d3e983"><span class="noteRef lang en">[22] 
                     <a href="https://huggingface.co/cmarkea/distilcamembert-base" onclick="window.open('https://huggingface.co/cmarkea/distilcamembert-base'); return false" class="ref">https://huggingface.co/cmarkea/distilcamembert-base</a>
                     </span></div>
               <div class="endnote" id="d3e994"><span class="noteRef lang en">[23] 
                     <a href="https://huggingface.co/uklfr/gottbert-base" onclick="window.open('https://huggingface.co/uklfr/gottbert-base'); return false" class="ref">https://huggingface.co/uklfr/gottbert-base</a>
                     </span></div>
               <div class="endnote" id="d3e1010"><span class="noteRef lang en">[24] 
                     <a href="https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-capitel-ner" onclick="window.open('https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-capitel-ner'); return false" class="ref">https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-capitel-ner</a>
                     </span></div>
               <div class="endnote" id="d3e1019"><span class="noteRef lang en">[25] 
                     <a href="https://huggingface.co/MMG/xlm-roberta-large-ner-spanish" onclick="window.open('https://huggingface.co/MMG/xlm-roberta-large-ner-spanish'); return false" class="ref">https://huggingface.co/MMG/xlm-roberta-large-ner-spanish</a>
                     </span></div>
               <div class="endnote" id="d3e1025"><span class="noteRef lang en">[26] 
                     <a href="https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl" onclick="window.open('https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl'); return false" class="ref">https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl</a>
                     </span></div>
               <div class="endnote" id="d3e1042"><span class="noteRef lang en">[27] 
                     <a href="https://sites.google.com/view/capitel2020" onclick="window.open('https://sites.google.com/view/capitel2020'); return false" class="ref">https://sites.google.com/view/capitel2020</a>
                     </span></div>
               <div class="endnote" id="d3e1571"><span class="noteRef lang en">[28] 
                     <a href="https://spacy.io" onclick="window.open('https://spacy.io'); return false" class="ref">https://spacy.io</a>
                     </span></div>
               <div class="endnote" id="d3e1983"><span class="noteRef lang en">[29] 
                     <a href="https://plantl.mineco.gob.es/tecnologias-lenguaje/comunicacion-formacion/eventos/eventosinfoday2019/Aspectos%20destacados%20del%20Plan%20TL/corpus-anotado-Jordi-Porta.pdf" onclick="window.open('https://plantl.mineco.gob.es/tecnologias-lenguaje/comunicacion-formacion/eventos/eventosinfoday2019/Aspectos%20destacados%20del%20Plan%20TL/corpus-anotado-Jordi-Porta.pdf'); return false" class="ref">https://plantl.mineco.gob.es/tecnologias-lenguaje/comunicacion-formacion/eventos/eventosinfoday2019/Aspectos%20destacados%20del%20Plan%20TL/corpus-anotado-Jordi-Porta.pdf</a></span></div>
               <div class="endnote" id="d3e2314"><span class="noteRef lang en">[30] 
                     <a href="https://docs.tagtog.com/collaboration.html#automatic-adjudication" onclick="window.open('https://docs.tagtog.com/collaboration.html#automatic-adjudication'); return false" class="ref">https://docs.tagtog.com/collaboration.html#automatic-adjudication</a>
                     </span></div>
               <div class="endnote" id="d3e2836"><span class="noteRef lang en">[31] 
                     <a href="https://dimh.hypotheses.org/author/dimh" onclick="window.open('https://dimh.hypotheses.org/author/dimh'); return false" class="ref">https://dimh.hypotheses.org/author/dimh</a>
                     </span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="akbik_blythe_vollgraf2018"><!-- close -->Akbik, Blythe, and Vollgraf 2018</span> Akbik,
                  A., Blythe, D., and Vollgraf, R. (2018) “Contextual String
                  Embeddings for Sequence Labeling”, <cite class="title italic">Proceedings of the
                     27th International Conference on Computational Linguistics</cite>, 1638-1649.
                  Available at: <a href="https://aclanthology.org/C18-1139" onclick="window.open('https://aclanthology.org/C18-1139'); return false" class="ref">https://aclanthology.org/C18-1139</a>
                  </div>
               <div class="bibl"><span class="ref" id="aldamaetal2022"><!-- close -->Aldama et al. 2022</span> Aldama, N., Guerrero, M.,
                  Montoro, H., and Samy, D. (2022) “Anotación de corpus
                  lingüísticos: Metodología utilizada en el Instituto de Ingeniería del Conocimiento
                  (IIC)”, 17.</div>
               <div class="bibl"><span class="ref" id="alrasheed_rao_grieco2021"><!-- close -->Alrasheed, Rao, and Grieco 2021</span> Alrasheed, N., Rao, P. and Grieco, V. (2021) “Character
                  Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep
                  Learning”, <cite class="title italic">DHQ</cite> 15.4. Available at: <a href="http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html" onclick="window.open('http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html'); return false" class="ref">http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="arandagarcia2022"><!-- close -->Aranda García 2022</span> Aranda García, N. (2022)
                  “Humanidades Digitales y literatura medieval española: La
                  integración de Transkribus en la base de datos COMEDIC”, <cite class="title italic">Historias Fingidas</cite>, 0, 127-149. Available at: <a href="https://doi.org/10.13136/2284-2667/1107" onclick="window.open('https://doi.org/10.13136/2284-2667/1107'); return false" class="ref">https://doi.org/10.13136/2284-2667/1107</a>
                  </div>
               <div class="bibl"><span class="ref" id="asahara_matsumoto2003"><!-- close -->Asahara and Matsumoto 2003</span> Asahara, M., and
                  Matsumoto, Y. (2003) “Japanese Named Entity Extraction with
                  Redundant Morphological Analysis”, <cite class="title italic">Proceedings of the
                     2003 Human Language Technology Conference of the North American Chapter of the
                     Association for Computational Linguistics</cite>, 8-15. Available at: <a href="https://aclanthology.org/N03-1002" onclick="window.open('https://aclanthology.org/N03-1002'); return false" class="ref">https://aclanthology.org/N03-1002</a>
                  </div>
               <div class="bibl"><span class="ref" id="ayusogarcia2022"><!-- close -->Ayuso García 2022</span> Ayuso García, M. (2022) “Las ediciones de Arnao Guillén de Brocar de BECLaR transcritas con
                  ayuda de Transkribus y OCR4all: Creación de un modelo para la red neuronal y
                  posible explotación de los resultados”, <cite class="title italic">Historias
                     Fingidas, 0</cite>, 151-173. Available at: <a href="https://doi.org/10.13136/2284-2667/1102" onclick="window.open('https://doi.org/10.13136/2284-2667/1102'); return false" class="ref">https://doi.org/10.13136/2284-2667/1102</a>
                  </div>
               <div class="bibl"><span class="ref" id="baptisteetal2021"><!-- close -->Baptiste et al. 2021</span> Baptiste, B., Favre, B.,
                  Auguste, J., and Henriot, C. (2021) “Transferring Modern Named
                  Entity Recognition to the Historical Domain: How to Take the Step?”, <cite class="title italic">Workshop on Natural Language Processing for Digital Humanities
                     (NLP4DH)</cite>. Available at: <a href="https://hal.archives-ouvertes.fr/hal-03550384" onclick="window.open('https://hal.archives-ouvertes.fr/hal-03550384'); return false" class="ref">https://hal.archives-ouvertes.fr/hal-03550384</a>
                  </div>
               <div class="bibl"><span class="ref" id="bazzacoetal2022"><!-- close -->Bazzaco et al. 2022</span> Bazzaco, S., Ruiz, A. M. J.,
                  Ruberte, Á. T., and Molares, M. M. (2022) “Sistemas de
                  reconocimiento de textos e impresos hispánicos de la Edad Moderna. La creación de
                  unos modelos de HTR para la transcripción automatizada de documentos en gótica y
                  redonda (s. XV-XVII)”, <cite class="title italic">Historias Fingidas</cite>, 0,
                  67-125. Available at: <a href="https://doi.org/10.13136/2284-2667/1190" onclick="window.open('https://doi.org/10.13136/2284-2667/1190'); return false" class="ref">https://doi.org/10.13136/2284-2667/1190</a>
                  </div>
               <div class="bibl"><span class="ref" id="bikeletal1997"><!-- close -->Bikel et al. 1997</span> Bikel, D. M., Miller, S.,
                  Schwartz, R., and Weischedel, R. (1997) “Nymble: A
                  High-Performance Learning Name-finder”, <cite class="title italic">Fifth
                     Conference on Applied Natural Language Processing</cite>, 194-201. Available at:
                  <a href="https://doi.org/10.3115/974557.974586" onclick="window.open('https://doi.org/10.3115/974557.974586'); return false" class="ref">https://doi.org/10.3115/974557.974586</a>
                  </div>
               <div class="bibl"><span class="ref" id="blouinetal2021"><!-- close -->Blouin et al. 2021</span> Blouin, B., Favre, B., Auguste,
                  J., &amp; Henriot, C. (2021). Transferring Modern Named Entity Recognition to the
                  Historical Domain: How to Take the Step? <cite class="title italic">Proceedings of the
                     Workshop on Natural Language Processing for Digital Humanities</cite>, 152-162.
                  <a href="https://aclanthology.org/2021.nlp4dh-1.18" onclick="window.open('https://aclanthology.org/2021.nlp4dh-1.18'); return false" class="ref">https://aclanthology.org/2021.nlp4dh-1.18</a></div>
               <div class="bibl"><span class="ref" id="bollmann2019"><!-- close -->Bollmann 2019</span> Bollmann, M. (2019) “A Large-Scale Comparison of Historical Text Normalization
                  Systems.”
                  <cite class="title italic">Proceedings of the 2019 Conference of the North American Chapter
                     of the Association for Computational Linguistics: Human Language Technologies,
                     Volume 1 (Long and Short Papers)</cite>, 3885-3898. Available at: <a href="https://doi.org/10.18653/v1/N19-1389" onclick="window.open('https://doi.org/10.18653/v1/N19-1389'); return false" class="ref">https://doi.org/10.18653/v1/N19-1389</a>
                  </div>
               <div class="bibl"><span class="ref" id="borosetal2020"><!-- close -->Boros et al. 2020</span> Boros, E., Hamdi, A., Linhares
                  Pontes, E., Cabrera-Diego, L. A., Moreno, J. G., Sidere, N., and Doucet, A. (2020)
                  “Alleviating Digitization Errors in Named Entity Recognition
                  for Historical Documents.”
                  <cite class="title italic">Proceedings of the 24th Conference on Computational Natural
                     Language Learning</cite>, 431-441. Available at: <a href="https://doi.org/10.18653/v1/2020.conll-1.35" onclick="window.open('https://doi.org/10.18653/v1/2020.conll-1.35'); return false" class="ref">https://doi.org/10.18653/v1/2020.conll-1.35</a>
                  </div>
               <div class="bibl"><span class="ref" id="borthwicketal1998"><!-- close -->Borthwick et al. 1998</span> Borthwick, A., Sterling,
                  J., Agichtein, E., and Grishman, R. (1998). “NYU: Description of
                  the MENE Named Entity System as Used in MUC-7.”
                  <cite class="title italic">Seventh Message Understanding Conference (MUC-7): Proceedings of
                     a Conference Held in Fairfax, Virginia, April 29 - May 1, 1998</cite>. MUC 1998.
                  Available at: <a href="https://aclanthology.org/M98-1018" onclick="window.open('https://aclanthology.org/M98-1018'); return false" class="ref">https://aclanthology.org/M98-1018</a>
                  </div>
               <div class="bibl"><span class="ref" id="calle-gomez_garcia-serrano_martinez2006"><!-- close -->Calle-Gómez, García-Serrano, and Martínez, 2006</span> Calle-Gómez, Javier;
                  García-Serrano, Ana and Martínez, Paloma. (2006). “Intentional
                  processing as a key for rational behaviour through Natural Interaction”,
                  <cite class="title italic">Interacting With Computers</cite>, Vol: 18 Nº: 6,
                  pp:1419-1446 <a href="10.1016/j.intcom.2006.05.002" onclick="window.open('10.1016/j.intcom.2006.05.002'); return false" class="ref">10.1016/j.intcom.2006.05.002</a>
                  </div>
               <div class="bibl"><span class="ref" id="calvotello2019"><!-- close -->Calvo Tello 2019</span> Calvo Tello, J. (2019). “Diseño de corpus literario para análisis cuantitativos.”
                  <cite class="title italic">Revista de Humanidades Digitales</cite>, <cite class="title italic">4</cite>, 115-135. Available at: <a href="https://doi.org/10.5944/rhd.vol.4.2019.25187" onclick="window.open('https://doi.org/10.5944/rhd.vol.4.2019.25187'); return false" class="ref">https://doi.org/10.5944/rhd.vol.4.2019.25187</a>
                  </div>
               <div class="bibl"><span class="ref" id="campillos-llanosetal2021"><!-- close -->Campillos-Llanos et al. 2021</span> Campillos-Llanos, L., Valverde-Mateos, A., Capllonch-Carrión, A. et al. (2021)
                  “A clinical trials corpus annotated with UMLS entities to
                  enhance the access to evidence-based medicine.” BMC Med Inform Decis Mak
                  21, 69 Available at: <a href="https://doi.org/10.1186/s12911-021-01395-z" onclick="window.open('https://doi.org/10.1186/s12911-021-01395-z'); return false" class="ref">https://doi.org/10.1186/s12911-021-01395-z</a>
                  </div>
               <div class="bibl"><span class="ref" id="campillos-llanosetal2022"><!-- close -->Campillos-Llanos et al. 2022</span> Campillos-Llanos, L., Terroba Reinares, A. R., Zakhir Puig, S., Valverde-Mateos, A.,
                  and Capllonch-Carrión, A. (2022) “Building a comparable corpus
                  and a benchmark for Spanish medical text simplification.”
                  <cite class="title italic">Proceedings of the Annual Conference of the Spanish Association
                     for Natural Language Processing 2022: Projects and Demonstrations (SEPLN-PD
                     2022)</cite>.</div>
               <div class="bibl"><span class="ref" id="chastang_torresaguilar_tannier2021"><!-- close -->Chastang, Torres Aguilar, and Tannier 2021</span> Chastang, P., Torres Aguilar, S.,
                  and Tannier, X. (2021). “A Named Entity Recognition Model for
                  Medieval Latin Charters.”
                  <cite class="title italic">DHQ</cite> 15.4. Available at: <a href="http://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html" onclick="window.open('http://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html'); return false" class="ref">http://www.digitalhumanities.org/dhq/vol/15/4/000574/000574.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="collobert_etal2011"><!-- close -->Collobert et al. 2011</span> Collobert, R., Weston,
                  J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011) “Natural Language Processing (Almost) from Scratch.”
                  <cite class="title italic">Natural Language Processing</cite>, 45.</div>
               <div class="bibl"><span class="ref" id="conneau_etal2020"><!-- close -->Conneau et al.2020</span> Conneau, A., Khandelwal, K.,
                  Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer,
                  L., and Stoyanov, V. (2020) “Unsupervised Cross-lingual
                  Representation Learning at Scale” (arXiv:1911.02116). ArXiv. Available at:
                  <a href="http://arxiv.org/abs/1911.02116" onclick="window.open('http://arxiv.org/abs/1911.02116'); return false" class="ref">http://arxiv.org/abs/1911.02116</a>
                  </div>
               <div class="bibl"><span class="ref" id="cuellar2021"><!-- close -->Cuéllar 2021</span> Cuéllar, Álvaro. (2021). “Spanish Golden Age Theatre Manuscripts (Spelling Modernization)
                  1.0”. Transkribus.</div>
               <div class="bibl"><span class="ref" id="cuellar_vegagarcia-luengos2021"><!-- close -->Cuéllar and Vega García-Luengos 2021</span> Cuéllar, Álvaro and Vega García-Luengos,
                  Germán. (2021) “ETSO. Estilometría aplicada al Teatro del Siglo
                  de Oro.”
                  <a href="https://etso.es/" onclick="window.open('https://etso.es/'); return false" class="ref">etso.es</a>.</div>
               <div class="bibl"><span class="ref" id="camara_molina_vazquez2020"><!-- close -->Cámara, Molina, and Vázquez 2020</span> Cámara, Alicia; Molina, Álvaro y Margarita A. Vázquez. (2020). Manassero (eds.).
                  “La ciudad de los saberes en la Edad Moderna”, Gijón,
                  Ediciones Trea, 296 pp. Available at: <a href="http://e-spacio.uned.es/fez/view/bibliuned:404-Amolina-1011" onclick="window.open('http://e-spacio.uned.es/fez/view/bibliuned:404-Amolina-1011'); return false" class="ref">http://e-spacio.uned.es/fez/view/bibliuned:404-Amolina-1011</a>.</div>
               <div class="bibl"><span class="ref" id="davies_parodi2022"><!-- close -->Davies and Parodi 2022</span> Davies, M., and Parodi,
                  G. (2022) “Constitución de corpus crecientes del español.”
                  At G. Parodi, P. Cantos-Gómez, C. Howe, M. Lacorte, J. Muñoz-Basol, and J.
                  Muñoz-Basol, <cite class="title italic">Lingüística de corpus en español</cite> (1.a ed.,
                  pp. 13-32). Routledge. Available at: <a href="https://doi.org/10.4324/9780429329296-3" onclick="window.open('https://doi.org/10.4324/9780429329296-3'); return false" class="ref">https://doi.org/10.4324/9780429329296-3</a>
                  </div>
               <div class="bibl"><span class="ref" id="detoni_etal2022"><!-- close -->De Toni et al. 2022</span> De Toni, F., Akiki, C., De La
                  Rosa, J., Fourrier, C., Manjavacas, E., Schweter, S., and Van Strien, D. (2022)
                  “Entities, Dates, and Languages: Zero-Shot on Historical Texts
                  with T0.”
                  <cite class="title italic">Proceedings of BigScience Episode #5 — Workshop on Challenges
                     and Perspectives in Creating Large Language Models</cite>, 75-83. Available at:
                  <a href="https://doi.org/10.18653/v1/2022.bigscience-1.7" onclick="window.open('https://doi.org/10.18653/v1/2022.bigscience-1.7'); return false" class="ref">https://doi.org/10.18653/v1/2022.bigscience-1.7</a>
                  </div>
               <div class="bibl"><span class="ref" id="delarosa_etal2022"><!-- close -->De la Rosa et al. 2022</span> De la Rosa, J.,
                  Ponferrada, E. G., Villegas, P., Salas, P. G. de P., Romero, M., and Grandury, M.
                  (2022) “BERTIN: Efficient Pre-Training of a Spanish Language
                  Model using Perplexity Sampling” (arXiv:2207.06814). ArXiv. Available at:
                  <a href="http://arxiv.org/abs/2207.06814" onclick="window.open('http://arxiv.org/abs/2207.06814'); return false" class="ref">http://arxiv.org/abs/2207.06814</a>
                  </div>
               <div class="bibl"><span class="ref" id="delestre_amar2022"><!-- close -->Delestre and Amar 2022</span> Delestre, C., and Amar,
                  A. (2022) “DistilCamemBERT: A distillation of the French model
                  CamemBERT” (arXiv:2205.11111). ArXiv. Available at: <a href="https://doi.org/10.48550/arXiv.2205.11111" onclick="window.open('https://doi.org/10.48550/arXiv.2205.11111'); return false" class="ref">https://doi.org/10.48550/arXiv.2205.11111</a>
                  </div>
               <div class="bibl"><span class="ref" id="derrick2019"><!-- close -->Derrick 2019</span> Derrick, T. (2019) “Using Transkribus For Automated Text Recognition of Historical Bengali
                  Books”
                  <cite class="title italic">British Library Digital Scholarship Blog</cite>. Available at:
                  <a href="https://blogs.bl.uk/digital-scholarship/2019/08/using-transkribus-for-automated-text-recognition-of-historical-bengali-books.html" onclick="window.open('https://blogs.bl.uk/digital-scholarship/2019/08/using-transkribus-for-automated-text-recognition-of-historical-bengali-books.html'); return false" class="ref">https://blogs.bl.uk/digital-scholarship/2019/08/using-transkribus-for-automated-text-recognition-of-historical-bengali-books.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="devlin_etal2019"><!-- close -->Devlin et al. 2019</span> Devlin, J., Chang, M.-W., Lee,
                  K., and Toutanova, K. (2019) “BERT: Pre-training of Deep
                  Bidirectional Transformers for Language Understanding” (arXiv:1810.04805).
                  ArXiv. Available at: <a href="http://arxiv.org/abs/1810.04805" onclick="window.open('http://arxiv.org/abs/1810.04805'); return false" class="ref">http://arxiv.org/abs/1810.04805</a>
                  </div>
               <div class="bibl"><span class="ref" id="ehrmannetal2020a"><!-- close -->Ehrmann et al. 2020a</span> Ehrmann, M., Romanello, M.,
                  Fluckiger, A., and Clematide, S. (2020a). “Extended Overview of
                  CLEF HIPE 2020: Named Entity Processing on Historical Newspapers.”
                  38.</div>
               <div class="bibl"><span class="ref" id="ehrmannetal2020b"><!-- close -->Ehrmann et al. 2020b</span> Ehrmann, M., Romanello, M.,
                  Clematide, S., Ströbel, P. B., and Barman, R. (2020b) “Language
                  Resources for Historical Newspapers: The Impresso Collection.”
                  <cite class="title italic">Proceedings of the 12th Language Resources and Evaluation
                     Conference</cite>, 958-968. Available at: <a href="https://aclanthology.org/2020.lrec-1.121" onclick="window.open('https://aclanthology.org/2020.lrec-1.121'); return false" class="ref">https://aclanthology.org/2020.lrec-1.121</a>
                  </div>
               <div class="bibl"><span class="ref" id="ehrmannetal2020c"><!-- close -->Ehrmann et al. 2020c</span> Ehrmann, M., Watter, C.,
                  Romanello, M., and Clematide, S. (2020c). “Impresso Named Entity
                  Annotation Guidelines”. Available at: <a href="https://doi.org/10.5281/zenodo.3604227" onclick="window.open('https://doi.org/10.5281/zenodo.3604227'); return false" class="ref">https://doi.org/10.5281/zenodo.3604227</a>
                  </div>
               <div class="bibl"><span class="ref" id="ehrmannetal2022"><!-- close -->Ehrmann et al. 2022</span> Ehrmann, M., Romanello, M.,
                  Najem-Meyer, S., Doucet, A., and Clematide, S. (2022). “Extended
                  Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual
                  Historical Documents.” 26.</div>
               <div class="bibl"><span class="ref" id="garcia-serrano_castellanos2016"><!-- close -->García-Serrano and Castellanos 2016</span> García-Serrano, A. and Castellanos, A.
                  (2016) “Representación y organización de documentos digitales:
                  detalles y práctica sobre la ontología DIMH”. <cite class="title italic">Revista
                     de Humanidades Digitales</cite>, v.1, 314-344, ISSN 2531-1786. Available at: <a href="https://doi.org/10.5944/rhd.vol.1.2017.17155" onclick="window.open('https://doi.org/10.5944/rhd.vol.1.2017.17155'); return false" class="ref">https://doi.org/10.5944/rhd.vol.1.2017.17155</a>
                  </div>
               <div class="bibl"><span class="ref" id="garcia-serrano_menta-garuz2022"><!-- close -->García-Serrano and Menta-Garuz 2022</span> García-Serrano, A., and Menta-Garuz, A
                  (2022). “La inteligencia artificial en las Humanidades Digitales:
                  dos experiencias con corpus digitales.”
                  <cite class="title italic">Revista de Humanidades Digitales</cite>, 7, 19-39.</div>
               <div class="bibl"><span class="ref" id="gebruetal2021"><!-- close -->Gebru et al. 2021</span> Gebru, T., Morgenstern, J.,
                  Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., and Crawford, K. (2021)
                  “Datasheets for datasets.”
                  <cite class="title italic">Communications of the ACM</cite>, 64(12), 86-92. Available at:
                  <a href="https://doi.org/10.1145/3458723" onclick="window.open('https://doi.org/10.1145/3458723'); return false" class="ref">https://doi.org/10.1145/3458723</a>
                  </div>
               <div class="bibl"><span class="ref" id="grishman_sundheim1996"><!-- close -->Grishman and Sundheim 1996</span> Grishman, R.,
                  and Sundheim, B. (1996) “Message Understanding Conference-6: A
                  brief history”
                  <cite class="title italic">Proceedings of the 16th conference on Computational linguistics
                     - Volume 1</cite>, 466-471. Available at: <a href="https://doi.org/10.3115/992628.992709" onclick="window.open('https://doi.org/10.3115/992628.992709'); return false" class="ref">https://doi.org/10.3115/992628.992709</a>
                  </div>
               <div class="bibl"><span class="ref" id="gruszczynski_etal2021"><!-- close -->Gruszczyński et al. 2021</span> Gruszczyński, W.,
                  Adamiec, D., Bronikowska, R., Kieraś, W., Modrzejewski, E., Wieczorek, A., and
                  Woliński, M. (2021) “The Electronic Corpus of 17th- and
                  18th-century Polish Texts.”
                  <cite class="title italic">Language Resources and Evaluation</cite>. Available at: <a href="https://doi.org/10.1007/s10579-021-09549-1" onclick="window.open('https://doi.org/10.1007/s10579-021-09549-1'); return false" class="ref">https://doi.org/10.1007/s10579-021-09549-1</a>
                  </div>
               <div class="bibl"><span class="ref" id="gutierrez-fandinoetal2022"><!-- close -->Gutiérrez-Fandiño et al. 2022</span> Gutiérrez-Fandiño, A., Armengol-Estapé, J., Pàmies, M., Llop-Palao, J.,
                  Silveira-Ocampo, J., Carrino, C. P., Armentano-Oller, C., Rodriguez-Penagos, C.,
                  Gonzalez-Agirre, A., and Villegas, M. (2022) “MarIA: Spanish
                  Language Models”, 22.</div>
               <div class="bibl"><span class="ref" id="hintz_biemann2016"><!-- close -->Hintz and Biemann 2016</span> Hintz, G., and Biemann,
                  C. (2016). “Language Transfer Learning for Supervised Lexical
                  Substitution.”
                  <cite class="title italic">Proceedings of the 54th Annual Meeting of the Association for
                     Computational Linguistics (Volume 1: Long Papers)</cite>, 118-129. <a href="https://doi.org/10.18653/v1/P16-1012" onclick="window.open('https://doi.org/10.18653/v1/P16-1012'); return false" class="ref">https://doi.org/10.18653/v1/P16-1012</a></div>
               <div class="bibl"><span class="ref" id="kabatek2013"><!-- close -->Kabatek 2013</span> Kabatek, J. (2013) “¿Es posible una lingüística histórica basada en un corpus representativo?”
                  <cite class="title italic">Iberoromania</cite>, 77(1). Available at: <a href="https://doi.org/10.1515/ibero-2013-0045" onclick="window.open('https://doi.org/10.1515/ibero-2013-0045'); return false" class="ref">https://doi.org/10.1515/ibero-2013-0045</a>
                  </div>
               <div class="bibl"><span class="ref" id="kettunenetal2017"><!-- close -->Kettunen et al. 2017</span> Kettunen, K., Mäkelä, E.,
                  Ruokolainen T., Kuokkala J. and Löfberg, L. (2017) “Old Content
                  and Modern Tools – Searching Named Entities in a Finnish OCRed Historical
                  Newspaper Collection 1771–1910.”
                  <cite class="title italic">DHQ </cite>11.3. Available at: <a href="http://digitalhumanities.org:8081/dhq/vol/11/3/000333/000333.html" onclick="window.open('http://digitalhumanities.org:8081/dhq/vol/11/3/000333/000333.html'); return false" class="ref">http://digitalhumanities.org:8081/dhq/vol/11/3/000333/000333.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="kirmizialtin_wrisley2022"><!-- close -->Kirmizialtin and Wrisley 2022</span> Kirmizialtin, Suphan and David Joseph Wrisley. (2022) “Automated
                  Transcription of Non-Latin Script Periodicals: A Case Study in the Ottoman Turkish
                  Print Archive.”
                  <cite class="title italic">DHQ</cite> 16.2. Available at: <a href="http://www.digitalhumanities.org/dhq/vol/16/2/000577/000577.html" onclick="window.open('http://www.digitalhumanities.org/dhq/vol/16/2/000577/000577.html'); return false" class="ref">http://www.digitalhumanities.org/dhq/vol/16/2/000577/000577.html</a>
                  </div>
               <div class="bibl"><span class="ref" id="lampleetal2016"><!-- close -->Lample et al. 2016</span> Lample, G., Ballesteros, M.,
                  Subramanian, S., Kawakami, K., and Dyer, C. (2016) “Neural
                  Architectures for Named Entity Recognition” (arXiv:1603.01360). ArXiv.
                  Available at: <a href="https://doi.org/10.48550/arXiv.1603.01360" onclick="window.open('https://doi.org/10.48550/arXiv.1603.01360'); return false" class="ref">https://doi.org/10.48550/arXiv.1603.01360</a>
                  </div>
               <div class="bibl"><span class="ref" id="lietal2022"><!-- close -->Li et al.2022</span> Li, J., Sun, A., Han, J., and Li, C.
                  (2022) “A Survey on Deep Learning for Named Entity
                  Recognition.”
                  <cite class="title italic">IEEE Transactions on Knowledge and Data Engineering</cite>,
                  34(1), 50-70. Available at: <a href="https://doi.org/10.1109/TKDE.2020.2981314" onclick="window.open('https://doi.org/10.1109/TKDE.2020.2981314'); return false" class="ref">https://doi.org/10.1109/TKDE.2020.2981314</a>
                  </div>
               <div class="bibl"><span class="ref" id="liuetal2019"><!-- close -->Liu et al. 2019</span> Liu, Y., Ott, M., Goyal, N., Du, J.,
                  Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019)
                  “RoBERTa: A Robustly Optimized BERT Pretraining
                  Approach” (arXiv:1907.11692). ArXiv. Available at: <a href="https://doi.org/10.48550/arXiv.1907.11692" onclick="window.open('https://doi.org/10.48550/arXiv.1907.11692'); return false" class="ref">https://doi.org/10.48550/arXiv.1907.11692</a>
                  </div>
               <div class="bibl"><span class="ref" id="malte_ratadiya2019"><!-- close -->Malte and Ratadiya 2019</span> Malte, A., and
                  Ratadiya, P. (2019). <cite class="title italic">Evolution of transfer learning in natural
                     language processing</cite> (arXiv:1910.07370). arXiv. <a href="https://doi.org/10.48550/arXiv.1910.07370" onclick="window.open('https://doi.org/10.48550/arXiv.1910.07370'); return false" class="ref">https://doi.org/10.48550/arXiv.1910.07370</a></div>
               <div class="bibl"><span class="ref" id="mccallum_li2003"><!-- close -->McCallum and Li 2003</span> McCallum, A., and Li, W.
                  (2003) “Early results for Named Entity Recognition with
                  Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons.”
                  <cite class="title italic">Proceedings of the Seventh Conference on Natural Language
                     Learning at HLT-NAACL 2003</cite>, 188-191. Available at: <a href="https://aclanthology.org/W03-0430" onclick="window.open('https://aclanthology.org/W03-0430'); return false" class="ref">https://aclanthology.org/W03-0430</a>
                  </div>
               <div class="bibl"><span class="ref" id="menta_garcia-serrano2022"><!-- close -->Menta and García-Serrano 2022</span> Menta, A.,
                  and García-Serrano, A. (2022) “Controllable Sentence
                  Simplification Using Transfer Learning.” Proceedings of the Working Notes
                  of CLEF.</div>
               <div class="bibl"><span class="ref" id="menta_sanchez-salido_garcia-serrano2022"><!-- close -->Menta, Sánchez-Salido, and García-Serrano 2022</span> Menta, A., Sánchez-Salido, E.,
                  and García-Serrano, A. (2022) “Transcripción de periódicos
                  históricos: Aproximación CLARA-HD”, <cite class="title italic">Proceedings of the
                     Annual Conference of the Spanish Association for Natural Language Processing 2022:
                     Projects and Demonstrations (SEPLN-PD 2022)</cite>. Available at: <a href="https://ceur-ws.org/Vol-3224/paper17.pdf" onclick="window.open('https://ceur-ws.org/Vol-3224/paper17.pdf'); return false" class="ref">https://ceur-ws.org/Vol-3224/paper17.pdf</a>
                  </div>
               <div class="bibl"><span class="ref" id="merinorecalde2022"><!-- close -->Merino Recalde 2022</span> Merino Recalde, David.
                  (2022) “El sistema de personajes de las comedias urbanas de Lope
                  de Vega. Propuesta metodológica y posibilidades del análisis de redes sociales
                  para el estudio del teatro del Siglo de Oro” Master Thesis, UNED. Facultad
                  de Filología. Departamento de Literatura Española y Teoría de la Literatura.
                  Available at: <a href="http://e-spacio.uned.es/fez/view/bibliuned:master-Filologia-FILTCE-Dmerino" onclick="window.open('http://e-spacio.uned.es/fez/view/bibliuned:master-Filologia-FILTCE-Dmerino'); return false" class="ref">http://e-spacio.uned.es/fez/view/bibliuned:master-Filologia-FILTCE-Dmerino</a>
                  </div>
               <div class="bibl"><span class="ref" id="molinamartin2021"><!-- close -->Molina Martín 2021</span> Molina Martín, Á. (2021)
                  “Cartografías del adorno en las residencias nobiliarias de la
                  corte de Carlos IV: redes y modelos de buen gusto y distinción”
                  <cite class="title italic">Magallanica. Revista de Historia Moderna</cite>, 7(14),
                  205-235.</div>
               <div class="bibl"><span class="ref" id="molina_vega2018"><!-- close -->Molina and Vega 2018</span> Molina, Á., and Vega, J.
                  (2018) “Adorno y representación: escenarios cotidianos de vida a
                  finales del siglo XVIII en Madrid”, 139-166.</div>
               <div class="bibl"><span class="ref" id="morenosandoval2019"><!-- close -->Moreno Sandoval 2019</span> Moreno Sandoval, A.
                  (2019). <cite class="title italic">Lenguas y computación</cite>. Síntesis.</div>
               <div class="bibl"><span class="ref" id="morenosandovaletal2018"><!-- close -->Moreno Sandoval et al. 2018</span> Moreno
                  Sandoval, A., Díaz García, J., Campillos Llanos, L., and Redondo, T. (2018) “Biomedical Term Extraction: NLP Techniques in Computational
                  Medicine”. Available at: <a href="https://doi.org/10.9781/ijimai.2018.04.001" onclick="window.open('https://doi.org/10.9781/ijimai.2018.04.001'); return false" class="ref">https://doi.org/10.9781/ijimai.2018.04.001</a>
                  </div>
               <div class="bibl"><span class="ref" id="morenosandoval_gisbert_montorozamorano2020"><!-- close -->Moreno Sandoval, Gisbert, and Montoro Zamorano 2020</span> Moreno Sandoval, Antonio,
                  Gisbert, Ana and Montoro Zamorano, Helena. (2020) “FinT-esp: A
                  corpus of financial reports in Spanish”.</div>
               <div class="bibl"><span class="ref" id="nadeau_sekine2007"><!-- close -->Nadeau and Sekine 2007</span> Nadeau, D., and Sekine,
                  S. (2007) “A Survey of Named Entity Recognition and
                  Classification”
                  <cite class="title italic">Lingvisticae Investigationes</cite>, 30. Available at: <a href="https://doi.org/10.1075/li.30.1.03nad" onclick="window.open('https://doi.org/10.1075/li.30.1.03nad'); return false" class="ref">https://doi.org/10.1075/li.30.1.03nad</a>
                  </div>
               <div class="bibl"><span class="ref" id="nakayama2021"><!-- close -->Nakayama 2021</span> Nakayama, E. (2021) “Implementación de un corpus comparable de español y japonés de
                  acceso abierto para la traducción especializada”, 29.</div>
               <div class="bibl"><span class="ref" id="neudecker2016"><!-- close -->Neudecker 2016</span> Neudecker, C. (2016) “An Open Corpus for Named Entity Recognition in Historic
                  Newspapers”, <cite class="title italic">Proceedings of the Tenth International
                     Conference on Language Resources and Evaluation (LREC’16)</cite>, 4348-4352.
                  Available at: <a href="https://aclanthology.org/L16-1689" onclick="window.open('https://aclanthology.org/L16-1689'); return false" class="ref">https://aclanthology.org/L16-1689</a>
                  </div>
               <div class="bibl"><span class="ref" id="nieuwenhuijsen2016"><!-- close -->Nieuwenhuijsen 2016</span> Nieuwenhuijsen, D. (2016)
                  “Notas sobre la aportación del análisis estadístico a la
                  lingüística de corpus”, <cite class="title italic">Notas sobre la aportación del
                     análisis estadístico a la lingüística de corpus</cite> (pp. 215-237). De Gruyter.
                  Available at: <a href="https://doi.org/10.1515/9783110462357-011" onclick="window.open('https://doi.org/10.1515/9783110462357-011'); return false" class="ref">https://doi.org/10.1515/9783110462357-011</a>
                  </div>
               <div class="bibl"><span class="ref" id="perdiki2022"><!-- close -->Perdiki 2022</span> Perdiki, Elpida. (2022) “Review of ‘Transkribus: Reviewing HTR training
                  on (Greek) manuscripts’.”
                  <cite class="title italic">RIDE 15</cite>. doi: 10.18716/ride.a.15.6. Accessed:
                  21.12.2022. Available at: <a href="https://ride.i-d-e.de/issues/issue-15/transkribus/" onclick="window.open('https://ride.i-d-e.de/issues/issue-15/transkribus/'); return false" class="ref">https://ride.i-d-e.de/issues/issue-15/transkribus/</a>
                  </div>
               <div class="bibl"><span class="ref" id="piotrowski2012"><!-- close -->Piotrowski 2012</span> Piotrowski, M. (2012). <cite class="title italic">Natural Language Processing for Historical Texts</cite>. Graeme
                  Hirst, University of Toronto.</div>
               <div class="bibl"><span class="ref" id="pruksachatkunetal2020"><!-- close -->Pruksachatkun et al. 2020</span> Pruksachatkun,
                  Y., Phang, J., Liu, H., Htut, P. M., Zhang, X., Pang, R. Y., Vania, C., Kann, K.,
                  and
                  Bowman, S. R. (2020). “Intermediate-Task Transfer Learning with
                  Pretrained Language Models: When and Why Does It Work?”
                  <cite class="title italic">Proceedings of the 58th Annual Meeting of the Association for
                     Computational Linguistics</cite>, 5231-5247. <a href="https://doi.org/10.18653/v1/2020.acl-main.467" onclick="window.open('https://doi.org/10.18653/v1/2020.acl-main.467'); return false" class="ref">https://doi.org/10.18653/v1/2020.acl-main.467</a></div>
               <div class="bibl"><span class="ref" id="rivero2022"><!-- close -->Rivero 2022</span> Rivero, Manuel. (2022) “Italian Madrid: Ambassadors, Regents, and Courtiers in the Hospital
                  de San Pedro y San Pablo”, <cite class="title italic">Culture &amp; History
                     Digital Journal</cite>, 11(1), e003. Available at: <a href="https://doi.org/10.3989/chdj.2022.003" onclick="window.open('https://doi.org/10.3989/chdj.2022.003'); return false" class="ref">https://doi.org/10.3989/chdj.2022.003</a>
                  </div>
               <div class="bibl"><span class="ref" id="rojo2010"><!-- close -->Rojo 2010</span> Rojo, G. (2010) “Sobre
                  codificación y explotación de corpus textuales: Otra comparación del Corpus del
                  español con el CORDE y el CREA”, <cite class="title italic">Lingüística</cite>,
                  24, 11-50.</div>
               <div class="bibl"><span class="ref" id="rojo2016"><!-- close -->Rojo 2016</span> Rojo, G. (2016) “Los
                  corpus textuales del español”, In book: <cite class="title italic">Enciclopedia
                     lingüística hispánica</cite>. Publisher: Routledge. Editors: Gutiérrez-Rexach.
                  Available at: <a href="https://www.researchgate.net/publication/294407007_Los_corpus_textuales_del_espanol" onclick="window.open('https://www.researchgate.net/publication/294407007_Los_corpus_textuales_del_espanol'); return false" class="ref">https://www.researchgate.net/publication/294407007_Los_corpus_textuales_del_espanol</a>
                  </div>
               <div class="bibl"><span class="ref" id="rosset_grouin_zweigenbaum2011"><!-- close -->Rosset, Grouin, and Zweigenbaum 2011</span> Rosset, S., Grouin, C., and Zweigenbaum,
                  P. (2011) “Entités nommées structurées: Guide d’annotation
                  Quaero.”. Available at: <a href="http://www.quaero.org/media/files/bibliographie/quaero-guide-annotation-2011.pdf" onclick="window.open('http://www.quaero.org/media/files/bibliographie/quaero-guide-annotation-2011.pdf'); return false" class="ref">http://www.quaero.org/media/files/bibliographie/quaero-guide-annotation-2011.pdf</a>
                  </div>
               <div class="bibl"><span class="ref" id="rubinstein_shmidman2021"><!-- close -->Rubinstein and Shmidman 2021</span> Rubinstein,
                  A., and Shmidman, A. (2021). “NLP in the DH pipeline:
                  Transfer-learning to a Chronolect.”
                  <cite class="title italic">Proceedings of the Workshop on Natural Language Processing for
                     Digital Humanities</cite>, 106-110. Available at: <a href="https://aclanthology.org/2021.nlp4dh-1.12" onclick="window.open('https://aclanthology.org/2021.nlp4dh-1.12'); return false" class="ref">https://aclanthology.org/2021.nlp4dh-1.12</a></div>
               <div class="bibl"><span class="ref" id="ruderetal2019"><!-- close -->Ruder et al. 2019</span> Ruder, S., Peters, M. E.,
                  Swayamdipta, S., and Wolf, T. (2019). “Transfer Learning in
                  Natural Language Processing.”
                  <cite class="title italic">Proceedings of the 2019 Conference of the North American Chapter
                     of the Association for Computational Linguistics: Tutorials</cite>, 15-18.
                  Available at: <a href="https://doi.org/10.18653/v1/N19-5004" onclick="window.open('https://doi.org/10.18653/v1/N19-5004'); return false" class="ref">https://doi.org/10.18653/v1/N19-5004</a></div>
               <div class="bibl"><span class="ref" id="ruizfaboetal2017"><!-- close -->Ruiz Fabo et al. 2017</span> Ruiz Fabo, P., Bermúdez
                  Sabel, H., Martínez-Cantón, C. and Calvo Tello J. (2017) “Diachronic Spanish Sonnet Corpus (DISCO)”, Madrid. UNED. Available at:
                  <a href="https://github.com/pruizf/disco" onclick="window.open('https://github.com/pruizf/disco'); return false" class="ref">https://github.com/pruizf/disco</a>
                  </div>
               <div class="bibl"><span class="ref" id="sanhetal2019"><!-- close -->Sanh et al. 2019</span> Sanh, V., Debut, L., Chaumond, J.,
                  and Wolf, T. (2019) “Distilbert, a distilled version of BERT:
                  smaller, faster, cheaper and lighter”, ArXiv preprint, <a href="abs/1910.01108" onclick="window.open('abs/1910.01108'); return false" class="ref">abs/1910.01108</a></div>
               <div class="bibl"><span class="ref" id="scheibleetal2020"><!-- close -->Scheible et al. 2020</span> Scheible, R., Thomczyk, F.,
                  Tippmann, P., Jaravine, V., and Boeker, M. (2020) “GottBERT: A
                  pure German Language Model”, (arXiv:2012.02110). ArXiv. Available at: <a href="https://doi.org/10.48550/arXiv.2012.02110" onclick="window.open('https://doi.org/10.48550/arXiv.2012.02110'); return false" class="ref">https://doi.org/10.48550/arXiv.2012.02110</a>
                  </div>
               <div class="bibl"><span class="ref" id="sekine1998"><!-- close -->Sekine 1998</span> Sekine, S. (1998) “Description of the Japanese NE System Used for MET-2,”
                  <cite class="title italic">Seventh Message Understanding Conference (MUC-7): Proceedings of
                     a Conference Held in Fairfax, Virginia, April 29 - May 1, 1998</cite>. MUC 1998.
                  Available at: <a href="https://aclanthology.org/M98-1019" onclick="window.open('https://aclanthology.org/M98-1019'); return false" class="ref">https://aclanthology.org/M98-1019</a>
                  </div>
               <div class="bibl"><span class="ref" id="sanchez-salido2022"><!-- close -->Sánchez-Salido 2022</span> Sánchez-Salido, Eva.
                  (2022) “Reconocimiento de entidades en corpus de dominios
                  específicos: experimentación con periódicos históricos”, Master Thesis (30
                  ECTS). ETSI Informática. UNED</div>
               <div class="bibl"><span class="ref" id="terras2011"><!-- close -->Terras 2011</span> Terras, M. M. (2011) “The Rise of Digitization”, En R. Rikowski (Ed.), <cite class="title italic">Digitisation Perspectives</cite> (pp. 3-20). SensePublishers. Available at: <a href="https://doi.org/10.1007/978-94-6091-299-3_1" onclick="window.open('https://doi.org/10.1007/978-94-6091-299-3_1'); return false" class="ref">https://doi.org/10.1007/978-94-6091-299-3_1</a>
                  </div>
               <div class="bibl"><span class="ref" id="torruellacasanas2017"><!-- close -->Torruella Casañas 2017</span> Torruella Casañas, J.
                  (2017) <cite class="title italic">Lingüística de corpus: Génesis y bases metodológicas de
                     los corpus (históricos) para la investigación en lingüística.</cite> Peter
                  Lang.</div>
               <div class="bibl"><span class="ref" id="vaswanietal2017"><!-- close -->Vaswani et al. 2017</span> Vaswani, A., Shazeer, N.,
                  Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.
                  (2017) “Attention Is All You Need”, (arXiv:1706.03762).
                  ArXiv. Available at: <a href="https://doi.org/10.48550/arXiv.1706.03762" onclick="window.open('https://doi.org/10.48550/arXiv.1706.03762'); return false" class="ref">https://doi.org/10.48550/arXiv.1706.03762</a>
                  </div>
               <div class="bibl"><span class="ref" id="wissleretal2014"><!-- close -->Wissler et al. 2014</span> Wissler, L., Almashraee, M.,
                  Monett, D., and Paschke, A. (2014) “The Gold Standard in Corpus
                  Annotation”, Available at: <a href="https://doi.org/10.13140/2.1.4316.3523" onclick="window.open('https://doi.org/10.13140/2.1.4316.3523'); return false" class="ref">https://doi.org/10.13140/2.1.4316.3523</a>
                  </div>
               <div class="bibl"><span class="ref" id="yadav_bethard2018"><!-- close -->Yadav and Bethard 2018</span> Yadav, V., and Bethard,
                  S. (2018) “A Survey on Recent Advances in Named Entity
                  Recognition from Deep Learning models”, <cite class="title italic">Proceedings of
                     the 27th International Conference on Computational Linguistics</cite>, 2145-2158.
                  Available at: <a href="https://aclanthology.org/C18-1182" onclick="window.open('https://aclanthology.org/C18-1182'); return false" class="ref">https://aclanthology.org/C18-1182</a>
                  
                  </div>
               <div class="bibl"><span class="ref" id="zophetal2016"><!-- close -->Zoph et al. 2016</span> Zoph, B., Yuret, D., May, J., and
                  Knight, K. (2016). “Transfer Learning for Low-Resource Neural
                  Machine Translation.”
                  <cite class="title italic">Proceedings of the 2016 Conference on Empirical Methods in
                     Natural Language Processing</cite>, 1568-1575. <a href="https://doi.org/10.18653/v1/D16-1163" onclick="window.open('https://doi.org/10.18653/v1/D16-1163'); return false" class="ref">https://doi.org/10.18653/v1/D16-1163</a></div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>