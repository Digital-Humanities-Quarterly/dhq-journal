<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 015 Number 1</div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            	
            	
            <div class="DHQheader">
               		
               			
               				
               <h1 class="articleTitle lang en">Music Theory, the Missing Link Between
                  					Music-Related Big Data and Artificial Intelligence</h1>
               				
               <div class="author"><span style="color: grey">Jeffrey A. T.
                     						Lupker</span> &lt;<a href="mailto:jlupker_at_uwo_dot_ca" onclick="javascript:window.location.href='mailto:'+deobfuscate('jlupker_at_uwo_dot_ca'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('jlupker_at_uwo_dot_ca'); return false;">jlupker_at_uwo_dot_ca</a>&gt;, The University of Western Ontario</div>
               				
               <div class="author"><span style="color: grey">William J.
                     						Turkel</span> &lt;<a href="mailto:wturkel_at_uwo_dot_ca" onclick="javascript:window.location.href='mailto:'+deobfuscate('wturkel_at_uwo_dot_ca'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('wturkel_at_uwo_dot_ca'); return false;">wturkel_at_uwo_dot_ca</a>&gt;, The University of Western Ontario</div>
               			
               			
               			
               		
               		
               		
               		
               	<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Music%20Theory,%20the%20Missing%20Link%20Between%20Music-Related%20Big%20Data%20and%20Artificial%20Intelligence&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2021-03-05&amp;rft.volume=015&amp;rft.issue=1&amp;rft.aulast=Lupker&amp;rft.aufirst=Jeffrey A. T.&amp;rft.au=Jeffrey A. T.%20Lupker&amp;rft.au=William J.%20Turkel"> </span></div>
            	
            
            	
            	
            <div id="DHQtext">
               		
               		
               			
               <div id="abstract">
                  <h2>Abstract</h2>
                  				
                  <p>This paper examines musical artificial intelligence (AI) algorithms that can
                     					not only learn from big data, but learn in ways that would be familiar to a
                     					musician or music theorist. This paper aims to find more effective links
                     					between music-related big data and artificial intelligence algorithms by
                     					incorporating principles with a strong grounding in music theory. We show
                     					that it is possible to increase the accuracy of two common algorithms (mode
                     					prediction and key prediction) by using music-theory based techniques
                     					during the data preparation process. We offer methods to alter often-used
                     					Krumhansl Kessler profiles [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>], and the manner
                     					in which they are employed during preprocessing, to aid the connection of
                     					musical big data and mode or key predicting algorithms.</p>
                  			</div>
               			
               		
               
               		
               		
               			
               <div class="div div0">
                  				
                  <h1 class="head">Introduction</h1>
                  				
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Research in music information retrieval has produced many possibilities for
                     					developing artificial intelligence (AI) algorithms that can perform a wide
                     					variety of musically-based tasks, including even music composition itself.
                     					The availability of vast musical datasets like the <em class="emph">Million Song
                        						Dataset</em>
                     					[<a class="ref" href="#bertin2011">Bertin et al. 2011</a>] and Spotify's <em class="emph">Web API</em> has made it
                     					possible for researchers to acquire algorithmically-determined
                     					characteristics of a song's key, mode, pitch content, and more. At the same
                     					time, the existence of these large datasets has made it possible for
                     					researchers to take a 'big data'<a class="noteRef" href="#d4e186">[1]</a> approach to various styles of Western music. One
                     					notable example is the work by Serrà et al [<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>]
                     					which showed the changes and trends related to pitch transitions, the
                     					homogenization of the timbral palette and growing loudness levels that have
                     					shaped pop music over the past 60 years. The authors went on to suggest
                     					that past songs might be modernized into new hits by restricting pitch
                     					transitions, reducing timbral variety and making them louder. Tanya Clement
                     					further suggests how studying musical big data lends itself quite well to
                     					music related tasks, especially music composition, since the “notion of scholars ‘reading’ visualizations
                     						[(complete representation of the data)] relates to composers or
                     						musicians who read scores … [as] the musical score is an attempt to
                     						represent complex relationships … across time and space”
                     					[<a class="ref" href="#clement2013">Clement 2013</a>].</div>
                  				
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Big data can also be used to create labelled instances for training
                     					supervised learners like neural nets (which tend to be
                     						“data-thirsty”) and can be easily parsed by
                     					unsupervised learners to find patterns. This more recent ability to train
                     					music-related AI programs has largely been directed towards autonomously
                     					generating music in the same vein as whatever genre, style or composer that
                     					particular program has been trained on. A good example of this is “The Bach Doodle,” which can supply a melody with
                     					appropriate counter-melodies, accompaniment and chord changes in the style
                     					of Johann Sebastian Bach [<a class="ref" href="#huang2019">Huang et al. 2019</a>].</div>
                  				
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">While the availability of approaches such as Spotify's has allowed for the
                     					development of AI algorithms in music, many previous research projects have
                     					struggled to find felicitous links between this music-related big data and
                     					music itself. In the past, a common method involving the use of
                     					Krumhansl-Kessler profiles [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>], vectors of pitch
                     					correlation to the major or minor modes, allowed for mode or key
                     					predictability in some limited capacity. While it showed promise when
                     					applied to music of specific genres, it suffered when applied to a wider
                     					scope of genres or styles. We offer methods to alter KK-profiles, and the
                     					manner in which they are employed during preprocessing, to aid autonomous
                     					mode and key predictors ability and accuracy without being genre-specific.
                     					Without the ability to connect the intermediate dots, the overall accuracy
                     					of these algorithms diminishes and their output suffers accordingly.
                     					Indeed, AI-based autonomous generation is rarely up to the standards of
                     					composers or musicians creating new music. This paper offers preliminary
                     					solutions to two existing problems for which AI is typically used, mode and
                     					key prediction. We show that by equipping our algorithms with more
                     					background in music theory we can significantly improve their accuracy. The
                     					more the program learns about the music theoretic principles of mode and
                     					key, the better it gets. Our more general argument is that one way to help
                     					bridge the gap between music-related big data and AI is to give algorithms
                     					a strong grounding in music theory.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Mode Prediction</h1>
                  				
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">For the purpose of this paper, we looked at only the two most common modes in
                     					Western Music, the major and minor modes. These are also the only modes
                     					analyzed by <em class="emph">The Million Song Dataset</em> and Spotify's <em class="emph">Web
                        						API</em>. The major and minor modes are a part of the “Diatonic Collection,” which refers to “any scale [or mode] where the octave is divided evenly
                     						into seven steps”
                     					[<a class="ref" href="#laitz2003">Laitz 2003</a>]. A step can be either a whole or half step
                     					(whole tone or semitone) and the way that these are arranged in order to
                     					divide the octave will determine if the mode is major or minor. A major
                     					scale consists of the pattern <span class="hi bold">W-W-H-W-W-W-H</span> and the
                     					minor scale consists of <span class="hi bold">W-H-W-W-H-W-W</span>
                     					[<a class="ref" href="#laitz2003">Laitz 2003</a>]. <a href="#figure01">Figure 1</a> shows a
                     					major scale starting on the pitch “C” and <a href="#figure02">Figure 2</a> shows two types of minor scales starting on “C”.
                     					The seventh “step” in the harmonic minor scale example
                     					is raised in order to create a “leading tone.”
                     					The leading tone occurs when the seventh scale degree is a half step away
                     					from the first scale degree, also called the “tonic.” This leading tone to tonic relationship will become an
                     					important music theory principle that we use to train our AIs more
                     					accurately than previous published attempts.</div>
                  				
                  <div id="figure01" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 1. </div>C major scale demonstrating its make-up of whole and half
                        						tones.</div>
                  </div>
                  				
                  <div id="figure02" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 2. </div>Natural and harmonic minor scales starting on “C”.
                        						This also shows the progression of a natural minor scale to a harmonic
                        						scale by raising the seventh interval to become a leading tone.</div>
                  </div>
                  				
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">Many previous papers which use supervised learning to determine mode or key
                     					test only against songs from specific genres or styles, and few make
                     					attempts at predicting mode regardless of genre. Even the often-cited
                     					yearly competition on musical key detection hosted by Music Information
                     					Retrieval Evaluation eXchange (MIREX) has participants' algorithms compete
                     					at classifying 1252 classical music pieces [<a class="ref" href="#i%CC%87zmirli2006">İzmirli 2006</a>]
                     					[<a class="ref" href="#pauws2004">Pauws 2004</a>]. However, if we look again at <a href="#figure01">Figure 1</a> and <a href="#figure02">Figure
                        						2</a>, we can see that mode is not exclusive to genre or style, it
                     					is simply a specific arrangement of whole and half steps. So for a
                     					supervised learner programmed to “think” like a musician
                     					and thus determine mode based on its understanding of these music theory
                     					principles, genre or style should not affect the outcome. While this might
                     					work in a perfect world, artists have always looked for ways to
                     						“break away” from the norm and this can indeed
                     					manifest itself more in certain genres than others. Taking this into
                     					consideration, in this research we only selected songs for our separate
                     					ground truth set involving various genres which obey exact specifications
                     					for what constitutes as major or minor. This ground truth set will be a
                     					separate list of 100 songs labeled by us to further check the accuracy of
                     					our AI algorithms during testing. We wish to discuss shortcomings in the
                     					accuracy of past research that uses AI algorithms for predicting major or
                     					minor mode rather than to suggest a universal method for identifying all
                     					modes and scales.</div>
                  				
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">This is one aspect where our research differs from previous papers. An AI
                     					system which incorporates a solid understanding of the rules of music
                     					theory pertaining to mode should be able to outperform others that do not
                     					incorporate such understanding or those that focus on specific genres.
                     					While certain genres or styles may increase the difficulty of
                     					algorithmically determining mode, the same is true for a human musical
                     					analyst. When successful, an AI algorithm for determining mode will process
                     					data much faster than a musician, who would either have to look through the
                     					score or figure it out by ear in order to make their decision. For parsing
                     					music-related big data quickly and accurately, speed is imperative. Thus we
                     					suggest the following framework (<a href="#figure03">Figure 3</a>) by
                     					which a supervised learner can be trained to make predictions exclusively
                     					from pitch data in order to determine the mode of a song. The process is
                     					akin to methods used by human musical analysts. Below we also outline other
                     					areas where we apply a more musician-like approach to our methods to
                     					achieve greater accuracy.</div>
                  				
                  <div id="figure03" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 3. </div>Framework for supervised learning mode prediction algorithm.</div>
                  </div>
                  				
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">As can be seen in <a href="#figure03">Figure 3</a>, any scale or mode
                     					that does not meet the exact specifications of major or minor we categorize
                     					as <em class="emph">other</em>, <em class="emph">ambiguous</em> or <em class="emph">nontonal</em>
                     					(OANs). The primary reason that past research has trained supervised
                     					learners on only one specific genre or style is to avoid OANs. When OANs
                     					are not segregated from major or minor modes, they are fit improperly,
                     					leading to misclassifications.</div>
                  				
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8"><em class="emph">Other</em> pertains to any scale separate from major or minor that
                     					still contains a tonal center. Some examples of this are: modes (except
                     					Ionian or Aeolian), whole tone scale, pentatonic scale, and non-Western
                     					scales. <em class="emph">Nontonal</em> refers to any song which does not center
                     					around a given pitch. A common occurrence of this can be found in various
                     					examples of “12-tone music,” where each pitch is
                     					given equal importance in the song and thus no tonic can be derived from
                     					it.</div>
                  				
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">Where our paper differs from previous work is the handling of songs related
                     					to the outcome <em class="emph">ambiguous</em>. This occurs when either major or
                     					minor can be seen as an equally correct determination from the given
                     					pitches in a song. This most often occurs when chords containing the
                     					leading tone are avoided (<span class="error"><a href="#figure04">Figure 4</a></span>) and thus
                     					the remaining chords are consistent with both the major key and its
                     					relative minor (<span class="error"><a href="#figure04">Figure 4</a></span> &amp; <a href="#figure05">Figure 5</a>). The leading tone is a
                     						“tendency tone” or a tone that pulls towards a
                     					given target, in this case the tonic. This specific pull is one that can
                     					often signify a given mode and is therefore avoided in songs that the
                     					composer wished to float between the two modes. This can also be
                     					accomplished by simply using the relative minor's natural minor scale.
                     					Since the natural minor scale does not raise any pitches, it actually has
                     					the exact same notes (and resultant triads) as its relative major scale.
                     						<a href="#figure06">Figure 6</a> gives an example of a
                     					well-known pop song <em class="emph">Despacito</em>
                     					[<a class="ref" href="#fonsi2017">Fonsi and Yankee 2017</a>] and given these rules, what mode can we say the
                     					song is in? This tough choice is a common occurrence, especially in modern
                     					pop music, which can explain why papers that focused heavily on dance music
                     					might have had accuracy issues.</div>
                  				
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">Other authors have noted that their AI algorithms can mistake the major scale
                     					for its relative natural minor scale during prediction and it is likely
                     					that their algorithms did not account for the raised leading tone to truly
                     					distinguish major from minor. Since we focused on achieving higher accuracy
                     					than existing major vs minor mode prediction AI algorithms by incorporating
                     					music theory principles, we removed any instances of songs with an
                     					ambiguous mode from our ground truth set in order to get a clearer picture
                     					of how our system compares with the existing models. Adding other mode
                     					outcomes in order to detect OANs algorithmically is a part of our ongoing
                     					and future research.</div>
                  				
                  <div id="figure05" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 4. </div>Shows the relative minor scale from the C major scale in Fig. 3a. As
                        						seen in Fig. 1b, the leading tone (“G#”)
                        						must be added in order to make the obvious distinction from a major
                        						scale to a minor scale.</div>
                  </div>
                  				
                  <div id="figure06" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 5. </div>Chord progression for the hit song from Fonsi and Yankee’s Despacito
                        							[<a class="ref" href="#fonsi2017">Fonsi and Yankee 2017</a>]. What mode is it in?</div>
                  </div>
                  				
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">The most popular method of turning pitch data into something that can be used
                     					to train machine learners comes in the form of “chroma
                     						features.” Chroma feature data is available for every song
                     					found in Spotify's giant database of songs through the use of their
                     						<em class="emph">Web API</em>. Chroma features are vectors containing 12 values
                     					(coded as real numbers between 0 and 1) reflecting the relative dominance
                     					of all 12 pitches over the course of a small segment, usually lasting no
                     					longer than a second [<a class="ref" href="#jehan2014">Jehan 2014</a>]. Each vector begins on the
                     					pitch “C” and continues in chromatic order (C#, D, D#, etc.) until all
                     					12 pitches are reached. In order to create an AI model that could make
                     					predictions on a wide variety of musical styles, we collected the chroma
                     					features for approximately 100,000 songs over the last 40 years. Spotify's
                     						<em class="emph">Web API</em> offers its data at different temporal
                     					resolutions, from the aforementioned short segments through sections of the
                     					work to the track as a whole.</div>
                  				
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Beyond chroma features, the API offers Spotify's own algorithmic analysis of
                     					musical features such as mode within these temporal units, and provides a
                     					corresponding level of confidence for each (coded as a real number between
                     					0 and 1). We used Spotify's mode confidence levels to find every section
                     					within our 100,000 song list which had a mode confidence level of 0.6 or
                     					higher. The API's documentation states that “<em class="emph">confidence</em> indicates the reliability of its
                     						corresponding attribute... elements carrying a small confidence value
                     						should be considered speculative... there may not be sufficient data
                     						in the audio to compute the element with high certainty”
                     					[<a class="ref" href="#jehan2014">Jehan 2014</a>] thus giving good reason to remove sections with
                     					lower confidence levels from the dataset. Previous work such as Serrà et al
                     						[<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>], Finley and Razi [<a class="ref" href="#finley2019">Finley and Razi 2019</a>] and Mahieu [<a class="ref" href="#mahieu2017">Mahieu 2017</a>] also used confidence thresholds,
                     					but at the temporal resolution of a whole track rather than the sections
                     					that we used. By analyzing at the level of sections, we were able to triple
                     					our training samples from 100,000 songs to approximately 350,000 sections.
                     					Not only did this method increase the number of potential training samples,
                     					but it allowed us to focus on specific areas of each song that were more
                     					likely to provide an accurate representation of each mode as they appeared.
                     					For example, a classical piece of music in “sonata form” will undergo
                     					a “development” section whereby it passes through
                     					contrasting keys and modes to build tension before its final resolve to the
                     					home key, mode and initial material. Pop music employs a similar tactic
                     					with “the bridge,” a section found after the midway
                     					point of a song to add contrast to the musical material found up until this
                     					point. Both of these contrasting sections might add confusion during the
                     					training process if the song is analyzed as a whole, but removing them or
                     					analyzing them separately gives the program more accurate training samples.
                     					The ability to gain more training samples from the original list of songs
                     					has the advantage of providing more data for training a supervised
                     					learner.</div>
                  				
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">In previous work, a central tendency vector was created by taking the mean of
                     					each of the 12 components of the chroma vectors for a whole track, and this
                     					was then labelled as either major or minor for training. In an effort to
                     					mitigate the effects of noise on our averaged vector in any given
                     					recording, we found that using the medians rather than means gave us a
                     					better representation of the actual pitch content unaffected by potential
                     					outliers in the data. One common example is due to percussive instruments,
                     					such as a drum kit's cymbals, that happen to emphasize pitches that are
                     						“undesirable” for determining the song's particular
                     					key or mode. If that cymbal hit only occurs every few bars of music, but
                     					the “desirable” pitches occur much more often, we can
                     					lessen the effect that cymbal hit will have on our outcome by using a
                     					robust estimator. A musician working with a score or by ear would also
                     					filter out any unwanted sounds that did not help in mode determination. We
                     					found the medians of every segment's chroma feature vector found within
                     					each of our 350,000 sections.</div>
                  				
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">The last step in the preparation process is to transpose each chroma vector
                     					such that they are all aligned into the same key. As our neural network
                     					(NN) will only output predictions of major or minor, we want to have the
                     					exact same tonal center for each chroma vector to easily compare between
                     					their whole and half step patterns (<a href="#figure03">Figure
                        					3</a>). We based our transposition method on the one described by Serrà
                     					et al [<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>] and also used in their 2012 work. This
                     					method determines an “optimal transposition index” (OTI)
                     					by creating a new vector of the dot products between a chroma vector
                     					reflecting the key they wish to transpose to and the twelve possible
                     					rotations (i.e., 12 possible keys) of a second chroma vector. Using a right
                     					circular shift operation, the chroma vector is rotated by one half step
                     					each time until the maximum of 12 is reached. Argmax, a function which
                     					returns the position of the highest value in a vector, provides the OTI
                     					from the list of dot product correlation values, thus returning the number
                     					of steps needed to transpose the key of one chroma vector to match another
                     					(see Appendix 1.1 for a more detailed formula). Our method differs slightly
                     					from Serrà et al: since our vectors are all normalized, we used cosine
                     					similarity instead of the related dot product.</div>
                  				
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">In order to train a neural network for mode prediction, some previous studies
                     					used the mode labels from the Spotify <em class="emph">Web API</em> for whole tracks
                     					or for sections of tracks. When we checked these measures against our own
                     					separate ground truth set (analyzed by Lupker), we discovered that the
                     					automated mode labeling was relatively inaccurate (Table 1). Instead we
                     					adapted the less complex method of Finley &amp; Razi [<a class="ref" href="#finley2019">Finley and Razi 2019</a>], which reduced the need for training NNs. They
                     					compared chroma vectors to “KK-Profiles” to distinguish mode and other
                     					musical elements. Krumhansl and Kessler profiles (<a href="#figure07">Figure 7</a>) come from a study where human subjects were asked to
                     					rate how well each of the twelve chromatic notes fit within a key after
                     					hearing musical elements such as scales, chords or cadences [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>]. The resulting vector can be normalized to
                     					the range between 0-1 for direct comparisons to chroma vectors using
                     					similarity measures. By incorporating both modified chroma transpositions
                     					and KK-profile similarity tests, we were able to label our training data in
                     					a novel way.</div>
                  				
                  <div id="figure07" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 6. </div>Krumhansl and Kessler profiles for major and minor keys [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>].</div>
                  </div>
                  				
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">To combine these two approaches, we first rewrite Serrà et al's formula
                     					(Appendix 1.1) to incorporate Finley and Razi's method by making both
                     					KK-profile vectors (for major and minor modes) the new 'desired vectors' by
                     					which we will transpose our chroma vector set. This will eventually
                     					transpose our entire set of vectors to C major and C minor since the tonic
                     					of the KK-profiles is the first value in the vector, or pitch “C”.
                     					Correlations between KK-profiles and each of the 12 possible rotations of
                     					any given chroma vector are determined using cosine similarity. Instead of
                     					using the function which would return the position of the vector rotation
                     					that has the highest correlation (argmax), we use a different function
                     					which tells us what that correlation value is (amax). Two new lists are
                     					created. One is a list of the highest possible correlations for each
                     					transposed chroma vector and the major KK-profile, while the other is a
                     					list of correlations between each transposed chroma vector and the minor
                     					KK-profile. Finally, to determine the mode of each chroma vector, we then
                     					simply use a function to determine the position of the higher correlated
                     					value between these two lists, position 0 for major and 1 for minor (see
                     					Appendixes 1.2.1 &amp; 1.2.2).</div>
                  				
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">As noted by Finley &amp; Razi, the most common issue affecting accuracy
                     					levels for supervised or unsupervised machine learners attempting to detect
                     					the mode or key is “being off by a perfect musical
                     						interval of a fifth from the true key, relative mode errors or
                     						parallel mode errors”
                     					[<a class="ref" href="#finley2019">Finley and Razi 2019</a>]. Unlike papers which followed the MIREX
                     					competition rules, our algorithm does not give partial credit to
                     					miscalculations no matter how closely related they may be to the true mode
                     					or key. Instead we offer methods to reduce these errors. To attempt to
                     					correct these issues for mode detection, it is necessary to address the
                     					potential differences between a result from music psychology, like the
                     					KK-profiles, and the music theoretic concepts that they quantify. As we
                     					mentioned earlier, the leading tone in a scale is one of the most important
                     					signifiers of mode. In the <em class="emph">Despacito</em> example, where the
                     					leading tone is avoided, it is hard to determine major or minor mode. In
                     					the (empirically determined) KK-profiles, the leading tone seems to be
                     					ranked comparatively low relative to the importance it holds theoretically.
                     					If the pitches are ordered from greatest to lowest perceived importance,
                     					the leading tone doesn't even register in the top five in either
                     					KK-profile. This might be a consequence of the study design, which asked
                     					subjects to judge how well each note seemed to fit after hearing other
                     					musical elements played.</div>
                  				
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">The distance from the tonic to the leading tone is a major seventh interval
                     					(11 semitones). Different intervals fall into groups known as consonant or
                     					dissonant. Laitz defines consonant intervals as “stable
                     						intervals… such as the unison, the third, the fifth (perfect
                     						only)” and dissonant intervals as “unstable
                     						intervals… [including] the second, the seventh, and all diminished and
                     						augmented intervals”
                     					[<a class="ref" href="#laitz2003">Laitz 2003</a>]. More dissonant intervals are perceived as
                     					having more tension. Rather than separating intervals into categories of
                     					consonant and dissonant, Hindemith ranks them on a spectrum, which
                     					represents their properties more effectively. He ranks the octave as the
                     						“most perfect,” the major seventh as the
                     						“least perfect” and all intervals in between as
                     						“decreasing in euphony in proportion to their
                     						distance from the octave and their proximity to the major
                     						seventh”
                     					[<a class="ref" href="#hindemith1984">Hindemith 1984</a>]. While determining the best method of
                     					interval ranking is irrelevant to this paper, both theorists identify the
                     					major seventh as one of the most dissonant intervals. Thus, if the leading
                     					tone were to be played by itself (that is, without the context of a chord
                     					after a musical sequence) it might sound off, unstable or tense due to the
                     					dissonance of a major seventh interval in relation to the tonic. In a
                     					song's chord progression or melody, this note will often be given context
                     					by its chordal accompaniment or the note might be resolved by subsequent
                     					notes. These methods and others will 'handle the dissonance' and make the
                     					leading tone sound less out of place. We concluded that the leading tone
                     					value found within the empirical KK-profiles should be boosted to reflect
                     					its importance in a chord progression in the major or minor mode. Our tests
                     					showed that boosting the original major KK-profile's 12th value from 2.88
                     					to 3.7 and the original minor KK-profile's 12th value from 3.17 to 4.1
                     					increased the accuracy of the model at determining the correct mode by
                     					removing all instances where misclassifications were made between relative
                     					major and minor keys.</div>
                  				
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">Our training samples include a list of mode determinations labelling our
                     					350,000 chroma vectors. However, the algorithm assumes that every vector is
                     					in a major or minor mode with no consideration for OANs. Trying to
                     					categorize every vector as either major or minor leads to highly inaccurate
                     					results during testing, and seems to be a main cause of miscalculations
                     					made by the mode prediction algorithms of Spotify's <em class="emph">Web API</em>
                     					and the <em class="emph">Million Song Dataset</em>. To account for other or nontonal
                     					scales, we can set a threshold of acceptable correlation values (major and
                     					minor modes) and unacceptable values (other or nontonal scales). Our
                     					testing showed that a threshold of greater than or equal to 0.9 gave the
                     					best accuracy on our ground truth set for determining major or minor modes.
                     					These unacceptable vectors contain other or nontonal scales and future
                     					research will determine ways of addressing and classifying these
                     					further.</div>
                  				
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">To address <em class="emph">ambiguous</em> mode determinations between relative major
                     					and minor modes, we can set another threshold for removing potentially
                     					misleading data for training samples. While observing the correlation
                     					values used to determine major or minor labels, we set a further constraint
                     					when these values are too close to pick between them confidently. If the
                     					absolute difference between the two values is less than or equal to 0.02,
                     					we determine these correlation values to be indistinguishable and thus
                     					likely to reflect an ambiguous mode. As mentioned earlier, this is likely
                     					due to the song's chord progression avoiding certain mode determining
                     					factors such as the leading tone, and therefore the song can fit almost
                     					evenly into either the major or minor classification.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Mode Prediction Results</h1>
                  				
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">After filtering out samples determined to be OANs, our sample size was
                     					reduced to approximately 100,000. From this dataset, 75% of the samples
                     					were selected to train the model and an even split of the remaining 25% of
                     					samples being withheld for testing and cross-validation to check the
                     					model's accuracy and performance. On the withheld test set, our model
                     					returned a very high accuracy of 99% during testing. We found that this was
                     					much better than the results reported from other studies on withheld test
                     					sets. This level of accuracy and the ability to compute the data quickly
                     					make it useful for parsing big data for any research that makes comparisons
                     					based on modes.</div>
                  				
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">In addition to testing using only samples withheld from our large dataset, we
                     					created a separate ground truth set of 100 songs taken from various
                     						“top 100” charts from different genres such as pop,
                     					country, classical and jazz. This ground truth set was labeled ourselves by
                     					looking at the score of each song and comparing it with the exact recording
                     					found on Spotify (in order to make sure it wasn't a version recorded in a
                     					different key). Our NN reached an accuracy of 88% on the outside ground
                     					truth set. The discrepancy is perhaps due to learning from samples that
                     					were improperly labelled during cosine similarity measures against
                     					KK-profiles. Since this model only outputs major or minor, it is hard to
                     					track the exact details of each misclassification. It could be
                     					miscalculating a result based on relative major or minors, parallel major
                     					or minors (C major vs C Minor) or just be completely off. These incorrectly
                     					identified modes will be further addressed in the next section where key is
                     					also determined, giving us a clearer understanding of the problem. It is
                     					important to note the very low accuracy of 18% in modes determined by
                     					Spotify's <em class="emph">Web API</em>. Future research based on the API's mode
                     					labelling algorithms should be tested against ground truth datasets before
                     					being used to make musicological claims.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Key Prediction</h1>
                  				
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">With a highly accurate working model for mode detection, adding the ability
                     					to predict key becomes fairly straightforward. Our mode detection algorithm
                     					is based on the music theory principle of determining mode by first finding
                     					the tonic, then calculating the subsequent intervals. Additionally, this
                     					method assumes the tonic to be the most frequent note and therefore the
                     					tonic should always register as the most prominent note in any given
                     					median-averaged chroma vector. Thus when transposing each chroma vector in
                     					our ground truth set to C major or C minor, we must keep track of the
                     					initial position of the most prominent pitches, as these are our key
                     					determining features. This method of analyzing songs based on a non
                     					key-specific approach first, and then adding key labels afterwards is
                     					derived from the method of 'roman numeral analysis' in musicology. This
                     					kind of analysis is used by music theorists to outline triadic chord
                     					progressions found within tonal music [<a class="ref" href="#laitz2003">Laitz 2003</a>]. In this
                     					method, uppercase numerals are used for major triads and lowercase numerals
                     					are used for minor triads (<a href="#figure08">Figure 8</a>). The
                     					method itself is not key-specific (besides a brief mention at the
                     					beginning), allowing the analysis of underlying chord relationships across
                     					multiple songs regardless of key. Considering that machine learning
                     					programs typically need large datasets for training, and that it is
                     					unlikely even large datasets will contain songs in every possible key in
                     					the same proportions, roman numeral analysis is ideal.</div>
                  				
                  <div id="figure08" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 7. </div>A roman numeral analysis of major scale triads where uppercase
                        						numerals equal major triads and lowercase for minor. The chord built
                        						upon the seventh uses a degree sign to denote that it is a diminished
                        						chord.</div>
                  </div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Key Prediction Results</h1>
                  				
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">As our key predictions result from a simple labelling of our mode prediction
                     					neural network's output, we cannot compute accuracy of training. Past
                     					testing did include a second NN where the dataset taken from the mode
                     					detection NN was rotated to each of the 12 possible keys and then trained
                     					on that with 24 outputs for each major and minor key. This training
                     					resulted in 93% accuracy but we didn't find any significant increase over
                     					the method of applying labels after running the mode NN when testing
                     					against the ground truth set, therefore we decided against training a
                     					second NN to detect key as it saves training time. With the key labels
                     					added to the output of our mode NN, our model returned an accuracy level of
                     					48%. While not nearly as accurate as our mode prediction algorithm, it is
                     					much higher than the Spotify <em class="emph">Web API</em> which returns an accuracy
                     					of 2% on our ground truth set.</div>
                  				
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">It is also difficult to compare our accuracy on a ground truth set with any
                     					paper which used the MIREX competition dataset. MIREX's dataset contains
                     					remakes of real songs using Musical Instrument Digital Interface (MIDI)
                     					synthesizers which can be recorded straight into a computer, without the
                     					need for microphones. Furthermore, it is unclear if this dataset recreates
                     					the percussion synthetically for each song or if this is removed to allow
                     					easier access to the harmonic content for contestants to test their AI
                     					systems. As our training and testing uses actual recorded music and no MIDI
                     					representations there is a higher likelihood of noise or percussive
                     					elements to throw off our algorithm during testing the outside ground truth
                     					set. As of this research, we have not entered the MIREX competition, and
                     					thus do not have access to their datasets and cannot compare the
                     					performance of our algorithm with those which did. In future research we
                     					plan to enter the 2020 MIREX key detection competition and to draw more
                     					accurate comparisons with other key detection AI models.</div>
                  				
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">Our much lower accuracy levels for key prediction in comparison to mode
                     					prediction are consistent with misclassification noted within previous
                     					research. While we were able to almost entirely remove instances of
                     					relative major and minor mode misclassifications, the big outstanding
                     					problem are keys classified by an interval of a fifth away. We see this as
                     					a potential limitation to the ability of supervised or unsupervised
                     					learning techniques to predict key (and mode to an almost perfect degree)
                     					from pitch content alone. If we imagine this method of determining mode and
                     					key from pitch content was performed by a human musician, the equivalent
                     					would be that of writing down each pitch in the first section (or the
                     					entirety) of a song and making predictions based on a tally. Since this
                     					method relies on the tonic being the most prominent pitch in this tally,
                     					the model will likely always fail when this is not the case. For example, a
                     					song might be in C major with a melody often repeating the pitch 'G'
                     					throughout the verse even while a tonic C major chord provides the
                     					accompaniment underneath. Since the pitch 'G' is found in the tonic chord
                     					of C major (<span class="error"><a href="#figure04">Figure 4</a></span>), it will sound good and
                     					be a theoretically correct decision. The repeated “G”, however, will
                     					be the most prominent note in the chroma vector section. The scale found a
                     					fifth away from a C major scale, G Major, shares all the same notes except
                     					one, thus confusing the model. Even with all the processes aimed at
                     					removing cases of OANs, a melody focused around the fifth interval of a
                     					tonic chord will constantly skew the data and the model will make
                     					miscalculations on key.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Conclusions</h1>
                  				
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">Our neural net model itself is quite simple and does not represent a novel
                     					approach to music AI models. Our research was instead focused on data
                     					preparation methods grounded in music theory helping to boost the accuracy
                     					of existing models by finding more appropriate links between music-related
                     					big data and the resultant outputs. Using our methods such as boosting the
                     					leading tone's prominence in the KK-profile and filtering out OANs, we were
                     					able to construct a model with a higher accuracy for mode prediction during
                     					both testing on a withheld subset of our dataset and on the external ground
                     					truth set labeled by Lupker. Our key prediction showed comparable results
                     					to previous research on a separate ground truth set and we see this as a
                     					limit to the ability of predicting key using a pitch-based tally
                     					classifier. The model's prediction can be too easily skewed by any song
                     					with a melody focused on the fifth interval of the tonic chord, mistaking
                     					the key for one a fifth away. Accessing the MIREX dataset would give us a
                     					better comparison with those papers which have competed in past
                     					competitions, but we predict similar results to those found in this
                     					paper.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Further Research</h1>
                  				
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">Further steps we could take with this research would be to identify and label
                     					OANs to create a more universal mode and key prediction NN. The similarity
                     					measures specified in this paper could be repeated given chroma vector
                     					profiles of other modes and scales to compare against. This would be useful
                     					for any research projects experimenting with big data related to
                     					non-Western music. Another area for further research would be to apply our
                     					music theory based processes to some existing chord retrieval algorithms.
                     					Our testing leads us to believe that training NNs on chord transition
                     					networks as related to mode or key is the only way to reach accuracy levels
                     					comparable to human predictions. When musicians are faced with determining
                     					mode or key of a song with less obvious features, the last resort method is
                     					to look at the chord progression and make a decision based on that. Our
                     					predictions are that a learner trained to look for mode or key determining
                     					features of a chord progression will outperform those based on averaging
                     					tally counts of pitch.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">Appendix 1.</h1>
                  				
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">Serrà et al.'s formula (<a href="#serr%C3%A02008">2008</a>) to find the OTI between vector \(\overrightarrow{g_{A}}\)(the
                     					desired key) and all 12 possible rotations of vector \(\overrightarrow{g_{B}}\). Rotations are
                     					accomplished using the <span class="hi italic">Circshift<span class="hi subscript">R</span></span> function with <span class="hi italic">M</span> being the
                     					maximum amount of rotations (12) and <span class="hi italic">j</span> being the
                     					amount to rotate by. The function <span class="hi italic">argmax</span> then
                     					selects the rotation amount <span class="hi italic">(j)</span>, which rotated by
                     					vector \(\overrightarrow{g_{B}}\) to the position with the maximum
                     					correlation value with vector \(\overrightarrow{g_{A}}\). 
                     					$$OTI\left( \overrightarrow{g_{A}},\overrightarrow{g_{B}} \right) = \underset{1 \leq
                     j \leq M}{\text{argmax}}\left\{ \overrightarrow{g_{A}} \cdot \text{Circshift}_{R}\left(
                     \overrightarrow{g_{B}},j - 1 \right) \right\}$$
                     				</div>
                  				
                  				
                  <div id="figure10" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 8. </div>Modified formula to transpose chroma vectors separated by major and
                        						minor modes</div>
                  </div>
                  				
                  <div id="figure11" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure11.png" rel="external"><img src="resources/images/figure11.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 9. </div>Training neural network</div>
                  </div>
                  				
                  <div id="figure12" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure12.png" rel="external"><img src="resources/images/figure12.png" style="" alt="" /></a></div>
                     				
                     <div class="caption">
                        <div class="label">Figure 10. </div>Computational workflow regarding the collection of data and the
                        						subsequent preprocessing steps required to create training samples and
                        						labels which can be fed into the neural network.</div>
                  </div>
                  			</div>
               		
               
               		
               		
               			
               		
               	</div>
            	
            
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e186"><span class="noteRef lang en">[1] Our use of the term 'big data' refers
                     						to datasets that are so large that conventional computers may have
                     						difficulty processing them. Researchers are now able to access
                     						additional computational power in the form of cloud resources such as
                     						GPUs, as we have done here. Our experiments were run using Google
                     						Colaboratory.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="bertin2011"><!-- close -->Bertin et al. 2011</span> Bertin-Mahieux, T., D.
                  					Ellis, B. Whitman, and P. Lamere, “The Million Song
                  						Dataset”
                  					<cite class="title italic">Proceedings of the 12th International Conference on
                     						Music Information</cite>, Miami, USA, October 2011.</div>
               <div class="bibl"><span class="ref" id="clement2013"><!-- close -->Clement 2013</span> Clement, T., “Distant Listening or Playing Visualisations Pleasantly
                  						with the Eyes and Ears.”
                  					<cite class="title italic">Digital Studies/le Champ Numérique</cite>, 3.2. DOI:
                  						<a href="http://doi.org/10.16995/dscn.236" onclick="window.open('http://doi.org/10.16995/dscn.236'); return false" class="ref">http://doi.org/10.16995/dscn.236</a> (2013).</div>
               <div class="bibl"><span class="ref" id="finley2019"><!-- close -->Finley and Razi 2019</span> Finley, M. and Razi, A.,
                  						“Musical Key Estimation with Unsupervised Pattern
                  						Recognition” IEEE 9th Annual Computing and Communication
                  					Workshop and Conference, Las Vegas, USA, January 2019.</div>
               <div class="bibl"><span class="ref" id="fonsi2017"><!-- close -->Fonsi and Yankee 2017</span> Fonsi, Luis and Daddy
                  					Yankee, “Despacito” Vida. Universal Latin, Los
                  					Angeles, 2017. <a href="http://doi.org/10.16995/dscn.236" onclick="window.open('http://doi.org/10.16995/dscn.236'); return false" class="ref">Online</a>.</div>
               <div class="bibl"><span class="ref" id="gomez2004"><!-- close -->Gomez and Herrera 2004</span> Gomez, E and P. Herrera,
                  						“Estimating the Tonality of Polyphonic Audio
                  						Files: Cognitive Versus Machine Learning Modelling Strategies”
                  					<cite class="title italic">Proceedings of the 5th International Society for Music
                     						Information Retrieval Conference</cite>, Barcelona, Spain, October
                  					2004.</div>
               <div class="bibl"><span class="ref" id="hindemith1984"><!-- close -->Hindemith 1984</span> Hindemith, P., <cite class="title italic">The Craft of Musical Composition: Theoretical Part -
                     						Book 1</cite>. Schott, Mainz, 1984.</div>
               <div class="bibl"><span class="ref" id="huang2019"><!-- close -->Huang et al. 2019</span> Huang, C. A., C. Hawthorne,
                  					A. Roberts, M. Dinculescu, J. Wexler, L. Hong, J. Howcroft, “The Bach Doodle: Approachable Music Composition with
                  						Machine Learning at Scale”
                  					<cite class="title italic">Proceedings of the 18th International Society for
                     						Music Information Retrieval Conference, ISMIR 2019</cite>. <a href="https://arxiv.org/abs/1907.06637" onclick="window.open('https://arxiv.org/abs/1907.06637'); return false" class="ref">arXiv:1907.06637</a>
                  					(2019).</div>
               <div class="bibl"><span class="ref" id="i̇zmirli2006"><!-- close -->İzmirli 2006</span> İzmirli, O., “Audio Key Finding Using Low Dimensional Spaces”
                  					<cite class="title italic">Proceedings from the 7th International Conference on
                     						Music Information Retrieval</cite>, Victoria, Canada, October
                  					2006.</div>
               <div class="bibl"><span class="ref" id="jehan2014"><!-- close -->Jehan 2014</span> Jehan, T., “Analyzer Documentation: The EchoNest.” Somerville: The Echo
                  					Nest Corporation, 2014, 5.</div>
               <div class="bibl"><span class="ref" id="krumhansl1982"><!-- close -->Krumhansl and Kessler 1982</span> Krumhansl, C.
                  					L., and E. J. Kessler, “Tracing the Dynamic Changes in
                  						Perceived Tonal Organization in a Spatial Representation of Musical
                  						Keys”
                  					<cite class="title italic">Psychological Review</cite>, 89.4 (1982),
                  					334-368.</div>
               <div class="bibl"><span class="ref" id="laitz2003"><!-- close -->Laitz 2003</span> Laitz, S. G., <cite class="title italic">The Complete Musician: An Integrated Approach to Tonal Theory,
                     						Analysis, and Listening</cite>. Oxford University Press, New York,
                  					2003.</div>
               <div class="bibl"><span class="ref" id="mahieu2017"><!-- close -->Mahieu 2017</span> Mahieu, R., “Detecting Musical Key with Supervised Learning” unpublished
                  					manuscript, 2017.</div>
               <div class="bibl"><span class="ref" id="pauws2004"><!-- close -->Pauws 2004</span> Pauws, S., “Musical Key Extraction from Audio”
                  					<cite class="title italic">Proceedings of the 5th International Society for Music
                     						Information Retrieval Conference</cite>, Barcelona, Spain, October
                  					2004.</div>
               <div class="bibl"><span class="ref" id="serrà2008"><!-- close -->Serrà and Gómez 2008</span> Serrà, J., E. Gómez &amp;
                  					P. Herrera. “Transposing Chroma Representations to a
                  						Common Key.” IEEE CS Conference on The Use of Symbols to
                  					Represent Music and Multimedia Objects (2008).</div>
               <div class="bibl"><span class="ref" id="serrà2012a"><!-- close -->Serrà et al. 2012a</span> Serrà, J. , Á. Corral, M.
                  					Boguñá, M. Haro &amp; J. Ll. Arcos. “Measuring the
                  						Evolution of Contemporary Western Popular Music”
                  					<cite class="title italic">Scientific Reports</cite>, 521.2 (2012).</div>
               <div class="bibl"><span class="ref" id="serrà2012b"><!-- close -->Serrà et al. 2012b</span> Serrà, J., Á. Corral, M.
                  					Boguñá, M. Haro &amp; J. Ll. Arcos. “Supplementary
                  						Information: Measuring the Evolution of Contemporary Western Popular
                  						Music”
                  					<cite class="title italic">Scientific Reports</cite> 521.2 (2012).</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>