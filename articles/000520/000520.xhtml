<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 014 Number 4
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol/14/4/000520.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol/14/4/000520/000520.html#disqus_thread" data-disqus-identifier="000520">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Music Theory, the Missing Link Between Music-Related Big
                  Data and Artificial Intelligence
               </h1>
               
               <div class="author"><span style="color: grey">Jeffrey A. T. Lupker</span> &lt;<a href="mailto:jlupker_at_uwo_dot_ca" onclick="javascript:window.location.href='mailto:'+deobfuscate('jlupker_at_uwo_dot_ca'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('jlupker_at_uwo_dot_ca'); return false;">jlupker_at_uwo_dot_ca</a>&gt;, The University of Western Ontario
               </div>
               
               <div class="author"><span style="color: grey">William J. Turkel</span> &lt;<a href="mailto:wturkel_at_uwo_dot_ca" onclick="javascript:window.location.href='mailto:'+deobfuscate('wturkel_at_uwo_dot_ca'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('wturkel_at_uwo_dot_ca'); return false;">wturkel_at_uwo_dot_ca</a>&gt;, The University of Western Ontario
               </div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Music%20Theory,%20the%20Missing%20Link%20Between%20Music-Related%20Big%20Data%20and%20Artificial%20Intelligence&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=014&amp;rft.issue=4&amp;rft.aulast=Lupker&amp;rft.aufirst=Jeffrey A. T.&amp;rft.au=Jeffrey A. T.%20Lupker&amp;rft.au=William J.%20Turkel"> </span></div>
            
            
            
            
            <div id="DHQtext">
               
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>This paper examines musical artificial intelligence (AI) algorithms that can not only
                     learn from big data, but learn in ways that would be familiar to a musician or music
                     theorist. This paper aims to find more effective links between music-related big data
                     and
                     artificial intelligence algorithms by incorporating principles with a strong grounding
                     in
                     music theory. We show that it is possible to increase the accuracy of two common
                     algorithms (mode prediction and key prediction) by using music-theory based techniques
                     during the data preparation process. We offer methods to alter often-used Krumhansl
                     Kessler profiles [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>], and the manner in which they are employed
                     during preprocessing, to aid the connection of musical big data and mode or key predicting
                     algorithms.
                  </p>
                  
               </div>
               
               
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Research in music information retrieval has produced many possibilities for developing
                     artificial intelligence (AI) algorithms that can perform a wide variety of musically-based
                     tasks, including even music composition itself. The availability of vast musical datasets
                     like the <em class="emph">Million Song Dataset</em>
                     [<a class="ref" href="#bertin2011">Bertin et al. 2011</a>] and Spotify's <em class="emph">Web API</em> has made it possible for
                     researchers to acquire algorithmically-determined characteristics of a song's key,
                     mode,
                     pitch content, and more. At the same time, the existence of these large datasets has
                     made
                     it possible for researchers to take a “big data”<a class="noteRef" href="#d4e202">[1]</a> approach to various styles of Western music. One
                     notable example is the work by Serrà et al [<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>] which showed the
                     changes and trends related to pitch transitions, the homogenization of the timbral
                     palette
                     and growing loudness levels that have shaped pop music over the past 60 years. The
                     authors
                     went on to suggest that past songs might be modernized into new hits by restricting
                     pitch
                     transitions, reducing timbral variety and making them louder. Tanya Clement further
                     suggests how studying musical big data lends itself quite well to music related tasks,
                     especially music composition, since the “notion of scholars
                     ‘reading’ visualizations [(complete representation of the data)] relates to
                     composers or musicians who read scores … [as] the musical score is an attempt to
                     represent complex relationships … across time and space”
                     [<a class="ref" href="#clement1699">Clement 1699</a>].
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Big data can also be used to create labelled instances for training supervised learners
                     like neural nets (which tend to be “data-thirsty”) and can be easily
                     parsed by unsupervised learners to find patterns. This more recent ability to train
                     music-related AI programs has largely been directed towards autonomously generating
                     music
                     in the same vein as whatever genre, style or composer that particular program has
                     been
                     trained on. A good example of this is “The Bach Doodle,” which
                     can supply a melody with appropriate counter-melodies, accompaniment and chord changes
                     in
                     the style of Johann Sebastian Bach [<a class="ref" href="#huang2019">Huang et al. 2019</a>].
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">While the availability of approaches such as Spotify's has allowed for the development
                     of
                     AI algorithms in music, many previous research projects have struggled to find felicitous
                     links between this music-related big data and music itself. In the past, a common
                     method
                     involving the use of Krumhansl-Kessler profiles [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>], vectors of
                     pitch correlation to the major or minor modes, allowed for mode or key predictability
                     in
                     some limited capacity. While it showed promise when applied to music of specific genres,
                     it suffered when applied to a wider scope of genres or styles. We offer methods to
                     alter
                     KK-profiles, and the manner in which they are employed during preprocessing, to aid
                     autonomous mode and key predictors ability and accuracy without being genre-specific.
                     Without the ability to connect the intermediate dots, the overall accuracy of these
                     algorithms diminishes and their output suffers accordingly. Indeed, AI-based autonomous
                     generation is rarely up to the standards of composers or musicians creating new music.
                     This paper offers preliminary solutions to two existing problems for which AI is typically
                     used, mode and key prediction. We show that by equipping our algorithms with more
                     background in music theory we can significantly improve their accuracy. The more the
                     program learns about the music theoretic principles of mode and key, the better it
                     gets.
                     Our more general argument is that one way to help bridge the gap between music-related
                     big
                     data and AI is to give algorithms a strong grounding in music theory.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Mode Prediction</h1>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">For the purpose of this paper, we looked at only the two most common modes in Western
                     Music, the major and minor modes. These are also the only modes analyzed by <em class="emph">The
                        Million Song Dataset</em> and Spotify's <em class="emph">Web API</em>. The major and minor
                     modes are a part of the “Diatonic Collection,” which refers to
                     “any scale [or mode] where the octave is divided evenly into seven
                     steps”
                     [<a class="ref" href="#laitz2003">Laitz 2003</a>]. A step can be either a whole or half step (whole tone or
                     semitone) and the way that these are arranged in order to divide the octave will determine
                     if the mode is major or minor. A major scale consists of the pattern
                     W-W-H-W-W-W-H and the minor scale consists of
                     W-H-W-W-H-W-W
                     [<a class="ref" href="#laitz2003">Laitz 2003</a>]. <a href="#figure01">Figure 1</a> shows a major scale
                     starting on the pitch “C” and <a href="#figure02">Figure 2</a> shows two types
                     of minor scales starting on “C”. The seventh “step” in the
                     harmonic minor scale example is raised in order to create a “leading
                     tone.” The leading tone occurs when the seventh scale degree is a half step away
                     from the first scale degree, also called the “tonic.” This
                     leading tone to tonic relationship will become an important music theory principle
                     that we
                     use to train our AIs more accurately than previous published attempts.
                  </div>
                  
                  <div id="figure01" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 1. </div>C major scale demonstrating its make-up of whole and half tones.
                     </div>
                  </div>
                  
                  <div id="figure02" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 2. </div>Natural and harmonic minor scales starting on “C”. This also
                        shows the progression of a natural minor scale to a harmonic scale by raising the
                        seventh interval to become a leading tone.
                     </div>
                  </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">Many previous papers which use supervised learning to determine mode or key test only
                     against songs from specific genres or styles, and few make attempts at predicting
                     mode
                     regardless of genre. Even the often-cited yearly competition on musical key detection
                     hosted by Music Information Retrieval Evaluation eXchange (MIREX) has participants'
                     algorithms compete at classifying 1252 classical music pieces [<a class="ref" href="#i%CC%87zmirli2006">İzmirli 2006</a>]
                     [<a class="ref" href="#pauws2004">Pauws 2004</a>]. However, if we look again at <a href="#figure01">Figure
                        1</a> and <a href="#figure02">Figure 2</a>, we can see that mode is not
                     exclusive to genre or style, it is simply a specific arrangement of whole and half
                     steps.
                     So for a supervised learner programmed to “think” like a musician and
                     thus determine mode based on its understanding of these music theory principles, genre
                     or
                     style should not affect the outcome. While this might work in a perfect world, artists
                     have always looked for ways to “break away” from the norm and this can
                     indeed manifest itself more in certain genres than others. Taking this into consideration,
                     in this research we only selected songs for our separate ground truth set involving
                     various genres which obey exact specifications for what constitutes as major or minor.
                     This ground truth set will be a separate list of 100 songs labeled by us to further
                     check
                     the accuracy of our AI algorithms during testing. We wish to discuss shortcomings
                     in the
                     accuracy of past research that uses AI algorithms for predicting major or minor mode
                     rather than to suggest a universal method for identifying all modes and scales.
                  </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">This is one aspect where our research differs from previous papers. An AI system which
                     incorporates a solid understanding of the rules of music theory pertaining to mode
                     should
                     be able to outperform others that do not incorporate such understanding or those that
                     focus on specific genres. While certain genres or styles may increase the difficulty
                     of
                     algorithmically determining mode, the same is true for a human musical analyst. When
                     successful, an AI algorithm for determining mode will process data much faster than
                     a
                     musician, who would either have to look through the score or figure it out by ear
                     in order
                     to make their decision. For parsing music-related big data quickly and accurately,
                     speed
                     is imperative. Thus we suggest the following framework (<a href="#figure03">Figure
                        3</a>) by which a supervised learner can be trained to make predictions exclusively
                     from pitch data in order to determine the mode of a song. The process is akin to methods
                     used by human musical analysts. Below we also outline other areas where we apply a
                     more
                     musician-like approach to our methods to achieve greater accuracy.
                  </div>
                  
                  <div id="figure03" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 3. </div>Framework for supervised learning mode prediction algorithm.
                     </div>
                  </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">As can be seen in <a href="#figure03">Figure 3</a>, any scale or mode that does not
                     meet the exact specifications of major or minor we categorize as <em class="emph">other</em>,
                     <em class="emph">ambiguous</em> or <em class="emph">nontonal</em> (OANs). The primary reason that past
                     research has trained supervised learners on only one specific genre or style is to
                     avoid
                     OANs. When OANs are not segregated from major or minor modes, they are fit improperly,
                     leading to misclassifications.
                  </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8"><em class="emph">Other</em> pertains to any scale separate from major or minor that still contains
                     a tonal center. Some examples of this are: modes (except Ionian or Aeolian), whole
                     tone
                     scale, pentatonic scale, and non-Western scales. <em class="emph">Nontonal</em> refers to any song
                     which does not center around a given pitch. A common occurrence of this can be found
                     in
                     various examples of “12-tone music,” where each pitch is given
                     equal importance in the song and thus no tonic can be derived from it.
                  </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">Where our paper differs from previous work is the handling of songs related to the
                     outcome <em class="emph">ambiguous</em>. This occurs when either major or minor can be seen as an
                     equally correct determination from the given pitches in a song. This most often occurs
                     when chords containing the leading tone are avoided (<span class="error"><a href="#figure04">Figure
                           4</a></span>) and thus the remaining chords are consistent with both the major key and its
                     relative minor (<span class="error"><a href="#figure04">Figure 4</a></span> &amp; <a href="#figure05">Figure 5</a>). The leading tone is a “tendency tone” or a tone
                     that pulls towards a given target, in this case the tonic. This specific pull is one
                     that
                     can often signify a given mode and is therefore avoided in songs that the composer
                     wished
                     to float between the two modes. This can also be accomplished by simply using the
                     relative
                     minor's natural minor scale. Since the natural minor scale does not raise any pitches,
                     it
                     actually has the exact same notes (and resultant triads) as its relative major scale.
                     <a href="#figure06">Figure 6</a> gives an example of a well-known pop song
                     <em class="emph">Despacito</em>
                     [<a class="ref" href="#fonsi2017">Fonsi and Yankee 2017</a>] and given these rules, what mode can we say the song is in?
                     This tough choice is a common occurrence, especially in modern pop music, which can
                     explain why papers that focused heavily on dance music might have had accuracy issues.
                  </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">Other authors have noted that their AI algorithms can mistake the major scale for
                     its
                     relative natural minor scale during prediction and it is likely that their algorithms
                     did
                     not account for the raised leading tone to truly distinguish major from minor. Since
                     we
                     focused on achieving higher accuracy than existing major vs minor mode prediction
                     AI
                     algorithms by incorporating music theory principles, we removed any instances of songs
                     with an ambiguous mode from our ground truth set in order to get a clearer picture
                     of how
                     our system compares with the existing models. Adding other mode outcomes in order
                     to
                     detect OANs algorithmically is a part of our ongoing and future research.
                  </div>
                  
                  <div id="figure05" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 4. </div>Shows the relative minor scale from the C major scale in Fig. 3a. As seen in Fig.
                        1b, the leading tone (“G#”) must be added in order to make
                        the obvious distinction from a major scale to a minor scale.
                     </div>
                  </div>
                  
                  <div id="figure06" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 5. </div>Chord progression for the hit song from Fonsi and Yankee’s Despacito [<a class="ref" href="#fonsi2017">Fonsi and Yankee 2017</a>]. What mode is it in?
                     </div>
                  </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">The most popular method of turning pitch data into something that can be used to train
                     machine learners comes in the form of “chroma features.” Chroma feature
                     data is available for every song found in Spotify's giant database of songs through
                     the
                     use of their <em class="emph">Web API</em>. Chroma features are vectors containing 12 values (coded
                     as real numbers between 0 and 1) reflecting the relative dominance of all 12 pitches
                     over
                     the course of a small segment, usually lasting no longer than a second [<a class="ref" href="#jehan2014">Jehan 2014</a>]. Each vector begins on the pitch “C” and continues in
                     chromatic order (C#, D, D#, etc.) until all 12 pitches are reached. In order to create
                     an
                     AI model that could make predictions on a wide variety of musical styles, we collected
                     the
                     chroma features for approximately 100,000 songs over the last 40 years. Spotify's
                     <em class="emph">Web API</em> offers its data at different temporal resolutions, from the
                     aforementioned short segments through sections of the work to the track as a whole.
                  </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Beyond chroma features, the API offers Spotify's own algorithmic analysis of musical
                     features such as mode within these temporal units, and provides a corresponding level
                     of
                     confidence for each (coded as a real number between 0 and 1). We used Spotify's mode
                     confidence levels to find every section within our 100,000 song list which had a mode
                     confidence level of 0.6 or higher. The API's documentation states that “<em class="emph">confidence</em> indicates the reliability of its corresponding
                     attribute... elements carrying a small confidence value should be considered
                     speculative... there may not be sufficient data in the audio to compute the element
                     with
                     high certainty”
                     [<a class="ref" href="#jehan2014">Jehan 2014</a>] thus giving good reason to remove sections with lower
                     confidence levels from the dataset. Previous work such as Serrà et al [<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>], Finley and Razi [<a class="ref" href="#finley2019">Finley and Razi 2019</a>] and Mahieu [<a class="ref" href="#mahieu2017">Mahieu 2017</a>] also used confidence thresholds, but at the temporal resolution
                     of a whole track rather than the sections that we used. By analyzing at the level
                     of
                     sections, we were able to triple our training samples from 100,000 songs to approximately
                     350,000 sections. Not only did this method increase the number of potential training
                     samples, but it allowed us to focus on specific areas of each song that were more
                     likely
                     to provide an accurate representation of each mode as they appeared. For example,
                     a
                     classical piece of music in “sonata form” will undergo a
                     “development” section whereby it passes through contrasting keys and
                     modes to build tension before its final resolve to the home key, mode and initial
                     material. Pop music employs a similar tactic with “the bridge,” a
                     section found after the midway point of a song to add contrast to the musical material
                     found up until this point. Both of these contrasting sections might add confusion
                     during
                     the training process if the song is analyzed as a whole, but removing them or analyzing
                     them separately gives the program more accurate training samples. The ability to gain
                     more
                     training samples from the original list of songs has the advantage of providing more
                     data
                     for training a supervised learner.
                  </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">In previous work, a central tendency vector was created by taking the mean of each
                     of the
                     12 components of the chroma vectors for a whole track, and this was then labelled
                     as
                     either major or minor for training. In an effort to mitigate the effects of noise
                     on our
                     averaged vector in any given recording, we found that using the medians rather than
                     means
                     gave us a better representation of the actual pitch content unaffected by potential
                     outliers in the data. One common example is due to percussive instruments, such as
                     a drum
                     kit's cymbals, that happen to emphasize pitches that are “undesirable”
                     for determining the song's particular key or mode. If that cymbal hit only occurs
                     every
                     few bars of music, but the “desirable” pitches occur much more often,
                     we can lessen the effect that cymbal hit will have on our outcome by using a robust
                     estimator. A musician working with a score or by ear would also filter out any unwanted
                     sounds that did not help in mode determination. We found the medians of every segment's
                     chroma feature vector found within each of our 350,000 sections.
                  </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">The last step in the preparation process is to transpose each chroma vector such that
                     they are all aligned into the same key. As our neural network (NN) will only output
                     predictions of major or minor, we want to have the exact same tonal center for each
                     chroma
                     vector to easily compare between their whole and half step patterns (<a href="#figure03">Figure 3</a>). We based our transposition method on the one
                     described by Serrà et al [<a class="ref" href="#serr%C3%A02012b">Serrà et al. 2012b</a>] and also used in their 2012 work.
                     This method determines an “optimal transposition index” (OTI) by
                     creating a new vector of the dot products between a chroma vector reflecting the key
                     they
                     wish to transpose to and the twelve possible rotations (i.e., 12 possible keys) of
                     a
                     second chroma vector. Using a right circular shift operation, the chroma vector is
                     rotated
                     by one half step each time until the maximum of 12 is reached. Argmax, a function
                     which
                     returns the position of the highest value in a vector, provides the OTI from the list
                     of
                     dot product correlation values, thus returning the number of steps needed to transpose
                     the
                     key of one chroma vector to match another (see Appendix 1.1 for a more detailed formula).
                     Our method differs slightly from Serrà et al: since our vectors are all normalized,
                     we
                     used cosine similarity instead of the related dot product.
                  </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">In order to train a neural network for mode prediction, some previous studies used
                     the
                     mode labels from the Spotify <em class="emph">Web API</em> for whole tracks or for sections of
                     tracks. When we checked these measures against our own separate ground truth set (analyzed
                     by Lupker), we discovered that the automated mode labeling was relatively inaccurate
                     (Table 1). Instead we adapted the less complex method of Finley &amp; Razi [<a class="ref" href="#finley2019">Finley and Razi 2019</a>], which reduced the need for training NNs. They compared chroma
                     vectors to “KK-Profiles” to distinguish mode and other musical elements. Krumhansl
                     and Kessler profiles (<a href="#figure07">Figure 7</a>) come from a study where
                     human subjects were asked to rate how well each of the twelve chromatic notes fit
                     within a
                     key after hearing musical elements such as scales, chords or cadences [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>]. The resulting vector can be normalized to the range between
                     0-1 for direct comparisons to chroma vectors using similarity measures. By incorporating
                     both modified chroma transpositions and KK-profile similarity tests, we were able
                     to label
                     our training data in a novel way.
                  </div>
                  
                  <div id="figure07" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 6. </div>Krumhansl and Kessler profiles for major and minor keys [<a class="ref" href="#krumhansl1982">Krumhansl and Kessler 1982</a>].
                     </div>
                  </div>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">To combine these two approaches, we first rewrite Serrà et al's formula (Appendix
                     1.1) to
                     incorporate Finley and Razi's method by making both KK-profile vectors (for major
                     and
                     minor modes) the new 'desired vectors' by which we will transpose our chroma vector
                     set.
                     This will eventually transpose our entire set of vectors to C major and C minor since
                     the
                     tonic of the KK-profiles is the first value in the vector, or pitch “C”. Correlations
                     between KK-profiles and each of the 12 possible rotations of any given chroma vector
                     are
                     determined using cosine similarity. Instead of using the function which would return
                     the
                     position of the vector rotation that has the highest correlation (argmax), we use
                     a
                     different function which tells us what that correlation value is (amax). Two new lists
                     are
                     created. One is a list of the highest possible correlations for each transposed chroma
                     vector and the major KK-profile, while the other is a list of correlations between
                     each
                     transposed chroma vector and the minor KK-profile. Finally, to determine the mode
                     of each
                     chroma vector, we then simply use a function to determine the position of the higher
                     correlated value between these two lists, position 0 for major and 1 for minor (see
                     Appendixes 1.2.1 &amp; 1.2.2).
                  </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">As noted by Finley &amp; Razi, the most common issue affecting accuracy levels for
                     supervised or unsupervised machine learners attempting to detect the mode or key is
                     “being off by a perfect musical interval of a fifth from the true key,
                     relative mode errors or parallel mode errors”
                     [<a class="ref" href="#finley2019">Finley and Razi 2019</a>]. Unlike papers which followed the MIREX competition rules, our
                     algorithm does not give partial credit to miscalculations no matter how closely related
                     they may be to the true mode or key. Instead we offer methods to reduce these errors.
                     To
                     attempt to correct these issues for mode detection, it is necessary to address the
                     potential differences between a result from music psychology, like the KK-profiles,
                     and
                     the music theoretic concepts that they quantify. As we mentioned earlier, the leading
                     tone
                     in a scale is one of the most important signifiers of mode. In the <em class="emph">Despacito</em>
                     example, where the leading tone is avoided, it is hard to determine major or minor
                     mode.
                     In the (empirically determined) KK-profiles, the leading tone seems to be ranked
                     comparatively low relative to the importance it holds theoretically. If the pitches
                     are
                     ordered from greatest to lowest perceived importance, the leading tone doesn't even
                     register in the top five in either KK-profile. This might be a consequence of the
                     study
                     design, which asked subjects to judge how well each note seemed to fit after hearing
                     other
                     musical elements played.
                  </div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">The distance from the tonic to the leading tone is a major seventh interval (11
                     semitones). Different intervals fall into groups known as consonant or dissonant.
                     Laitz
                     defines consonant intervals as “stable intervals… such as the unison,
                     the third, the fifth (perfect only)” and dissonant intervals as “unstable intervals… [including] the second, the seventh, and all
                     diminished and augmented intervals”
                     [<a class="ref" href="#laitz2003">Laitz 2003</a>]. More dissonant intervals are perceived as having more tension.
                     Rather than separating intervals into categories of consonant and dissonant, Hindemith
                     ranks them on a spectrum, which represents their properties more effectively. He ranks
                     the
                     octave as the “most perfect,” the major seventh as the “least
                     perfect” and all intervals in between as “decreasing in
                     euphony in proportion to their distance from the octave and their proximity to the
                     major
                     seventh”
                     [<a class="ref" href="#hindemith1984">Hindemith 1984</a>]. While determining the best method of interval ranking is
                     irrelevant to this paper, both theorists identify the major seventh as one of the
                     most
                     dissonant intervals. Thus, if the leading tone were to be played by itself (that is,
                     without the context of a chord after a musical sequence) it might sound off, unstable
                     or
                     tense due to the dissonance of a major seventh interval in relation to the tonic.
                     In a
                     song's chord progression or melody, this note will often be given context by its chordal
                     accompaniment or the note might be resolved by subsequent notes. These methods and
                     others
                     will 'handle the dissonance' and make the leading tone sound less out of place. We
                     concluded that the leading tone value found within the empirical KK-profiles should
                     be
                     boosted to reflect its importance in a chord progression in the major or minor mode.
                     Our
                     tests showed that boosting the original major KK-profile's 12th value from 2.88 to
                     3.7 and
                     the original minor KK-profile's 12th value from 3.17 to 4.1 increased the accuracy
                     of the
                     model at determining the correct mode by removing all instances where misclassifications
                     were made between relative major and minor keys.
                  </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">Our training samples include a list of mode determinations labelling our 350,000 chroma
                     vectors. However, the algorithm assumes that every vector is in a major or minor mode
                     with
                     no consideration for OANs. Trying to categorize every vector as either major or minor
                     leads to highly inaccurate results during testing, and seems to be a main cause of
                     miscalculations made by the mode prediction algorithms of Spotify's <em class="emph">Web API</em>
                     and the <em class="emph">Million Song Dataset</em>. To account for other or nontonal scales, we can
                     set a threshold of acceptable correlation values (major and minor modes) and unacceptable
                     values (other or nontonal scales). Our testing showed that a threshold of greater
                     than or
                     equal to 0.9 gave the best accuracy on our ground truth set for determining major
                     or minor
                     modes. These unacceptable vectors contain other or nontonal scales and future research
                     will determine ways of addressing and classifying these further.
                  </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">To address <em class="emph">ambiguous</em> mode determinations between relative major and minor
                     modes, we can set another threshold for removing potentially misleading data for training
                     samples. While observing the correlation values used to determine major or minor labels,
                     we set a further constraint when these values are too close to pick between them
                     confidently. If the absolute difference between the two values is less than or equal
                     to
                     0.02, we determine these correlation values to be indistinguishable and thus likely
                     to
                     reflect an ambiguous mode. As mentioned earlier, this is likely due to the song's
                     chord
                     progression avoiding certain mode determining factors such as the leading tone, and
                     therefore the song can fit almost evenly into either the major or minor
                     classification.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Mode Prediction Results</h1>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">After filtering out samples determined to be OANs, our sample size was reduced to
                     approximately 100,000. From this dataset, 75% of the samples were selected to train
                     the
                     model and an even split of the remaining 25% of samples being withheld for testing
                     and
                     cross-validation to check the model's accuracy and performance. On the withheld test
                     set,
                     our model returned a very high accuracy of 99% during testing. We found that this
                     was much
                     better than the results reported from other studies on withheld test sets. This level
                     of
                     accuracy and the ability to compute the data quickly make it useful for parsing big
                     data
                     for any research that makes comparisons based on modes.
                  </div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">In addition to testing using only samples withheld from our large dataset, we created
                     a
                     separate ground truth set of 100 songs taken from various “top 100”
                     charts from different genres such as pop, country, classical and jazz. This ground
                     truth
                     set was labeled ourselves by looking at the score of each song and comparing it with
                     the
                     exact recording found on Spotify (in order to make sure it wasn't a version recorded
                     in a
                     different key). Our NN reached an accuracy of 88% on the outside ground truth set.
                     The
                     discrepancy is perhaps due to learning from samples that were improperly labelled
                     during
                     cosine similarity measures against KK-profiles. Since this model only outputs major
                     or
                     minor, it is hard to track the exact details of each misclassification. It could be
                     miscalculating a result based on relative major or minors, parallel major or minors
                     (C
                     major vs C Minor) or just be completely off. These incorrectly identified modes will
                     be
                     further addressed in the next section where key is also determined, giving us a clearer
                     understanding of the problem. It is important to note the very low accuracy of 18%
                     in
                     modes determined by Spotify's <em class="emph">Web API</em>. Future research based on the API's
                     mode labelling algorithms should be tested against ground truth datasets before being
                     used
                     to make musicological claims.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Key Prediction</h1>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">With a highly accurate working model for mode detection, adding the ability to predict
                     key becomes fairly straightforward. Our mode detection algorithm is based on the music
                     theory principle of determining mode by first finding the tonic, then calculating
                     the
                     subsequent intervals. Additionally, this method assumes the tonic to be the most frequent
                     note and therefore the tonic should always register as the most prominent note in
                     any
                     given median-averaged chroma vector. Thus when transposing each chroma vector in our
                     ground truth set to C major or C minor, we must keep track of the initial position
                     of the
                     most prominent pitches, as these are our key determining features. This method of
                     analyzing songs based on a non key-specific approach first, and then adding key labels
                     afterwards is derived from the method of 'roman numeral analysis' in musicology. This
                     kind
                     of analysis is used by music theorists to outline triadic chord progressions found
                     within
                     tonal music [<a class="ref" href="#laitz2003">Laitz 2003</a>]. In this method, uppercase numerals are used for
                     major triads and lowercase numerals are used for minor triads (<a href="#figure08">Figure 8</a>). The method itself is not key-specific (besides a brief mention at the
                     beginning), allowing the analysis of underlying chord relationships across multiple
                     songs
                     regardless of key. Considering that machine learning programs typically need large
                     datasets for training, and that it is unlikely even large datasets will contain songs
                     in
                     every possible key in the same proportions, roman numeral analysis is ideal.
                  </div>
                  
                  <div id="figure08" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 7. </div>A roman numeral analysis of major scale triads where uppercase numerals equal major
                        triads and lowercase for minor. The chord built upon the seventh uses a degree sign
                        to
                        denote that it is a diminished chord.
                     </div>
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Key Prediction Results</h1>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">As our key predictions result from a simple labelling of our mode prediction neural
                     network's output, we cannot compute accuracy of training. Past testing did include
                     a
                     second NN where the dataset taken from the mode detection NN was rotated to each of
                     the 12
                     possible keys and then trained on that with 24 outputs for each major and minor key.
                     This
                     training resulted in 93% accuracy but we didn't find any significant increase over
                     the
                     method of applying labels after running the mode NN when testing against the ground
                     truth
                     set, therefore we decided against training a second NN to detect key as it saves training
                     time. With the key labels added to the output of our mode NN, our model returned an
                     accuracy level of 48%. While not nearly as accurate as our mode prediction algorithm,
                     it
                     is much higher than the Spotify <em class="emph">Web API</em> which returns an accuracy of 2% on
                     our ground truth set.
                  </div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">It is also difficult to compare our accuracy on a ground truth set with any paper
                     which
                     used the MIREX competition dataset. MIREX's dataset contains remakes of real songs
                     using
                     Musical Instrument Digital Interface (MIDI) synthesizers which can be recorded straight
                     into a computer, without the need for microphones. Furthermore, it is unclear if this
                     dataset recreates the percussion synthetically for each song or if this is removed
                     to
                     allow easier access to the harmonic content for contestants to test their AI systems.
                     As
                     our training and testing uses actual recorded music and no MIDI representations there
                     is a
                     higher likelihood of noise or percussive elements to throw off our algorithm during
                     testing the outside ground truth set. As of this research, we have not entered the
                     MIREX
                     competition, and thus do not have access to their datasets and cannot compare the
                     performance of our algorithm with those which did. In future research we plan to enter
                     the
                     2020 MIREX key detection competition and to draw more accurate comparisons with other
                     key
                     detection AI models.
                  </div>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">Our much lower accuracy levels for key prediction in comparison to mode prediction
                     are
                     consistent with misclassification noted within previous research. While we were able
                     to
                     almost entirely remove instances of relative major and minor mode misclassifications,
                     the
                     big outstanding problem are keys classified by an interval of a fifth away. We see
                     this as
                     a potential limitation to the ability of supervised or unsupervised learning techniques
                     to
                     predict key (and mode to an almost perfect degree) from pitch content alone. If we
                     imagine
                     this method of determining mode and key from pitch content was performed by a human
                     musician, the equivalent would be that of writing down each pitch in the first section
                     (or
                     the entirety) of a song and making predictions based on a tally. Since this method
                     relies
                     on the tonic being the most prominent pitch in this tally, the model will likely always
                     fail when this is not the case. For example, a song might be in C major with a melody
                     often repeating the pitch 'G' throughout the verse even while a tonic C major chord
                     provides the accompaniment underneath. Since the pitch 'G' is found in the tonic chord
                     of
                     C major (<span class="error"><a href="#figure04">Figure 4</a></span>), it will sound good and be a
                     theoretically correct decision. The repeated “G”, however, will be the most prominent
                     note in the chroma vector section. The scale found a fifth away from a C major scale,
                     G
                     Major, shares all the same notes except one, thus confusing the model. Even with all
                     the
                     processes aimed at removing cases of OANs, a melody focused around the fifth interval
                     of a
                     tonic chord will constantly skew the data and the model will make miscalculations
                     on
                     key.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Conclusions</h1>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">Our neural net model itself is quite simple and does not represent a novel approach
                     to
                     music AI models. Our research was instead focused on data preparation methods grounded
                     in
                     music theory helping to boost the accuracy of existing models by finding more appropriate
                     links between music-related big data and the resultant outputs. Using our methods
                     such as
                     boosting the leading tone's prominence in the KK-profile and filtering out OANs, we
                     were
                     able to construct a model with a higher accuracy for mode prediction during both testing
                     on a withheld subset of our dataset and on the external ground truth set labeled by
                     Lupker. Our key prediction showed comparable results to previous research on a separate
                     ground truth set and we see this as a limit to the ability of predicting key using
                     a
                     pitch-based tally classifier. The model's prediction can be too easily skewed by any
                     song
                     with a melody focused on the fifth interval of the tonic chord, mistaking the key
                     for one
                     a fifth away. Accessing the MIREX dataset would give us a better comparison with those
                     papers which have competed in past competitions, but we predict similar results to
                     those
                     found in this paper.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Further Research</h1>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">Further steps we could take with this research would be to identify and label OANs
                     to
                     create a more universal mode and key prediction NN. The similarity measures specified
                     in
                     this paper could be repeated given chroma vector profiles of other modes and scales
                     to
                     compare against. This would be useful for any research projects experimenting with
                     big
                     data related to non-Western music. Another area for further research would be to apply
                     our
                     music theory based processes to some existing chord retrieval algorithms. Our testing
                     leads us to believe that training NNs on chord transition networks as related to mode
                     or
                     key is the only way to reach accuracy levels comparable to human predictions. When
                     musicians are faced with determining mode or key of a song with less obvious features,
                     the
                     last resort method is to look at the chord progression and make a decision based on
                     that.
                     Our predictions are that a learner trained to look for mode or key determining features
                     of
                     a chord progression will outperform those based on averaging tally counts of pitch.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix 1.</h1>
                  
                  <div id="figure09" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 8. </div>Formula to find the Optimal Transposition Index (OTI)
                     </div>
                  </div>
                  
                  <div id="figure10" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 9. </div>Modified formula to transpose chroma vectors separated by major and minor
                        modes
                     </div>
                  </div>
                  
                  <div id="figure11" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure11.png" rel="external"><img src="resources/images/figure11.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 10. </div>Training neural network
                     </div>
                  </div>
                  
                  <div id="figure12" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure12.png" rel="external"><img src="resources/images/figure12.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 11. </div>Computational workflow regarding the collection of data and the subsequent
                        preprocessing steps required to create training samples and labels which can be fed
                        into
                        the neural network.
                     </div>
                  </div>
                  
               </div>
               
               
               
               
               
               
               
            </div>
            
            
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e202"><span class="noteRef lang en">[1] Our use of the term 'big data'
                     refers to datasets that are so large that conventional computers may have difficulty
                     processing them. Researchers are now able to access additional computational power
                     in
                     the form of cloud resources such as GPUs, as we have done here. Our experiments were
                     run
                     using Google Colaboratory.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="bertin2011">
                     <!-- close -->Bertin et al. 2011</span> Bertin-Mahieux, T., D. Ellis, B.
                  Whitman, and P. Lamere, “The Million Song Dataset”
                  <cite class="title italic">Proceedings of the 12th International Conference on Music
                     Information</cite>, Miami, USA, October 2011.
               </div>
               <div class="bibl"><span class="ref" id="clement1699">
                     <!-- close -->Clement 1699</span> Clement, T., “Distant
                  Listening or Playing Visualisations Pleasantly with the Eyes and Ears.”
                  <cite class="title italic">Digital Studies/le Champ Numérique</cite>, 3.2. DOI: <a href="http://doi.org/10.16995/dscn.236" onclick="window.open('http://doi.org/10.16995/dscn.236'); return false" class="ref">http://doi.org/10.16995/dscn.236</a>
                  (2013).
               </div>
               <div class="bibl"><span class="ref" id="finley2019">
                     <!-- close -->Finley and Razi 2019</span> Finley, M. and Razi, A., “Musical Key Estimation with Unsupervised Pattern Recognition” IEEE
                  9th Annual Computing and Communication Workshop and Conference, Las Vegas, USA, January
                  2019.
               </div>
               <div class="bibl"><span class="ref" id="fonsi2017">
                     <!-- close -->Fonsi and Yankee 2017</span> Fonsi, Luis and Daddy Yankee, “Despacito” Vida. Universal Latin, Los Angeles, 2017. <a href="http://doi.org/10.16995/dscn.236" onclick="window.open('http://doi.org/10.16995/dscn.236'); return false" class="ref">Online</a>.
               </div>
               <div class="bibl"><span class="ref" id="gomez2004">
                     <!-- close -->Gomez and Herrera 2004</span> Gomez, E and P. Herrera, “Estimating the Tonality of Polyphonic Audio Files: Cognitive Versus
                  Machine Learning Modelling Strategies”
                  <cite class="title italic">Proceedings of the 5th International Society for Music Information
                     Retrieval Conference</cite>, Barcelona, Spain, October 2004.
               </div>
               <div class="bibl"><span class="ref" id="hindemith1984">
                     <!-- close -->Hindemith 1984</span> Hindemith, P., <cite class="title italic">The
                     Craft of Musical Composition: Theoretical Part - Book 1</cite>. Schott, Mainz,
                  1984.
               </div>
               <div class="bibl"><span class="ref" id="huang2019">
                     <!-- close -->Huang et al. 2019</span> Huang, C. A., C. Hawthorne, A. Roberts,
                  M. Dinculescu, J. Wexler, L. Hong, J. Howcroft, “The Bach Doodle:
                  Approachable Music Composition with Machine Learning at Scale”
                  <cite class="title italic">Proceedings of the 18th International Society for Music Information
                     Retrieval Conference, ISMIR 2019</cite>. <a href="https://arxiv.org/abs/1907.06637" onclick="window.open('https://arxiv.org/abs/1907.06637'); return false" class="ref">arXiv:1907.06637</a> (2019).
               </div>
               <div class="bibl"><span class="ref" id="i̇zmirli2006">
                     <!-- close -->İzmirli 2006</span> İzmirli, O., “Audio
                  Key Finding Using Low Dimensional Spaces”
                  <cite class="title italic">Proceedings from the 7th International Conference on Music
                     Information Retrieval</cite>, Victoria, Canada, October 2006.
               </div>
               <div class="bibl"><span class="ref" id="jehan2014">
                     <!-- close -->Jehan 2014</span> Jehan, T., “Analyzer
                  Documentation: The EchoNest.”Somerville: The Echo Nest Corporation, 2014,
                  5.
               </div>
               <div class="bibl"><span class="ref" id="krumhansl1982">
                     <!-- close -->Krumhansl and Kessler 1982</span> Krumhansl, C. L., and E. J.
                  Kessler, “Tracing the Dynamic Changes in Perceived Tonal Organization
                  in a Spatial Representation of Musical Keys”
                  <cite class="title italic">Psychological Review</cite>, 89.4 (1982), 334-368.
               </div>
               <div class="bibl"><span class="ref" id="laitz2003">
                     <!-- close -->Laitz 2003</span> Laitz, S. G., <cite class="title italic">The Complete
                     Musician: An Integrated Approach to Tonal Theory, Analysis, and Listening</cite>.
                  Oxford University Press, New York, 2003.
               </div>
               <div class="bibl"><span class="ref" id="mahieu2017">
                     <!-- close -->Mahieu 2017</span> Mahieu, R., “Detecting
                  Musical Key with Supervised Learning” unpublished manuscript, 2017.
               </div>
               <div class="bibl"><span class="ref" id="pauws2004">
                     <!-- close -->Pauws 2004</span> Pauws, S., “Musical Key
                  Extraction from Audio”
                  <cite class="title italic">Proceedings of the 5th International Society for Music Information
                     Retrieval Conference</cite>, Barcelona, Spain, October 2004.
               </div>
               <div class="bibl"><span class="ref" id="serrà2008">
                     <!-- close -->Serrà and Gómez 2008</span> Serrà, J., E. Gómez &amp; P. Herrera.
                  “Transposing Chroma Representations to a Common Key.”IEEE CS
                  Conference on The Use of Symbols to Represent Music and Multimedia Objects (2008).
               </div>
               <div class="bibl"><span class="ref" id="serrà2012a">
                     <!-- close -->Serrà et al. 2012a</span> Serrà, J. , Á. Corral, M. Boguñá, M.
                  Haro &amp; J. Ll. Arcos. “Measuring the Evolution of Contemporary
                  Western Popular Music”
                  <cite class="title italic">Scientific Reports</cite>, 521.2 (2012).
               </div>
               <div class="bibl"><span class="ref" id="serrà2012b">
                     <!-- close -->Serrà et al. 2012b</span> Serrà, J., Á. Corral, M. Boguñá, M.
                  Haro &amp; J. Ll. Arcos. “Supplementary Information: Measuring the
                  Evolution of Contemporary Western Popular Music”
                  <cite class="title italic">Scientific Reports</cite> 521.2 (2012).
               </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>