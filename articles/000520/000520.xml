<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">

	<!-- BEGIN TEI HEADER ELEMENTS -->
	<teiHeader>
		<fileDesc>
			<titleStmt>
				<title type="article" xml:lang="en">Music Theory, the Missing Link Between
					Music-Related Big Data and Artificial Intelligence</title>
				<dhq:authorInfo>
					<dhq:author_name>Jeffrey A. T.
						<dhq:family>Lupker</dhq:family></dhq:author_name>
					<dhq:affiliation>The University of Western Ontario</dhq:affiliation>
					<email>jlupker@uwo.ca</email>
					<dhq:bio>
						<p>Jeffrey A. T. Lupker is a PhD candidate at the University of
							Western Ontario in the Don Wright Faculty of Music. His current
							research involves the application of deep learning approaches
							into music composition and artificially modelling aspects of
							human creativity.</p>
					</dhq:bio>
				</dhq:authorInfo>
				<dhq:authorInfo>
					<dhq:author_name>William J.
						<dhq:family>Turkel</dhq:family></dhq:author_name>
					<dhq:affiliation>The University of Western Ontario</dhq:affiliation>
					<email>wturkel@uwo.ca</email>
					<dhq:bio>
						<p>William J. Turkel is Professor of History at The University of
							Western Ontario. His research involves computational history, big
							history, and science and technology studies, with a focus on
							methods. He co-founded <title rend="italic">The Programming
								Historian</title> and is the author of <title rend="italic"
								>Digital Research Methods with Mathematica</title>, 2nd rev
							ed (2020).</p>
					</dhq:bio>
				</dhq:authorInfo>
			</titleStmt>
			<publicationStmt>
				<publisher>Alliance of Digital Humanities Organizations</publisher>
				<publisher>Association for Computers and the Humanities</publisher>
				<idno type="DHQarticle-id">000520</idno>
				<idno type="volume">015</idno>
				<idno type="issue">1</idno>
				<date when="2021-03-05">05 March 2021</date>
				<dhq:articleType>article</dhq:articleType>
				<availability status="CC-BY-ND">
					<cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
				</availability>
			</publicationStmt>
			<sourceDesc>
				<p>This is the source</p>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<classDecl>
				<taxonomy xml:id="dhq_keywords">
					<bibl>DHQ classification scheme; full list available at <ref
							target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
							>http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
				</taxonomy>
				<taxonomy xml:id="authorial_keywords">
					<bibl>Keywords supplied by author; no controlled vocabulary</bibl>
				</taxonomy>
			</classDecl>
		</encodingDesc>
		<profileDesc>
			<langUsage>
				<language ident="en" extent="original"/>
			</langUsage>
			<textClass>
				<keywords scheme="#dhq_keywords">
					<list type="simple">
						<item/>
					</list>
				</keywords>
				<keywords scheme="#authorial_keywords">
					<list type="simple">
						<item/>
					</list>
				</keywords>
			</textClass>
		</profileDesc>
		<revisionDesc>
			<change when="2020-09-18" who="Taylor Arnold">Created file</change>
			<change>The version history for this file can be found on <ref target=
			"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000520/000520.xml">GitHub
		</ref></change>
		</revisionDesc>
	
	</teiHeader>
	<!-- END TEI HEADER ELEMENTS -->

	<!-- BEGIN TEXT -->
	<text xml:lang="en" type="original">
		<!-- FRONT TEXT -->
		<front>
			<dhq:abstract>
				<p>This paper examines musical artificial intelligence (AI) algorithms that can
					not only learn from big data, but learn in ways that would be familiar to a
					musician or music theorist. This paper aims to find more effective links
					between music-related big data and artificial intelligence algorithms by
					incorporating principles with a strong grounding in music theory. We show
					that it is possible to increase the accuracy of two common algorithms (mode
					prediction and key prediction) by using music-theory based techniques
					during the data preparation process. We offer methods to alter often-used
					Krumhansl Kessler profiles <ptr target="#krumhansl1982"/>, and the manner
					in which they are employed during preprocessing, to aid the connection of
					musical big data and mode or key predicting algorithms.</p>
			</dhq:abstract>
			<dhq:teaser>
				<p>This paper examines musical artificial intelligence (AI) algorithms that can
					not only learn from big data, but learn in ways that would be familiar to a
					musician or music theorist.</p>
			</dhq:teaser>
		</front>

		<!-- BODY TEXT -->
		<body>
			<div>
				<head>Introduction</head>
				<p>Research in music information retrieval has produced many possibilities for
					developing artificial intelligence (AI) algorithms that can perform a wide
					variety of musically-based tasks, including even music composition itself.
					The availability of vast musical datasets like the <emph>Million Song
						Dataset</emph>
					<ptr target="#bertin2011"/> and Spotify's <emph>Web API</emph> has made it
					possible for researchers to acquire algorithmically-determined
					characteristics of a song's key, mode, pitch content, and more. At the same
					time, the existence of these large datasets has made it possible for
					researchers to take a 'big data'<note>Our use of the term 'big data' refers
						to datasets that are so large that conventional computers may have
						difficulty processing them. Researchers are now able to access
						additional computational power in the form of cloud resources such as
						GPUs, as we have done here. Our experiments were run using Google
						Colaboratory.</note> approach to various styles of Western music. One
					notable example is the work by Serrà et al <ptr target="#serrà2012b"/>
					which showed the changes and trends related to pitch transitions, the
					homogenization of the timbral palette and growing loudness levels that have
					shaped pop music over the past 60 years. The authors went on to suggest
					that past songs might be modernized into new hits by restricting pitch
					transitions, reducing timbral variety and making them louder. Tanya Clement
					further suggests how studying musical big data lends itself quite well to
					music related tasks, especially music composition, since the <quote
						rend="inline">notion of scholars <q>reading</q> visualizations
						[(complete representation of the data)] relates to composers or
						musicians who read scores … [as] the musical score is an attempt to
						represent complex relationships … across time and space</quote>
					<ptr target="#clement2013"/>.</p>
				<p>Big data can also be used to create labelled instances for training
					supervised learners like neural nets (which tend to be
						<soCalled>data-thirsty</soCalled>) and can be easily parsed by
					unsupervised learners to find patterns. This more recent ability to train
					music-related AI programs has largely been directed towards autonomously
					generating music in the same vein as whatever genre, style or composer that
					particular program has been trained on. A good example of this is <title
						rend="quotes">The Bach Doodle,</title> which can supply a melody with
					appropriate counter-melodies, accompaniment and chord changes in the style
					of Johann Sebastian Bach <ptr target="#huang2019"/>.</p>
				<p>While the availability of approaches such as Spotify's has allowed for the
					development of AI algorithms in music, many previous research projects have
					struggled to find felicitous links between this music-related big data and
					music itself. In the past, a common method involving the use of
					Krumhansl-Kessler profiles <ptr target="#krumhansl1982"/>, vectors of pitch
					correlation to the major or minor modes, allowed for mode or key
					predictability in some limited capacity. While it showed promise when
					applied to music of specific genres, it suffered when applied to a wider
					scope of genres or styles. We offer methods to alter KK-profiles, and the
					manner in which they are employed during preprocessing, to aid autonomous
					mode and key predictors ability and accuracy without being genre-specific.
					Without the ability to connect the intermediate dots, the overall accuracy
					of these algorithms diminishes and their output suffers accordingly.
					Indeed, AI-based autonomous generation is rarely up to the standards of
					composers or musicians creating new music. This paper offers preliminary
					solutions to two existing problems for which AI is typically used, mode and
					key prediction. We show that by equipping our algorithms with more
					background in music theory we can significantly improve their accuracy. The
					more the program learns about the music theoretic principles of mode and
					key, the better it gets. Our more general argument is that one way to help
					bridge the gap between music-related big data and AI is to give algorithms
					a strong grounding in music theory.</p>
			</div>
			<div>
				<head>Mode Prediction</head>
				<p>For the purpose of this paper, we looked at only the two most common modes in
					Western Music, the major and minor modes. These are also the only modes
					analyzed by <emph>The Million Song Dataset</emph> and Spotify's <emph>Web
						API</emph>. The major and minor modes are a part of the <title
						rend="quotes">Diatonic Collection,</title> which refers to <quote
						rend="inline">any scale [or mode] where the octave is divided evenly
						into seven steps</quote>
					<ptr target="#laitz2003"/>. A step can be either a whole or half step
					(whole tone or semitone) and the way that these are arranged in order to
					divide the octave will determine if the mode is major or minor. A major
					scale consists of the pattern <hi rend="bold">W-W-H-W-W-W-H</hi> and the
					minor scale consists of <hi rend="bold">W-H-W-W-H-W-W</hi>
					<ptr target="#laitz2003"/>. <ref target="#figure01">Figure 1</ref> shows a
					major scale starting on the pitch <q>C</q> and <ref target="#figure02"
						>Figure 2</ref> shows two types of minor scales starting on <q>C</q>.
					The seventh <soCalled>step</soCalled> in the harmonic minor scale example
					is raised in order to create a <quote rend="inline">leading tone.</quote>
					The leading tone occurs when the seventh scale degree is a half step away
					from the first scale degree, also called the <quote rend="inline"
						>tonic.</quote> This leading tone to tonic relationship will become an
					important music theory principle that we use to train our AIs more
					accurately than previous published attempts.</p>
				<figure xml:id="figure01">
					<head>C major scale demonstrating its make-up of whole and half
						tones.</head>
					<graphic url="resources/images/figure01.png"/>
				</figure>
				<figure xml:id="figure02">
					<head>Natural and harmonic minor scales starting on <soCalled>C</soCalled>.
						This also shows the progression of a natural minor scale to a harmonic
						scale by raising the seventh interval to become a leading tone.</head>
					<graphic url="resources/images/figure02.png"/>
				</figure>
				<p>Many previous papers which use supervised learning to determine mode or key
					test only against songs from specific genres or styles, and few make
					attempts at predicting mode regardless of genre. Even the often-cited
					yearly competition on musical key detection hosted by Music Information
					Retrieval Evaluation eXchange (MIREX) has participants' algorithms compete
					at classifying 1252 classical music pieces <ptr target="#i̇zmirli2006"/>
					<ptr target="#pauws2004"/>. However, if we look again at <ref
						target="#figure01">Figure 1</ref> and <ref target="#figure02">Figure
						2</ref>, we can see that mode is not exclusive to genre or style, it
					is simply a specific arrangement of whole and half steps. So for a
					supervised learner programmed to <soCalled>think</soCalled> like a musician
					and thus determine mode based on its understanding of these music theory
					principles, genre or style should not affect the outcome. While this might
					work in a perfect world, artists have always looked for ways to
						<soCalled>break away</soCalled> from the norm and this can indeed
					manifest itself more in certain genres than others. Taking this into
					consideration, in this research we only selected songs for our separate
					ground truth set involving various genres which obey exact specifications
					for what constitutes as major or minor. This ground truth set will be a
					separate list of 100 songs labeled by us to further check the accuracy of
					our AI algorithms during testing. We wish to discuss shortcomings in the
					accuracy of past research that uses AI algorithms for predicting major or
					minor mode rather than to suggest a universal method for identifying all
					modes and scales.</p>
				<p>This is one aspect where our research differs from previous papers. An AI
					system which incorporates a solid understanding of the rules of music
					theory pertaining to mode should be able to outperform others that do not
					incorporate such understanding or those that focus on specific genres.
					While certain genres or styles may increase the difficulty of
					algorithmically determining mode, the same is true for a human musical
					analyst. When successful, an AI algorithm for determining mode will process
					data much faster than a musician, who would either have to look through the
					score or figure it out by ear in order to make their decision. For parsing
					music-related big data quickly and accurately, speed is imperative. Thus we
					suggest the following framework (<ref target="#figure03">Figure 3</ref>) by
					which a supervised learner can be trained to make predictions exclusively
					from pitch data in order to determine the mode of a song. The process is
					akin to methods used by human musical analysts. Below we also outline other
					areas where we apply a more musician-like approach to our methods to
					achieve greater accuracy.</p>
				<figure xml:id="figure03">
					<head>Framework for supervised learning mode prediction algorithm.</head>
					<graphic url="resources/images/figure03.png"/>
				</figure>
				<p>As can be seen in <ref target="#figure03">Figure 3</ref>, any scale or mode
					that does not meet the exact specifications of major or minor we categorize
					as <emph>other</emph>, <emph>ambiguous</emph> or <emph>nontonal</emph>
					(OANs). The primary reason that past research has trained supervised
					learners on only one specific genre or style is to avoid OANs. When OANs
					are not segregated from major or minor modes, they are fit improperly,
					leading to misclassifications.</p>
				<p><emph>Other</emph> pertains to any scale separate from major or minor that
					still contains a tonal center. Some examples of this are: modes (except
					Ionian or Aeolian), whole tone scale, pentatonic scale, and non-Western
					scales. <emph>Nontonal</emph> refers to any song which does not center
					around a given pitch. A common occurrence of this can be found in various
					examples of <quote rend="inline">12-tone music,</quote> where each pitch is
					given equal importance in the song and thus no tonic can be derived from
					it.</p>
				<p>Where our paper differs from previous work is the handling of songs related
					to the outcome <emph>ambiguous</emph>. This occurs when either major or
					minor can be seen as an equally correct determination from the given
					pitches in a song. This most often occurs when chords containing the
					leading tone are avoided (<ref target="#figure04">Figure 4</ref>) and thus
					the remaining chords are consistent with both the major key and its
					relative minor (<ref target="#figure04">Figure 4</ref> &amp; <ref
						target="#figure05">Figure 5</ref>). The leading tone is a
						<soCalled>tendency tone</soCalled> or a tone that pulls towards a
					given target, in this case the tonic. This specific pull is one that can
					often signify a given mode and is therefore avoided in songs that the
					composer wished to float between the two modes. This can also be
					accomplished by simply using the relative minor's natural minor scale.
					Since the natural minor scale does not raise any pitches, it actually has
					the exact same notes (and resultant triads) as its relative major scale.
						<ref target="#figure06">Figure 6</ref> gives an example of a
					well-known pop song <emph>Despacito</emph>
					<ptr target="#fonsi2017"/> and given these rules, what mode can we say the
					song is in? This tough choice is a common occurrence, especially in modern
					pop music, which can explain why papers that focused heavily on dance music
					might have had accuracy issues.</p>
				<p>Other authors have noted that their AI algorithms can mistake the major scale
					for its relative natural minor scale during prediction and it is likely
					that their algorithms did not account for the raised leading tone to truly
					distinguish major from minor. Since we focused on achieving higher accuracy
					than existing major vs minor mode prediction AI algorithms by incorporating
					music theory principles, we removed any instances of songs with an
					ambiguous mode from our ground truth set in order to get a clearer picture
					of how our system compares with the existing models. Adding other mode
					outcomes in order to detect OANs algorithmically is a part of our ongoing
					and future research.</p>
				<figure xml:id="figure04">
					<head>Triads built from a major scale contain only pitches found in that 
						scale. The relative minor of a major scale begins on the sixth 
						interval. Chords containing the leading tone (‘B’) have been 
						highlighted in red.</head>
					<graphic url="resources/images/figure04.png"></graphic>
				</figure>
				<figure xml:id="figure05">
					<head>Shows the relative minor scale from the C major scale in <ref target="#figure04">Figure 4</ref>. As
						seen in <ref target="#figure02">Figure 2</ref>, the leading tone (<quote rend="inline">G#</quote>)
						must be added in order to make the obvious distinction from a major
						scale to a minor scale.</head>
					<graphic url="resources/images/figure05.png"/>
				</figure>
				<figure xml:id="figure06">
					<head>Chord progression for the hit song from Fonsi and Yankee’s Despacito
							<ptr target="#fonsi2017"/>. What mode is it in?</head>
					<graphic url="resources/images/figure06.png"/>
				</figure>
				<p>The most popular method of turning pitch data into something that can be used
					to train machine learners comes in the form of <soCalled>chroma
						features.</soCalled> Chroma feature data is available for every song
					found in Spotify's giant database of songs through the use of their
						<emph>Web API</emph>. Chroma features are vectors containing 12 values
					(coded as real numbers between 0 and 1) reflecting the relative dominance
					of all 12 pitches over the course of a small segment, usually lasting no
					longer than a second <ptr target="#jehan2014"/>. Each vector begins on the
					pitch <q>C</q> and continues in chromatic order (C#, D, D#, etc.) until all
					12 pitches are reached. In order to create an AI model that could make
					predictions on a wide variety of musical styles, we collected the chroma
					features for approximately 100,000 songs over the last 40 years. Spotify's
						<emph>Web API</emph> offers its data at different temporal
					resolutions, from the aforementioned short segments through sections of the
					work to the track as a whole.</p>
				<p>Beyond chroma features, the API offers Spotify's own algorithmic analysis of
					musical features such as mode within these temporal units, and provides a
					corresponding level of confidence for each (coded as a real number between
					0 and 1). We used Spotify's mode confidence levels to find every section
					within our 100,000 song list which had a mode confidence level of 0.6 or
					higher. The API's documentation states that <quote rend="inline"
							><emph>confidence</emph> indicates the reliability of its
						corresponding attribute... elements carrying a small confidence value
						should be considered speculative... there may not be sufficient data
						in the audio to compute the element with high certainty</quote>
					<ptr target="#jehan2014"/> thus giving good reason to remove sections with
					lower confidence levels from the dataset. Previous work such as Serrà et al
						<ptr target="#serrà2012b"/>, Finley and Razi <ptr target="#finley2019"
					/> and Mahieu <ptr target="#mahieu2017"/> also used confidence thresholds,
					but at the temporal resolution of a whole track rather than the sections
					that we used. By analyzing at the level of sections, we were able to triple
					our training samples from 100,000 songs to approximately 350,000 sections.
					Not only did this method increase the number of potential training samples,
					but it allowed us to focus on specific areas of each song that were more
					likely to provide an accurate representation of each mode as they appeared.
					For example, a classical piece of music in <q>sonata form</q> will undergo
					a <soCalled>development</soCalled> section whereby it passes through
					contrasting keys and modes to build tension before its final resolve to the
					home key, mode and initial material. Pop music employs a similar tactic
					with <soCalled>the bridge,</soCalled> a section found after the midway
					point of a song to add contrast to the musical material found up until this
					point. Both of these contrasting sections might add confusion during the
					training process if the song is analyzed as a whole, but removing them or
					analyzing them separately gives the program more accurate training samples.
					The ability to gain more training samples from the original list of songs
					has the advantage of providing more data for training a supervised
					learner.</p>
				<p>In previous work, a central tendency vector was created by taking the mean of
					each of the 12 components of the chroma vectors for a whole track, and this
					was then labelled as either major or minor for training. In an effort to
					mitigate the effects of noise on our averaged vector in any given
					recording, we found that using the medians rather than means gave us a
					better representation of the actual pitch content unaffected by potential
					outliers in the data. One common example is due to percussive instruments,
					such as a drum kit's cymbals, that happen to emphasize pitches that are
						<soCalled>undesirable</soCalled> for determining the song's particular
					key or mode. If that cymbal hit only occurs every few bars of music, but
					the <soCalled>desirable</soCalled> pitches occur much more often, we can
					lessen the effect that cymbal hit will have on our outcome by using a
					robust estimator. A musician working with a score or by ear would also
					filter out any unwanted sounds that did not help in mode determination. We
					found the medians of every segment's chroma feature vector found within
					each of our 350,000 sections.</p>
				<p>The last step in the preparation process is to transpose each chroma vector
					such that they are all aligned into the same key. As our neural network
					(NN) will only output predictions of major or minor, we want to have the
					exact same tonal center for each chroma vector to easily compare between
					their whole and half step patterns (<ref target="#figure03">Figure
					3</ref>). We based our transposition method on the one described by Serrà
					et al <ptr target="#serrà2012b"/> and also used in their 2012 work. This
					method determines an <soCalled>optimal transposition index</soCalled> (OTI)
					by creating a new vector of the dot products between a chroma vector
					reflecting the key they wish to transpose to and the twelve possible
					rotations (i.e., 12 possible keys) of a second chroma vector. Using a right
					circular shift operation, the chroma vector is rotated by one half step
					each time until the maximum of 12 is reached. Argmax, a function which
					returns the position of the highest value in a vector, provides the OTI
					from the list of dot product correlation values, thus returning the number
					of steps needed to transpose the key of one chroma vector to match another
					(see Appendix 1.1 for a more detailed formula). Our method differs slightly
					from Serrà et al: since our vectors are all normalized, we used cosine
					similarity instead of the related dot product.</p>
				<p>In order to train a neural network for mode prediction, some previous studies
					used the mode labels from the Spotify <emph>Web API</emph> for whole tracks
					or for sections of tracks. When we checked these measures against our own
					separate ground truth set (analyzed by Lupker), we discovered that the
					automated mode labeling was relatively inaccurate (Table 1). Instead we
					adapted the less complex method of Finley &amp; Razi <ptr
						target="#finley2019"/>, which reduced the need for training NNs. They
					compared chroma vectors to <q>KK-Profiles</q> to distinguish mode and other
					musical elements. Krumhansl and Kessler profiles (<ref target="#figure07"
						>Figure 7</ref>) come from a study where human subjects were asked to
					rate how well each of the twelve chromatic notes fit within a key after
					hearing musical elements such as scales, chords or cadences <ptr
						target="#krumhansl1982"/>. The resulting vector can be normalized to
					the range between 0-1 for direct comparisons to chroma vectors using
					similarity measures. By incorporating both modified chroma transpositions
					and KK-profile similarity tests, we were able to label our training data in
					a novel way.</p>
				<figure xml:id="figure07">
					<head>Krumhansl and Kessler profiles for major and minor keys <ptr
							target="#krumhansl1982"/>.</head>
					<graphic url="resources/images/figure07.png"/>
				</figure>
				<p>To combine these two approaches, we first rewrite Serrà et al's formula
					(Appendix 1.1) to incorporate Finley and Razi's method by making both
					KK-profile vectors (for major and minor modes) the new 'desired vectors' by
					which we will transpose our chroma vector set. This will eventually
					transpose our entire set of vectors to C major and C minor since the tonic
					of the KK-profiles is the first value in the vector, or pitch <q>C</q>.
					Correlations between KK-profiles and each of the 12 possible rotations of
					any given chroma vector are determined using cosine similarity. Instead of
					using the function which would return the position of the vector rotation
					that has the highest correlation (argmax), we use a different function
					which tells us what that correlation value is (amax). Two new lists are
					created. One is a list of the highest possible correlations for each
					transposed chroma vector and the major KK-profile, while the other is a
					list of correlations between each transposed chroma vector and the minor
					KK-profile. Finally, to determine the mode of each chroma vector, we then
					simply use a function to determine the position of the higher correlated
					value between these two lists, position 0 for major and 1 for minor (see
					Appendixes 1.2.1 &amp; 1.2.2).</p>
				<p>As noted by Finley &amp; Razi, the most common issue affecting accuracy
					levels for supervised or unsupervised machine learners attempting to detect
					the mode or key is <quote rend="inline">being off by a perfect musical
						interval of a fifth from the true key, relative mode errors or
						parallel mode errors</quote>
					<ptr target="#finley2019"/>. Unlike papers which followed the MIREX
					competition rules, our algorithm does not give partial credit to
					miscalculations no matter how closely related they may be to the true mode
					or key. Instead we offer methods to reduce these errors. To attempt to
					correct these issues for mode detection, it is necessary to address the
					potential differences between a result from music psychology, like the
					KK-profiles, and the music theoretic concepts that they quantify. As we
					mentioned earlier, the leading tone in a scale is one of the most important
					signifiers of mode. In the <emph>Despacito</emph> example, where the
					leading tone is avoided, it is hard to determine major or minor mode. In
					the (empirically determined) KK-profiles, the leading tone seems to be
					ranked comparatively low relative to the importance it holds theoretically.
					If the pitches are ordered from greatest to lowest perceived importance,
					the leading tone doesn't even register in the top five in either
					KK-profile. This might be a consequence of the study design, which asked
					subjects to judge how well each note seemed to fit after hearing other
					musical elements played.</p>
				<p>The distance from the tonic to the leading tone is a major seventh interval
					(11 semitones). Different intervals fall into groups known as consonant or
					dissonant. Laitz defines consonant intervals as <quote rend="inline">stable
						intervals… such as the unison, the third, the fifth (perfect
						only)</quote> and dissonant intervals as <quote rend="inline">unstable
						intervals… [including] the second, the seventh, and all diminished and
						augmented intervals</quote>
					<ptr target="#laitz2003"/>. More dissonant intervals are perceived as
					having more tension. Rather than separating intervals into categories of
					consonant and dissonant, Hindemith ranks them on a spectrum, which
					represents their properties more effectively. He ranks the octave as the
						<soCalled>most perfect,</soCalled> the major seventh as the
						<soCalled>least perfect</soCalled> and all intervals in between as
						<quote rend="inline">decreasing in euphony in proportion to their
						distance from the octave and their proximity to the major
						seventh</quote>
					<ptr target="#hindemith1984"/>. While determining the best method of
					interval ranking is irrelevant to this paper, both theorists identify the
					major seventh as one of the most dissonant intervals. Thus, if the leading
					tone were to be played by itself (that is, without the context of a chord
					after a musical sequence) it might sound off, unstable or tense due to the
					dissonance of a major seventh interval in relation to the tonic. In a
					song's chord progression or melody, this note will often be given context
					by its chordal accompaniment or the note might be resolved by subsequent
					notes. These methods and others will 'handle the dissonance' and make the
					leading tone sound less out of place. We concluded that the leading tone
					value found within the empirical KK-profiles should be boosted to reflect
					its importance in a chord progression in the major or minor mode. Our tests
					showed that boosting the original major KK-profile's 12th value from 2.88
					to 3.7 and the original minor KK-profile's 12th value from 3.17 to 4.1
					increased the accuracy of the model at determining the correct mode by
					removing all instances where misclassifications were made between relative
					major and minor keys.</p>
				<p>Our training samples include a list of mode determinations labelling our
					350,000 chroma vectors. However, the algorithm assumes that every vector is
					in a major or minor mode with no consideration for OANs. Trying to
					categorize every vector as either major or minor leads to highly inaccurate
					results during testing, and seems to be a main cause of miscalculations
					made by the mode prediction algorithms of Spotify's <emph>Web API</emph>
					and the <emph>Million Song Dataset</emph>. To account for other or nontonal
					scales, we can set a threshold of acceptable correlation values (major and
					minor modes) and unacceptable values (other or nontonal scales). Our
					testing showed that a threshold of greater than or equal to 0.9 gave the
					best accuracy on our ground truth set for determining major or minor modes.
					These unacceptable vectors contain other or nontonal scales and future
					research will determine ways of addressing and classifying these
					further.</p>
				<p>To address <emph>ambiguous</emph> mode determinations between relative major
					and minor modes, we can set another threshold for removing potentially
					misleading data for training samples. While observing the correlation
					values used to determine major or minor labels, we set a further constraint
					when these values are too close to pick between them confidently. If the
					absolute difference between the two values is less than or equal to 0.02,
					we determine these correlation values to be indistinguishable and thus
					likely to reflect an ambiguous mode. As mentioned earlier, this is likely
					due to the song's chord progression avoiding certain mode determining
					factors such as the leading tone, and therefore the song can fit almost
					evenly into either the major or minor classification.</p>
			</div>
			<div>
				<head>Mode Prediction Results</head>
				<p>After filtering out samples determined to be OANs, our sample size was
					reduced to approximately 100,000. From this dataset, 75% of the samples
					were selected to train the model and an even split of the remaining 25% of
					samples being withheld for testing and cross-validation to check the
					model's accuracy and performance. On the withheld test set, our model
					returned a very high accuracy of 99% during testing. We found that this was
					much better than the results reported from other studies on withheld test
					sets. This level of accuracy and the ability to compute the data quickly
					make it useful for parsing big data for any research that makes comparisons
					based on modes.</p>
				<p>In addition to testing using only samples withheld from our large dataset, we
					created a separate ground truth set of 100 songs taken from various
						<soCalled>top 100</soCalled> charts from different genres such as pop,
					country, classical and jazz. This ground truth set was labeled ourselves by
					looking at the score of each song and comparing it with the exact recording
					found on Spotify (in order to make sure it wasn't a version recorded in a
					different key). Our NN reached an accuracy of 88% on the outside ground
					truth set. The discrepancy is perhaps due to learning from samples that
					were improperly labelled during cosine similarity measures against
					KK-profiles. Since this model only outputs major or minor, it is hard to
					track the exact details of each misclassification. It could be
					miscalculating a result based on relative major or minors, parallel major
					or minors (C major vs C Minor) or just be completely off. These incorrectly
					identified modes will be further addressed in the next section where key is
					also determined, giving us a clearer understanding of the problem. It is
					important to note the very low accuracy of 18% in modes determined by
					Spotify's <emph>Web API</emph>. Future research based on the API's mode
					labelling algorithms should be tested against ground truth datasets before
					being used to make musicological claims.</p>
			</div>
			<div>
				<head>Key Prediction</head>
				<p>With a highly accurate working model for mode detection, adding the ability
					to predict key becomes fairly straightforward. Our mode detection algorithm
					is based on the music theory principle of determining mode by first finding
					the tonic, then calculating the subsequent intervals. Additionally, this
					method assumes the tonic to be the most frequent note and therefore the
					tonic should always register as the most prominent note in any given
					median-averaged chroma vector. Thus when transposing each chroma vector in
					our ground truth set to C major or C minor, we must keep track of the
					initial position of the most prominent pitches, as these are our key
					determining features. This method of analyzing songs based on a non
					key-specific approach first, and then adding key labels afterwards is
					derived from the method of 'roman numeral analysis' in musicology. This
					kind of analysis is used by music theorists to outline triadic chord
					progressions found within tonal music <ptr target="#laitz2003"/>. In this
					method, uppercase numerals are used for major triads and lowercase numerals
					are used for minor triads (<ref target="#figure08">Figure 8</ref>). The
					method itself is not key-specific (besides a brief mention at the
					beginning), allowing the analysis of underlying chord relationships across
					multiple songs regardless of key. Considering that machine learning
					programs typically need large datasets for training, and that it is
					unlikely even large datasets will contain songs in every possible key in
					the same proportions, roman numeral analysis is ideal.</p>
				<figure xml:id="figure08">
					<head>A roman numeral analysis of major scale triads where uppercase
						numerals equal major triads and lowercase for minor. The chord built
						upon the seventh uses a degree sign to denote that it is a diminished
						chord.</head>
					<graphic url="resources/images/figure08.png"/>
				</figure>
			</div>
			<div>
				<head>Key Prediction Results</head>
				<p>As our key predictions result from a simple labelling of our mode prediction
					neural network's output, we cannot compute accuracy of training. Past
					testing did include a second NN where the dataset taken from the mode
					detection NN was rotated to each of the 12 possible keys and then trained
					on that with 24 outputs for each major and minor key. This training
					resulted in 93% accuracy but we didn't find any significant increase over
					the method of applying labels after running the mode NN when testing
					against the ground truth set, therefore we decided against training a
					second NN to detect key as it saves training time. With the key labels
					added to the output of our mode NN, our model returned an accuracy level of
					48%. While not nearly as accurate as our mode prediction algorithm, it is
					much higher than the Spotify <emph>Web API</emph> which returns an accuracy
					of 2% on our ground truth set.</p>
				<p>It is also difficult to compare our accuracy on a ground truth set with any
					paper which used the MIREX competition dataset. MIREX's dataset contains
					remakes of real songs using Musical Instrument Digital Interface (MIDI)
					synthesizers which can be recorded straight into a computer, without the
					need for microphones. Furthermore, it is unclear if this dataset recreates
					the percussion synthetically for each song or if this is removed to allow
					easier access to the harmonic content for contestants to test their AI
					systems. As our training and testing uses actual recorded music and no MIDI
					representations there is a higher likelihood of noise or percussive
					elements to throw off our algorithm during testing the outside ground truth
					set. As of this research, we have not entered the MIREX competition, and
					thus do not have access to their datasets and cannot compare the
					performance of our algorithm with those which did. In future research we
					plan to enter the 2020 MIREX key detection competition and to draw more
					accurate comparisons with other key detection AI models.</p>
				<p>Our much lower accuracy levels for key prediction in comparison to mode
					prediction are consistent with misclassification noted within previous
					research. While we were able to almost entirely remove instances of
					relative major and minor mode misclassifications, the big outstanding
					problem are keys classified by an interval of a fifth away. We see this as
					a potential limitation to the ability of supervised or unsupervised
					learning techniques to predict key (and mode to an almost perfect degree)
					from pitch content alone. If we imagine this method of determining mode and
					key from pitch content was performed by a human musician, the equivalent
					would be that of writing down each pitch in the first section (or the
					entirety) of a song and making predictions based on a tally. Since this
					method relies on the tonic being the most prominent pitch in this tally,
					the model will likely always fail when this is not the case. For example, a
					song might be in C major with a melody often repeating the pitch 'G'
					throughout the verse even while a tonic C major chord provides the
					accompaniment underneath. Since the pitch 'G' is found in the tonic chord
					of C major (<ref target="#figure04">Figure 4</ref>), it will sound good and
					be a theoretically correct decision. The repeated <q>G</q>, however, will
					be the most prominent note in the chroma vector section. The scale found a
					fifth away from a C major scale, G Major, shares all the same notes except
					one, thus confusing the model. Even with all the processes aimed at
					removing cases of OANs, a melody focused around the fifth interval of a
					tonic chord will constantly skew the data and the model will make
					miscalculations on key.</p>
			</div>
			<div>
				<head>Conclusions</head>
				<p>Our neural net model itself is quite simple and does not represent a novel
					approach to music AI models. Our research was instead focused on data
					preparation methods grounded in music theory helping to boost the accuracy
					of existing models by finding more appropriate links between music-related
					big data and the resultant outputs. Using our methods such as boosting the
					leading tone's prominence in the KK-profile and filtering out OANs, we were
					able to construct a model with a higher accuracy for mode prediction during
					both testing on a withheld subset of our dataset and on the external ground
					truth set labeled by Lupker. Our key prediction showed comparable results
					to previous research on a separate ground truth set and we see this as a
					limit to the ability of predicting key using a pitch-based tally
					classifier. The model's prediction can be too easily skewed by any song
					with a melody focused on the fifth interval of the tonic chord, mistaking
					the key for one a fifth away. Accessing the MIREX dataset would give us a
					better comparison with those papers which have competed in past
					competitions, but we predict similar results to those found in this
					paper.</p>
			</div>
			<div>
				<head>Further Research</head>
				<p>Further steps we could take with this research would be to identify and label
					OANs to create a more universal mode and key prediction NN. The similarity
					measures specified in this paper could be repeated given chroma vector
					profiles of other modes and scales to compare against. This would be useful
					for any research projects experimenting with big data related to
					non-Western music. Another area for further research would be to apply our
					music theory based processes to some existing chord retrieval algorithms.
					Our testing leads us to believe that training NNs on chord transition
					networks as related to mode or key is the only way to reach accuracy levels
					comparable to human predictions. When musicians are faced with determining
					mode or key of a song with less obvious features, the last resort method is
					to look at the chord progression and make a decision based on that. Our
					predictions are that a learner trained to look for mode or key determining
					features of a chord progression will outperform those based on averaging
					tally counts of pitch.</p>
			</div>
			<div>
				<head>Appendix 1</head>
				<div type="appendix">
				<head>1.1 Formula to find the Optimal Transposition Index (OTI)</head>
				<!-- Note: the data from each image has been encoded as text above its previous reference. 
				Also, the @xml:id of each figure has been reassigned to the main formula as accurately as possible.-->
				<p>Serrà et al.'s formula (<ref target="#serrà2008">2008</ref>) to find the OTI
					between vector <formula rend="inline" notation="tex"
						>\(\overrightarrow{g_{A}}\)</formula>(the desired key) and all 12
					possible rotations of vector <formula rend="inline" notation="tex"
						>\(\overrightarrow{g_{B}}\)</formula>. Rotations are accomplished
					using the <hi rend="italic">Circshift<hi rend="subscript">R</hi></hi>
					function with <hi rend="italic">M</hi> being the maximum amount of
					rotations (12) and <hi rend="italic">j</hi> being the amount to rotate by.
					The function <hi rend="italic">argmax</hi> then selects the rotation amount
						<hi rend="italic">(j)</hi>, which rotated by vector <formula
						rend="inline" notation="tex">\(\overrightarrow{g_{B}}\)</formula> to
					the position with the maximum correlation value with vector <formula
						rend="inline" notation="tex">\(\overrightarrow{g_{A}}\)</formula>.
						<formula rend="block" notation="tex" xml:id="figure09">$$\text{OTI}\left(
						\overrightarrow{g_{A}},\overrightarrow{g_{B}} \right) = \underset{1
						\leq j \leq M}{argmax}\left\{ \overrightarrow{g_{A}} \cdot
						\text{Circshift}_{R}\left( \overrightarrow{g_{B}},j - 1 \right)
						\right\}$$</formula>
				</p>
				</div>
				<!--<figure xml:id="figure09">
					<head>Formula to find the Optimal Transposition Index (OTI)</head>
					<graphic url="resources/images/figure09.png"/>
				</figure>-->
				<div type="appendix">
				<head>1.2.1 Modified formula to transpose chroma vectors separated by major 
					and minor nodes</head>
				<p>Our modified version of Serrà et al.'s <hi rend="italic">OTI</hi> formula
						(<ref target="#serrà2008">2008</ref>) using cosine similarity instead
					of dot product. Instead of finding the best possible transposition amount
						<hi rend="italic">(j)</hi>, we use the function <hi rend="italic"
						>amax</hi> to find the highest correlation value between the desired
					vector <formula rend="inline" notation="tex"
						>\(\overrightarrow{maj}\)</formula> or <formula rend="inline"
						notation="tex">\(\overrightarrow{min}\)</formula> and each rotation of
					vector <formula rend="inline" notation="tex"
						>\(\overrightarrow{cv}\)</formula>. <formula rend="block"
						notation="tex" xml:id="figure10">$$S_{A}\left(
						\overrightarrow{maj},\overrightarrow{cv} \right) = \underset{1 \leq j
						\leq M}{amax}\left\{ \left| \overrightarrow{maj} \right|\left|
						Circshift_{R}\left( \overrightarrow{cv},j - 1 \right)
						\right|\cos\Theta \right\}$$</formula>
					<formula rend="block" notation="tex">$$S_{B}\left(
						\overrightarrow{min},\overrightarrow{cv} \right) = \underset{1 \leq j
						\leq M}{amax}\left\{ \left| \overrightarrow{min} \right|\left|
						Circshift_{R}\left( \overrightarrow{cv},j - 1 \right)
						\right|\cos\Theta \right\}$$</formula>
				</p>
				</div>
				<!--<figure xml:id="figure10">
					<head>Modified formula to transpose chroma vectors separated by major and
						minor modes</head>
					<graphic url="resources/images/figure10.png"/>
				</figure>-->
				<div type="appendix">
				<head>1.2.2</head>
				<p>Find the mode <hi rend="italic">(TS)</hi> of each vector by finding the
					position of the higher value using the function <hi rend="italic"
						>argmax</hi>. These will become the training samples used to train the
					neural network. <formula rend="block" notation="tex" xml:id="figure11"
						>$$TS\left( S_{A},S_{B} \right) = \underset{}{argmax}\left(
						S_{A},S_{B} \right)$$</formula>
				</p>
				<!--<figure xml:id="figure11">
					<head>Training neural network</head>
					<graphic url="resources/images/figure11.png"/>
				</figure>-->
				</div>
			</div>
			<div>
				<head>Appendix 2</head>
				<div type="appendix">
				<head>2.1Computational Workflow</head>
				<figure xml:id="figure12">
					<head>Computational workflow regarding the collection of data and the
						subsequent preprocessing steps required to create training samples and
						labels which can be fed into the neural network.</head>
					<graphic url="resources/images/figure12.png"/>
				</figure>
				</div>
			</div>
		</body>

		<!-- BACK TEXT -->
		<back>
			<listBibl>
				<bibl xml:id="bertin2011" label="Bertin et al. 2011">Bertin-Mahieux, T., D.
					Ellis, B. Whitman, and P. Lamere, <title rend="quotes">The Million Song
						Dataset</title>
					<title rend="italic">Proceedings of the 12th International Conference on
						Music Information</title>, Miami, USA, October 2011.</bibl>
				<bibl xml:id="clement2013" label="Clement 2013">Clement, T., <title
						rend="quotes">Distant Listening or Playing Visualisations Pleasantly
						with the Eyes and Ears.</title>
					<title rend="italic">Digital Studies/le Champ Numérique</title>, 3.2. DOI:
						<ref target="http://doi.org/10.16995/dscn.236"
						>http://doi.org/10.16995/dscn.236</ref> (2013).</bibl>
				<bibl xml:id="finley2019" label="Finley and Razi 2019">Finley, M. and Razi, A.,
						<title rend="quotes">Musical Key Estimation with Unsupervised Pattern
						Recognition</title> IEEE 9th Annual Computing and Communication
					Workshop and Conference, Las Vegas, USA, January 2019.</bibl>
				<bibl xml:id="fonsi2017" label="Fonsi and Yankee 2017">Fonsi, Luis and Daddy
					Yankee, <title rend="quotes">Despacito</title> Vida. Universal Latin, Los
					Angeles, 2017. <ref target="http://doi.org/10.16995/dscn.236"
					>Online</ref>.</bibl>
				<bibl xml:id="gomez2004" label="Gomez and Herrera 2004">Gomez, E and P. Herrera,
						<title rend="quotes">Estimating the Tonality of Polyphonic Audio
						Files: Cognitive Versus Machine Learning Modelling Strategies</title>
					<title rend="italic">Proceedings of the 5th International Society for Music
						Information Retrieval Conference</title>, Barcelona, Spain, October
					2004.</bibl>
				<bibl xml:id="hindemith1984" label="Hindemith 1984">Hindemith, P., <title
						rend="italic">The Craft of Musical Composition: Theoretical Part -
						Book 1</title>. Schott, Mainz, 1984.</bibl>
				<bibl xml:id="huang2019" label="Huang et al. 2019">Huang, C. A., C. Hawthorne,
					A. Roberts, M. Dinculescu, J. Wexler, L. Hong, J. Howcroft, <title
						rend="quotes">The Bach Doodle: Approachable Music Composition with
						Machine Learning at Scale</title>
					<title rend="italic">Proceedings of the 18th International Society for
						Music Information Retrieval Conference, ISMIR 2019</title>. <ref
						target="https://arxiv.org/abs/1907.06637">arXiv:1907.06637</ref>
					(2019).</bibl>
				<bibl xml:id="i̇zmirli2006" label="İzmirli 2006">İzmirli, O., <title
						rend="quotes">Audio Key Finding Using Low Dimensional Spaces</title>
					<title rend="italic">Proceedings from the 7th International Conference on
						Music Information Retrieval</title>, Victoria, Canada, October
					2006.</bibl>
				<bibl xml:id="jehan2014" label="Jehan 2014">Jehan, T., <title rend="quotes"
						>Analyzer Documentation: The EchoNest.</title> Somerville: The Echo
					Nest Corporation, 2014, 5.</bibl>
				<bibl xml:id="krumhansl1982" label="Krumhansl and Kessler 1982">Krumhansl, C.
					L., and E. J. Kessler, <title rend="quotes">Tracing the Dynamic Changes in
						Perceived Tonal Organization in a Spatial Representation of Musical
						Keys</title>
					<title rend="italic">Psychological Review</title>, 89.4 (1982),
					334-368.</bibl>
				<bibl xml:id="laitz2003" label="Laitz 2003">Laitz, S. G., <title rend="italic"
						>The Complete Musician: An Integrated Approach to Tonal Theory,
						Analysis, and Listening</title>. Oxford University Press, New York,
					2003.</bibl>
				<bibl xml:id="mahieu2017" label="Mahieu 2017">Mahieu, R., <title rend="quotes"
						>Detecting Musical Key with Supervised Learning</title> unpublished
					manuscript, 2017.</bibl>
				<bibl xml:id="pauws2004" label="Pauws 2004">Pauws, S., <title rend="quotes"
						>Musical Key Extraction from Audio</title>
					<title rend="italic">Proceedings of the 5th International Society for Music
						Information Retrieval Conference</title>, Barcelona, Spain, October
					2004.</bibl>
				<bibl xml:id="serrà2008" label="Serrà and Gómez 2008">Serrà, J., E. Gómez &amp;
					P. Herrera. <title rend="quotes">Transposing Chroma Representations to a
						Common Key.</title> IEEE CS Conference on The Use of Symbols to
					Represent Music and Multimedia Objects (2008).</bibl>
				<bibl xml:id="serrà2012a" label="Serrà et al. 2012a">Serrà, J. , Á. Corral, M.
					Boguñá, M. Haro &amp; J. Ll. Arcos. <title rend="quotes">Measuring the
						Evolution of Contemporary Western Popular Music</title>
					<title rend="italic">Scientific Reports</title>, 521.2 (2012).</bibl>
				<bibl xml:id="serrà2012b" label="Serrà et al. 2012b">Serrà, J., Á. Corral, M.
					Boguñá, M. Haro &amp; J. Ll. Arcos. <title rend="quotes">Supplementary
						Information: Measuring the Evolution of Contemporary Western Popular
						Music</title>
					<title rend="italic">Scientific Reports</title> 521.2 (2012).</bibl>
			</listBibl>
		</back>
	</text>
	<!-- END TEXT -->

</TEI>
