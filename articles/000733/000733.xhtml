<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Cuneiform Stroke Recognition and Vectorization in 2D Images</h1>
               
               <div class="author"><span style="color: grey">Adéla Hamplová
                     </span> &lt;<a href="mailto:hamplova_at_pef_dot_czu_dot_cz" onclick="javascript:window.location.href='mailto:'+deobfuscate('hamplova_at_pef_dot_czu_dot_cz'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('hamplova_at_pef_dot_czu_dot_cz'); return false;">hamplova_at_pef_dot_czu_dot_cz</a>&gt;, Czech University of Life Sciences Prague</div>
               
               <div class="author"><span style="color: grey">Avital Romach
                     </span> &lt;<a href="mailto:avital_dot_romach_at_yale_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('avital_dot_romach_at_yale_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('avital_dot_romach_at_yale_dot_edu'); return false;">avital_dot_romach_at_yale_dot_edu</a>&gt;, Yale University</div>
               
               <div class="author"><span style="color: grey">Josef Pavlíček
                     </span> &lt;<a href="mailto:pavlicek_at_pef_dot_czu_dot_cz" onclick="javascript:window.location.href='mailto:'+deobfuscate('pavlicek_at_pef_dot_czu_dot_cz'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('pavlicek_at_pef_dot_czu_dot_cz'); return false;">pavlicek_at_pef_dot_czu_dot_cz</a>&gt;, Czech University of Life Sciences Prague</div>
               
               <div class="author"><span style="color: grey">Arnošt Veselý
                     </span> &lt;<a href="mailto:vesely_at_pef_dot_czu_dot_cz" onclick="javascript:window.location.href='mailto:'+deobfuscate('vesely_at_pef_dot_czu_dot_cz'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('vesely_at_pef_dot_czu_dot_cz'); return false;">vesely_at_pef_dot_czu_dot_cz</a>&gt;, Czech University of Life Sciences Prague</div>
               
               <div class="author"><span style="color: grey">Martin Čejka
                     </span> &lt;<a href="mailto:cejkamartin_at_pef_dot_czu_dot_cz" onclick="javascript:window.location.href='mailto:'+deobfuscate('cejkamartin_at_pef_dot_czu_dot_cz'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('cejkamartin_at_pef_dot_czu_dot_cz'); return false;">cejkamartin_at_pef_dot_czu_dot_cz</a>&gt;, Czech University of Life Sciences Prague</div>
               
               <div class="author"><span style="color: grey">David Franc
                     </span> &lt;<a href="mailto:francd_at_pef_dot_czu_dot_cz" onclick="javascript:window.location.href='mailto:'+deobfuscate('francd_at_pef_dot_czu_dot_cz'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('francd_at_pef_dot_czu_dot_cz'); return false;">francd_at_pef_dot_czu_dot_cz</a>&gt;, Czech University of Life Sciences Prague</div>
               
               <div class="author"><span style="color: grey">Shai Gordin
                     </span> &lt;<a href="mailto:shaigo_at_ariel_dot_ac_dot_il" onclick="javascript:window.location.href='mailto:'+deobfuscate('shaigo_at_ariel_dot_ac_dot_il'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('shaigo_at_ariel_dot_ac_dot_il'); return false;">shaigo_at_ariel_dot_ac_dot_il</a>&gt;, Ariel University; Open University of Israel</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Cuneiform%20Stroke%20Recognition%20and%20Vectorization%20in%202D%20Images&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Hamplová&amp;rft.aufirst=Adéla&amp;rft.au=Adéla%20Hamplová&amp;rft.au=Avital%20Romach&amp;rft.au=Josef%20Pavlíček&amp;rft.au=Arnošt%20Veselý&amp;rft.au=Martin%20Čejka&amp;rft.au=David%20Franc&amp;rft.au=Shai%20Gordin"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>A vital part of the publication process of ancient cuneiform tablets is creating hand-copies,
                     which are 2D line art representations of 
                     the 3D cuneiform clay tablets, created manually by scholars. This research provides
                     an innovative method using Convolutional Neural 
                     Networks (CNNs) to identify strokes, the constituent parts of cuneiform characters,
                     and display them as vectors — semi-automatically 
                     creating cuneiform hand-copies. This is a major step in optical character recognition
                     (OCR) for cuneiform texts, which would contribute 
                     significantly to their digitization and create efficient tools for dealing with the
                     unique challenges of Mesopotamian cultural heritage. 
                     Our research has resulted in the successful identification of horizontal strokes in
                     2D images of cuneiform tablets, some of them from 
                     very different periods, separated by hundreds of years from each other. With the Detecto
                     algorithm, we achieved an F-measure of 81.7% 
                     and an accuracy of 90.5%. The data and code of the project are available on GitHub.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1 Introduction</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">1.1 Cuneiform Texts and Artificial Intelligence</h2>
                     
                     <div class="counter"><a href="#p1">1</a></div>
                     <div class="ptext" id="p1">Cuneiform is one of the earliest attested writing systems, and for thousands of years
                        cuneiform has also been the dominant script 
                        of the ancient Middle East, a region stretching roughly from the Persian Gulf to modern
                        Turkey's highlands, and south across the 
                        Levant into Egypt. Cuneiform texts appeared from the end of the fourth millennium
                        BCE until they fell out of use in the early 
                        centuries CE. The script was used for writing a plethora of different documents: legal,
                        administrative, and economic documents; 
                        correspondence between commoners, or high-officials and their kings; some of the oldest
                        works of literature; royal inscriptions 
                        describing the deeds of great kings; as well as lexical and scientific compendia,
                        some of which form the basis of the Greco-Roman 
                        sciences the Western world is built upon today. Hundreds of thousands of cuneiform
                        documents have been discovered since excavations 
                        began in the 1850s. Recent estimates indicate the cuneiform text corpus is second
                        in size only to that of ancient Greek 
                        [<a class="ref" href="#streck_2010">Streck 2010</a>].
                        </div>
                     
                     <div class="counter"><a href="#p2">2</a></div>
                     <div class="ptext" id="p2">The rising application of artificial intelligence to various tasks provides a prime
                        opportunity for training object detection 
                        models to assist the digital publication of cuneiform texts on a large scale. This
                        will help set up a framework for cultural 
                        heritage efforts of preservation and knowledge dissemination that can support the
                        small group of specialists in the field.
                        </div>
                     
                     <div class="counter"><a href="#p3">3</a></div>
                     <div class="ptext" id="p3">Cuneiform provides unique challenges for object detection algorithms, particularly
                        OCR methods. Cuneiform tablets, on which the 
                        texts were written, are 3D objects: pieces of clay which were shaped to particular
                        sizes. While the clay was still moist, scribes 
                        used styli with triangular edges to create impressions on the clay in three possible
                        directions: horizontal, vertical, or oblique 
                        (also <em class="term">Winkelhaken</em>; see Figre 1). “Diagonal” strokes are also found in the literature, but these are technically 
                        either another type of horizontal or an elongated oblique impression [<a class="ref" href="#cammarosano_2014">Cammarosano 2014</a>] 
                        [<a class="ref" href="#cammarosano_et_al_2014">Commarosano et al. 2014</a>] [<a class="ref" href="#bramanti_2015">Bramanti 2015</a>]. Each of these impressions is called a stroke (or wedge, due 
                        to their shape). Combinations of different strokes create characters, usually referred
                        to as signs [<a class="ref" href="#taylor_2015">Taylor 2015</a>]. 
                        Cuneiform can be more easily read when there is direct light on the tablet, especially
                        from a specific angle that casts shadows on 
                        the different strokes.
                        </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="media/image1.png" rel="external"><img src="media/image1.png" style="height: 500px" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>The main cuneiform strokes taken from Neo-Babylonian signs, from left to right: AŠ
                           (<a href="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341" onclick="window.open('https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341'); return false" class="ref">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341</a>), 
                           DIŠ (<a href="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474" onclick="window.open('https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474'); return false" class="ref">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474</a>), and U or <em class="term">Winkelhaken</em> 
                           (<a href="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430" onclick="window.open('https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430'); return false" class="ref">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430</a>), as recorded in the LaBaSi palaeographical database.</div>
                     </div>
                     
                     <div class="counter"><a href="#p4">4</a></div>
                     <div class="ptext" id="p4">From the inception of research in cuneiform studies, tablets were difficult to represent
                        in a modern 2D publishing format. Two 
                        solutions were found:
                        
                        <ul class="list">
                           <li class="item">When possible, 2D images of cuneiform tablets were taken. However, for most of the
                              field's history, such images were  
                              extremely costly to produce and print, and they were often not in sufficient quality
                              for easy sign identification (for 
                              the history of early photography and cuneiform studies, see [<a class="ref" href="#andre-salvini_lombard_1997">André-Salvini and Lombard 1997</a>]).
                              </li>
                           <li class="item">The second solution was creating hand-copies, 2D black and white line art made by
                              scholars of the tablets' strokes. This 
                              was the most popular solution. The disadvantage of this method is that it adds a layer
                              of subjectivity, based on what the 
                              scholar has seen and on their steady drawing hand. Nowadays these hand-copies are
                              still in use, often drawn using vectors 
                              in special programs (the most popular being the open source vector graphics editor
                              
                              <a href="https://inkscape.org/" onclick="window.open('https://inkscape.org/'); return false" class="ref">Inkscape</a>).
                              </li>
                        </ul>
                        </div>
                     
                     <div class="counter"><a href="#p5">5</a></div>
                     <div class="ptext" id="p5">In recent years, the quality of 2D images has risen significantly, while the costs
                        of production and reproduction dropped. A 
                        constantly growing number of images of cuneiform tablets are currently available in
                        various online databases. The largest 
                        repositories are the <a href="https://www.britishmuseum.org/collection" onclick="window.open('https://www.britishmuseum.org/collection'); return false" class="ref">British Museum</a>, the 
                        <a href="https://collections.louvre.fr/" onclick="window.open('https://collections.louvre.fr/'); return false" class="ref">Louvre Museum</a>, the <a href="https://cdli.ucla.edu/" onclick="window.open('https://cdli.ucla.edu/'); return false" class="ref">Cuneiform 
                           Digital Library Initiative</a>, the <a href="https://www.ebl.lmu.de/" onclick="window.open('https://www.ebl.lmu.de/'); return false" class="ref">Electronic Babylonian Library</a>, and the 
                        <a href="https://babylonian-collection.yale.edu/" onclick="window.open('https://babylonian-collection.yale.edu/'); return false" class="ref">Yale Babylonian Collection</a>. 2D+ and 3D models of cuneiform tablets have 
                        also become a possibility, although these are still more expensive and labour-intensive
                        to produce [<a class="ref" href="#earl_et_al_2011">Earl et al. 2011</a>] 
                        [<a class="ref" href="#hameeuw_willems_2011">Hameeuw and Willems 2011</a>] [<a class="ref" href="#collins_et_al_2019">Collins et al. 2019</a>]; cf. overview in [<a class="ref" href="#dahl_hameeuw_wagensonner_2019">Dahl, Hammeeuw, and Wagensonner 2019</a>].
                        </div>
                     
                     <div class="counter"><a href="#p6">6</a></div>
                     <div class="ptext" id="p6">Previous research of identifying cuneiform signs or strokes have used mostly 3D models.
                        Two research groups have developed programs 
                        for manipulating 3D models of cuneiform tablets: the 
                        <a href="https://www.hethport.uni-wuerzburg.de/HPM/hpm.php?p=3djoins" onclick="window.open('https://www.hethport.uni-wuerzburg.de/HPM/hpm.php?p=3djoins'); return false" class="ref">CuneiformAnalyser</a> [<a class="ref" href="#fisseler_et_al_2013">Fisseler et al. 2013</a>] 
                        [<a class="ref" href="#rothacker_et_al_2015">Rothacker et al. 2015</a>] and <a href="https://gigamesh.eu/?page=home" onclick="window.open('https://gigamesh.eu/?page=home'); return false" class="ref">GigaMesh</a> [<a class="ref" href="#mara_et_al_2010">Mara et al. 2010</a>]. 
                        Eeach group developed stroke extraction through geometrical features identification,
                        while one team also used the 3D models 
                        for joining broken tablet fragments [<a class="ref" href="#fisseler_et_al_2014">Fisseler et al. 2014</a>]. In addition, the GigaMesh team extracted strokes as 
                        Scalable Vector Graphic (SVG) images [<a class="ref" href="#mara_kromker_2013">Mara and Krömker 2013</a>], which practically means creating hand-copies automatically 
                        as vector images. This was used as a basis for querying stroke configurations when
                        looking for different examples of the same sign, 
                        using graph similarity methods [<a class="ref" href="#bogacz_gertz_mara_2015a">Bogacz, Gertz, and Mara 2015a</a>] [<a class="ref" href="#bogacz_gertz_mara_2015b">Bogacz, Gertz, and Mara 2015b</a>] 
                        [<a class="ref" href="#bogacz_howe_mara_2016">Bogacz, Howe, and Mara 2016</a>] [<a class="ref" href="#bogacz_mara_2018">Bogacz and Mara 2018</a>] [<a class="ref" href="#kriege_et_al_2018">Kriege et al. 2018</a>].
                        </div>
                     
                     <div class="counter"><a href="#p7">7</a></div>
                     <div class="ptext" id="p7">Work on hand-copies includes transforming raster images of hand-copies into vector
                        images (SVG) [<a class="ref" href="#massa_et_al_2016">Massa et al. 2016</a>]. 
                        Hand-copies and 2D projections of 3D models were used for querying signs by example
                        using CNNs with data augmentation 
                        [<a class="ref" href="#rusakov_et_al_2019">Rusakov et al. 2019</a>]. Previous work on 2D images has only recently started. Dencker et al. used 2D images
                        for 
                        training a weakly supervised machine learning model in the task of sign detection
                        in a given image 
                        [<a class="ref" href="#dencker_et_al_2020">Dencker et al. 2020</a>]. Rusakov et al. used 2D images of cuneiform tablets for querying cuneiform signs by 
                        example and by schematic expressions representing the stroke combinations [<a class="ref" href="#rusakov_et_al_2020">Rusakov et al. 2020</a>]. A more comprehensive 
                        survey of computational methods in use for visual cuneiform research can be found
                        in [<a class="ref" href="#bogacz_mara_2022">Bogacz and Mara 2022</a>].
                        </div>
                     
                     <div class="counter"><a href="#p8">8</a></div>
                     <div class="ptext" id="p8">As there are no published attempts to extract strokes directly from 2D images of cuneiform
                        tablets, the purpose of this paper is a 
                        proof of concept to show it is possible to extract and vectorize strokes from 2D images
                        of cuneiform using machine learning methods. 
                        The quantity and quality of 2D images is improving, and for the most part they provide
                        a more accurate representation of the tablet 
                        than hand-copies, as well as being cheaper and quicker to produce in comparison to
                        3D models. Furthermore, since there are only 
                        three basic types of strokes, but hundreds of signs and variants, one can label a
                        significantly smaller number of tablets to attain 
                        a sufficient number of strokes for training machine learning models. The resulting
                        model will be able to recognize strokes in 
                        cuneiform signs from very different periods, separated by hundreds of years. This
                        semi-automation of hand-copies will be a 
                        significant step in the publication of cuneiform texts and knowledge distribution
                        of the history and culture of the ancient Near 
                        East.
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">1.2 Object Detection</h2>
                     
                     <div class="counter"><a href="#p9">9</a></div>
                     <div class="ptext" id="p9">Identifying cuneiform signs or strokes in an image is considered an object detection
                        task in computer vision. Object detection 
                        involves the automatic identification and localization of multiple objects within
                        an image or a video frame. Unlike simpler tasks 
                        such as image classification, where the goal is to assign a single label to an entire
                        image, object detection aims to provide more 
                        detailed information by detecting and delineating the boundaries of individual objects
                        present. Object detection algorithms work by 
                        analysing the contents of an image and searching for specific patterns or features
                        that are associated with the object. These 
                        patterns or features can include things like color, texture, shape, and size.
                        </div>
                     
                     <div class="counter"><a href="#p10">10</a></div>
                     <div class="ptext" id="p10">There are different types of computational models for object detection, ranging from
                        the purely mathematical to deep learning 
                        models [<a class="ref" href="#wevers_smits_2019">Wevers and Smits 2019</a>]. <em class="term">Mathematical models</em> often involve traditional computer vision techniques that 
                        rely on well-defined algorithms and handcrafted features. These methods typically
                        follow a series of steps to detect objects in an 
                        image. First is extracting relevant features from the image (edges, corners, textures,
                        or color information), where the algorithms 
                        are usually adapted based on domain knowledge. Then mathematical operations are performed
                        to determine the location and extent of 
                        potential objects based on the identified features. Further methods can be used in
                        post-processing to refine the results. 
                        Computational methods work best in ideal or near-ideal conditions, meaning there needs
                        to be standardization in the types of cameras 
                        used for taking the images, the lighting situation, and the background. The objects
                        themselves should also be as uniform as possible 
                        in size, shape, and color. This means that in complex and diverse real-world scenarios,
                        mathematical models are often insufficient 
                        for satisfactory results.
                        </div>
                     
                     <div class="counter"><a href="#p11">11</a></div>
                     <div class="ptext" id="p11">
                        <em class="term">Deep learning models</em>, particularly convolutional neural networks (CNNs), have revolutionized object detection
                        
                        [<a class="ref" href="#girshick_et_al_2014">Girshick et al. 2014</a>]. Instead of manual feature engineering used by mathematical models, deep learning
                        methods can 
                        automatically detect relevant features and objects. The models are trained on labelled
                        data: a set of images where the objects of 
                        interest have been marked, usually in rectangular bounding boxes, by humans. After
                        training, the models can be tested and used on 
                        unseen images that were not in the labelled training dataset to detect the same type
                        of objects. Their biggest advantage is that they 
                        can handle a wide range of object shapes, sizes, and orientations, making them adaptable
                        to diverse scenarios, and generalize well 
                        across different datasets. The disadvantages of such models are that they require
                        large amounts of labelled data for effective 
                        training; they are more computationally intensive compared to traditional mathematical
                        models, requiring more computational power 
                        outside the scope of the average computer; and they are black boxes, meaning it is
                        not always possible to explain why the model makes 
                        a certain prediction or not.
                        </div>
                     
                     <div class="counter"><a href="#p12">12</a></div>
                     <div class="ptext" id="p12">In a previous research project, we combined the use of mathematical and deep learning
                        object detection methods for wildlife mapping 
                        [<a class="ref" href="#pavlicek_2018">Pavlíček 2018</a>]. However, for this project, mathematical models proved insufficient for the complexity
                        and 
                        variability of images of cuneiform tablets, which include different tablet shapes,
                        colors, broken sections, etc. Therefore, in 
                        this article we present our results on stroke recognition for 2D images of cuneiform
                        tablets using several deep leaning models, and 
                        compare their advantages and disadvantages for this type of object detection on ancient
                        and complex writing systems.
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">1.2.1 Convolutional Neural Networks</h3>
                        
                        <div class="counter"><a href="#p13">13</a></div>
                        <div class="ptext" id="p13">Convolutional neural networks (CNNs) are multilayer networks, specifically designed
                           for processing and analyzing visual data. 
                           The convolutional layers in the network process the input image through different
                           filters that help the network detect features 
                           of interest like edges, corners, textures, etc. The additional layers process the
                           resulting feature maps to detect relevant 
                           combinations of features. There can be several iterations of convolutional layers,
                           depending on the specific architecture of 
                           the neural network used.
                           </div>
                        
                        <div class="counter"><a href="#p14">14</a></div>
                        <div class="ptext" id="p14">There are two main types of convolutional neural networks: two-stage detectors and
                           single-stage detectors 
                           [<a class="ref" href="#jiao_et_al_2019">Jiao et al. 2019</a>]. Two-stage detectors, like Faster R-CNN (Region-based Convolutional Neural Network),
                           first 
                           identify regions of the image that might contain objects before analyzing those regions
                           more closely to detect objects 
                           [<a class="ref" href="#girshick_et_al_2014">Girshick et al. 2014</a>]. Single-stage detectors, like YOLO (You Only Look Once; 
                           [<a class="ref" href="#redmon_et_al_2016">Redmon et al. 2016</a>]), can detect objects directly without first identifying regions of interest. In what
                           
                           follows, we provide a brief overview of the advantages and disadvantages of both methods.
                           See also [<a class="ref" href="#mishra_2022">Mishra 2022</a>].
                           </div>
                        
                        <div class="div div3">
                           
                           <h4 class="head">1.2.1.1 YOLO</h4>
                           
                           <div class="counter"><a href="#p15">15</a></div>
                           <div class="ptext" id="p15">YOLO, short for You Only Look Once, is a family of convolutional neural network architectures
                              that was first introduced 
                              in 2015 by Joseph Redmon et al. The version used in this paper, YOLOv5, was published
                              in 2020 
                              [<a class="ref" href="#jocher_et_al_2020">Jocher et al. 2020</a>]. The YOLOv5 architecture consists of 232 layers<a class="noteRef" href="#d4e513">[1]</a>, multiple 
                              convolutional layers that extract features from the image at different scales, and
                              a series of prediction layers, which 
                              output the object detection results.
                              </div>
                           
                           <div class="counter"><a href="#p16">16</a></div>
                           <div class="ptext" id="p16">The algorithm divides the input image into a grid of cells, with each cell responsible
                              for predicting the presence of 
                              one or more objects. For each cell, the network predicts the confidence score, which
                              reflects the likelihood that an 
                              object is present, and the bounding box coordinates that describe the location and
                              size of the object.
                              </div>
                           
                           <div class="counter"><a href="#p17">17</a></div>
                           <div class="ptext" id="p17">The YOLOv5 algorithm has achieved state-of-the-art performance on several benchmark
                              datasets. Its main advantage has 
                              been speed: YOLO performs significantly faster than other CNN models. It has become
                              a popular choice for a wide range of 
                              applications in computer vision, including object detection in real-time video streams,
                              autonomous driving, and 
                              surveillance systems. The latest version at the time of publication is YOLOv8.<a class="noteRef" href="#d4e531">[2]</a>
                              </div>
                           </div>
                        
                        <div class="div div3">
                           
                           <h4 class="head">1.2.1.2 R-CNN, Fast R-CNN, and Faster R-CNN</h4>
                           
                           <div class="counter"><a href="#p18">18</a></div>
                           <div class="ptext" id="p18">Region-based Convolutional Neural Networks (R-CNN) were first introduced in 2014 by
                              Girshick et al. This type of 
                              detector has four key components: (1) it generates region proposals that suggest potential
                              object locations in an image 
                              using a selective search method; (2) it extracts fixed-length feature vectors from
                              each of these proposed regions; (3) 
                              for each region of interest, it computes relevant features for object identification;
                              and (4) based on the extracted CNN 
                              features, the regions of interest are classified (see Figure 2).
                              </div>
                           
                           <div class="figure">
                              
                              
                              <div class="ptext"><a href="media/image2.jpeg" rel="external"><img src="media/image2.jpeg" style="height: 500px" alt="" /></a></div>
                              
                              <div class="caption">
                                 <div class="label">Figure 2. </div>The components of the R-CNN detector, from [<a class="ref" href="#girshick_et_al_2014">Girshick et al. 2014</a>].</div>
                           </div>
                           
                           <div class="counter"><a href="#p19">19</a></div>
                           <div class="ptext" id="p19">A year later, Girshick proposed a more efficient version called Fast R-CNN [<a class="ref" href="#girshick_2015">Girshick 2015</a>]. In 
                              contrast to R-CNN, which processes individual region suggestions separately through
                              the CNN, Fast R-CNN computes features 
                              for the entire input image at once. This significantly speeds up the process and allows
                              for better use of storage space 
                              for feature storage.
                              </div>
                           
                           <div class="counter"><a href="#p20">20</a></div>
                           <div class="ptext" id="p20">Building on the improvements of Fast R-CNN, Faster R-CNN was introduced just three
                              months later 
                              [<a class="ref" href="#ren_et_al_2015">Ren et al. 2015</a>]. It introduced the region proposal network (RPN), which generates regions of interest
                              
                              (RoI), potential identifications of the desired objects. It does so by sliding a small
                              window (known as an anchor) over 
                              the feature maps and predicting whether the anchor contains an object or not. For
                              this project, we used a combination of 
                              Faster R-CNN and ResNet-50, a deep learning architecture that optimizes the network's
                              performance. This model was 
                              implemented by Bi in Detecto python library, using the PyTorch python framework.<a class="noteRef" href="#d4e577">[3]</a>
                              </div>
                           </div>
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2 Dataset and Evaluations</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.1 Dataset</h2>
                     
                     <div class="div div2">
                        
                        <h3 class="head">2.1.1. Dataset Creation, Division, and Augmentation</h3>
                        
                        <div class="counter"><a href="#p21">21</a></div>
                        <div class="ptext" id="p21">The Assyriologists on our team tagged thousands of horizontal strokes in eight tablets
                           from the Yale Babylonian 
                           Collection (see Table 1), made available through the kind permission of Agnete W. Lassen and Klaus 
                           Wagensonner. For the first stage of research, we labelled 7,355 horizontal strokes in the tablets
                           chosen, divided into 
                           823 images.
                           </div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row label">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Yale ID</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">CDLI ID</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Material</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Period</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Genre</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Content</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Hand-Copy Publication</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-014442" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-014442'); return false" class="ref">YPM BC 014442</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://cdli.ucla.edu/P504832" onclick="window.open('https://cdli.ucla.edu/P504832'); return false" class="ref">P504832</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Neo-Assyrian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Literary</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Enuma Eliš ll. 1-16, 143-61</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">CT 13 1,3</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-023856" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-023856'); return false" class="ref">YPM BC 023856</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P293426" onclick="window.open('http://cdli.ucla.edu/P293426'); return false" class="ref">P293426</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Old-Babylonian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Literary</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Gilgamesh and Huwawa ll. 1-36</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">JCS 1 22-23</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-002575" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-002575'); return false" class="ref">YPM BC 002575</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P297024" onclick="window.open('http://cdli.ucla.edu/P297024'); return false" class="ref">P297024</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Neo/Late-Babylonian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Commentary</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Iqqur īpuš i 36, ii 31, iii 22, iv 5, v 13</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">BRM 4 24</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-016773" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-016773'); return false" class="ref">YPM BC 016773</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P293444" onclick="window.open('http://cdli.ucla.edu/P293444'); return false" class="ref">P293444</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Limestone</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Early Old-Babylonian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Inscription</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Building inscription of Anam, No. 4</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">YOS 1 36</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-016780" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-016780'); return false" class="ref">YPM BC 016780</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P293445" onclick="window.open('http://cdli.ucla.edu/P293445'); return false" class="ref">P293445</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Limestone</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Early Old-Babylonian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Inscription</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Building inscription of Anam, No. 2</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">YOS 1 35</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-016869" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-016869'); return false" class="ref">YPM BC 016869</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P429204" onclick="window.open('http://cdli.ucla.edu/P429204'); return false" class="ref">P429204</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Middle Assyrian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Inscription</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Inscription of Aššur-nadin-apli</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">YOS 9 71</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-021204" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-021204'); return false" class="ref">YPM BC 021204</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P308129" onclick="window.open('http://cdli.ucla.edu/P308129'); return false" class="ref">P308129</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Middle Assyrian?</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Medical Text</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">FS Sachs 18, no. 16</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="https://collections.peabody.yale.edu/search/Record/YPM-BC-021234" onclick="window.open('https://collections.peabody.yale.edu/search/Record/YPM-BC-021234'); return false" class="ref">YPM BC 021234</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"><a href="http://cdli.ucla.edu/P308150" onclick="window.open('http://cdli.ucla.edu/P308150'); return false" class="ref">P308150</a></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Clay</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Old-Babylonian</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Hymn</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Hymn to Inanna-nin-me-šar2-ra, ll. 52-102</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">YNER 3 6-7</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 1. </div>Table showing the eight tablets that were labelled and their metadata. The information
                              is taken from the 
                              <a href="https://babylonian-collection.yale.edu/" onclick="window.open('https://babylonian-collection.yale.edu/'); return false" class="ref">Yale Babylonian Collection website</a>. “CDLI ID” 
                              refers to the catalogue number in the 
                              <a href="https://cdli.ucla.edu/" onclick="window.open('https://cdli.ucla.edu/'); return false" class="ref">Cuneiform Digital Library Initiative</a> database. The publication 
                              abbreviations in the column labelled "Hand-Copy Publication" follow the 
                              <a href="https://rla.badw.de/en/reallexikon/abkuerzungslisten/literatur-und-koerperschaften.html" onclick="window.open('https://rla.badw.de/en/reallexikon/abkuerzungslisten/literatur-und-koerperschaften.html'); return false" class="ref">Reallexikon 
                                 der Assyriologie online list</a>.</div>
                        </div>
                        
                        <div class="counter"><a href="#p22">22</a></div>
                        <div class="ptext" id="p22">To train an artificial neural network, a dataset divided into training, validation,
                           and test subsets needs to be created. In 
                           order to increase the number of images in the dataset, several augmentation methods
                           were used. The recommended number of images 
                           for each class is at least a thousand images [<a class="ref" href="#cho_et_al_2016">Cho et al. 2016</a>]. Roboflow<a class="noteRef" href="#d4e879">[4]</a> is a web application used to create extended 
                           datasets from manually labelled data using labelling tools such as Labelimg.<a class="noteRef" href="#d4e891">[5]</a>
                           </div>
                        
                        <div class="counter"><a href="#p23">23</a></div>
                        <div class="ptext" id="p23">For pre-processing, the images were divided into equal squares of 416 x 416 pixels
                           (for Detecto and YOLOv5). For R-CNN, the 
                           size of images was downsized to 224 x 224 pixels. For the final version of the model,
                           the images were augmented using Roboflow. 
                           The final dataset contains 1,975 images with more than 20,000 labels. The augmented
                           horizontal stroke dataset is available 
                           online.<a class="noteRef" href="#d4e906">[6]</a>
                           </div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">2.1.2 Labelling Criteria</h3>
                        
                        <div class="counter"><a href="#p24">24</a></div>
                        <div class="ptext" id="p24">For machine learning purposes, the images of the cuneiform tablets needed to be split
                           into squares of equal size (see Appendix). 
                           Labelling was performed after splitting the images. This meant the loss of a lot of
                           the context necessary to identify strokes 
                           with certainty. We used tablet images with existing hand-copies, which were used as
                           a guide and previous interpretations of the 
                           tablets.
                           </div>
                        
                        <div class="counter"><a href="#p25">25</a></div>
                        <div class="ptext" id="p25">However, a greater emphasis was given to what is currently visible on the image than
                           what appears in the hand-copy. The 
                           hand-copies were not always true to what is seen on the images for three main reasons:
                           (1) the hand-copy preserves signs which 
                           were visible on the tablet at the moment of their creation, but by the time the image
                           was taken, they had eroded; (2) the 
                           camera angle when taking the image did not capture all the detail the tablet contains.
                           This is a common scenario, since signs 
                           at the edges of the tablet will not be seen as clearly when taking a frontal image;
                           and (3) strokes may have been cut off where 
                           the image was split.
                           </div>
                        
                        <div class="counter"><a href="#p26">26</a></div>
                        <div class="ptext" id="p26">If a stroke was completely unrecognizable as a horizontal stroke on the image at hand,
                           because of either of the aforementioned 
                           restrictions, it was not labelled. If enough of the characteristic features of the
                           stroke (particularly its triangular head) 
                           were present on the image, it was labelled. Being able to identify partially broken
                           strokes is still useful for real-life 
                           scenarios, since the tablets themselves are often broken, a common problem in sign
                           identification.
                           </div>
                        
                        <div class="counter"><a href="#p27">27</a></div>
                        <div class="ptext" id="p27">Additionally, strokes which are usually considered diagonal were also labelled. A
                           relative leniency was given to this issue, 
                           since in general, the lines on cuneiform tablets are not always straight (i.e., creating
                           a 90° angle with the tablet itself). 
                           Therefore, a horizontal stroke that may be exactly horizontal when viewed in its line
                           will appear diagonal on the image if the 
                           lines themselves are somewhat diagonal. For an example of labelled images, see Figure
                           3.
                           </div>
                        
                        <div class="figure">
                           
                           
                           <div class="ptext"><a href="media/image3.png" rel="external"><img src="media/image3.png" style="height: 500px" alt="" /></a></div>
                           
                           <div class="caption">
                              <div class="label">Figure 3. </div>Training set example of labelled horizontal strokes on tablet YPM BC 014442.</div>
                        </div>
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.2 Evaluation Metrics</h2>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">Standard evaluation metrics include precision, sensitivity, and F-measure, calculated
                        from true positive rate (TP), false positive 
                        rate (FP), and false negative rate (FN). These can be displayed in Table 2. From these,
                        the following metrics can be calculated to 
                        quantitatively assess the efficacy of the model.
                        </div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1"></td>
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Predicted Positive</td>
                              
                              <td valign="top" class="cell label" colspan="1" rowspan="1">Predicted Negative</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Actual Positive</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">True Positive (TP)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">False Negative (FN)</td>
                              </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">Actual Negative</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">False Positive (FP)</td>
                              
                              <td valign="top" class="cell" colspan="1" rowspan="1">True Negative (TN)</td>
                              </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>“TP” refers to the the proportion of cases that are correctly identified as positive by
                           the model. “FP” marks the 
                           proportion of cases that are incorrectly classified as positive by the model. “FN” reflects the proportion of cases that are 
                           incorrectly identified as negative by the model. “TN” indicates the proportion of cases that are correctly identified as 
                           negative.</div>
                     </div>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">Accuracy or Precision (p): Precision measures how many of the total number of predicted
                        positive cases are true positives. In other 
                        words, it is the ratio of true positives to the total number of predicted positive
                        cases, whether true or false.
                        $$p = \frac{TP}{TP+FP}$$
                        </div>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">Sensitivity or Recall (s): Sensitivity measures how many of the true positive cases
                        are correctly identified as positive by the 
                        model. In other words, it is the ratio of true positives to the total number of actual
                        positive cases, which includes both true 
                        positives and false negatives.
                        $$s = \frac{TP}{TP+FN}$$
                        </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">F-measure (F1): The F1 measure combines precision and sensitivity into a single score
                        that is commonly used as the final 
                        assessment of a model. It is the harmonic mean of precision and sensitivity, calculated
                        as two times the product of precision and 
                        sensitivity divided by their sum, resulting in a number between 0 and 1 that can be
                        viewed as a percentage of overall 
                        accuracy.
                        $$F = \frac{2×p×s}{p+s}$$
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3 Results</h1>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">Our goal was to test several types of object detectors to compare which one gives
                     the best results for our task. According to 
                     theoretical comparisons on public datasets, one-stage algorithms (here YOLOv5) should
                     give faster but less accurate results compared to 
                     two-stage detectors (here Detecto and R-CNN).
                     </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">While testing with the YOLOv5 detector took only 1.189 seconds, the overall accuracy
                     was just over 40%, which is not sufficient for 
                     practical usability. The prediction using the R-CNN network took on average 45 seconds,
                     but the results did not even reach the 
                     YOLOv5 level. We believe that this was due to a lack of tuning of the hyperparameters
                     and may be the subject of further experiments. 
                     Detecto, which was not as fast as YOLOv5 but not as slow as R-CNN, achieved results
                     that far outperformed both previous algorithms with 
                     its 90.5% sensitivity and 81.7% F-score. The reason behind this fact may be that Detecto
                     is an optimised network that combines the 
                     principles of a two-stage detector with ResNet. Detailed evaluation results are shown
                     in Table 3, Figure 4, and Table 4.
                     </div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Name</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">TP</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">FN</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">FP</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">p</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">s</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">F1</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Fake of All Found Strokes</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Detecto (Threshold 0.4)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">669</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">70</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">229</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">74.4%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">90.5%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">81.7%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">25.5%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">YOLOv5 (Threshold 0.2)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">323</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">419</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">444</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">42.1%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">43.5%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">42.8%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">57.8%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">YOLOv5 (Threshold 0.3)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">256</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">486</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">305</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">45.6%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">34.5%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">39.2%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">54.4%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">YOLOv5 (Threshold 0.4)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">196</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">546</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">190</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">50.7%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">26.4%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">34.7%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">49.2%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">R-CNN (Threshold 0.4)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">191</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">595</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">941</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">16.9%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">24.8%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">19.9%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">83.1%</td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 3. </div>Evaluation results.</div>
                  </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="media/image4.png" rel="external"><img src="media/image4.png" style="height: 500px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 4. </div>Evaluation results of object detection algorithms.</div>
                  </div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Prediction</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Network</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Tablet</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1"><a href="media/table1.png" rel="external"><img src="media/table1.png" style="" alt="" /></a></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Detecto</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">YPM BC 016773</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1"><a href="media/table2.png" rel="external"><img src="media/table2.png" style="" alt="" /></a></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">YOLOv5</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">YPM BC 018686</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1"><a href="media/table3.png" rel="external"><img src="media/table3.png" style="" alt="" /></a></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">R-CNN</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">YPM BC 023856</td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 4. </div>Prediction with respective detectors.</div>
                  </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">4 Discussion</h1>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">The results of the machine learning model we developed (90.5% accuracy and 81.7% F-measure
                     on Detecto) are very promising, particularly 
                     considering the relatively small amount of labelled data. It shows that this previously
                     untested approach, namely stroke identification 
                     from 2D images, can be highly efficient for the vectorization of cuneiform tablets.
                     While stroke identification has already been 
                     achieved in 3D models of cuneiform-bearing objects (see Section 1.1), our approach
                     shows the same is possible for 2D images which are far 
                     cheaper to produce.
                     </div>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">Furthermore, our model is not period-dependent, meaning that some of the tablets we
                     have chosen are over a thousand years apart (see 
                     Table 1). But since the writing technique itself has not changed during that period,
                     there were no significant differences in the 
                     model's ability to recognize the strokes. The major attested difference between stroke
                     types in Mesopotamia is Babylonian vs. Assyrian 
                     strokes, the former having longer bodies (the tracing line), and the latter having
                     bigger heads (the central point of the impression 
                     left on the clay tablet [<a class="ref" href="#edzard_1980">Edzard 1980</a>] [<a class="ref" href="#labat_1988">Labat 1988</a>]. This, however, does not seem to affect our model.
                     </div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">Since it was possible to effectively identify horizontal strokes, the same is possible
                     for verticals and obliques. With this additional 
                     ability, it will be possible to create a full vectorization of cuneiform tablets,
                     which will need to be minimally corrected by an expert. 
                     These vectorizations are in effect like hand-copies, which are a first step in assyriological
                     research in interpreting cuneiform texts. 
                     It will be the first step in a human-in-the-loop pipeline of cuneiform identification
                     from image to digital text. 
                     </div>
                  
                  <div class="counter"><a href="#p37">37</a></div>
                  <div class="ptext" id="p37">The subsequent step, identification of constellations of strokes as cuneiform signs,
                     is under development by part of the authors 
                     [<a class="ref" href="#gordin_romach_2022">Gordin and Romach 2022</a>], currently using traditional hand-copies as input (see 
                     <a href="https://www.ben-digpasts.com/demo" onclick="window.open('https://www.ben-digpasts.com/demo'); return false" class="ref">demo</a>; see [<a class="ref" href="#yamauchi_et_al_2018">Yamauchi et al. 2018</a>] for previous work in this 
                     direction). Once the signs are identified with their equivalent in Unicode cuneiform
                     [<a class="ref" href="#cohen_et_al_2004">Cohen et al. 2004</a>], these can be 
                     transliterated and segmented into words using the model Akkademia, previously developed
                     by the assyriologists on our team and others 
                     [<a class="ref" href="#gordin_et_al_2020">Gordin et al. 2020</a>]. This can be further followed by machine translation for the two main languages which
                     used the 
                     cuneiform writing system, Sumerian and Akkadian. The Machine Translation and Automated
                     Analysis of Cuneiform Languages (MTAAC) project 
                     has begun developing models for translating Ur III administrative texts (dated to
                     the 21st century BCE) 
                     [<a class="ref" href="#punia_et_al_2020">Punia et al. 2020</a>]. Machine translation of Akkadian has also been achieved, focusing primarily on first
                     millennium BCE 
                     texts from a variety of genres, available through <a href="http://oracc.org/" onclick="window.open('http://oracc.org/'); return false" class="ref">ORACC</a> [<a class="ref" href="#gutherz_et_al_2023">Gutherz et al. 2023</a>].
                     </div>
                  
                  <div class="counter"><a href="#p38">38</a></div>
                  <div class="ptext" id="p38">This pipeline can become a vital part of assyriological research by making accessible
                     to experts and laypeople alike countless 
                     cuneiform texts that have previously received less scholarly attention. However, it
                     is important to note the limitations of this 
                     pipeline for assyriological research.
                     </div>
                  
                  <div class="counter"><a href="#p39">39</a></div>
                  <div class="ptext" id="p39">The vector images we produce are not an accurate representation of the stroke, but
                     rather a chosen schema. Although various schemas can 
                     be selected, they are still one simplistic representation on how a stroke looks, which
                     is then applied across the corpus. Therefore, for 
                     purposes of scribal hand identification, as well as palaeography, they lack important
                     aspects, such as <em class="term">ductus</em>, or the 
                     formation of individual signs) and <em class="term">aspect</em> (cf. Latin <em class="term">equilibrium</em>), or the visual impression created by the 
                     set hand of a scribe (i.e., the style of writing). This is the same, however, for
                     manually created hand-copies, since there are 
                     limitations to how these 3D objects can be captured in 2D, and some scholars tend
                     to simplify what they see on the tablet when creating 
                     hand-copies.
                     </div>
                  
                  <div class="counter"><a href="#p40">40</a></div>
                  <div class="ptext" id="p40">In addition, our results worked well on very high quality 2D images, curated by an
                     expert [<a class="ref" href="#wagensonner_2015">Wagensonner 2015</a>]. Although 
                     anyone can take high-quality images on their phone, ensuring that the signs and strokes
                     are as legible as possible usually requires an 
                     expert knowledge of the cuneiform script and the application of light sources. For
                     this to efficiently work on a large scale, preferably 
                     only high-quality 2D images of cuneiform artifacts should be used.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5 Towards Quantitative Epigraphy</h1>
                  
                  <div class="counter"><a href="#p41">41</a></div>
                  <div class="ptext" id="p41">The task of the epigrapher is to decipher the ancient writing surface — not merely
                     to decipher the script or any linguistic element 
                     on its own, but rather to produce a holistic decipherment of the inscription, its
                     material aspects, and its contextual meaning. 
                     Therefore, it is challenging to translate epigraphic tasks into one or more computational
                     tasks. The current contribution is a step in 
                     this direction, by attempting to gouge out the atomized elements of the script and
                     its arrangement on the writing surface. This 
                     diplomatic approach to texts has a long history in medieval scholarly practice [<a class="ref" href="#duranti_1998">Duranti 1998</a>] 
                     [<a class="ref" href="#bertrand_2010">Bertrand 2010</a>], and it is a <em class="term">desideratum</em> in order to piece together computational tasks for quantitative 
                     epigraphy. It is further a way to bridge the differences across large numbers of ancient
                     or little-known languages and scripts, since 
                     much of the literature surrounding their study involves discussions on reconstructing
                     the writing surface, traces, and their proper 
                     sequence in their <em class="term">Sitz im Leben</em>.
                     </div>
                  
                  <div class="counter"><a href="#p42">42</a></div>
                  <div class="ptext" id="p42">The problem begins when one tries to harmonize tasks from the disciplines in the realm
                     of computer science and the different research 
                     questions in the humanities, which do not necessarily overlap. For an epigrapher,
                     identifying and classifying an object in an image is 
                     not the end goal, as it might be in computer science. Rtaher, it is a step in a process
                     to reach historical understanding of a certain 
                     genre of text, writing tradition, or historical period. Furthermore, the amounts of
                     data that are available to train generative models 
                     like ChatGPT or the many image generator applications made available in recent months,
                     is beyond the scope of the digital data at the 
                     hands of the average epigrapher, historian, or digital humanist.
                     </div>
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">For that end, an interdisciplinary group of scholars dealing with ancient language
                     processing and machine learning for ancient languages 
                     [<a class="ref" href="#anderson_et_al_2023">Anderson et al. 2023</a>] [<a class="ref" href="#sommerschield_et_al_2023">Sommerschield et al. 2023</a>] has set out to better define and standardize data 
                     formats, tasks, and benchmarks. This initiative adds to the growing movement of computational
                     and quantitative studies in classics, 
                     biblical studies, ancient Near Eastern studies, and so on. The present paper is aimed
                     to contribute to the standardization of the 
                     epigrapher's computational tasks in ancient scripts. This paper also provides an example
                     of harmony and collaboration between computer 
                     scientists and humanists, as well as between computer science tasks and humanistic
                     research questions.
                     </div>
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">Furthermore, the methodology and techniques used in this study can be applied to other
                     writing systems beyond cuneiform. The 
                     semi-automatic vectorization approach can be adapted to identify and extract specific
                     features of other ancient scripts. In ancient 
                     Chinese and Japanese for example, one can try to find the common denominator made
                     up of strokes, the components of each character. In 
                     classical Maya writing, one could focus on the anthropomorphic elements of signs,
                     like noses, eyes, ears, etc. The same can be said for 
                     other hieroglyphic scripts, like Egyptian or Anatolian hieroglyphs.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.1 Training Convolutional Neural Networks</h2>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">In the following section, we present the parameters necessary to replicate our results.
                        </div>
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46">For all the models we employed, image augumentation methods were necessary to increase
                        the number of available images for training. 
                        Grayscale augmentations were applied with three samples per augmentation: 
                        
                        <ul class="list">
                           <li class="item">Saturation applied to 50% of the images</li>
                           <li class="item">Saturation between -20% and +20%</li>
                           <li class="item">Exposure between -20% and +20%</li>
                        </ul>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">6.1.1 Detecto Training</h3>
                        
                        <div class="counter"><a href="#p47">47</a></div>
                        <div class="ptext" id="p47">For Detecto training, the dataset was divided into three subsets. The training set
                           contains 3,456 images, the validation set 
                           contains 330 images, and the testing set contains 164 images. The training was performed
                           on Google Collaboratory, and the 
                           layers from the fasterrcnn_resnet50_fpn_coco-258fb6c6.pth model were unfrozen and
                           re-trained. Fifty epochs were run, with three 
                           steps per epoch, and the validation loss dropped from 0.74 to 0.64 after all epochs,
                           which took 302 minutes. After five epochs, 
                           the validation loss did not decrease, so we could have used early stopping for this
                           model.
                           </div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">6.1.2 YOLOv5 Training</h3>
                        
                        <div class="counter"><a href="#p48">48</a></div>
                        <div class="ptext" id="p48">The YOLOv5 architecture (with 232 layers) was trained in Google Colaboratory using
                           CUDA on a Tesla T4 GPU with 40 
                           multiprocessors and 15,109 MB of total memory. 100 epochs were executed with a batch
                           of 16 images. The training loss (MAE) was 
                           reduced from 0.1 in the first epoch to 0.03 in the last epoch, as can be seen in Figure
                           5.
                           </div>
                        
                        <div class="figure">
                           
                           
                           <div class="ptext"><a href="media/image5.png" rel="external"><img src="media/image5.png" style="height: 500px" alt="" /></a></div>
                           
                           <div class="caption">
                              <div class="label">Figure 5. </div>Training set loss; source: Tensorboard</div>
                        </div>
                        </div>
                     
                     <div class="div div2">
                        
                        <h3 class="head">6.1.3 R-CNN Training</h3>
                        
                        <div class="counter"><a href="#p49">49</a></div>
                        <div class="ptext" id="p49">The whole implementation was done using the artificial intelligence lab at the Czech University of Life Sciences 
                           in Prague, because R-CNN has high memory requirements and caused Google Collaboratory
                           to crash (due to lack of memory). The 
                           environment settings as seen in Table 5 were used:
                           </div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">IDE</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">VS Code with Jupyter Extension</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Kernel</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Python 3.8.12 within Anaconda</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">AI Framework</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Tensorflow 2.7.0 for GPU</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Nvidia Configuration</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">NVIDIA Quadro P400, cuda 11.2, cudnn 8.1</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 5. </div>R-CNN environment settings.</div>
                        </div>
                        
                        <div class="counter"><a href="#p50">50</a></div>
                        <div class="ptext" id="p50">We have implemented region proposals with selective search using IoU (Intersection
                           over Union) configured as seen in 
                           Table 6:
                           </div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Max Samples</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">55 (based on the maximum in training set)</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Selective Search Iterate Results</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">2000 (proposed in original paper)</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">IoU Object Limit</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">0.7</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">IoU Background Limit</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">0.3</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 6. </div>R-CNN configuration.</div>
                        </div>
                        
                        <div class="counter"><a href="#p51">51</a></div>
                        <div class="ptext" id="p51">The images used were 224 x 224 in size. We chose a VGG model pre-trained on the ImageNet
                           dataset (input layer, thirteen 
                           convolutional layers, five MaxPooling layers, Flatten, Dense). After encoding the
                           label set once and splitting it into 
                           training (90%) and test sets (10%), we proceeded to train with the configurations
                           as seen in Table 7. Early stopping caused 
                           the training process to stop after thirty-nine epochs.
                           </div>
                        
                        <div class="table">
                           <table class="table">
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Error Function</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Binary cross-entropy</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Optimizer</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Adam</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Learning Rate</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">0.0001</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Training Epochs</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">100</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Steps in Epoch</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">10</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell label" colspan="1" rowspan="1">Patience Epochs for Early Stopping</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">20</td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 7. </div>
                           </div>
                        </div>
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6.2 Utilities for Further Research</h2>
                     
                     <div class="counter"><a href="#p52">52</a></div>
                     <div class="ptext" id="p52">In order to ease the process of data creation for the next steps of the project, we
                        developed three image and label processing 
                        tools: an image splitter, a vector visualization, and an image merger. These tools
                        are available in the GitHub repository of our 
                        project.<a class="noteRef" href="#d4e1565">[7]</a> 
                        The neural networks that we used for object detection accept square input, and if
                        it is not square, the image is reshaped to a 
                        standard input size. For large tablets, there would be a significant loss of data,
                        so it is necessary to slice large images into 
                        smaller, uniformly sized square images and train the network on these slices. We chose
                        a fixed image size of 416 x 416 (a multiple 
                        of eight, which is generally better for machine learning purposes [<a class="ref" href="#chollet_pecinovsky_2019">Chollet and Pecinovský 2019</a>]).
                        </div>
                     
                     <div class="counter"><a href="#p53">53</a></div>
                     <div class="ptext" id="p53">While for the research presented in this article, we split the images before labelling,
                        this slowed down the labelling process. 
                        Therefore, we developed an image splitter and an image merger. Our proposed system
                        works as follows: after labelling, a large 
                        image with a cuneiform-bearing object is cut into squares with 50% overlap, so there
                        is no data loss if strokes are on the edge 
                        of one square, since they are in the middle of the next one. Then the neural network
                        predicts where the horizontal strokes are in 
                        the image. The networks return bounding boxes which indicate the location of the strokes.
                        These bounding boxes are replaced by 
                        vectors of strokes in an empty image, and the strokes in the whole tablet are reconstructed
                        using merging. In this way we can 
                        create an automatic vectorization of horizontal (and other) strokes in the whole tablet
                        (see Figure 6).
                        </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="media/image6.png" rel="external"><img src="media/image6.png" style="height: 500px" alt="" /></a></div>
                        </div>
                     
                     <div class="counter"><a href="#p54">54</a></div>
                     <div class="ptext" id="p54">The main challenge in preparing the set of tools was dealing with splitting labels.
                        When splitting the image into smaller squares, 
                        there is a cut-off threshold for deciding whether the annotated strokes are still
                        significant enough to be used in training. The 
                        threshold is based on a percentage that determines what portion of the annotated strokes
                        can be kept and what should be removed.
                        </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p55">55</a></div>
                  <div class="ptext" id="p55">This research was funded by two grants. The project cuneiform analysis using Convolutional
                     Neural Networks reg. no. 31/2021 was 
                     financed from the OP RDE project Improvement in Quality of the Internal Grant Scheme
                     at CZU, reg. no. 
                     CZ.02.2.69/0.0/0.0/19_073/0016944. The project no. RA2000000010 was financed by the
                     CULS – Ariel University cooperation grant.
                     </div>
                  </div>
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e513"><span class="noteRef lang en">[1] See Ultralytics/Yolov5 (2021) 
                     “Yolov5”, <cite class="title italic">GitHub</cite>, available at: 
                     <a href="https://github.com/ultralytics/yolov5" onclick="window.open('https://github.com/ultralytics/yolov5'); return false" class="ref">https://github.com/ultralytics/yolov5</a>.</span></div>
               <div class="endnote" id="d4e531"><span class="noteRef lang en">[2] See Jocher, G., Chaurasia, A., and 
                     Qiu, J. (2023) “YOLO by Ultralytics (Version 8.0.0)”, <cite class="title italic">GitHub</cite>, 
                     available at: <a href="https://github.com/ultralytics/ultralytics" onclick="window.open('https://github.com/ultralytics/ultralytics'); return false" class="ref">https://github.com/ultralytics/ultralytics</a>.</span></div>
               <div class="endnote" id="d4e577"><span class="noteRef lang en">[3] See Bi, A. (2020)  
                     alankbi/detecto “Build fully-functioning computer vision models with PyTorch”, 
                     <cite class="title italic">GitHub</cite>, available at: 
                     <a href="https://github.com/alankbi/detecto" onclick="window.open('https://github.com/alankbi/detecto'); return false" class="ref">https://github.com/alankbi/detecto</a>.</span></div>
               <div class="endnote" id="d4e879"><span class="noteRef lang en">[4] See Approboflowcom (2021) 
                     “Roboflow Dashboard”, <cite class="title italic">GitHub</cite>, available at: 
                     <a href="https://app.roboflow.com/" onclick="window.open('https://app.roboflow.com/'); return false" class="ref">https://app.roboflow.com/</a>.</span></div>
               <div class="endnote" id="d4e891"><span class="noteRef lang en">[5] See tzutalin/labelIm (2021)  
                     “LabelImg”, <cite class="title italic">Github</cite>, available at: 
                     <a href="https://github.com/tzutalin/labelImg" onclick="window.open('https://github.com/tzutalin/labelImg'); return false" class="ref">https://github.com/tzutalin/labelImg</a>.</span></div>
               <div class="endnote" id="d4e906"><span class="noteRef lang en">[6] See Roboflow (2021) “Augmented Horizontal Dataset”, available at: 
                     <a href="https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV" onclick="window.open('https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV'); return false" class="ref">https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV</a>.</span></div>
               <div class="endnote" id="d4e1565"><span class="noteRef lang en">[7] See 
                     <a href="https://github.com/adelajelinkova/cuneiform/tree/main/Utilities" onclick="window.open('https://github.com/adelajelinkova/cuneiform/tree/main/Utilities'); return false" class="ref">https://github.com/adelajelinkova/cuneiform/tree/main/Utilities</a>.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="anderson_et_al_2023"><!-- close -->Anderson et al. 2023</span> Anderson, A. et al. (eds) (2023) <cite class="title italic">Proceedings of 
                     the ancient language processing workshop (RANLP-ALP 2023)</cite>. Shoumen, Bulgaria: INCOMA Ltd.
                  </div>
               <div class="bibl"><span class="ref" id="andre-salvini_lombard_1997"><!-- close -->André-Salvini and Lombard 1997</span> André-Salvini, B. and Lombard, P. (1997) 
                  “La découverte épigraphique de 1995 à Qal'at al-Bahrein: un jalon pour la chronologie
                  de la phase Dilmoun Moyen 
                  dans le Golfe arabe”, <cite class="title italic">Proceedings of the seminar for Arabian studies, 1995</cite>. pp. 165–170.
                  </div>
               <div class="bibl"><span class="ref" id="bertrand_2010"><!-- close -->Bertrand 2010</span> Bertrand, P. (2010)“Du De re diplomatica au Nouveau traité de 
                  diplomatique: réception des textes fondamentaux d’une discipline”, in Leclant, J., Vauchez, A., and Hurel, D.O. (eds) 
                  <cite class="title italic">Dom Jean Mabillon, figure majeure de l'Europe des lettres: Actes des deux colloques
                     du tricentenaire de la mort de 
                     Dom Mabillon (abbaye de Solesmes, 18-19 mai 2007</cite>, pp. 605-619. Paris: Académie des Inscriptions et Belles-Lettres.
                  </div>
               <div class="bibl"><span class="ref" id="bogacz_mara_2018"><!-- close -->Bogacz and Mara 2018</span> Bogacz, B. and Mara, H. (2018) “Feature descriptors for 
                  spotting 3D characters on triangular meshes”, <cite class="title italic">Proceedings of the 16th international conference on 
                     frontiers in handwriting recognition, IEEE, 2018.</cite>Niagara Falls, NY, USA, 5-8 August. pp. 363-368. Available at: 
                  <a href="https://ieeexplore.ieee.org/document/8583788" onclick="window.open('https://ieeexplore.ieee.org/document/8583788'); return false" class="ref">https://ieeexplore.ieee.org/document/8583788</a>
                  </div>
               <div class="bibl"><span class="ref" id="bogacz_mara_2022"><!-- close -->Bogacz and Mara 2022</span> Bogacz, B. and Mara, H. (2022) “Digital assyriology: 
                  Advances in visual cuneiform analysis”, <cite class="title italic">Journal on Computing and Cultural Heritage</cite>, 15(2). 
                  <a href="https://doi.org/10.1145/3491239" onclick="window.open('https://doi.org/10.1145/3491239'); return false" class="ref">https://doi.org/10.1145/3491239</a>.
                  </div>
               <div class="bibl"><span class="ref" id="bogacz_gertz_mara_2015a"><!-- close -->Bogacz, Gertz, and Mara 2015a</span> Bogacz, B., Gertz, M., and Mara, H. (2015a) 
                  “Cuneiform character similarity using graph representations”, <cite class="title italic">Proceedings of the 
                     15th international conference on frontiers in handwriting recognition, IEEE, 2016</cite>. Shenzhen, China, 23-26 October. pp. 326-330. 
                  <a href="https://doi.org/10.1109/ICDAR.2015.7333777" onclick="window.open('https://doi.org/10.1109/ICDAR.2015.7333777'); return false" class="ref">https://doi.org/10.1109/ICDAR.2015.7333777</a> 
                  </div>
               <div class="bibl"><span class="ref" id="bogacz_gertz_mara_2015b"><!-- close -->Bogacz, Gertz, and Mara 2015b</span> Bogacz, B., Gertz, M., and Mara, H. (2015b) 
                  “Cuneiform character similarity using graphic representations”, <cite class="title italic">Proceedings of the 
                     20th computer vision winter workshop, 2015.</cite> Seggau, Austria, 9-11 February. pp. 105-112. Available at: 
                  <a href="https://d-nb.info/1191851303/34" onclick="window.open('https://d-nb.info/1191851303/34'); return false" class="ref">https://d-nb.info/1191851303/34</a>.
                  </div>
               <div class="bibl"><span class="ref" id="bogacz_howe_mara_2016"><!-- close -->Bogacz, Howe, and Mara 2016</span> Bogacz, B., Howe, N., and Mara, H. (2016) 
                  “Segmentation free spotting of cuneiform using part structured models”, <cite class="title italic">Proceedings of 
                     the 15th international conference on frontiers in handwriting recognition, IEEE, 2016</cite>. Shenzhen, China, 23-26 October. pp. 301-306. 
                  <a href="https://doi.org/10.1109/ICFHR.2016.0064" onclick="window.open('https://doi.org/10.1109/ICFHR.2016.0064'); return false" class="ref">https://doi.org/10.1109/ICFHR.2016.0064</a>.
                  </div>
               <div class="bibl"><span class="ref" id="bramanti_2015"><!-- close -->Bramanti 2015</span> Bramanti, A. (2015) “The cuneiform stylus: Some addenda”, 
                  <cite class="title italic">Cuneiform digital library notes</cite>. Available at: 
                  <a href="https://www.academia.edu/14510982/2015_The_Cuneiform_Stylus_Some_Addenda" onclick="window.open('https://www.academia.edu/14510982/2015_The_Cuneiform_Stylus_Some_Addenda'); return false" class="ref">
                     https://www.academia.edu/14510982/2015_The_Cuneiform_Stylus_Some_Addenda</a>.
                  </div>
               <div class="bibl"><span class="ref" id="cammarosano_2014"><!-- close -->Cammarosano 2014</span> Cammarosano, M. (2014) “The cuneiform stylus”, 
                  <cite class="title italic">Mesopotamia: Rivista di Archeologia, Epigrafia e Storia Orientale Antica</cite>, 69, pp. 53–90.
                  </div>
               <div class="bibl"><span class="ref" id="cho_et_al_2016"><!-- close -->Cho et al. 2016</span> Cho, J. et al. (2016) “How much data is needed to train a 
                  medical image deep learning system to achieve necessary high accuracy?” <cite class="title italic">arXiv</cite>, 1511.06348. 
                  Available at: <a href="http://arxiv.org/abs/1511.06348" onclick="window.open('http://arxiv.org/abs/1511.06348'); return false" class="ref">http://arxiv.org/abs/1511.06348</a>.
                  </div>
               <div class="bibl"><span class="ref" id="chollet_pecinovsky_2019"><!-- close -->Chollet and Pecinovský 2019</span> Chollet, F. and Pecinovský, R. (2019) <cite class="title italic">Deep 
                     learning v Jazyku Python: Knihovny Keras, TensorFlow</cite>. Praha, Prague: Grada Publishing a.s.
                  </div>
               <div class="bibl"><span class="ref" id="cohen_et_al_2004"><!-- close -->Cohen et al. 2004</span> Cohen, J. et al. (2004) “iClay: Digitizing cuneiform”, 
                  <cite class="title italic">Proceedings of the 5th international symposium on virtual reality, archaeology and
                     cultural heritage, EG, 
                     2004.</cite> Oudenaarde, Belgium, 7-10 December. pp. 135-143. Available at: 
                  <a href="https://doi.org/10.2312/VAST/VAST04/135-143" onclick="window.open('https://doi.org/10.2312/VAST/VAST04/135-143'); return false" class="ref">https://doi.org/10.2312/VAST/VAST04/135-143</a>.
                  </div>
               <div class="bibl"><span class="ref" id="collins_et_al_2019"><!-- close -->Collins et al. 2019</span> Collins, T. et al. (2019) “Automated low-cost 
                  photogrammetric acquisition of 3D models from small form-factor artefacts”, <cite class="title italic">Electronics</cite>, 8, p. 1441. 
                  <a href="https://doi.org/10.3390/electronics8121441" onclick="window.open('https://doi.org/10.3390/electronics8121441'); return false" class="ref">https://doi.org/10.3390/electronics8121441</a>.
                  </div>
               <div class="bibl"><span class="ref" id="cammarosano_et_al_2014"><!-- close -->Commarosano et al. 2014</span> Cammarosano, M. et al. (2014) 
                  “Schriftmetrologie des Keils: Dreidimensionale Analyse von Keileindrücken und Handschriften”, 
                  <cite class="title italic">Die Welt des Orients</cite>, 44(1), pp. 2-36.
                  </div>
               <div class="bibl"><span class="ref" id="dahl_hameeuw_wagensonner_2019"><!-- close -->Dahl, Hammeeuw, and Wagensonner 2019</span> Dahl, J.L., Hameeuw, H., and Wagensonner, K. (2019) 
                  “Looking both forward and back: imaging cuneiform”, <cite class="title italic">Cuneiform digital library 
                     preprints</cite> [Preprint]. 
                  Available at: <a href="https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0" onclick="window.open('https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0'); return false" class="ref">https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0</a>.
                  </div>
               <div class="bibl"><span class="ref" id="dencker_et_al_2020"><!-- close -->Dencker et al. 2020</span> Dencker, T. et al. (2020) “Deep learning of cuneiform sign 
                  detection with weak supervision using transliteration alignment”, <cite class="title italic">PLOS ONE</cite>, 15(12), p. e0243039.
                  <a href="https://doi.org/10.1371/journal.pone.0243039" onclick="window.open('https://doi.org/10.1371/journal.pone.0243039'); return false" class="ref">https://doi.org/10.1371/journal.pone.0243039</a>.
                  </div>
               <div class="bibl"><span class="ref" id="duranti_1998"><!-- close -->Duranti 1998</span> Duranti, L. (1998) <cite class="title italic">New uses for an old science</cite>. Chicago, IL: 
                  Scarecrow Press.
                  </div>
               <div class="bibl"><span class="ref" id="earl_et_al_2011"><!-- close -->Earl et al. 2011</span> Earl, G. et al. (2011) “Reflectance transformation imaging 
                  systems for ancient documentary artefacts”, in Dunnand, S., Bowen, J.P., and Ng K.C. (eds) 
                  <cite class="title italic">EVA London 2011: Electronic visualisation and the arts</cite>, pp. 147-154. London: BCS.
                  </div>
               <div class="bibl"><span class="ref" id="edzard_1980"><!-- close -->Edzard 1980</span> Edzard, D.O. (1980) “Keilschrift”,  
                  <cite class="title italic">Reallexikon der Assyriologie</cite>, 5, pp. 544-568.
                  </div>
               <div class="bibl"><span class="ref" id="fisseler_et_al_2013"><!-- close -->Fisseler et al. 2013</span> Fisseler, D. et al. (2013) “Towards an interactive and 
                  automated script feature Analysis of 3D scanned cuneiform tablets”, <cite class="title italic">Proceedings of the scientific computing 
                     and cultural heritage conference, 2013</cite>. Heidelberg, Germany, 18-20 November. Available at: 
                  <a href="https://www.researchgate.net/publication/267921266_Towards_an_interactive_and_automated_script_feature_Analysis_of_3D_scanned_cuneiform_tablets" onclick="window.open('https://www.researchgate.net/publication/267921266_Towards_an_interactive_and_automated_script_feature_Analysis_of_3D_scanned_cuneiform_tablets'); return false" class="ref">https://www.researchgate.net/publication/267921266_Towards_an_interactive_and_automated_script_feature_Analysis_of_3D_scanned_cuneiform_tablets</a>.
                  </div>
               <div class="bibl"><span class="ref" id="fisseler_et_al_2014"><!-- close -->Fisseler et al. 2014</span> Fisseler, D. et al. (2013) “Extending philological 
                  research with methods of 3D computer graphics applied to analysis of cultural heritage”, <cite class="title italic">Proceedings of the 
                     eurographics workshop on graphics and cultural heritage, 2014</cite>. Darmstadt, Germany, 6-8 October, pp. 165-172. 
                  <a href="https://doi.org/10.2312/gch.20141314" onclick="window.open('https://doi.org/10.2312/gch.20141314'); return false" class="ref">https://doi.org/10.2312/gch.20141314</a>.
                  </div>
               <div class="bibl"><span class="ref" id="girshick_2015"><!-- close -->Girshick 2015</span> Girshick, R. (2015) “Fast R-CNN”, 
                  <cite class="title italic">Proceedings of the IEEE international conference on computer vision, IEEE, 2015</cite>. Santiago, Chile, 
                  11-18 December. pp. 1440-1448. <a href="https://doi.org/10.1109/ICCV.2015.169" onclick="window.open('https://doi.org/10.1109/ICCV.2015.169'); return false" class="ref">https://doi.org/10.1109/ICCV.2015.169</a>.
                  </div>
               <div class="bibl"><span class="ref" id="girshick_et_al_2014"><!-- close -->Girshick et al. 2014</span> Girshick, R. et al. (2014) “Rich feature hierarchies for 
                  accurate object detection and semantic segmentation”, <cite class="title italic">Proceedings of the IEEE conference on computer vision and 
                     pattern recognition, IEEE, 2014</cite>. Columbus, OH, USA, 23-28 June. 
                  <a href="https://doi.org/10.1109/CVPR.2014.81" onclick="window.open('https://doi.org/10.1109/CVPR.2014.81'); return false" class="ref">https://doi.org/10.1109/CVPR.2014.81</a>.
                  </div>
               <div class="bibl"><span class="ref" id="gordin_romach_2022"><!-- close -->Gordin and Romach 2022</span> Gordin, S. and Romach, A. (2022) “Optical character 
                  recognition for complex scripts: A case-study in cuneiform”, <cite class="title italic">Proceedings of the alliance of digital 
                     humanities organizations conference, IDHC, 2022</cite>. Tokyo, Japan, 25-29 July. Available at: 
                  <a href="https://dh-abstracts.library.cmu.edu/works/11708" onclick="window.open('https://dh-abstracts.library.cmu.edu/works/11708'); return false" class="ref">https://dh-abstracts.library.cmu.edu/works/11708</a>.
                  </div>
               <div class="bibl"><span class="ref" id="gordin_et_al_2020"><!-- close -->Gordin et al. 2020</span> Gordin, S. et al. (2020) 
                  “Reading Akkadian cuneiform using natural language processing”, <cite class="title italic">PLOS ONE</cite>, 15(10). 
                  <a href="https://doi.org/10.1371/journal.pone.0240511" onclick="window.open('https://doi.org/10.1371/journal.pone.0240511'); return false" class="ref">https://doi.org/10.1371/journal.pone.0240511</a>.
                  </div>
               <div class="bibl"><span class="ref" id="gutherz_et_al_2023"><!-- close -->Gutherz et al. 2023</span> Gutherz, G. et al. (2023) “Translating 
                  Akkadian to English with neural machine translation”, <cite class="title italic">PNAS Nexus</cite>, 2(5). 
                  <a href="https://doi.org/10.1093/pnasnexus/pgad096" onclick="window.open('https://doi.org/10.1093/pnasnexus/pgad096'); return false" class="ref">https://doi.org/10.1093/pnasnexus/pgad096</a>.
                  </div>
               <div class="bibl"><span class="ref" id="hameeuw_willems_2011"><!-- close -->Hameeuw and Willems 2011</span> Hameew, H. and Willems, G. (2011) “New 
                  visualization techniques for cuneiform texts and sealings”, <cite class="title italic">Akkadica</cite>, 132(3), pp. 163-178.
                  </div>
               <div class="bibl"><span class="ref" id="jiao_et_al_2019"><!-- close -->Jiao et al. 2019</span> Jiao, L. et al. (2019) “A survey of deep learning-based object 
                  detection”, <cite class="title italic">IEEE Access</cite>, 7, pp. 128837-128868. 
                  <a href="https://doi.org/10.1109/ACCESS.2019.2939201" onclick="window.open('https://doi.org/10.1109/ACCESS.2019.2939201'); return false" class="ref">https://doi.org/10.1109/ACCESS.2019.2939201</a>.
                  </div>
               <div class="bibl"><span class="ref" id="jocher_et_al_2020"><!-- close -->Jocher et al. 2020</span> Jocher, G. et al. (2020) “ultralytics/yolov5: Initial 
                  release”, <cite class="title italic">Zenodo</cite>. 
                  <a href="https://doi.org/10.5281/zenodo.3908560" onclick="window.open('https://doi.org/10.5281/zenodo.3908560'); return false" class="ref">https://doi.org/10.5281/zenodo.3908560</a>.
                  </div>
               <div class="bibl"><span class="ref" id="kriege_et_al_2018"><!-- close -->Kriege et al. 2018</span> Kriege, N.M. et al. (2018) “Recognizing cuneiform signs 
                  using graph based methods”, <cite class="title italic">Proceedings of the international workshop on cost-sensitive learning, PMLR, 
                     2018</cite>. San Diego, CA, USA, 5 May. pp. 31-44. Available at: 
                  <a href="https://proceedings.mlr.press/v88/kriege18a.html" onclick="window.open('https://proceedings.mlr.press/v88/kriege18a.html'); return false" class="ref">https://proceedings.mlr.press/v88/kriege18a.html</a>.
                  </div>
               <div class="bibl"><span class="ref" id="labat_1988"><!-- close -->Labat 1988</span> Labat, R. and Malbran-Labat, F. (1988) <cite class="title italic">Manuel d’épigraphie 
                     akkadienne</cite>, 6th ed. Paris: Librairie Orientaliste Paul Geuthner.
                  </div>
               <div class="bibl"><span class="ref" id="mara_kromker_2013"><!-- close -->Mara and Krömker 2013</span> Mara, H. and Krömker, S. (2013) “Vectorization of 
                  3D-characters by integral invariant filtering of high-resolution triangular meshes”, <cite class="title italic">Proceedings of the 
                     international conference on document analysis and recognition, IEEE, 2013</cite>. Washington, DC, USA, 25-28 August. pp. 62-66. 
                  <a href="https://doi.org/10.1109/ICDAR.2013.21" onclick="window.open('https://doi.org/10.1109/ICDAR.2013.21'); return false" class="ref">https://doi.org/10.1109/ICDAR.2013.21</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mara_et_al_2010"><!-- close -->Mara et al. 2010</span> Mara, H. et al. “GigaMesh and Gilgamesh: 3D multiscale integral 
                  invariant cuneiform character extraction”, <cite class="title italic">Proceedings of the 11th international symposium on virtual 
                     reality, archaeology, and cultural heritage, EG, 2010</cite>. Paris, France, 21-24 September. 
                  <a href="https://doi.org/10.2312/VAST/VAST10/131-138" onclick="window.open('https://doi.org/10.2312/VAST/VAST10/131-138'); return false" class="ref">https://doi.org/10.2312/VAST/VAST10/131-138</a>.
                  </div>
               <div class="bibl"><span class="ref" id="massa_et_al_2016"><!-- close -->Massa et al. 2016</span> Massa, J. et al. (2016) “Cuneiform detection in vectorized 
                  raster images”, <cite class="title italic">Proceedings of the 21st computer vision winter workshop, 2016</cite>. Rimske Toplice, 
                  Slovenia, 3-5 February. Available at: 
                  <a href="https://d-nb.info/1191851524/34" onclick="window.open('https://d-nb.info/1191851524/34'); return false" class="ref">https://d-nb.info/1191851524/34</a>.
                  </div>
               <div class="bibl"><span class="ref" id="mishra_2022"><!-- close -->Mishra 2022</span> Mishra, D. (2022) “Deep learning based object detection methods: A 
                  review”, <cite class="title italic">Medicon Engineering Themes</cite>, 2(4). 
                  <a href="https://doi.org/10.55162/MCET.02.027" onclick="window.open('https://doi.org/10.55162/MCET.02.027'); return false" class="ref">https://doi.org/10.55162/MCET.02.027</a>.
                  </div>
               <div class="bibl"><span class="ref" id="pavlicek_2018"><!-- close -->Pavlíček 2018</span> Pavlíček, J. et al. (2018) “Automated wildlife recognition”, 
                  <cite class="title italic">AGRIS on-line papers in economics and informatics</cite>, 10(1), pp. 51-60. 
                  <a href="https://doi.org/10.7160/aol.2018.100105" onclick="window.open('https://doi.org/10.7160/aol.2018.100105'); return false" class="ref">https://doi.org/10.7160/aol.2018.100105</a>.
                  </div>
               <div class="bibl"><span class="ref" id="punia_et_al_2020"><!-- close -->Punia et al. 2020</span> Punia, R. et al. (2020) “Towards the first machine 
                  translation system for Sumerian transliterations”, <cite class="title italic">Proceedings of the 28th international conference on 
                     computational linguistics, ACL, 2020</cite>. Barcelona, Spain, 13-18 September. pp. 3454-3460. 
                  <a href="https://doi.org/10.18653/v1/2020.coling-main.308" onclick="window.open('https://doi.org/10.18653/v1/2020.coling-main.308'); return false" class="ref">https://doi.org/10.18653/v1/2020.coling-main.308</a>.
                  </div>
               <div class="bibl"><span class="ref" id="redmon_et_al_2016"><!-- close -->Redmon et al. 2016</span> Redmon, J. et al. (2016) “You only look once: Unified 
                  real-time object detection”, <cite class="title italic">Proceedings of the IEEE conference on computer vision and pattern recognition, 
                     IEEE, 2016</cite>. Las Vegas, NV, USA, 26 June-1 July. 
                  <a href="https://doi.org/10.1109/CVPR.2016.91" onclick="window.open('https://doi.org/10.1109/CVPR.2016.91'); return false" class="ref">https://doi.org/10.1109/CVPR.2016.91</a>.
                  </div>
               <div class="bibl"><span class="ref" id="ren_et_al_2015"><!-- close -->Ren et al. 2015</span> Ren, S. et al. (2015) “Faster R-CNN: Towards real-time object 
                  detection with region proposal networks”, <cite class="title italic">Proceedings of the advances in neural processing systems 
                     conference, NeurIPS, 2015</cite>. Montreal, Canada, 7-12 December. Available at: 
                  <a href="https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" onclick="window.open('https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html'); return false" class="ref">
                     https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html</a>.
                  </div>
               <div class="bibl"><span class="ref" id="rothacker_et_al_2015"><!-- close -->Rothacker et al. 2015</span> Rothacker, L. et al. (2015) “Retrieving cuneiform 
                  structures in a segmentation-free word spotting framework”, <cite class="title italic">Proceedings of the 3rd international workshop 
                     on historical document imaging and processing, ACM, 2015</cite>. Nancy, France, 22 August. pp. 129-136. 
                  <a href="https://doi.org/10.1145/2809544.2809562" onclick="window.open('https://doi.org/10.1145/2809544.2809562'); return false" class="ref">https://doi.org/10.1145/2809544.2809562</a>.
                  </div>
               <div class="bibl"><span class="ref" id="rusakov_et_al_2019"><!-- close -->Rusakov et al. 2019</span> Rusakov, E. et al. (2019) “Generating cuneiform 
                  signs with cycle-consistent adversarial networks”, <cite class="title italic">Proceedings of the 5th international workshop on historical 
                     document imaging and processing, ACM, 2019</cite>. Sydney, Australia, 20-21 September. pp. 19-24. 
                  <a href="https://doi.org/10.1145/3352631.3352632" onclick="window.open('https://doi.org/10.1145/3352631.3352632'); return false" class="ref">https://doi.org/10.1145/3352631.3352632</a>.
                  </div>
               <div class="bibl"><span class="ref" id="rusakov_et_al_2020"><!-- close -->Rusakov et al. 2020</span> Rusakov, E. et al. (2020) “Towards query-by-expression 
                  retrieval of cuneiform signs”, <cite class="title italic">Proceedings of the 17th international conference on frontiers in handwriting 
                     recognition, IEEE, 2020</cite>. Dortmund, Germany, 7-10 September. pp. 43-48. 
                  <a href="https://doi.org/10.1109/ICFHR2020.2020.00019" onclick="window.open('https://doi.org/10.1109/ICFHR2020.2020.00019'); return false" class="ref">https://doi.org/10.1109/ICFHR2020.2020.00019</a>.
                  </div>
               <div class="bibl"><span class="ref" id="sommerschield_et_al_2023"><!-- close -->Sommerschield et al. 2023</span> Sommerschield, T. et al. (2023) “Machine 
                  learning for ancient languages: A survey”, <cite class="title italic">Computational Linguistics</cite>, 49(3), pp. 1-44. 
                  <a href="https://doi.org/10.1162/coli_a_00481" onclick="window.open('https://doi.org/10.1162/coli_a_00481'); return false" class="ref">https://doi.org/10.1162/coli_a_00481</a>.
                  </div>
               <div class="bibl"><span class="ref" id="streck_2010"><!-- close -->Streck 2010</span> Streck, M.P. (2010) “Großes Fach Altorientalistik: Der Umfang des 
                  keilschriftlichen Textkorpus”, <cite class="title italic">Mitteilungen der Deutschen Orient-Gesellschaft</cite>.
                  </div>
               <div class="bibl"><span class="ref" id="taylor_2015"><!-- close -->Taylor 2015</span> Taylor, J. (2014) “Wedge order in cuneiform: A preliminary survey”, 
                  <cite class="title italic">Proceedings of the 60th recontre assyriologique internationale, 2014</cite>. Warsaw, Poland, 21-25 July.
                  </div>
               <div class="bibl"><span class="ref" id="wagensonner_2015"><!-- close -->Wagensonner 2015</span> Wagensonner, K. (2015) “On an alternative way of capturing 
                  RTI images with the camera dome”, <cite class="title italic">CDLN</cite>, 2015(1), pp. 1-12.
                  </div>
               <div class="bibl"><span class="ref" id="wevers_smits_2019"><!-- close -->Wevers and Smits 2019</span> Wevers, M. and Smits, T. (2020) “The visual digital 
                  turn: Using neural networks to study historical images”, <cite class="title italic">Digital Scholarship in the Humanities</cite>, 35(1), 
                  pp. 194-207. <a href="https://doi.org/10.1093/llc/fqy085" onclick="window.open('https://doi.org/10.1093/llc/fqy085'); return false" class="ref">https://doi.org/10.1093/llc/fqy085</a>.
                  </div>
               <div class="bibl"><span class="ref" id="yamauchi_et_al_2018"><!-- close -->Yamauchi et al. 2018</span> Yamauchi, K., Yamamoto, H., and Mori, W. (2018) 
                  “Building a handwritten cuneiform character image set”, <cite class="title italic">Proceedings of the 11th 
                     international conference on language resources and evaluation, ACL, 2018</cite>. Miyazaki, Japan, 7-12 May. pp. 719-722. 
                  Available at: <a href="https://aclanthology.org/L18-1115" onclick="window.open('https://aclanthology.org/L18-1115'); return false" class="ref">https://aclanthology.org/L18-1115</a>.
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>