<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:array="http://www.w3.org/2005/xpath-functions/array"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:map="http://www.w3.org/2005/xpath-functions/map"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">Cuneiform Stroke Recognition and Vectorization in 2D Images</title>
            <dhq:authorInfo>
               <dhq:author_name>Adéla <dhq:family>Hamplová</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-1012-650X</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>hamplova@pef.czu.cz</email>
               <dhq:bio>
                  <p>Adéla Hamplová is a doctoral student and lecturer in the field of informatics and system engineering with an emphasis on 
                     artificial intelligence at the Czech University of Life Sciences in Prague. She earned her Master's degree at the same 
                     institution, establishing a strong foundation in computer science. Her research focuses on the innovative application of 
                     deep learning and Convolutional Neural Networks for the recognition of Palmyrene characters and cuneiform in historical 
                     documents. Her passion for technology extends beyond her research, including interests in application development, 3D printing, 
                     and technical innovations. She has contributed to the field through her publications at multiple conferences and in the 
                     journal <title rend="italic">Neural Network World</title>, as well as her reviews for the same journal. Her explorations of 
                     deep learning methods and creations of practical applications show her dedication to expanding knowledge boundaries of artificial 
                     intelligence and contributing meaningfully to the scientific community.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Avital <dhq:family>Romach</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0001-9199-3228</idno>
               <dhq:affiliation>Yale University</dhq:affiliation>
               <email>avital.romach@yale.edu</email>
               <dhq:bio>
                  <p>Avital Romach is a PhD student of Assyriology at Yale University. Her second focus is on the implementation of digital 
                     humanities methodologies in traditional philological research and ancient language processing. She wrote her MA at 
                     Tel Aviv University on the fifth tablet of the <title rend="italic">Epic of Gilgamesh</title>, and she has published articles 
                     on computational and machine learning methods used to study ancient cuneiform inscriptions and texts.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Josef <dhq:family>Pavlíček</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-3959-5406</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>pavlicek@pef.czu.cz</email>
               <dhq:bio>
                  <p>Josef Pavlicek, Ph.D., is an assistant professor at the Department of Information Engineering at the Czech University of 
                     Life Sciences in Prague, and a lecturer at the Department of Software Engineering at the Czech Technical University, 
                     Faculty Information Technology. His areas of expertise are computer vision, implementation of artificial intelligence 
                     algorithms in image recognition, and music pattern detection.  He is a researcher and creator of the intelligent robotic 
                     agents educational game <title rend="italic">FactOrEasy.cz</title>.  Currently, Josef is involved in a project on weed plant 
                     recognition using a robotic platform in the specialization of machine vision and image processing automation and cooperation 
                     with other teams, especially in the areas of algorithm development.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Arnošt <dhq:family>Veselý</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0001-8979-1336</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>vesely@pef.czu.cz</email>
               <dhq:bio>
                  <p>Arnost Vesely is an associate professor of computer science at the University of Live Sciences in Prague. His research interests 
                     include deep learning, natural language processing, computer vision, bioinformatics, and artificial intelligence applications in 
                     agriculture and environmental sciences.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Martin <dhq:family>Čejka</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-2909-486X</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>cejkamartin@pef.czu.cz</email>
               <dhq:bio>
                  <p>Martin Cejka, a PhD candidate in Information Engineering at the Czech University of Life Sciences in Prague, holds a Master's 
                     degree in Information and Control Technology. His expertise lies in machine learning, demonstrated through his work on 
                     <title rend="quotes">Preprocessing Audiovisual Data Using Computer Vision to Recognize UI Elements</title>. Currently involved 
                     in ML projects, Martin focuses on SETI data analysis, unsupervised topic modeling for social networks, and eyetracking 
                     applications.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>David <dhq:family>Franc</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0003-3160-9559</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>francd@pef.czu.cz</email>
               <dhq:bio>
                  <p>David Franc is a doctoral student in the Information Engineering department at Czech University of Life Sciences in Prague. His 
                     research focuses on image object recognition. From 2021 to 2023 he participated in research addressing cuneiform recognition, as 
                     well as Palmyrene alphabet recognition, with his contribution in the area of augmenting learning datasets using GANs.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Shai <dhq:family>Gordin</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-8359-382X</idno>
               <dhq:affiliation>Ariel University; Open University of Israel</dhq:affiliation>
               <email>shaigo@ariel.ac.il</email>
               <dhq:bio>
                  <p>Shai Gordin is a senior lecturer for ancient Near Eastern history and digital humanities at Ariel University, and a visiting 
                     professor at the Digital Humanities and Social Sciences Hub (DHSS Hub) at the Open University of Israel. He is the PI of the 
                     Babylonian Engine project and heads the Digital Pasts Lab, which conducts research in the computational analysis of ancient 
                     texts and artefacts (<ref target="https://digitalpasts.github.io/docs/projects.html">
                     https://digitalpasts.github.io/docs/projects.html</ref>). He is the co-initiator of the Digital Ancient Near Eastern Studies 
                     Network (DANES), co-organizer of the Ancient Language Processing Workshop at RANLP2023, and co-organizer of the first international 
                     shared task of ancient cuneiform languages translation at MT-SUMMIT2023. He currently serves on the editorial board of 
                     <title rend="italic">PLOS ONE</title>.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000733</idno>
            <idno type="volume">018</idno>
            <idno type="issue">1</idno>
            <date when="2024-02-23">23 February 2024</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#machine_learning"/>
               <term corresp="#digitization"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item>cuneiform</item>
                  <item>convolutional neural networks</item>
                  <item>artificial intelligence</item>
                  <item>OCR</item>
                  <item>object detection</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000733/000733.xml">GitHub
        	   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <p>A vital part of the publication process of ancient cuneiform tablets is creating hand-copies, which are 2D line art representations of 
               the 3D cuneiform clay tablets, created manually by scholars. This research provides an innovative method using Convolutional Neural 
               Networks (CNNs) to identify strokes, the constituent parts of cuneiform characters, and display them as vectors — semi-automatically 
               creating cuneiform hand-copies. This is a major step in optical character recognition (OCR) for cuneiform texts, which would contribute 
               significantly to their digitization and create efficient tools for dealing with the unique challenges of Mesopotamian cultural heritage. 
               Our research has resulted in the successful identification of horizontal strokes in 2D images of cuneiform tablets, some of them from 
               very different periods, separated by hundreds of years from each other. With the Detecto algorithm, we achieved an F-measure of 81.7% 
               and an accuracy of 90.5%. The data and code of the project are available on GitHub.
            </p>
         </dhq:abstract>
         <dhq:teaser>
            <p>An innovative approach towards cuneiform OCR of identifying strokes instead of signs offers insight into the challenges and 
               methodologies of quantitative epigraphy.
            </p>
         </dhq:teaser>
      </front>
      <body>
        <div>
          <head>1 Introduction</head>
            <div>
              <head>1.1 Cuneiform Texts and Artificial Intelligence</head>
                <p>Cuneiform is one of the earliest attested writing systems, and for thousands of years cuneiform has also been the dominant script 
                   of the ancient Middle East, a region stretching roughly from the Persian Gulf to modern Turkey's highlands, and south across the 
                   Levant into Egypt. Cuneiform texts appeared from the end of the fourth millennium BCE until they fell out of use in the early 
                   centuries CE. The script was used for writing a plethora of different documents: legal, administrative, and economic documents; 
                   correspondence between private individuals, or high-officials and their kings; some of the oldest works of literature; royal 
                   inscriptions describing the deeds of great kings; as well as lexical and scientific compendia, some of which form the basis of the 
                   Greco-Roman sciences the Western world is built upon today. Hundreds of thousands of cuneiform documents have been discovered since 
                   excavations began in the 1850s. Recent estimates indicate the cuneiform text corpus is second in size only to that of ancient Greek 
                   <ptr target="#streck_2010"/>.
                </p>
                <p>The rising application of artificial intelligence to various tasks provides a prime opportunity for training object detection 
                   models to assist the digital publication of cuneiform texts on a large scale. This will help set up a framework for cultural 
                   heritage efforts of preservation and knowledge dissemination that can support the small group of specialists in the field.
                </p>
                <p>Cuneiform provides unique challenges for object detection algorithms, particularly OCR methods. Cuneiform tablets, on which the 
                   texts were written, are 3D objects: pieces of clay which were shaped to particular sizes. While the clay was still moist, scribes 
                   used styli with triangular edges to create impressions on the clay in three possible directions: horizontal, vertical, or oblique 
                   (also <term>Winkelhaken</term>; see Figure 1). <q>Diagonal</q> strokes are also found in the literature, but these are technically 
                   either another type of horizontal or an elongated oblique impression <ptr target="#cammarosano_2014"/> 
                   <ptr target="#cammarosano_et_al_2014"/> <ptr target="#bramanti_2015"/>. Each of these impressions is called a stroke (or wedge, due 
                   to their shape). Combinations of different strokes create characters, usually referred to as signs <ptr target="#taylor_2015"/>. 
                   Cuneiform can be more easily read when there is direct light on the tablet, especially from a specific angle that casts shadows on 
                   the different strokes.
                </p>
                <figure>
                  <head>The main cuneiform strokes taken from Neo-Babylonian signs, from left to right: AŠ (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341</ref>), 
                    DIŠ (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474</ref>), and U or <term>Winkelhaken</term> 
                    (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430</ref>), as recorded in the LaBaSi palaeographical database.</head>
                  <graphic url="resources/images/figure01.png" style="width: 1000px"/>
                </figure>
                <p>From the inception of research in cuneiform studies, tablets were difficult to represent in a modern 2D publishing format. Two 
                   solutions were found:
                   <list style="unordered">
                     <item>When possible, 2D images of cuneiform tablets were taken. However, for most of the field's history, such images were  
                           extremely costly to produce and print, and they were often not in sufficient quality for easy sign identification (for 
                           the history of early photography and cuneiform studies, see <ptr target="#brusius_2015"/>).
                      </item>
                      <item>The second solution was creating hand-copies, 2D black and white line art made by scholars of the tablets' strokes. This 
                            was the most popular solution. The disadvantage of this method is that it adds a layer of subjectivity, based on what the 
                            scholar has seen and on their steady drawing hand. Nowadays these hand-copies are still in use, often drawn using vectors 
                            in special programs (the most popular being the open source vector graphics editor 
                            <ref target="https://inkscape.org/">Inkscape</ref>).
                      </item>
                   </list>
                </p>
                <p>In recent years, the quality of 2D images has risen significantly, while the costs of production and reproduction dropped. A 
                   constantly growing number of images of cuneiform tablets are currently available in various online databases. The largest 
                   repositories are the <ref target="https://www.britishmuseum.org/collection"><name>British Museum</name></ref>, the 
                   <ref target="https://collections.louvre.fr/"><name>Louvre Museum</name></ref>, the <ref target="https://cdli.ucla.edu/"><name>Cuneiform 
                   Digital Library Initiative</name></ref>, the <ref target="https://www.ebl.lmu.de/"><name>Electronic Babylonian Library</name></ref>, and the 
                   <ref target="https://babylonian-collection.yale.edu/"><name>Yale Babylonian Collection</name></ref>. 2D+ and 3D models of cuneiform tablets have 
                   also become a possibility, although these are still more expensive and labour-intensive to produce <ptr target="#earl_et_al_2011"/> 
                   <ptr target="#hameeuw_willems_2011"/> <ptr target="#collins_et_al_2019"/>; cf. overview in <ptr target="#dahl_hameeuw_wagensonner_2019"/>.
                </p>
                <p>Previous research of identifying cuneiform signs or strokes have used mostly 3D models. Two research groups have developed programs 
                   for manipulating 3D models of cuneiform tablets: the 
                   <ref target="https://www.hethport.uni-wuerzburg.de/HPM/hpm.php?p=3djoins">CuneiformAnalyser</ref> <ptr target="#fisseler_et_al_2013"/> 
                   <ptr target="#rothacker_et_al_2015"/> and <ref target="https://gigamesh.eu/?page=home">GigaMesh</ref> <ptr target="#mara_et_al_2010"/>. 
                   Each group developed stroke extraction through geometrical features identification, while one team also used the 3D models 
                   for joining broken tablet fragments <ptr target="#fisseler_et_al_2014"/>. In addition, the GigaMesh team extracted strokes as 
                   Scalable Vector Graphic (SVG) images <ptr target="#mara_kromker_2013"/>, which practically means creating hand-copies automatically 
                   as vector images. This was used as a basis for querying stroke configurations when looking for different examples of the same sign, 
                   using graph similarity methods <ptr target="#bogacz_gertz_mara_2015a"/> <ptr target="#bogacz_gertz_mara_2015b"/> 
                   <ptr target="#bogacz_howe_mara_2016"/> <ptr target="#bogacz_mara_2018"/> <ptr target="#kriege_et_al_2018"/>.
                </p>
                <p>Work on hand-copies includes transforming raster images of hand-copies into vector images (SVG) <ptr target="#massa_et_al_2016"/>. 
                   Hand-copies and 2D projections of 3D models were used for querying signs by example using CNNs with data augmentation 
                   <ptr target="#rusakov_et_al_2019"/>. Previous work on 2D images has only recently started. Dencker et al. used 2D images for 
                   training a weakly supervised machine learning model in the task of sign detection in a given image 
                   <ptr target="#dencker_et_al_2020"/>. <name>Rusakov</name> et al. used 2D images of cuneiform tablets for querying cuneiform signs by 
                   example and by schematic expressions representing the stroke combinations <ptr target="#rusakov_et_al_2020"/>. A more comprehensive 
                   survey of computational methods in use for visual cuneiform research can be found in <ptr target="#bogacz_mara_2022"/>.
                </p>
                <p>As there are no published attempts to extract strokes directly from 2D images of cuneiform tablets, the purpose of this paper is a 
                   proof of concept to show it is possible to extract and vectorize strokes from 2D images of cuneiform using machine learning methods. 
                   The quantity and quality of 2D images is improving, and for the most part they provide a more accurate representation of the tablet 
                   than hand-copies, as well as being cheaper and quicker to produce in comparison to 3D models. Furthermore, since there are only 
                   three basic types of strokes, but hundreds of signs and variants, one can label a significantly smaller number of tablets to attain 
                   a sufficient number of strokes for training machine learning models. The resulting model will be able to recognize strokes in 
                   cuneiform signs from very different periods, separated by hundreds of years. This semi-automation of hand-copies will be a 
                   significant step in the publication of cuneiform texts and knowledge distribution of the history and culture of the ancient Near 
                   East. Our data and code are available on GitHub.<note>See Hamplova, A., Franc, D., Pavlicek, J., Romach, A., Gordin, S., Cejka, M., 
                   and Vesely, A. (2022) <title rend="quotes">adelajelinkova/cuneiform</title>, <title rend="italic">GitHub</title>, available at: 
                   <ref target="https://github.com/adelajelinkova/cuneiform">https://github.com/adelajelinkova/cuneiform</ref>.</note>
                </p>
            </div>
            <div>
              <head>1.2 Object Detection</head>
                <p>Identifying cuneiform signs or strokes in an image is considered an object detection task in computer vision. Object detection 
                   involves the automatic identification and localization of multiple objects within an image or a video frame. Unlike simpler tasks 
                   such as image classification, where the goal is to assign a single label to an entire image, object detection aims to provide more 
                   detailed information by detecting and delineating the boundaries of individual objects present. Object detection algorithms work by 
                   analysing the contents of an image and searching for specific patterns or features that are associated with the object. These 
                   patterns or features can include things like color, texture, shape, and size.
                </p>
                <p>There are different types of computational models for object detection, ranging from the purely mathematical to deep learning 
                   models <ptr target="#wevers_smits_2019"/>. <term>Mathematical models</term> often involve traditional computer vision techniques that 
                   rely on well-defined algorithms and handcrafted features. These methods typically follow a series of steps to detect objects in an 
                   image. First is extracting relevant features from the image (edges, corners, textures, or color information), where the algorithms 
                   are usually adapted based on domain knowledge. Then mathematical operations are performed to determine the location and extent of 
                   potential objects based on the identified features. Further methods can be used in post-processing to refine the results. 
                   Computational methods work best in ideal or near-ideal conditions, meaning there needs to be standardization in the types of cameras 
                   used for taking the images, the lighting situation, and the background. The objects themselves should also be as uniform as possible 
                   in size, shape, and color. This means that in complex and diverse real-world scenarios, mathematical models are often insufficient 
                   for satisfactory results.
                </p>
                <p>
                  <term>Deep learning models</term>, particularly convolutional neural networks (CNNs), have revolutionized object detection 
                  <ptr target="#girshick_et_al_2014"/>. Instead of manual feature engineering used by mathematical models, deep learning methods can 
                  automatically detect relevant features and objects. The models are trained on labelled data: a set of images where the objects of 
                  interest have been marked, usually in rectangular bounding boxes, by humans. After training, the models can be tested and used on 
                  unseen images that were not in the labelled training dataset to detect the same type of objects. Their biggest advantage is that they 
                  can handle a wide range of object shapes, sizes, and orientations, making them adaptable to diverse scenarios, and generalize well 
                  across different datasets. The disadvantages of such models are that they require large amounts of labelled data for effective 
                  training; they are more computationally intensive compared to traditional mathematical models, requiring more computational power 
                  outside the scope of the average computer; and they are black boxes, meaning it is not always possible to explain why the model makes 
                  a certain prediction or not.
                </p>
                <p>In a previous research project, we combined the use of mathematical and deep learning object detection methods for wildlife mapping 
                   <ptr target="#pavlicek_2018"/>. However, for this project, mathematical models proved insufficient for the complexity and 
                   variability of images of cuneiform tablets, which include different tablet shapes, colors, broken sections, etc. Therefore, in 
                   this article we present our results on stroke recognition for 2D images of cuneiform tablets using several deep learning models, and 
                   compare their advantages and disadvantages for this type of object detection on ancient and complex writing systems.
                </p>
                  <div>
                    <head>1.2.1 Convolutional Neural Networks</head>
                      <p>Convolutional neural networks (CNNs) are multilayer networks, specifically designed for processing and analyzing visual data. 
                         The convolutional layers in the network process the input image through different filters that help the network detect features 
                         of interest like edges, corners, textures, etc. The additional layers process the resulting feature maps to detect relevant 
                         combinations of features. There can be several iterations of convolutional layers, depending on the specific architecture of 
                         the neural network used.
                      </p>
                      <p>There are two main types of convolutional neural networks: two-stage detectors and single-stage detectors 
                         <ptr target="#jiao_et_al_2019"/>. Two-stage detectors, like Faster R-CNN (Region-based Convolutional Neural Network), first 
                         identify regions of the image that might contain objects before analyzing those regions more closely to detect objects 
                         <ptr target="#girshick_et_al_2014"/>. Single-stage detectors, like YOLO (You Only Look Once; 
                         <ptr target="#redmon_et_al_2016"/>), can detect objects directly without first identifying regions of interest. In what 
                         follows, we provide a brief overview of the advantages and disadvantages of both methods. See also <ptr target="#mishra_2022"/>.
                      </p>
                        <div>
                          <head>1.2.1.1 YOLO</head>
                            <p>YOLO, short for You Only Look Once, is a family of convolutional neural network architectures that was first introduced 
                               in 2015 by <name>Joseph Redmon</name> et al <ptr target="#redmon_et_al_2016"/>. The version used in this paper, YOLOv5, was published in 2020 
                               <ptr target="#jocher_et_al_2020"/>. The YOLOv5 architecture consists of 232 layers<note>See Ultralytics/Yolov5 (2021) 
                               <title rend="quotes">Yolov5</title>, <title rend="italic">GitHub</title>, available at: 
                               <ref target="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</ref>.</note>, multiple 
                               convolutional layers that extract features from the image at different scales, and a series of prediction layers, which 
                               output the object detection results.
                            </p>
                            <p>The algorithm divides the input image into a grid of cells, with each cell responsible for predicting the presence of 
                               one or more objects. For each cell, the network predicts the confidence score, which reflects the likelihood that an 
                               object is present, and the bounding box coordinates that describe the location and size of the object.
                            </p>
                            <p>The YOLOv5 algorithm has achieved state-of-the-art performance on several benchmark datasets. Its main advantage has 
                               been speed: YOLO performs significantly faster than other CNN models. It has become a popular choice for a wide range of 
                               applications in computer vision, including object detection in real-time video streams, autonomous driving, and 
                               surveillance systems. The latest version at the time of publication is YOLOv8.<note>See Jocher, G., Chaurasia, A., and 
                               Qiu, J. (2023) <title rend="quotes">YOLO by Ultralytics (Version 8.0.0)</title>, <title rend="italic">GitHub</title>, 
                               available at: <ref target="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</ref>.</note>
                            </p>
                        </div>
                        <div>
                          <head>1.2.1.2 R-CNN, Fast R-CNN, and Faster R-CNN</head>
                            <p>Region-based Convolutional Neural Networks (R-CNN) were first introduced in 2014 by <name>Ross Girshick</name> et al 
                               <ptr target="#girshick_et_al_2014"/>. This type of detector has four key components: (1) it generates region proposals that 
                               suggest potential object locations in an image using a selective search method; (2) it extracts fixed-length feature vectors 
                               from each of these proposed regions; (3) for each region of interest, it computes relevant features for object 
                               identification; and (4) based on the extracted CNN features, the regions of interest are classified (see Figure 2).
                            </p>
                            <figure>
                              <head>The components of the R-CNN detector, from <ptr target="#girshick_et_al_2014"/>.</head>
                                <graphic url="resources/images/figure02.png" style="width: 1000px"/>
                            </figure>
                            <p>A year later, <name>Girshick</name> proposed a more efficient version called Fast R-CNN <ptr target="#girshick_2015"/>. In 
                               contrast to R-CNN, which processes individual region suggestions separately through the CNN, Fast R-CNN computes features 
                               for the entire input image at once. This significantly speeds up the process and allows for better use of storage space 
                               for feature storage.
                            </p>
                            <p>Building on the improvements of Fast R-CNN, Faster R-CNN was introduced just three months later 
                               <ptr target="#ren_et_al_2015"/>. It introduced the region proposal network (RPN), which generates regions of interest 
                               (RoI), potential identifications of the desired objects. It does so by sliding a small window (known as an anchor) over 
                               the feature maps and predicting whether the anchor contains an object or not. For this project, we used a combination of 
                               Faster R-CNN and ResNet-50, a deep learning architecture that optimizes the network's performance. This model was 
                               implemented by <name>Bi</name> in Detecto python library, using the PyTorch python framework.<note>See Bi, A. (2020)  
                               alankbi/detecto <title rend="quotes">Build fully-functioning computer vision models with PyTorch</title>, 
                               <title rend="italic">GitHub</title>, available at: 
                               <ref target="https://github.com/alankbi/detecto">https://github.com/alankbi/detecto</ref>.</note>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div>
            <head>2 Dataset and Evaluations</head>
              <div>
                <head>2.1 Dataset</head>
                  <div>
                    <head>2.1.1. Dataset Creation, Division, and Augmentation</head>
                      <p>The Assyriologists on our team tagged thousands of horizontal strokes in eight tablets from the <name>Yale Babylonian 
                         Collection</name> (see Table 1), made available through the kind permission of <name>Agnete W. Lassen</name> and <name>Klaus 
                         Wagensonner</name>. For the first stage of research, we labelled 7,355 horizontal strokes in the tablets chosen, divided into 
                         823 images.
                      </p>
                      <table>
                        <head>Table showing the eight tablets that were labelled and their metadata. The information is taken from the 
                          <ref target="https://babylonian-collection.yale.edu/"><name>Yale Babylonian Collection</name> website</ref>. <q>CDLI ID</q> 
                          refers to the catalogue number in the 
                          <ref target="https://cdli.ucla.edu/"><name>Cuneiform Digital Library Initiative</name></ref> database. The publication 
                          abbreviations in the column labelled "Hand-Copy Publication" follow the 
                          <ref target="https://rla.badw.de/en/reallexikon/abkuerzungslisten/literatur-und-koerperschaften.html"><name>Reallexikon 
                          der Assyriologie</name> online list</ref>.</head>
                          <row role="label">
                            <cell>Yale ID</cell>
                            <cell>CDLI ID</cell>
                            <cell>Material</cell>
                            <cell>Period</cell>
                            <cell>Genre</cell>
                            <cell>Content</cell>
                            <cell>Hand-Copy Publication</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-014442">YPM BC 014442</ref></cell>
                           <cell><ref target="https://cdli.ucla.edu/P504832">P504832</ref></cell>
                           <cell>Clay</cell>
                           <cell>Neo-Assyrian (ca. 911-612 BCE)</cell>
                           <cell>Literary</cell>
                           <cell>Enuma Eliš ll. 1-16, 143-61</cell>
                           <cell>CT 13 1,3</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-023856">YPM BC 023856</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P293426">P293426</ref></cell>
                           <cell>Clay</cell>
                           <cell>Old-Babylonian (ca. 1900-1600 BCE)</cell>
                           <cell>Literary</cell>
                           <cell>Gilgamesh and Huwawa ll. 1-36</cell>
                           <cell>JCS 1 22-23</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-002575">YPM BC 002575</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P297024">P297024</ref></cell>
                           <cell>Clay</cell>
                           <cell>Neo/Late-Babylonian (ca. 626-63 BCE)</cell>
                           <cell>Commentary</cell>
                           <cell>Iqqur īpuš i 36, ii 31, iii 22, iv 5, v 13</cell>
                           <cell>BRM 4 24</cell>
                         </row>
                         <row role="data">
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016773">YPM BC 016773</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P293444">P293444</ref></cell>
                           <cell>Limestone</cell>
                           <cell>Early Old-Babylonian (ca. 2000-1900 BCE)</cell>
                           <cell>Inscription</cell>
                           <cell>Building inscription of Anam, No. 4</cell>
                           <cell>YOS 1 36</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016780">YPM BC 016780</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P293445">P293445</ref></cell>
                           <cell>Limestone</cell>
                           <cell>Early Old-Babylonian (ca. 2000-1900 BCE)</cell>
                           <cell>Inscription</cell>
                           <cell>Building inscription of Anam, No. 2</cell>
                           <cell>YOS 1 35</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016869">YPM BC 016869</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P429204">P429204</ref></cell>
                           <cell>Clay</cell>
                           <cell>Middle Assyrian (ca. 1400-1000 BCE)</cell>
                           <cell>Inscription</cell>
                           <cell>Inscription of Aššur-nadin-apli</cell>
                           <cell>YOS 9 71</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-021204">YPM BC 021204</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P308129">P308129</ref></cell>
                           <cell>Clay</cell>
                           <cell>Middle Assyrian? (ca. 1400-1000 BCE)</cell>
                           <cell>Medical Text</cell>
                           <cell/>
                           <cell>FS Sachs 18, no. 16</cell>
                         </row>
                         <row>
                           <cell><ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-021234">YPM BC 021234</ref></cell>
                           <cell><ref target="http://cdli.ucla.edu/P308150">P308150</ref></cell>
                           <cell>Clay</cell>
                           <cell>Old-Babylonian (ca. 1900-1600 BCE)</cell>
                           <cell>Hymn</cell>
                           <cell>Hymn to Inanna-nin-me-šar2-ra, ll. 52-102</cell>
                           <cell>YNER 3 6-7</cell>
                         </row>
                     </table>
                     <p>To train an artificial neural network, a dataset divided into training, validation, and test subsets needs to be created. In 
                        order to increase the number of images in the dataset, several augmentation methods were used. The recommended number of images 
                        for each class is at least a thousand images <ptr target="#cho_et_al_2016"/>. Roboflow<note>See Approboflowcom (2021) 
                        <title rend="quotes">Roboflow Dashboard</title>, <title rend="italic">GitHub</title>, available at: 
                        <ref target="https://app.roboflow.com/">https://app.roboflow.com/</ref>.</note> is a web application used to create extended 
                        datasets from manually labelled data using labelling tools such as LabelImg.<note>See tzutalin/labelIm (2021)  
                        <title rend="quotes">LabelImg</title>, <title rend="italic">Github</title>, available at: 
                        <ref target="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</ref>.</note>
                     </p>
                     <p>For pre-processing, the images were divided into equal squares of 416 x 416 pixels (for Detecto and YOLOv5). For R-CNN, the 
                        size of images was downsized to 224 x 224 pixels. For the final version of the model, the images were augmented using Roboflow. 
                        The final dataset contains 1,975 images with more than 20,000 labels. The augmented horizontal stroke dataset is available 
                        online.<note>See Roboflow (2021) <title rend="quotes">Augmented Horizontal Dataset</title>, available at: 
                        <ref target="https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV">https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV</ref>.</note>
                     </p>
                 </div>
                 <div>
                  <head>2.1.2 Labelling Criteria</head>
                    <p>For machine learning purposes, the images of the cuneiform tablets needed to be split into squares of equal size (see Appendix). 
                       Labelling was performed after splitting the images. This meant the loss of a lot of the context necessary to identify strokes 
                       with certainty. We used tablet images with existing hand-copies, which were used as a guide and previous interpretations of the 
                       tablets.
                    </p>
                    <p>However, a greater emphasis was given to what is currently visible on the image than what appears in the hand-copy. The 
                       hand-copies were not always true to what is seen on the images for three main reasons: (1) the hand-copy preserves signs which 
                       were visible on the tablet at the moment of their creation, but by the time the image was taken, they had eroded; (2) the 
                       camera angle when taking the image did not capture all the detail the tablet contains. This is a common scenario, since signs 
                       at the edges of the tablet will not be seen as clearly when taking a frontal image; and (3) strokes may have been cut off where 
                       the image was split.
                    </p>
                    <p>If a stroke was completely unrecognizable as a horizontal stroke on the image at hand, because of either of the aforementioned 
                       restrictions, it was not labelled. If enough of the characteristic features of the stroke (particularly its triangular head) 
                       were present on the image, it was labelled. Being able to identify partially broken strokes is still useful for real-life 
                       scenarios, since the tablets themselves are often broken, a common problem in sign identification.
                    </p>
                    <p>Additionally, strokes which are usually considered diagonal were also labelled. A relative leniency was given to this issue, 
                       since in general, the lines on cuneiform tablets are not always straight (i.e., creating a 90° angle with the tablet itself). 
                       Therefore, a horizontal stroke that may be exactly horizontal when viewed in its line will appear diagonal on the image if the 
                       lines themselves are somewhat diagonal. For an example of labelled images, see Figure 3.
                    </p>
                    <figure>
                      <head>Training set example of labelled horizontal strokes on tablet YPM BC 014442.</head>
                        <graphic url="resources/images/figure03.png" style="width: 1000px"/>
                    </figure>
                </div>
            </div>
            <div>
              <head>2.2 Evaluation Metrics</head>
                <p>Standard evaluation metrics include precision, sensitivity, and F-measure, calculated from true positive rate (TP), false positive 
                   rate (FP), and false negative rate (FN). These are displayed in Table 2. From these, the following metrics can be calculated to 
                   quantitatively assess the efficacy of the model.
                </p>
                <table>
                  <head><q>TP</q> refers to the the proportion of cases that are correctly identified as positive by the model. <q>FP</q> marks the 
                    proportion of cases that are incorrectly classified as positive by the model. <q>FN</q> reflects the proportion of cases that are 
                    incorrectly identified as negative by the model. <q>TN</q> indicates the proportion of cases that are correctly identified as 
                    negative.</head>
                    <row role="label">
                      <cell role="label"/>
                      <cell role="label">Predicted Positive</cell>
                      <cell role="label">Predicted Negative</cell>
                    </row>
                    <row>
                      <cell>Actual Positive</cell>
                      <cell>True Positive (TP)</cell>
                      <cell>False Negative (FN)</cell>
                    </row>
                    <row>
                      <cell>Actual Negative</cell>
                      <cell>False Positive (FP)</cell>
                      <cell>True Negative (TN)</cell>
                    </row>
                </table>
                <p>Accuracy or Precision (p): Precision measures how many of the total number of predicted positive cases are true positives. In other 
                   words, it is the ratio of true positives to the total number of predicted positive cases, whether true or false.
                   <formula rend="block" notation="tex">$$p = \frac{TP}{TP+FP}$$</formula>
                </p>
                <p>Sensitivity or Recall (s): Sensitivity measures how many of the true positive cases are correctly identified as positive by the 
                   model. In other words, it is the ratio of true positives to the total number of positive cases, which includes both true positives and 
                   false negatives.
                  <formula rend="block" notation="tex">$$s = \frac{TP}{TP+FN}$$</formula>
                </p>
                <p>F-measure (F1): The F1 measure combines precision and sensitivity into a single score that is commonly used as the final 
                   assessment of a model. It is the harmonic mean of precision and sensitivity, calculated as two times the product of precision and 
                   sensitivity divided by their sum, resulting in a number between 0 and 1 that can be viewed as a percentage of overall 
                   accuracy.
                  <formula rend="block" notation="tex">$$F = \frac{2×p×s}{p+s}$$</formula>
                </p>
            </div>
        </div>
        <div>
          <head>3 Results</head>
            <p>Our goal was to test several types of object detectors to compare which one gives the best results for our task. According to 
               theoretical comparisons on public datasets, one-stage algorithms (here YOLOv5) should give faster but less accurate results compared to 
               two-stage detectors (here Detecto and R-CNN).
            </p>
            <p>While testing with the YOLOv5 detector took only 1.189 seconds, the overall accuracy was just over 40%, which is not sufficient for 
               practical usability. The prediction using the R-CNN network took on average 45 seconds, but the results did not even reach the 
               YOLOv5 level. We believe that this was due to a lack of tuning of the hyperparameters and may be the subject of further experiments. 
               Detecto, which was not as fast as YOLOv5 but not as slow as R-CNN, achieved results that far outperformed both previous algorithms with 
               its 90.5% sensitivity and 81.7% F-score. The reason behind this fact may be that Detecto is an optimised network that combines the 
               principles of a two-stage detector with ResNet. Detailed evaluation results are shown in Table 3, Figure 4, and Table 4.
            </p>
            <table>
              <head>Evaluation results.</head>
                <row role="label">
                  <cell role="label">Name</cell>
                  <cell role="label">TP</cell>
                  <cell role="label">FN</cell>
                  <cell role="label">FP</cell>
                  <cell role="label">p</cell>
                  <cell role="label">s</cell>
                  <cell role="label">F1</cell>
                  <cell role="label">Fake of All Found Strokes</cell>
                </row>
                <row>
                  <cell role="label">Detecto (Threshold 0.4)</cell>
                  <cell>669</cell>
                  <cell>70</cell>
                  <cell>229</cell>
                  <cell>74.4%</cell>
                  <cell>90.5%</cell>
                  <cell>81.7%</cell>
                  <cell>25.5%</cell>
                </row>
                <row>
                  <cell role="label">YOLOv5 (Threshold 0.2)</cell>
                  <cell>323</cell>
                  <cell>419</cell>
                  <cell>444</cell>
                  <cell>42.1%</cell>
                  <cell>43.5%</cell>
                  <cell>42.8%</cell>
                  <cell>57.8%</cell>
                </row>
                <row>
                  <cell role="label">YOLOv5 (Threshold 0.3)</cell>
                  <cell>256</cell>
                  <cell>486</cell>
                  <cell>305</cell>
                  <cell>45.6%</cell>
                  <cell>34.5%</cell>
                  <cell>39.2%</cell>
                  <cell>54.4%</cell>
                </row>
                <row>
                  <cell role="label">YOLOv5 (Threshold 0.4)</cell>
                  <cell>196</cell>
                  <cell>546</cell>
                  <cell>190</cell>
                  <cell>50.7%</cell>
                  <cell>26.4%</cell>
                  <cell>34.7%</cell>
                  <cell>49.2%</cell>
                </row>
                <row>
                  <cell role="label">R-CNN (Threshold 0.4)</cell>
                  <cell>191</cell>
                  <cell>595</cell>
                  <cell>941</cell>
                  <cell>16.9%</cell>
                  <cell>24.8%</cell>
                  <cell>19.9%</cell>
                  <cell>83.1%</cell>
                </row>
            </table>
            <figure>
              <head>Evaluation results of object detection algorithms.</head>
                <graphic url="resources/images/figure04.png" style="width: 1000px"/>
            </figure>
            <table>
              <head>Prediction with respective detectors.</head>
                <row role="label">
                  <cell role="label">Prediction</cell>
                  <cell role="label">Network</cell>
                  <cell role="label">Tablet</cell>
                </row>
                <row>
                  <cell><graphic url="resources/images/figure05.png" style="width:250px"/></cell>
                  <cell>Detecto</cell>
                  <cell>YPM BC 016773</cell>
                </row>
                <row>
                  <cell><graphic url="resources/images/figure06.png" style="width:250px"/></cell>
                  <cell>YOLOv5</cell>
                  <cell>YPM BC 018686</cell>
                </row>
                <row>
                  <cell><graphic url="resources/images/figure07.png" style="width:250px"/></cell>
                  <cell>R-CNN</cell>
                  <cell>YPM BC 023856</cell>
                </row>
            </table>
        </div>
        <div>
          <head>4 Discussion</head>
            <p>The results of the machine learning model we developed (90.5% accuracy and 81.7% F-measure on Detecto) are very promising, particularly 
              considering the relatively small amount of labelled data. It shows that this previously untested approach, namely stroke identification 
              from 2D images, can be highly efficient for the vectorization of cuneiform tablets. While stroke identification has already been 
              achieved in 3D models of cuneiform-bearing objects (see Section 1.1), our approach shows the same is possible for 2D images which are far 
              cheaper to produce.
            </p>
            <p>Furthermore, our model is not period-dependent, meaning that some of the tablets we have chosen are over a thousand years apart (see 
               Table 1). But since the writing technique itself has not changed during that period, there were no significant differences in the 
               model's ability to recognize the strokes. The major attested difference between stroke types in Mesopotamia is Babylonian vs. Assyrian 
               strokes, the former having longer bodies (the tracing line), and the latter having bigger heads (the central point of the impression 
               left on the clay tablet <ptr target="#edzard_1980"/> <ptr target="#labat_1988"/>. This, however, does not seem to affect our model.
            </p>
            <p>Since it was possible to effectively identify horizontal strokes, the same is possible for verticals and obliques. With this additional 
               ability, it will be possible to create a full vectorization of cuneiform tablets, which will need to be minimally corrected by an expert. 
               These vectorizations are in effect like hand-copies, which are a first step in assyriological research in interpreting cuneiform texts. 
               It will be the first step in a human-in-the-loop pipeline of cuneiform identification from image to digital text. 
            </p>
            <p>The subsequent step, identification of constellations of strokes as cuneiform signs, is under development by part of the authors 
               <ptr target="#gordin_romach_2022"/>, currently using traditional hand-copies as input (see 
               <ref target="https://www.ben-digpasts.com/demo">demo</ref>; see <ptr target="#yamauchi_et_al_2018"/> for previous work in this 
               direction). Once the signs are identified with their equivalent in Unicode cuneiform <ptr target="#cohen_et_al_2004"/>, these can be 
               transliterated and segmented into words using the model Akkademia, previously developed by the assyriologists on our team and others 
               <ptr target="#gordin_et_al_2020"/>. This can be further followed by machine translation for the two main languages which used the 
               cuneiform writing system, Sumerian and Akkadian. The Machine Translation and Automated Analysis of Cuneiform Languages (MTAAC) project 
               has begun developing models for translating Ur III administrative texts (dated to the 21st century BCE) 
               <ptr target="#punia_et_al_2020"/>. Machine translation of Akkadian has also been achieved, focusing primarily on first millennium BCE 
               texts from a variety of genres, available through <ref target="http://oracc.org/">ORACC</ref> <ptr target="#gutherz_et_al_2023"/>.
            </p>
            <p>This pipeline can become a vital part of assyriological research by making accessible to experts and laypeople alike countless 
               cuneiform texts that have previously received less scholarly attention. However, it is important to note the limitations of this 
               pipeline for assyriological research.
            </p>
            <p>The vector images we produce are not an accurate representation of the stroke, but rather a chosen schema. Although various schemas can 
               be selected, they are still one simplistic representation on how a stroke looks, which is then applied across the corpus. Therefore, for 
               purposes of scribal hand identification, as well as palaeography, they lack important aspects, such as <term>ductus</term>, or the 
               formation of individual signs, and <term>aspect</term> (cf. Latin <term>equilibrium</term>), or the visual impression created by the 
               set hand of a scribe (i.e., the style of writing). This is the same, however, for manually created hand-copies, since there are 
               limitations to how these 3D objects can be captured in 2D, and some scholars tend to simplify what they see on the tablet when creating 
               hand-copies.
            </p>
            <p>In addition, our results worked well on very high quality 2D images, curated by an expert <ptr target="#wagensonner_2015"/>. Although 
               anyone can take high-quality images on their phone, ensuring that the signs and strokes are as legible as possible usually requires an 
               expert knowledge of the cuneiform script and the application of light sources. For this to efficiently work on a large scale, preferably 
               only high-quality 2D images of cuneiform artifacts should be used.
            </p>
        </div>
        <div>
          <head>5 Towards Quantitative Epigraphy</head>
            <p>The task of the epigrapher is to decipher the ancient writing surface — not merely to decipher the script or any linguistic element 
               on its own, but rather to produce a holistic decipherment of the inscription, its material aspects, and its contextual meaning. 
               Therefore, it is challenging to translate epigraphic tasks into one or more computational tasks. The current contribution is a step in 
               this direction, by attempting to gouge out the atomized elements of the script and its arrangement on the writing surface. This 
               diplomatic approach to texts has a long history in medieval scholarly practice <ptr target="#duranti_1998"/> 
               <ptr target="#bertrand_2010"/>, and it is a <term>desideratum</term> in order to piece together computational tasks for quantitative 
               epigraphy. It is further a way to bridge the differences across large numbers of ancient or little-known languages and scripts, since 
               much of the literature surrounding their study involves discussions on reconstructing the writing surface, traces, and their proper 
               sequence in their <term>Sitz im Leben</term>.
            </p>
            <p>The problem begins when one tries to harmonize tasks from the disciplines in the realm of computer science and the different research 
               questions in the humanities, which do not necessarily overlap. For an epigrapher, identifying and classifying an object in an image is 
               not the end goal, as it might be in computer science. Rather, it is a step in a process to reach historical understanding of a certain 
               genre of text, writing tradition, or historical period. Furthermore, the amounts of data that are available to train generative models 
               like ChatGPT or the many image generator applications made available in recent months, is beyond the scope of the digital data at the 
               hands of the average epigrapher, historian, or digital humanist.
            </p>
            <p>For that end, an interdisciplinary group of scholars dealing with ancient language processing and machine learning for ancient languages 
               <ptr target="#anderson_et_al_2023"/> <ptr target="#sommerschield_et_al_2023"/> has set out to better define and standardize data 
               formats, tasks, and benchmarks. This initiative adds to the growing movement of computational and quantitative studies in classics, 
               biblical studies, ancient Near Eastern studies, and so on. The present paper is aimed to contribute to the standardization of the 
               epigrapher's computational tasks in ancient scripts. This paper also provides an example of harmony and collaboration between computer 
               scientists and humanists, as well as between computer science tasks and humanistic research questions.
            </p>
            <p>Furthermore, the methodology and techniques used in this study can be applied to other writing systems beyond cuneiform. The 
               semi-automatic vectorization approach can be adapted to identify and extract specific features of other ancient scripts. In ancient 
               Chinese and Japanese for example, one can try to find the common denominator made up of strokes, the components of each character. In 
               classical Maya writing, one could focus on the anthropomorphic elements of signs, like noses, eyes, ears, etc. The same can be said for 
               other hieroglyphic scripts, like Egyptian or Anatolian hieroglyphs.
            </p>
        </div>
        <div type="appendix">
          <head>Appendix</head>
            <div>
              <head>6.1 Training Convolutional Neural Networks</head>
                <p>In the following section, we present the parameters necessary to replicate our results.
                </p>
                <p>For all the models we employed, image augmentation methods were necessary to increase the number of available images for training. 
                   Grayscale augmentations were applied with three samples per augmentation: 
                   <list type="unordered">
                     <item>Saturation applied to 50% of the images</item>
                     <item>Saturation between -20% and +20%</item>
                     <item>Exposure between -20% and +20%</item></list>
                </p>
                  <div>
                    <head>6.1.1 Detecto Training</head>
                      <p>For Detecto training, the dataset was divided into three subsets. The training set contains 3,456 images, the validation set 
                         contains 330 images, and the testing set contains 164 images. The training was performed on Google Collaboratory, and the 
                         layers from the fasterrcnn_resnet50_fpn_coco-258fb6c6.pth model were unfrozen and re-trained. Fifty epochs were run, with three 
                         steps per epoch, and the validation loss dropped from 0.74 to 0.64 after all epochs, which took 302 minutes. After five epochs, 
                         the validation loss did not decrease, so we could have used early stopping for this model.
                      </p>
                  </div>
                  <div>
                    <head>6.1.2 YOLOv5 Training</head>
                      <p>The YOLOv5 architecture (with 232 layers) was trained in Google Colaboratory using CUDA on a Tesla T4 GPU with 40 
                         multiprocessors and 15,109 MB of total memory. 100 epochs were executed with a batch of 16 images. The training loss (MAE) was 
                         reduced from 0.1 in the first epoch to 0.03 in the last epoch, as can be seen in Figure 5.
                      </p>
                      <figure>
                        <head>Training set loss; source: Tensorboard</head>
                          <graphic url="resources/images/figure08.png" style="width: 1000px"/>
                      </figure>
                  </div>
                  <div>
                    <head>6.1.3 R-CNN Training</head>
                      <p>The whole implementation was done using the artificial intelligence lab at the <name>Czech University of Life Sciences</name> 
                         in Prague, because R-CNN has high memory requirements and caused Google Collaboratory to crash (due to lack of memory). The 
                         environment settings as seen in Table 5 were used:
                      </p>
                      <table>
                        <head>R-CNN environment settings.</head>
                          <row>
                            <cell role="label">IDE</cell>
                            <cell>VS Code with Jupyter Extension</cell>
                          </row>
                          <row>
                            <cell role="label">Kernel</cell>
                            <cell>Python 3.8.12 within Anaconda</cell>
                          </row>
                          <row>
                            <cell role="label">AI Framework</cell>
                            <cell>Tensorflow 2.7.0 for GPU</cell>
                          </row>
                          <row>
                            <cell role="label">Nvidia Configuration</cell>
                            <cell>NVIDIA Quadro P400, cuda 11.2, cudnn 8.1</cell>
                          </row>
                      </table>
                      <p>We have implemented region proposals with selective search using IoU (Intersection over Union) configured as seen in 
                         Table 6:
                      </p>
                      <table>
                        <head>R-CNN configuration.</head>
                          <row>
                            <cell role="label">Max Samples</cell>
                            <cell>55 (based on the maximum in training set)</cell>
                          </row>
                          <row>
                            <cell role="label">Selective Search Iterate Results</cell>
                            <cell>2000 (proposed in original paper)</cell>
                          </row>
                          <row>
                            <cell role="label">IoU Object Limit</cell>
                            <cell>0.7</cell>
                          </row>
                          <row>
                            <cell role="label">IoU Background Limit</cell>
                            <cell>0.3</cell>
                          </row>
                      </table>
                      <p>The images used were 224 x 224 in size. We chose a VGG model pre-trained on the ImageNet dataset (input layer, thirteen 
                         convolutional layers, five MaxPooling layers, Flatten, Dense). After encoding the label set once and splitting it into 
                         training (90%) and test sets (10%), we proceeded to train with the configurations as seen in Table 7. Early stopping caused 
                         the training process to stop after thirty-nine epochs.
                      </p>
                      <table>
                        <head>R-CNN training hyperparameters.</head>
                          <row>
                            <cell role="label">Error Function</cell>
                            <cell>Binary cross-entropy</cell>
                          </row>
                          <row>
                            <cell role="label">Optimizer</cell>
                            <cell>Adam</cell>
                          </row>
                          <row>
                            <cell role="label">Learning Rate</cell>
                            <cell>0.0001</cell>
                          </row>
                          <row>
                            <cell role="label">Training Epochs</cell>
                            <cell>100</cell>
                          </row>
                          <row>
                            <cell role="label">Steps in Epoch</cell>
                            <cell>10</cell>
                          </row>
                          <row>
                            <cell role="label">Patience Epochs for Early Stopping</cell>
                            <cell>20</cell>
                          </row>
                      </table>
                  </div>
              </div>
              <div>
                <head>6.2 Utilities for Further Research</head>
                  <p>In order to ease the process of data creation for the next steps of the project, we developed three image and label processing 
                     tools: an image splitter, a vector visualization, and an image merger. These tools are available in the GitHub repository of our 
                     project.<note>See 
                     <ref target="https://github.com/adelajelinkova/cuneiform/tree/main/Utilities">https://github.com/adelajelinkova/cuneiform/tree/main/Utilities</ref>.</note> 
                     The neural networks that we used for object detection accept square input, and if it is not square, the image is reshaped to a 
                     standard input size. For large tablets, there would be a significant loss of data, so it is necessary to slice large images into 
                     smaller, uniformly sized square images and train the network on these slices. We chose a fixed image size of 416 x 416 (a multiple 
                     of eight, which is generally better for machine learning purposes <ptr target="#chollet_pecinovsky_2019"/>).
                  </p>
                  <p>While for the research presented in this article, we split the images before labelling, this slowed down the labelling process. 
                     Therefore, we developed an image splitter and an image merger. Our proposed system works as follows: after labelling, a large 
                     image with a cuneiform-bearing object is cut into squares with 50% overlap, so there is no data loss if strokes are on the edge 
                     of one square, since they are in the middle of the next one. Then the neural network predicts where the horizontal strokes are in 
                     the image. The networks return bounding boxes which indicate the location of the strokes. These bounding boxes are replaced by 
                     vectors of strokes in an empty image, and the strokes in the whole tablet are reconstructed using merging. In this way we can 
                     create an automatic vectorization of horizontal (and other) strokes in the whole tablet (see Figure 6).
                  </p>
                  <figure>
                    <head>Output image from the vector visualisation tool, tablet YPM BC 021234.</head>
                      <graphic url="resources/images/figure09.png" style="width: 1000px"/>
                  </figure>
                  <p>The main challenge in preparing the set of tools was dealing with splitting labels. When splitting the image into smaller squares, 
                     there is a cut-off threshold for deciding whether the annotated strokes are still significant enough to be used in training. The 
                     threshold is based on a percentage that determines what portion of the annotated strokes can be kept and what should be removed.
                  </p>
              </div>
          </div>
          <div>
            <head>Acknowledgements</head>
              <p>This research was funded by two grants. The project cuneiform analysis using Convolutional Neural Networks reg. no. 31/2021 was 
                 financed from the OP RDE project Improvement in Quality of the Internal Grant Scheme at CZU, reg. no. 
                 CZ.02.2.69/0.0/0.0/19_073/0016944. The project no. RA2000000010 was financed by the CULS – Ariel University cooperation grant.
              </p>
          </div>
      </body>
      <back>
        <listBibl>
          <bibl xml:id="anderson_et_al_2023" label="Anderson et al. 2023">Anderson, A. et al. (eds) (2023) <title rend="italic">Proceedings of 
             the ancient language processing workshop (RANLP-ALP 2023)</title>. Shoumen, Bulgaria: INCOMA Ltd.
          </bibl>
          <bibl xml:id="andre-salvini_lombard_1997" label="André-Salvini and Lombard 1997">André-Salvini, B. and Lombard, P. (1997) 
             <title rend="quotes">La découverte épigraphique de 1995 à Qal'at al-Bahrein: un jalon pour la chronologie de la phase Dilmoun Moyen 
             dans le Golfe arabe</title>, <title rend="italic">Proceedings of the seminar for Arabian studies, 1995</title>. pp. 165–170.
          </bibl>
          <bibl xml:id="bertrand_2010" label="Bertrand 2010">Bertrand, P. (2010)<title rend="quotes">Du De re diplomatica au Nouveau traité de 
             diplomatique: réception des textes fondamentaux d’une discipline</title>, in Leclant, J., Vauchez, A., and Hurel, D.O. (eds) 
             <title rend="italic">Dom Jean Mabillon, figure majeure de l'Europe des lettres: Actes des deux colloques du tricentenaire de la mort de 
              Dom Mabillon (abbaye de Solesmes, 18-19 mai 2007</title>, pp. 605-619. Paris: Académie des Inscriptions et Belles-Lettres.
          </bibl>
          <bibl xml:id="bogacz_gertz_mara_2015a" label="Bogacz, Gertz, and Mara 2015a">Bogacz, B., Gertz, M., and Mara, H. (2015a) 
             <title rend="quotes">Character retrieval of vectorized cuneiform script</title>, <title rend="italic">Proceedings of the 
             13th international conference on document analysis and recognition, IEEE, 2015</title>. Piscataway, NJ, 23-26 August. pp. 326-330. 
             <ref target="https://doi.org/10.1109/ICDAR.2015.7333777">https://doi.org/10.1109/ICDAR.2015.7333777</ref> 
          </bibl>
          <bibl xml:id="bogacz_gertz_mara_2015b" label="Bogacz, Gertz, and Mara 2015b">Bogacz, B., Gertz, M., and Mara, H. (2015b) 
            <title rend="quotes">Cuneiform character similarity using graphic representations</title>, in Wohlhart, P. and Lepetit, V. (eds) 
            <title rend="italic">20th computer vision winter workshop</title>. Graz, Austria: Verlag der Technischen Universität Graz, pp. 105–112.
          </bibl>
          <bibl xml:id="bogacz_howe_mara_2016" label="Bogacz, Howe, and Mara 2016">Bogacz, B., Howe, N., and Mara, H. (2016) 
             <title rend="quotes">Segmentation free spotting of cuneiform using part structured models</title>, <title rend="italic">Proceedings of 
             the 15th international conference on frontiers in handwriting recognition, IEEE, 2016</title>. Shenzhen, China, 23-26 October. pp. 301-306. 
             <ref target="https://doi.org/10.1109/ICFHR.2016.0064">https://doi.org/10.1109/ICFHR.2016.0064</ref>.
          </bibl>
          <bibl xml:id="bogacz_mara_2018" label="Bogacz and Mara 2018">Bogacz, B. and Mara, H. (2018) <title rend="quotes">Feature descriptors for 
             spotting 3D characters on triangular meshes</title>, <title rend="italic">Proceedings of the 16th international conference on 
             frontiers in handwriting recognition, IEEE, 2018.</title>Niagara Falls, NY, USA, 5-8 August. pp. 363-368. Available at: 
             <ref target="https://ieeexplore.ieee.org/document/8583788">https://ieeexplore.ieee.org/document/8583788</ref>
          </bibl>
          <bibl xml:id="bogacz_mara_2022" label="Bogacz and Mara 2022">Bogacz, B. and Mara, H. (2022) <title rend="quotes">Digital assyriology: 
             Advances in visual cuneiform analysis</title>, <title rend="italic">Journal on Computing and Cultural Heritage</title>, 15(2). 
             <ref target="https://doi.org/10.1145/3491239">https://doi.org/10.1145/3491239</ref>.
          </bibl>
          <bibl xml:id="bramanti_2015" label="Bramanti 2015">Bramanti, A. (2015) <title rend="quotes">The cuneiform stylus. Some addenda</title>, 
             <title rend="italic">Cuneiform digital library notes</title>, 2015(12). Available at: 
             <ref target="https://cdli.mpiwg-berlin.mpg.de/articles/cdln/2015-12">https://cdli.mpiwg-berlin.mpg.de/articles/cdln/2015-12</ref> 
             (Accessed: 26 January 2024).
          </bibl>
          <bibl xml:id="brusius_2015" label="Brusius 2015">Brusius, M. (2015) <title rend="italic">Fotografie und Museales Wissen: William Henry Fox 
             Talbot, das Altertum und die Absenz der Fotografie</title>. Berlin: De Gruyter.</bibl>
          <bibl xml:id="cammarosano_et_al_2014" label="Cammarosano et al. 2014">Cammarosano, M. et al. (2014) 
             <title rend="quotes">Schriftmetrologie des Keils: Dreidimensionale Analyse von Keileindrücken und Handschriften</title>, 
             <title rend="italic">Die Welt des Orients</title>, 44(1), pp. 2-36.
          </bibl>
          <bibl xml:id="cammarosano_2014" label="Cammarosano 2014">Cammarosano, M. (2014) <title rend="quotes">The cuneiform stylus</title>, 
             <title rend="italic">Mesopotamia: Rivista di Archeologia, Epigrafia e Storia Orientale Antica</title>, 69, pp. 53–90.
          </bibl>
          <bibl xml:id="cho_et_al_2016" label="Cho et al. 2016">Cho, J. et al. (2016) <title rend="quotes">How much data is needed to train a 
             medical image deep learning system to achieve necessary high accuracy?</title> <title rend="italic">arXiv</title>, 1511.06348. 
             Available at: <ref target="http://arxiv.org/abs/1511.06348">http://arxiv.org/abs/1511.06348</ref>.
          </bibl>
          <bibl xml:id="chollet_pecinovsky_2019" label="Chollet and Pecinovský 2019">Chollet, F. and Pecinovský, R. (2019) <title rend="italic">Deep 
             learning v Jazyku Python: Knihovny Keras, TensorFlow</title>. Praha, Prague: Grada Publishing a.s.
          </bibl>
          <bibl xml:id="cohen_et_al_2004" label="Cohen et al. 2004">Cohen, J. et al. (2004) <title rend="quotes">iClay: Digitizing cuneiform</title>, 
             <title rend="italic">Proceedings of the 5th international symposium on virtual reality, archaeology and cultural heritage, EG, 
             2004.</title> Oudenaarde, Belgium, 7-10 December. pp. 135-143. Available at: 
             <ref target="https://doi.org/10.2312/VAST/VAST04/135-143">https://doi.org/10.2312/VAST/VAST04/135-143</ref>.
          </bibl>
          <bibl xml:id="collins_et_al_2019" label="Collins et al. 2019">Collins, T. et al. (2019) <title rend="quotes">Automated low-cost 
             photogrammetric acquisition of 3D models from small form-factor artefacts</title>, <title rend="italic">Electronics</title>, 8, p. 1441. 
             <ref target="https://doi.org/10.3390/electronics8121441">https://doi.org/10.3390/electronics8121441</ref>.
          </bibl>
          <bibl xml:id="dahl_hameeuw_wagensonner_2019" label="Dahl, Hameeuw, and Wagensonner 2019">Dahl, J.L., Hameeuw, H., and Wagensonner, K. (2019) 
             <title rend="quotes">Looking both forward and back: imaging cuneiform</title>, <title rend="italic">Cuneiform digital library 
             preprints</title> [Preprint]. 
             Available at: <ref target="https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0">https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0</ref>.
          </bibl>
          <bibl xml:id="dencker_et_al_2020" label="Dencker et al. 2020">Dencker, T. et al. (2020) <title rend="quotes">Deep learning of cuneiform sign 
             detection with weak supervision using transliteration alignment</title>, <title rend="italic">PLOS ONE</title>, 15(12), p. e0243039.
             <ref target="https://doi.org/10.1371/journal.pone.0243039">https://doi.org/10.1371/journal.pone.0243039</ref>.
          </bibl>
          <bibl xml:id="duranti_1998" label="Duranti 1998">Duranti, L. (1998) <title rend="italic">New uses for an old science</title>. Chicago, IL: 
             Scarecrow Press.
          </bibl>
          <bibl xml:id="earl_et_al_2011" label="Earl et al. 2011">Earl, G. et al. (2011) <title rend="quotes">Reflectance transformation imaging 
             systems for ancient documentary artefacts</title>, in Dunnand, S., Bowen, J.P., and Ng K.C. (eds) 
             <title rend="italic">EVA London 2011: Electronic visualisation and the arts</title>, pp. 147-154. London: BCS.
          </bibl>
          <bibl xml:id="edzard_1980" label="Edzard 1980">Edzard, D.O. (1980) <title rend="quotes">Keilschrift</title>,  
             <title rend="italic">Reallexikon der Assyriologie</title>, 5, pp. 544-568.
          </bibl>
          <bibl xml:id="fisseler_et_al_2013" label="Fisseler et al. 2013">Fisseler, D. et al. (2013) <title rend="quotes">Towards an interactive and 
            automated script feature Analysis of 3D scanned cuneiform tablets</title>, <title rend="italic">Proceedings of the scientific computing 
              and cultural heritage conference, 2013</title>. Heidelberg, Germany, 18-20 November. Available at: 
            <ref target="https://www.researchgate.net/publication/267921266_Towards_an_interactive_and_automated_script_feature_Analysis_of_3D_scanned_cuneiform_tablets">https://www.researchgate.net/publication/267921266_Towards_an_interactive_and_automated_script_feature_Analysis_of_3D_scanned_cuneiform_tablets</ref>.
          </bibl>
          <bibl xml:id="fisseler_et_al_2014" label="Fisseler et al. 2014">Fisseler, D. et al. (2014) <title rend="quotes">Extending philological 
             research with methods of 3D computer graphics applied to analysis of cultural heritage</title>, <title rend="italic">Proceedings of the 
             eurographics workshop on graphics and cultural heritage, 2014</title>. Darmstadt, Germany, 6-8 October, pp. 165-172. 
             <ref target="https://doi.org/10.2312/gch.20141314">https://doi.org/10.2312/gch.20141314</ref>.
          </bibl>
          <bibl xml:id="girshick_et_al_2014" label="Girshick et al. 2014">Girshick, R. et al. (2014) <title rend="quotes">Rich feature hierarchies for 
             accurate object detection and semantic segmentation</title>, <title rend="italic">Proceedings of the IEEE conference on computer vision and 
             pattern recognition, IEEE, 2014</title>. Columbus, OH, USA, 23-28 June. 
             <ref target="https://doi.org/10.1109/CVPR.2014.81">https://doi.org/10.1109/CVPR.2014.81</ref>.
          </bibl>
          <bibl xml:id="girshick_2015" label="Girshick 2015">Girshick, R. (2015) <title rend="quotes">Fast R-CNN</title>, 
             <title rend="italic">Proceedings of the IEEE international conference on computer vision, IEEE, 2015</title>. Santiago, Chile, 
             11-18 December. pp. 1440-1448. <ref target="https://doi.org/10.1109/ICCV.2015.169">https://doi.org/10.1109/ICCV.2015.169</ref>.
          </bibl>
          <bibl xml:id="gordin_et_al_2020" label="Gordin et al. 2020">Gordin, S. et al. (2020) 
             <title rend="quotes">Reading Akkadian cuneiform using natural language processing</title>, <title rend="italic">PLOS ONE</title>, 15(10). 
             <ref target="https://doi.org/10.1371/journal.pone.0240511">https://doi.org/10.1371/journal.pone.0240511</ref>.
          </bibl>
          <bibl xml:id="gordin_romach_2022" label="Gordin and Romach 2022">Gordin, S. and Romach, A. (2022) <title rend="quotes">Optical character 
             recognition for complex scripts: A case-study in cuneiform</title>, <title rend="italic">Proceedings of the alliance of digital 
             humanities organizations conference, IDHC, 2022</title>. Tokyo, Japan, 25-29 July. Available at: 
             <ref target="https://dh-abstracts.library.cmu.edu/works/11708">https://dh-abstracts.library.cmu.edu/works/11708</ref>.
          </bibl>
          <bibl xml:id="gutherz_et_al_2023" label="Gutherz et al. 2023">Gutherz, G. et al. (2023) <title rend="quotes">Translating 
             Akkadian to English with neural machine translation</title>, <title rend="italic">PNAS Nexus</title>, 2(5). 
             <ref target="https://doi.org/10.1093/pnasnexus/pgad096">https://doi.org/10.1093/pnasnexus/pgad096</ref>.
          </bibl>
          <bibl xml:id="hameeuw_willems_2011" label="Hameeuw and Willems 2011">Hameeuw, H. and Willems, G. (2011) <title rend="quotes">New 
             visualization techniques for cuneiform texts and sealings</title>, <title rend="italic">Akkadica</title>, 132(3), pp. 163-178.
          </bibl>
          <bibl xml:id="jiao_et_al_2019" label="Jiao et al. 2019">Jiao, L. et al. (2019) <title rend="quotes">A survey of deep learning-based object 
             detection</title>, <title rend="italic">IEEE Access</title>, 7, pp. 128837-128868. 
             <ref target="https://doi.org/10.1109/ACCESS.2019.2939201">https://doi.org/10.1109/ACCESS.2019.2939201</ref>.
          </bibl>
          <bibl xml:id="jocher_et_al_2020" label="Jocher et al. 2020">Jocher, G. et al. (2020) <title rend="quotes">ultralytics/yolov5: Initial 
             release</title>, <title rend="italic">Zenodo</title>. 
             <ref target="https://doi.org/10.5281/zenodo.3908560">https://doi.org/10.5281/zenodo.3908560</ref>.
          </bibl>
          <bibl xml:id="kriege_et_al_2018" label="Kriege et al. 2018">Kriege, N.M. et al. (2018) <title rend="quotes">Recognizing cuneiform signs 
             using graph based methods</title>, <title rend="italic">Proceedings of the international workshop on cost-sensitive learning, PMLR, 
             2018</title>. San Diego, CA, USA, 5 May. pp. 31-44. Available at: 
             <ref target="https://proceedings.mlr.press/v88/kriege18a.html">https://proceedings.mlr.press/v88/kriege18a.html</ref>.
          </bibl>
          <bibl xml:id="labat_1988" label="Labat 1988">Labat, R. and Malbran-Labat, F. (1988) <title rend="italic">Manuel d’épigraphie 
             akkadienne</title>, 6th ed. Paris: Librairie Orientaliste Paul Geuthner.
          </bibl>
          <bibl xml:id="mara_et_al_2010" label="Mara et al. 2010">Mara, H. et al. <title rend="quotes">GigaMesh and Gilgamesh: 3D multiscale integral 
             invariant cuneiform character extraction</title>, <title rend="italic">Proceedings of the 11th international symposium on virtual 
             reality, archaeology, and cultural heritage, EG, 2010</title>. Paris, France, 21-24 September. 
             <ref target="https://doi.org/10.2312/VAST/VAST10/131-138">https://doi.org/10.2312/VAST/VAST10/131-138</ref>.
          </bibl>
          <bibl xml:id="mara_kromker_2013" label="Mara and Krömker 2013">Mara, H. and Krömker, S. (2013) <title rend="quotes">Vectorization of 
             3D-characters by integral invariant filtering of high-resolution triangular meshes</title>, <title rend="italic">Proceedings of the 
             international conference on document analysis and recognition, IEEE, 2013</title>. Washington, DC, USA, 25-28 August. pp. 62-66. 
             <ref target="https://doi.org/10.1109/ICDAR.2013.21">https://doi.org/10.1109/ICDAR.2013.21</ref>.
          </bibl>
          <bibl xml:id="massa_et_al_2016" label="Massa et al. 2016">Massa, J. et al. (2016) <title rend="quotes">Cuneiform detection in vectorized 
             raster images</title>, <title rend="italic">Proceedings of the 21st computer vision winter workshop, 2016</title>. Rimske Toplice, 
             Slovenia, 3-5 February. Available at: 
             <ref target="https://d-nb.info/1191851524/34">https://d-nb.info/1191851524/34</ref>.
          </bibl>
          <bibl xml:id="mishra_2022" label="Mishra 2022">Mishra, D. (2022) <title rend="quotes">Deep learning based object detection methods: A 
             review</title>, <title rend="italic">Medicon Engineering Themes</title>, 2(4). 
             <ref target="https://doi.org/10.55162/MCET.02.027">https://doi.org/10.55162/MCET.02.027</ref>.
          </bibl>
          <bibl xml:id="pavlicek_2018" label="Pavlíček 2018">Pavlíček, J. et al. (2018) <title rend="quotes">Automated wildlife recognition</title>, 
             <title rend="italic">AGRIS on-line papers in economics and informatics</title>, 10(1), pp. 51-60. 
             <ref target="https://doi.org/10.7160/aol.2018.100105">https://doi.org/10.7160/aol.2018.100105</ref>.
          </bibl>
          <bibl xml:id="punia_et_al_2020" label="Punia et al. 2020">Punia, R. et al. (2020) <title rend="quotes">Towards the first machine 
             translation system for Sumerian transliterations</title>, <title rend="italic">Proceedings of the 28th international conference on 
             computational linguistics, ACL, 2020</title>. Barcelona, Spain, 13-18 September. pp. 3454-3460. 
             <ref target="https://doi.org/10.18653/v1/2020.coling-main.308">https://doi.org/10.18653/v1/2020.coling-main.308</ref>.
          </bibl>
          <bibl xml:id="redmon_et_al_2016" label="Redmon et al. 2016">Redmon, J. et al. (2016) <title rend="quotes">You only look once: Unified 
             real-time object detection</title>, <title rend="italic">Proceedings of the IEEE conference on computer vision and pattern recognition, 
             IEEE, 2016</title>. Las Vegas, NV, USA, 26 June-1 July. 
             <ref target="https://doi.org/10.1109/CVPR.2016.91">https://doi.org/10.1109/CVPR.2016.91</ref>.
          </bibl>
          <bibl xml:id="ren_et_al_2015" label="Ren et al. 2015">Ren, S. et al. (2015) <title rend="quotes">Faster R-CNN: Towards real-time object 
             detection with region proposal networks</title>, <title rend="italic">Proceedings of the advances in neural processing systems 
             conference, NeurIPS, 2015</title>. Montreal, Canada, 7-12 December. Available at: 
             <ref target="https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html">
             https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html</ref>.
          </bibl>
          <bibl xml:id="rothacker_et_al_2015" label="Rothacker et al. 2015">Rothacker, L. et al. (2015) <title rend="quotes">Retrieving cuneiform 
             structures in a segmentation-free word spotting framework</title>, <title rend="italic">Proceedings of the 3rd international workshop 
             on historical document imaging and processing, ACM, 2015</title>. Nancy, France, 22 August. pp. 129-136. 
             <ref target="https://doi.org/10.1145/2809544.2809562">https://doi.org/10.1145/2809544.2809562</ref>.
          </bibl>
          <bibl xml:id="rusakov_et_al_2019" label="Rusakov et al. 2019">Rusakov, E. et al. (2019) <title rend="quotes">Generating cuneiform 
             signs with cycle-consistent adversarial networks</title>, <title rend="italic">Proceedings of the 5th international workshop on historical 
             document imaging and processing, ACM, 2019</title>. Sydney, Australia, 20-21 September. pp. 19-24. 
             <ref target="https://doi.org/10.1145/3352631.3352632">https://doi.org/10.1145/3352631.3352632</ref>.
          </bibl>
          <bibl xml:id="rusakov_et_al_2020" label="Rusakov et al. 2020">Rusakov, E. et al. (2020) <title rend="quotes">Towards query-by-expression 
             retrieval of cuneiform signs</title>, <title rend="italic">Proceedings of the 17th international conference on frontiers in handwriting 
             recognition, IEEE, 2020</title>. Dortmund, Germany, 7-10 September. pp. 43-48. 
             <ref target="https://doi.org/10.1109/ICFHR2020.2020.00019">https://doi.org/10.1109/ICFHR2020.2020.00019</ref>.
          </bibl>
          <bibl xml:id="sommerschield_et_al_2023" label="Sommerschield et al. 2023">Sommerschield, T. et al. (2023) <title rend="quotes">Machine 
             learning for ancient languages: A survey</title>, <title rend="italic">Computational Linguistics</title>, 49(3), pp. 1-44. 
             <ref target="https://doi.org/10.1162/coli_a_00481">https://doi.org/10.1162/coli_a_00481</ref>.
          </bibl>
          <bibl xml:id="streck_2010" label="Streck 2010">Streck, M.P. (2010) <title rend="quotes">Großes Fach Altorientalistik: Der Umfang des 
             keilschriftlichen Textkorpus</title>, <title rend="italic">Mitteilungen der Deutschen Orient-Gesellschaft</title>, 142, pp. 35-58.
          </bibl>
          <bibl xml:id="taylor_2015" label="Taylor 2015">Taylor, J. (2014) <title rend="quotes">Wedge order in cuneiform: A preliminary survey</title>, 
            in Devecchi, E., Müller, G.G.W., and Mynářová, J. (eds) <title rend="italic">Current research in cuneiform palaeography: Proceedings of the 
            workshop organised at the 60th rencontre assyriologique internationale, Warsaw, Poland, 2014</title>, pp. 1–30. Gladbeck, Germany: PeWe-Verlag.
          </bibl>
          <bibl xml:id="wagensonner_2015" label="Wagensonner 2015">Wagensonner, K. (2015) <title rend="quotes">On an alternative way of capturing 
             RTI images with the camera dome</title>, <title rend="italic">CDLN</title>, 2015(1), pp. 1-12.
          </bibl>
          <bibl xml:id="wevers_smits_2019" label="Wevers and Smits 2019">Wevers, M. and Smits, T. (2020) <title rend="quotes">The visual digital 
             turn: Using neural networks to study historical images</title>, <title rend="italic">Digital Scholarship in the Humanities</title>, 35(1), 
             pp. 194-207. <ref target="https://doi.org/10.1093/llc/fqy085">https://doi.org/10.1093/llc/fqy085</ref>.
          </bibl>
          <bibl xml:id="yamauchi_et_al_2018" label="Yamauchi et al. 2018">Yamauchi, K., Yamamoto, H., and Mori, W. (2018) 
             <title rend="quotes">Building a handwritten cuneiform character image set</title>, <title rend="italic">Proceedings of the 11th 
             international conference on language resources and evaluation, ACL, 2018</title>. Miyazaki, Japan, 7-12 May. pp. 719-722. 
             Available at: <ref target="https://aclanthology.org/L18-1115">https://aclanthology.org/L18-1115</ref>.
          </bibl>
        </listBibl>
      </back>
   </text>
</TEI>
