<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:array="http://www.w3.org/2005/xpath-functions/array"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:map="http://www.w3.org/2005/xpath-functions/map"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">Cuneiform Stroke Recognition and Vectorization in 2D Images</title>
            <dhq:authorInfo>
               <dhq:author_name>Adéla <dhq:family>Hamplová</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-1012-650X</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>hamplova@pef.czu.cz</email>
               <dhq:bio>
                  <p><!--AWAITING-->NEED TO REQUEST BIO FROM AUTHOR AGAIN
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Avital <dhq:family>Romach</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0001-9199-3228</idno>
               <dhq:affiliation>Yale University</dhq:affiliation>
               <email>avital.romach@yale.edu</email>
               <dhq:bio>
                  <p>Avital Romach is a PhD student of Assyriology at Yale University. Her second focus is on the implementation of digital 
                     humanities methodologies in traditional philological research and ancient language processing. She wrote her MA at 
                     Tel Aviv University on the fifth tablet of the <title rend="italic">Epic of Gilgamesh</title>, and she has published articles 
                     on computational and machine learning methods used to study ancient cuneiform inscriptions and texts.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Josef <dhq:family>Pavlíček</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-3959-5406</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>pavlicek@pef.czu.cz</email>
               <dhq:bio>
                  <p><!--AWAITING-->NEED TO REQUEST BIO FROM AUTHOR AGAIN
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Arnošt <dhq:family>Veselý</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0001-8979-1336</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>vesely@pef.czu.cz</email>
               <dhq:bio>
                  <p><!--AWAITING-->NEED TO REQUEST BIO FROM AUTHOR AGAIN
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Martin <dhq:family>Čejka</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-2909-486X</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>cejkamartin@pef.czu.cz</email>
               <dhq:bio>
                  <p>Martin Cejka, a PhD candidate in Information Engineering at the Czech University of Life Sciences in Prague, holds a Master's 
                     degree in Information and Control Technology. His expertise lies in machine learning, demonstrated through his work on 
                     <title rend="quotes">Preprocessing Audiovisual Data Using Computer Vision to Recognize UI Elements</title>. Currently involved 
                     in ML projects, Martin focuses on SETI data analysis, unsupervised topic modeling for social networks, and eyetracking 
                     applications.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>David <dhq:family>Franc</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0003-3160-9559</idno>
               <dhq:affiliation>Czech University of Life Sciences Prague</dhq:affiliation>
               <email>francd@pef.czu.cz</email>
               <dhq:bio>
                  <p><!--AWAITING-->NEED TO REQUEST BIO FROM AUTHOR AGAIN
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Shai <dhq:family>Gordin</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-8359-382X</idno>
               <dhq:affiliation>Ariel University; Open University of Israel</dhq:affiliation>
               <email>shaigo@ariel.ac.il</email>
               <dhq:bio>
                  <p>Shai Gordin is a senior lecturer for ancient Near Eastern history and digital humanities at Ariel University, and a visiting 
                     professor at the Digital Humanities and Social Sciences Hub (DHSS Hub) at the Open University of Israel. He is the PI of the 
                     Babylonian Engine project and heads the Digital Pasts Lab, which conducts research in the computational analysis of ancient 
                     texts and artefacts (<ref target="https://digitalpasts.github.io/docs/projects.html">
                     https://digitalpasts.github.io/docs/projects.html</ref>). He is the co-initiator of the Digital Ancient Near Eastern Studies 
                     Network (DANES), co-organizer of the Ancient Language Processing Workshop at RANLP2023, and co-organizer of the first international 
                     shared task of ancient cuneiform languages translation at MT-SUMMIT2023. He currently serves on the editorial board of 
                     <title rend="italic">PLOS ONE</title>.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000733</idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date/>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#machine_learning"/>
               <term corresp="#digitization"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item>cuneiform</item>
                  <item>convolutional neural networks</item>
                  <item>artificial intelligence</item>
                  <item>OCR</item>
                  <item>object detection</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000733/000733.xml">GitHub
        	   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract><!--Include a brief abstract of the article-->
            <p/>
         </dhq:abstract>
         <dhq:teaser>
            <p>An innovative approach towards cuneiform OCR of identifying strokes instead of signs offers insight into the challenges and 
               methodologies of quantitative epigraphy.
            </p>
         </dhq:teaser>
      </front>
      <body>
         <div>
            <p>Cuneiform
                        stroke recognition and vectorization in 2D images</p>
            <p>Adéla Hamplová, Avital Romach, Josef Pavlíček, Arnošt
                    Veselý, Martin Čejka, David Franc, Shai Gordin</p>
            <p>Abstract</p>
            <p>A vital part of the publication process of ancient cuneiform tablets is creating
                    hand-copies, which are 2D line art representations of the 3D cuneiform clay
                    tablets, created manually by scholars. This research provides an innovative
                    method using Convolutional Neural Networks (CNNs) to identify strokes, the
                    constituent parts of cuneiform characters, and display them as vectors –
                    semi-automatically creating cuneiform hand-copies. This is a major step in
                    optical character recognition (OCR) for cuneiform texts, which would contribute
                    significantly to their digitization and create efficient tools for dealing with
                    the unique challenges of Mesopotamian cultural heritage. Our research has
                    resulted in the successful identification of horizontal strokes in 2D images of
                    cuneiform tablets, some of them from very different periods, separated by
                    hundreds of years from each other. With the Detecto algorithm, we achieved an
                    F-measure of 81.7 % and an accuracy of 90.5 %. The data and code of the project
                    are available on GitHub.<note> Hamplova, A.,
                            Franc, D., Pavlicek, J., Romach, A., Gordin, S., Cejka, M. and Vesely,
                            A., “adelajelinkova/cuneiform”, GitHub, Available at:
                            &lt;https://github.com/adelajelinkova/cuneiform&gt;
                    (2022).</note>
            </p>
            <p>Keywords: Cuneiform, Convolutional Neural Networks,
                    Artificial Intelligence, OCR, Object Detection</p>
         </div>
         <div>
            <head>Introduction</head>
            <div>
               <head>Cuneiform texts and Artificial
                        Intelligence</head>
               <p>The cuneiform writing system is one of
                        the earliest attested, and for thousands of years also the dominant script
                        of the ancient Middle East–a region stretching roughly from the Persian Gulf
                        to modern Turkey’s highlands, and south across the Levant into Egypt.
                        Cuneiform texts appeared from the end of the fourth millennium BCE until
                        they fell out of use in the early centuries CE. The script was used for
                        writing a plethora of different documents: legal, administrative, and
                        economic documents; correspondence between commoners, or high-officials and
                        their kings; some of the oldest works of literature; royal inscriptions
                        describing deeds of great kings; as well as lexical and scientific
                        compendia, some of which form the basis of the Greco-Roman sciences the
                        western world is built upon today. Hundreds of thousands of cuneiform
                        documents have been discovered since excavations began in the 1850s. Recent
                        estimates indicate the cuneiform text corpus is second in size only to that
                        of ancient Greek [Streck 2010].</p>
               <p>The rising application of artificial intelligence to various tasks provides a
                        prime opportunity for training object detection models to assist the digital
                        publication of cuneiform texts on a large scale. This will help set up a
                        framework for cultural heritage efforts of preservation and knowledge
                        dissemination that can support the small group of specialists in the
                        field.</p>
               <p>Cuneiform provides unique challenges for object detection algorithms,
                        particularly OCR methods. Cuneiform tablets, on which the texts were
                        written, are 3D objects: pieces of clay which were shaped to particular
                        sizes. While the clay was still moist, scribes used styli with triangular
                        edges to create impressions on the clay in three possible directions:
                        horizontal, vertical, or oblique (also <hi rend="italic">Winkelhaken</hi>,
                        see [Fig. 1]); “diagonal” strokes are also found in the literature, but
                        these are technically either another type of horizontal or elongated
                        obliques [Cammarosano 2014] [Cammarosano et al. 2014] [Bramanti 2015]. Each
                        of these impressions is called a stroke (or wedge, due to their shape).
                        Combinations of different strokes create characters, usually referred to as
                        signs [Taylor 2015]. Cuneiform can be more easily read when there is direct
                        light on the tablet, especially from a specific angle that casts shadows on
                        the different strokes.</p>
               <figure>
                  <head/>
                  <graphic url="media/image1.png"/>
               </figure>
               <p>Figure 1: The main cuneiform strokes taken from Neo-Babylonian signs, from
                        left to right: AŠ (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10341</ref>),
                        DIŠ (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/10474</ref>),
                        and U or <hi rend="italic">Winkelhaken</hi> (<ref target="https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430">https://labasi.acdh.oeaw.ac.at/tablets/glyph/detail/11430</ref>),
                        as recorded in the LaBaSi palaeographical database.</p>
               <p>From the inception of research in cuneiform studies, tablets were difficult
                        to represent in a modern 2D publishing format. Two solutions were found:</p>
               <list rend="bulleted">
                  <item>When possible, 2D images of cuneiform tablets were taken. These,
                            however, for most of the field’s history, were extremely costly to
                            produce and to print, and they were often not in sufficient quality for
                            easy sign identification (for the history of early photography and
                            cuneiform studies, see [André-Salvini and Lombard 1997]). </item>
                  <item>The second solution was creating hand-copies, 2D black and white line
                            art made by scholars of the tablets’ strokes. This was the most popular
                            solution. The disadvantage of this method is that it adds a layer of
                            subjectivity, based on what the scholar has seen and on his steady
                            drawing hand. Nowadays these are still in use, often drawn using vectors
                            in special programs (most popular is the open source vector graphics
                            editor <ref target="https://inkscape.org/">Inkscape</ref>).</item>
               </list>
               <p>In recent years, the quality of 2D images has risen significantly, while the
                        costs of production and reproduction dropped. A constantly growing number of
                        images of cuneiform tablets are currently available in various online
                        databases. The largest repositories are the <ref target="https://www.britishmuseum.org/collection">British Museum</ref>, <ref target="https://collections.louvre.fr/">Louvre Museum</ref>, <ref target="https://cdli.ucla.edu/">Cuneiform Digital Library Initiative</ref>, <ref target="https://www.ebl.lmu.de/">electronic Babylonian Library</ref>,
                        and <ref target="https://babylonian-collection.yale.edu/">Yale Babylonian
                        Collection</ref>. 2D+ and 3D models of cuneiform tablets have also
                        become a possibility, although these are still more expensive and
                        labour-intensive to produce [Earl et al. 2011] [Hameeuw and Willems 2011]
                        [Collins et al. 2019] cf. overview in [Dahl, Hameeuw and Wagensonner
                        2019].</p>
               <p>Previous research of identifying cuneiform signs or strokes have used mostly
                        3D models. Two research groups have developed programs for manipulating 3D
                        models of cuneiform tablets: the <ref target="https://www.hethport.uni-wuerzburg.de/HPM/hpm.php?p=3djoins">CuneiformAnalyser</ref> [Fisseler et al. 2013] [Rothacker et al. 2015]
                        and <ref target="https://gigamesh.eu/?page=home">GigaMesh</ref> [Mara et al.
                        2010]. Additionally, each group developed stroke extraction through
                        geometrical features identification. One team also used the 3D models for
                        joining broken tablet fragments [Fisseler et al. 2014]. In addition, the
                        GigaMesh team extracted strokes as Scalable Vector Graphic (SVG) images
                        [Mara and Krömker 2013], which practically means creating hand-copies
                        automatically as vector images. This was used as a basis for querying stroke
                        configurations when looking for different examples of the same sign, using
                        graph similarity methods [Bogacz, Gertz and Mara 2015a] [Bogacz, Gertz and
                        Mara 2015b] [Bogacz, Howe and Mara 2016] [Bogacz and Mara 2018] [Kriege et
                        al. 2018].</p>
               <p>Work on hand-copies includes transforming raster images of hand-copies into
                        vector images (SVG) [Massa et al. 2016]. Hand-copies and 2D projections of
                        3D models were used for querying signs by example using CNNs with data
                        augmentation [Rusakov et al. 2019]. Previous work on 2D images has only
                        recently started. Dencker et al. [Dencker et al. 2020] used 2D images for
                        training a weakly supervised machine learning model in the task of sign
                        detection in a given image. Rusakov et al. [Rusakov et al. 2020] used 2D
                        images of cuneiform tablets for querying cuneiform signs by example and by
                        schematic expressions representing the stroke combinations. A more
                        comprehensive survey of computational methods in use for visual cuneiform
                        research can be found in Bogacz and Mara [Bogacz and Mara 2022].</p>
               <p>As there are no published attempts to extract strokes directly from 2D images
                        of cuneiform tablets, the purpose of this paper is a proof of concept to
                        show it is possible to extract and vectorize strokes from 2D images of
                        cuneiform using machine learning methods. The quantity and quality of 2D
                        images is improving, and for the most part they provide a more accurate
                        representation of the tablet than hand-copies, as well as being cheaper and
                        quicker to produce in comparison to 3D models. Furthermore, since there are
                        only three basic types of strokes, but hundreds of signs and their variants,
                        one can label a significantly smaller number of tablets to attain a
                        sufficient number of strokes for training machine learning models. The
                        resulting model will be able to recognize strokes in cuneiform signs from
                        very different periods, separated by hundreds of years from each other. This
                        semi-automation of creating hand-copies will be a significant step in the
                        publication of cuneiform texts and knowledge distribution of the history and
                        culture of the ancient Near East.</p>
            </div>
            <div>
               <head>Object Detection</head>
               <p>Identifying cuneiform signs or strokes in an image is considered an object
                        detection task in computer vision. Object detection involves the automatic
                        identification and localization of multiple objects within an image or a
                        video frame. Unlike simpler tasks such as image classification, where the
                        goal is to assign a single label to an entire image, object detection aims
                        to provide more detailed information by detecting and delineating the
                        boundaries of individual objects present. Object detection algorithms work
                        by analysing the contents of an image and searching for specific patterns or
                        features that are associated with the object. These patterns or features can
                        include things like color, texture, shape, and size.</p>
               <p>There are different types of computational models for object detection,
                        ranging from the purely mathematical to deep learning models [Wevers and
                        Smits 2019]. <hi rend="italic">Mathematical models</hi> often involve
                        traditional computer vision techniques that rely on well-defined algorithms
                        and handcrafted features. These methods typically follow a series of steps
                        to detect objects in an image. First is extracting relevant features from
                        the image (edges, corners, textures, or color information), where the
                        algorithms are usually adapted based on domain knowledge. Then mathematical
                        operations are performed to determine the location and extent of potential
                        objects based on the identified features. Further methods can be used in
                        post-processing to refine the results. Computational methods work best in
                        ideal or near-ideal conditions, meaning there needs to be standardization in
                        the types of cameras used for taking the images, the lightening situation,
                        the background should be consistent, and the objects themselves should be as
                        uniform as possible in size, shape, and color. This means that for complex
                        and diverse real-world scenarios, mathematical models are often insufficient
                        for satisfactory results.</p>
               <p>
                  <hi rend="italic">Deep learning models</hi>, particularly convolutional
                        neural networks (CNNs), have revolutionized object detection [Girshick et
                        al. 2014]. Instead of manual feature engineering used by mathematical
                        models, deep learning methods can automatically detect relevant features and
                        objects. The models are trained on labelled data: a set of images where the
                        objects of interest have been marked, usually in rectangular bounding boxes,
                        by humans. After training, the models can be tested and used on unseen
                        images that were not in the labelled training dataset to detect the same
                        type of objects. Their biggest advantage is that they can handle a wide
                        range of object shapes, sizes, and orientations, making them adaptable to
                        diverse scenarios, and generalize well across different datasets. The
                        disadvantages of such models are that they require large amounts of labelled
                        data for effective training; they are more computationally intensive
                        compared to traditional mathematical models, requiring more computational
                        power outside the scope of the average computer; and they are black boxes,
                        meaning it is not always possible to explain why the model makes a certain
                        prediction or not.</p>
               <p>In a previous research project, we combined the use of mathematical and deep
                        learning object detection methods for wildlife mapping [Pavlíček 2018].
                        However, for this project, mathematical models proved insufficient for the
                        complexity and variability of images of cuneiform tablets, which include
                        different tablet shapes, colors, broken sections, etc. Therefore, in this
                        article we present our results on stroke recognition for 2D images of
                        cuneiform tablets using several deep leaning models, and compare their
                        advantages and disadvantages for this type of object detection on ancient
                        and complex writing systems.</p>
               <div>
                  <head>Convolutional Neural
                            Networks</head>
                  <p>Convolutional neural networks (CNNs) are multilayer networks,
                            specifically designed for processing and analyzing visual data. The
                            convolutional layers in the network process the input image through
                            different filters that help the network detect features of interest like
                            edges, corners, textures, etc. The additional layers process the
                            resulting feature maps to detect relevant combinations of features.
                            There can be several iterations of convolutional layers, depending on
                            the specific architecture of the neural network used.</p>
                  <p>There are two main types of convolutional neural networks: two-stage
                            detectors and single-stage detectors [Jiao et al. 2019]. Two-stage
                            detectors, like Faster R-CNN (Region-based Convolutional Neural
                            Network), first identify regions of the image that might contain objects
                            before analyzing those regions more closely to detect objects [Girshick
                            et al. 2014]. Single-stage detectors, like YOLO (You Only Look Once;
                            [Redmon et al. 2016]), can detect objects directly without first
                            identifying regions of interest. In what follows, we provide a brief
                            overview of the advantages and disadvantages of both methods. See also
                            [Mishra 2022].</p>
                  <div>
                     <head>YOLO</head>
                     <p>YOLO, short for You Only Look Once, is a family of convolutional
                                neural network architectures that was first introduced in 2015 by
                                Joseph Redmon et al. [Redmon et al. 2016]. The version used in this
                                paper, YOLOv5, was published in 2020 [Jocher et al. 2020]. The
                                YOLOv5 architecture consists of 232 layers<note> Ultralytics/Yolov5, “Yolov5.”, GitHub,
                                        Available at: &lt;<ref target="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</ref>&gt;,
                                        (2021).</note>; multiple convolutional layers that
                                extract features from the image at different scales, and a series of
                                prediction layers, which output the object detection results.</p>
                     <p>The algorithm divides the input image into a grid of cells, with each
                                cell responsible for predicting the presence of one or more objects.
                                For each cell, the network predicts the confidence score, which
                                reflects the likelihood that an object is present, and the bounding
                                box coordinates that describe the location and size of the
                                object.</p>
                     <p>The YOLOv5 algorithm has achieved state-of-the-art performance on
                                several benchmark datasets. Its main advantage has been speed: YOLO
                                performs significantly faster than other CNN models. It has become a
                                popular choice for a wide range of applications in computer vision,
                                including object detection in real-time video streams, autonomous
                                driving, and surveillance systems. The latest version at the time of
                                publication is YOLOv8.<note>
                                        Jocher, G., Chaurasia, A., &amp; Qiu, J, “YOLO by
                                        Ultralytics (Version 8.0.0)”, GitHub, Available at: &lt;<ref target="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</ref>&gt;
                                        (2023).</note>
                     </p>
                  </div>
                  <div>
                     <head>R-CNN, Fast R-CNN, and Faster
                                R-CNN</head>
                     <p>Region-based Convolutional Neural
                                Networks (R-CNN) were first introduced in 2014 by Girshick [Girshick
                                et al. 2014]. This type of detector has four key components: (1) it
                                generates region proposals that suggest potential object locations
                                in an image using a selective search method; (2) it extracts
                                fixed-length feature vectors from each of these proposed regions;
                                (3) for each region of interest, it computes relevant features for
                                object identification (4) based on the extracted CNN features, the
                                regions of interest are classified (see Fig. 2).</p>
                     <figure>
                        <head/>
                        <graphic url="media/image2.jpeg"/>
                     </figure>
                     <p>Figure 2: The components of the R-CNN detector, from Girshick
                                [Girshick et al. 2014]</p>
                     <p>A year later, Girshick [Girshick 2015] proposed a more efficient
                                version called Fast R-CNN. In contrast to R-CNN, which processes
                                individual region suggestions separately through the CNN, Fast R-CNN
                                computes features for the entire input image at once. This
                                significantly speeds up the process and allows for better use of
                                storage space for feature storage.</p>
                     <p>Building on the improvements of Fast R-CNN, Faster R-CNN was
                                introduced just three months later [Ren et al. 2015]. It introduced
                                the region proposal network (RPN), which generates regions of
                                interest (RoI), potential identifications of the desired objects. It
                                does so by sliding a small window (known as an anchor) over the
                                feature maps and predicting whether the anchor contains an object or
                                not. For this project, we used a combination of Faster R-CNN and
                                ResNet-50, a deep learning architecture that optimizes the network’s
                                performance. This model was implemented by Bi in Detecto python
                                library, using the PyTorch python framework.<note>
                                        Bi, A., alankbi/detecto “Build
                                            fully-functioning computer vision models with PyTorch”,
                                            GitHub, Available at: &lt;<ref target="https://github.com/alankbi/detecto">https://github.com/alankbi/detecto</ref>&gt;,
                                (2020).</note>
                     </p>
                  </div>
               </div>
            </div>
         </div>
         <div>
            <head>Dataset and Evaluations</head>
            <div>
               <head>Dataset</head>
               <div>
                  <head>2.1.1. Dataset creation, division, and augmentation</head>
                  <p>The Assyriologists on our team tagged thousands of horizontal strokes in
                            eight tablets from the Yale Babylonian Collection (see Table 1), made
                            available through the kind permission of Agnete W. Lassen and Klaus
                            Wagensonner. For the first stage of research, we labelled 7,355
                            horizontal strokes in the tablets chosen, divided into 823 images.</p>
                  <table>
                     <row role="data">
                        <cell>Yale ID</cell>
                        <cell>CDLI ID</cell>
                        <cell>material</cell>
                        <cell>period</cell>
                        <cell>genre</cell>
                        <cell>content</cell>
                        <cell>hand-copy publication</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-014442">YPM BC
                                    014442</ref>
                        </cell>
                        <cell>
                           <ref target="https://cdli.ucla.edu/P504832">P504832</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Neo-Assyrian</cell>
                        <cell>literary</cell>
                        <cell>Enuma Eliš
                                        ll. 1-16, 143-61</cell>
                        <cell>CT 13 1,
                                        3</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-023856">YPM BC
                                    023856</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P293426">P293426</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Old-Babylonian</cell>
                        <cell>literary</cell>
                        <cell>Gilgamesh
                                        and Huwawa ll. 1-36</cell>
                        <cell>JCS 1
                                        22-23</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-002575">YPM BC
                                    002575</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P297024">P297024</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Neo/Late-Babylonian</cell>
                        <cell>commentary</cell>
                        <cell>Iqqur īpuš
                                        i 36, ii 31, iii 22, iv 5, v 13
                                </cell>
                        <cell>BRM 4
                                        24</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016773">YPM BC
                                    016773</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P293444">P293444</ref>
                        </cell>
                        <cell>limestone</cell>
                        <cell>Early
                                        Old-Babylonian</cell>
                        <cell>inscription</cell>
                        <cell>Building
                                        inscription of Anam, No. 4</cell>
                        <cell>YOS 1
                                        36</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016780">YPM BC
                                    016780</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P293445">P293445</ref>
                        </cell>
                        <cell>limestone</cell>
                        <cell>Early
                                        Old-Babylonian</cell>
                        <cell>inscription</cell>
                        <cell>Building
                                        inscription of Anam, No. 2</cell>
                        <cell>YOS 1
                                        35</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-016869">YPM BC
                                    016869</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P429204">P429204</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Middle
                                        Assyrian</cell>
                        <cell>inscription</cell>
                        <cell>Inscription of Aššur-nadin-apli</cell>
                        <cell>YOS 9
                                        71</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-021204">YPM BC
                                    021204</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P308129">P308129</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Middle
                                        Assyrian?</cell>
                        <cell>medical
                                        text</cell>
                        <cell/>
                        <cell>FS Sachs
                                        18, no. 16</cell>
                     </row>
                     <row role="data">
                        <cell>
                           <ref target="https://collections.peabody.yale.edu/search/Record/YPM-BC-021234">YPM BC
                                    021234</ref>
                        </cell>
                        <cell>
                           <ref target="http://cdli.ucla.edu/P308150">P308150</ref>
                        </cell>
                        <cell>clay</cell>
                        <cell>Old-Babylonian</cell>
                        <cell>hymn</cell>
                        <cell>Hymn to
                                        Inanna-nin-me-šar2-ra, ll. 52-102</cell>
                        <cell>YNER 3
                                        6-7</cell>
                     </row>
                  </table>
                  <p>Table 1: The eight tablets that were labelled and their metadata. The
                            information is taken from the <ref target="https://babylonian-collection.yale.edu/">Yale Babylonian
                                Collection</ref> website. CDLI ID refers to the catalogue
                            number in the <ref target="https://cdli.ucla.edu/">Cuneiform Digital Library
                                    Initiative</ref> database. Publication abbreviations under
                            hand-copy publication follow the <ref target="https://rla.badw.de/en/reallexikon/abkuerzungslisten/literatur-und-koerperschaften.html">Reallexikon der Assyriologie
                                    online list</ref>.</p>
                  <p>To train an artificial neural network, a dataset divided into training,
                            validation, and test subsets needs to be created. In order to increase
                            the number of images in the dataset, several augmentation methods were
                            used. The recommended number of images for each class is at least a
                            thousand images [Cho et al. 2016]. Roboflow<note> Approboflowcom “Roboflow
                                        Dashboard.”,<ref target="https://app.roboflow.com/" xml:space="preserve"> </ref>Available at:<ref target="https://github.com/WongKinYiu/yolor"> &lt;</ref>
                        <ref target="https://app.roboflow.com/">https://app.roboflow.com/</ref>&gt;,
                                (2021).</note> is a web application used to create extended
                            datasets from manually labelled data in labelling tools such as
                                Labelimg.<note>
                                    tzutalin/labelIm, “LabelImg.” Github, Available at: &lt;<ref target="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</ref>&gt;,
                                    (2021).</note>
                  </p>
                  <p>For pre-processing, the images were divided into equal squares of 416 x
                            416 pixels (for Detecto and YOLOv5), for R-CNN the size of images was
                            downsized to 224 x 224 pixels. For the final version of the model, the
                            images were augmented using Roboflow. The final dataset contains 1975
                            images with more than 20,000 labels. The augmented horizontal stroke
                            dataset is available at the following link.<note> Roboflow. “Augmented Horizontal Dataset.”
                                    [online]. Available at: &lt;<ref target="https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV">https://app.roboflow.com/ds/t1bzmgivYH?key=qSadLELYkV</ref>&gt;,
                                    (2021).</note>
                  </p>
               </div>
               <div>
                  <head>Labelling criteria</head>
                  <p>For machine learning purposes, the images of the cuneiform tablets needed
                            to be split into squares of equal size (see further in Appendix).
                            Labelling was performed after splitting the images. This meant the loss
                            of a lot of the context necessary to identify strokes with certainty. We
                            used tablet images with existing hand-copies, which were used as a guide
                            and previous interpretations of the tablets.</p>
                  <p>However, a greater emphasis was given to what is currently visible on the
                            image than what appears in the hand-copy. The hand-copies were not
                            always true to what is seen on the images for three main reasons: (1)
                            the hand-copy preserves signs which were visible on the tablet at the
                            moment of their creation, but by the time the image was taken, they had
                            eroded; (2) The camera angle when taking the image did not capture all
                            the detail the tablet contains. This is a common scenario, since signs
                            at the edges of the tablet will not be seen as clearly when taking a
                            frontal image; (3) strokes may have been cut off where the image was
                            split.</p>
                  <p>If a stroke was completely unrecognizable as a horizontal stroke on the
                            image at hand, because of either of the aforementioned restrictions, it
                            was not labelled. If enough of the characteristic features of the stroke
                            (particularly its triangular head) were present on the image, it was
                            labelled. Being able to identify partially broken strokes is still
                            useful for real-life scenarios, since the tablets themselves are often
                            broken, a common problem in sign identification.</p>
                  <p>Additionally, strokes which are usually considered diagonal were also
                            labelled. A relative leniency was given to this issue, since in general,
                            the lines on cuneiform tablets are not always straight (i.e., creating a
                            90° angle with the tablet itself). Therefore, a horizontal which may be
                            exactly horizontal when viewed in its line, will appear diagonal on the
                            image if the lines themselves are somewhat diagonal. For an example of
                            labelled images, see Fig. 3.</p>
                  <figure>
                     <head/>
                     <graphic url="media/image3.png"/>
                  </figure>
                  <p>Figure 3: Labelled horizontal strokes–training set example, tablet YPM BC
                            014442.</p>
               </div>
            </div>
            <div>
               <head>Evaluation Metrics</head>
               <p>Standard evaluation metrics include precision, sensitivity, and F-measure,
                        calculated from true positive rate (TP), false positive rate (FP), and false
                        negative rate (FN). These can be displayed in Table 2. From these, the
                        following metrics can be calculated to quantitatively assess the efficacy of
                        the model.</p>
               <table>
                  <row role="data">
                     <cell/>
                     <cell>Predicted Positive</cell>
                     <cell>Predicted Negative</cell>
                  </row>
                  <row role="data">
                     <cell>Actual Positive</cell>
                     <cell>True Positive (TP)</cell>
                     <cell>False Negative (FN)</cell>
                  </row>
                  <row role="data">
                     <cell>Actual Negative</cell>
                     <cell>False Positive (FP)</cell>
                     <cell>True Negative (TN)</cell>
                  </row>
               </table>
               <p>Table 2: TP is the proportion of cases that are correctly identified as
                        positive by the model. FP is the proportion of cases that are incorrectly
                        classified as positive by the model. FN is the proportion of cases that are
                        incorrectly identified as negative by the model. TN is the proportion of
                        cases that are correctly identified as negative.</p>
               <list rend="bulleted">
                  <item>Accuracy or Precision (p): precision measures how many of the total
                            number of predicted positive cases are true positives. In other words,
                            it is the ratio of true positives to the total number of predicted
                            positive cases, whether true or false. </item>
               </list>
               <p>
                  <mml:math>
                     <mml:mi>p</mml:mi>
                     <mml:mi> </mml:mi>
                     <mml:mo>=</mml:mo>
                     <mml:mfrac>
                        <mml:mrow>
                           <mml:mi>T</mml:mi>
                           <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                           <mml:mi>T</mml:mi>
                           <mml:mi>P</mml:mi>
                           <mml:mo>+</mml:mo>
                           <mml:mi>F</mml:mi>
                           <mml:mi>P</mml:mi>
                        </mml:mrow>
                     </mml:mfrac>
                  </mml:math>
                  <note>
                     <date when="2023-03-30T09:29:00Z"/>In
                            XML:&lt;math&gt; &lt;mrow&gt; &lt;mi&gt;p&lt;/mi&gt;
                            &lt;mo&gt;=&lt;/mo&gt; &lt;mfrac&gt; &lt;mrow&gt; &lt;mrow&gt;
                            &lt;mi&gt;TP&lt;/mi&gt; &lt;/mrow&gt; &lt;/mrow&gt; &lt;mrow&gt;
                            &lt;mi&gt;TP&lt;/mi&gt; &lt;mo&gt;+&lt;/mo&gt; &lt;mi&gt;FP&lt;/mi&gt;
                            &lt;/mrow&gt; &lt;/mfrac&gt; &lt;/mrow&gt;&lt;/math&gt;</note>
               </p>
               <list rend="bulleted">
                  <item>Sensitivity or Recall (s): sensitivity measures how many of the true
                            positive cases are correctly identified as positive by the model. In
                            other words, it is the ratio of true positives to the total number of
                            actual positive cases, which includes both true positives and false
                            negatives.</item>
               </list>
               <p>
                  <mml:math>
                     <mml:mi>s</mml:mi>
                     <mml:mi> </mml:mi>
                     <mml:mo>=</mml:mo>
                     <mml:mi> </mml:mi>
                     <mml:mfrac>
                        <mml:mrow>
                           <mml:mi>T</mml:mi>
                           <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                           <mml:mi>T</mml:mi>
                           <mml:mi>P</mml:mi>
                           <mml:mo>+</mml:mo>
                           <mml:mi>F</mml:mi>
                           <mml:mi>N</mml:mi>
                        </mml:mrow>
                     </mml:mfrac>
                  </mml:math>
                  <note>
                     <date when="2023-03-30T09:29:00Z"/>In
                            XML:&lt;math&gt; &lt;mrow&gt; &lt;mi&gt;s&lt;/mi&gt;
                            &lt;mo&gt;=&lt;/mo&gt; &lt;mfrac&gt; &lt;mrow&gt; &lt;mrow&gt;
                            &lt;mi&gt;TP&lt;/mi&gt; &lt;/mrow&gt; &lt;/mrow&gt; &lt;mrow&gt;
                            &lt;mi&gt;TP&lt;/mi&gt; &lt;mo&gt;+&lt;/mo&gt; &lt;mi&gt;FN&lt;/mi&gt;
                            &lt;/mrow&gt; &lt;/mfrac&gt; &lt;/mrow&gt;&lt;/math&gt;</note>
               </p>
               <list rend="bulleted">
                  <item>F-measure (F1): the F1 measure combines precision and sensitivity into
                            a single score, that is commonly used as the final assessment of a
                            model. It is the harmonic mean of precision and sensitivity, calculated
                            as 2 times the product of precision and sensitivity divided by their
                            sum, resulting in a number between 0 and 1 that can be viewed as a
                            percentage of overall accuracy.</item>
               </list>
               <p>
                  <mml:math>
                     <mml:mi>F</mml:mi>
                     <mml:mi> </mml:mi>
                     <mml:mo>=</mml:mo>
                     <mml:mi> </mml:mi>
                     <mml:mfrac>
                        <mml:mrow>
                           <mml:mn>2</mml:mn>
                           <mml:mo>∙</mml:mo>
                           <mml:mi>p</mml:mi>
                           <mml:mo>∙</mml:mo>
                           <mml:mi>s</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                           <mml:mi>p</mml:mi>
                           <mml:mo>+</mml:mo>
                           <mml:mi>s</mml:mi>
                        </mml:mrow>
                     </mml:mfrac>
                  </mml:math>
                  <note>
                     <date when="2023-03-30T09:30:00Z"/>In
                            XML:&lt;math&gt; &lt;mrow&gt; &lt;mi&gt;F-measure&lt;/mi&gt;
                            &lt;mo&gt;=&lt;/mo&gt; &lt;mfrac&gt; &lt;mrow&gt; &lt;mrow&gt;
                            &lt;mn&gt;2&lt;/mn&gt; &lt;mo&gt;*&lt;/mo&gt; &lt;mi&gt;p&lt;/mi&gt;
                            &lt;mo&gt;*&lt;/mo&gt; &lt;mi&gt;s&lt;/mi&gt; &lt;/mrow&gt;
                            &lt;/mrow&gt; &lt;mrow&gt; &lt;mi&gt;p&lt;/mi&gt; &lt;mo&gt;+&lt;/mo&gt;
                            &lt;mi&gt;s&lt;/mi&gt; &lt;/mrow&gt; &lt;/mfrac&gt;
                            &lt;/mrow&gt;&lt;/math&gt;</note>
               </p>
            </div>
         </div>
         <div>
            <head>Results</head>
            <p>Our goal was to test several types of object detectors to compare which one gives
                    the best results for our task. According to theoretical comparisons on public
                    datasets, one-stage algorithms (here YOLOv5) should give faster but less
                    accurate results compared to two-stage detectors (here Detecto and R-CNN).</p>
            <p>While testing with the YOLOv5 detector took only 1.189 seconds, the overall
                    accuracy was just over 40%, which is not sufficient for practical usability. The
                    prediction using the R-CNN network took on average 45 seconds, but the results
                    did not even reach the YOLOv5 level. We believe that this was due to a lack of
                    tuning of the hyperparameters and may be the subject of further experiments.
                    Detecto, which was not as fast as YOLOv5 but not as slow as R-CNN, achieved
                    results that far outperformed both previous algorithms with its 90.5%
                    sensitivity and 81.7% F-score. The reason behind this fact may be that Detecto
                    is an optimised network that combines the principles of a two-stage detector
                    with ResNet. Detailed evaluation results are shown in Table 3 and Figs. 4 and
                    5.</p>
            <table>
               <row role="data">
                  <cell>Name</cell>
                  <cell>TP</cell>
                  <cell>FN</cell>
                  <cell>FP</cell>
                  <cell>p</cell>
                  <cell>s</cell>
                  <cell>F1</cell>
                  <cell>Fake of all found strokes</cell>
               </row>
               <row role="data">
                  <cell>Detecto (threshold 0.4)</cell>
                  <cell>669</cell>
                  <cell>70</cell>
                  <cell>229</cell>
                  <cell>74.4%</cell>
                  <cell>90.5%</cell>
                  <cell>81.7%</cell>
                  <cell>25.5%</cell>
               </row>
               <row role="data">
                  <cell>YOLOv5 (threshold 0.2)</cell>
                  <cell>323</cell>
                  <cell>419</cell>
                  <cell>444</cell>
                  <cell>42.1%</cell>
                  <cell>43.5%</cell>
                  <cell>42.8%</cell>
                  <cell>57.8%</cell>
               </row>
               <row role="data">
                  <cell>YOLOv5 (threshold 0.3)</cell>
                  <cell>256</cell>
                  <cell>486</cell>
                  <cell>305</cell>
                  <cell>45.6%</cell>
                  <cell>34.5%</cell>
                  <cell>39.2%</cell>
                  <cell>54.4%</cell>
               </row>
               <row role="data">
                  <cell>YOLOv5 (threshold 0.4)</cell>
                  <cell>196</cell>
                  <cell>546</cell>
                  <cell>190</cell>
                  <cell>50.7%</cell>
                  <cell>26.4%</cell>
                  <cell>34.7%</cell>
                  <cell>49.2%</cell>
               </row>
               <row role="data">
                  <cell>R-CNN (threshold 0.4)</cell>
                  <cell>191</cell>
                  <cell>595</cell>
                  <cell>941</cell>
                  <cell>16.9%</cell>
                  <cell>24.8%</cell>
                  <cell>19.9%</cell>
                  <cell>83.1%</cell>
               </row>
            </table>
            <p>Table 3: Evaluation results</p>
            <figure>
               <head/>
               <graphic url=""/>
            </figure>
            <p>Figure 4: Evaluation results of object detection
                    algorithms</p>
            <table>
               <row role="data">
                  <cell>Prediction</cell>
                  <cell>Network</cell>
                  <cell>Tablet</cell>
               </row>
               <row role="data">
                  <cell>�<note>unable to handle Word object, possibly embedded
                                    spreadsheet or equation</note>
                  </cell>
                  <cell>Detecto</cell>
                  <cell>YPM BC 016773</cell>
               </row>
               <row role="data">
                  <cell>�<note>unable to handle Word object, possibly embedded
                                    spreadsheet or equation</note>
                  </cell>
                  <cell>YOLOv5</cell>
                  <cell>YPM BC 018686</cell>
               </row>
               <row role="data">
                  <cell>�<note>unable to handle Word object, possibly embedded
                                    spreadsheet or equation</note>
                  </cell>
                  <cell>R-CNN</cell>
                  <cell>YPM BC 023856</cell>
               </row>
            </table>
            <p>Figure 5: Prediction with respective
                        detectors<note>
                  <date when="2023-09-03T17:56:00Z"/>@Adela, please either have every tablet
                                image with its corresponding vectorization, or remove the
                                vectorization altogether. Best to have all three viewed one next to
                                each other: with bounding boxes, without bounding boxes, and
                                vectorization.Thanks!!</note>
            </p>
         </div>
         <div>
            <head>Discussion</head>
            <p>The results of the machine learning model we developed–90.5 % accuracy and 81.7 %
                    F-measure on Detecto–are very promising, particularly considering the relatively
                    small amount of labelled data. It shows that this previously untested approach,
                    namely, stroke identification from 2D images, can be highly efficient for the
                    vectorization of cuneiform tablets. While stroke identification has already been
                    achieved in 3D models of cuneiform-bearing objects (see section 1.1), our
                    approach shows the same is possible for 2D images which are far cheaper to
                    produce.</p>
            <p>Furthermore, our model is not period-dependent, i.e., some of the tablets we have
                    chosen are over a thousand years apart (see Table 1). But since the writing
                    technique itself has not changed during that period, there were no significant
                    differences in the model’s ability to recognize the strokes. The major attested
                    difference between stroke types in Mesopotamia, is Babylonian vs. Assyrian
                    strokes, the former having longer bodies (the tracing line), and the latter
                    bigger heads (the central point of the impression left on the clay tablet;
                    [Edzard 1980]; [Labat 1988]. This, however, does not seem to affect our
                    model.</p>
            <p>Since it was possible to effectively identify horizontal strokes, the same is
                    possible for verticals and obliques. With this additional ability, it will be
                    possible to create a full vectorization of cuneiform tablets, which will need to
                    be minimally corrected by an expert. These vectorizations are in effect like
                    hand-copies, which are a first step in assyriological research in interpreting
                    cuneiform texts. It will be the first step in a human-in-the-loop pipeline of
                    cuneiform identification from image to digital text. </p>
            <p>The subsequent step, identification of constellations of strokes as cuneiform
                    signs, is under development by part of the authors [Gordin and Romach 2022],
                    currently using traditional hand-copies as input (see <ref target="https://www.ben-digpasts.com/demo">demo</ref>; see [Yamauchi et al.
                    2018] for previous work in this direction). Once the signs are identified with
                    their equivalent in Unicode cuneiform [Cohen et al. 2004], these can be
                    transliterated and segmented into words using the model Akkademia, previously
                    developed by the assyriologists on our team and others [Gordin et al. 2020].
                    This can be further followed by machine translation for the two main languages
                    which used the cuneiform writing system, Sumerian and Akkadian. The Machine
                    Translation and Automated Analysis of Cuneiform Languages (MTAAC) project has
                    begun developing models for translating Ur III administrative texts (dated to
                    the 21st century BCE) [Punia et al. 2020]. Machine
                    translation for Akkadian has also been achieved, focusing primarily on first
                    millennium BCE texts from a variety of genres, available through <ref target="http://oracc.org/">ORACC</ref> [Gutherz and Gordin et al. 2023].</p>
            <p>This pipeline can become a vital part of assyriological research, by making
                    accessible countless cuneiform texts that have received less scholarly attention
                    before, to experts and laypeople alike. However, it is important to note the
                    limitations of this pipeline for assyriological research.</p>
            <p>The vector images we produce are not an accurate representation of the stroke,
                    but rather a chosen schema. Although various schemas can be selected, they are
                    still one simplistic representation on how a stroke looks, which is then applied
                    across the corpus. Therefore, for purposes of scribal hand identification, as
                    well as palaeography, they lack important aspects, such as <hi rend="italic">ductus</hi>, i.e. the formation of individual signs, and <hi rend="italic">aspect</hi> (cf. Latin <hi rend="italic">equilibrium</hi>), which is the
                    visual impression created by the set hand of a scribe (i.e. the style of
                    writing). This is the same, however, for manually created hand-copies, since
                    there are limitations to how these 3D objects can be captured in 2D, and some
                    scholars tend to simplify what they see on the tablet when creating
                    hand-copies.</p>
            <p>In addition, our results worked well on very high quality 2D images, curated by
                    an expert [Wagensonner 2015]. Although anyone can take high-quality images on
                    their phone, it usually requires an expert knowledge of the cuneiform script and
                    applying light sources in order for the signs and strokes to be as legible as
                    possible. For this to efficiently work on a large scale, preferably only
                    high-quality 2D images of cuneiform artifacts should be used.</p>
         </div>
         <div>
            <head>Towards Quantitative Epigraphy</head>
            <p>The task of the epigrapher is to decipher the ancient writing surface. Not merely
                    to decipher the script or any linguistic element on its own, but rather to
                    produce a holistic decipherment of the inscription, its material aspects, as
                    well as its contextual meaning. Therefore, it is challenging to translate
                    epigraphic tasks into one or more computational tasks. The current contribution
                    is a step in this direction, by attempting to gouge out the atomized elements of
                    the script and its arrangement on the writing surface. This diplomatic approach
                    to texts has a long history in medieval scholarly practice [Duranti 1998];
                    [Bertrand 2010], and it is a <hi rend="italic">desideratum</hi> in order to
                    piece together computational tasks for quantitative epigraphy. It is further a
                    way to bridge the differences across large numbers of ancient or little-known
                    languages and scripts, since much of the literature surrounding their study
                    involves discussions on reconstructing the writing surface, traces, and their
                    proper sequence, in their <hi rend="italic">Sitz im Leben</hi>.</p>
            <p>The problem begins when one tries to harmonize tasks from the disciplines in the
                    realm of computer science and the different research questions in the
                    humanities, which do not necessarily overlap. For an epigrapher, identifying and
                    classifying an object in an image is not the end goal, as it might be in
                    computer science–it is a step in a process to reach historical understanding of
                    a certain genre of text, a certain writing tradition, a certain historical
                    period. Furthermore, the amounts of data that are available to train generative
                    models like ChatGPT or the many image generator applications made available in
                    recent months, is beyond the scope of digital data at the hands of the average
                    epigrapher, historian, or digital humanist.</p>
            <p>For that end, an interdisciplinary group of scholars dealing with ancient
                    language processing and machine learning for ancient languages [Anderson et al.
                    2023]; [Sommerschield et al. 2023], has set out to better define and standardize
                    data formats, tasks, and benchmarks. This initiative adds to the growing
                    movement of computational and quantitative studies in classics, biblical
                    studies, ancient Near Eastern studies, and so on. The present paper is aimed to
                    contribute to the standardization of the epigrapher’s computational tasks in
                    ancient scripts, and as an example of how to harmonize and collaborate between
                    computer scientists and humanists, and computer science tasks and humanistic
                    research questions.</p>
            <p>Furthermore, the methodology and techniques used in this study can be applied to
                    other writing systems beyond cuneiform. The semi-automatic vectorization
                    approach can be adapted to identify and extract specific features of other
                    ancient scripts. In ancient Chinese and Japanese for example, one can try to
                    find the common denominator made up of strokes, the components of each
                    character. In classical Maya writing, one could focus on the anthropomorphic
                    elements of signs, like noses, eyes, ears, etc., and the same can be said for
                    other Hieroglyphic scripts, like Egyptian or Anatolian Hieroglyphs.</p>
         </div>
         <div>
            <head>Appendix</head>
            <div>
               <head>6.1 Training Convolutional Neural Networks</head>
               <p>In the following section, we present the parameters necessary to replicate
                        our results.</p>
               <p>For all the models we employed, image augumentation methods were necessary to
                        increase the number of available images for training. Grayscale
                        augmentations were applied: Saturation: applied to 50 % of the images,
                        Saturation: applied to 50 % of the images: Saturation: between -20 % and +20
                        %, Exposure: between -20 % and +20 % with 3 samples per augmentation.</p>
               <div>
                  <head>Detecto training</head>
                  <p>For Detecto training, the dataset was divided into 3 subsets, the
                            training set contains 3456 images, the validation set contains 330
                            images, and the testing set contains 164 images. The training was
                            performed on Google Collaboratory, and the layers from the
                            fasterrcnn_resnet50_fpn_coco-258fb6c6.pth model were unfrozen and
                            re-trained. Fifty epochs were run, 3 steps per epoch, and the validation
                            loss dropped from 0.74 to 0.64 after all epochs, which took 302 minutes.
                            After 5 epochs, the validation loss did not decrease, so could have used
                            early stopping for this model.</p>
               </div>
               <div>
                  <head>YOLOv5 training</head>
                  <p>The YOLOv5 architecture (with 232 layers) was trained in Google
                            Colaboratory using CUDA on a Tesla T4 GPU with 40 multiprocessors and
                            15109 MB of total memory. 100 epochs were executed with a batch of 16
                            images. The training loss (MAE) was reduced from 0.1 in the first epoch
                            to 0.03 in the last epoch, as can be seen in Fig. 6.</p>
                  <figure>
                     <head/>
                     <graphic url="media/image7.png"/>
                  </figure>
                  <p>Figure 6: Training set loss, source: Tensorboard</p>
               </div>
               <div>
                  <head>R-CNN training</head>
                  <p>The whole implementation was done using the artificial intelligence lab
                            at the Czech University of Life Sciences in Prague, because R-CNN has
                            high memory requirements and caused Google Collaboratory to crash (due
                            to lack of memory). The environment settings as seen in Table 4 were
                            used:</p>
                  <table>
                     <row role="data">
                        <cell>IDE</cell>
                        <cell>VS Code with Jupyter
                                    extension</cell>
                     </row>
                     <row role="data">
                        <cell>Kernel</cell>
                        <cell>Python 3.8.12 within Anaconda</cell>
                     </row>
                     <row role="data">
                        <cell>AI framework</cell>
                        <cell>Tensorflow 2.7.0 for GPU</cell>
                     </row>
                     <row role="data">
                        <cell>Nvidia configuration</cell>
                        <cell>NVIDIA Quadro P400, cuda 11.2, cudnn
                                    8.1</cell>
                     </row>
                  </table>
                  <p>Table 4: R-CNN environment settings</p>
                  <p>We have implemented region proposals with selective search using IoU
                            (Intersection over Union) configured as seen in Table 5:</p>
                  <table>
                     <row role="data">
                        <cell>Max samples</cell>
                        <cell>55 (based on the maximum in training
                                    set)</cell>
                     </row>
                     <row role="data">
                        <cell>Selective search iterate
                                    results</cell>
                        <cell>2000 (proposed in original
                                    paper)</cell>
                     </row>
                     <row role="data">
                        <cell>IoU object limit</cell>
                        <cell>0.7</cell>
                     </row>
                     <row role="data">
                        <cell>IoU background limit</cell>
                        <cell>0.3</cell>
                     </row>
                  </table>
                  <p>Table 5: R-CNN configuration</p>
                  <p>The images used were 224 x 224 in size. We chose a VGG model pre-trained
                            on the ImageNet dataset (input layer, thirteen convolutional layers,
                            five MaxPooling layers, Flatten, Dense). After encoding the label set
                            once and splitting it into training (90 %) and test sets (10 %), we
                            proceeded to train with the configurations as seen in Table 6. Early
                            stopping caused the training process to stop after thirty-nine
                            epochs.</p>
                  <table>
                     <row role="data">
                        <cell>error function</cell>
                        <cell>binary cross-entropy</cell>
                     </row>
                     <row role="data">
                        <cell>optimizer</cell>
                        <cell>Adam</cell>
                     </row>
                     <row role="data">
                        <cell>learning rate</cell>
                        <cell>0.0001</cell>
                     </row>
                     <row role="data">
                        <cell>training epochs</cell>
                        <cell>100</cell>
                     </row>
                     <row role="data">
                        <cell>steps in epoch</cell>
                        <cell>10</cell>
                     </row>
                     <row role="data">
                        <cell>patience epochs for early
                                    stopping</cell>
                        <cell>20</cell>
                     </row>
                  </table>
                  <p>Table 6: R-CNN Training hyperparameters</p>
               </div>
            </div>
            <div>
               <head>Utilities for Further Research</head>
               <p>In order to ease the process of data creation for the next steps of the
                        project, we developed three image and label processing tools: image
                        splitter, vector visualization, and image merger. These tools are available
                        in the GitHub repository of our project.<note>
                     <ref target="https://github.com/adelajelinkova/cuneiform/tree/main/Utilities">https://github.com/adelajelinkova/cuneiform/tree/main/Utilities</ref>
                  </note> The neural networks that we used for object detection accept
                        square input, and if it is not square, the image is reshaped to a standard
                        input size. For large tablets, there would be a significant loss of data, so
                        it is necessary to slice large images into smaller, uniformly sized square
                        images and train the network on these slices. We chose a fixed image size of
                        416 x 416–a multiple of 8, which is generally better for machine learning
                        purposes [Chollet and Pecinovský 2019].</p>
               <p>While for the research presented in this article, we split the images before
                        labelling, this slowed down the labelling process. Therefore, we developed
                        an image splitter and an image merger. Our proposed system works as follows:
                        after labelling, a large image with a cuneiform-bearing object is cut into
                        squares with 50% overlap, so there is no data loss if strokes are on the
                        edge of one square, since they are in the middle of the next one. Then the
                        neural network predicts where the horizontal strokes are in the image. The
                        networks return bounding boxes which indicate the location of the strokes.
                        These bounding boxes are replaced by vectors of strokes in an empty image,
                        and the strokes in the whole tablet are reconstructed using merging. In this
                        way we can create an automatic vectorization of horizontal (and other)
                        strokes in the whole tablet (see Fig. 7).</p>
               <figure>
                  <head/>
                  <graphic url="media/image8.png"/>
               </figure>
               <p>Figure 7: Output image from the vector visualisation tool, tablet YPM BC
                        021234.</p>
               <p>The main challenge in preparing the set of tools was dealing with splitting
                        labels. When splitting the image into smaller squares, there is a cut-off
                        threshold for deciding whether the annotated strokes are still significant
                        enough to be used in training. The threshold is based on a percentage that
                        determines what portion of the annotated strokes can be kept and what should
                        be removed.</p>
            </div>
         </div>
         <div>
            <head>Acknowledgements</head>
            <p>This research was funded by two grants: project Cuneiform analysis using
                    Convolutional Neural Networks reg. no. 31/2021 was financed from the OP RDE
                    project Improvement in Quality of the Internal Grant Scheme at CZU, reg. no.
                    CZ.02.2.69/0.0/0.0/19_073/0016944; The project no. RA2000000010 was financed by
                    the CULS – Ariel University cooperation grant.</p>
         </div>
         <div>
            <head>Bibliography</head>
            <p>[Anderson et al. 2023] Anderson, A. <hi rend="italic">et al.</hi> (eds) (2023) <hi rend="italic">Proceedings of the
                        Ancient Language Processing Workshop (RANLP-ALP 2023)</hi>. Shoumen,
                    Bulgaria: INCOMA Ltd.</p>
            <p>[André-Salvini and Lombard 1997] André-Salvini, B.
                    and Lombard, P. (1997) ‘La découverte épigraphique de 1995 à Qal’at al-Bahrein:
                    un jalon pour la chronologie de la phase Dilmoun Moyen dans le Golfe arabe’, <hi rend="italic">Proceedings of the Seminar for Arabian Studies</hi>, 27, pp.
                    165–170.</p>
            <p>[Bertrand 2010] Bertrand, P. (2010) ‘Du De re
                    diplomatica au Nouveau traité de diplomatique: réception des textes fondamentaux
                    d’une discipline’, in J. Leclant, A. Vauchez, and D.-O. Hurel (eds) <hi rend="italic">Dom Jean Mabillon, figure majeure de l’Europe des lettres:
                        Actes des deux colloques du tricentenaire de la mort de Dom Mabillon (abbaye
                        de Solesmes, 18-19 mai 2007</hi>. Paris: Académie des Inscriptions et
                    Belles-Lettres, pp. 605–619.</p>
            <p>[Bogacz, Gertz and Mara 2015a] Bogacz, B., Gertz, M.
                    and Mara, H. (2015a) ‘Character retrieval of vectorized cuneiform script’, in
                        <hi rend="italic">13th international conference on document analysis and
                        recognition (ICDAR 2015)</hi>. Piscataway, NJ: IEEE Computer Society, pp.
                    326–330. Available at:<ref target="https://doi.org/10.1109/ICDAR.2015.7333777">
                        https://doi.org/10.1109/ICDAR.2015.7333777</ref>.</p>
            <p>[Bogacz, Gertz and Mara 2015b] Bogacz, B., Gertz, M.
                    and Mara, H. (2015b) ‘Cuneiform character similarity using graph
                    representations’, in P. Wohlhart and V. Lepetit (eds) <hi rend="italic">20th
                        computer vision winter workshop</hi>. Graz: Verlag der Technischen
                    Universität Graz, pp. 105–112.</p>
            <p>[Bogacz, Howe and Mara 2016] Bogacz, B., Howe, N.
                    and Mara, H. (2016) ‘Segmentation free spotting of cuneiform using part
                    structured models’, in <hi rend="italic">15th international conference on
                        frontiers in handwriting recognition (ICFHR 2016)</hi>. Piscataway, NJ: IEEE
                    Computer Society, pp. 301–306. Available at:<ref target="https://doi.org/10.1109/ICFHR.2016.0064">
                        https://doi.org/10.1109/ICFHR.2016.0064</ref>.</p>
            <p>[Bogacz and Mara 2018] Bogacz, B. and Mara, H.
                    (2018) ‘Feature descriptors for spotting 3D characters on triangular meshes’, in
                        <hi rend="italic">16th international conference on frontiers in handwriting
                        recognition (ICFHR 2018)</hi>. Piscataway, NJ: IEEE Computer Society, pp.
                    363–368.</p>
            <p>[Bogacz and Mara 2022] Bogacz, B. and Mara, H.
                    (2022) ‘Digital Assyriology — Advances in visual cuneiform analysis’, <hi rend="italic">Journal on Computing and Cultural Heritage</hi>, 15(2).
                    Available at:<ref target="https://doi.org/10.1145/3491239">
                        https://doi.org/10.1145/3491239</ref>.</p>
            <p>[Bramanti 2015] Bramanti, A. (2015) ‘The Cuneiform
                    Stylus. Some Addenda’, <hi rend="italic">Cuneiform Digital LIbrary Notes</hi>,
                    2015, p. 12.</p>
            <p>[Cammarosano et al. 2014] Cammarosano, M. <hi rend="italic">et al.</hi> (2014) ‘Schriftmetrologie des Keils:
                    Dreidimensionale Analyse von Keileindrücken und Handschriften’, <hi rend="italic">Die Welt des Orients</hi>, 44(1), pp. 2–36.</p>
            <p>[Cammarosano 2014] Cammarosano, M. (2014) ‘The
                    Cuneiform Stylus’, <hi rend="italic">Mesopotamia: Rivista di Archeologia,
                        Epigrafia e Storia Orientale Antica</hi>, 69, pp. 53–90.</p>
            <p>[Cho et al. 2016] Cho, J. <hi rend="italic">et
                        al.</hi> (2016) ‘How much data is needed to train a medical image deep
                    learning system to achieve necessary high accuracy?’ arXiv. Available at:<ref target="http://arxiv.org/abs/1511.06348">
                        http://arxiv.org/abs/1511.06348</ref>
                    (Accessed: 5 September 2023).</p>
            <p>[Chollet and Pecinovský 2019] Chollet, F. and
                    Pecinovský, R. (2019) <hi rend="italic">Deep Learning V Jazyku Python: Knihovny
                        Keras, TensorFlow</hi>. Praha: Grada Publishing a.s.</p>
            <p>[Cohen et al. 2004] Cohen, J. <hi rend="italic">et
                        al.</hi> (2004) ‘iClay: Digitizing cuneiform’, in Y. Chrysanthou et al.
                    (eds) <hi rend="italic">The 5th international symposium on virtual reality,
                        archaeology and cultural heritage (VAST 2004)</hi>. Goslar: The Eurographics
                    Association, pp. 135–143. Available at:<ref target="https://doi.org/10.2312/VAST/VAST04/135-143">
                        https://doi.org/10.2312/VAST/VAST04/135-143</ref>.</p>
            <p>[Collins et al. 2019] Collins, T. <hi rend="italic">et al.</hi> (2019) ‘Automated low-cost photogrammetric acquisition of 3D
                    models from small form-factor artefacts’, <hi rend="italic">Electronics</hi>, 8,
                    p. 1441. Available at:<ref target="https://doi.org/10.3390/electronics8121441">
                        https://doi.org/10.3390/electronics8121441</ref>.</p>
            <p>[Dahl, Hameeuw and Wagensonner 2019] Dahl, J.L.,
                    Hameeuw, H. and Wagensonner, K. (2019) ‘Looking both forward and back: imaging
                    cuneiform’, <hi rend="italic">Cuneiform Digital Library Preprints</hi>
                    [Preprint], (14.0). Available at:<ref target="https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0">
                        https://cdli.mpiwg-berlin.mpg.de/articles/cdlp/14.0</ref>
                    (Accessed: 5 September 2023).</p>
            <p>[Dencker et al. 2020] Dencker, T. <hi rend="italic">et al.</hi> (2020) ‘Deep learning of cuneiform sign detection with weak
                    supervision using transliteration alignment’, <hi rend="italic">PLOS ONE</hi>,
                    15(12), p. e0243039. Available at:<ref target="https://doi.org/10.1371/journal.pone.0243039">
                        https://doi.org/10.1371/journal.pone.0243039</ref>.</p>
            <p>[Duranti 1998] Duranti, L. (1998) ‘Diplomatics: New
                    Uses for an Old Science’. Chicago: Society of American Archivists, Association
                    of Canadian Archivists and Scarecrow Press.</p>
            <p>[Earl et al. 2011] Earl, G. <hi rend="italic">et
                        al.</hi> (2011) ‘Reflectance transformation imaging systems for ancient
                    documentary artefacts’, in S. Dunnand, J.P. Bowen, and K.C. Ng (eds) <hi rend="italic">Electronic visualisation and the arts (EVA 2011)</hi>. BCS:
                    The Chartered Institute for IT, pp. 147–154.</p>
            <p>[Edzard 1980] Edzard, D.O. (1980) ‘Keilschrift’, <hi rend="italic">Reallexikon der Assyriologie</hi>, 5, pp. 544–568.</p>
            <p>[Fisseler et al. 2013] Fisseler, D. <hi rend="italic">et al.</hi> (2013) ‘Towards an interactive and automated
                    script feature Analysis of 3D scanned cuneiform tablets’, in <hi rend="italic">Scientific computing and cultural heritage 2013</hi>. Available at:<ref target="http://www.cuneiform.de/fileadmin/user_upload/documents/scch2013_fisseler.pdf">
                        http://www.cuneiform.de/fileadmin/user_upload/documents/scch2013_fisseler.pdf</ref>.</p>
            <p>[Fisseler et al. 2014] Fisseler, D. <hi rend="italic">et al.</hi> (2014) ‘Extending philological research with
                    methods of 3D computer graphics applied to analysis of cultural heritage’, in R.
                    Klein and P. Santos (eds) <hi rend="italic">Eurographics workshop on graphics
                        and cultural heritage (GCH 2014)</hi>. Goslar: The Eurographics Association,
                    pp. 165–172. Available at:<ref target="https://doi.org/10.2312/gch.20141314">
                        https://doi.org/10.2312/gch.20141314</ref>.</p>
            <p>[Girshick et al. 2014] Girshick, R. <hi rend="italic">et al.</hi> (2014) ‘Rich Feature Hierarchies for Accurate
                    Object Detection and Semantic Segmentation’, in. <hi rend="italic">2014 IEEE
                        Conference on Computer Vision and Pattern Recognition (CVPR)</hi>, IEEE
                    Computer Society, pp. 580–587. Available at:<ref target="https://doi.org/10.1109/CVPR.2014.81">
                        https://doi.org/10.1109/CVPR.2014.81</ref>.</p>
            <p>[Girshick 2015] Girshick, R. (2015) ‘Fast R-CNN’, in
                        <hi rend="italic">2015 IEEE International Conference on Computer Vision
                        (ICCV)</hi>. <hi rend="italic">2015 IEEE International Conference on
                        Computer Vision (ICCV)</hi>, pp. 1440–1448. Available at:<ref target="https://doi.org/10.1109/ICCV.2015.169">
                        https://doi.org/10.1109/ICCV.2015.169</ref>.</p>
            <p>[Gordin et al. 2020] Gordin, Sh. <hi rend="italic">et al.</hi> (2020) ‘Reading Akkadian Cuneiform using Natural Language
                    Processing’, <hi rend="italic">PLOS ONE</hi>, 15(10), p. e0240511. Available
                        at:<ref target="https://doi.org/10.1371/journal.pone.0240511">
                        https://doi.org/10.1371/journal.pone.0240511</ref>.</p>
            <p>[Gordin and Romach 2022] Gordin, Sh. and Romach, A.
                    (2022) ‘Optical Character Recognition for Complex Scripts: A Case-study in
                    Cuneiform’, in. <hi rend="italic">ADHO 2022 - Tokyo</hi>. Available at:<ref target="https://dh-abstracts.library.cmu.edu/works/11708">
                        https://dh-abstracts.library.cmu.edu/works/11708</ref>.</p>
            <p>[Gutherz and Gordin et al. 2023] Gutherz, G. <hi rend="italic">et al.</hi> (2023) ‘Translating Akkadian to English with
                    neural machine translation’, <hi rend="italic">PNAS Nexus</hi>, 2(5), p.
                    pgad096. Available at:<ref target="https://doi.org/10.1093/pnasnexus/pgad096">
                        https://doi.org/10.1093/pnasnexus/pgad096</ref>.</p>
            <p>[Hameeuw and Willems 2011] Hameeuw, H. and Willems,
                    G. (2011) ‘New visualization techniques for cuneiform texts and sealings’, <hi rend="italic">Akkadica</hi>, 132(3), pp. 163–178.</p>
            <p>[Jiao et al. 2019] Jiao, L. <hi rend="italic">et
                        al.</hi> (2019) ‘A survey of deep learning-based object detection’, <hi rend="italic">IEEE Access</hi>, 7, pp. 128837–128868. Available at:<ref target="https://doi.org/10.1109/ACCESS.2019.2939201">
                        https://doi.org/10.1109/ACCESS.2019.2939201</ref>.</p>
            <p>[Jocher et al. 2020] Jocher, G. <hi rend="italic">et
                        al.</hi> (2020) ‘ultralytics/yolov5: Initial Release’. Zenodo. Available
                        at:<ref target="https://doi.org/10.5281/zenodo.3908560">
                        https://doi.org/10.5281/zenodo.3908560</ref>.</p>
            <p>[Kriege et al. 2018] Kriege, N.M. <hi rend="italic">et al.</hi> (2018) ‘Recognizing cuneiform signs using graph based methods’,
                    in L. Torgo et al. (eds) <hi rend="italic">Proceedings of the international
                        workshop on cost-sensitive learning</hi>. PMLR (Proceedings of machine
                    learning research), pp. 31–44. Available at:<ref target="https://proceedings.mlr.press/v88/kriege18a.html">
                        https://proceedings.mlr.press/v88/kriege18a.html</ref>.</p>
            <p>[Labat 1988] Labat, R. and Malbran-Labat, F. (1988)
                        <hi rend="italic">Manuel d’épigraphie akkadienne</hi>. 6th edn. Paris.</p>
            <p>[Mara et al. 2010] Mara, H. <hi rend="italic">et
                        al.</hi> (2010) ‘GigaMesh and gilgamesh – 3D multiscale integral invariant
                    cuneiform character extraction’, in A. Artusi et al. (eds) <hi rend="italic">The
                        11th international symposium on virtual reality, archaeology and cultural
                        heritage</hi>. <hi rend="italic">VAST: International Symposium on Virtual
                        Reality, Archaeology and Intelligent Cultural Heritage</hi>, The
                    Eurographics Association. Available at:<ref target="https://doi.org/10.2312/VAST/VAST10/131-138">
                        https://doi.org/10.2312/VAST/VAST10/131-138</ref>.</p>
            <p>[Mara and Krömker 2013] Mara, H. and Krömker, S.
                    (2013) ‘Vectorization of 3D-Characters by integral invariant filtering of
                    high-resolution triangular meshes’, in <hi rend="italic">Proceedings of the
                        international conference on document analysis and recognition (ICDAR
                        2013)</hi>. Piscataway, NJ: IEEE Computer Society, pp. 62–66. Available
                        at:<ref target="https://doi.org/10.1109/ICDAR.2013.21">
                        https://doi.org/10.1109/ICDAR.2013.21</ref>.</p>
            <p>[Massa et al. 2016] Massa, J. <hi rend="italic">et
                        al.</hi> (2016) ‘Cuneiform detection in vectorized raster images’, in L.
                    Čehovin, R. Mandeljc, and V. Štruc (eds) <hi rend="italic">21st computer vision
                        winter workshop</hi>. Ljubljana: Slovenian Pattern Recognition Society.</p>
            <p>[Mishra 2022] Mishra, D. (2022) ‘Deep Learning based
                    Object Detection Methods: A Review’, <hi rend="italic">Medicon Engineering
                        Themes</hi>, 2(4). Available at:<ref target="https://doi.org/10.55162/MCET.02.027">
                        https://doi.org/10.55162/MCET.02.027</ref>.</p>
            <p>[Pavlíček 2018] Pavlíček, J. <hi rend="italic">et
                        al.</hi> (2018) ‘Automated Wildlife Recognition"’, <hi rend="italic">AGRIS
                        on-line Papers in Economics and Informatics</hi>, 10(1), pp. 51–60.
                    Available at:<ref target="https://doi.org/10.7160/aol.2018.100105">
                        https://doi.org/10.7160/aol.2018.100105</ref>.</p>
            <p>[Punia et al. 2020] Punia, R. <hi rend="italic">et
                        al.</hi> (2020) ‘Towards the first machine translation system for Sumerian
                    transliterations’, in <hi rend="italic">Proceedings of the 28th international
                        conference on computational linguistics</hi>. Barcelona: International
                    Committee on Computational Linguistics, pp. 3454–3460. Available at:<ref target="https://doi.org/10.18653/v1/2020.coling-main.308">
                        https://doi.org/10.18653/v1/2020.coling-main.308</ref>.</p>
            <p>[Redmon et al. 2016] Redmon, J. <hi rend="italic">et
                        al.</hi> (2016) ‘You Only Look Once: Unified, Real-Time Object Detection’,
                    in <hi rend="italic">2016 IEEE Conference on Computer Vision and Pattern
                        Recognition (CVPR)</hi>. <hi rend="italic">2016 IEEE Conference on Computer
                        Vision and Pattern Recognition (CVPR)</hi>, Las Vegas, NV, USA: IEEE, pp.
                    779–788. Available at:<ref target="https://doi.org/10.1109/CVPR.2016.91">
                        https://doi.org/10.1109/CVPR.2016.91</ref>.</p>
            <p>[Ren et al. 2015] Ren, S. <hi rend="italic">et
                        al.</hi> (2015) ‘Faster R-CNN: Towards Real-Time Object Detection with
                    Region Proposal Networks’, in <hi rend="italic">Advances in Neural Information
                        Processing Systems</hi>. Curran Associates, Inc. Available at:<ref target="https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html">
                        https://papers.nips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html</ref>
                    (Accessed: 5 September 2023).</p>
            <p>[Rothacker et al. 2015] Rothacker, L. <hi rend="italic">et al.</hi> (2015) ‘Retrieving cuneiform structures in a
                    segmentation-free word spotting framework’, in <hi rend="italic">Proceedings of
                        the 3rd international workshop on historical document imaging and processing
                        (HIP 2015)</hi>. New York, NY: Association for Computing Machinery, pp.
                    129–136. Available at:<ref target="https://doi.org/10.1145/2809544.2809562">
                        https://doi.org/10.1145/2809544.2809562</ref>.</p>
            <p>[Rusakov et al. 2019] Rusakov, E. <hi rend="italic">et al.</hi> (2019) ‘Generating cuneiform signs with cycle-consistent
                    adversarial networks’, in <hi rend="italic">Proceedings of the 5th international
                        workshop on historical document imaging and processing</hi>. New York, NY,
                    USA: Association for Computing Machinery (HIP ’19), pp. 19–24. Available at:<ref target="https://doi.org/10.1145/3352631.3352632">
                        https://doi.org/10.1145/3352631.3352632</ref>.</p>
            <p>[Rusakov et al. 2020] Rusakov, E. <hi rend="italic">et al.</hi> (2020) ‘Towards query-by-expression retrieval of cuneiform
                    signs’, in <hi rend="italic">2020 17th international conference on frontiers in
                        handwriting recognition (ICFHR)</hi>, pp. 43–48. Available at:<ref target="https://doi.org/10.1109/ICFHR2020.2020.00019">
                        https://doi.org/10.1109/ICFHR2020.2020.00019</ref>.</p>
            <p>[Sommerschield et al. 2023] Sommerschield, T. <hi rend="italic">et al.</hi> (2023) ‘Machine Learning for Ancient Languages: A
                    Survey’, <hi rend="italic">Computational Linguistics</hi>, pp. 1–44. Available
                        at:<ref target="https://doi.org/10.1162/coli_a_00481">
                        https://doi.org/10.1162/coli_a_00481</ref>.</p>
            <p>[Streck 2010] Streck, M.P. (2010) ‘Großes Fach
                    Altorientalistik: Der Umfang des keilschriftlichen Textkorpus’, <hi rend="italic">Mitteilungen der Deutschen Orient-Gesellschaft</hi>.</p>
            <p>[Taylor 2015] Taylor, J. (2015) ‘Wedge Order in
                    Cuneiform: a Preliminary Survey’, in E. Devecchi, G.G.W. Müller, and J. Mynářová
                    (eds) <hi rend="italic">Current Research in Cuneiform Palaeography: Proceedings
                        of the Workshop organised at the 60th Rencontre Assyriologique
                        Internationale, Warsaw 2014</hi>. Gladbeck: PeWe-Verlag, pp. 1–30.</p>
            <p>[Wagensonner 2015] Wagensonner, K. (2015) ‘On an
                    alternative way of capturing RTI images with the camera dome’, <hi rend="italic">CDLN</hi>, 2015(1), pp. 1–12.</p>
            <p>[Wevers and Smits 2019] Wevers, M. and Smits, T.
                    (2020) ‘The visual digital turn: Using neural networks to study historical
                    images’, <hi rend="italic">Digital Scholarship in the Humanities</hi>, 35(1),
                    pp. 194–207. Available at:<ref target="https://doi.org/10.1093/llc/fqy085">
                        https://doi.org/10.1093/llc/fqy085</ref>.</p>
            <p>[Yamauchi et al. 2018] Yamauchi, K., Yamamoto, H.
                    and Mori, W. (2018) ‘Building A Handwritten Cuneiform Character Imageset’, in
                    N.C. (Conference chair) et al. (eds) <hi rend="italic">Proceedings of the
                        Eleventh International Conference on Language Resources and Evaluation (LREC
                        2018)</hi>. <hi rend="italic">LREC 2018</hi>, Miyazaki, Japan: European
                    Language Resources Association (ELRA), pp. 719–722. Available at:<ref target="https://aclanthology.org/L18-1115">
                        https://aclanthology.org/L18-1115</ref>.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl/>
         </listBibl>
      </back>
   </text>
</TEI>
