<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 015 Number 1</div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Using an Advanced Text Index Structure for Corpus Exploration
                  in Digital Humanities</h1>
               
               <div class="author"><span style="color: grey">Tobias Englmeier</span> &lt;<a href="mailto:englmeier_at_cis_dot_uni-muenchen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('englmeier_at_cis_dot_uni-muenchen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('englmeier_at_cis_dot_uni-muenchen_dot_de'); return false;">englmeier_at_cis_dot_uni-muenchen_dot_de</a>&gt;, CIS, Ludwig-Maximilians University, Munich,
                  Germany</div>
               
               <div class="author"><span style="color: grey">Marco Büchler</span> &lt;<a href="mailto:mbuechler_at_etrap_dot_eu" onclick="javascript:window.location.href='mailto:'+deobfuscate('mbuechler_at_etrap_dot_eu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('mbuechler_at_etrap_dot_eu'); return false;">mbuechler_at_etrap_dot_eu</a>&gt;, Institute of Computer Science, University of Göttingen,
                  Göttingen, Germany</div>
               
               <div class="author"><span style="color: grey">Stefan Gerdjikov</span> &lt;<a href="mailto:st_gerdjikov_at_abv_dot_bg" onclick="javascript:window.location.href='mailto:'+deobfuscate('st_gerdjikov_at_abv_dot_bg'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('st_gerdjikov_at_abv_dot_bg'); return false;">st_gerdjikov_at_abv_dot_bg</a>&gt;, FMI, University of Sofia "St. Kliment Ohridski", Sofia,
                  Bulgaria</div>
               
               <div class="author"><span style="color: grey">Klaus U. Schulz</span> &lt;<a href="mailto:schulz_at_cis_dot_uni-muenchen_dot_de" onclick="javascript:window.location.href='mailto:'+deobfuscate('schulz_at_cis_dot_uni-muenchen_dot_de'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('schulz_at_cis_dot_uni-muenchen_dot_de'); return false;">schulz_at_cis_dot_uni-muenchen_dot_de</a>&gt;, CIS, Ludwig-Maximilians University, Munich,
                  Germany</div>
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Using%20an%20Advanced%20Text%20Index%20Structure%20for%20Corpus%20Exploration%20in%20Digital%20Humanities&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2021-05-21&amp;rft.volume=015&amp;rft.issue=1&amp;rft.aulast=Englmeier&amp;rft.aufirst=Tobias&amp;rft.au=Tobias%20Englmeier&amp;rft.au=Marco%20Büchler&amp;rft.au=Stefan%20Gerdjikov&amp;rft.au=Klaus U.%20Schulz"> </span></div>
            
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p> With suitable index structures many corpus exploration tasks can be solved in an
                     efficient way without rescanning the text repository in an online manner. In
                     this paper we show that symmetric compacted directed acyclic word graphs
                     (SCDAWGs) - a refinement of suffix trees - offer an ideal basis for corpus
                     exploration, helping to answer many of the questions raised in DH research in an
                     elegant way. From a simplified point of view, the advantages of SCDAWGs rely on
                     two properties. First, needing linear computation time, the index offers a joint
                     view on the similarities (in terms of common substrings) and differences between
                     all text. Second, structural regularities of the index help to mine interesting
                     portions of texts (such as phrases and concept names) and their relationship in
                     a language independent way without using prior linguistic knowledge. As a
                     demonstration of the power of these principles we look at text alignment, text
                     reuse in distinct texts or between distinct authors, automated detection of
                     concepts, temporal distribution of phrases in diachronic corpora, and related
                     problems. </p>
                  </div>
               
               
               
               
               <div id="section01" class="div div0">
                  
                  <h1 class="head">Introduction</h1>
                  
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">A text/corpus index is a kind of table that, given a string \(w\), stores the positions of all
                     occurrences of \(w\) in the
                     given text/corpus. The computation of the index is a preprocessing step to be
                     applied only once. Corpus index structures considerably simplify corpus analysis
                     since they help to avoid rescanning the complete texts for each task. The index
                     helps to directly answer many interesting questions, hence index based methods
                     in general are much faster and more elegant. In this paper we look at an
                     advanced corpus index structure and explain its potential use in Digital
                     Humanities. This index, called symmetric compacted directed acyclid word graph
                     (SCDAWG), is used to represent a collection of texts, each text is considered as
                     a flat sequence of symbols [<a class="ref" href="#inenaga2005">Inenaga et al. 2005</a>]. SCDAWGs can be
                     considered as a refinement of suffix trees [<a class="ref" href="#weiner1973">Weiner 1973</a>], [<a class="ref" href="#mccreight1976">McCreight 1976</a>], [<a class="ref" href="#ukkonen95">Ukkonen 1995</a>], [<a class="ref" href="#gusfield1997">Gusfield 1997</a>]. The SCDAWG for a given text collection, as the
                     suffix tree, can be computed in time and space linear in the size of the corpus.
                     As a first advantage, the SCDAWG of a corpus is much smaller than the
                     corresponding suffix tree. Even more importantly, SCDAWGs have two special
                     features that make them very attractive for various corpus exploration tasks. </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2"><span class="label bold">Finding co-occurrences.</span> Using the SCDAWG it is simple to find all
                     co-occurrences of the same portions of text in two, several, or all texts of the
                     corpus. The index enables a joint search in all texts, there is no need for
                     comparing all pairs of texts individually. This joint search, which leads to all
                     co-occurrences, can be realized using a single scan of the index. The importance
                     of this feature in our context should be obvious: research in Digital Humanities
                     is often concentrated on the problem of finding phrases and text portions that
                     occur in distinct (two, several, or all) texts of a collection. For all these
                     problems the use of SCDAWGs offers an elegant solution. As a demonstration we
                     look at alignment of two or several texts, detection of text reuses in distinct
                     texts, and detection of text reuses between distinct authors. </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3"><span class="label bold">Mining interesting concepts and their relationship.</span> The nodes of
                     the SCDAWG represent portions of text (sequences of symbols, infixes). In
                     general, the number of all distinct infixes of a corpus \(C\) is quadratic in size \(|C|\) of the corpus. In contrast, the number of
                     nodes of the SCDAWG is always linear in \(|C|\). Yet, in a sense to be explained below, it contains all
                     infixes that are interesting from a linguistic point of view.<a class="noteRef" href="#d4e253">[1]</a> For example, names, concepts and
                     phrases etc. in general correspond to nodes of the SCDAWG. Compared to suffix
                     trees, SCDAWGs yield a much smaller set of nodes/infixes with this property.
                     Using structural regularities of the index, the set of all nodes can be even
                     further filtered to approximate the subset of linguistically relevant infixes.
                     Also hierarchical relationships and dependencies between these concepts and
                     phrases can be detected. From the perspective of Digital Humanities, these
                     mining techniques are relevant since they are completely language independent
                     and do not use any kind of prior linguistic knowledge. They point to interesting
                     concepts and bricks of text, the index helps to study their use, structure and
                     relationship. </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">In this paper we first formally introduce SCDAWGs in <a href="#section02">Section 2</a>. We compare three related index structures, SCDAWGs, suffix
                     trees and DAWGs (directed acyclic graphs [<a class="ref" href="#blumer1987">Blumer et al. 1987</a>]), compare
                     the sizes of these index structures and explain the advantages of the SCDAWG for
                     corpus exploration tasks as those mentioned above. In <a href="#section03">
                        Section 3</a> we give a brief overview of the corpora used for the
                     experiments described in the paper. In <a href="#section04"> Section 4</a>
                     we show how SCDAWGs can be utilized to solve tasks in the fields of text
                     alignment and text reuse detection [<a class="ref" href="#b%C3%BCchler2012">Büchler et al. 2012</a>]. For these
                     problems, the full set of nodes of the SCDAWG is used. In <a href="#section05"> Section 5</a> we sketch the afore-mentioned
                     filtering of nodes/infixes based on structural regularities (in <a href="#gerdjikov2016">Gerdjikov and Schulz [2016]</a> the filtering method is described in full detail).
                     Using the example corpora we illustrate how concepts and their relationships can
                     be mined in a language independent way. In <a href="#section06"> Section
                        6</a> we look at extensions of the SCDAWG index, adding metadata
                     information (e.g., on authors, temporal periods) from the texts. Examples from
                     our corpora show how to detect text reuses, e.g., between distinct authors and
                     to reveal the temporal flow of phrases across texts/authors. Before we come to a
                     short conclusion, <a href="#section07">Section 7</a> provides loose ends
                     for future research, indicating how SCDAWGs might help to treat diachronic
                     language variation and various text classification tasks. </div>
                  </div>
               
               
               <div id="section02" class="div div0">
                  
                  <h1 class="head">SCDAWGs as a corpus index structure</h1>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">In the introduction we explained the general benefits that can be obtained for
                     corpus analysis when using an index structure. If we are only interested in the
                     distribution and occurrences of single words, a simple index structure is
                     sufficient that represents the corpus as a “bag of words”. Usually an
                     “inverted file” then stores for each word \(w\) occurring in the corpus the texts and
                     positions where \(w\) occurs.
                     However, corpus analysis in Digital Humanities and other fields is often
                     focussed on other pieces of text: re-used portions of text, names,
                     terminological and other multi-word expressions, phrases that express semantic
                     relationships, syllables, morphems, to name a few. In this situation advanced
                     index structures are preferable that give direct access to the occurrences of
                     arbitrary infixes<a class="noteRef" href="#d4e302">[2]</a>. This
                     explains why corpus analysis tools in Digital Humanities or Computational
                     Biology are often based on the latter, more powerful type of index structures.
                     Among the latter type of index structure, directed acyclic word graphs (DAWGs)
                     and suffix trees have often been used for research, but refinements have not
                     received proper attention. In this paper we describe compacted directed acyclic
                     word graphs (CDAWGs) and a symmetric variant (SCDAWGs) and argue that these
                     index structures are preferable for many corpus analysis tasks.</div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">The common ideas behind DAWGS, suffix trees, CDAWGs and SCDAWGs are summarized in
                     the following way. 
                     <ul class="list">
                        <li class="item">Each index represents a graph, nodes representing specific infixes of
                           the corpus. </li>
                        <li class="item">Each infix of the corpus is represented at most once in the index. </li>
                        <li class="item">The total size of the index structure is linear in the size of the
                           corpus. </li>
                        <li class="item">Given any string \(v\)
                           we may use the index to decide in time \(O(\vert v\vert)\) if \(v\) is an infix of the
                           corpus.<a class="noteRef" href="#d4e339">[3]</a> It is also possible
                           to use the index “as a guide” for finding all texts and positions
                           where \(v\) occurs.
                           </li>
                     </ul> The nodes of a DAWG represent the infixes \(v\) that are “left-closed” in the sense that
                     there does <em class="emph">not</em> exist a unique symbol \(\sigma\) directly in front of each occurrence of \(v\) in the corpus. The nodes of a
                     suffix tree represent the infixes \(v\) that are “right-closed” in the sense that there does
                     <em class="emph">not</em> exist a unique symbol \(\sigma\) directly after each occurrence of \(v\) in the corpus. The CDAWG and
                     the SCDAWG for a corpus have the same set of nodes representing the infixes
                     \(v\) that are “left- and
                     right-closed” at the same time. Edges of a suffix tree, DAWG, or CDAWG
                     represent extensions of infixes in reading order (i.e., to the right). SCDAWGs
                     have two type of edges, respectively representing right extensions in standard
                     reading order and left-extensions in left-to-right order. <span class="label bold"> Example
                        2.1</span> As an example, consider the corpus with the two toy “texts”:
                     
                     <ul class="list">
                        <li class="item"><span class="monospace">op 1 in A</span></li>
                        <li class="item">
                           <span class="monospace">op 2 in B</span></li>
                     </ul>
                     
                     <div id="figure01" class="figure">
                        
                        <a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>DAWG, suffix tree, and symmetric compact DAWG (SCDAWG) for the corpus
                           with two toy texts <span class="monospace">op 1 in A</span> and <span class="monospace">op 2 in B</span>.</div>
                     </div> Infixes <span class="monospace">n</span> and <span class="monospace">in</span> are <em class="emph">not</em> left-closed since we find the symbol
                     <span class="monospace">i</span> directly in front of each occurrence
                     of <span class="monospace">n</span> and the blank directly in front of each
                     occurence of <span class="monospace">in</span> in the corpus. In contrast,
                     infixes <span class="monospace">in</span> , <span class="monospace">i</span> and <span class="monospace">op 1</span> are left-closed. Each
                     prefix of each text is left-closed, the corpus contains \(20\) left-closed infixes, its DAWG has \(20\) nodes (<a href="#figure01">cf. Figure 1</a>). Infixes <span class="monospace">o</span> and <span class="monospace">op</span> are <em class="emph">not</em> right-closed since we
                     find the symbol <span class="monospace">p</span> directly after each
                     occurrence of <span class="monospace">o</span> and the blank directly after
                     each occurence of <span class="monospace">op</span> in the corpus. In
                     contrast, infixes <span class="monospace">op</span> and <span class="monospace">p</span>, <span class="monospace">1 in A</span> are
                     right-closed. Each suffix of each text is right-closed, the corpus contains
                     \(25\) right-closed infixes,
                     accordingly its suffix tree has \(25\) nodes (<a href="#figure01">cf. Figure 1</a>). The
                     only infixes that are left- and right-closed are the empty string, the blank,
                     the strings <span class="monospace">op</span> and <span class="monospace">in
                        </span> and the two texts. Hence the (S)CDAWG only has \(6\) nodes (<a href="#figure01">cf. Figure 1</a>). Lines/transitions in the figures represent extensions
                     of nodes/infixes. In the DAWG in <a href="#figure01">Figure 1</a>, node
                     <span class="monospace">in</span> has two extensions with \(A\) and \(B\), respectively leading to the left-closed
                     infixes <span class="monospace">op 1 in A</span> and <span class="monospace">op 2 in B</span>. In the suffix tree, node <span class="monospace">op
                        </span> has two extensions with symbols \(1\) and \(2\), respectively leading to the right-closed infixes <span class="monospace">op 1 in A</span> and <span class="monospace">op 2 in
                        B</span>. In the SCDAWG in <a href="#figure01">Figure 1</a>,
                     black (blue) links represent right (left) extensions. Labels of left extensions
                     are written in right-to-left order. </div>
                  
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7"> As we mentioned above, each index can be used to check if a string \(v\) represents an infix of the
                     corpus in time linear in \(\vert
                     v\vert\). We sketch the procedure for the SCDAWG in <a href="#figure01"> Figure 1</a>. Assume we want to check if <span class="monospace">p 1</span> is an infix. Starting from the top node
                     (empty string) we look for an extension to the right where the label starts with
                     the letter <span class="monospace">p</span>. We find an extension with label
                     <span class="monospace">p</span> leading to <span class="monospace">op</span>. We continue to look for a right extension of the new node
                     starting with <span class="monospace">1</span>. We find the extension with
                     label <span class="monospace">1 in A</span> leading to <span class="monospace">op 1 in A</span>. Hence <span class="monospace">p 1</span> is an
                     infix.</div>
                  
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">Advantages of the SCDAWG index</h2>
                     
                     <div class="counter"><a href="#p8">8</a></div>
                     <div class="ptext" id="p8">The strength of SCDAWGs compared to DAWGs and suffix trees relies on three
                        features. 
                        <ol class="list">
                           <li class="item">
                              <span class="label bold">Number of nodes.</span> In general, the number of nodes in
                              the (S)CDAWG of a corpus is much smaller than the number of nodes in
                              a suffix tree or DAWG. This is seen in <a href="#figure01">Figure 1</a>. A more informative picture is found in <a href="#table01">Table 1</a> where we compare the size of
                              these index structures for a small collection of corpora of distinct
                              sizes. For the sake of completeness, the table also includes data
                              for suffix tries, a simplified version of suffix trees where each
                              suffix represents a node.</li>
                           <li class="item">
                              <span class="label bold">Naturalness of nodes.</span> Most of the infixes
                              representing the nodes of a (S)CDAWG are very natural from an
                              intuitive point of view<a class="noteRef" href="#d4e568">[4]</a>: imagine we index the German version of Goethe's
                              Faust. Among the nodes of the suffix tree, we will find right-closed
                              infixes such as <span class="monospace">ephisto</span>, <span class="monospace">phisto</span>, <span class="monospace">retchen</span>, and <span class="monospace">etchen</span>. In the DAWG we find left-closed infixes such
                              as <span class="monospace">Mephist</span>, <span class="monospace">Mephis</span>, <span class="monospace"> Gretche</span>
                              and <span class="monospace"> Gretch</span>. In the (S)CDAWG we
                              only have the two-sided closures <span class="monospace">Mephisto</span> and <span class="monospace">Gretchen</span>. The example indicates that two-sided closure
                              is a very natural property when looking for “interesting” and
                              “meaningful” infixes of a corpus.</li>
                           <li class="item">
                              <span class="label bold">Node Extension Property.</span> Among the infixes that
                              represent the nodes of the index (suffix tree, DAWG, or (S)CDAWG) in
                              general there are many inclusion relations where one infix \(v\) is a substring of
                              another one \(w\).
                              In a suffix tree, DAWG, or CDAWG extension steps departing from node
                              \(v\) in many
                              cases do not lead to node \(w\). For example, in the suffix tree in <a href="#figure01">Figure 1</a> we cannot reach <span class="monospace">op 1 in A</span> from <span class="monospace">in</span> following the transitions. In
                              contrast, the two-directional edge structure of SCDAWGs completely
                              covers infix containment in the sense that <em class="emph">whenever node
                                 \(v\) is a
                                 substring of \(w\) there exists a chain of left and right
                                 extensions leading from \(v\) to \(w\)</em>. In other words, starting from a
                              node \(v\) and
                              traversing the index we find each context containing \(v\). For the sake of
                              reference, this property will be called the “Node Extension
                              Property”. It can be used, for example, to find in a very
                              elegant way all texts where an infix \(v\) occurs: starting from the node
                              that represents (the two-sided closure of) \(v\), compute all maximal chains of
                              left extensions followed by all maximal chains of right extensions.
                              The nodes reached at the end represent the texts in which \(v\) occurs. For
                              example, in <a href="#figure01">Figure 1</a> starting from
                              <span class="monospace">op</span> or <span class="monospace">in</span> we reach the nodes representing
                              the two input texts.</li>
                        </ol>
                        
                        <div id="table01" class="table">
                           <table class="table">
                              <tr class="row label">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Suffix Trie</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">Suffix Tree</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">DAWG</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">CDAWG</td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1">SCDAWG</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="5" rowspan="1"><span class="monospace">abcbc\(abcab\)</span>: 12 Bytes; 12
                                    symbols</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Nodes </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 66 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 17 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 16 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 6 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 6 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Edges </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 65 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 16 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 23 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 13 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 21 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="5" rowspan="1"><span class="monospace">OCR page</span>: 7 KB; 6.945
                                    symbols</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Nodes </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 21.418.626 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 7.355 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 11.995 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.163 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.163 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Edges </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 21.418.625 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 7.354 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 14.915 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 4.083 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 8.094 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="5" rowspan="1"><span class="monospace">Excerpt EU-Corpus</span>: 106 KB;
                                    ca. 106.000 symbols</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Nodes </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> - </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 161.001 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 165.962 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 25.273 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 25.273 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Edges </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> - </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 161.000 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 227.515 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 86.826 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 170.358 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"></td>
                                 
                                 <td valign="top" class="cell" colspan="5" rowspan="1"><span class="monospace">Small Corpus</span>: 1.2 MB; ca.
                                    1.250.000 symbols</td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Nodes </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> - </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.921.704 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.922.811 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 366.070 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 366.070 </td>
                                 </tr>
                              <tr class="row">
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> Edges </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> - </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.921.703 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 2.730.597 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 1.173.856 </td>
                                 
                                 <td valign="top" class="cell" colspan="1" rowspan="1"> 2.355.669 </td>
                                 </tr>
                           </table>
                           <div class="caption">
                              <div class="label">Table 1. </div>Comparing the size of distinct index structures for four input
                              texts.</div>
                        </div>
                        <span class="label bold">Node-documents function.</span> Using the Node Extension Property in
                        one scan of the SCDAWG we may compute and store a function \(T\) that assigns to each node
                        \(\mu\) (represented as
                        a number) the set of texts where \(\mu\) occurs as an infix [<a class="ref" href="#blumer1987">Blumer et al. 1987</a>].
                        \(T\) is called the
                        <em class="emph">node-documents function</em>. Let us assume that each text comes
                        with a number. We define a procedure FindTexts, the input is a node number
                        \(\mu\), the output is a
                        set of text numbers. The procedure is called in the form FindTexts(root). As
                        a result, the function \(T\)
                        is computed. 
                        
                        
                        <blockquote class="eg">
                           <pre><code class="code-general">
Procedure FindTexts(\(\mu\)) 
if \(T(\mu)\) is already defined, then
    output \(T(\mu)\);
else if \(\mu\) is a leaf (text), then
    let \(n\) be the number of the text;
    define \(T(\mu) := \{n\}\);
    output \(\{n\}\);
otherwise
    let \(\nu_1,\ldots,\nu_k\) be the (left or right) immediate successors of \(\mu\).
    call FindTexts(\(\nu_1\)), ...., FindTexts(\(\nu_k\));
    define \(T(\mu) :=\bigcup_{i=1,\ldots, k} T(\nu_i);\)
    output \(T(\mu)\);
                        </code></pre>
                        </blockquote> 
                        
                        The procedure first visits all successor nodes of the input node
                        \(\mu\) and then assigns
                        the union of the sets of text numbers of the direct successors to \(\mu\). It is simple to see that
                        each link of the index is used only once. Hence, if the number of all
                        distinct texts is treated as a constant, the procedure is linear in the size
                        of the index. 
                        <div id="figure02" class="figure">
                           
                           <a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 2. </div>A text \(A\)
                              leading to a value \(r=130,000\) for the LIS problem if compared with a
                              similar parallel text \(B\).</div>
                        </div>
                        </div>
                     
                     </div>
                  </div>
               
               <div id="section03" class="div div0">
                  
                  <h1 class="head">Corpora used for experiments</h1>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">For the experiments we selected corpora of different periods, languages, and
                     genres. Languages covered are Latin, English, German, French and Italian.
                     Domains vary from political texts to poetry, lyrics and literature. Metadata
                     include author, temporal data, and others.</div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10"><span class="label bold">Poems Corpus.</span> The Poems Corpus consists of roughly 40k poems which
                     had been compiled from TEI – XML annotated documents from the TextGrid
                     Repository1. It contains poems written by German poets from about the time when
                     printing was invented to the beginning of the 20th century. Amongst these
                     authors are well known names like Johann Wolfgang v. Goethe, Friedrich Schiller,
                     Joseph v. Eichendorff, or Georg Trakl. All poems are written in German language.
                     In addition to the author each poem is furnished with the volume in which the
                     poem was published as well as dates referring to the timespan in which the
                     volume had been created. If this is not known the lifespan of the author is used
                     for the dates.</div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11"><span class="label bold">Lyrics Corpus.</span> The Lyrics Corpus contains about 15k of popular
                     lyrics from songs, which had been present in the single charts of the
                     German-speaking world, reaching from the mid nineteen fifties to nowadays. This
                     corpus had been created by means of webcrawling. Aside from the interpreter it
                     stores the year, in which the song had been in the charts, the chart position
                     and whether the chart position had been a top 10 position, as well as the
                     language, in which the song was performed, as its metadata. The language had
                     been assigned automatically via n-gram based language classification. Since the
                     manner of appearance of the considered single charts has changed over time,
                     songs of the 80s and 90s are prevailing.</div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12"><span class="label bold">Parallel OCR Corpus.</span> A smaller corpus of historic books, which
                     also had been digitized by means of OCR. All documents in this collection had
                     been created by two different OCR engines, leading to a parallel corpus of the
                     same content. The corpus was only used in the alignments, and there no metadata
                     had been considered.</div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13"><span class="label bold">Wittgenstein Corpus.</span> The Wittgenstein Corpus consists of 3,871
                     original remarks of the German philosopher Ludwig Wittgenstein. Although the
                     general language is German, the corpus also contains a few remarks in
                     English.</div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14"><span class="label bold">Medline Corpus.</span> The Medline Corpus is a collection of about 15
                     Million medical abstracts in English. Each abstract consists of about 10
                     sentences. We considered two subsets of this corpus. They were obtained by
                     randomly selecting 0.5% and 10%, respectively, of the abstracts and gathering
                     all the sentences in the resulting abstracts. The 0.5%-Medline Corpus consists
                     of 800 K sentences, whereas the 10%-Medline Corpus consists of about 15 M
                     sentences.</div>
                  </div>
               
               
               <div id="section04" class="div div0">
                  
                  <h1 class="head">Using the index for alignment and text reuse</h1>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15"> In this section we use the node-documents function defined in <a href="#section02">Section 2</a> to find large strings that co-occur in
                     several texts of the collection. Such strings point to some form of text
                     parallelism/reuse. Two applications are considered. In the first subsection we
                     show how to efficiently align two or more parallel texts in a linear manner. In
                     the second subsection we show how to explore large collections, looking at
                     maximal strings that co-occur in several texts of the corpus</div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Index-based text alignment</h2>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">In our context, to “align” two strings means to find a common linear
                        representation where “common parts” of the two strings are shown,
                        defining a linear sceleton, and where “deviations” or regions where the
                        two strings do not agree are associated with the “holes” of the
                        sceleton. The classical method for this task is the Needleman-Wunsch
                        Algorithm [<a class="ref" href="#needleman1970">Needleman and Wunsch 1970</a>]: The input are two texts \(A\) and \(B\) of length \(m=|A|\), \(n=|B|\). The algorithm finds an optimal alignment, given a
                        scoring function. Using dynamic programming it fills the cells of a \(|A|\times |B|\) matrix, cells
                        come with “backward pointers”. The backward pointers are then used to
                        find an optimal alignment. The complexity is \(O(mn)\), which is problematic for large texts.
                        E.g., comparing two texts with \(10^6\) symbols each, we need several terrabytes to store
                        the full matrix. For a better solution we look at a strategy to translate
                        the problem</div>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17"><span class="label bold">Finding longest common subsequences and longest increasing
                           subsequences.</span> Such a translation can be found in the longest
                        common subsequence or the longest increasing subsequence problem: Assume the
                        scoring function for an alignment assigns value 1 to each pair of matching
                        symbols and value 0 to each disagreement (here: insertion, deletion, or
                        substitution). Then the matching symbols of the alignment are given by the
                        longest common subsequence (LCS) of the two strings. A subsequence of a
                        string \(A= a_1\ldots a_m\)
                        is any string of the form \(A=
                        a_{i_1}\ldots a_{j_k}\) where \(k \leq m\) and \(i_1 &lt; \ldots &lt; i_k\). For example,
                        “bdf” is a subsequence of “abcdef”. </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">This means that for a particular and natural scoring function, finding the
                        LCS of two strings \(A\) and
                        \(B\) helps to solve the
                        global alignment problem for \(A\) and \(B\). To avoid the above matrix computation with its large
                        complexity we may use an alternative approach to solve the LCS, utilizing
                        the longest increasing subsequence (LIS) problem. Given a sequence of
                        natural numbers, we look for the longest increasing subsequence. For
                        example, the LIS for \(1-5-2-7-3-4\) is \(1-2-3-4\). Given \(A=
                        a_1\ldots a_m\) and \(B=
                        b_1\ldots b_n\), the LCS problem for \(A\) and \(B\) can be reduced to the LIS
                        problem [<a class="ref" href="#gusfield1997">Gusfield 1997</a>]. We ignore the details of the
                        translation, but look at the complexity of the new procedure for solving the
                        LCS. Let \(r(i)\) denote the
                        number of occurrences of the letter \(a_i\) in \(B\), let \(r =
                        \sum^m_{i=1} r(i)\). Using translation into LIS, the LCS
                        problem for \(A\) and
                        \(B\) can be solved
                        without dynamic programming in time \(O(r\ log(n))\). </div>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">In many cases, using the LIS method helps to reduce the computation time for
                        an alignment tasks. However, for large texts the value for the number
                        \(r\) is typically very
                        large and the computation time remains enormous. For example, the
                        “innocent” text \(A\) shown in <a href="#figure02">Figure 2</a>, which
                        is taken from a historical document, already leads to a value of \(r \sim 130,000\) when compared
                        with a similar text \(B\).
                        For more details on the LCS- or LIS- problem see [<a class="ref" href="#gusfield1997">Gusfield 1997</a>]. <span class="label bold">Improved index-based alignment.</span> Consider the SCDAWG of
                        two texts \(A\) and \(B\). Applying the
                        Node-documents function \(T\) it can be determined whether the infix represented by
                        each node occurs in both texts. The main observation now is that using this
                        information we immediately find large portions of the two texts that have a
                        perfect alignment. We use the following definition.</div>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20"><span class="label bold">Definition 4.1</span><a id="def_quasi_max"><!--This comment is a hack.--></a> An index node \(\mu\) of the SCDAWG for two
                        texts \(A\) and \(B\) is called <em class="emph">quasi
                           maximal</em> if 
                        <ol class="list">
                           <li class="item">the infix associated with \(\mu\) occurs in both texts \(A\) and \(B\),</li>
                           <li class="item">\(\mu\) does not
                              have any transition that leads to another node \(\mu\)’ with the
                              Property 1.</li>
                        </ol> As an illustration we refer to <a href="#figure01">Figure
                           1</a>. Our two example “texts” lead to two quasi maximal nodes
                        <span class="monospace">op </span> and <span class="monospace"> in
                           </span>. Based on the <em class="emph">Node-documents function</em> there
                        exists a simple procedure to find all quasi-maximal nodes: a queue is
                        initialized with the root of the SCDAWG (empty string). We treat all entries
                        of the queue in the order they are added to the queue. The treatment of a
                        node \(\mu\) involves two
                        steps: 
                        <ul class="list">
                           <li class="item"> We consider each node reached from \(\mu\) with a single left or right
                              transition in the SCDAWG. Each such node representing an infix
                              occurring both in \(A\) and \(B\) is added to the end of the queue. </li>
                           <li class="item"> If \(\mu\) does
                              not have any extension representing an infix occurring both in
                              \(A\) and
                              \(B\), then
                              \(\mu\) is added
                              to the list of quasi maximal nodes. </li>
                        </ul> Note that each index node is added to the queue and treated at most
                        once. At the end we obtain the full list of all quasi maximal nodes. Simple
                        additional calculations provide the positions of the occurrences of each
                        quasi maximal nodes. Technical details are omitted. </div>
                     
                     
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">The full alignment method proceeds as follows: We compute the SCDAWG index
                        for the texts \(A\) and
                        \(B\) (linear time). We
                        traverse the index to find all quasi maximal nodes and the end positions of
                        occurrences in \(A\) and
                        \(B\). We use end
                        positions and the LIS method to define the linear sceleton of the alignment.
                        When using the LIS method, the important new point to note is that the
                        strings corresponding to the quasi maximal nodes are treated as single
                        “symbols”. In this way, the factor \(r\) mentioned in the above formula is
                        drastically reduced. In the experiment from <a href="#figure02"> Figure
                           2</a>, the usual procedure leads to \(r \sim 130.000\), using the SCDAWG and quasi
                        maximal nodes the value is \(r=29\). When aligning two OCR output texts of \(5.500\) symbols, the standard
                        method yields \(r \sim
                        2.300.000\), using the SCDAWG and quasi maximal nodes the
                        value is only \(r=169\).</div>
                     
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">In a project related to improving OCR on historical documents we use this
                        technique to align the outputs of distinct OCR engines for historical texts.
                        “Holes” obtained from the alignment based on quasi maximal nodes
                        sometimes cover portions of texts where a finer subanalysis helps to improve
                        results. <a href="#figure03">Figure 3</a> shows an alignment result
                        for two OCR outputs and two texts, with distinct levels of disagreement.</div>
                     
                     <div id="figure03" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" style="" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Two OCR outputs for two pages aligned using index technology. Black
                           parts represent quasi maximal nodes and thus matches, red parts are
                           disagreement regions.</div>
                     </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">
                        <span class="label bold">Multiple string alignment.</span> When dealing with \(n&gt;2\) texts \(A_1,\ldots, A_n\), the notion
                        of a quasi maximal node is generalized. <span class="label bold"> Definition 4.2 </span> An
                        index node \(\mu\) of the
                        SCDAWG for \(A_1,\ldots,
                        A_n\) is called <em class="emph"> quasi maximal</em> if 
                        <ol class="list">
                           <li class="item">the infix associated with \(\mu\) occurs in all texts \(A_1,\ldots, A_n\),</li>
                           <li class="item">
                              \(\mu\) does not
                              have any transition that leads to another node \(\mu\)’ with Property 1.
                              </li>
                        </ol> For technical reasons we actually have the additional restriction
                        that quasi maximal nodes used for alignment only occur once in each text. A
                        generalization of the LIS algorithm can be used to find a maximal linear
                        sequence of quasi maximal nodes of all texts, which gives the desired
                        alignment sceleton. As an illustration, <a href="#figure04">Figure
                           4</a> shows the simultaneous alignment of three texts. Two sequences
                        represent parallel parts of the OCR-outputs of two OCR engines on a
                        historical document, the third sequence shows the corresponding part of the
                        ground truth file. 
                        <div id="figure04" class="figure">
                           
                           <a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 4. </div>Subsegment of the multi-alignment of
                              three texts (OCR outputs and ground truth) using quasi-maximal
                              nodes. Grey parts represent quasi-maximal nodes, for the coloured
                              regions at least two texts show a disagreement.</div>
                        </div>
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Detecting text reuse and similarities across collections}</h2>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">When comparing large collections of texts \(A_1,\ldots A_n\), in most cases it is not
                        natural to ask for substrings that co-occur in all texts. For the following
                        experiment a node of the SCDAWG index for \(A_1,\ldots A_n\) is called quasi maximal if it
                        is quasi maximal for two texts in sense of <a href="#def_quasi_max">Definition 4.1</a>. We have seen that we can extract the set of all
                        quasi maximal nodes from the SCDAWG index in this sense, and for each such
                        node \(\mu\) with string
                        \(v_{\mu}\) we
                        immediately get the information in which texts \(A_i\) the string \(v_{\mu}\) occurs. In <a href="#figure05">Figure 5</a> this information is used to compute “survey” graphs
                        for text reuses in two complete collections (Poems Corpus and Lyrics
                        Corpus). Each graph contains two types of nodes. Large and small ellipses
                        respectively represent quasi maximal nodes and texts numbers of the
                        collection. For each node \(\mu\) a prefix and a suffix of the node text \(v_{\mu}\) is shown. A link from
                        a quasi maximal node \(\mu\)
                        to a text number \(k\)
                        indicates that \(v_{\mu}\)
                        is a substring of \(A_k\).
                        
                        <div id="figure05" class="figure">
                           
                           <a href="./resources/images/figure05_1.png" rel="external"><img src="./resources/images/figure05_1.png" style="" alt="" /></a>
                           <a href="./resources/images/figure05_2.png" rel="external"><img src="./resources/images/figure05_2.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 5. </div>Text reuses in the Poems Corpus (top) and Lyrics Corpus (bottom) -
                              quasi maximal nodes and pointers to poems/lyrics. This bird's eye
                              perspective can be used to find interesting regions for closer
                              inspection using zooming techniques. To view these images in more detail, download
                              the high-resolution PDFs (<a href="resources/images/figure05_1.pdf" onclick="window.open('resources/images/figure05_1.pdf'); return false" class="ref">5.1</a>, <a href="resources/images/figure05_2.pdf" onclick="window.open('resources/images/figure05_2.pdf'); return false" class="ref">5.2</a>).</div>
                        </div>
                        </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">Depending on the text reuses in the collection, the graph becomes very large.
                        Still it is very useful to find interesting subregions with eye-catching
                        multiple text reuses. In the lyrics corpus, many reuse effects can be traced
                        back to covered songs. Since the song texts in the corpus were collected in
                        a community based way, texts differ in details. In the survey graph
                        eye-catching regions are mainly caused by songs that have been covered many
                        times. It is then possible to zoom to these regions and to see reused text
                        portions and the texts where these findings occur. Figures <a href="#figure06">6</a>and <a href="#figure07">7</a> show such
                        zoomed regions. In <a href="#figure06">Figure 6</a> it can be seen
                        that five poems in the collection have many text reuse connections. Some
                        examples for reused text are (cf. <a href="#figure06">Figure 6</a>): 
                        <ul class="list">
                           <li class="item">
                              <span class="monospace">laut und leise, Unterric... träumt,
                                 Angenehme zu hören</span>
                              </li>
                           <li class="item"><span class="monospace">wälzend kam die Sturmesf ... der Chor vom
                                 ganzen Haine</span>
                              </li>
                        </ul>
                        
                        <div id="figure06" class="figure">
                           
                           <a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 6. </div>Text reuses in the Poems Corpus - quasi maximal nodes and pointers
                              to 5 poems, zoomed subregion of upper graph in <a href="#figure05">Figure 5</a>.</div>
                        </div> In <a href="#figure07">Figure 7</a> many test reuses are
                        centered around the word “Hallelujah”. Examples are 
                        <ul class="list">
                           <li class="item"><span class="monospace">She tied you to a kitche .. Hallelujah,
                                 Hallelojah}</span>
                              
                              </li>
                           <li class="item"><span class="monospace">s seen the light It's a -- ah Hallelujah,
                                 Hallelujah}</span>
                              
                              </li>
                        </ul>
                        
                        <div id="figure07" class="figure">
                           
                           <a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 7. </div>Text reuses in the Lyrics Corpus - quasi maximal nodes and
                              pointers to lyrics, zoomed subregion of lower graph in <a href="#figure05">Figure 5</a>.</div>
                        </div>
                        </div>
                     </div>
                  </div>
               
               
               <div id="section05" class="div div0">
                  
                  <h1 class="head">Refined “linguistic” view on a corpus </h1>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26"> We have seen that the set of nodes of the (S)CDAWG for a corpus represents a
                     relatively small set of infixes that often are “natural” portions of text.
                     Experiments in <a href="#gerdjikov2016">Gerdjikov and Schulz (2016)</a> show that many of the phrases
                     corpus analyzers would look at (names, terminological expressions, stylistic
                     phrases, ...) are typically left- and right-closed and appear among the nodes of
                     the (S)CDAWG. This means that when just using the index nodes for corpus
                     exploration we do not miss important portions of text. </div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">From this point, the (S)CDAWG can be considered as a first step towards a new
                     kind of “linguistic corpus index” where nodes - in the ideal case - <em class="emph">
                        exactly</em> represent the linguistic units of interest (morphems, words,
                     phrases, sentences, ...) of the corpus and their relationships. In a way, such
                     an index would be the optimal basis for all corpus analysis tasks focussed on
                     linguistic units <a class="noteRef" href="#d4e1550">[5]</a>. However, for
                     arbitrary texts there is not even a useful formal definition of a “phrase”
                     or “linguistic unit”, let alone a procedure for finding this strings. </div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">There are two possible main paths how to come closer to a “linguistic
                     refinement” of the (S)CDAWG. On the one hand side, we can take any
                     available NLP prodecure (parser, lexical analyser, phrase detector,..). Assuming
                     that the units found represent nodes of the (S)CDAWG, we arrive at a (S)CDAWG
                     substructure that directly points to units of interest and their occurrences in
                     the corpus. On the other hand we may try to further analyze regularities found
                     in the corpus to find a restricted set of nodes that comes closer to the idea of
                     a linguistic phrase. In <a href="#gerdjikov2016">Gerdjikov and Schulz (2016)</a>, we followed the second
                     path, which is completely language independent. Crucial assumptions are: 
                     <ul class="list">
                        <li class="item"> phrases appear in distinct contexts, </li>
                        <li class="item"> phrases are combined using function words as connectors, and </li>
                        <li class="item"> sentences have natural decompositions into phrases, overlaps
                           representing function words. </li>
                     </ul> We then describe a bootstrapping method for finding function words,
                     phrases and sentence decompositions into phrases.<a class="noteRef" href="#d4e1573">[6]</a> Using
                     the same kind of techniques, phrases can be further decomposed into 
                     <em class="term">sub(sub)phrases</em>. As a matter of fact, for this form of decomposition also given
                     lists of function words could be used. The main point is that the bootstrapping
                     method in a completely unsupervised and language independent way leads to an
                     interesting set of phrases and subphrases that helps to considerably reduce the
                     set of nodes considered, thus coming closer to a “linguistic” index. </div>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29"><span class="label bold">Finding important concepts of the corpus.</span> When ignoring bordering
                     function words, <em class="emph">atomic subphrases</em> obtained in this way typically
                     represent content words or “concepts”. Analyzing the role of these concepts
                     in phrase decomposition it is possible to compute a ranked list of
                     characteristic concepts of the corpus. See <a href="#gerdjikov2016">Gerdjikov and Schulz (2016)</a> for
                     details. Below some examples for <em class="emph">most characteristic atomic phrases</em>
                     in the example corpora are given. For example, in the Wittgenstein Corpus,
                     central concepts found using the above methods are (cf. <a href="#figure09">Figure 9</a>) <span class="monospace">Bedeutung</span> (meaning),
                     <span class="monospace">Farbe</span>(colour), <span class="monospace">Sprache</span> (language), and <span class="monospace">Erklärung</span> (explanation). </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">
                     <span class="label bold">Exploring the use of concepts.</span> After computing characteristic
                     concepts, the aforementioned decomposition of phrases helps to find all larger
                     phrases where a given concept occurs. In this way the use of the concept in the
                     corpus can be studied. <a href="#figure08">Figure 8</a> shows the
                     hierarchical structure of phrases of the Wittgenstein Corpus extending <span class="monospace">Bedeutung</span>. 
                     <div id="figure08" class="figure">
                        
                        <a href="./resources/images/figure08.png" rel="external"><img src="./resources/images/figure08.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 8. </div><span class="hi bold">Exploring the use of concepts</span>. The hierarchical
                           structure of phrases extending the concept <span class="monospace">Bedeutung</span> in the Wittgenstein Corpus detected by our
                           approach.</div>
                     </div>
                     </div>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">
                     <span class="label bold">Exploring the relationship of concepts in a visual way.</span> A third
                     goal is to explore interesting relationships between the concepts. Existing
                     corpus exploration tools and automated approaches for exploring the
                     paradigmatics of lexical units typically look at the co-occurrence of terms in
                     documents, paragraphs, sentences or fixed size neighbourhoods [<a class="ref" href="#sch%C3%BCtze1993">Schütze and Pedersen 1993</a>],[<a class="ref" href="#storjohann2010">Storjohann 2010</a>]. Another, more
                     “syntactic” view is obtained when looking at the co-occurrence of
                     concepts in <em class="emph">phrases</em>. In Figures <span class="error"><a href="#figure9">9</a></span> and
                     <a href="#figure10">10</a> we see the relationship of concepts
                     (characteristic atomic kernels) in terms of co-occurrences in phrases. For
                     example, in the Wittgenstein Corpus, <span class="monospace">Bedeutung</span>
                     (meaning) is strongly related to <span class="monospace">Erklärung</span>
                     (explanation), as can be seen from phrases like <span class="monospace">Erklärung der Bedeutung eines Worts</span> (explanation of the
                     meaning of a word). In the Medline Corpus (cf. <a href="#figure10">Figure
                        10</a>), concepts found to be related to <span class="monospace">tumor</span> are, e.g., <span class="monospace">risk</span>
                     (witness phrase <span class="monospace">risk for tumor</span>), <span class="monospace">chemotherapy</span> (witness phrase <span class="monospace">tumor cells to chemotherapy</span>), and <span class="monospace">vaccines</span> (witness phrase <span class="monospace">tumor vaccines</span>). 
                     <div id="figure09" class="figure">
                        
                        <a href="./resources/images/figure09.png" rel="external"><img src="./resources/images/figure09.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 9. </div><span class="hi bold">Exploring the relationship of concepts in a visual way</span>.
                           Network with “witness phrases” for the relationship between
                           concepts derived from Wittgenstein Corpus.</div>
                     </div>
                     
                     <div id="figure10" class="figure">
                        
                        <a href="./resources/images/figure10.png" rel="external"><img src="./resources/images/figure10.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 10. </div><span class="hi bold">Exploring the relationship of concepts in a visual way</span>.
                           Network with “witness phrases” for the relationship between
                           concepts derived from the Medline.</div>
                     </div>
                     </div>
                  </div>
               
               <div id="section06" class="div div0">
                  
                  <h1 class="head">Corpus exploration using metadata</h1>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32"> When metadata are available, many of the aforementioned techniques can be
                     refined for revealing similarities in the space of metadata. Technically, we
                     assign the metadata for the original documents to the nodes in the SCDAWG that
                     represent those documents. For the rest of the nodes we can retrieve this
                     information on demand by simple traversal of small parts of the SCDAWG. </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">
                     <span class="label bold">Comparing authors.</span> Our first illustration uses the Poems Corpus,
                     where we have authorship and temporal metadata for texts. In <a href="#figure11">Figure 11</a> we use the aforementioned refined
                     “linguistic” view. After adding authorship information to the maximal
                     nodes of the index we first adapt the notion of a quasi maximal node: a node
                     \(\mu\) with text \(v_{\mu}\) is called <em class="term">quasi
                        maximal with respect to authors</em> if \(v_{\mu}\) occurs in texts of two distinct authors, while any
                     phrase that properly extends \(v_{\mu}\) only occurs in the works of one author. </div>
                  
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34"> The nodes in <a href="#figure11">Figure 11</a> represent important
                     concepts in the corpus - some of the most characteristic atomic subphrases for
                     the Poems Corpus in the sense considered above. Our language independent
                     techniques revealed that <span class="monospace">Welt</span> (world), <span class="monospace">Sonne</span> (sun), <span class="monospace">Himmel</span> (sky/heaven), <span class="monospace">Gold</span>
                     (gold), and <span class="monospace">Schnee</span> (snow) are important
                     concepts in the Poems Corpus. In <a href="#figure11">Figure 11</a>, each
                     link between two concepts stands for a phrase that 
                     <ul class="list">
                        <li class="item"> contains both concepts, and </li>
                        <li class="item"> is quasi maximal with respect to authors. </li>
                     </ul> The authors that have used the phrase are annotated with the link. For
                     example it is seen that 
                     <ul class="list">
                        <li class="item">
                           <span class="monospace">Schnee zur Erde</span> (snow to earth) was
                           both used by Nikolaus Lenau and Hermann Löns, and </li>
                        <li class="item">
                           <span class="monospace">der Himmel mit der Erde</span> (the heaven/sky
                           with the earth) was both used by Betty Paoli and Friedrich
                           Rückert.</li>
                     </ul> This gives a basis for comparing two authors, now asking if both authors
                     used similar (quasi-maximal) phrases with the central concepts of the corpus.
                     
                     <div id="figure11" class="figure">
                        
                        <a href="./resources/images/figure11.png" rel="external"><img src="./resources/images/figure11.png" style="" alt="" /></a>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 11. </div><span class="hi bold">Use of Metadata</span>. Connecting concepts (nodes) by means
                           of phrases that are maximal with respect to the property of being used
                           by two poets. Edges represent phrases plus the two poets that used the
                           phrase.</div>
                     </div>
                     </div>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">
                     <span class="label bold">Temporal development of similarities between authors.</span> In <a href="#figure12">Figure 12</a> we see a dual variant of the graph where
                     the role of nodes and edges is changed. Nodes now represent authors, authors are
                     linked if they have used identical quasi maximal (w.r.t authors) phrases. Using
                     colouring of nodes, also temporal information about authors is added - we simply
                     used the year defining the middle point in the life of an author. The spectral
                     ordering of colours red-green-blue-purple represents the temporal timeline from
                     earlier periods to later periods. Links always point from earlier authors to
                     later authors. </div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36"> In the lower part of the graph we find a path from Anna Louisa Karsch (1756)
                     <a href="#figure12">Figure 12</a>, circle marker (1)) \(\longrightarrow\) Friedrich
                     Gottlieb Klopstock (1763) \(\longrightarrow\) Ernst Schulze (1803) <a href="#figure12">Figure 12</a>, circle marker (2)). The latter is also reached with a link
                     from Johann Wolfgang von Goethe (1794) <a href="#figure12">Figure 12</a>,
                     circle marker (3)). Goethe has several successors. In this graph, an even more
                     central person is Ludwig Achim von Arnim (1806) <a href="#figure12">Figure
                        12</a>, circle marker (4)), who builds the center of the large cluster in
                     the middle of the figure. The temporal spectrum of this cluster corresponds to
                     the first half of the 19th century. Authors of the 17th and 18th century are
                     found in the second cluster on the top right part. An early “influential”
                     author of this period is Simon Dach (1632) <a href="#figure12">Figure
                        12</a>, circle marker (5)). </div>
                  
                  <div class="counter"><a href="#p37">37</a></div>
                  <div class="ptext" id="p37"> The two clusters indicate a change of literary concepts covered between the
                     Baroque/Pre-Romantic period and the Romantic period. The three authors (John
                     Brinckman (1842), Fritz Reuter (1842), and Klaus Groth (1859) <a href="#figure12">Figure 12</a>, circle marker (6)), who form the cluster
                     right to Romantic period cluster however are not connected to the cluster
                     consisting of other authors of their period due to the fact that all three were
                     writing poems in the german dialect of niederdeutsch. 
                     <div id="figure12" class="figure">
                        
                        <a href="./resources/images/figure12.png" rel="external"><img src="./resources/images/figure12.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 12. </div><span class="hi bold">Use of Metadata in the Poems Corpus.</span> The graph is
                           obtained as the dual variant of the graph in <a href="#figure11">Figure 11</a>. Links between two poets mean that both have used
                           similar maximal phrases around the concepts shown in <a href="#figure11">Figure11</a>. Colours correspond to temporal
                           information for authors. Links point from earlier authors to later
                           authors.</div>
                     </div>
                     </div>
                  
                  <div class="counter"><a href="#p38">38</a></div>
                  <div class="ptext" id="p38"> When constructing this kind of graph for the Lyrics Corpus similar observations
                     can be made. The nodes of <a href="#figure13">Figure 13</a> again are
                     connected by co-occurring quasi maximal phrases. Instead of the author now band
                     names are used together with the mean of the release years of their
                     corresponding songs. The colour coding is the same as used in the previous
                     experiment. One of the first things that can be observed is a cluster of Italian
                     singers centered around Eros Ramazotti (1998) (See <a href="#figure13">Figure 13</a>, circle marker (1)), located at the right to the center of
                     the graph. Also lots of subgraphs, which only contain of two or three
                     interpreters are seen. They often vizualize relations between cover songs, which
                     are contained in the corpus. Therefore for instance Right Said Fred (1998) and
                     Peter Sarstedt (1969) (See <a href="#figure13">Figure 13</a>, circle
                     marker (2)) can be found at the bottom left. Another example of this is a three
                     node cluster at the top right with edges leading from Julie Covington (1977) and
                     Don McLean (1972) to Madonna (1996) (See <a href="#figure13">Figure
                        13</a>, circle marker (3)), who famously released covers of the songs “Don't
                     Cry For Me Argentina” and “American Pie”. Other subgraphs connect
                     interpreters who obviously belong to the same genre, e.g. at little to the right
                     of the center a path can be found leading from 2Pac (2000) \(\longrightarrow\) Busta Rhymes
                     (2001) \(\longrightarrow\) Kanye
                     West (2007) (See <a href="#figure13">Figure 13</a>, circle marker (4)),
                     with the interpreters representing famous hip-hop artists. 
                     <div id="figure13" class="figure">
                        
                        <a href="./resources/images/figure13.png" rel="external"><img src="./resources/images/figure13.png" style="" alt="" /></a>
                        
                        <div class="caption">
                           <div class="label">Figure 13. </div><span class="hi bold">Use of Metadata in the Lyrics Corpus.</span> The graph is
                           obtained by connecting interpreters by their commonly used phrases. Node
                           colours again encode temporal metadata. Links point from older
                           interpreters to younger ones.</div>
                     </div>
                     </div>
                  </div>
               
               
               <div id="section07" class="div div0">
                  
                  <h1 class="head">Loose ends - diachronic language variation and classification tasks</h1>
                  
                  <div class="counter"><a href="#p39">39</a></div>
                  <div class="ptext" id="p39">Even though the focus of the current paper are the problems of text-reuse and
                     mining of relationships based on it, we suggest that our approach can be
                     extended to other related problems, e.g. diachronic language variation and text
                     classification. </div>
                  
                  <div class="counter"><a href="#p40">40</a></div>
                  <div class="ptext" id="p40">In <a href="#gerdjikov2013">Gerdjikov et al. (2013)</a> and <a href="#sariev2014">Sariev et al. (2014)</a>, the authors
                     develop a historical text normalization system. Based on a modern dictionary and
                     several thousands of pairs, historical word and its modern variant, they show
                     that the DAWG structure can be used to automatically learn and extract spelling
                     variations. The main benefit of the infix structure is that: (i) it does not
                     restrict the spelling variation to any particular predefined patterns and (ii)
                     it reflects the entire structure of the language (on word level). In <a href="#sariev2014">Sariev et al. (2014)</a> the combination of this technique with a language
                     model for the modern language yields a complete historical text normalization
                     system.</div>
                  
                  <div class="counter"><a href="#p41">41</a></div>
                  <div class="ptext" id="p41">Of course, the requirement for training data sets practical limitations --
                     training data is often unavailable and its production is expensive and
                     time-consuming. In <a href="#mitankin2014">Mitankin et al. (2014)</a> the authors try to address this
                     problem and suggest a completely automatic historical text normalization
                     approach. The main idea is to automatically generate pairs of historical word
                     and its modern variant. <a href="#mitankin2014">Mitankin et al. (2014)</a> proposes to use
                     Levenshtein edit distance and approximate search in the modern dictionary in
                     order to generate for each historical word its most relevant modern variant(s).
                     Afterwards we are in the situation to run the historical text normalization
                     system from <a href="#mitankin2014">Mitankin et al. (2014)</a> and <a href="#sariev2014">Sariev et al. (2014)</a>.
                     Unfortunately, the Levenshtein edit distance, as a generator of pairs, turns out
                     to introduce a lot of noise in the system. In particular, the accuracy of the
                     normalization system drops from 94% to 82%. </div>
                  
                  <div class="counter"><a href="#p42">42</a></div>
                  <div class="ptext" id="p42">The results from the current paper suggest two straightforward approaches in
                     order to reduce the noise introduced by the unsupervised approximate search. The
                     first approach is the following. Instead of searching for <em class="emph">historical
                        words</em> and their <em class="emph">modern word variants</em>, to search for
                     <em class="emph">historical phrases</em> and their <em class="emph">modern phrase
                        variants</em>, automatically extracted from the SCDAWG structure for the
                     historical texts and modern texts, respectively. In this way we plug in
                     additional language context in the query, which in general will exclude
                     orthographically similar but semantically irrelevant candidates. On the other
                     hand, if the modern text corpus covers the domain of the historical texts, it is
                     possible that the basic phrases in the historic language have indeed been
                     preserved in the modern language and do have their spelling variants in the
                     modern language. Thus, without seriously reducing the recall, we could increase
                     the quality of the automatically generated pairs <em class="emph">historical phrases</em>
                     and their <em class="emph">modern phrase var</em>. The production of pairs of words is
                     then straightforward<a class="noteRef" href="#d4e1936">[7]</a>. </div>
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">The second approach is the following. Instead of modifying the searching space,
                     we can modify the edit-distance and use more relevant edit-distance mechanism.
                     As described in <a href="#section04">Section 4</a>, we can use the SCDAWG
                     structure in order to align different editions of the same source. In
                     particular, if the editions belong to close time periods, the most disagreements
                     in the alignment will be due to uncertainties in the spelling that have been
                     typical in this time period. This phenomenon can be used in order to constrain
                     the elementary edit-operations and the contexts in which they are applicable. As
                     a result we can expect that the generator of pairs, historic word and its modern
                     variant, will be more relevant with respect to the structure of the historical
                     language. </div>
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">We should stress that the arguments raised in the previous two paragraphs require
                     further research. It is also a challenging open problem to directly map the
                     SCDAWG structure of a historical corpus to a substructure of the SCDAWG
                     structure for a large modern language. The latter, of course, would resolve the
                     normalization problem. </div>
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">The substrings of a SCDAWG index could also be used to train models for text
                     classification tasks. A similiar approach, which uses maximal substrings of a
                     suffix tree and led to promising results, had been persued in <a href="#okanohara2009">Okanohara and Tsujii (2009)</a>. As stated in <a href="#section02">Section
                        2</a> the two-sided closures, which form the nodes of the SCDAWG,
                     represent more natural infixes than those of a suffix tree or DAWG. Therefore
                     they may render descriptive features which still are not restricted to fixed
                     word boundaries. To find optimal features in the substrings of a SCDAWG of a
                     corpus again metadata can be used to find longest or shortest substrings which
                     occur only in a certain instance of a metadata attribute. For example the
                     longest/shortest substrings which belong to the author “Goethe” can be used
                     as features to train a text classification model with regard of this metadata
                     attribute.</div>
                  </div>
               
               <div id="section08" class="div div0">
                  
                  <h1 class="head">Conclusion</h1>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">In this paper we have shown how symmetric compact acyclic word graphs (SCDAWGs)
                     can help to efficiently mine interesting portions of texts in corpora, which
                     serve as a basis for many ways of comparing texts like alignment and the
                     detection of text reuse (<a href="#section04">Section 4</a>). These
                     findings can be refined by adding a form of “linguistic analysis” where
                     “phrases”, “subphrases”, “function words”, and “important
                     content words” (characteristic atomic kernels) are computed in an
                     unsupervised way without using prior linguistic knowledge (<a href="#section05">Section 5</a>). If metadata for the texts in the
                     collection are available, additional information stored in the index and the
                     above techniques give a basis for finding similarities in the space of metadata
                     (<a href="#section06">Section 6</a>).</div>
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">Of course many of the here mentioned techniques would require more detailed
                     analysis and comparsion with other existing methods. However this was not the
                     purpose of this paper, since its focus didn't lie on the development and
                     description of a certain method but rather on showing how manifold and versatile
                     the application of such an index can be to the task of large scale corpus
                     exploration. In many cases, various variants exist for the concrete methods used
                     in our examples. When moving to real applications, experts from Digital
                     Humanities are needed to find those variants that are most adequate and lead to
                     real insights. </div>
                  </div>
               
               <div id="section09" class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">We express our gratitude to: Uwe Springmann for his help with OCR recognition of
                     historic documents and the corpus derived from them. We also thank Stefanie
                     Schneider for the provision of the lyrics corpus. Part of the results described
                     in this paper were obtained during the stay of the third author at LMU for which
                     he has received funding from the People Programme (Marie Curie Actions) of the
                     European Union's Seventh Framework Programme (FP7/2007--2013) under REA grant
                     agreement 625160.</div>
                  
                  </div>
               
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e253"><span class="noteRef lang ">[1] As a matter
                     of fact, there is no formal definition of text units (sequences of symbols)
                     that are linguistically relevant.</span></div>
               <div class="endnote" id="d4e302"><span class="noteRef lang ">[2] A string \(v\) is an infix of a text \(t\) if \(t\) can be represented as a concatenation of the form
                     \(t=uvw\). A string is
                     an infix of a corpus if it is an infix of a text of the corpus.</span></div>
               <div class="endnote" id="d4e339"><span class="noteRef lang ">[3] \(\vert
                     v\vert\) denotes the length of the string \(v\).</span></div>
               <div class="endnote" id="d4e568"><span class="noteRef lang ">[4] An exception are very small infixes,
                     which often also are left- and right-closed in the above
                     sense.</span></div>
               <div class="endnote" id="d4e1550"><span class="noteRef lang ">[5] More generally, the ideal index structure for corpus
                     analyzers would point to all and exactly those infixes that are relevant for
                     the analysis, their positions and their relationships.</span></div>
               <div class="endnote" id="d4e1573"><span class="noteRef lang ">[6] Since we have a
                     symbol-based view on corpora, the expression function “word” is
                     misleading. The blank, as well as strings such as <span class="monospace">as
                        well as</span> may serve as a function “word”.</span></div>
               <div class="endnote" id="d4e1936"><span class="noteRef lang ">[7] Actually, the approach in <a href="#gerdjikov2013">Gerdjikov et al. (2013)</a> is not limited to words.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="blumer1987"><!-- close -->Blumer et al. 1987</span> Blumer, A., Blumer, J.,
                  Haussler, D., McConnell, R., and Ehrenfeucht, A.“Complete
                  inverted files for efficient text retrieval and analysis”. <cite class="title italic">Journal of the ACM (JACM)</cite> 34 (1987): 578-595.</div>
               <div class="bibl"><span class="ref" id="büchler2012"><!-- close -->Büchler et al. 2012</span> Büchler, M., Crane, G.,
                  Moritz, M., and Babeu, A. “ Increasing recall for text
                  re-use in historical documents to support research in the humanities
                  ”. In <cite class="title italic">Proceedings of 16th International Conference
                     on Theory and Practice of Digital Libraries, (tpdl 2012): </cite> pp.
                  95–100 <cite class="title italic">Springer Berlin Heidelberg</cite>.</div>
               <div class="bibl"><span class="ref" id="gerdjikov2016"><!-- close -->Gerdjikov and Schulz 2016</span> Gerdjikov, S., and
                  Schulz, K. U. “Corpus analysis without prior linguistic
                  knowledge-unsupervised mining of phrases and subphrase structure”.
                  <cite class="title italic">ArXiv e-prints</cite> (2016): 1602.05772.</div>
               <div class="bibl"><span class="ref" id="gerdjikov2013"><!-- close -->Gerdjikov et al. 2013</span> Gerdjikov, S. and Mihov,
                  S. and Nenchev, V. “Extraction of spelling variations from
                  language structure for noisy text correction”. In <cite class="title italic">Proc. Int. Conf. for Document Analysis and Recognition</cite>
                  (2013): 324-328.</div>
               <div class="bibl"><span class="ref" id="gusfield1997"><!-- close -->Gusfield 1997</span> Gusfield, D. <cite class="title italic">Algorithms on strings, trees and sequences: computer science and
                     computational biology</cite>. Cambridge university press, Cambridge,
                  1997.</div>
               <div class="bibl"><span class="ref" id="inenaga2005"><!-- close -->Inenaga et al. 2005</span> Inenaga, S., Hoshino, H.,
                  Shinohara, A., Takeda, M., Arikawa, S., Mauri, G., and Pavesi, G.“On-line construction of compact directed acyclic word
                  graphs”. <cite class="title italic">Discrete Applied Mathematics</cite>
                  146 (2005): 156-179. </div>
               <div class="bibl"><span class="ref" id="mccreight1976"><!-- close -->McCreight 1976</span> McCreight, Edward M.“A space-economical suffix tree construction algorithm”.
                  <cite class="title italic">Journal of the ACM (JACM)</cite> 23 (1976): 262-272. </div>
               <div class="bibl"><span class="ref" id="mitankin2014"><!-- close -->Mitankin et al. 2014</span> Mitankin, P. and Gerdjikov,
                  S. and Mihov, S. “ An approach to unsupervised historical
                  text normalization ”. In <cite class="title italic">Proceedings of the
                     First International Conference on Digital Access to Textual Cultural
                     Heritage: </cite> (2014) 29-34.</div>
               <div class="bibl"><span class="ref" id="needleman1970"><!-- close -->Needleman and Wunsch 1970</span> Needleman, S. B., and
                  Wunsch, C. D.“A general method applicable to the search for
                  similarities in the amino acid sequence of two proteins.”. <cite class="title italic">Journal of molecular biology</cite> 48 (1970): 443-453. </div>
               <div class="bibl"><span class="ref" id="okanohara2009"><!-- close -->Okanohara and Tsujii 2009</span> Okanohara, Daisuke
                  and Tsujii, Jun'ichi“Text categorization with all substring
                  features”. In <cite class="title italic">Proceedings of the 2009 SIAM
                     International Conference on Data Mining: </cite> (2009) 839-846.</div>
               <div class="bibl"><span class="ref" id="sariev2014"><!-- close -->Sariev et al. 2014</span> Sariev, Andrei and Nenchev,
                  Vladislav and Gerdjikov, Stefan and Mitankin, Petar and Ganchev, Hristo and
                  Mihov, Stoyan and Tinchev, Tinko“Flexible noisy text
                  correction”. In <cite class="title italic">Proceedings of Document Analysis
                     Systems: </cite> (2014)</div>
               <div class="bibl"><span class="ref" id="schütze1993"><!-- close -->Schütze and Pedersen 1993</span> Schütze, H., and
                  Pedersen, J.“ A vector model for syntagmatic and
                  paradigmatic relatedness ”. In <cite class="title italic">Proceedings of
                     the 9th Annual Conference of the UW Centre for the New OED and Text
                     Research, (oed 1993): </cite> pp. 104–113.</div>
               <div class="bibl"><span class="ref" id="storjohann2010"><!-- close -->Storjohann 2010</span> Storjohann, P. (Ed.) <cite class="title italic">Lexical-semantic relations: theoretical and practical
                     perspectives (Vol. 28)</cite>. John Benjamins Publishing, Cambridge,
                  2010.</div>
               <div class="bibl"><span class="ref" id="ukkonen95"><!-- close -->Ukkonen 1995</span> Ukkonen, Esko. “On-line construction of suffix trees”. <cite class="title italic">Algorithmica</cite>14 (1995): 249-260. </div>
               <div class="bibl"><span class="ref" id="weiner1973"><!-- close -->Weiner 1973</span> Weiner, P. “Linear
                  pattern matching algorithms”. In <cite class="title italic">Proceedings of
                     14th Annual Symposium on Switching and Automata Theory, (swat 1973):
                     </cite> pp. 1–11 <cite class="title italic">IEEE</cite>.</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>