<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.digitalhumanities.org/dhq/common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="http://www.digitalhumanities.org/dhq/common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article">Using an Advanced Text Index Structure for
Corpus Exploration in Digital Humanities</title>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Tobias <dhq:family>Englmeier</dhq:family></dhq:author_name>
                    <dhq:affiliation>CIS, Ludwig-Maximilians University, Munich,
                        Germany</dhq:affiliation>
                    <email>englmeier@cis.uni-muenchen.de</email>
                    <dhq:bio>
                        <p> Tobias Englmeier is a PhD candidate at the Centrum für Informations- und
                            Sprachverarbeitung (CIS) at the Ludwig Maximilians University of Munich.
                            His PhD project is centered around the topics of string matching and OCR
                            postcorrection. Additionally he has been involved in the conception and
                            implementation of numerous Digital Humanities projects coordinated by
                            the IT Gruppe Geisteswissenschaften (ITG) at the Ludwig Maximilians
                            University of Munich. </p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Marco <dhq:family>Büchler</dhq:family></dhq:author_name>
                    <dhq:affiliation>Institute of Computer Science, University of Göttingen,
                        Göttingen, Germany</dhq:affiliation>
                    <email>mbuechler@etrap.eu</email>
                    <dhq:bio>
                        <p> Marco Büchler holds a Diploma in Computer Science. From 2006 to 2014 he
                            worked as a Research Associate in the Natural Language Processing Group
                            at Leipzig University. From April 2008 to March 2011 Marco served as the
                            technical Project Manager for the eAQUA project and continued to work in
                            that capacity for the following eTRACES project. In March 2013 he
                            received his PhD in eHumanities. Since May 2014 he leads a Digital
                            Humanities Research Group at the Göttingen Centre for Digital
                            Humanities. His research includes Natural Language Processing on Big
                            Humanities Data. Specifically, he works on Historical Text Reuse
                            Detection and its application in the business world. In addition to his
                            primary responsibilities, Marco manages the Medusa project (Big Scale
                            co-occurrence and NGram framework) as well as the TRACER machine for
                            detecting historical text reuse.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Stefan <dhq:family>Gerdjikov</dhq:family></dhq:author_name>
                    <dhq:affiliation>FMI, University of Sofia "St. Kliment Ohridski", Sofia,
                        Bulgaria</dhq:affiliation>
                    <email>st_gerdjikov@abv.bg</email>
                    <dhq:bio>
                        <p> Stefan Gerdjikov is an Assistent Professor at the Faculty for
                            Informatics and Mathematics in the University of Sofia. He holds a PhD
                            degree in Mathematics from the University of Sofia. His prime research
                            area is Natural Language Processing where he studies approximate search
                            techniques and index structures for text mining.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Klaus U. <dhq:family>Schulz</dhq:family></dhq:author_name>
                    <dhq:affiliation>CIS, Ludwig-Maximilians University, Munich,
                        Germany</dhq:affiliation>
                    <email>schulz@cis.uni-muenchen.de</email>
                    <dhq:bio>
                        <p> Klaus U. Schulz is Professor in Computational Linguitics and since 1992
                            the technical director of the Centrum für Informations- und
                            Sprachverarbeitung (CIS) at the Ludwig Maximilians University of Munich.
                            The work of Professor Schulz concentrates on Semantic Search,
                            Construction of Ontologies and Taxonomies, Digital Libraries, Language
                            Technology for Optical Character Recognition and Document Analysis and
                            Finite-State Technology. </p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <!-- This information will be completed at publication -->
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>                
                <idno type="DHQarticle-id">000522</idno>
                <idno type="volume">015</idno>
                <idno type="issue">1</idno>
                <date/>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nc-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item>corpus exploration</item>
                        <item>metadata</item>
                        <item>phrase extraction</item>
                        <item>text alignment</item>
                        <item>SCDAWG</item>
                        <item>index structures</item>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
        </revisionDesc>
    </teiHeader>

    <text>
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p> With suitable index structures many corpus exploration tasks can be solved in an
                    efficient way without rescanning the text repository in an online manner. In
                    this paper we show that symmetric compacted directed acyclic word graphs
                    (SCDAWGs) - a refinement of suffix trees - offer an ideal basis for corpus
                    exploration, helping to answer many of the questions raised in DH research in an
                    elegant way. From a simplified point of view, the advantages of SCDAWGs rely on
                    two properties. First, needing linear computation time, the index offers a joint
                    view on the similarities (in terms of common substrings) and differences between
                    all text. Second, structural regularities of the index help to mine interesting
                    portions of texts (such as phrases and concept names) and their relationship in
                    a language independent way without using prior linguistic knowledge. As a
                    demonstration of the power of these principles we look at text alignment, text
                    reuse in distinct texts or between distinct authors, automated detection of
                    concepts, temporal distribution of phrases in diachronic corpora, and related
                    problems. </p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>A study investigating the application of the SCDAWG index structure for large
                    scale corpus exploration</p>
            </dhq:teaser>
        </front>
        <body>
            <div xml:id="section01">
                <head>1. Introduction</head>

                <p> A text/corpus index is a kind of table that, given a string <formula
                        notation="tex" rend="inline">\(w\)</formula>, stores the positions of all
                    occurrences of <formula notation="tex" rend="inline">\(w\)</formula> in the
                    given text/corpus. The computation of the index is a preprocessing step to be
                    applied only once. Corpus index structures considerably simplify corpus analysis
                    since they help to avoid rescanning the complete texts for each task. The index
                    helps to directly answer many interesting questions, hence index based methods
                    in general are much faster and more elegant. In this paper we look at an
                    advanced corpus index structure and explain its potential use in Digital
                    Humanities. This index, called symmetric compacted directed acyclid word graph
                    (SCDAWG), is used to represent a collection of texts, each text is considered as
                    a flat sequence of symbols <ptr target="#inenaga2005"/>. SCDAWGs can be
                    considered as a refinement of suffix trees <ptr target="#weiner1973"/>, <ptr
                        target="#mccreight1976"/>, <ptr target="#ukkonen95"/>, <ptr
                        target="#gusfield97"/>. The SCDAWG for a given text collection, as the
                    suffix tree, can be computed in time and space linear in the size of the corpus.
                    As a first advantage, the SCDAWG of a corpus is much smaller than the
                    corresponding suffix tree. Even more importantly, SCDAWGs have two special
                    features that make them very attractive for various corpus exploration tasks. </p>
                <p>
                    <emph>Finding co-occurrences.</emph> Using the SCDAWG it is simple to find all
                    co-occurrences of the same portions of text in two, several, or all texts of the
                    corpus. The index enables a joint search in all texts, there is no need for
                    comparing all pairs of texts individually. This joint search, which leads to all
                    co-occurrences, can be realized using a single scan of the index. The importance
                    of this feature in our context should be obvious: research in Digital Humanities
                    is often concentrated on the problem of finding phrases and text portions that
                    occur in distinct (two, several, or all) texts of a collection. For all these
                    problems the use of SCDAWGs offers an elegant solution. As a demonstration we
                    look at alignment of two or several texts, detection of text reuses in distinct
                    texts, and detection of text reuses between distinct authors. </p>
                <p>
                    <emph>Mining interesting concepts and their relationship.</emph> The nodes of
                    the SCDAWG represent portions of text (sequences of symbols, infixes). In
                    general, the number of all distinct infixes of a corpus <formula notation="tex"
                        rend="inline">\(C\)</formula> is quadratic in size <formula notation="tex"
                        rend="inline">\(|C|\)</formula> of the corpus. In contrast, the number of
                    nodes of the SCDAWG is always linear in <formula notation="tex" rend="inline"
                        >\(|C|\)</formula>. Yet, in a sense to be explained below, it contains all
                    infixes that are interesting from a linguistic point of view <note>As a matter
                        of fact, there is no formal definition of text units (sequences of symbols)
                        that are linguistically relevant.</note>. For example, names, concepts and
                    phrases etc. in general correspond to nodes of the SCDAWG. Compared to suffix
                    trees, SCDAWGs yield a much smaller set of nodes/infixes with this property.
                    Using structural regularities of the index, the set of all nodes can be even
                    further filtered to approximate the subset of linguistically relevant infixes.
                    Also hierarchical relationships and dependencies between these concepts and
                    phrases can be detected. From the perspective of Digital Humanities, these
                    mining techniques are relevant since they are completely language independent
                    and do not use any kind of prior linguistic knowledge. They point to interesting
                    concepts and bricks of text, the index helps to study their use, structure and
                    relationship. </p>
                <p> In this paper we first formally introduce SCDAWGs in <ref target="#section02"
                        >Section 2</ref>. We compare three related index structures, SCDAWGs, suffix
                    trees and DAWGs (directed acyclic graphs, <ptr target="#blumer1987"/>), compare
                    the sizes of these index structures and explain the advantages of the SCDAWG for
                    corpus exploration tasks as those mentioned above. In <ref target="#section03">
                        Section 3</ref> we give a brief overview of the corpora used for the
                    experiments described in the paper. In <ref target="#section04"> Section 4</ref>
                    we show how SCDAWGs can be utilized to solve tasks in the fields of text
                    alignment and text reuse detection <ptr target="#büchler2012"/>. For these
                    problems, the full set of nodes of the SCDAWG is used. In <ref
                        target="#section05"> Section 5</ref> we sketch the afore\-mentioned
                    filtering of nodes/infixes based on structural regularities (in <ptr
                        target="#gerdjikov2016"/> the filtering method is described in full detail).
                    Using the example corpora we illustrate how concepts and their relationships can
                    be mined in a language independent way. In <ref target="#section06"> Section
                        6</ref> we look at extensions of the SCDAWG index, adding metadata
                    information (e.g., on authors, temporal periods) from the texts. Examples from
                    our corpora show how to detect text reuses, e.g., between distinct authors and
                    to reveal the temporal flow of phrases across texts/authors. Before we come to a
                    short conclusion, <ref target="#section07"> Section 7</ref> provides loose ends
                    for future research, indicating how SCDAWGs might help to treat diachronic
                    language variation and various text classification tasks. </p>
            </div>

            <div xml:id="section02">
                <head>SCDAWGs as a corpus index structure</head>
                <p>In the introduction we explained the general benefits that can be obtained for
                    corpus analysis when using an index structure. If we are only interested in the
                    distribution and occurrences of single words, a simple index structure is
                    sufficient that represents the corpus as a <q>bag of words</q>. Usually
                    an <q>inverted file</q> then stores for each word <formula
                        notation="tex" rend="inline">\(w\)</formula> occurring in the corpus the
                    texts and positions where <formula notation="tex" rend="inline">\(w\)</formula>
                    occurs. However, corpus analysis in Digital Humanities and other fields is often
                    focussed on other pieces of text: re-used portions of text, names,
                    terminological and other multi-word expressions, phrases that express semantic
                    relationships, syllables, morphems, to name a few. In this situation advanced
                    index structures are preferable that give direct access to the occurrences of
                    arbitrary infixes<note>A string <formula notation="tex" rend="inline"
                            >\(v\)</formula> is an infix of a text <formula notation="tex"
                            rend="inline">\(t\)</formula> if <formula notation="tex" rend="inline"
                            >\(t\)</formula> can be represented as a concatenation of the form
                            <formula notation="tex" rend="inline">\(t=uvw\)</formula>. A string is
                        an infix of a corpus if it is an infix of a text of the corpus.</note>. This
                    explains why corpus analysis tools in Digital Humanities or Computational
                    Biology are often based on the latter, more powerful type of index structures.
                    Among the latter type of index structure, directed acyclic word graphs (DAWGs)
                    and suffix trees have often been used for research, but refinements have not
                    received proper attention. In this paper we describe compacted directed acyclic
                    word graphs (CDAWGs) and a symmetric variant (SCDAWGs) and argue that these
                    index structures are preferable for many corpus analysis tasks.</p>
                <p> The common ideas behind DAWGS, suffix trees, CDAWGs and SCDAWGs are summarized
                    in the following way. <list>
                        <item> Each index represents a graph, nodes representing specific infixes of
                            the corpus. </item>
                        <item> Each infix of the corpus is represented at most once in the index. </item>
                        <item> The total size of the index structure is linear in the size of the
                            corpus. </item>
                        <item> Given any string <formula notation="tex" rend="inline"
                                >\(v\)</formula> we may use the index to decide in time <formula
                                notation="tex" rend="inline">\(O(\vert v\vert)\)</formula> if
                                <formula notation="tex" rend="inline">\(v\)</formula> is an infix of
                            the corpus.\footnote{<formula notation="tex" rend="inline">\(\vert
                                v\vert\)</formula> denotes the length of the string <formula
                                notation="tex" rend="inline">\(v\)</formula>.} It is also possible
                            to use the index <q>as a guide</q> for finding all texts and
                            positions where <formula notation="tex" rend="inline">\(v\)</formula>
                            occurs. </item>
                    </list> The nodes of a DAWG represent the infixes <formula notation="tex"
                        rend="inline">\(v\)</formula> that are <q>left-closed</q> in the
                    sense that there does {\em not} exist a unique symbol <formula notation="tex"
                        rend="inline">\(\sigma\)</formula> directly in front of each occurrence of
                        <formula notation="tex" rend="inline">\(v\)</formula> in the corpus. The
                    nodes of a suffix tree represent the infixes <formula notation="tex"
                        rend="inline">\(v\)</formula> that are <q>right-closed</q> in the
                    sense that there does {\em not} exist a unique symbol <formula notation="tex"
                        rend="inline">\(\sigma\)</formula> directly after each occurrence of
                        <formula notation="tex" rend="inline">\(v\)</formula> in the corpus. The
                    CDAWG and the SCDAWG for a corpus have the same set of nodes representing the
                    infixes <formula notation="tex" rend="inline">\(v\)</formula> that are
                        <q>left- and right-closed</q> at the same time. Edges of a suffix
                    tree, DAWG, or CDAWG represent extensions of infixes in reading order (i.e., to
                    the right). SCDAWGs have two type of edges, respectively representing right
                    extensions in standard reading order and left-extensions in left-to-right order.
                        <hi rend="bold"> Example 2.1 </hi> As an example, consider the corpus with
                    the two toy <q>texts</q>
                    <list><item><hi rend="monospace">op 1 in A</hi></item>
                        <item>
                        <hi rend="monospace">op 2 in B</hi></item>
                    </list>
                    <figure xml:id="figure01">
                        <head><hi rend="bold">Figure 1:</hi> DAWG, suffix tree, and symmetric
                            compact DAWG (SCDAWG) for the corpus with two toy texts <q>op 1 in
                                A</q> and <q>op 2 in B</q>.</head>
                        <graphic url="images/figure01.pdf"/>
                    </figure> Infixes <q><hi rend="monospace">n</hi></q> and <q><hi
                            rend="monospace">in</hi></q> are <emph>not</emph> left-closed since
                    we find the symbol <q><hi rend="monospace">i</hi></q> directly in front
                    of each occurrence of <q><hi rend="monospace">n</hi></q> and the blank
                    directly in front of each occurence of <q><hi rend="monospace"
                        >in</hi></q> in the corpus. In contrast, infixes <q><hi
                            rend="monospace"> in</hi></q> , <q><hi rend="monospace">
                        i</hi></q> and <q><hi rend="monospace">op 1</hi></q> are
                    left-closed. Each prefix of each text is left-closed, the corpus contains
                        <formula notation="tex" rend="inline">\(20\)</formula> left-closed infixes,
                    its DAWG has <formula notation="tex" rend="inline">\(20\)</formula> nodes (<ref
                        target="#figure01">cf. Figure 1</ref>). Infixes <q><hi rend="monospace"
                            >o</hi></q> and <q><hi rend="monospace">op</hi></q> are
                        <emph>not</emph> right-closed since we find the symbol <q><hi
                            rend="monospace">p</hi></q> directly after each occurrence of
                            <q><hi rend="monospace">o</hi></q> and the blank directly after
                    each occurence of <q><hi rend="monospace">op</hi></q> in the corpus. In
                    contrast, infixes <q><hi rend="monospace">op </hi></q> and <q><hi
                            rend="monospace">p </hi></q>, <q><hi rend="monospace">1 in
                            A</hi></q> are right-closed. Each suffix of each text is
                    right-closed, the corpus contains <formula notation="tex" rend="inline"
                        >\(25\)</formula> right-closed infixes, accordingly its suffix tree has
                        <formula notation="tex" rend="inline">\(25\)</formula> nodes (<ref
                        target="#figure01">cf. Figure 1</ref>). The only infixes that are left- and
                    right-closed are the empty string, the blank, the strings <q><hi
                            rend="monospace">op </hi></q> and <q><hi rend="monospace"> in
                        </hi></q> and the two texts. Hence the (S)CDAWG only has <formula
                        notation="tex" rend="inline">\(6\)</formula> nodes (<ref target="#figure01"
                        >cf. Figure 1</ref>). Lines/transitions in the figures represent extensions
                    of nodes/infixes. In the DAWG in <ref target="#figure01"> Figure 1</ref>, node
                            <q><hi rend="monospace"> in </hi></q> has two extensions with
                        <formula notation="tex" rend="inline">\(A\)</formula> and <formula
                        notation="tex" rend="inline">\(B\)</formula>, respectively leading to the
                    left-closed infixes <q><hi rend="monospace">op 1 in A</hi></q> and
                            <q><hi rend="monospace">op 2 in B</hi></q>. In the suffix tree,
                    node <q><hi rend="monospace">op </hi></q> has two extensions with
                    symbols <formula notation="tex" rend="inline">\(1\)</formula> and <formula
                        notation="tex" rend="inline">\(2\)</formula>, respectively leading to the
                    right-closed infixes <q><hi rend="monospace">op 1 in A</hi></q> and
                            <q><hi rend="monospace">op 2 in B</hi></q>. In the SCDAWG in
                        <ref target="#figure01"> Figure 1</ref>, black (blue) links represent right
                    (left) extensions. Labels of left extensions are written in right-to-left order. </p>

                <p> As we mentioned above, each index can be used to check if a string <formula
                        notation="tex" rend="inline">\(v\)</formula> represents an infix of the
                    corpus in time linear in <formula notation="tex" rend="inline">\(\vert
                        v\vert\)</formula>. We sketch the procedure for the SCDAWG in <ref
                        target="#figure01"> Figure 1</ref>. Assume we want to check if <q><hi
                            rend="monospace">p 1</hi></q> is an infix. Starting from the top
                    node (empty string) we look for an extension to the right where the label starts
                    with the letter <q><hi rend="monospace">p</hi></q>. We find an extension
                    with label <q><hi rend="monospace">p </hi></q> leading to <q><hi
                            rend="monospace">op </hi></q>. We continue to look for a right
                    extension of the new node starting with <q><hi rend="monospace"
                        >1</hi></q>. We find the extension with label <q><hi
                            rend="monospace">1 in A</hi></q> leading to <q><hi
                            rend="monospace">op 1 in A</hi></q>. Hence <q><hi
                            rend="monospace">p 1</hi></q> is an infix.</p>

</div>
            <div xml:id="section03">
                <head>Advantages of the SCDAWG index</head>
                <p>The strength of SCDAWGs compared to DAWGs and suffix trees relies on three
                    features. <list><item>
                            <hi rend="bold"> 1. Number of nodes.</hi> In general, the number of
                            nodes in the (S)CDAWG of a corpus is much smaller than the number of
                            nodes in a suffix tree or DAWG. This is seen in <ref target="#figure01"
                                >Figure 1</ref>. A more informative picture is found in <ref
                                target="#table01"> Table 1</ref> where we compare the size of these
                            index structures for a small collection of corpora of distinct sizes.
                            For the sake of completeness, the table also includes data for suffix
                            tries, a simplified version of suffix trees where each suffix represents
                            a node.</item>
                        <item>
                            <hi rend="bold"> 2. Naturalness of nodes.</hi> Most of the infixes
                            representing the nodes of a (S)CDAWG are very natural from an intuitive
                            point of view<note>An exception are very small infixes, which often also
                                are left- and right-closed in the above sense.</note>: imagine we
                            index the German version of Goethe's Faust. Among the nodes of the
                            suffix tree, we will find right-closed infixes such as <q><hi
                                    rend="monospace">ephisto</hi></q>, <q><hi
                                    rend="monospace">phisto</hi></q>, <q><hi
                                    rend="monospace">retchen</hi></q>, and <q><hi
                                    rend="monospace">etchen</hi></q>. In the DAWG we find
                            left-closed infixes such as <q><hi rend="monospace">
                                Mephist</hi></q>, <q><hi rend="monospace">
                                Mephis</hi></q>, <q><hi rend="monospace">
                                Gretche</hi></q> and <q><hi rend="monospace">
                                Gretch</hi></q>. In the (S)CDAWG we only have the two-sided
                            closures <q><hi rend="monospace"> Mephisto</hi></q> and
                                    <q><hi rend="monospace"> Gretchen</hi></q>. The example
                            indicates that two-sided closure is a very natural property when looking
                            for <q>interesting</q> and <q>meaningful</q> infixes of
                            a corpus.</item>
                        <item>
                            <hi rend="bold"> 3. Node Extension Property.</hi> Among the infixes that
                            represent the nodes of the index (suffix tree, DAWG, or (S)CDAWG) in
                            general there are many inclusion relations where one infix <formula
                                notation="tex" rend="inline">\(v\)</formula> is a substring of
                            another one <formula notation="tex" rend="inline">\(w\)</formula>. In a
                            suffix tree, DAWG, or CDAWG extension steps departing from node <formula
                                notation="tex" rend="inline">\(v\)</formula> in many cases do not
                            lead to node <formula notation="tex" rend="inline">\(w\)</formula>. For
                            example, in the suffix tree in <ref target="#figure01"> Figure 1</ref>
                            we cannot reach <q><hi rend="monospace">op 1 in A</hi></q> from
                                    <q><hi rend="monospace"> in</hi></q> following the
                            transitions. In contrast, the two-directional edge structure of SCDAWGs
                            completely covers infix containment in the sense that <emph> whenever
                                node <formula notation="tex" rend="inline">\(v\)</formula> is a
                                substring of <formula notation="tex" rend="inline">\(w\)</formula>
                                there exists a chain of left and right extensions leading from
                                    <formula notation="tex" rend="inline">\(v\)</formula> to
                                    <formula notation="tex" rend="inline">\(w\)</formula></emph>. In
                            other words, starting from a node <formula notation="tex" rend="inline"
                                >\(v\)</formula> and traversing the index we find each context
                            containing <formula notation="tex" rend="inline">\(v\)</formula>. For
                            the sake of reference, this property will be called the <q>Node
                                Extension Property</q>. It can be used, for example, to find in
                            a very elegant way all texts where an infix <formula notation="tex"
                                rend="inline">\(v\)</formula> occurs: starting from the node that
                            represents (the two-sided closure of) <formula notation="tex"
                                rend="inline">\(v\)</formula>, compute all maximal chains of left
                            extensions followed by all maximal chains of right extensions. The nodes
                            reached at the end represent the texts in which <formula notation="tex"
                                rend="inline">\(v\)</formula> occurs. For example, in <ref
                                target="#figure01">Figure 1</ref> starting from <q><hi
                                    rend="monospace">op </hi></q> or <q><hi rend="monospace"
                                    > in </hi></q> we reach the nodes representing the two input
                            texts.</item>
                    </list>
                    <table xml:id="table01">
                        <head><hi rend="bold">Table 1:</hi>Comparing the size of distinct index
                            structures for four input texts.</head>
                        <thead>
                            <th/><th>Suffix Trie </th><th> Suffix Tree </th><th> DAWG </th><th>
                                CDAWG </th><th> SCDAWG</th>
                        </thead>
                        <tbody>
                            <tr><td/><td colspan="6"><hi rend="monospace">abcbc<formula
                                            notation="tex" rend="inline">\(abcab\)</formula></hi>:
                                    12 Bytes; 12 symbols</td></tr>
                            <tr>
                                <td> Nodes </td>
                                <td> 66 </td>
                                <td> 17 </td>
                                <td> 16 </td>
                                <td> 6 </td><td> 6 </td>
                            </tr>
                            <tr>
                                <td> Edges </td><td> 65 </td>
                                <td> 16 </td>
                                <td> 23 </td>
                                <td> 13 </td>
                                <td> 21 </td>
                            </tr>
                            <tr><td/><td colspan="6"><hi rend="monospace">OCR page</hi>: 7 KB; 6.945
                                    symbols</td></tr>
                            <tr>
                                <td> Nodes </td>
                                <td> 21.418.626 </td>
                                <td> 7.355 </td>
                                <td> 11.995 </td>
                                <td> 1.163 </td><td> 1.163 </td>
                            </tr>
                            <tr>
                                <td> Edges </td><td> 21.418.625 </td>
                                <td> 7.354 </td>
                                <td> 14.915 </td>
                                <td> 4.083 </td>
                                <td> 8.094 </td>
                            </tr>
                            <tr><td/><td colspan="6"><hi rend="monospace">Excerpt EU-Corpus</hi>:
                                    106 KB; ca. 106.000 symbols</td></tr>
                            <tr>
                                <td> Nodes </td><td> - </td>
                                <td> 161.001 </td>
                                <td> 165.962 </td>
                                <td> 25.273 </td><td> 25.273 </td>
                            </tr>
                            <tr>
                                <td> Edges </td><td> - </td>
                                <td> 161.000 </td>
                                <td> 227.515 </td>
                                <td> 86.826 </td>
                                <td> 170.358 </td>
                            </tr>
                            <tr><td/><td colspan="6"><hi rend="monospace">Small Corpus</hi>: 1.2 MB;
                                    ca. 1.250.000 symbols</td></tr>
                            <tr>
                                <td> Nodes </td><td> - </td>
                                <td> 1.921.704 </td>
                                <td> 1.922.811 </td>
                                <td> 366.070 </td><td> 366.070 </td>
                            </tr>
                            <tr>
                                <td> Edges </td><td> - </td>
                                <td> 1.921.703 </td>
                                <td> 2.730.597 </td>
                                <td> 1.173.856 </td>
                                <td> 2.355.669 </td>
                            </tr>
                        </tbody>
                    </table>
                    <hi rend="bold"> Node-documents function.</hi> Using the Node Extension Property
                    in one scan of the SCDAWG we may compute and store a function <formula
                        notation="tex" rend="inline">\(T\)</formula> that assigns to each node
                        <formula notation="tex" rend="inline">\(\mu\)</formula> (represented as a
                    number) the set of texts where <formula notation="tex" rend="inline"
                        >\(\mu\)</formula> occurs as an infix,<ptr target="#blumer1987"/>. <formula
                        notation="tex" rend="inline">\(T\)</formula> is called the
                        <emph>node-documents function</emph>. Let us assume that each text comes
                    with a number. We define a procedure FindTexts, the input is a node number
                        <formula notation="tex" rend="inline">\(\mu\)</formula>, the output is a set
                    of text numbers. The procedure is called in the form FindTexts(root). As a
                    result, the function <formula notation="tex" rend="inline">\(T\)</formula> is
                    computed. <code>
                        <formula notation="tex" rend="block"><formula notation="tex" rend="inline"
                                >\(\)</formula> Procedure FindTexts(<formula notation="tex"
                                rend="inline">\(\mu\)</formula>)\\ if <formula notation="tex"
                                rend="inline">\(T(\mu)\)</formula> is already defined, then\\
                            \indent output <formula notation="tex" rend="inline"
                                >\(T(\mu)\)</formula>;\\ else if <formula notation="tex"
                                rend="inline">\(\mu\)</formula> is a leaf (text), then\\ \indent let
                                <formula notation="tex" rend="inline">\(n\)</formula> be the number
                            of the text; \\ \indent define <formula notation="tex" rend="inline"
                                >\(T(\mu) := \{n\}\)</formula>;\\ \indent output <formula
                                notation="tex" rend="inline">\(\{n\}\)</formula>;\\ otherwise\\
                            \indent let <formula notation="tex" rend="inline"
                                >\(\nu_1,\ldots,\nu_k\)</formula> be the (left or right) immediate
                            successors of <formula notation="tex" rend="inline">\(\mu\)</formula>.\\
                            \indent call FindTexts(<formula notation="tex" rend="inline"
                                >\(\nu_1\)</formula>), ...., FindTexts(<formula notation="tex"
                                rend="inline">\(\nu_k\)</formula>);\\ \indent define $T(\mu) :=
                            \bigcup_{i=1,\ldots, k} T(\nu_i)<formula notation="tex" rend="inline"
                                >\(;\\ \indent output \)</formula>T(\mu)<formula notation="tex"
                                rend="inline">\(; \)</formula>$</formula>
                    </code> The procedure first visits all successor nodes of the input node
                        <formula notation="tex" rend="inline">\(\mu\)</formula> and then assigns the
                    union of the sets of text numbers of the direct successors to <formula
                        notation="tex" rend="inline">\(\mu\)</formula>. It is simple to see that
                    each link of the index is used only once. Hence, if the number of all distinct
                    texts is treated as a constant, the procedure is linear in the size of the
                    index. <figure xml:id="figure02">
                        <head><hi rend="bold">Figure 2:</hi> A text <formula notation="tex"
                                rend="inline">\(A\)</formula> leading to a value <formula
                                notation="tex" rend="inline">\(r=130,000\)</formula> for the LIS
                            problem if compared with a similar parallel text <formula notation="tex"
                                rend="inline">\(B\)</formula>.</head>
                        <graphic url="images/figure02.png"/>
                    </figure>
                </p>

            </div>

            <div xml:id="section03">
                <head>Corpora used for experiments</head>
                <p>For the experiments we selected corpora of different periods, languages, and
                    genres. Languages covered are Latin, English, German, French and Italian.
                    Domains vary from political texts to poetry, lyrics and literature. Metadata
                    include author, temporal data, and others.</p>
                <p><hi rend="bold"> Poems Corpus.</hi> The Poems Corpus consists of roughly 40k
                    poems which had been compiled from TEI – XML annotated documents from the
                    TextGrid Repository1. It contains poems written by German poets from about the
                    time when printing was invented to the beginning of the 20th century. Amongst
                    these authors are well known names like Johann Wolfgang v. Goethe, Friedrich
                    Schiller, Joseph v. Eichendorff, or Georg Trakl. All poems are written in German
                    language. In addition to the author each poem is furnished with the volume in
                    which the poem was published as well as dates referring to the timespan in which
                    the volume had been created. If this is not known the lifespan of the author is
                    used for the dates.</p>
                <p><hi rend="bold"> Lyrics Corpus.</hi> The Lyrics Corpus contains about 15k of
                    popular lyrics from songs, which had been present in the single charts of the
                    German-speaking world, reaching from the mid nineteen fifties to nowadays. This
                    corpus had been created by means of webcrawling. Aside from the interpreter it
                    stores the year, in which the song had been in the charts, the chart position
                    and whether the chart position had been a top 10 position, as well as the
                    language, in which the song was performed, as its metadata. The language had
                    been assigned automatically via n-gram based language classification. Since the
                    manner of appearance of the considered single charts has changed over time,
                    songs of the 80s and 90s are prevailing.</p>
                <p><hi rend="bold"> Parallel OCR Corpus.</hi> A smaller corpus of historic books,
                    which also had been digitized by means of OCR. All documents in this collection
                    had been created by two different OCR engines, leading to a parallel corpus of
                    the same content. The corpus was only used in the alignments, and there no
                    metadata had been considered.</p>
                <p><hi rend="bold"> Wittgenstein Corpus.</hi> The Wittgenstein Corpus consists of
                    3,871 original remarks of the German philosopher Ludwig Wittgenstein. Although
                    the general language is German, the corpus also contains a few remarks in
                    English.</p>
                <p><hi rend="bold"> Medline Corpus.</hi> The Medline Corpus is a collection of about
                    15 Million medical abstracts in English. Each abstract consists of about 10
                    sentences. We considered two subsets of this corpus. They were obtained by
                    randomly selecting 0.5% and 10%, respectively, of the abstracts and gathering
                    all the sentences in the resulting abstracts. The 0.5%-Medline Corpus consists
                    of 800 K sentences, whereas the 10%-Medline Corpus consists of about 15 M
                    sentences.</p>
            </div>

            <div xml:id="section04">
                <head>Using the index for alignment and text reuse</head>
                <p> In this section we use the node-documents function defined in <ref
                        target="#section02">Section 2</ref> to find large strings that co-occur in
                    several texts of the collection. Such strings point to some form of text
                    parallelism/reuse. Two applications are considered. In the first subsection we
                    show how to efficiently align two or more parallel texts in a linear manner. In
                    the second subsection we show how to explore large collections, looking at
                    maximal strings that co-occur in several texts of the corpus</p>
                <h2 xml:id="section04_1">Index-based text alignment</h2>
                <p>In our context, to <q>align</q> two strings means to find a common linear
                    representation where <q>common parts</q> of the two strings are shown,
                    defining a linear sceleton, and where <q>deviations</q> or regions where
                    the two strings do not agree are associated with the <q>holes</q> of the
                    sceleton. The classical method for this task is the Needleman-Wunsch Algorithm
                        <ptr target="#needleman1970"/>: The input are two texts <formula
                        notation="tex" rend="inline">\(A\)</formula> and <formula notation="tex"
                        rend="inline">\(B\)</formula> of length <formula notation="tex"
                        rend="inline">\(m=|A|\)</formula>, <formula notation="tex" rend="inline"
                        >\(n=|B|\)</formula>. The algorithm finds an optimal alignment, given a
                    scoring function. Using dynamic programming it fills the cells of a <formula
                        notation="tex" rend="inline">\(|A|\times |B|\)</formula> matrix, cells come
                    with <q>backward pointers</q>. The backward pointers are then used to
                    find an optimal alignment. The complexity is <formula notation="tex"
                        rend="inline">\(O(mn)\)</formula>, which is problematic for large texts.
                    E.g., comparing two texts with <formula notation="tex" rend="inline"
                        >\(10^6\)</formula> symbols each, we need several terrabytes to store the
                    full matrix. For a better solution we look at a strategy to translate the
                    problem</p>
                <p>
                    <hi rend="bold">Finding longest common subsequences and longest increasing
                        subsequences.</hi> Such a translation can be found in the longest common
                    subsequence or the longest increasing subsequence problem: Assume the scoring
                    function for an alignment assigns value 1 to each pair of matching symbols and
                    value 0 to each disagreement (here: insertion, deletion, or substitution). Then
                    the matching symbols of the alignment are given by the longest common
                    subsequence (LCS) of the two strings. A subsequence of a string <formula
                        notation="tex" rend="inline">\(A= a_1\ldots a_m\)</formula> is any string of
                    the form <formula notation="tex" rend="inline">\(A= a_{i_1}\ldots
                        a_{j_k}\)</formula> where <formula notation="tex" rend="inline">\(k \leq
                        m\)</formula> and <formula notation="tex" rend="inline">\(i_1 &lt; \ldots
                        &lt; i_k\)</formula>. For example, <q>bdf</q> is a subsequence of
                        <q>abcdef</q>. </p>
                <p> This means that for a particular and natural scoring function, finding the LCS
                    of two strings <formula notation="tex" rend="inline">\(A\)</formula> and
                        <formula notation="tex" rend="inline">\(B\)</formula> helps to solve the
                    global alignment problem for <formula notation="tex" rend="inline"
                        >\(A\)</formula> and <formula notation="tex" rend="inline">\(B\)</formula>.
                    To avoid the above matrix computation with its large complexity we may use an
                    alternative approach to solve the LCS, utilizing the longest increasing
                    subsequence (LIS) problem. Given a sequence of natural numbers, we look for the
                    longest increasing subsequence. For example, the LIS for <formula notation="tex"
                        rend="inline">\(1-5-2-7-3-4\)</formula> is <formula notation="tex"
                        rend="inline">\(1-2-3-4\)</formula>. Given <formula notation="tex"
                        rend="inline">\(A= a_1\ldots a_m\)</formula> and <formula notation="tex"
                        rend="inline">\(B= b_1\ldots b_n\)</formula>, the LCS problem for <formula
                        notation="tex" rend="inline">\(A\)</formula> and <formula notation="tex"
                        rend="inline">\(B\)</formula> can be reduced to the LIS
                    problem~\cite{gusfield1997algorithms}. We ignore the details of the translation,
                    but look at the complexity of the new procedure for solving the LCS. Let
                        <formula notation="tex" rend="inline">\(r(i)\)</formula> denote the number
                    of occurrences of the letter <formula notation="tex" rend="inline"
                        >\(a_i\)</formula> in <formula notation="tex" rend="inline">\(B\)</formula>,
                    let <formula notation="tex" rend="inline">\(r = \sum^m_{i=1} r(i)\)</formula>.
                    Using translation into LIS, the LCS problem for <formula notation="tex"
                        rend="inline">\(A\)</formula> and <formula notation="tex" rend="inline"
                        >\(B\)</formula> can be solved without dynamic programming in time <formula
                        notation="tex" rend="inline">\(O(r\ log(n))\)</formula>. </p>
                <p> In many cases, using the LIS method helps to reduce the computation time for an
                    alignment tasks. However, for large texts the value for the number <formula
                        notation="tex" rend="inline">\(r\)</formula> is typically very large and the
                    computation time remains enormous. For example, the <q>innocent</q> text
                        <formula notation="tex" rend="inline">\(A\)</formula> shown in <ref
                        target="#figure02">Figure 2</ref>, which is taken from a historical
                    document, already leads to a value of <formula notation="tex" rend="inline">\(r
                        \sim 130,000\)</formula> when compared with a similar text <formula
                        notation="tex" rend="inline">\(B\)</formula>. For more details on the LCS-
                    or LIS- problem see <ptr target="#gusfield97"/>. <hi rend="bold">Improved
                        index-based alignment.</hi> Consider the SCDAWG of two texts <formula
                        notation="tex" rend="inline">\(A\)</formula> and <formula notation="tex"
                        rend="inline">\(B\)</formula>. Applying the Node-documents function <formula
                        notation="tex" rend="inline">\(T\)</formula> it can be determined whether
                    the infix represented by each node occurs in both texts. The main observation
                    now is that using this information we immediately find large portions of the two
                    texts that have a perfect alignment. We use the following definition.</p>
                <p>
                    <hi rend="bold" xml:id="def_quasi_max">Definition 4.1</hi> An index node
                        <formula notation="tex" rend="inline">\(\mu\)</formula> of the SCDAWG for
                    two texts <formula notation="tex" rend="inline">\(A\)</formula> and <formula
                        notation="tex" rend="inline">\(B\)</formula> is called <emph>quasi
                        maximal</emph> if <list>
                        <item> 1. the infix associated with <formula notation="tex" rend="inline"
                                >\(\mu\)</formula> occurs in both texts <formula notation="tex"
                                rend="inline">\(A\)</formula> and <formula notation="tex"
                                rend="inline">\(B\)</formula>,</item>
                        <item> 2. <formula notation="tex" rend="inline">\(\mu\)</formula> does not
                            have any transition that leads to another node <formula notation="tex"
                                rend="inline">\(\mu\)</formula>’ with the Property 1.</item>
                    </list> As an illustration we refer to <ref target="#figure01">Figure 1</ref>.
                    Our two example <q>texts</q> lead to two quasi maximal nodes <q><hi
                            rend="monospace">op </hi></q> and <q><hi rend="monospace"> in
                        </hi></q>. Based on the <emph>Node-documents function</emph> there
                    exists a simple procedure to find all quasi-maximal nodes: a queue is
                    initialized with the root of the SCDAWG (empty string). We treat all entries of
                    the queue in the order they are added to the queue. The treatment of a node
                        <formula notation="tex" rend="inline">\(\mu\)</formula> involves two steps: <list>
                        <item> We consider each node reached from <formula notation="tex"
                                rend="inline">\(\mu\)</formula> with a single left or right
                            transition in the SCDAWG. Each such node representing an infix occurring
                            both in <formula notation="tex" rend="inline">\(A\)</formula> and
                                <formula notation="tex" rend="inline">\(B\)</formula> is added to
                            the end of the queue. </item>
                        <item> If <formula notation="tex" rend="inline">\(\mu\)</formula> does not
                            have any extension representing an infix occurring both in <formula
                                notation="tex" rend="inline">\(A\)</formula> and <formula
                                notation="tex" rend="inline">\(B\)</formula>, then <formula
                                notation="tex" rend="inline">\(\mu\)</formula> is added to the list
                            of quasi maximal nodes. </item>
                    </list> Note that each index node is added to the queue and treated at most
                    once. At the end we obtain the full list of all quasi maximal nodes. Simple
                    additional calculations provide the positions of the occurrences of each quasi
                    maximal nodes. Technical details are omitted. </p>


                <p> The full alignment method proceeds as follows: We compute the SCDAWG index for
                    the texts <formula notation="tex" rend="inline">\(A\)</formula> and <formula
                        notation="tex" rend="inline">\(B\)</formula> (linear time). We traverse the
                    index to find all quasi maximal nodes and the end positions of occurrences in
                        <formula notation="tex" rend="inline">\(A\)</formula> and <formula
                        notation="tex" rend="inline">\(B\)</formula>. We use end positions and the
                    LIS method to define the linear sceleton of the alignment. When using the LIS
                    method, the important new point to note is that the strings corresponding to the
                    quasi maximal nodes are treated as single <q>symbols</q>. In this way,
                    the factor <formula notation="tex" rend="inline">\(r\)</formula> mentioned in
                    the above formula is drastically reduced. In the experiment from <ref
                        target="#figure02"> Figure 2</ref>, the usual procedure leads to <formula
                        notation="tex" rend="inline">\(r \sim 130.000\)</formula>, using the SCDAWG
                    and quasi maximal nodes the value is <formula notation="tex" rend="inline"
                        >\(r=29\)</formula>. When aligning two OCR output texts of <formula
                        notation="tex" rend="inline">\(5.500\)</formula> symbols, the standard
                    method yields <formula notation="tex" rend="inline">\(r \sim
                        2.300.000\)</formula>, using the SCDAWG and quasi maximal nodes the value is
                    only <formula notation="tex" rend="inline">\(r=169\)</formula>.</p>

                <p>In a project related to improving OCR on historical documents we use this
                    technique to align the outputs of distinct OCR engines for historical texts.
                        <q>Holes</q> obtained from the alignment based on quasi maximal
                    nodes sometimes cover portions of texts where a finer subanalysis helps to
                    improve results. <ref target="#figure03">Figure 3</ref> shows an alignment
                    result for two OCR outputs and two texts, with distinct levels of
                    disagreement.</p>
                <figure xml:id="figure03">
                    <head><hi rend="bold">Figure 3:</hi> Two OCR outputs for two pages aligned using
                        index technology. Black parts represent quasi maximal nodes and thus
                        matches, red parts are disagreement regions.</head>
                    <graphic url="images/figure03.pdf"/>
                </figure>
                <p>
                    <hi rend="bold"> Multiple string alignment.</hi> When dealing with <formula
                        notation="tex" rend="inline">\(n>2\)</formula> texts <formula notation="tex"
                        rend="inline">\(A_1,\ldots, A_n\)</formula>, the notion of a quasi maximal
                    node is generalized. <hi rend="bold"> Definition 4.2 </hi> An index node
                        <formula notation="tex" rend="inline">\(\mu\)</formula> of the SCDAWG for
                        <formula notation="tex" rend="inline">\(A_1,\ldots, A_n\)</formula> is
                    called <emph> quasi maximal</emph> if <list>
                        <item> 1. the infix associated with <formula notation="tex" rend="inline"
                                >\(\mu\)</formula> occurs in all texts <formula notation="tex"
                                rend="inline">\(A_1,\ldots, A_n\)</formula>,</item>
                        <item>
                            <formula notation="tex" rend="inline">\(\mu\)</formula> does not have
                            any transition that leads to another node <formula notation="tex"
                                rend="inline">\(\mu\)</formula>’ with Property 1. </item>
                    </list> For technical reasons we actually have the additional restriction that
                    quasi maximal nodes used for alignment only occur once in each text. A
                    generalization of the LIS algorithm can be used to find a maximal linear
                    sequence of quasi maximal nodes of all texts, which gives the desired alignment
                    sceleton. As an illustration, <ref target="#figure04">Figure 4</ref> shows the
                    simultaneous alignment of three texts. Two sequences represent parallel parts of
                    the OCR-outputs of two OCR engines on a historical document, the third sequence
                    shows the corresponding part of the ground truth file. <figure xml:id="figure04">
                        <head><hi rend="bold">Figure 4:</hi> Subsegment of the multi-alignment of
                            three texts (OCR outputs and ground truth) using quasi-maximal nodes.
                            Grey parts represent quasi-maximal nodes, for the coloured regions at
                            least two texts show a disagreement.</head>
                        <graphic url="images/figure04.png"/>
                    </figure>
                </p>
                <h2 xml:id="section04_2">Detecting text reuse and similarities across
                    collections}</h2>
                <p> When comparing large collections of texts <formula notation="tex" rend="inline"
                        >\(A_1,\ldots A_n\)</formula>, in most cases it is not natural to ask for
                    substrings that co-occur in all texts. For the following experiment a node of
                    the SCDAWG index for <formula notation="tex" rend="inline">\(A_1,\ldots
                        A_n\)</formula> is called quasi maximal if it is quasi maximal for two texts
                    in sense of <ref target="#def_quasi_max">Definition 4.1</ref>. We have seen that
                    we can extract the set of all quasi maximal nodes from the SCDAWG index in this
                    sense, and for each such node <formula notation="tex" rend="inline"
                        >\(\mu\)</formula> with string <formula notation="tex" rend="inline"
                        >\(v_{\mu}\)</formula> we immediately get the information in which texts
                        <formula notation="tex" rend="inline">\(A_i\)</formula> the string <formula
                        notation="tex" rend="inline">\(v_{\mu}\)</formula> occurs. In <ref
                        target="#figure05">Figure 5</ref> this information is used to compute
                        <q>survey</q> graphs for text reuses in two complete collections
                    (Poems Corpus and Lyrics Corpus). Each graph contains two types of nodes. Large
                    and small ellipses respectively represent quasi maximal nodes and texts numbers
                    of the collection. For each node <formula notation="tex" rend="inline"
                        >\(\mu\)</formula> a prefix and a suffix of the node text <formula
                        notation="tex" rend="inline">\(v_{\mu}\)</formula> is shown. A link from a
                    quasi maximal node <formula notation="tex" rend="inline">\(\mu\)</formula> to a
                    text number <formula notation="tex" rend="inline">\(k\)</formula> indicates that
                        <formula notation="tex" rend="inline">\(v_{\mu}\)</formula> is a substring
                    of <formula notation="tex" rend="inline">\(A_k\)</formula>. <figure
                        xml:id="figure05">
                        <head><hi rend="bold">Figure 5:</hi> Text reuses in the Poems Corpus (top)
                            and Lyrics Corpus (bottom) - quasi maximal nodes and pointers to
                            poems/lyrics. This bird's eye perspective can be used to find
                            interesting regions for closer inspection using zooming
                            techniques.</head>
                        <embed src="images/figure05_1.pdf" width="100%" height="700"
                            type="application/pdf"/>
                        <embed src="images/figure05_2.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
                <p> Depending on the text reuses in the collection, the graph becomes very large.
                    Still it is very useful to find interesting subregions with eye-catching
                    multiple text reuses. In the lyrics corpus, many reuse effects can be traced
                    back to covered songs. Since the song texts in the corpus were collected in a
                    community based way, texts differ in details. In the survey graph eye-catching
                    regions are mainly caused by songs that have been covered many times. It is then
                    possible to zoom to these regions and to see reused text portions and the texts
                    where these findings occur. Figures <ref target="#figure06">6</ref>and <ref
                        target="#figure07">7</ref> show such zoomed regions. In <ref
                        target="#figure06">Figure 6</ref> it can be seen that five poems in the
                    collection have many text reuse connections. Some examples for reused text are
                    (cf. <ref target="#figure06">Figure 6</ref>) <list>
                        <item>
                            <q><hi rend="monospace">laut und leise, Unterric... träumt,
                                    Angenehme zu hören</hi></q>
                        </item>
                        <item><q><hi rend="monospace">wälzend kam die Sturmesf ... der Chor vom
                                    ganzen Haine</hi></q>
                        </item>
                    </list>
                    <figure xml:id="figure06">
                        <head><hi rend="bold">Figure 6:</hi>Text reuses in the Poems Corpus - quasi
                            maximal nodes and pointers to 5 poems, zoomed subregion of upper graph
                            in <ref target="#figure05">Figure 5</ref>.</head>
                        <graphic url="images/figure06.pdf"/>
                    </figure> In <ref target="#figure07">Figure 7</ref> many test reuses are
                    centered around the word <q>Hallelujah</q>. Examples are <list>
                        <item><q><hi rend="monospace">She tied you to a kitche .. Hallelujah,
                                    Hallelojah}</hi>
                            </q>
                        </item>
                        <item><q><hi rend="monospace">s seen the light It's a -- ah Hallelujah,
                                    Hallelujah}</hi>
                            </q>
                        </item>
                    </list>
                    <figure xml:id="figure07">
                        <head><hi rend="bold">Figure 7:</hi>Text reuses in the Lyrics Corpus - quasi
                            maximal nodes and pointers to lyrics, zoomed subregion of lower graph in
                                <ref target="#figure05">Figure 5</ref>.</head>
                        <graphic url="images/figure07.pdf"/>
                    </figure>
                </p>
            </div>

            <div xml:id="section05">
                <head>Refined <q>linguistic</q> view on a corpus </head>
                <p> We have seen that the set of nodes of the (S)CDAWG for a corpus represents a
                    relatively small set of infixes that often are <q>natural</q> portions
                    of text. Experiments in <ptr target="#gerdjikov2016"/> show that many of the
                    phrases corpus analyzers would look at (names, terminological expressions,
                    stylistic phrases, ...) are typically left- and right-closed and appear among
                    the nodes of the (S)CDAWG. This means that when just using the index nodes for
                    corpus exploration we do not miss important portions of text. </p>
                <p>From this point, the (S)CDAWG can be considered as a first step towards a new
                    kind of <q>linguistic corpus index</q> where nodes - in the ideal case -
                        <emph> exactly</emph> represent the linguistic units of interest (morphems,
                    words, phrases, sentences, ...) of the corpus and their relationships. In a way,
                    such an index would be the optimal basis for all corpus analysis tasks focussed
                    on linguistic units <note>More generally, the ideal index structure for corpus
                        analyzers would point to all and exactly those infixes that are relevant for
                        the analysis, their positions and their relationships.</note>. However, for
                    arbitrary texts there is not even a useful formal definition of a
                        <q>phrase</q> or <q>linguistic unit</q>, let alone a
                    procedure for finding this strings. </p>
                <p>There are two possible main paths how to come closer to a <q>linguistic
                        refinement</q> of the (S)CDAWG. On the one hand side, we can take any
                    available NLP prodecure (parser, lexical analyser, phrase detector,..). Assuming
                    that the units found represent nodes of the (S)CDAWG, we arrive at a (S)CDAWG
                    substructure that directly points to units of interest and their occurrences in
                    the corpus. On the other hand we may try to further analyze regularities found
                    in the corpus to find a restricted set of nodes that comes closer to the idea of
                    a linguistic phrase. In <ptr target="#gerdjikov2016"/> we followed the second
                    path, which is completely language independent. Crucial assumptions are: <list>
                        <item> phrases appear in distinct contexts, </item>
                        <item> phrases are combined using function words as connectors, and </item>
                        <item> sentences have natural decompositions into phrases, overlaps
                            representing function words. </item>
                    </list> We then describe a bootstrapping method for finding function words,
                    phrases and sentence decompositions into phrases.<note>Since we have a
                        symbol-based view on corpora, the expression function <q>word</q> is
                        misleading. The blank, as well as strings such as <q><hi
                                rend="monospace"> as well as </hi></q> may serve as a function
                            <q>word</q>.</note> Using the same kind of techniques, phrases
                    can be further decomposed into {\em sub(sub)phrases}. As a matter of fact, for
                    this form of decomposition also given lists of function words could be used. The
                    main point is that the bootstrapping method in a completely unsupervised and
                    language independent way leads to an interesting set of phrases and subphrases
                    that helps to considerably reduce the set of nodes considered, thus coming
                    closer to a <q>linguistic</q> index. </p>
                <p><hi rend="bold"> Finding important concepts of the corpus.</hi> When ignoring
                    bordering function words, <emph>atomic subphrases</emph> obtained in this way
                    typically represent content words or <q>concepts</q>. Analyzing the role
                    of these concepts in phrase decomposition it is possible to compute a ranked
                    list of characteristic concepts of the corpus. See <ptr target="#gerdjikov2016"
                    /> for details. Below some examples for <emph>most characteristic atomic
                        phrases</emph> in the example corpora are given. For example, in the
                    Wittgenstein Corpus, central concepts found using the above methods are (cf.
                        <ref target="#figure09">Figure 9</ref>) <q><hi rend="monospace"
                            >Bedeutung</hi></q> (meaning), <q><hi rend="monospace"
                            >Farbe</hi></q>(colour), <q><hi rend="monospace"
                        >Sprache</hi></q> (language), and <q><hi rend="monospace"
                            >Erklärung</hi></q>quote> (explanation). </p>
                <p>
                    <hi rend="bold"> Exploring the use of concepts.</hi> After computing
                    characteristic concepts, the aforementioned decomposition of phrases helps to
                    find all larger phrases where a given concept occurs. In this way the use of the
                    concept in the corpus can be studied. <ref target="#figure08">Figure 8</ref>
                    shows the hierarchical structure of phrases of the Wittgenstein Corpus extending
                            <q><hi rend="monospace">Bedeutung</hi></q>. <figure
                        xml:id="figure08">
                        <head><hi rend="bold">Figure 8: Exploring the use of concepts</hi>. The
                            hierarchical structure of phrases extending the concept <hi
                                rend="monospace">Bedeutung</hi> in the Wittgenstein Corpus detected
                            by our approach.</head>
                        <embed src="images/figure08.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
                <p>
                    <hi rend="bold"> Exploring the relationship of concepts in a visual way.</hi> A
                    third goal is to explore interesting relationships between the concepts.
                    Existing corpus exploration tools and automated approaches for exploring the
                    paradigmatics of lexical units typically look at the co-occurrence of terms in
                    documents, paragraphs, sentences or fixed size neighbourhoods <ptr
                        target="#schütze1993"/>,<ptr target="#storjohann2010"/>. Another, more
                        <q>syntactic</q> view is obtained when looking at the co-occurrence
                    of concepts in <emph>phrases</emph>. In Figures <ref target="#figure9">9</ref>
                    and <ref target="#figure10">10</ref> we see the relationship of concepts
                    (characteristic atomic kernels) in terms of co-occurrences in phrases. For
                    example, in the Wittgenstein Corpus, <q><hi rend="monospace"
                        >Bedeutung</hi></q> (meaning) is strongly related to <q><hi
                            rend="monospace">Erklärung</hi></q> (explanation), as can be seen
                    from phrases like <q><hi rend="monospace">Erklärung der Bedeutung eines
                            Worts</hi></q> (explanation of the meaning of a word). In the
                    Medline Corpus (cf. <ref target="#figure10">Figure 10</ref>), concepts found to
                    be related to <q><hi rend="monospace">tumor</hi></q> are, e.g.,
                            <q><hi rend="monospace">risk</hi></q> (witness phrase <q><hi
                            rend="monospace">risk for tumor</hi></q>), <q><hi
                            rend="monospace">chemotherapy</hi></q> (witness phrase <q><hi
                            rend="monospace">tumor cells to chemotherapy</hi></q>), and
                            <q><hi rend="monospace">vaccines</hi></q> (witness phrase
                            <q><hi rend="monospace">tumor vaccines</hi></q>). <figure
                        xml:id="figure09">
                        <head><hi rend="bold">Figure 9: Exploring the relationship of concepts in a
                                visual way</hi>. Network with <q>witness phrases</q> for the
                            relationship between concepts derived from Wittgenstein Corpus.</head>
                        <embed src="images/figure09.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                    <figure xml:id="figure10">
                        <head><hi rend="bold">Figure 10: Exploring the relationship of concepts in a
                                visual way</hi>. Network with <q>witness phrases</q> for the
                            relationship between concepts derived from the Medline.</head>
                        <embed src="images/figure10.eps" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
            </div>
            <div xml:id="section06">
                <head>Corpus exploration using metadata</head>
                <p> When metadata are available, many of the aforementioned techniques can be
                    refined for revealing similarities in the space of metadata. Technically, we
                    assign the metadata for the original documents to the nodes in the SCDAWG that
                    represent those documents. For the rest of the nodes we can retrieve this
                    information on demand by simple traversal of small parts of the SCDAWG. </p>
                <p>
                    <hi rend="bold"> Comparing authors.</hi> Our first illustration uses the Poems
                    Corpus, where we have authorship and temporal metadata for texts. In <ref
                        target="#figure11">Figure 11</ref> we use the aforementioned refined
                        <q>linguistic</q> view. After adding authorship information to the
                    maximal nodes of the index we first adapt the notion of a quasi maximal node: a
                    node <formula notation="tex" rend="inline">\(\mu\)</formula> with text <formula
                        notation="tex" rend="inline">\(v_{\mu}\)</formula> is called {\em quasi
                    maximal with respect to authors} if <formula notation="tex" rend="inline"
                        >\(v_{\mu}\)</formula> occurs in texts of two distinct authors, while any
                    phrase that properly extends <formula notation="tex" rend="inline"
                        >\(v_{\mu}\)</formula> only occurs in the works of one author. </p>

                <p> The nodes in <ref target="#figure11">Figure 11</ref> represent important
                    concepts in the corpus - some of the most characteristic atomic subphrases for
                    the Poems Corpus in the sense considered above. Our language independent
                    techniques revealed that <q><hi rend="monospace">Welt</hi></q> (world),
                            <q><hi rend="monospace">Sonne</hi></q> (sun), <q><hi
                            rend="monospace">Himmel</hi></q> (sky/heaven), <q><hi
                            rend="monospace">Gold</hi></q> (gold), and <q><hi
                            rend="monospace">Schnee</hi></q> (snow) are important concepts in
                    the Poems Corpus. In <ref target="#figure11">Figure 11</ref>, each link between
                    two concepts stands for a phrase that <list>
                        <item> contains both concepts, and </item>
                        <item> is quasi maximal with respect to authors. </item>
                    </list> The authors that have used the phrase are annotated with the link. For
                    example it is seen that <list>
                        <item>
                            <q><hi rend="monospace">Schnee zur Erde</hi></q> (snow to earth)
                            was both used by Nikolaus Lenau and Hermann Löns, and </item>
                        <item>
                            <q><hi rend="monospace">der Himmel mit der Erde</hi></q> (the
                            heaven/sky with the earth) was both used by Betty Paoli and Friedrich
                            Rückert</item>. </list> This gives a basis for comparing two authors,
                    now asking if both authors used similar (quasi-maximal) phrases with the central
                    concepts of the corpus. <figure xml:id="figure11">
                        <head><hi rend="bold">Figure 11: Use of Metadata</hi>. Connecting concepts
                            (nodes) by means of phrases that are maximal with respect to the
                            property of being used by two poets. Edges represent phrases plus the
                            two poets that used the phrase.</head>
                        <embed src="images/figure11.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
                <p>
                    <hi rend="bold">Temporal development of similarities between authors.</hi> In
                        <ref target="#figure12">Figure 12</ref> we see a dual variant of the graph
                    where the role of nodes and edges is changed. Nodes now represent authors,
                    authors are linked if they have used identical quasi maximal (w.r.t authors)
                    phrases. Using colouring of nodes, also temporal information about authors is
                    added - we simply used the year defining the middle point in the life of an
                    author. The spectral ordering of colours red-green-blue-purple represents the
                    temporal timeline from earlier periods to later periods. Links always point from
                    earlier authors to later authors. </p>
                <p> In the lower part of the graph we find a path from Anna Louisa Karsch (1756)
                        <ref target="#figure12">Figure 12</ref>, circle marker (1)) <formula
                        notation="tex" rend="inline">\(\longrightarrow\)</formula> Friedrich
                    Gottlieb Klopstock (1763) <formula notation="tex" rend="inline"
                        >\(\longrightarrow\)</formula> Ernst Schulze (1803) <ref target="#figure12"
                        >Figure 12</ref>, circle marker (2)). The latter is also reached with a link
                    from Johann Wolfgang von Goethe (1794) <ref target="#figure12">Figure 12</ref>,
                    circle marker (3)). Goethe has several successors. In this graph, an even more
                    central person is Ludwig Achim von Arnim (1806) <ref target="#figure12">Figure
                        12</ref>, circle marker (4)), who builds the center of the large cluster in
                    the middle of the figure. The temporal spectrum of this cluster corresponds to
                    the first half of the 19th century. Authors of the 17th and 18th century are
                    found in the second cluster on the top right part. An early
                        <q>influential</q> author of this period is Simon Dach (1632) <ref
                        target="#figure12">Figure 12</ref>, circle marker (5)). </p>
                <p> The two clusters indicate a change of literary concepts covered between the
                    Baroque/Pre-Romantic period and the Romantic period. The three authors (John
                    Brinckman (1842), Fritz Reuter (1842), and Klaus Groth (1859) <ref
                        target="#figure12">Figure 12</ref>, circle marker (6)), who form the cluster
                    right to Romantic period cluster however are not connected to the cluster
                    consisting of other authors of their period due to the fact that all three were
                    writing poems in the german dialect of niederdeutsch. <figure xml:id="figure12">
                        <head><hi rend="bold">Figure 12: Use of Metadata in the Poems Corpus.</hi>
                            The graph is obtained as the dual variant of the graph in <ref
                                target="figure11">Figure11</ref>. Links between two poets mean that
                            both have used similar maximal phrases around the concepts shown in <ref
                                target="figure11">Figure11</ref>. Colours correspond to temporal
                            information for authors. Links point from earlier authors to later
                            authors.</head>
                        <embed src="images/figure12.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
                <p> When constructing this kind of graph for the Lyrics Corpus similar observations
                    can be made. The nodes of <ref target="#figure13">Figure 13</ref> again are
                    connected by co-occurring quasi maximal phrases. Instead of the author now band
                    names are used together with the mean of the release years of their
                    corresponding songs. The colour coding is the same as used in the previous
                    experiment. One of the first things that can be observed is a cluster of Italian
                    singers centered around Eros Ramazotti (1998) (See <ref target="#figure13"
                        >Figure 13</ref>, circle marker (1)), located at the right to the center of
                    the graph. Also lots of subgraphs, which only contain of two or three
                    interpreters are seen. They often vizualize relations between cover songs, which
                    are contained in the corpus. Therefore for instance Right Said Fred (1998) and
                    Peter Sarstedt (1969) (See <ref target="#figure13">Figure 13</ref>, circle
                    marker (2)) can be found at the bottom left. Another example of this is a three
                    node cluster at the top right with edges leading from Julie Covington (1977) and
                    Don McLean (1972) to Madonna (1996) (See <ref target="#figure13">Figure
                    13</ref>, circle marker (3)), who famously released covers of the songs
                        <q>Don't Cry For Me Argentina</q> and <q>American Pie</q>.
                    Other subgraphs connect interpreters who obviously belong to the same genre,
                    e.g. at little to the right of the center a path can be found leading from 2Pac
                    (2000) <formula notation="tex" rend="inline">\(\longrightarrow\)</formula> Busta
                    Rhymes (2001) <formula notation="tex" rend="inline"
                        >\(\longrightarrow\)</formula> Kanye West (2007) (See <ref
                        target="#figure13">Figure 13</ref>, circle marker (4)), with the
                    interpreters representing famous hip-hop artists. <figure xml:id="figure13">
                        <head><hi rend="bold">Figure 13: Use of Metadata in the Lyrics Corpus.</hi>
                            The graph is obtained by connecting interpreters by their commonly used
                            phrases. Node colours again encode temporal metadata. Links point from
                            older interpreters to younger ones.</head>
                        <embed src="images/figure13.pdf" width="100%" height="700"
                            type="application/pdf"/>
                    </figure>
                </p>
            </div>

            <div xml:id="section07">
                <head>Loose ends - diachronic language variation and classification tasks</head>
                <p> Even though the focus of the current paper are the problems of text-reuse and
                    mining of relationships based on it, we suggest that our approach can be
                    extended to other related problems, e.g. diachronic language variation and text
                    classification. </p>
                <p> In <ptr target="#gerdjikov2013"/>,<ptr target="#sariev2014"/>}, the authors
                    develop a historical text normalization system. Based on a modern dictionary and
                    several thousands of pairs, historical word and its modern variant, they show
                    that the DAWG structure can be used to automatically learn and extract spelling
                    variations. The main benefit of the infix structure is that: (i) it does not
                    restrict the spelling variation to any particular predefined patterns and (ii)
                    it reflects the entire structure of the language (on word level). In <ptr
                        target="#sariev2014"/> the combination of this technique with a language
                    model for the modern language yields a complete historical text normalization
                    system. </p>
                <p> Of course, the requirement for training data sets practical limitations --
                    training data is often unavailable and its production is expensive and
                    time-consuming. In <ptr target="#mitankin2014"/> the authors try to address this
                    problem and suggest a completely automatic historical text normalization
                    approach. The main idea is to automatically generate pairs of historical word
                    and its modern variant. <ptr target="#mitankin2014"/> proposes to use
                    Levenshtein edit distance and approximate search in the modern dictionary in
                    order to generate for each historical word its most relevant modern variant(s).
                    Afterwards we are in the situation to run the historical text normalization
                    system from <ptr target="#mitankin2014"/> and<ptr target="#sariev2014"/>.
                    Unfortunately, the Levenshtein edit distance, as a generator of pairs, turns out
                    to introduce a lot of noise in the system. In particular, the accuracy of the
                    normalization system drops from 94% to 82%. </p>
                <p> The results from the current paper suggest two straightforward approaches in
                    order to reduce the noise introduced by the unsupervised approximate search. The
                    first approach is the following. Instead of searching for <emph>historical
                        words</emph> and their <emph>modern word variants</emph>, to search for
                        <emph>historical phrases</emph> and their <emph>modern phrase
                        variants</emph>, automatically extracted from the SCDAWG structure for the
                    historical texts and modern texts, respectively. In this way we plug in
                    additional language context in the query, which in general will exclude
                    orthographically similar but semantically irrelevant candidates. On the other
                    hand, if the modern text corpus covers the domain of the historical texts, it is
                    possible that the basic phrases in the historic language have indeed been
                    preserved in the modern language and do have their spelling variants in the
                    modern language. Thus, without seriously reducing the recall, we could increase
                    the quality of the automatically generated pairs <emph>historical phrases</emph>
                    and their <emph>modern phrase var</emph>. The production of pairs of words is
                    then straightforward<note>Actually, the approach in<ptr target="#gerdjikov2013"
                        /> is not limited to words.</note>. </p>
                <p> The second approach is the following. Instead of modifying the searching space,
                    we can modify the edit-distance and use more relevant edit-distance mechanism.
                    As described in <ref target="#section04">Section 4</ref>, we can use the SCDAWG
                    structure in order to align different editions of the same source. In
                    particular, if the editions belong to close time periods, the most disagreements
                    in the alignment will be due to uncertainties in the spelling that have been
                    typical in this time period. This phenomenon can be used in order to constrain
                    the elementary edit-operations and the contexts in which they are applicable. As
                    a result we can expect that the generator of pairs, historic word and its modern
                    variant, will be more relevant with respect to the structure of the historical
                    language. </p>
                <p> We should stress that the arguments raised in the previous two paragraphs
                    require further research. It is also a challenging open problem to directly map
                    the SCDAWG structure of a historical corpus to a substructure of the SCDAWG
                    structure for a large modern language. The latter, of course, would resolve the
                    normalization problem. </p>
                <p> The substrings of a SCDAWG index could also be used to train models for text
                    classification tasks. A similiar approach, which uses maximal substrings of a
                    suffix tree and led to promising results, had been persued in <ptr
                        target="#okanohara2009"/>. As stated in <ref target="#section02">Section
                        2</ref> the two-sided closures, which form the nodes of the SCDAWG,
                    represent more natural infixes than those of a suffix tree or DAWG. Therefore
                    they may render descriptive features which still are not restricted to fixed
                    word boundaries. To find optimal features in the substrings of a SCDAWG of a
                    corpus again metadata can be used to find longest or shortest substrings which
                    occur only in a certain instance of a metadata attribute. For example the
                    longest/shortest substrings which belong to the author <q>Goethe</q> can
                    be used as features to train a text classification model with regard of this
                    metadata attribute.</p>
            </div>
            <div xml:id="section08">
                <head>Conclusion</head>
                <p> In this paper we have shown how symmetric compact acyclic word graphs (SCDAWGs)
                    can help to efficiently mine interesting portions of texts in corpora, which
                    serve as a basis for many ways of comparing texts like alignment and the
                    detection of text reuse (<ref target="#section04">Section 4</ref>). These
                    findings can be refined by adding a form of <q>linguistic analysis</q>
                    where <q>phrases</q>, <q>subphrases</q>, <q>function
                        words</q>, and <q>important content words</q> (characteristic
                    atomic kernels) are computed in an unsupervised way without using prior
                    linguistic knowledge (<ref target="#section05">Section 5</ref>). If metadata for
                    the texts in the collection are available, additional information stored in the
                    index and the above techniques give a basis for finding similarities in the
                    space of metadata (<ref target="#section06">Section 6</ref>).</p>
                <p> Of course many of the here mentioned techniques would require more detailed
                    analysis and comparsion with other existing methods. However this was not the
                    purpose of this paper, since its focus didn't lie on the development and
                    description of a certain method but rather on showing how manifold and versatile
                    the application of such an index can be to the task of large scale corpus
                    exploration. In many cases, various variants exist for the concrete methods used
                    in our examples. When moving to real applications, experts from Digital
                    Humanities are needed to find those variants that are most adequate and lead to
                    real insights. </p>
            </div>
            <div xml:id="section09">
                <head>Acknowledgements</head>
                <p>We express our gratitude to: Uwe Springmann for his help with OCR recognition of
                    historic documents and the corpus derived from them. We also thank Stefanie
                    Schneider for the provision of the lyrics corpus. Part of the results described
                    in this paper were obtained during the stay of the third author at LMU for which
                    he has received funding from the People Programme (Marie Curie Actions) of the
                    European Union's Seventh Framework Programme (FP7/2007--2013) under REA grant
                    agreement 625160.</p>

            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="blumer1987" label="Blumer et al. 1987">Blumer, A., Blumer, J.,
                    Haussler, D., McConnell, R., and Ehrenfeucht, A.<title rend="quotes">Complete
                        inverted files for efficient text retrieval and analysis</title>. <title
                        rend="italic">Journal of the ACM (JACM)</title> 34 (1987): 578-595.</bibl>

                <bibl xml:id="büchler2012" label="Büchler et al. 2012">Büchler, M., Crane, G.,
                    Moritz, M., and Babeu, A. <title rend="quotes"> Increasing recall for text
                        re-use in historical documents to support research in the humanities
                    </title>. In <title rend="italic">Proceedings of 16th International Conference
                        on Theory and Practice of Digital Libraries, (tpdl 2012): </title> pp.
                    95–100 <title rend="italic">Springer Berlin Heidelberg</title>.</bibl>

                <bibl xml:id="gerdjikov2013" label="Gerdjikov et al. 2013">Gerdjikov, S. and Mihov,
                    S. and Nenchev, V. <title rend="quotes">Extraction of spelling variations from
                        language structure for noisy text correction</title>. In <title
                        rend="italic">Proc. Int. Conf. for Document Analysis and Recognition</title>
                    (2013): 324-328.</bibl>

                <bibl xml:id="gerdjikov2016" label="Gerdjikov and Schulz 2016">Gerdjikov, S., and
                    Schulz, K. U. <title rend="quotes">Corpus analysis without prior linguistic
                        knowledge-unsupervised mining of phrases and subphrase structure</title>.
                        <title rend="italic">ArXiv e-prints</title> (2016): 1602.05772.</bibl>

                <bibl xml:id="gusfield97" label="Gusfield 1997">Gusfield, D. <title rend="italic"
                        >Algorithms on strings, trees and sequences: computer science and
                        computational biology</title>. Cambridge university press, Cambridge,
                    1997.</bibl>

                <bibl xml:id="inenaga2005" label="Inenaga et al. 2005">Inenaga, S., Hoshino, H.,
                    Shinohara, A., Takeda, M., Arikawa, S., Mauri, G., and Pavesi, G.<title
                        rend="quotes">On-line construction of compact directed acyclic word
                        graphs</title>. <title rend="italic">Discrete Applied Mathematics</title>
                    146 (2005): 156-179. </bibl>

                <bibl xml:id="mccreight1976" label="McCreight 1976">McCreight, Edward M.<title
                        rend="quotes">A space-economical suffix tree construction algorithm</title>.
                        <title rend="italic">Journal of the ACM (JACM)</title> 23 (1976): 262-272. </bibl>

                <bibl xml:id="mitankin2014" label="Mitankin et al. 2014">Mitankin, P. and Gerdjikov,
                    S. and Mihov, S. <title rend="quotes"> An approach to unsupervised historical
                        text normalization </title>. In <title rend="italic">Proceedings of the
                        First International Conference on Digital Access to Textual Cultural
                        Heritage: </title> (2014) 29-34.</bibl>

                <bibl xml:id="needleman1970" label="Needleman and Wunsch 1970">Needleman, S. B., and
                    Wunsch, C. D.<title rend="quotes">A general method applicable to the search for
                        similarities in the amino acid sequence of two proteins.</title>. <title
                        rend="italic">Journal of molecular biology</title> 48 (1970): 443-453. </bibl>


                <bibl xml:id="okanohara2009" label="Okanohara and Tsujii 2009">Okanohara, Daisuke
                    and Tsujii, Jun'ichi<title rend="quotes">Text categorization with all substring
                        features</title>. In <title rend="italic">Proceedings of the 2009 SIAM
                        International Conference on Data Mining: </title> (2009) 839-846.</bibl>

                <bibl xml:id="sariev2014" label="Sariev et al. 2014">Sariev, Andrei and Nenchev,
                    Vladislav and Gerdjikov, Stefan and Mitankin, Petar and Ganchev, Hristo and
                    Mihov, Stoyan and Tinchev, Tinko<title rend="quotes">Flexible noisy text
                        correction</title>. In <title rend="italic">Proceedings of Document Analysis
                        Systems: </title> (2014)</bibl>

                <bibl xml:id="schütze1993" label="Schütze and Pedersen 1993">Schütze, H., and
                    Pedersen, J.<title rend="quotes"> A vector model for syntagmatic and
                        paradigmatic relatedness </title>. In <title rend="italic">Proceedings of
                        the 9th Annual Conference of the UW Centre for the New OED and Text
                        Research, (oed 1993): </title> pp. 104–113.</bibl>

                <bibl xml:id="storjohann2010" label="Storjohann 2010">Storjohann, P. (Ed.) <title
                        rend="italic">Lexical-semantic relations: theoretical and practical
                        perspectives (Vol. 28)</title>. John Benjamins Publishing, Cambridge,
                    2010.</bibl>

                <bibl xml:id="ukkonen95" label="Ukkonen 1995">Ukkonen, Esko. <title rend="quotes"
                        >On-line construction of suffix trees</title>. <title rend="italic"
                        >Algorithmica</title>14 (1995): 249-260. </bibl>

                <bibl xml:id="weiner1973" label="Weiner 1973">Weiner, P. <title rend="quotes">Linear
                        pattern matching algorithms</title>. In <title rend="italic">Proceedings of
                        14th Annual Symposium on Switching and Automata Theory, (swat 1973):
                    </title> pp. 1–11 <title rend="italic">IEEE</title>.</bibl>


            </listBibl>

        </back>
    </text>
</TEI>