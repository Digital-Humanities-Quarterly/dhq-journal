<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet"/>
      <link href="../../common/css/dhq_screen.css"
            media="screen"
            type="text/css"
            rel="stylesheet"/>
      <link href="../../common/css/dhq_print.css"
            media="print"
            type="text/css"
            rel="stylesheet"/>
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
      <script src="../../common/js/javascriptLibrary.js" type="text/javascript"/>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br/>Volume  Number </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div>
                     <a href="/dhq/preview/index.html">Preview</a>
                     | 
                    <a rel="external"
                        href="/dhq/vol///&#xA;                    000168&#xA;                .xml">XML</a>

| 
		   Discuss
			(<a href="/dhq/vol///&#xA;                    000168&#xA;                /&#xA;                    000168&#xA;                .html#disqus_thread"
                        data-disqus-identifier="&#xA;                    000168&#xA;                ">
				Comments
			</a>)
                </div>
               </form>
            </div>
            <div class="DHQheader">
        
            
                
                <h1 class="articleTitle">Mining for the Meanings of a Murder: The
                    Impact of OCR Quality on the Use of Digitized Historical
                    Newspapers</h1>
                <div class="author">
                  <span style="color: grey">Carolyn Strange</span> &lt;<a href="mailto:carolyn_dot_strange_at_anu_dot_edu_dot_au"
                     onclick="javascript:window.location.href='mailto:'+deobfuscate('carolyn_dot_strange_at_anu_dot_edu_dot_au'); return false;"
                     onkeypress="javascript:window.location.href='mailto:'+deobfuscate('carolyn_dot_strange_at_anu_dot_edu_dot_au'); return false;">carolyn_dot_strange_at_anu_dot_edu_dot_au</a>&gt;, Australian National
                    University</div>
            
            

            
        
        
        
        
               <span xmlns=""
                     class="Z3988"
                     title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Mining%20for%20the%20Meanings%20of%20a%20Murder%3A%20The%20Impact%20of%20OCR%20Quality%20on%20the%20Use%20of%20Digitized%20Historical%20Newspapers&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Strange&amp;rft.aufirst=Carolyn&amp;rft.au=Carolyn%20Strange"> </span>
            </div>

            <div id="DHQtext">
        
               <div id="abstract">
                  <h2>Abstract</h2>
                  <p>
                Digital humanities research that requires the digitization of medium-scale, project-specific texts confronts a significant
                methodological and practical question: is labour-intensive
                cleaning of the Optical Character Recognition (OCR) output
                necessary to produce robust results through text mining
                analysis? This paper traces the steps taken in a collaborative
                research project that aimed to analyze newspaper coverage of a
                high-profile murder trial, which occurred in New York City in
                1873. A corpus of approximately one-half million words was
                produced by converting original print sources and image files
                into digital texts, which produced a substantial rate of
                OCR-generated errors. We then corrected the scans and added
                document-level genre metadata. This allowed us to evaluate the
                impact of our quality upgrade procedures when we tested for
                possible differences in word usage across two key phases in the
                trial's coverage using log likelihood ratio [<a class="ref" href="#dunning1993">Dunning 1993</a>]. The
                same tests were run on each dataset – the original OCR scans, a
                subset of OCR scans selected through the addition of genre
                metadata, and the metadata-enhanced scans corrected to 98%
                accuracy. Our results revealed that error correction is
                desirable but not essential. However, metadata to distinguish
                between different genres of trial coverage, obtained during the
                correction process, had a substantial impact. This was true both
                when investigating all words and when testing for a subset of
                "judgment words" we created to explore the murder’s emotive
                elements and its moral implications. Deeper analysis of this
                case, and others like it, will require more sophisticated text
                mining techniques to disambiguate word sense and context, which
                may be more sensitive to OCR-induced errors.
                </p>
               </div>
            
        
        
               <div class="div div0">
                  <h1 class="head">Introduction</h1>
                  <div class="counter">
                     <a href="#p1">1</a>
                  </div>
                  <div class="ptext" id="p1">Digitized historical newspapers have enriched scholars’ capacity
                to pose research questions about the past, but the quality of
                these texts is rarely ideal, due especially to OCR errors.
                Although scholarly concern about this problem in relation to the
                use of large, web-based historical resources is growing
                [<a class="ref" href="#hitchcock2013">Hitchcock 2013</a>], research that involves the digitization of
                smaller corpora of original print documents confronts the same
                challenge. The perennial question in digital humanities research
                – how accurate must digitized sources be to produce robust
                results – arose in the course of our attempt to interrogate the
                meanings of a highly publicized murder case prosecuted in New
                York in 1873. This paper traces how we addressed this challenge:
                firstly by searching for the techniques best suited to digitize
                and to interpret news coverage of the trial; and secondly, by
                applying a statistical tool to test for possible shifts in
                popular appraisals of the case. </div>
                  <div class="counter">
                     <a href="#p2">2</a>
                  </div>
                  <div class="ptext" id="p2">Our project transformed over several years, from an individual,
                archive-based inquiry into a text mining collaboration that
                required the conversion of original news accounts into
                searchable texts through OCR. Our initial scans had an error
                rate of 20%, which raised the prospect that text mining might
                not substantially enhance our capacity to analyze word usage in
                the case’s coverage. Consequently, we proceeded to reduce noise
                through manual correction of the scans and through the addition
                of genre metadata, thereby creating three datasets – the
                original OCR scans, a subset of the OCR scans selected through
                the addition of genre metadata, and the metadata-enhanced scans
                corrected to 98% accuracy. By analysing each dataset with the
                log likelihood ratio statistical tool, we determined that the
                labour-intensive work of cleaning the data modestly improved the
                reliability of our test – to establish whether or not popular
                judgment of the case altered after controversial evidence was
                introduced in the course of the murder trial. However, the
                addition of genre-related metadata proved to be considerably
                more significant. Most importantly, the digitization of the
                previously unsearchable primary sources did make it possible to
                pose a research question that could not have been answered
                persuasively using unsearchable texts. </div>
                  <div class="counter">
                     <a href="#p3">3</a>
                  </div>
                  <div class="ptext" id="p3">As Tim Hitchcock argues, the development of digital history into
                a discipline requires that we expose and evaluate the research
                processes that allow us to compose "subtle maps of meaning" from
                    piles of primary sources [<a class="ref" href="#hitchcock2013">Hitchcock 2013</a>, 20]. Accordingly, we
                begin with an account of the murder case in Section 1, and
                discuss how the digitization of its news coverage opened up new
                ways to mine its meanings. In Section 2, we discuss the nature
                of our corpus and the ways in which we produced machine-readable
                text using Adobe Photoshop Lightroom 3 and ABBYY FineReader 11.
                In Section 3, we outline the nature of the errors produced in
                the scanning processes, while Section 4 details the steps we
                took to correct them and to add genre metadata post OCR. Section
                5 explains how we used the log likelihood ratio tool to analyze
                word frequency and to test the use of judgment words in trial
                coverage, using our three different datasets. The results of our
                tests appear in Section 6, which is followed by a discussion of
                the possible future directions of text mining research based on
                small- to medium-sized corpora. We conclude that text mining can
                enrich historians’ capacity to analyze large bodies of text,
                even in the presence of OCR-induced errors. Supplementing
                digitized text with genre metadata permits a finer-grained and
                more reliable analysis of historical newspapers. Investing the
                time to produce clean data and metadata improves performance,
                and our study suggests that this is essential for more
                sophisticated analysis, such as language parsing. Finally, our
                project underlines the need for interdisciplinary teams to
                ensure the integrity of the digital tools used, as well as the
                reliability of their outputs’ interpretation. </div>
               </div>
              <div class="div div0">
                  <h1 class="head">From Historical Analysis to Digital
                    Historical Research</h1>
                  <div class="counter">
                     <a href="#p4">4</a>
                  </div>
                  <div class="ptext" id="p4">The Walworth murder project began in 2003 as a humanistic
                enterprise conducted by an historian of gender and criminal
                justice who read over four hundred newspaper accounts of the
                case on microfilm, which were reproduced by printing out hard
                copies of images. This body of unsearchable records was
                augmented as the number of digitized newspapers available
                through open-source and proprietary online databases grew
                exponentially over the 2000s, although many of those texts were
                unsearchable image files.
                    <a class="noteRef" href="#d3e184">[1]</a>
             A research grant made it possible in 2012 to digitize the
                entire body of primary sources (the paper-based prints and pdf
                images of newspapers) through OCR scans.
                        <a class="noteRef" href="#d3e202">[2]</a>.
               This funding meant that hypotheses developed in the course
                of the historian’s earlier close reading of the case could be
                tested in a collaboration that included two hired computer
                scientists and a digital humanities scholar.</div>
                  <div class="counter">
                     <a href="#p5">5</a>
                  </div>
                  <div class="ptext" id="p5">The Walworth case’s extensive and sensational newspaper coverage
                indicated that the murder of Mansfield Walworth, a second-rate
                novelist and third-rate family man, stirred deep feelings,
                particularly because the killer was his son, Frank. The murder
                provoked troubling questions: Was it legally or morally
                excusable for a son to kill his father, no matter how
                despicable? And why, in a family filled with lawyers and judges
                (including the murdered man’s father, Judge Reuben Hyde
                Walworth) had the law not provided a remedy [<a class="ref" href="#obrien2010">O'Brien 2010</a>]? The
                event occurred on 3 June 1873, when Frank Walworth, a youth of
                nineteen, travelled to Manhattan to confront his father, who was
                recently divorced from his mother. Mansfield Walworth had sent a
                raft of letters to his ex-wife, full of murderous threats and
                mad ravings. After intercepting these alarming letters, Frank
                Walworth shot his father dead, then informed the police that he
                had done so to save himself, his mother and his siblings. Was
                the shooter an honourable son? A maudlin youth? Insane?
                Speculation swirled but the initial response to the murder was
                one of shock: a refined young man from a highly respectable
                white family had committed cold-blooded murder [<a class="ref" href="#obrien2010">O'Brien 2010</a>].
                Scores of headlines announced that this was no ordinary murder
                but a "PARRICIDAL TRAGEDY."</div>
                  <div class="counter">
                     <a href="#p6">6</a>
                  </div>
                  <div class="ptext" id="p6">Our working hypothesis was that popular readings of the Walworth
                murder changed as the trial progressed – from initial horror
                over the crime of parricide, to an appreciation of domestic
                cruelty and the menacing nature of the victim. The trial
                resembled a real-life domestic melodrama [<a class="ref" href="#powell2004">Powell 2004</a>], and its
                turning point occurred when the victim’s vile letters to his
                ex-wife were read into evidence, as the defence attempted to
                verify the threat Mansfield Walworth had presented to his son
                and ex-wife. At this point in the trial, the dead man’s profane
                abuse was recorded by trial reporters for the nation to read: 
                <blockquote>
                        <p>
            You have blasted my heart and think now as you always thought
                that you could rob me of the sweet faces of my children and then
                gradually after a year or two rob me of my little inheritance.
                You will see, you God damned bitch of hell, I have always
                intended to murder you as a breaker of my heart. God damn you,
                you will die and my poor broken heart will lie dead across your
                God damned body. Hiss, hiss, I'm after you… I will kill you on
                sight.</p>
                     </blockquote>
                  </div>
                  <div class="counter">
                     <a href="#p7">7</a>
                  </div>
                  <div class="ptext" id="p7">What was the impact of this obscene evidence on the public’s
                judgment of the case? Did it change over the course of the
                trial, and if so, how? These were research questions best
                addressed through the mining of the newspaper coverage, a
                substantial corpus beyond the capacity of human assessment.</div>
                  <div class="counter">
                     <a href="#p8">8</a>
                  </div>
                  <div class="ptext" id="p8">Although the volume of the Walworth case’s news coverage was
                modest compared to large-scale, institutionally funded text
                mining projects, the standards set in several benchmark
                historical newspaper text-mining research projects informed our
                approach. Many, such as Mining the <cite class="title italic">Dispatch</cite>,<a class="noteRef" href="#d3e252">[3]</a>
use manually double-keyed documentation, followed by
                comparison-based correction, to produce datasets with 98+%
                accuracy. Mining the <cite class="title italic">Dispatch</cite> used a
                large corpus of nineteenth-century U.S. newspapers to 
                    "explore –
                        and encourage exploration of – the dramatic and often traumatic
                        changes as well as the sometimes surprising continuities in the
                        social and political life of Civil War Richmond."
              </div>
                    <div class="counter">
                     <a href="#p9">9</a>
                  </div>
                  <div class="ptext" id="p9">
              That project combined distant and close reading of every
                issue of one newspaper (112,000 texts totalling almost 24
                million words) "to uncover categories and discover patterns in
                    and among texts."  [<a class="ref" href="#nelson2010">Nelson 2010</a>]
                    As its director explained, high
                accuracy levels were necessary to combine text mining with
                historical interpretation most productively: "the challenge is
                    to toggle between distant and close readings; not to rely
                    solely on topic modelling and visualizations." [<a class="ref" href="#nelson2010">Nelson 2010</a>] Studies
                that pursue similar objectives must first determine the best
                methods to produce machine-readable text. The next section
                details the scanning process preliminary to our analysis.</div>
               </div>
               <div class="div div0">
                  <h1 class="head">The Production of Machine-searchable
                    Texts </h1>
                  <div class="counter">
                     <a href="#p10">10</a>
                  </div>
                  <div class="ptext" id="p10">Using a variety of sources, including photocopied prints of
                microfilmed newspapers and pdf image files of stories sourced
                from several databases, we gleaned 600 pages, comprising
                approximately 500,000 words of digitized text. <a href="#figure01">Figure 1</a> shows an
                example article ready for scanning.</div>
                  <div class="counter">
                     <a href="#p11">11</a>
                  </div>
                  <div class="ptext" id="p11">Since optimal machine-readable text was integral to our analysis
                of the murder trial’s meanings, we reviewed the quality control
                methods used by ten large public and private institutions, from
                scanning hardware and software through to a diverse array of OCR
                software.<a class="noteRef" href="#d3e299">[4]</a>
                 We intended to use non-proprietary software,<a class="noteRef" href="#d3e302">[5]</a>
               but we ultimately selected ABBYY FineReader 11, since it
                allows for the customisation of features.<a class="noteRef" href="#d3e308">[6]</a>
                The first source of text was photocopied pages printed in
                2003 from microfilmed newspaper images. Since online
                repositories of newspapers have subsequently become more
                numerous, we were able to replace the poor quality text in
                microfilm scans with pdf image files of the photocopies.
                However, this strategy proved to be too time-consuming, since
                each of these page-scan pdfs contained upwards of 60 paragraphs
                of text (an average of 5,000 words in small print), spread
                across upwards of 8 columns, which required laborious searches
                for references to the Walworth murder trial. Consequently, this
                replacement strategy was used only for the worst 10% of the
                photocopies. </div>
                  <div class="counter">
                     <a href="#p12">12</a>
                  </div>
                  <div class="ptext" id="p12">All the scanned files viable to use from the existing scanned
                microfilm were imported into Adobe Photoshop Lightroom 3. This
                process involved batch scanning pages in black and white into
                300DPI TIFFs according to newspaper, and then placing them in
                physical folders by newspaper, in the same order in which they
                had been scanned, to facilitate cross-referencing of particular
                pages with their corresponding files. Within Lightroom each file
                was then manually cropped one-by-one to select only those
                columns related to the Walworth murder.<a class="noteRef" href="#d3e317">[7]</a>
Although Lightroom is designed for working with large
                catalogues of photographs rather than "photographs of text", it
                allowed us to batch process select images iteratively through
                non-destructive image editing, so that tests could be made to
                determine which combination of image processing was likely to
                achieve the highest OCR accuracy. </div>
                  <div id="figure01" class="figure">
                
                
                     <div class="ptext">
                        <a href="./resources/images/figure01.png" rel="external">
                           <img src="./resources/images/figure01.png"
                                alt="An example of a newspaper article image that shows&#xA;                    details of image degeneration alongside a transcript"/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 1. </div>An example of one of the newspaper
                articles on the Walworth case (from the <cite class="title italic">New York
                    Herald</cite>, 10 June 1873), showing average
                level image degradation. The OCR output appears to the
                right, with errors highlighted in bold.</div>
                  </div>         
                  <div class="counter">
                     <a href="#p13">13</a>
                  </div>
                  <div class="ptext" id="p13">Training software to recognize patterns of font and content is
                one of the most challenging aspects of OCR, as it draws on
                Artificial Intelligence to "recognize" multitudes of shapes as
                belonging to corresponding letters. Our project revealed that
                ABBYY FineReader’s training capacity is limited. After we
                exported files produced through Lightroom, newspaper by
                    newspaper,<a class="noteRef" href="#d3e350">[8]</a>
        we trained ABBYY to recognize each newspaper’s fonts, and
           each file was further "cleaned up" by straightening text lines
                and by correcting for perspective distortion. Although ABBYY
                appeared to "recognize" frequently occurring words, like the
                surname Walworth, the OCRd results produced variations, such as
           "Wolwarth" and "Warworth". It became evident that ABBYY cannot
                recognize that all such variations of "Walworth" should have
                been converted automatically to "Walworth", given the high
                statistical likelihood that they were in fact "Walworth".</div>
                  <div class="counter">
                     <a href="#p14">14</a>
                  </div>
                  <div class="ptext" id="p14">Our process exposed the sorts of image degradation common in the
                digitization of historical newspapers, including: smudged, faded
                and warped text; ripped or crumpled originals; image bleed from
                the reverse side of the paper; crooked and curved text lines;
                and overexposed and underexposed microfilm scans. As a result,
                ABBYY’s deficiencies required that customized automated
                corrections be applied in the post-OCR phase of our project.</div>
                  <div class="counter">
                     <a href="#p15">15</a>
                  </div>
                  <div class="ptext" id="p15">Nevertheless, the production of machine-readable text resulted in
                a uniform dataset of newspaper articles that offers considerable
                granularity, including the capacity to analyze a corpus of
                articles on the Walworth murder trial according to date, a
                critical factor in our study, considering the admission of
                Mansfield Walworth’s extraordinary letters into evidence. The
                coverage of the corpus, disaggregated by newspaper, is shown in
                Figure 2.</div>
                  <div class="figure">
                
                     <div class="ptext">
                        <a href="./resources/images/figure02.png" rel="external">
                           <img src="./resources/images/figure02.png"
                                alt="Bar graph showing word counts per newspaper"/>
                        </a>
                     </div>
               
                     <div class="caption">
                        <div class="label">Figure 2. </div>Word counts per newspaper,
                    original text from OCR scans (left) and cleaned text
                    (right). The correction process is discussed in Section
                    3.</div>
                  </div>
               </div>
               <div class="div div0">
                  <h1 class="head">OCR Errors and Quality Control for
                    Text Mining</h1>  
                  <div class="counter">
                     <a href="#p16">16</a>
                  </div>
                  <div class="ptext" id="p16">The variable quality of digitized historical newspapers has long
                been a challenge for digital scholarship [<a class="ref" href="#arlitsch2004">Arlitsch2004</a>] Much of that variability is associated with the
                historical and contemporary resources of publishing houses,
                meaning that major metropolitan papers typically sit at one end
                of the legibility spectrum and smaller, regional papers sit at
                the other. In our study, articles from the <cite class="title italic">New
                    York Times</cite> yielded accuracy levels of 94.5%, and they
                are obtainable through the paper’s own search engine.
                Furthermore, articles in the <cite class="title italic">Times</cite>
                repository are cropped and cleaned, which means they are ready
                for OCRing with minimal image manipulation. In contrast, OCR
                scans from an important upstate New York newspaper based in the
                state capital, the <cite class="title italic">Albany Argus,</cite> yielded
                results of only 65% accuracy. Unfortunately, due to the
                substantial additional labour required to raise this and other
                smaller papers’ level of accuracy, these scans were mostly too
                poor to incorporate. Thus the variation in file quality from
                different newspapers presented a limitation on the project’s
                initial ambitions. More broadly, this problem flags the
                significant impact that OCR quality can make in the range of
                sources used for text mining. OCR errors are part of a wider
                problem of dealing with "noise" in text mining [<a class="ref" href="#knoblock2007">Knoblock 2007</a>], which may also stem from other sources such as historical
                spelling variations or language specific to different media
                texts. </div>
                  <div class="counter">
                     <a href="#p17">17</a>
                  </div>
                  <div class="ptext" id="p17">The impact of OCR errors varies depending on the task performed,
                however [<a class="ref" href="#eder2013">Eder 2013</a>]. The tasks of sentence boundary detection,
                tokenization, and part-of-speech tagging on text are all
                compromised by OCR errors [<a class="ref" href="#lopresti2008">Lopresti 2008</a>]. As Lopresti concludes: "While most such errors
                are localised, in the worst case some have an amplifying effect
                that extends well beyond the site of the original error, thereby
                degrading the performance of the end-to-end system." Another
                study performed document clustering and topic modelling on text
                obtained from OCR [<a class="ref" href="#walker2010">Walker et al. 2010</a>]. These authors found that
                for the clustering task the errors had little impact on
                performance, although the errors had a greater impact on
                performance for the topic modelling task. A study involving the
                task of stylistic text classification found that OCR errors had
                little impact on performance [<a class="ref" href="#stein2006">Stein et al. 2006</a>]. In contrast,
                Eder advises that "tidily prepared corpora are integral to
                tests of authorship attribution"  [<a class="ref" href="#eder2013">Eder 2013</a>, 10]. Thus, the
                relevance of scanning errors remains a matter of debate.</div>
                  <div class="counter">
                     <a href="#p18">18</a>
                  </div>
                  <div class="ptext" id="p18">
                Some studies of the effect of OCR
                errors [<a class="ref" href="#lopresti2008">Lopresti 2008</a>] [<a class="ref" href="#walker2010">Walker et al. 2010</a>] [<a class="ref" href="#stein2006">Stein et al. 2006</a>]
                have conducted comparisons by analysing two corpora, identical
                except for corrections of individual words. Our study was
                distinct in two respects. First, it analyzed the effect of OCR
                corrections on corpora at the word level, and it removed
                duplicate, irrelevant and very poorly scanned text. We then
                added genre metadata and verified newspaper and date metadata.
                From the point of view of a scientific experiment about the
                effect of OCR errors, these extra steps may be considered
                "confounding variables". In contrast, projects such as ours make
                these corpus preparation steps necessary, since questions of
                content as well as subtleties of word use are both critical.
                Second, rather than conducting "canonical" tests, such as
                document classification tasks through supervised machine
                learning, we selected key word analysis with log likelihood
                ratio significance testing. These decisions situated our text
                analysis in a real-world digital humanities workflow.</div>
                  <div class="counter">
                     <a href="#p19">19</a>
                  </div>
                  <div class="ptext" id="p19">The accuracy of character recognition at the word level is
                especially significant in projects that involve the
                interpretation of sentiment [<a class="ref" href="#wiebe2005">Wiebe 2005</a>]. Words that
                appear rarely, as opposed to ones that appear most frequently,
                tend to convey deep meaning, particularly words associated with intense emotions, such as
                    anger or disgust [<a class="ref" href="#strapparava2008">Strapparava and Mihalcea                     2008</a>]. Because we attempted to determine the
                    Walworth case’s meanings for contemporaries, including their
                    moral judgments of the principals, we considered our initial
                    scanning error rate of 20% to be unacceptable. This
                    assessment led us to invest the time required to clean
                the text manually after the OCR process by correcting errors at
                the character level as well as removing duplicate and irrelevant
                text. Additionally, because we expected that opinion pieces such
                as editorials and letters to the editor would provide the
                clearest indication of public perception of the Walworth case,
                we added genre metadata to the corpus as a supplement to the
                cleaning process. We then conducted the log likelihood ratio
                comparison of word frequency across two phases of the case’s
                reportage, both to analyze the impact of the cleaning process
                and the addition of genre metadata, and to test our historical
                hypothesis through text mining. </div>
               </div>
               <div class="div div0">
                  <h1 class="head">Correcting OCR-induced Errors and
                    Adding Genre Metadata</h1>
                  <div class="counter">
                     <a href="#p20">20</a>
                  </div>
                  <div class="ptext" id="p20">This section discusses the strategies we undertook to reach a
                level of accuracy comparable to that achieved in benchmark
                historical newspaper text mining projects. It also explains how
                and why we added genre-based metadata before we performed
                analysis using log likelihood ratio [<a class="ref" href="#dunning1993">Dunning 1993</a>].</div>
                  <div class="counter">
                     <a href="#p21">21</a>
                  </div>
                  <div class="ptext" id="p21">Measuring the accuracy of OCR scans can be conducted at both the
                character and word level, which is determined by dividing the
                number of units that are correct by the total number of units
                [<a class="ref" href="#rice1993">Rice et al. 1993</a>]. Calculating such accuracy involves
                hand-labelling all characters and words with their correct
                values and is very time consuming, however. To avoid this
                evaluation step, a word accuracy approximation can be measured
                as a proportion of words appearing in a standard dictionary.<a class="noteRef" href="#d3e480">[9]</a>
                This approach does not consider two opposing factors:
                those words which are correct but not in the dictionary, and
                those that are incorrect but in the dictionary. Despite this
                limitation, a reliable indication of the digitized text’s
                accuracy is possible. </div>
                  <div class="counter">
                     <a href="#p22">22</a>
                  </div>
                  <div class="ptext" id="p22">Because the coverage of the Walworth case included proper names
                and archaic terminology, it was unrealistic to anticipate 100
                per cent accuracy. Words that were split or joined through OCR
                errors were another confounding factor in this estimation of
                accuracy. For example, in one instance the word "prosecution"
                was split into two words ("prosec" and "ution") by the OCR scan,
                while in another case the words "was severely" were merged into
                one garbled word, "was""Aevertyy". To assess this effect, we
                calculated that the average word length for the uncorrected
                (5.84 letters) and corrected (5.68 letters) texts had
                approximately a 3% error rate, which we deemed small enough to
                ignore for the purposes of our study. </div>
                  <div class="counter">
                     <a href="#p23">23</a>
                  </div>
                  <div class="ptext" id="p23">
                     <a href="#table01">Table 1</a> shows the
                approximate word accuracy calculated according to this method,
                both before correction and after the corrections, which we
                describe in the remainder of this section. The pre-correction
                accuracy was comparable to the 78% achieved for the British
                Library’s 19<span class="hi superscript">th</span> Century Online
                Newspaper Archive [<a class="ref" href="#tanner2009">Tanner et al. 2009</a>]. The post-correction
                accuracy is near the target of 98% used by the National Library
                of Australia Newspaper Digitization Program [<a class="ref" href="#holley2009">Holley 2009</a>].
            </div>
                  <div id="table01" class="table">
                     <table class="table">
                        <tr class="row label">
                           <td valign="top" class="cell"/>
                           <td valign="top" class="cell">Words</td>
                           <td valign="top" class="cell">Words in dictionary</td>
                           <td valign="top" class="cell">Words not in Dictionary </td>
                           <td valign="top" class="cell">Approximate Word Accuracy</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell label">Original</td>
                           <td valign="top" class="cell">478762</td>
                           <td valign="top" class="cell">391384</td>
                           <td valign="top" class="cell">87378</td>
                           <td valign="top" class="cell">81.7%</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell label">Clean </td>
                           <td valign="top" class="cell">345181</td>
                           <td valign="top" class="cell">336779</td>
                           <td valign="top" class="cell">8402</td>
                           <td valign="top" class="cell">97.6%</td>
                        </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 1. </div>Effect of post-OCR correction on
                      accuracy.</div>
                  </div>
                  <div class="counter">
                     <a href="#p24">24</a>
                  </div>
                  <div class="ptext" id="p24">A process of manual correction was undertaken to remove the
                errors generated through OCR, because we sought a clearer signal
                in the analysis of the texts. Working with "noise", whether
                induced by OCR or from other sources such as spelling variations
                or language variants used on social media, is common in the
                fields of text mining and corpus linguistics [<a class="ref" href="#knoblock2007">Knoblock 2007</a>] 
                <a class="noteRef" href="#d3e593">[10]</a>
                However, historical interpretation relies on data
                sufficiently clean to boost the credibility of the analysis at
                this scale. Automated techniques were used in a limited way, but
                to achieve results at the high standard desired, we determined
                that manual correction was essential. </div>
                  <div class="counter">
                     <a href="#p25">25</a>
                  </div>
                  <div class="ptext" id="p25">Manual correction offered the benefit of removing duplicate and
                irrelevant sections of text; in addition, it allowed us to add
                document-level metadata tags, which is a critical step in
                complex text analysis. For practical reasons a single corrector
                was used, but to achieve even greater accuracy, multiple
                correctors could be used and their results compared. </div>
                  <div class="counter">
                     <a href="#p26">26</a>
                  </div>
                  <div class="ptext" id="p26">The post-OCR correction process entailed five steps:</div>
                  <div class="ptext">
                     <ol class="list">
                        <li class="item">Simple automatic corrections were made. These included:
                    the removal of hyphens at line breaks, which are mostly a
                    product of words appearing across lines; correcting some
                    simple errors (such as "thb" → "the"); and the correction of
                    principal names in the text, such as "Walworth" or
                    "Mansfield". Full stops not marking the end of sentences
                    were also removed to permit the documents to be broken into
                    semantically meaningful chunks using the full stop
                    delimiter.</li>
                        <li class="item">Articles with an approximate word accuracy below a
                    threshold, set to 80%, were in general discarded to speed
                    the correction process. However, those falling below the
                    threshold, but hand-selected for their rich content, were
                    retained.</li>
                        <li class="item">The text was corrected by hand, comparing the original
                    image file and the post-OCR text version of the same
                    articles.</li>
                        <li class="item">Duplicate and irrelevant text was removed.</li>
                        <li class="item">Metadata tags for article genre were added, broken into
                    four categories: "editorial"; "incidental reportage"; "trial
                    proceedings"; and "letter to the editor". Previously added
                    tags for the name of the newspaper and date were also
                    verified and corrected where required.</li>
                     </ol>
                  </div>
                  <div class="counter">
                     <a href="#p27">27</a>
                  </div>
                  <div class="ptext" id="p27">Automated correction using search and replace with regular
                expressions was necessarily limited to avoid introducing new
                errors, since we considered a garbled word preferable to a
                "correction" leading to a wrong word. We anticipated that the
                clear patterns in the observed errors would lend themselves to
                more sophisticated correction processes using supervised machine
                learning techniques. However this application proved beyond the
                scope of this project.</div>
                  <div class="counter">
                     <a href="#p28">28</a>
                  </div>
                  <div class="ptext" id="p28">Given the modest size of the corpus and the research funding
                available it was feasible to hand-tag genre, delivering accuracy
                benefits over automated approaches. Although we considered
                automatic inclusion of metadata (for example, within the TEI
                standard) as well as automatic part-of-speech tagging (valuable
                for tasks such as document classification), we determined that
                plain text plus article-level genre/date/newspaper metadata was
                sufficient for keyword analysis in our project.</div>
                  <div class="counter">
                     <a href="#p29">29</a>
                  </div>
                  <div class="ptext" id="p29">Manual correction is inescapably a time-consuming process,
                although it does offer collaborative benefits, since it involves
                all team members in the close examination of texts. Some
                projects opt to offshore OCR correction, but ethical
                considerations concerning the exploitation of foreign labour as
                well as quality control concerns ruled out this option in our
                study. The efficiency of inputting corrections was improved by
                using spelling and grammar error highlighting in Microsoft Word.
                This phase took approximately one hundred hours, at an average
                rate of 57.5 words per minute, which is comparable to that of an
                efficient typist. Although this procedure was efficient for
                moderately corrupted text, and easier to sustain over long work
                sessions, highly inaccurate scans rendered typing from scratch
                necessary, as it was quicker than correcting the garbled OCR
                output. When added to the lengthy OCR scanning process, the
                labour required to correct scanned text does raise the question
                of whether OCR is the most efficient way to digitize a
                medium-size corpus of historical newspapers to a high degree of
                accuracy. </div>
                  <div class="counter">
                     <a href="#p30">30</a>
                  </div>
                  <div class="ptext" id="p30">As well as typing from original scans, we transcribed texts using
                a voice recognition program (Dragon Naturally Speaking 12),
                another option for the correction and input process. Typing was
                predominantly used, since it tends to be quicker than
                transcriptions of dictation for corrections. For inputting
                longer sections from scratch, dictation was slightly faster and
                more convenient to use. However, it tends to fail "silently", in
                that it substitutes unrecognized words with other words, which a
                spell-checker cannot detect. Typographical errors, on the other
                hand, are more likely to form non-words that spell checkers can
                identify. Dictation is also more likely to fail on names and
                uncommon words and proper nouns, precisely those words which the
                study is most interested in identifying.</div>
                  <div class="counter">
                     <a href="#p31">31</a>
                  </div>
                  <div class="ptext" id="p31">In summary, while OCR achieves relatively accurate results
                (around 80%) on historical newspaper collections such as the one
                used in this study, manual correction is required to achieve
                high accuracy (around 98%). Depending on the corpus size and the
                resources at hand, this two-step process may be no more
                efficient than directly inputting the original texts from
                scratch.</div>
               </div>
               <div class="div div0">
                  <h1 class="head">Measuring the Effect of Post-OCR
                    Correction Using a Sample Task</h1>
                  <div class="counter">
                     <a href="#p32">32</a>
                  </div>
                  <div class="ptext" id="p32">Determining the tenor of the Walworth case’s newspaper coverage
                and testing for possible shifts over the course of trial was the
                object of our text mining analysis, but the methods we selected
                to do so are relevant to wider debates over the utility of OCR
                and post-OCR correction processes. In order to evaluate changes
                in the popular assessment of the case we created two subsets of
                the digitized corpus: Phase I (news accounts before the
                introduction of Mansfield Walworth’s shocking letters), and
                Phase II (trial coverage subsequent to the letters'
                introduction, including Frank Walworth's conviction and sentence
                of life in prison).<a class="noteRef" href="#d3e688">[11]</a>
                We investigated which words varied at statistically
                significant rates from Phase I to Phase II, particularly those
                indicative of the sentiments stirred by the crime and the
                characters involved. To undertake this analysis we used a list
                of "judgment words". Through a close reading of the texts and
                knowledge of common words used in criminal trial reportage in
                this period the historian produced a preliminary list of words
                of moral judgment and character assessment, which we
                supplemented through the addition of similar words selected with
                the aid of topic modelling of the corpus. Finally, we further
                augmented our list by adding other forms of the selected words
                that appeared in the 2011 edition of the American English Spell
                Checker Oriented Word Lists.<a class="noteRef" href="#d3e698">[12]</a>
                 We chose the statistical tool log likelihood ratio, since
                it is designed to measure variation in the word frequency
                between two sections of a corpus [<a class="ref" href="#dunning1993">Dunning 1993</a>]. Most
                importantly, log likelihood ratio discerns statistically
                significant word frequency variations which are highly likely to
                appear as a result of true properties of the corpora, rather
                than by chance. By calculating log likelihood ratio across the
                two phases of newspaper reportage, we tested for changes in the
                popular judgment of the Walworth case; this test also allowed us
                to analyze the effectiveness of post-OCR cleaning by comparing
                the results of the task performed on the text before and after
                    correction.<a class="noteRef" href="#d3e713">[13]</a>
                  </div>
                  <div class="counter">
                     <a href="#p33">33</a>
                  </div>
                  <div class="ptext" id="p33">Log likelihood ratio, which identifies meaningful variation in
                word frequency in one corpus relative to another [<a class="ref" href="#dunning1993">Dunning 1993</a>],
                produces a p-value on the corresponding test statistic, which
                can be interpreted as the probability of the observed word
                frequencies, given the null hypothesis that there is no
                difference between the two corpora. For words with a p-value
                below some significance level (for example p≤0.05) the null
                hypothesis may be rejected; in other words, the difference in
                word frequency between the two corpora may be considered
                statistically significant when the variation is highly unlikely
                to be a result of chance. However, it is worth noting that while
                this holds for any given word, if we use a given significance
                level to select a set of words, it may still be likely that the
                result for at least one of the selected words may appear to be
                significant by chance alone. Multiple hypothesis testing
                provides a rubric for managing this phenomenon, for example by
                reducing the p-value used for individual words. We chose not to
                pursue this approach; instead we qualitatively analyzed words,
                identified by log likelihood ratio, which helped to detect the
                minority of words incorrectly identified as significant. Because
                we worked as an interdisciplinary team, the historian
                contributed to this critical examination of the output of a
                statistical technique. </div>
                  <div class="counter">
                     <a href="#p34">34</a>
                  </div>
                  <div class="ptext" id="p34">Dunning introduced the log likelihood ratio as a tool for word
                frequency analysis that would be more robust than the previously
                prevalent chi-squared test for small samples of text. It has
                been used in previous studies comparing corpora, for example
                looking at the proceedings of a 19th
                century British murder trial [<a class="ref" href="#archerforthcoming">Archer forthcoming</a>]; the
                distinctive lexicon used in a professional environment [<a class="ref" href="#rayson2000">Rayson and Garside 2000</a>]<a class="noteRef" href="#d3e727">[14]</a>; historical spelling variations [<a class="ref" href="#baron2009">Baron 2009</a>]; and
                the lines of a particular character in a play [<a class="ref" href="#mcintyre2010">McIntyre 2010</a>]. We did consider other tests, which have been
                proposed as alternatives to the log likelihood ratio. For
                instance, Fisher’s exact test [<a class="ref" href="#moore2004">Moore 2004</a>] calculates exactly
                what the log likelihood ratio approximates but requires greater
                computational resources, while the Mann-Whitney Ranks test
                [<a class="ref" href="#kilgarriff2001">Kilgarriff 2001</a>] considers the distribution of word frequency
                within a corpus, as does the t-test [<a class="ref" href="#paquot2009">Paquot and Bestgen 2009</a>].
                After reviewing these options we decided that log likelihood
                ratio was the best option, due to its well-established use in
                the comparison of corpora. </div>
                  <div class="counter">
                     <a href="#p35">35</a>
                  </div>
                  <div class="ptext" id="p35">As described in Section 4, the correction process was enhanced
                through the inclusion of metadata about article genre. The two
                phases differed in genre mix, since the reportage from Phase II
                was dominated by coverage of the Walworth case trial
                proceedings. While we were interested in detecting changing
                public opinion, differences in genre could possibly have
                obscured the shift we anticipated.
                Where genre metadata was available, we restricted our analysis
                to opinion articles about the case, consisting of editorials and
                letters to the editor, since this genre is most likely to
                capture words of interest. Furthermore, technical judgment words
                appearing in trial proceedings – particularly those used by
                lawyers and the judge in court – indicate legal constructions
                that may not have reflected public opinion. This caveat was
                another reason for the restriction of the corpus to opinion
                articles using genre metadata. As <a href="#figure03">Figure 3</a> shows, we compared
                the original text without metadata; the original text restricted
                using genre metadata; and the cleaned text also restricted using
                genre metadata.</div>
                  <div id="figure03" class="figure">
                
                
                     <div class="ptext">
                        <a href="./resources/images/figure03.png" rel="external">
                           <img src="./resources/images/figure03.png"
                                alt="Original and Clean Text displayed as a schematic"/>
                        </a>
                     </div> 
                     <div class="caption">
                        <div class="label">Figure 3. </div>Schematic of the datasets used in
                    the experiments presented in this paper. The word counts
                    for each dataset for Phase I and Phase II are
                    shown.</div>
                  </div>
                  <div class="counter">
                     <a href="#p36">36</a>
                  </div>
                  <div class="ptext" id="p36">Pre-processing of the texts was performed to improve the quality
                of results returned. The following four steps were taken:</div>
                  <div class="ptext">
                     <ol class="list">
                        <li class="item">All text converted to lower case.</li>
                        <li class="item">Punctuation removed and the possessive form "’s".<a class="noteRef" href="#d3e778">[15]</a>
                        </li>
                        <li class="item">Stopwords removed, such as "the" and "of", from a standard
                    stopwords list<a class="noteRef" href="#d3e790">[16]</a>
                        </li>
                        <li class="item">Words identified from the custom-built list of 357
                    judgment words, consisting primarily of adjectives, adverbs
                    and abstract nouns.</li>
                     </ol>
                  </div>
                  <div class="counter">
                     <a href="#p37">37</a>
                  </div>
                  <div class="ptext" id="p37">These steps, as well as the log likelihood ratio calculations,
                were performed using several open source tools.<a class="noteRef" href="#d3e803">[17]</a>
               The results of the experiments are detailed in the
                following section.</div>
               </div>
               <div class="div div0">
                  <h1 class="head">Results</h1>
                  <div class="counter">
                     <a href="#p38">38</a>
                  </div>
                  <div class="ptext" id="p38">The tests we conducted involved comparing newspaper coverage of
                the Walworth case over our two periods: Phase I (the crime, the
                arrest, the coroner’s inquest and the trial’s opening); and
                Phase II (subsequent to the letters’ introduction up to the
                verdict and sentencing). By using data with and without post-OCR
                correction we were able to address our historical question and
                to evaluate the effect of this correction.</div>
                  <div class="counter">
                     <a href="#p39">39</a>
                  </div>
                  <div class="ptext" id="p39">
                     <a href="#figure04">Figure 4</a> shows the ten Phase I words that appeared at most
                significant frequency, measured by log likelihood ratio, for the
                three datasets presented in <a href="#figure03">Figure 3</a>. The
                defendant is the focus of early reportage, with terms such as
                "young", and "son" appearing, as well as the negative word
                "murderer" (considering that his conviction had not yet
                occurred). The opinion article genre focuses on the defendant’s
                family background, including the word "chancellor" (Judge Reuben
                Hyde Walworth, Mansfield’s father), "albany" (the state capital,
                where the defendant’s uncle lived) and "literary", the last of
                which referred to Mansfield Walworth’s career as a gothic
                novelist, rather than his negative character traits. Even
                without OCR correction, most of these words of interest were
                identified. Despite the presence of non-words caused by OCR
                error, they do not appear in these top few words. Without
                metadata, the words tend to focus on the minutiae of the murder
                scene, such as "stairs", "body" and "door", rather than on more
                substantive issues of character. This points to the need for
                historical researchers to consider adding genre metadata prior
                to calculating log likelihood ratios.</div>
                  <div id="figure04" class="figure">
                
                     <div class="ptext">
                        <a href="./resources/images/figure04.png" rel="external">
                           <img src="./resources/images/figure04.png"
                                alt="Venn diagram graphic of top 10 phase 1 words described&#xA;                    in table 2"/>
                        </a>
                     </div> 
                
                     <div class="caption">
                        <div class="label">Figure 4. </div>Top 10 Phase I words for each
                    dataset described in <a href="#table02">Table 2</a>, ranked by log-likelihood
                    ratio. All words shown were significant at p≤0.0001. No
                    judgment words appear.</div>
                  </div>
                  <div class="counter">
                     <a href="#p40">40</a>
                  </div>
                  <div class="ptext" id="p40">The same approach for Phase II yielded primarily legal terms,
                which disclosed little about changing opinion. Therefore, we
                show in Figure 5 those words from our judgment word list that
                occurred more frequently in this second trial period at a
                statistically significant rate (using a significance level of
                    p≤0.05). The words "insanity" and
                "insane" may refer to both Frank and Mansfield, since the
                defence suggested that the son may have suffered from a form of
                madness inherited from his disturbed father. The terms "threats"
                and "madman" reflect a new focus on the condemnation of
                Mansfield, although there is some possibility "madman" could
                also refer to Frank. The words "deliberation" and "deliberate"
                may negatively describe Frank’s actions, but they may equally be
                procedural legal terms relating to the jury. The differences
                between the datasets are less pronounced in this experiment,
                aside from the fact that the original dataset contained more
                statistically significant words, since it includes substantially
                more words overall (see <a href="#table01">Table 1</a>for details). Some of these words suggest a condemnation of
                Mansfield ("demon") and potential approval of Frank ("honor"),
                since he claimed he had killed his father to protect his mother.
                This pattern suggests that using judgment words may be an
                alternative to adding genre metadata, since these words
                implicitly refer to genre.
                <div id="figure05" class="figure"> 
                    
                        <a href="./resources/images/figure05.png" rel="external">
                           <img src="./resources/images/figure05.png" alt="Frequent Phase II words"/>
                        </a>
               
                        <div class="caption">
                           <div class="label">Figure 5. </div>Frequent Phase II words for each
                        dataset described in <a href="#table02">Table 2</a>, using judgment words which
                        are statistically significant at p≤0.05.</div>
                     </div>
           Fortunately, ambiguities such as
                whether "insanity" refers to Frank or Mansfield, or whether
                "deliberate" refers to Frank’s shooting, the judge or the jury,
                may be resolved using more sophisticated techniques. For
                example, words may be matched to characters in the case through
                sentence blocks, word proximity, or full-scale parsing for
                semantic structure. Such techniques typically require very high
                quality text to be effective. For words with relatively low
                frequencies, manually investigating the contexts of occurrences
                can also be used.
            <div id="table02" class="table">
                        <table class="table">
                           <tr class="row label">
                              <td valign="top" class="cell">Significant Words</td>
                              <td valign="top" class="cell">Precision</td>
                              <td valign="top" class="cell">Recall</td>
                              <td valign="top" class="cell">Significant Judgment Words</td>
                              <td valign="top" class="cell">Judgment Words Precision</td>
                              <td valign="top" class="cell">Judgment Words Recall</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell label">Original with Metadata</td>
               
                              <td valign="top" class="cell">751</td>
                              <td valign="top" class="cell">0.48</td>
                              <td valign="top" class="cell">0.66</td>
                              <td valign="top" class="cell">23</td>
                              <td valign="top" class="cell">0.65</td>
                              <td valign="top" class="cell">0.93</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell label">Original</td>
                              <td valign="top" class="cell">2852</td>
                              <td valign="top" class="cell">0.12</td>
                              <td valign="top" class="cell">0.61</td>
                              <td valign="top" class="cell">38</td>
                              <td valign="top" class="cell">0.34</td>
                              <td valign="top" class="cell">0.81</td>
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Performance of Original
                        with Metadata and Original Datasets compared to Clean
                        with Metadata dataset. The Clean with Metadata dataset
                        returned 545 significant words of which 16 were judgment
                        words. A significance level of p≤0.05 was used.</div>
                     </div>
                  </div>
                  <div class="counter">
                     <a href="#p41">41</a>
                  </div>
                  <div class="ptext" id="p41">
                     <a href="#table02">Table 2</a> shows the precision
                and recall of the results of words with a significance level of
                    p≤0.05 for the original with genre
                metadata and original datasets compared to the "gold standard",
                that is, the clean with metadata dataset. Recall refers to the
                proportion of words significant in the clean with metadata
                dataset, which are also significant in the original (with or
                without metadata) dataset. Precision is the proportion of words
                significant in the original (with or without metadata) dataset,
                which are also significant in the clean with metadata dataset.
                This evaluation methodology allowed us to drill down deeper than
                we could by using the small shortlist of words shown in <a href="#figure04">Figure 4</a>
                    and <a href="#figure05">Figure 5</a>, and it revealed a strong
                discrepancy between the datasets. </div>
                  <div class="counter">
                     <a href="#p42">42</a>
                  </div>
                  <div class="ptext" id="p42">The precision scores of 0.48 and 0.12 indicate that many words
                were incorrectly identified as significant, while the recall
                scores of 0.66 and 0.61 suggest that a substantial portion of
                significant words was missed. The original dataset approached
                the original with metadata dataset on recall, but it had much
                lower precision, indicating that it returned many results with
                limited usefulness. Some non-words induced by OCR error appear
                at a statistically significant level in the original and
                original with metadata lists, such as "ol" (instead of "of") and
                "ihe" (instead of "the"). Using the judgment word list, the
                precision scores of 0.65 and 0.34 suggest, again, that many
                "false positive" words were identified, with the problem
                magnified without adding in genre metadata. The recall results
                of 0.93 and 0.81 were stronger for the judgment word list,
                however, which is of interest given the emotive nature of the
                case and its coverage. Overall, there was a substantial
                discrepancy in the words identified in the original datasets,
                with and without metadata, compared to the clean with metadata
                dataset. This confirms that OCR errors can, indeed, influence
                later analysis of this nature. </div>
                  <div class="counter">
                     <a href="#p43">43</a>
                  </div>
                  <div class="ptext" id="p43">It is worth examining in detail one example in which an OCR error
                produced a judgment word found to be significant using the
                original with metadata dataset, but not by using the clean with
                metadata dataset. In the original with metadata dataset, the
                word "maudlin" was identified as occurring significantly more in
                Phase I, with a frequency of 3 compared to 0 in Phase II.
                However, there was one instance of "maudlin" occurring in Phase
                II in the clean with metadata dataset which was missed due to an
                OCR error – swapping "maudlin" for "inaudlin". In the clean with
                metadata dataset the frequency counts of 3 for Phase I versus 1
                for Phase II were not significantly different. While these
                frequencies may seem low, relative scarcity does not indicate
                low significance. In fact, our test indicated the opposite to be
                the case.</div>
                  <div class="counter">
                     <a href="#p44">44</a>
                  </div>
                  <div class="ptext" id="p44">The contexts of "maudlin" appear in <a href="#table03">Table 3</a>, which indicates that
                the uses of the term in Phase I occurred in the context of
                disapproval of Frank’s parricidal motive and cool demeanor. The
                use of the term in Phase II was different, we discovered,
                because it referred to the state of mind of another murderer in
                an earlier trial, in which a plea of insanity had been
                successful. This shows that further work is required to identify
                the implications of word use based on their contexts. Indeed, it
                seems that there was a suggestive change in the frequency of
                "maudlin" between Phase I and Phase II. 
            </div>
                  <div id="table03" class="table">
                     <table class="table">
                        <tr class="row label">
                           <td valign="top" class="cell">Newspaper</td>
                           <td valign="top" class="cell">Date</td>
                           <td valign="top" class="cell">Phase</td>
                           <td valign="top" class="cell">Correct in Original?</td>
                           <td valign="top" class="cell">Context</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell">
                              <cite class="title italic">NY Tribune</cite>
                           </td>
                           <td valign="top" class="cell">1873-06-04</td>
                           <td valign="top" class="cell">I</td>
                           <td valign="top" class="cell">Yes</td>
                           <td valign="top" class="cell">"We protest in advance against such resort to maudlin
                        sentimentality"</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell">
                              <cite class="title italic">NY Tribune</cite>
                           </td>
                           <td valign="top" class="cell">1873-06-05</td>
                           <td valign="top" class="cell">I</td>
                           <td valign="top" class="cell">Yes</td>
                           <td valign="top" class="cell">"There’s a something 
                        indefinable about this maudlin sentimentalism that
                        throws a glow of heroism round the murder"</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell">
                              <cite class="title italic">The Saratogian</cite> (quoting <cite class="title italic">NY Tribune</cite>)</td>
                           <td valign="top" class="cell">1873-06-12</td>
                           <td valign="top" class="cell">I</td>
                           <td valign="top" class="cell">Yes</td>
                           <td valign="top" class="cell">"We protest in advance against such resort to maudlin
                        sentimentality"</td>
                        </tr>
                        <tr class="row">
                           <td valign="top" class="cell">
                              <cite class="title italic">NY Tribune</cite>
                           </td>
                           <td valign="top" class="cell">1873-07-04</td>
                           <td valign="top" class="cell">II</td>
                           <td valign="top" class="cell">No</td>
                           <td valign="top" class="cell">"the maudlin sorrow of a drunkard" (referring to
                        another case where insanity was successfully pled, an
                        outcome the author critiques)</td>
                        </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 3. </div>Contexts of the judgment
                        word "maudlin".</div>
                  </div>
                  <div class="counter">
                     <a href="#p45">45</a>
                  </div>
                  <div class="ptext" id="p45">Overall, the cleaning of the data was not essential to achieving
                results of interest on the two-phase comparison task, since many
                significant words could still be identified. Still, there were
                substantial differences between the results of the clean and
                original datasets, as significant words were missed and "false
                positives" were generated. The adding of genre metadata
                permitted the filtering of more significant words through the
                use of opinion articles, something that was not possible with
                the original dataset. Our list of judgment words likely
                performed a similar filtering function to the genre metadata,
                though it lacks the flexibility to detect unexpected words. </div>
               </div>
               <div class="div div0">
                  <h1 class="head">Future Work</h1>
                  <div class="counter">
                     <a href="#p46">46</a>
                  </div>
                  <div class="ptext" id="p46">Moving beyond the analysis presented in this paper, it would be
                desirable to identify which words refer to which characters in
                the case. With the clean corpus we now have at our disposal,
                this identification could be achieved through the automated
                tagging of syntactic metadata. This analysis would allow us to
                track public opinion at the level of the individual with greater
                precision. Words which may refer to multiple individuals may be
                disambiguated using techniques such as sentence blocks, word
                proximity and semantic parsing. It is expected that this more
                complex task would show greater differences between the raw OCR
                output and the corrected text, since it depends on the presence
                of grammatically well-formed sentences rather than word counts
                alone. </div>
                  <div class="counter">
                     <a href="#p47">47</a>
                  </div>
                  <div class="ptext" id="p47">An alternative approach that may be useful in similar projects
                would involve identifying which terms are distinctive
                "hallmarks" of particular subcorpora (for instance, selected on
                the basis of date or news source). A feature selection metric
                such as mutual information could be used to identify which terms
                are most predictive in classifying documents as belonging to
                particular subcorpora. Turning from supervised to unsupervised
                learning, our team anticipates producing results based on topic
                modelling, a common strategy in the digital humanities which has
                been applied to US historical newspapers [<a class="ref" href="#newman2006">Newman and Block 2006</a>],
                [<a class="ref" href="#yang2011">Yang et al. 2011</a>], and [<a class="ref" href="#nelson2012">Nelson 2012</a>]. The goal of such projects is to
                find topics in large volumes of newspaper reportage, and to
                track changes as indices of shifts in public discourse. The
                effect of OCR errors on such topic models is also an active
                subject of research, and our results suggest that scholars
                consider this issue thoroughly before undertaking large-scale
                projects [<a class="ref" href="#walker2010">Walker et al. 2010</a>]. </div>
                  <div class="counter">
                     <a href="#p48">48</a>
                  </div>
                  <div class="ptext" id="p48">The broader ambition of this research project is to situate the
                Walworth case in its wider historical context. Can it be shown
                through text mining that prevailing understandings of masculine
                honour, morality and family values were challenged by this
                dramatic incident? In future work we will compare our digitized
                collection with larger newspaper corpora. Google
                    n-grams<a class="noteRef" href="#d3e1233">[18]</a>
               is a common and accessible choice for researchers, but its
                contents are different from our corpus in both format (the full
                text of its sources is unavailable) and in genre (it covers
                non-fiction, arcane technical writings and literary works). A
                more promising collection is Gale’s Nineteenth Century Newspaper
                    collection.<a class="noteRef" href="#d3e1242">[19]</a>
               If it becomes fully searchable it will provide a vast
                dataset from which subsets of texts (such as editorials on
                domestic homicide) can be selected to evaluate the distinct and
                shared features of the Walworth case’s coverage. Nevertheless,
                there are reasons for caution. In large corpora such as these,
                OCR induced errors will remain an issue, since hand correction
                of texts on a vast scale is infeasible.</div>
               </div>
               <div class="div div0">
                  <h1 class="head">Conclusion</h1>
                  <div class="counter">
                     <a href="#p49">49</a>
                  </div>
                  <div class="ptext" id="p49">Digital humanities scholars have been drawn to text mining as a
                technique well suited to the analysis of historical newspapers,
                since it allows for meaning to be drawn from volumes of text
                that would be unmanageable for an individual researcher to
                absorb and analyze. It provides a tool that can test hypotheses
                generated through traditional historical analysis, and ideally,
                generate new possibilities for study that could not have been
                generated through close reading alone. However, the digitization
                of historical texts is a complex and time-consuming process
                which is worthy of consideration in itself. Through the example
                of the Walworth murder case’s newspaper coverage, this paper has
                outlined the two-step digitization process our team undertook:
                first, performing OCR scans from original newspapers and image
                files; and second, cleaning and post-processing to ensure that
                all text included is accurate, relevant, and labelled with genre
                metadata. We have provided an original, detailed methodology for
                conducting digitization of a medium-size corpus. </div>
                  <div class="counter">
                     <a href="#p50">50</a>
                  </div>
                  <div class="ptext" id="p50">OCR, we determined, is effective in digitising historical
                newspapers to roughly 80% accuracy. However, to achieve high
                levels of accuracy (around 98%), the labour-intensive cleaning
                required to remove OCR errors means the two-step process may be
                no more efficient than manually inputting texts from scratch, a
                procedure that suits small- to medium-scale projects. While our
                research involved both scanning and cleaning texts, historical
                researchers more commonly perform keyword searches on existing
                databases of historical documents to conduct text mining
                analysis [<a class="ref" href="#hitchcock2013">Hitchcock 2013</a>]. Importantly, our study shows that the
                result set may contain OCR errors, irrelevant and duplicate
                content; similarly, insufficient metadata can generate spurious
                results that are difficult to detect. Our method proposed for
                the cleaning process, as well as our appraisal of the value of
                this step, signals the way forward to overcome this problem.</div>
                  <div class="counter">
                     <a href="#p51">51</a>
                  </div>
                  <div class="ptext" id="p51">The value of correcting OCR output from around 80% accuracy to
                near 100% is an important consideration for researchers, in view
                of the labour-intensive process required. We demonstrated this
                empirically by performing a sample task of interest on both the
                clean and original versions of the corpora. This task involved
                finding words, including those from a list of pertinent judgment
                words, which changed in frequency across two phases of the
                case’s reportage. Log likelihood ratio was used as a test for
                statistical significance. With the uncorrected OCR output it was
                possible to identify words appearing significantly more
                frequently in one time period relative to another, but a
                substantial proportion were missed and "false positives" were
                introduced. The cleaning was thus desirable but not essential.
                The addition of genre metadata led to results of greater
                interest, since it allowed a focus on articles more clearly
                relevant to the research question. This paper is unique in
                situating OCR error correction in a digitization workflow also
                involving content selection, document-level metadata enhancement
                and practical time and cost constraints, as it evaluates this
                text cleaning phase holistically. </div>
                  <div class="counter">
                     <a href="#p52">52</a>
                  </div>
                  <div class="ptext" id="p52">Like many digital humanities projects, this study underlines the
                value of input from researchers across the disciplines of
                history and computer science to design the project, select the
                methodology, implement the tasks and interpret the results
                [<a class="ref" href="#ayers2013">Ayers 2013</a>]; [<a class="ref" href="#nelson2012">Nelson 2012</a>]. Without this combination of skills
                and expertise, as well as facilitative research funding, such
                studies are unfeasible. Our team’s scientific expertise allowed
                us to customize software for text mining analysis rather than
                using off-the-shelf solutions, which gave us full control over
                the integrity of the tools used, while the historian posed the
                research question and critically examined test results against
                the initial close reading of the case. This collaborative,
                interdisciplinary model will continue to be critical to foster
                robust research in the field of digital humanities. </div>
               </div>
        
        
            

            </div>
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d3e184">
                  <span class="noteRef">[1]</span> The online sources used for this project were the Library of Congress’s Chronicling
                            America, Historic American Newspapers (<a class="ref"
                     href="http://chroniclingamerica.loc.gov/"
                     onclick="window.open('http://chroniclingamerica.loc.gov/'); return false">http://chroniclingamerica.loc.gov/</a>);
                            America’s Historical Newspapers (<a class="ref"
                     href="http://www.newsbank.com/readex/?content=96/"
                     onclick="window.open('http://www.newsbank.com/readex/?content=96/'); return false">http://www.newsbank.com/readex/?content=96</a>);
                            Gale 19thC US Newspapers (<a class="ref"
                     href="http://gdc.gale.com/products/19th-century-u.s.-newspapers/"
                     onclick="window.open('http://gdc.gale.com/products/19th-century-u.s.-newspapers/'); return false">http://gdc.gale.com/products/19th-century-u.s.-newspapers/</a>);
                            and New York State Historical Newspapers – Old
                            Fulton NY Postcards (<a class="ref"
                     href="http://fultonhistory.com/Fulton.html"
                     onclick="window.open('http://fultonhistory.com/Fulton.html'); return false">http://fultonhistory.com/Fulton.html</a>). In
                            addition, photocopies of microfilm were made in 2003
                            at the NY State Newspaper Project hosted by the New
                            York State Public Library (<a class="ref"
                     href="http://www.nysl.nysed.gov/nysnp/"
                     onclick="window.open('http://www.nysl.nysed.gov/nysnp/'); return false">http://www.nysl.nysed.gov/nysnp/</a>).
                  </div>
               <div class="endnote" id="d3e202">
                  <span class="noteRef">[2]</span> Research funding for this
                            project was provided by the Australian Research
                            Council</div>
               <div class="endnote" id="d3e252">
                  <span class="noteRef">[3]</span> Mining the <cite class="title italic">
                        Dispatch</cite> (<a class="ref"
                     href="http://dsl.richmond.edu/dispatch/Topics"
                     onclick="window.open('http://dsl.richmond.edu/dispatch/Topics'); return false">http://dsl.richmond.edu/dispatch/Topics</a>).
                            Historian Robert K. Nelson directs the University of
                            Richmond’s Digital Scholarship Lab, which developed
                            this project. </div>
               <div class="endnote" id="d3e299">
                  <span class="noteRef">[4]</span> The 10 public and private
                            institutions are Improving Access to Text;
                            Australian Newspapers Digitisation Program; The Text
                            Creation Partnership; British Newspapers 1800-1900;
                            Early English Books Online; American National
                            Digital Newspaper Program; Project Gutenberg;
                            Universal Digital Library Million Book Collection;
                            and Gale Eighteenth Century Collections Online. 
                    </div>
               <div class="endnote" id="d3e302">
                  <span class="noteRef">[5]</span> Other commercial software that
                            were evaluated, but were not found to be as suitable
                            as ABBYY include: ExperVision OCR, Vividata,
                            VelOCRaptor, Presto! OCR, OmniPage, Olive, and
                            Prizmo. Other open source software that was
                            evaluated for its suitability was OCRopus,
                            hocr-tools, isri-ocr-evaluation-tools, Tesseract,
                            and GOCR. For a comprehensive list of OCR software
                            see <a class="ref"
                     href="http://en.wikipedia.org/wiki/List_of_optical_character_recognition_software"
                     onclick="window.open('http://en.wikipedia.org/wiki/List_of_optical_character_recognition_software'); return false">http://en.wikipedia.org/wiki/List_of_optical_character_recognition_software</a>.
                   </div>
               <div class="endnote" id="d3e308">
                  <span class="noteRef">[6]</span> We are also grateful for
                            advice provided by the Digitisation Facility at the
                            National Centre of Biography, Australian National
                            University (<a class="ref"
                     href="http://ncb.anu.edu.au/scanner"
                     onclick="window.open('http://ncb.anu.edu.au/scanner'); return false">http://ncb.anu.edu.au/scanner</a>). 
                    </div>
               <div class="endnote" id="d3e317">
                  <span class="noteRef">[7]</span> Since Lightroom cannot import
                            PDFs all files were sorted and processed in the one
                            application. Only the high quality pdfs of the
                            <cite class="title italic">New York Times</cite> were
                    "clean" enough to be OCRed directly from the downloaded PDF,
                    so did not require processing through Lightroom.
                    </div>
               <div class="endnote" id="d3e350">
                  <span class="noteRef">[8]</span> One unfortunate downside to this
                            workflow is that Lightroom cannot export greyscale
                            images, so it only exported each 20MB TIFF as a
                            110MB TIFF.
                    </div>
               <div class="endnote" id="d3e480">
                  <span class="noteRef">[9]</span>The dictionary was compiled from
                            Kevin Atkinson’s SCOWL wordlists (Spell Checker
                            Oriented Wordlists) available at <a class="ref"
                     href="http://wordlist.sourceforge.net"
                     onclick="window.open('http://wordlist.sourceforge.net'); return false">http://wordlist.sourceforge.net</a>.
                    </div>
               <div class="endnote" id="d3e593">
                  <span class="noteRef">[10]</span>The Corpus Analysis with Noise
                            in the Signal 2013 conference (<a class="ref"
                     href="http://ucrel.lancs.ac.uk/cans2013/"
                     onclick="window.open('http://ucrel.lancs.ac.uk/cans2013/'); return false">http://ucrel.lancs.ac.uk/cans2013/</a>) is a
                    good example.</div>
               <div class="endnote" id="d3e688">
                  <span class="noteRef">[11]</span>Frank Walworth was pardoned four years after his
                            conviction, but this twist to the story attracted
                            little attention from the press [<a class="ref" href="#strange2010">Strange 2010</a>]. 
                    </div>
               <div class="endnote" id="d3e698">
                  <span class="noteRef">[12]</span>
                For information on the Spell Checker Oriented Word List see <a class="ref"
                     href="http://wordlist.sourceforge.net/"
                     onclick="window.open('http://wordlist.sourceforge.net/'); return false">http://wordlist.sourceforge.net/</a>. The list, or "dictionary",
                                is a concatenation of word lists compiled for
                                use in spell checkers. We are grateful for
                                Loretta Auville’s
                               advice on this aspect of our study.</div>
               <div class="endnote" id="d3e713">
                  <span class="noteRef">[13]</span> A first principles approach to
                            this question is also possible, but due to the
                            mathematical complexity of incorporating OCR errors
                            into calculations finding significant words with log
                            likelihood ratio, we used an empirical approach in
                            this paper.</div>
               <div class="endnote" id="d3e727">
                  <span class="noteRef">[14]</span>
                   Apparently by accident, this study uses an incorrect variant of the log
                            likelihood ratio. In the second equation the authors
                            present on page 3, the sum should run over all four
                            cells of the contingency table (rather than just
                            those in the top row), and the observed and expected
                            values for each of these should be calculated. With
                            a large corpus size relative to word frequency, the
                            ratio of observed to expected values for the bottom
                            row cells will be approximately 1 and hence the
                            contribution of these cells will be negligible.
                            However, with a small corpus size relative to word
                            frequency these cells make a substantial
                            contribution and should not be ignored. Several
                            open-source tools including Meandre (<a class="ref"
                     href="http://seasr.org/meandre"
                     onclick="window.open('http://seasr.org/meandre'); return false">http://seasr.org/meandre</a>), which we used
                            in this study, repeat this error. We modified the
                            source code in order to use the log likelihood ratio
                            as it originally appeared in [<a class="ref" href="#dunning1993">Dunning 1993</a>]. This
                            confirms the need for humanities scholars to work
                            with experts in computer science and digital
                            humanities, to ensure a deep understanding of
                            statistical techniques, rather than rely on
                            off-the-shelf tools which may occasionally have
                            inaccuracies in their implementation. 
                    </div>
               <div class="endnote" id="d3e778">
                  <span class="noteRef">[15]</span> A more thorough stemming or
                                lemmatisation approach was not performed but may
                                be useful in future.
                        </div>
               <div class="endnote" id="d3e790">
                  <span class="noteRef">[16]</span>
                            An in-house stopword list
                                from NICTA (National ICT Australia) was
                                used.
                        </div>
               <div class="endnote" id="d3e803">
                  <span class="noteRef">[17]</span>Meandre (<a class="ref"
                     href="http://seasr.org/meandre"
                     onclick="window.open('http://seasr.org/meandre'); return false">http://seasr.org/meandre</a>), Monk (<a class="ref"
                     href="http://monkproject.org"
                     onclick="window.open('http://monkproject.org'); return false">http://monkproject.org</a>) and OpenNLP (<a class="ref"
                     href="http://opennlp.apache.org"
                     onclick="window.open('http://opennlp.apache.org'); return false">http://opennlp.apache.org</a>). These tools
                            were sufficiently powerful for our study, though
                            there are a range of other similar tools available,
                            such as Wmatrix (<a class="ref"
                     href="http://ucrel.lancs.ac.uk/wmatrix/"
                     onclick="window.open('http://ucrel.lancs.ac.uk/wmatrix/'); return false">http://ucrel.lancs.ac.uk/wmatrix/</a>) and
                    WordSmith (<a class="ref"
                     href="http://www.lexically.net/wordsmith/"
                     onclick="window.open('http://www.lexically.net/wordsmith/'); return false">http://www.lexically.net/wordsmith/</a>).
                    </div>
               <div class="endnote" id="d3e1233">
                  <span class="noteRef">[18]</span>
                        <a class="ref"
                     href="http://books.google.com/ngrams"
                     onclick="window.open('http://books.google.com/ngrams'); return false">http://books.google.com/ngrams</a>
                    </div>
               <div class="endnote" id="d3e1242">
                  <span class="noteRef">[19]</span>
                        <a class="ref"
                     href="http://gdc.gale.com/products/19th-century-u.s.-newspapers/"
                     onclick="window.open('http://gdc.gale.com/products/19th-century-u.s.-newspapers/'); return false">http://gdc.gale.com/products/19th-century-u.s.-newspapers/</a>
                   </div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl">
                  <span class="ref" id="archerforthcoming"><!-- close -->Archer forthcoming</span> . Archer,
                Dawn. "Tracing the crime narratives
                within the Palmer Trial (1856): From the lawyer’s opening
                speeches to the judge’s summing up."</div>
               <div class="bibl">
                  <span class="ref" id="arlitsch2004"><!-- close -->Arlitsch2004</span> . Arlitsch, Kenning and John Herbert.
                "Microﬁlm, paper, and OCR: issues in
                    newspaper digitization.”"
                   <cite class="title italic">Microform and Imaging Review</cite>, 33.2:
                58–67.</div>
               <div class="bibl">
                  <span class="ref" id="ayers2013"><!-- close -->Ayers 2013</span> . Ayers, Edward L. <cite class="title italic">Does
                    Digital Scholarship have a Future?.”</cite>
                  <cite class="title italic">EDUCASEreview</cite>, 48.4: 24-34.</div>
               <div class="bibl">
                  <span class="ref" id="baron2009"><!-- close -->Baron 2009</span> .Baron, Alistair, Paul Rayson, and Dawn
                Archer. "Word frequency and key word statistics in corpus
                linguistics."<cite class="title italic">Anglistik</cite>20.1 (2009):
                41-67.</div>
               <div class="bibl">
                  <span class="ref" id="dunning1993"><!-- close -->Dunning 1993</span> . Dunning, Ted.
                "Accurate Methods for the Statistics
                of Surprise and Coincidence". <cite class="title italic">Computational
                    Linguistics</cite>19.1 (1993): 61-74.</div>
               <div class="bibl">
                  <span class="ref" id="eder2013"><!-- close -->Eder 2013</span> . Eder, Maciej. "Mind your Corpus: Systematic Errors in
                Authorship Attribution."” <cite class="title italic">Literary and
                    Linguistic Computing</cite> doi: 10.1093/llc/fqt039
                (2013).</div>
               <div class="bibl">
                  <span class="ref" id="hitchcock2013"><!-- close -->Hitchcock 2013</span> . Hitchcock,
                Tim. <cite class="title italic">Confronting the Digital, or How
                Academic History Writing Lost the Plot.</cite>” <cite class="title italic">Cultural and Social History</cite> 10.1 (2013)</div>
               <div class="bibl">
                  <span class="ref" id="holley2009"><!-- close -->Holley 2009</span> . Holley, Rose. "How Good Can It Get? Analysing and
                Improving OCR Accuracy in Large Scale Historic Newspaper
                Digitization Programs." <cite class="title italic">D-Lib
                Magazine </cite>15.3/4 (2009). </div>
               <div class="bibl">
                  <span class="ref" id="kagan2009"><!-- close -->Kagan 2009</span> . Kagan, Jerome. <cite class="title italic">The Three Cultures:
                    Natural Sciences, Social Sciences, and the Humanities in the
                    21st Century</cite>. Cambridge: Cambridge University Press,
                2009.</div>
               <div class="bibl">
                  <span class="ref" id="kilgarriff2001"><!-- close -->Kilgarriff 2001</span> . Kilgarriff,
                Adam. <cite class="title italic">Comparing corpora.</cite> 
                  <cite class="title italic">International journal of corpus linguistics
                </cite>6.1 (2001): 97-133.</div>
               <div class="bibl">
                  <span class="ref" id="knoblock2007"><!-- close -->Knoblock 2007</span> . Knoblock, Craig, Daniel Lopresti, Shourya
                Roy and L. Venkata Subramaniam, eds. "Special Issue on Noisy
                Text Analytics". <cite class="title italic">International Journal on
                    Document Analysis and Recognition</cite> 10.3/4 (2007). </div>
               <div class="bibl">
                  <span class="ref" id="lopresti2008"><!-- close -->Lopresti 2008</span> . Lopresti,
                    Daniel. "Optical Character Recognition
                Errors and their Effects on Natural Language
                Processing." <cite class="title italic">Proceedings of the Second Workshop on Analytics for
                Noisy Unstructured Text Data.</cite> ACM, 2008.</div>
               <div class="bibl">
                  <span class="ref" id="mcintyre2010"><!-- close -->McIntyre 2010</span> . McIntyre, Dan,
                and Dawn Archer. "A
                corpus-based approach to mind style." <cite class="title italic">Journal
                    of Literary Semantics</cite> 39.2 (2010): 167-182.</div>
               <div class="bibl">
                  <span class="ref" id="moore2004"><!-- close -->Moore 2004</span> . Moore, Robert C.
                    “<cite class="title italic">On log-likelihood-ratios and the
                significance of rare events.</cite>
                  <cite class="title italic">Proceedings of
                    the 2004 Conference on Empirical Methods in Natural Language
                    Processing</cite>. 2004.</div>
               <div class="bibl">
                  <span class="ref" id="nelson2010"><!-- close -->Nelson 2010</span> . Nelson, Robert K.
                "Mining the <em class="emph">Dispatch</em> –
                    Introduction." <cite class="title italic">Mining the <em class="emph">Dispatch</em>
                website.</cite> 
                  <a class="ref"
                     href="http://dsl.richmond.edu/dispatch/pages/home"
                     onclick="window.open('http://dsl.richmond.edu/dispatch/pages/home'); return false">
                    http://dsl.richmond.edu/dispatch/pages/home</a>.</div>
               <div class="bibl">
                  <span class="ref" id="nelson2012"><!-- close -->Nelson 2012</span> . Nelson, Robert
                    K. <cite class="title italic">A Conversation with Digital
                Historians</cite>, <cite class="title italic">Southern Spaces</cite>, 31 January
                2012. <a class="ref"
                     href="http://www.southernspaces.org/2012/conversation-digital-historians"
                     onclick="window.open('http://www.southernspaces.org/2012/conversation-digital-historians'); return false">www.southernspaces.org/2012/conversation-digital-historians</a>.</div>
               <div class="bibl">
                  <span class="ref" id="newman2006"><!-- close -->Newman and Block 2006</span>  Newman, David J., and Sharon Block.
                "Probabilistic topic decomposition of an eighteenth‐century
                American newspaper". <cite class="title italic">Journal of the American
                    Society for Information Science and Technology</cite> 57.6
                (2006): 753-767.</div>
               <div class="bibl">
                  <span class="ref" id="obrien2010"><!-- close -->O'Brien 2010</span> . O’Brien,
                Geoffrey. <cite class="title italic">The Fall of
                    the House of Walworth: A Tale of Madness and Murder in
                    Gilded Age America</cite>. New York: Henry Holt and Company,
                2010. </div>
               <div class="bibl">
                  <span class="ref" id="paquot2009"><!-- close -->Paquot and Bestgen 2009</span> . Paquot, Magali, and Yves Bestgen.
                "Distinctive words in academic writing: A comparison of three
                statistical tests for keyword extraction". <cite class="title italic">Language and Computers</cite> 68.1 (2009): 247-269.</div>
               <div class="bibl">
                  <span class="ref" id="powell2004"><!-- close -->Powell 2004</span>  Powell, Kerry. <cite class="title italic">The Cambridge
                    Companion to Victorian and Edwardian Theatre</cite>.
                Cambridge: Cambridge University Press, 2004. </div>
               <div class="bibl">
                  <span class="ref" id="rayson2000"><!-- close -->Rayson and Garside 2000</span> . Rayson, Paul, and Roger Garside.
                "Comparing corpora using frequency
                    profiling". <cite class="title italic">Proceedings of the
                        workshop on Comparing Corpora</cite>.
                Association for Computational Linguistics, 2000.</div>
               <div class="bibl">
                  <span class="ref" id="rice1993"><!-- close -->Rice et al. 1993</span>  Rice, Stephen V., Junichi Kanai, and Thomas A.
                Nartker. "An Evaluation of OCR Accuracy". Information Science
                Research Institute, 1993 Annual Research Report (1993):
                9-20.</div>
               <div class="bibl">
                  <span class="ref" id="stein2006"><!-- close -->Stein et al. 2006</span> . Stein, Sterling Stuart, Shlomo Argamon, and
                Ophir Frieder. “"The effect of OCR errors on stylistic text
                    classification".” <cite class="title italic">Proceedings of the 29th
                    annual international ACM SIGIR conference on Research and
                    development in information retrieval</cite>. ACM,
                2006.</div>
               <div class="bibl">
                  <span class="ref" id="strange2010"><!-- close -->Strange 2010</span> . Strange, Carolyn.
                "The Unwritten Law of Executive
                Justice: Pardoning Patricide in Reconstruction-Era New York",
                    <cite class="title italic">Law and History Review</cite> 28. 4 (2010):
                891-30.</div>
               <div class="bibl">
                  <span class="ref" id="strapparava2008"><!-- close -->Strapparava and Mihalcea                     2008</span>  Strapparava, Carlo and Rada
                Mihalcea. "Learning to Identify Emotions in
                    Texts" <cite class="title italic">SAC:
                        Proceedings of the 2008 ACM symposium on Applied computing</cite>
                (2008): 1556-1560.</div>
               <div class="bibl">
                  <span class="ref" id="svensson2010"><!-- close -->Svensson 2010</span>  Svensson,
                    Patrik. “"The Landscape of the Digital
                Humanities".<cite class="title italic">Digital Humanities
                    Quarterly</cite> 4.1 (2010).</div>
               <div class="bibl">
                  <span class="ref" id="tanner2009"><!-- close -->Tanner et al. 2009</span>  Tanner, Simon, Trevor Muñoz, and Pich Hemy
                Ros. "Measuring Mass Text Digitization Quality and
                    Usefulness". <cite class="title italic">D-Lib
                        Magazine </cite>15.7/8 (2009).</div>
               <div class="bibl">
                  <span class="ref" id="walker2010"><!-- close -->Walker et al. 2010</span>  Walker, Daniel D., William B. Lund and Eric
                K. Ringger. "Evaluating Models of Latent Document Semantics in
                the Presence of OCR Errors". <cite class="title italic">Proceedings of
                    the 2010 Conference on Empirical Methods in Natural Language
                    Processing,</cite> Boston: Association for Computational
                Linguistics, October 2010. (2010): 240-50. </div>
               <div class="bibl">
                  <span class="ref" id="wiebe2005"><!-- close -->Wiebe 2005</span> Wiebe, Janyce,
                    Theresa Wilson, and Claire Cardie. "Annotating
                        Expressions of Opinions and Emotions in
                        Language". <cite class="title italic">Language Resources
                        and Evaluation</cite>39.2/3 (2005): 165-210.</div>
               <div class="bibl">
                  <span class="ref" id="williams2011"><!-- close -->Williams 2011</span>  Williams, Jeffrey
                J. "The Statistical Turn in
                Literary Studies". <cite class="title italic">The Chronicle
                    Review</cite> 57.18 (2011): B14-15.</div>
               <div class="bibl">
                  <span class="ref" id="yang2011"><!-- close -->Yang et al. 2011</span>  Yang, Tze-I., Andrew J. Torget, and Rada
                Mihalcea. “Topic modelling on historical newspapers.” <cite class="title italic">Proceedings of the 5th ACL-HLT Workshop on
                    Language Technology for Cultural Heritage, Social Sciences,
                    and Humanities</cite>. Association for Computational
                Linguistics, 2011.
            </div>
            </div>
            <div class="toolbar">
               <a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#"
                  onclick="javascript:window.print();"
                  title="Click for print friendly version">Print Article</a>
            </div>
         </div>
      </div>
   </body>
</html>
