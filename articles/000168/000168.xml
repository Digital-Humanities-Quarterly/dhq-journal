<?xml version="1.0" encoding="UTF-8"?><?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?><?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article">Mining for the Meanings of a Murder: The Impact of OCR Quality
                    on the Use of Digitized Historical Newspapers</title>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Carolyn <dhq:family>Strange</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>Australian National University</dhq:affiliation>
                    <email>carolyn.strange@anu.edu</email>
                    <dhq:bio>
                        <p>Carolyn Strange is Graduate Director in the School of History, ANU, and
                            Adjunct Professor of Arts, Education and Creative Media at Murdoch
                            University, Perth. She has published and taught in numerous humanities
                            and social science fields, including criminology and women’s studies.
                            Her next book concerns the history of discretionary justice in New York
                            State, from the Revolution to the Depression.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Daniel <dhq:family>McNamara</dhq:family>
                    </dhq:author_name>
                    <email>dpmcna@gmail.com</email>
                    <dhq:bio>
                        <p>Daniel McNamara has an honours degree in computer science from the ANU
                            and a passion for the humanities. His research applies data mining
                            techniques to a range of domains, including analysis of historical
                            newspapers and trend prediction in academic citation networks.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Josh <dhq:family>Wodak</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>Australian National University</dhq:affiliation>
                    <email>josh.wodak@anu.edu.au</email>
                    <dhq:bio>
                        <p>Josh Wodak's research in Environmental Humanities and Digital Humanities
                            explores climate change, biodiversity loss, and species extinction. He
                            holds a Bachelor of Arts (Honours) in Visual Anthropology (University of
                            Sydney) and a PhD in Interdisciplinary Cross-Cultural Research (ANU). He
                            is currently a Visiting Fellow at ANU’s Humanities Research Centre and
                            an Honorary Research Fellow at the Faculty of Architecture, Design&amp;
                            Planning, University of Sydney.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <dhq:author_name>Ian <dhq:family>Wood</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>Research School of Computer Science, Australian National
                        University</dhq:affiliation>
                    <email>ian.wood@anu.edu.au </email>
                    <dhq:bio>
                        <p>Ian is a mathematician and computer scientist studying the detection and
                            measurement of social processes in social media data. These goals have
                            led to his collaboration with historians, social scientists and social
                            psychologists. He is currently completing a PhD with the Research School
                            of Computer Science, ANU.</p>
                    </dhq:bio>
                </dhq:authorInfo>

            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id">000168</idno>
                <idno type="volume">008</idno>
                <idno type="issue">1</idno>
                <date when="2014-04-17">17 April 2014</date>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
                    </bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change when="2014-01-29" who="EH">Encoded file</change>
        </revisionDesc>
    </teiHeader>

    <text xml:lang="en">
        <front>
            <dhq:abstract>
                <p> Digital humanities research that requires the digitization of medium-scale,
                    project-specific texts confronts a significant methodological and practical
                    question: is labour-intensive cleaning of the Optical Character Recognition
                    (OCR) output necessary to produce robust results through text mining analysis?
                    This paper traces the steps taken in a collaborative research project that aimed
                    to analyze newspaper coverage of a high-profile murder trial, which occurred in
                    New York City in 1873. A corpus of approximately one-half million words was
                    produced by converting original print sources and image files into digital
                    texts, which produced a substantial rate of OCR-generated errors. We then
                    corrected the scans and added document-level genre metadata. This allowed us to
                    evaluate the impact of our quality upgrade procedures when we tested for
                    possible differences in word usage across two key phases in the trial's coverage
                    using log likelihood ratio <ptr target="#dunning1993"/>. The same tests were run
                    on each dataset – the original OCR scans, a subset of OCR scans selected through
                    the addition of genre metadata, and the metadata-enhanced scans corrected to 98%
                    accuracy. Our results revealed that error correction is desirable but not
                    essential. However, metadata to distinguish between different genres of trial
                    coverage, obtained during the correction process, had a substantial impact. This
                    was true both when investigating all words and when testing for a subset of
                        <q>judgment words</q> we created to explore the murder’s emotive elements
                    and its moral implications. Deeper analysis of this case, and others like it,
                    will require more sophisticated text mining techniques to disambiguate word
                    sense and context, which may be more sensitive to OCR-induced errors. </p>
            </dhq:abstract>
            <dhq:teaser>
                <p>OCR offers humanities scholars a double-edged sword, involving speed versus
                    accuracy. This paper evaluates OCR’s capacities and trade-offs through the
                    analysis of a 19th century murder case’s newspaper coverage. </p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>Introduction</head>
                <p>Digitized historical newspapers have enriched scholars’ capacity to pose research
                    questions about the past, but the quality of these texts is rarely ideal, due
                    especially to OCR errors. Although scholarly concern about this problem in
                    relation to the use of large, web-based historical resources is growing <ptr
                        target="#hitchcock2013"/>, research that involves the digitization of
                    smaller corpora of original print documents confronts the same challenge. The
                    perennial question in digital humanities research – how accurate must digitized
                    sources be to produce robust results – arose in the course of our attempt to
                    interrogate the meanings of a highly publicized murder case prosecuted in New
                    York in 1873. This paper traces how we addressed this challenge: firstly by
                    searching for the techniques best suited to digitize and to interpret news
                    coverage of the trial; and secondly, by applying a statistical tool to test for
                    possible shifts in popular appraisals of the case. </p>
                <p>Our project transformed over several years, from an individual, archive-based
                    inquiry into a text mining collaboration that required the conversion of
                    original news accounts into searchable texts through OCR. Our initial scans had
                    an error rate of 20%, which raised the prospect that text mining might not
                    substantially enhance our capacity to analyze word usage in the case’s coverage.
                    Consequently, we proceeded to reduce noise through manual correction of the
                    scans and through the addition of genre metadata, thereby creating three
                    datasets – the original OCR scans, a subset of the OCR scans selected through
                    the addition of genre metadata, and the metadata-enhanced scans corrected to 98%
                    accuracy. By analysing each dataset with the log likelihood ratio statistical
                    tool, we determined that the labour-intensive work of cleaning the data modestly
                    improved the reliability of our test – to establish whether or not popular
                    judgment of the case altered after controversial evidence was introduced in the
                    course of the murder trial. However, the addition of genre-related metadata
                    proved to be considerably more significant. Most importantly, the digitization
                    of the previously unsearchable primary sources did make it possible to pose a
                    research question that could not have been answered persuasively using
                    unsearchable texts. </p>
                <p>As Tim Hitchcock argues, the development of digital history into a discipline
                    requires that we expose and evaluate the research processes that allow us to
                    compose <quote rend="inline" source="#hitchcock2013">subtle maps of
                        meaning</quote> from piles of primary sources <ptr target="#hitchcock2013"
                        loc="20"/>. Accordingly, we begin with an account of the murder case in
                    Section 1, and discuss how the digitization of its news coverage opened up new
                    ways to mine its meanings. In Section 2, we discuss the nature of our corpus and
                    the ways in which we produced machine-readable text using Adobe Photoshop
                    Lightroom 3 and ABBYY FineReader 11. In Section 3, we outline the nature of the
                    errors produced in the scanning processes, while Section 4 details the steps we
                    took to correct them and to add genre metadata post OCR. Section 5 explains how
                    we used the log likelihood ratio tool to analyze word frequency and to test the
                    use of judgment words in trial coverage, using our three different datasets. The
                    results of our tests appear in Section 6, which is followed by a discussion of
                    the possible future directions of text mining research based on small- to
                    medium-sized corpora. We conclude that text mining can enrich historians’
                    capacity to analyze large bodies of text, even in the presence of OCR-induced
                    errors. Supplementing digitized text with genre metadata permits a finer-grained
                    and more reliable analysis of historical newspapers. Investing the time to
                    produce clean data and metadata improves performance, and our study suggests
                    that this is essential for more sophisticated analysis, such as language
                    parsing. Finally, our project underlines the need for interdisciplinary teams to
                    ensure the integrity of the digital tools used, as well as the reliability of
                    their outputs’ interpretation. </p>
            </div>
            <div>
                <head>From Historical Analysis to Digital Historical Research</head>
                <p>The Walworth murder project began in 2003 as a humanistic enterprise conducted by
                    an historian of gender and criminal justice who read over four hundred newspaper
                    accounts of the case on microfilm, which were reproduced by printing out hard
                    copies of images. This body of unsearchable records was augmented as the number
                    of digitized newspapers available through open-source and proprietary online
                    databases grew exponentially over the 2000s, although many of those texts were
                    unsearchable image files. <note> The online sources used for this project were
                        the Library of Congress’s Chronicling America, Historic American Newspapers
                            (<ref target="http://chroniclingamerica.loc.gov/"
                            >http://chroniclingamerica.loc.gov/</ref>); America’s Historical
                        Newspapers (<ref target="http://www.newsbank.com/readex/?content=96/"
                            >http://www.newsbank.com/readex/?content=96</ref>); Gale 19thC US
                        Newspapers (<ref
                            target="http://gdc.gale.com/products/19th-century-u.s.-newspapers/"
                            >http://gdc.gale.com/products/19th-century-u.s.-newspapers/</ref>); and
                        New York State Historical Newspapers – Old Fulton NY Postcards (<ref
                            target="http://fultonhistory.com/Fulton.html"
                            >http://fultonhistory.com/Fulton.html</ref>). In addition, photocopies
                        of microfilm were made in 2003 at the NY State Newspaper Project hosted by
                        the New York State Public Library (<ref
                            target="http://www.nysl.nysed.gov/nysnp/"
                            >http://www.nysl.nysed.gov/nysnp/</ref>). </note> A research grant made
                    it possible in 2012 to digitize the entire body of primary sources (the
                    paper-based prints and PDF images of newspapers) through OCR scans. <note>
                        Research funding for this project was provided by the <name>Australian
                            Research Council</name>
                    </note>. This funding meant that hypotheses developed in the course of the
                    historian’s earlier close reading of the case could be tested in a collaboration
                    that included two hired computer scientists and a digital humanities
                    scholar.</p>
                <p>The Walworth case’s extensive and sensational newspaper coverage indicated that
                    the murder of <name>Mansfield Walworth</name>, a second-rate novelist and
                    third-rate family man, stirred deep feelings, particularly because the killer
                    was his son, Frank. The murder provoked troubling questions: Was it legally or
                    morally excusable for a son to kill his father, no matter how despicable? And
                    why, in a family filled with lawyers and judges (including the murdered man’s
                    father, <name>Judge Reuben Hyde Walworth</name>) had the law not provided a
                    remedy <ptr target="#obrien2010"/>? The event occurred on 3 June 1873, when
                        <name>Frank Walworth</name>, a youth of nineteen, travelled to Manhattan to
                    confront his father, who was recently divorced from his mother. <name>Mansfield
                        Walworth</name> had sent a raft of letters to his ex-wife, full of murderous
                    threats and mad ravings. After intercepting these alarming letters, <name>Frank
                        Walworth</name> shot his father dead, then informed the police that he had
                    done so to save himself, his mother and his siblings. Was the shooter an
                    honourable son? A maudlin youth? Insane? Speculation swirled but the initial
                    response to the murder was one of shock: a refined young man from a highly
                    respectable white family had committed cold-blooded murder <ptr
                        target="#obrien2010"/>. Scores of headlines announced that this was no
                    ordinary murder but a <quote rend="inline" source="#obrien2010">PARRICIDAL
                        TRAGEDY.</quote>
                </p>
                <p>Our working hypothesis was that popular readings of the Walworth murder changed
                    as the trial progressed – from initial horror over the crime of parricide, to an
                    appreciation of domestic cruelty and the menacing nature of the victim. The
                    trial resembled a real-life domestic melodrama <ptr target="#powell2004"/>, and
                    its turning point occurred when the victim’s vile letters to his ex-wife were
                    read into evidence, as the defence attempted to verify the threat
                        <name>Mansfield Walworth</name> had presented to his son and ex-wife. At
                    this point in the trial, the dead man’s profane abuse was recorded by trial
                    reporters for the nation to read: <quote rend="block" source="#obrien2010">
                        You have blasted my heart and think now as you always thought that you could
                        rob me of the sweet faces of my children and then gradually after a year or
                        two rob me of my little inheritance. You will see, you God damned bitch of
                        hell, I have always intended to murder you as a breaker of my heart. God
                        damn you, you will die and my poor broken heart will lie dead across your
                        God damned body. Hiss, hiss, I'm after you… I will kill you on
                        sight.</quote>
                </p>
                <p>What was the impact of this obscene evidence on the public’s judgment of the
                    case? Did it change over the course of the trial, and if so, how? These were
                    research questions best addressed through the mining of the newspaper coverage,
                    a substantial corpus beyond the capacity of human assessment.</p>
                <p>Although the volume of the Walworth case’s news coverage was modest compared to
                    large-scale, institutionally funded text mining projects, the standards set in
                    several benchmark historical newspaper text-mining research projects informed
                    our approach. Many, such as Mining the <title rend="italic"
                        >Dispatch</title>,<note> Mining the <title rend="italic"> Dispatch</title>
                            (<ref target="http://dsl.richmond.edu/dispatch/Topics"
                            >http://dsl.richmond.edu/dispatch/Topics</ref>). Historian <name>Robert
                            K. Nelson</name> directs the University of Richmond’s Digital
                        Scholarship Lab, which developed this project. </note> use manually
                    double-keyed documentation, followed by comparison-based correction, to produce
                    datasets with 98+% accuracy. Mining the <title rend="italic">Dispatch</title>
                    used a large corpus of nineteenth-century U.S. newspapers to <q>explore – and encourage exploration of – the
                        dramatic and often traumatic changes as well as the sometimes surprising
                        continuities in the social and political life of Civil War Richmond.</q>
                </p>
                <p> That project combined distant and close reading of every issue of one newspaper
                    (112,000 texts totalling almost 24 million words) <cit>
                        <quote rend="inline" source="#nelson2010">to uncover categories and discover
                            patterns in and among texts.</quote>
                        <ptr target="#nelson2010"/>
                    </cit> As its director explained, high accuracy levels were necessary to combine
                    text mining with historical interpretation most productively: <cit>
                        <quote rend="inline" source="#nelson2010">the challenge is to toggle between
                            distant and close readings; not to rely solely on topic modelling and
                            visualizations.</quote>
                        <ptr target="#nelson2010"/>
                    </cit> Studies that pursue similar objectives must first determine the best
                    methods to produce machine-readable text. The next section details the scanning
                    process preliminary to our analysis.</p>
            </div>
            <div>
                <head>The Production of Machine-searchable Texts </head>
                <p>Using a variety of sources, including photocopied prints of microfilmed
                    newspapers and PDF image files of stories sourced from several databases, we
                    gleaned 600 pages, comprising approximately 500,000 words of digitized text.
                        <ref target="#figure01">Figure 1</ref> shows an example article ready for
                    scanning.</p>
                <p>Since optimal machine-readable text was integral to our analysis of the murder
                    trial’s meanings, we reviewed the quality control methods used by ten large
                    public and private institutions, from scanning hardware and software through to
                    a diverse array of OCR software.<note> The 10 public and private institutions
                        are Improving Access to Text; Australian Newspapers Digitisation Program;
                        The Text Creation Partnership; British Newspapers 1800-1900; Early English
                        Books Online; American National Digital Newspaper Program; Project
                        Gutenberg; Universal Digital Library Million Book Collection; and Gale
                        Eighteenth Century Collections Online. </note> We intended to use
                    non-proprietary software,<note> Other commercial software that were evaluated,
                        but were not found to be as suitable as ABBYY include: ExperVision OCR,
                        Vividata, VelOCRaptor, Presto! OCR, OmniPage, Olive, and Prizmo. Other open
                        source software that was evaluated for its suitability was OCRopus,
                        hocr-tools, isri-ocr-evaluation-tools, Tesseract, and GOCR. For a
                        comprehensive list of OCR software see <ref
                            target="http://en.wikipedia.org/wiki/List_of_optical_character_recognition_software"
                            >http://en.wikipedia.org/wiki/List_of_optical_character_recognition_software</ref>.
                    </note> but we ultimately selected ABBYY FineReader 11, since it allows for the
                    customisation of features.<note> We are also grateful for advice provided by the
                        Digitisation Facility at the National Centre of Biography, Australian
                        National University (<ref target="http://ncb.anu.edu.au/scanner"
                            >http://ncb.anu.edu.au/scanner</ref>). </note> The first source of text
                    was photocopied pages printed in 2003 from microfilmed newspaper images. Since
                    online repositories of newspapers have subsequently become more numerous, we
                    were able to replace the poor quality text in microfilm scans with PDF image
                    files of the photocopies. However, this strategy proved to be too
                    time-consuming, since each of these page-scan PDFs contained upwards of 60
                    paragraphs of text (an average of 5,000 words in small print), spread across
                    upwards of 8 columns, which required laborious searches for references to the
                    Walworth murder trial. Consequently, this replacement strategy was used only for
                    the worst 10% of the photocopies. </p>
                <p>All the scanned files viable to use from the existing scanned microfilm were
                    imported into Adobe Photoshop Lightroom 3. This process involved batch scanning
                    pages in black and white into 300DPI TIFFs according to newspaper, and then
                    placing them in physical folders by newspaper, in the same order in which they
                    had been scanned, to facilitate cross-referencing of particular pages with their
                    corresponding files. Within Lightroom each file was then manually cropped
                    one-by-one to select only those columns related to the Walworth murder.<note>
                        Since Lightroom cannot import PDFs all files were sorted and processed in
                        the one application. Only the high quality PDFs of the <title rend="italic"
                            >New York Times</title> were <q>clean</q> enough to be OCRed directly
                        from the downloaded PDF, so did not require processing through Lightroom.
                    </note> Although Lightroom is designed for working with large catalogues of
                    photographs rather than <q>photographs of text</q>, it allowed us to batch
                    process select images iteratively through non-destructive image editing, so that
                    tests could be made to determine which combination of image processing was
                    likely to achieve the highest OCR accuracy. </p>
                <figure xml:id="figure01">
                    <head>An example of one of the newspaper articles on the Walworth case (from the
                            <title rend="italic">New York Herald</title>, 10 June 1873), showing
                        average level image degradation. The OCR output appears to the right, with
                        errors highlighted in bold.</head>
                    <figDesc>An example of a newspaper article image that shows details of image
                        degeneration alongside a transcript</figDesc>
                    <graphic url="./resources/images/figure01.png"/>
                </figure>
                <p>Training software to recognize patterns of font and content is one of the most
                    challenging aspects of OCR, as it draws on Artificial Intelligence to
                        <soCalled>recognize</soCalled> multitudes of shapes as belonging to
                    corresponding letters. Our project revealed that ABBYY FineReader’s training
                    capacity is limited. After we exported files produced through Lightroom,
                    newspaper by newspaper,<note> One unfortunate downside to this workflow is that
                        Lightroom cannot export greyscale images, so it only exported each 20MB TIFF
                        as a 110MB TIFF. </note> we trained ABBYY to recognize each newspaper’s
                    fonts, and each file was further <soCalled>cleaned up</soCalled> by
                    straightening text lines and by correcting for perspective distortion. Although
                    ABBYY appeared to <soCalled>recognize</soCalled> frequently occurring words,
                    like the surname Walworth, the OCRd results produced variations, such as
                        <q>Wolwarth</q> and <q>Warworth</q>. It became evident that ABBYY cannot
                    recognize that all such variations of <q>Walworth</q> should have been converted
                    automatically to <q>Walworth</q>, given the high statistical likelihood that
                    they were in fact <q>Walworth</q>.</p>
                <p>Our process exposed the sorts of image degradation common in the digitization of
                    historical newspapers, including: smudged, faded and warped text; ripped or
                    crumpled originals; image bleed from the reverse side of the paper; crooked and
                    curved text lines; and overexposed and underexposed microfilm scans. As a
                    result, ABBYY’s deficiencies required that customized automated corrections be
                    applied in the post-OCR phase of our project.</p>
                <p>Nevertheless, the production of machine-readable text resulted in a uniform
                    dataset of newspaper articles that offers considerable granularity, including
                    the capacity to analyze a corpus of articles on the Walworth murder trial
                    according to date, a critical factor in our study, considering the admission of
                        <name>Mansfield Walworth</name>’s extraordinary letters into evidence. The
                    coverage of the corpus, disaggregated by newspaper, is shown in Figure 2.</p>
                <figure>
                    <head>Word counts per newspaper, original text from OCR scans (left) and cleaned
                        text (right). The correction process is discussed in Section 3.</head>
                    <graphic url="./resources/images/figure02.png"/>
                    <figDesc>Bar graph showing word counts per newspaper</figDesc>
                </figure>
            </div>
            <div>
                <head>OCR Errors and Quality Control for Text Mining</head>
                <p>The variable quality of digitized historical newspapers has long been a challenge
                    for digital scholarship <ptr target="#arlitsch2004"/> Much of that variability
                    is associated with the historical and contemporary resources of publishing
                    houses, meaning that major metropolitan papers typically sit at one end of the
                    legibility spectrum and smaller, regional papers sit at the other. In our study,
                    articles from the <title rend="italic">New York Times</title> yielded accuracy
                    levels of 94.5%, and they are obtainable through the paper’s own search engine.
                    Furthermore, articles in the <title rend="italic">Times</title> repository are
                    cropped and cleaned, which means they are ready for OCRing with minimal image
                    manipulation. In contrast, OCR scans from an important upstate New York
                    newspaper based in the state capital, the <title rend="italic">Albany
                        Argus,</title> yielded results of only 65% accuracy. Unfortunately, due to
                    the substantial additional labour required to raise this and other smaller
                    papers’ level of accuracy, these scans were mostly too poor to incorporate. Thus
                    the variation in file quality from different newspapers presented a limitation
                    on the project’s initial ambitions. More broadly, this problem flags the
                    significant impact that OCR quality can make in the range of sources used for
                    text mining. OCR errors are part of a wider problem of dealing with <q>noise</q>
                    in text mining <ptr target="#knoblock2007"/>, which may also stem from other
                    sources such as historical spelling variations or language specific to different
                    media texts. </p>
                <p>The impact of OCR errors varies depending on the task performed, however <ptr
                        target="#eder2013"/>. The tasks of sentence boundary detection,
                    tokenization, and part-of-speech tagging on text are all compromised by OCR
                    errors <ptr target="#lopresti2008"/>. As Lopresti concludes: <quote
                        rend="inline" source="#lopresti2008">While most such errors are localised,
                        in the worst case some have an amplifying effect that extends well beyond
                        the site of the original error, thereby degrading the performance of the
                        end-to-end system.</quote> Another study performed document clustering and
                    topic modelling on text obtained from OCR <ptr target="#walker2010"/>. These
                    authors found that for the clustering task the errors had little impact on
                    performance, although the errors had a greater impact on performance for the
                    topic modelling task. A study involving the task of stylistic text
                    classification found that OCR errors had little impact on performance <ptr
                        target="#stein2006"/>. In contrast, Eder advises that <cit>
                        <quote rend="inline" source="#eder2013">tidily prepared corpora are integral
                            to tests of authorship attribution</quote>
                        <ptr target="#eder2013" loc="10"/>
                    </cit>. Thus, the relevance of scanning errors remains a matter of debate.</p>
                <p> Some studies of the effect of OCR errors <ptr target="#lopresti2008"/>
                    <ptr target="#walker2010"/>
                    <ptr target="#stein2006"/> have conducted comparisons by analysing two corpora,
                    identical except for corrections of individual words. Our study was distinct in
                    two respects. First, it analyzed the effect of OCR corrections on corpora at the
                    word level, and it removed duplicate, irrelevant and very poorly scanned text.
                    We then added genre metadata and verified newspaper and date metadata. From the
                    point of view of a scientific experiment about the effect of OCR errors, these
                    extra steps may be considered <q>confounding variables</q>. In contrast,
                    projects such as ours make these corpus preparation steps necessary, since
                    questions of content as well as subtleties of word use are both critical.
                    Second, rather than conducting <q>canonical</q> tests, such as document
                    classification tasks through supervised machine learning, we selected key word
                    analysis with log likelihood ratio significance testing. These decisions
                    situated our text analysis in a real-world digital humanities workflow.</p>
                <p>The accuracy of character recognition at the word level is especially significant
                    in projects that involve the interpretation of sentiment <ptr
                        target="#wiebe2005"/>. Words that appear rarely, as opposed to ones that
                    appear most frequently, tend to convey deep meaning, particularly words
                    associated with intense emotions, such as anger or disgust <ptr
                        target="#strapparava2008"/>. Because we attempted to determine the Walworth
                    case’s meanings for contemporaries, including their moral judgments of the
                    principals, we considered our initial scanning error rate of 20% to be
                    unacceptable. This assessment led us to invest the time required to clean the
                    text manually after the OCR process by correcting errors at the character level
                    as well as removing duplicate and irrelevant text. Additionally, because we
                    expected that opinion pieces such as editorials and letters to the editor would
                    provide the clearest indication of public perception of the Walworth case, we
                    added genre metadata to the corpus as a supplement to the cleaning process. We
                    then conducted the log likelihood ratio comparison of word frequency across two
                    phases of the case’s reportage, both to analyze the impact of the cleaning
                    process and the addition of genre metadata, and to test our historical
                    hypothesis through text mining. </p>
            </div>
            <div>
                <head>Correcting OCR-induced Errors and Adding Genre Metadata</head>
                <p>This section discusses the strategies we undertook to reach a level of accuracy
                    comparable to that achieved in benchmark historical newspaper text mining
                    projects. It also explains how and why we added genre-based metadata before we
                    performed analysis using log likelihood ratio <ptr target="#dunning1993"/>.</p>
                <p>Measuring the accuracy of OCR scans can be conducted at both the character and
                    word level, which is determined by dividing the number of units that are correct
                    by the total number of units <ptr target="#rice1993"/>. Calculating such
                    accuracy involves hand-labelling all characters and words with their correct
                    values and is very time consuming, however. To avoid this evaluation step, a
                    word accuracy approximation can be measured as a proportion of words appearing
                    in a standard dictionary.<note>The dictionary was compiled from <name>Kevin
                            Atkinson</name>’s SCOWL wordlists (Spell Checker Oriented Wordlists)
                        available at <ref target="http://wordlist.sourceforge.net"
                            >http://wordlist.sourceforge.net</ref>. </note> This approach does not
                    consider two opposing factors: those words which are correct but not in the
                    dictionary, and those that are incorrect but in the dictionary. Despite this
                    limitation, a reliable indication of the digitized text’s accuracy is possible. </p>
                <p>Because the coverage of the Walworth case included proper names and archaic
                    terminology, it was unrealistic to anticipate 100 per cent accuracy. Words that
                    were split or joined through OCR errors were another confounding factor in this
                    estimation of accuracy. For example, in one instance the word <q>prosecution</q>
                    was split into two words (<q>prosec</q> and <q>ution</q>) by the OCR scan, while
                    in another case the words <q>was severely</q> were merged into one garbled word,
                        <q>was</q>
                    <q>Aevertyy</q>. To assess this effect, we calculated that the average word
                    length for the uncorrected (5.84 letters) and corrected (5.68 letters) texts had
                    approximately a 3% error rate, which we deemed small enough to ignore for the
                    purposes of our study. </p>
                <p>
                    <ref target="#table01">Table 1</ref> shows the approximate word accuracy
                    calculated according to this method, both before correction and after the
                    corrections, which we describe in the remainder of this section. The
                    pre-correction accuracy was comparable to the 78% achieved for the British
                    Library’s 19<hi rend="superscript">th</hi> Century Online Newspaper Archive <ptr
                        target="#tanner2009"/>. The post-correction accuracy is near the target of
                    98% used by the <name>National Library of Australia Newspaper Digitization
                        Program</name>
                    <ptr target="#holley2009"/>. </p>
                <table xml:id="table01">
                    <head>Effect of post-OCR correction on accuracy.</head>
                    <row role="label">
                        <cell/>
                        <cell>Words</cell>
                        <cell>Words in dictionary</cell>
                        <cell>Words not in Dictionary </cell>
                        <cell>Approximate Word Accuracy</cell>
                    </row>
                    <row>
                        <cell role="label">Original</cell>
                        <cell>478762</cell>
                        <cell>391384</cell>
                        <cell>87378</cell>
                        <cell>81.7%</cell>
                    </row>
                    <row>
                        <cell role="label">Clean </cell>
                        <cell>345181</cell>
                        <cell>336779</cell>
                        <cell>8402</cell>
                        <cell>97.6%</cell>
                    </row>

                </table>
                <p>A process of manual correction was undertaken to remove the errors generated
                    through OCR, because we sought a clearer signal in the analysis of the texts.
                    Working with <q>noise</q>, whether induced by OCR or from other sources such as
                    spelling variations or language variants used on social media, is common in the
                    fields of text mining and corpus linguistics <ptr target="#knoblock2007"/>
                    <note>The Corpus Analysis with Noise in the Signal 2013 conference (<ref
                            target="http://ucrel.lancs.ac.uk/cans2013/"
                            >http://ucrel.lancs.ac.uk/cans2013/</ref>) is a good example.</note>
                    However, historical interpretation relies on data sufficiently clean to boost
                    the credibility of the analysis at this scale. Automated techniques were used in
                    a limited way, but to achieve results at the high standard desired, we
                    determined that manual correction was essential. </p>
                <p>Manual correction offered the benefit of removing duplicate and irrelevant
                    sections of text; in addition, it allowed us to add document-level metadata
                    tags, which is a critical step in complex text analysis. For practical reasons a
                    single corrector was used, but to achieve even greater accuracy, multiple
                    correctors could be used and their results compared. </p>
                <p>The post-OCR correction process entailed five steps:</p>
                <list type="ordered">
                    <item>Simple automatic corrections were made. These included: the removal of
                        hyphens at line breaks, which are mostly a product of words appearing across
                        lines; correcting some simple errors (such as <q>thb</q> → <q>the</q>); and
                        the correction of principal names in the text, such as <q>Walworth</q> or
                            <q>Mansfield</q>. Full stops not marking the end of sentences were also
                        removed to permit the documents to be broken into semantically meaningful
                        chunks using the full stop delimiter.</item>
                    <item>Articles with an approximate word accuracy below a threshold, set to 80%,
                        were in general discarded to speed the correction process. However, those
                        falling below the threshold, but hand-selected for their rich content, were
                        retained.</item>
                    <item>The text was corrected by hand, comparing the original image file and the
                        post-OCR text version of the same articles.</item>
                    <item>Duplicate and irrelevant text was removed.</item>
                    <item>Metadata tags for article genre were added, broken into four categories:
                            <q>editorial</q>; <q>incidental reportage</q>; <q>trial proceedings</q>;
                        and <q>letter to the editor</q>. Previously added tags for the name of the
                        newspaper and date were also verified and corrected where required.</item>
                </list>
                <p>Automated correction using search and replace with regular expressions was
                    necessarily limited to avoid introducing new errors, since we considered a
                    garbled word preferable to a <soCalled>correction</soCalled> leading to a wrong
                    word. We anticipated that the clear patterns in the observed errors would lend
                    themselves to more sophisticated correction processes using supervised machine
                    learning techniques. However this application proved beyond the scope of this
                    project.</p>
                <p>Given the modest size of the corpus and the research funding available it was
                    feasible to hand-tag genre, delivering accuracy benefits over automated
                    approaches. Although we considered automatic inclusion of metadata (for example,
                    within the TEI standard) as well as automatic part-of-speech tagging (valuable
                    for tasks such as document classification), we determined that plain text plus
                    article-level genre/date/newspaper metadata was sufficient for keyword analysis
                    in our project.</p>
                <p>Manual correction is inescapably a time-consuming process, although it does offer
                    collaborative benefits, since it involves all team members in the close
                    examination of texts. Some projects opt to offshore OCR correction, but ethical
                    considerations concerning the exploitation of foreign labour as well as quality
                    control concerns ruled out this option in our study. The efficiency of inputting
                    corrections was improved by using spelling and grammar error highlighting in
                    Microsoft Word. This phase took approximately one hundred hours, at an average
                    rate of 57.5 words per minute, which is comparable to that of an efficient
                    typist. Although this procedure was efficient for moderately corrupted text, and
                    easier to sustain over long work sessions, highly inaccurate scans rendered
                    typing from scratch necessary, as it was quicker than correcting the garbled OCR
                    output. When added to the lengthy OCR scanning process, the labour required to
                    correct scanned text does raise the question of whether OCR is the most
                    efficient way to digitize a medium-size corpus of historical newspapers to a
                    high degree of accuracy. </p>
                <p>As well as typing from original scans, we transcribed texts using a voice
                    recognition program (<name>Dragon Naturally Speaking 12</name>), another option
                    for the correction and input process. Typing was predominantly used, since it
                    tends to be quicker than transcriptions of dictation for corrections. For
                    inputting longer sections from scratch, dictation was slightly faster and more
                    convenient to use. However, it tends to fail <soCalled>silently</soCalled>, in
                    that it substitutes unrecognized words with other words, which a spell-checker
                    cannot detect. Typographical errors, on the other hand, are more likely to form
                    non-words that spell checkers can identify. Dictation is also more likely to
                    fail on names and uncommon words and proper nouns, precisely those words which
                    the study is most interested in identifying.</p>
                <p>In summary, while OCR achieves relatively accurate results (around 80%) on
                    historical newspaper collections such as the one used in this study, manual
                    correction is required to achieve high accuracy (around 98%). Depending on the
                    corpus size and the resources at hand, this two-step process may be no more
                    efficient than directly inputting the original texts from scratch.</p>
            </div>
            <div>
                <head>Measuring the Effect of Post-OCR Correction Using a Sample Task</head>
                <p>Determining the tenor of the Walworth case’s newspaper coverage and testing for
                    possible shifts over the course of trial was the object of our text mining
                    analysis, but the methods we selected to do so are relevant to wider debates
                    over the utility of OCR and post-OCR correction processes. In order to evaluate
                    changes in the popular assessment of the case we created two subsets of the
                    digitized corpus: Phase I (news accounts before the introduction of
                        <name>Mansfield Walworth</name>’s shocking letters), and Phase II (trial
                    coverage subsequent to the letters' introduction, including <name>Frank
                        Walworth</name>'s conviction and sentence of life in prison).<note>
                        <name>Frank Walworth</name> was pardoned four years after his conviction,
                        but this twist to the story attracted little attention from the press <ptr
                            target="#strange2010"/>. </note> We investigated which words varied at
                    statistically significant rates from Phase I to Phase II, particularly those
                    indicative of the sentiments stirred by the crime and the characters involved.
                    To undertake this analysis we used a list of <soCalled>judgment
                    words</soCalled>. Through a close reading of the texts and knowledge of common
                    words used in criminal trial reportage in this period the historian produced a
                    preliminary list of words of moral judgment and character assessment, which we
                    supplemented through the addition of similar words selected with the aid of
                    topic modelling of the corpus. Finally, we further augmented our list by adding
                    other forms of the selected words that appeared in the 2011 edition of the
                    American English Spell Checker Oriented Word Lists.<note> For information on the
                        Spell Checker Oriented Word List see <ref
                            target="http://wordlist.sourceforge.net/"
                            >http://wordlist.sourceforge.net/</ref>. The list, or
                            <soCalled>dictionary</soCalled>, is a concatenation of word lists
                        compiled for use in spell checkers. We are grateful for <name>Loretta
                            Auville</name>’s advice on this aspect of our study.</note> We chose the
                    statistical tool log likelihood ratio, since it is designed to measure variation
                    in the word frequency between two sections of a corpus <ptr
                        target="#dunning1993"/>. Most importantly, log likelihood ratio discerns
                    statistically significant word frequency variations which are highly likely to
                    appear as a result of true properties of the corpora, rather than by chance. By
                    calculating log likelihood ratio across the two phases of newspaper reportage,
                    we tested for changes in the popular judgment of the Walworth case; this test
                    also allowed us to analyze the effectiveness of post-OCR cleaning by comparing
                    the results of the task performed on the text before and after correction.<note>
                        A first principles approach to this question is also possible, but due to
                        the mathematical complexity of incorporating OCR errors into calculations
                        finding significant words with log likelihood ratio, we used an empirical
                        approach in this paper.</note>
                </p>
                <p>Log likelihood ratio, which identifies meaningful variation in word frequency in
                    one corpus relative to another <ptr target="#dunning1993"/>, produces a p-value
                    on the corresponding test statistic, which can be interpreted as the probability
                    of the observed word frequencies, given the null hypothesis that there is no
                    difference between the two corpora. For words with a p-value below some
                    significance level (for example p≤0.05) the null hypothesis may be rejected; in
                    other words, the difference in word frequency between the two corpora may be
                    considered statistically significant when the variation is highly unlikely to be
                    a result of chance. However, it is worth noting that while this holds for any
                    given word, if we use a given significance level to select a set of words, it
                    may still be likely that the result for at least one of the selected words may
                    appear to be significant by chance alone. Multiple hypothesis testing provides a
                    rubric for managing this phenomenon, for example by reducing the p-value used
                    for individual words. We chose not to pursue this approach; instead we
                    qualitatively analyzed words, identified by log likelihood ratio, which helped
                    to detect the minority of words incorrectly identified as significant. Because
                    we worked as an interdisciplinary team, the historian contributed to this
                    critical examination of the output of a statistical technique. </p>
                <p>Dunning introduced the log likelihood ratio as a tool for word frequency analysis
                    that would be more robust than the previously prevalent chi-squared test for
                    small samples of text. It has been used in previous studies comparing corpora,
                    for example looking at the proceedings of a 19th century British murder trial
                        <ptr target="#archerforthcoming"/>; the distinctive lexicon used in a
                    professional environment <ptr target="#rayson2000"/>
                    <note> Apparently by accident, this study uses an incorrect variant of the log
                        likelihood ratio. In the second equation the authors present on page 3, the
                        sum should run over all four cells of the contingency table (rather than
                        just those in the top row), and the observed and expected values for each of
                        these should be calculated. With a large corpus size relative to word
                        frequency, the ratio of observed to expected values for the bottom row cells
                        will be approximately 1 and hence the contribution of these cells will be
                        negligible. However, with a small corpus size relative to word frequency
                        these cells make a substantial contribution and should not be ignored.
                        Several open-source tools including Meandre (<ref
                            target="http://seasr.org/meandre">http://seasr.org/meandre</ref>), which
                        we used in this study, repeat this error. We modified the source code in
                        order to use the log likelihood ratio as it originally appeared in <ptr
                            target="#dunning1993"/>. This confirms the need for humanities scholars
                        to work with experts in computer science and digital humanities, to ensure a
                        deep understanding of statistical techniques, rather than rely on
                        off-the-shelf tools which may occasionally have inaccuracies in their
                        implementation. </note>; historical spelling variations <ptr
                        target="#baron2009"/>; and the lines of a particular character in a play
                        <ptr target="#mcintyre2010"/>. We did consider other tests, which have been
                    proposed as alternatives to the log likelihood ratio. For instance, Fisher’s
                    exact test <ptr target="#moore2004"/> calculates exactly what the log likelihood
                    ratio approximates but requires greater computational resources, while the
                    Mann-Whitney Ranks test <ptr target="#kilgarriff2001"/> considers the
                    distribution of word frequency within a corpus, as does the t-test <ptr
                        target="#paquot2009"/>. After reviewing these options we decided that log
                    likelihood ratio was the best option, due to its well-established use in the
                    comparison of corpora. </p>
                <p>As described in Section 4, the correction process was enhanced through the
                    inclusion of metadata about article genre. The two phases differed in genre mix,
                    since the reportage from Phase II was dominated by coverage of the Walworth case
                    trial proceedings. While we were interested in detecting changing public
                    opinion, differences in genre could possibly have obscured the shift we
                    anticipated. Where genre metadata was available, we restricted our analysis to
                    opinion articles about the case, consisting of editorials and letters to the
                    editor, since this genre is most likely to capture words of interest.
                    Furthermore, technical judgment words appearing in trial proceedings –
                    particularly those used by lawyers and the judge in court – indicate legal
                    constructions that may not have reflected public opinion. This caveat was
                    another reason for the restriction of the corpus to opinion articles using genre
                    metadata. As <ref target="#figure03">Figure 3</ref> shows, we compared the
                    original text without metadata; the original text restricted using genre
                    metadata; and the cleaned text also restricted using genre metadata.</p>
                <figure xml:id="figure03">
                    <head>Schematic of the datasets used in the experiments presented in this paper.
                        The word counts for each dataset for Phase I and Phase II are shown.</head>
                    <figDesc>Original and Clean Text displayed as a schematic</figDesc>
                    <graphic url="./resources/images/figure03.png"/>
                </figure>
                <p>Pre-processing of the texts was performed to improve the quality of results
                    returned. The following four steps were taken:</p>
                <list type="ordered">
                    <item>All text converted to lower case.</item>
                    <item>Punctuation removed and the possessive form <q>’s</q>.<note> A more
                            thorough stemming or lemmatisation approach was not performed but may be
                            useful in future. </note>
                    </item>
                    <item>Stopwords removed, such as <q>the</q> and <q>of</q>, from a standard
                        stopwords list<note> An in-house stopword list from <name>NICTA (National
                                ICT Australia)</name> was used. </note>
                    </item>
                    <item>Words identified from the custom-built list of 357 judgment words,
                        consisting primarily of adjectives, adverbs and abstract nouns.</item>
                </list>
                <p>These steps, as well as the log likelihood ratio calculations, were performed
                    using several open source tools.<note>Meandre (<ref
                            target="http://seasr.org/meandre">http://seasr.org/meandre</ref>), Monk
                            (<ref target="http://monkproject.org">http://monkproject.org</ref>) and
                        OpenNLP (<ref target="http://opennlp.apache.org"
                            >http://opennlp.apache.org</ref>). These tools were sufficiently
                        powerful for our study, though there are a range of other similar tools
                        available, such as Wmatrix (<ref target="http://ucrel.lancs.ac.uk/wmatrix/"
                            >http://ucrel.lancs.ac.uk/wmatrix/</ref>) and WordSmith (<ref
                            target="http://www.lexically.net/wordsmith/"
                            >http://www.lexically.net/wordsmith/</ref>). </note> The results of the
                    experiments are detailed in the following section.</p>
            </div>
            <div>
                <head>Results</head>
                <p>The tests we conducted involved comparing newspaper coverage of the Walworth case
                    over our two periods: Phase I (the crime, the arrest, the coroner’s inquest and
                    the trial’s opening); and Phase II (subsequent to the letters’ introduction up
                    to the verdict and sentencing). By using data with and without post-OCR
                    correction we were able to address our historical question and to evaluate the
                    effect of this correction.</p>
                <p>
                    <ref target="#figure04">Figure 4</ref> shows the ten Phase I words that appeared
                    at most significant frequency, measured by log likelihood ratio, for the three
                    datasets presented in <ref target="#figure03">Figure 3</ref>. The defendant is
                    the focus of early reportage, with terms such as <q>young</q>, and <q>son</q>
                    appearing, as well as the negative word <q>murderer</q> (considering that his
                    conviction had not yet occurred). The opinion article genre focuses on the
                    defendant’s family background, including the word <q>chancellor</q> (<name>Judge
                        Reuben Hyde Walworth</name>, Mansfield’s father), <q>albany</q> (the state
                    capital, where the defendant’s uncle lived) and <q>literary</q>, the last of
                    which referred to <name>Mansfield Walworth</name>’s career as a gothic novelist,
                    rather than his negative character traits. Even without OCR correction, most of
                    these words of interest were identified. Despite the presence of non-words
                    caused by OCR error, they do not appear in these top few words. Without
                    metadata, the words tend to focus on the minutiae of the murder scene, such as
                        <q>stairs</q>, <q>body</q> and <q>door</q>, rather than on more substantive
                    issues of character. This points to the need for historical researchers to
                    consider adding genre metadata prior to calculating log likelihood ratios.</p>
                <figure xml:id="figure04">
                    <head>Top 10 Phase I words for each dataset described in <ref target="#table02"
                            >Table 2</ref>, ranked by log-likelihood ratio. All words shown were
                        significant at p≤0.0001. No judgment words appear.</head>
                    <graphic url="./resources/images/figure04.png"/>
                    <figDesc>Venn diagram graphic of top 10 phase 1 words described in table
                        2</figDesc>
                </figure>
                <p>The same approach for Phase II yielded primarily legal terms, which disclosed
                    little about changing opinion. Therefore, we show in Figure 5 those words from
                    our judgment word list that occurred more frequently in this second trial period
                    at a statistically significant rate (using a significance level of p≤0.05). The
                    words <q>insanity</q> and <q>insane</q> may refer to both Frank and Mansfield,
                    since the defence suggested that the son may have suffered from a form of
                    madness inherited from his disturbed father. The terms <q>threats</q> and
                        <q>madman</q> reflect a new focus on the condemnation of Mansfield, although
                    there is some possibility <q>madman</q> could also refer to Frank. The words
                        <q>deliberation</q> and <q>deliberate</q> may negatively describe Frank’s
                    actions, but they may equally be procedural legal terms relating to the jury.
                    The differences between the datasets are less pronounced in this experiment,
                    aside from the fact that the original dataset contained more statistically
                    significant words, since it includes substantially more words overall (see <ref
                        target="#table01">Table 1</ref>for details). Some of these words suggest a
                    condemnation of Mansfield (<q>demon</q>) and potential approval of Frank
                        (<q>honor</q>), since he claimed he had killed his father to protect his
                    mother. This pattern suggests that using judgment words may be an alternative to
                    adding genre metadata, since these words implicitly refer to genre. <figure
                        xml:id="figure05">
                        <head>Frequent Phase II words for each dataset described in <ref
                                target="#table02">Table 2</ref>, using judgment words which are
                            statistically significant at p≤0.05.</head>
                        <graphic url="./resources/images/figure05.png"/>
                        <figDesc>Frequent Phase II words</figDesc>
                    </figure> Fortunately, ambiguities such as whether <q>insanity</q> refers to
                    Frank or Mansfield, or whether <q>deliberate</q> refers to Frank’s shooting, the
                    judge or the jury, may be resolved using more sophisticated techniques. For
                    example, words may be matched to characters in the case through sentence blocks,
                    word proximity, or full-scale parsing for semantic structure. Such techniques
                    typically require very high quality text to be effective. For words with
                    relatively low frequencies, manually investigating the contexts of occurrences
                    can also be used. <table xml:id="table02">
                        <head>Performance of Original with Metadata and Original Datasets compared
                            to Clean with Metadata dataset. The Clean with Metadata dataset returned
                            545 significant words of which 16 were judgment words. A significance
                            level of p≤0.05 was used.</head>
                        <row role="label">
                            <cell>Significant Words</cell>
                            <cell>Precision</cell>
                            <cell>Recall</cell>
                            <cell>Significant Judgment Words</cell>
                            <cell>Judgment Words Precision</cell>
                            <cell>Judgment Words Recall</cell>
                        </row>
                        <row>
                            <cell role="label">Original with Metadata</cell>

                            <cell>751</cell>
                            <cell>0.48</cell>
                            <cell>0.66</cell>
                            <cell>23</cell>
                            <cell>0.65</cell>
                            <cell>0.93</cell>
                        </row>
                        <row>
                            <cell role="label">Original</cell>
                            <cell>2852</cell>
                            <cell>0.12</cell>
                            <cell>0.61</cell>
                            <cell>38</cell>
                            <cell>0.34</cell>
                            <cell>0.81</cell>
                        </row>

                    </table>
                </p>
                <p>
                    <ref target="#table02">Table 2</ref> shows the precision and recall of the
                    results of words with a significance level of p≤0.05 for the original with genre
                    metadata and original datasets compared to the <q>gold standard</q>, that is,
                    the clean with metadata dataset. Recall refers to the proportion of words
                    significant in the clean with metadata dataset, which are also significant in
                    the original (with or without metadata) dataset. Precision is the proportion of
                    words significant in the original (with or without metadata) dataset, which are
                    also significant in the clean with metadata dataset. This evaluation methodology
                    allowed us to drill down deeper than we could by using the small shortlist of
                    words shown in <ref target="#figure04">Figure 4</ref> and <ref
                        target="#figure05">Figure 5</ref>, and it revealed a strong discrepancy
                    between the datasets. </p>
                <p>The precision scores of 0.48 and 0.12 indicate that many words were incorrectly
                    identified as significant, while the recall scores of 0.66 and 0.61 suggest that
                    a substantial portion of significant words was missed. The original dataset
                    approached the original with metadata dataset on recall, but it had much lower
                    precision, indicating that it returned many results with limited usefulness.
                    Some non-words induced by OCR error appear at a statistically significant level
                    in the original and original with metadata lists, such as <q>ol</q> (instead of
                        <q>of</q>) and <q>ihe</q> (instead of <q>the</q>). Using the judgment word
                    list, the precision scores of 0.65 and 0.34 suggest, again, that many <q>false
                        positive</q> words were identified, with the problem magnified without
                    adding in genre metadata. The recall results of 0.93 and 0.81 were stronger for
                    the judgment word list, however, which is of interest given the emotive nature
                    of the case and its coverage. Overall, there was a substantial discrepancy in
                    the words identified in the original datasets, with and without metadata,
                    compared to the clean with metadata dataset. This confirms that OCR errors can,
                    indeed, influence later analysis of this nature. </p>
                <p>It is worth examining in detail one example in which an OCR error produced a
                    judgment word found to be significant using the original with metadata dataset,
                    but not by using the clean with metadata dataset. In the original with metadata
                    dataset, the word <q>maudlin</q> was identified as occurring significantly more
                    in Phase I, with a frequency of 3 compared to 0 in Phase II. However, there was
                    one instance of <q>maudlin</q> occurring in Phase II in the clean with metadata
                    dataset which was missed due to an OCR error – swapping <q>maudlin</q> for
                        <q>inaudlin</q>. In the clean with metadata dataset the frequency counts of
                    3 for Phase I versus 1 for Phase II were not significantly different. While
                    these frequencies may seem low, relative scarcity does not indicate low
                    significance. In fact, our test indicated the opposite to be the case.</p>
                <p>The contexts of <q>maudlin</q> appear in <ref target="#table03">Table 3</ref>,
                    which indicates that the uses of the term in Phase I occurred in the context of
                    disapproval of Frank’s parricidal motive and cool demeanor. The use of the term
                    in Phase II was different, we discovered, because it referred to the state of
                    mind of another murderer in an earlier trial, in which a plea of insanity had
                    been successful. This shows that further work is required to identify the
                    implications of word use based on their contexts. Indeed, it seems that there
                    was a suggestive change in the frequency of <q>maudlin</q> between Phase I and
                    Phase II. </p>
                <table xml:id="table03">
                    <head>Contexts of the judgment word <q>maudlin</q>.</head>
                    <row role="label">
                        <cell>Newspaper</cell>
                        <cell>Date</cell>
                        <cell>Phase</cell>
                        <cell>Correct in Original?</cell>
                        <cell>Context</cell>
                    </row>
                    <row>
                        <cell>
                            <title rend="italic">NY Tribune</title>
                        </cell>
                        <cell>1873-06-04</cell>
                        <cell>I</cell>
                        <cell>Yes</cell>
                        <cell>
                            <quote rend="inline" source="undocumented">We protest in advance against such
                                resort to maudlin sentimentality</quote>
                        </cell>
                    </row>
                    <row>
                        <cell>
                            <title rend="italic">NY Tribune</title>
                        </cell>
                        <cell>1873-06-05</cell>
                        <cell>I</cell>
                        <cell>Yes</cell>
                        <cell>
                            <quote rend="inline" source="#undocumented">There’s a [sic] something indefinable
                                about this maudlin sentimentalism that throws a glow of heroism
                                round the murder</quote>
                        </cell>
                    </row>
                    <row>
                        <cell>
                            <title rend="italic">The Saratogian</title> (quoting <title
                                rend="italic">NY Tribune</title>)</cell>
                        <cell>1873-06-12</cell>
                        <cell>I</cell>
                        <cell>Yes</cell>
                        <cell>
                            <quote rend="inline" source="#undocumented">We protest in advance against such
                                resort to maudlin sentimentality</quote>
                        </cell>
                    </row>
                    <row>
                        <cell>
                            <title rend="italic">NY Tribune</title>
                        </cell>
                        <cell>1873-07-04</cell>
                        <cell>II</cell>
                        <cell>No</cell>
                        <cell>
                            <quote rend="inline" source="#undocumented">the maudlin sorrow of a
                                drunkard</quote> (referring to another case where insanity was
                            successfully pled, an outcome the author critiques)</cell>
                    </row>
                </table>
                <p>Overall, the cleaning of the data was not essential to achieving results of
                    interest on the two-phase comparison task, since many significant words could
                    still be identified. Still, there were substantial differences between the
                    results of the clean and original datasets, as significant words were missed and
                        <q>false positives</q> were generated. The adding of genre metadata
                    permitted the filtering of more significant words through the use of opinion
                    articles, something that was not possible with the original dataset. Our list of
                    judgment words likely performed a similar filtering function to the genre
                    metadata, though it lacks the flexibility to detect unexpected words. </p>
            </div>
            <div>
                <head>Future Work</head>
                <p>Moving beyond the analysis presented in this paper, it would be desirable to
                    identify which words refer to which characters in the case. With the clean
                    corpus we now have at our disposal, this identification could be achieved
                    through the automated tagging of syntactic metadata. This analysis would allow
                    us to track public opinion at the level of the individual with greater
                    precision. Words which may refer to multiple individuals may be disambiguated
                    using techniques such as sentence blocks, word proximity and semantic parsing.
                    It is expected that this more complex task would show greater differences
                    between the raw OCR output and the corrected text, since it depends on the
                    presence of grammatically well-formed sentences rather than word counts alone. </p>
                <p>An alternative approach that may be useful in similar projects would involve
                    identifying which terms are distinctive <q>hallmarks</q> of particular
                    subcorpora (for instance, selected on the basis of date or news source). A
                    feature selection metric such as mutual information could be used to identify
                    which terms are most predictive in classifying documents as belonging to
                    particular subcorpora. Turning from supervised to unsupervised learning, our
                    team anticipates producing results based on topic modelling, a common strategy
                    in the digital humanities which has been applied to US historical newspapers
                        <ptr target="#newman2006"/>, <ptr target="#yang2011"/>, and <ptr
                        target="#nelson2012"/>. The goal of such projects is to find topics in large
                    volumes of newspaper reportage, and to track changes as indices of shifts in
                    public discourse. The effect of OCR errors on such topic models is also an
                    active subject of research, and our results suggest that scholars consider this
                    issue thoroughly before undertaking large-scale projects <ptr
                        target="#walker2010"/>. </p>
                <p>The broader ambition of this research project is to situate the Walworth case in
                    its wider historical context. Can it be shown through text mining that
                    prevailing understandings of masculine honour, morality and family values were
                    challenged by this dramatic incident? In future work we will compare our
                    digitized collection with larger newspaper corpora. <name>Google n-grams</name>
                    <note>
                        <ref target="http://books.google.com/ngrams"
                            >http://books.google.com/ngrams</ref>
                    </note> is a common and accessible choice for researchers, but its contents are
                    different from our corpus in both format (the full text of its sources is
                    unavailable) and in genre (it covers non-fiction, arcane technical writings and
                    literary works). A more promising collection is <name>Gale’s Nineteenth Century
                        Newspaper collection</name>.<note>
                        <ref target="http://gdc.gale.com/products/19th-century-u.s.-newspapers/"
                            >http://gdc.gale.com/products/19th-century-u.s.-newspapers/</ref>
                    </note> If it becomes fully searchable it will provide a vast dataset from which
                    subsets of texts (such as editorials on domestic homicide) can be selected to
                    evaluate the distinct and shared features of the Walworth case’s coverage.
                    Nevertheless, there are reasons for caution. In large corpora such as these, OCR
                    induced errors will remain an issue, since hand correction of texts on a vast
                    scale is infeasible.</p>
            </div>
            <div>
                <head>Conclusion</head>
                <p>Digital humanities scholars have been drawn to text mining as a technique well
                    suited to the analysis of historical newspapers, since it allows for meaning to
                    be drawn from volumes of text that would be unmanageable for an individual
                    researcher to absorb and analyze. It provides a tool that can test hypotheses
                    generated through traditional historical analysis, and ideally, generate new
                    possibilities for study that could not have been generated through close reading
                    alone. However, the digitization of historical texts is a complex and
                    time-consuming process which is worthy of consideration in itself. Through the
                    example of the Walworth murder case’s newspaper coverage, this paper has
                    outlined the two-step digitization process our team undertook: first, performing
                    OCR scans from original newspapers and image files; and second, cleaning and
                    post-processing to ensure that all text included is accurate, relevant, and
                    labelled with genre metadata. We have provided an original, detailed methodology
                    for conducting digitization of a medium-size corpus. </p>
                <p>OCR, we determined, is effective in digitising historical newspapers to roughly
                    80% accuracy. However, to achieve high levels of accuracy (around 98%), the
                    labour-intensive cleaning required to remove OCR errors means the two-step
                    process may be no more efficient than manually inputting texts from scratch, a
                    procedure that suits small- to medium-scale projects. While our research
                    involved both scanning and cleaning texts, historical researchers more commonly
                    perform keyword searches on existing databases of historical documents to
                    conduct text mining analysis <ptr target="#hitchcock2013"/>. Importantly, our
                    study shows that the result set may contain OCR errors, irrelevant and duplicate
                    content; similarly, insufficient metadata can generate spurious results that are
                    difficult to detect. Our method proposed for the cleaning process, as well as
                    our appraisal of the value of this step, signals the way forward to overcome
                    this problem.</p>
                <p>The value of correcting OCR output from around 80% accuracy to near 100% is an
                    important consideration for researchers, in view of the labour-intensive process
                    required. We demonstrated this empirically by performing a sample task of
                    interest on both the clean and original versions of the corpora. This task
                    involved finding words, including those from a list of pertinent judgment words,
                    which changed in frequency across two phases of the case’s reportage. Log
                    likelihood ratio was used as a test for statistical significance. With the
                    uncorrected OCR output it was possible to identify words appearing significantly
                    more frequently in one time period relative to another, but a substantial
                    proportion were missed and <q>false positives</q> were introduced. The cleaning
                    was thus desirable but not essential. The addition of genre metadata led to
                    results of greater interest, since it allowed a focus on articles more clearly
                    relevant to the research question. This paper is unique in situating OCR error
                    correction in a digitization workflow also involving content selection,
                    document-level metadata enhancement and practical time and cost constraints, as
                    it evaluates this text cleaning phase holistically. </p>
                <p>Like many digital humanities projects, this study underlines the value of input
                    from researchers across the disciplines of history and computer science to
                    design the project, select the methodology, implement the tasks and interpret
                    the results <ptr target="#ayers2013"/>; <ptr target="#nelson2012"/>. Without
                    this combination of skills and expertise, as well as facilitative research
                    funding, such studies are unfeasible. Our team’s scientific expertise allowed us
                    to customize software for text mining analysis rather than using off-the-shelf
                    solutions, which gave us full control over the integrity of the tools used,
                    while the historian posed the research question and critically examined test
                    results against the initial close reading of the case. This collaborative,
                    interdisciplinary model will continue to be critical to foster robust research
                    in the field of digital humanities. </p>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="archerforthcoming" label="Archer forthcoming" key="[unlisted]">.
                    Archer, Dawn. <title rend="quotes">Tracing the crime narratives within the
                        Palmer Trial (1856): From the lawyer’s opening speeches to the judge’s
                        summing up.</title>
                </bibl>
                <bibl xml:id="arlitsch2004" label="Arlitsch2004" key="arlitsch2004">. Arlitsch,
                    Kenning and John Herbert. <title rend="quotes">Microﬁlm, paper, and OCR: issues
                        in newspaper digitization.”</title>
                    <title rend="italic">Microform and Imaging Review</title>, 33.2: 58–67.</bibl>
                <bibl xml:id="ayers2013" label="Ayers 2013" key="ayers2013">. Ayers, Edward L.
                        <title rend="italic">Does Digital Scholarship have a Future?.”</title>
                    <title rend="italic">EDUCASEreview</title>, 48.4: 24-34.</bibl>
                <bibl xml:id="baron2009" label="Baron 2009" key="baron2009">.Baron, Alistair, Paul
                    Rayson, and Dawn Archer. <title rend="quotes">Word frequency and key word
                        statistics in corpus linguistics.</title>
                    <title rend="italic">Anglistik</title>20.1 (2009): 41-67.</bibl>
                <bibl xml:id="dunning1993" label="Dunning 1993" key="dunning1993">. Dunning, Ted.
                        <title rend="quotes">Accurate Methods for the Statistics of Surprise and
                        Coincidence</title>. <title rend="italic">Computational
                    Linguistics</title>19.1 (1993): 61-74.</bibl>
                <bibl xml:id="eder2013" label="Eder 2013" key="eder2013">. Eder, Maciej. <title
                        rend="quotes">Mind your Corpus: Systematic Errors in Authorship
                        Attribution.</title>” <title rend="italic">Literary and Linguistic
                        Computing</title> doi: 10.1093/llc/fqt039 (2013).</bibl>
                <bibl xml:id="hitchcock2013" label="Hitchcock 2013" key="hitchcock2013">. Hitchcock,
                    Tim. <title rend="italic">Confronting the Digital, or How Academic History
                        Writing Lost the Plot.</title>” <title rend="italic">Cultural and Social
                        History</title> 10.1 (2013)</bibl>
                <bibl xml:id="holley2009" label="Holley 2009" key="holley2009b">. Holley, Rose.
                        <title rend="quotes">How Good Can It Get? Analysing and Improving OCR
                        Accuracy in Large Scale Historic Newspaper Digitization
                        Programs.</title> <title rend="italic">D-Lib Magazine </title>15.3/4 (2009). </bibl>
                <bibl xml:id="kagan2009" label="Kagan 2009" key="kagan2009">. Kagan, Jerome. <title
                        rend="italic">The Three Cultures: Natural Sciences, Social Sciences, and the
                        Humanities in the 21st Century</title>. Cambridge: Cambridge University
                    Press, 2009.</bibl>
                <bibl xml:id="kilgarriff2001" label="Kilgarriff 2001" key="kilgarriff2001">.
                    Kilgarriff, Adam. <title rend="italic">Comparing corpora.</title>
                    <title rend="italic">International journal of corpus linguistics </title>6.1
                    (2001): 97-133.</bibl>
                <bibl xml:id="knoblock2007" label="Knoblock 2007" key="knoblock2007">. Knoblock,
                    Craig, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, eds. <title
                        rend="quotes">Special Issue on Noisy Text Analytics</title>. <title
                        rend="italic">International Journal on Document Analysis and
                        Recognition</title> 10.3/4 (2007). </bibl>
                <bibl xml:id="lopresti2008" label="Lopresti 2008" key="lopresti2008">. Lopresti,
                    Daniel. <title rend="quotes">Optical Character Recognition Errors and their
                        Effects on Natural Language Processing.</title> <title rend="italic"
                        >Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text
                        Data.</title> ACM, 2008.</bibl>
                <bibl xml:id="mcintyre2010" label="McIntyre 2010" key="mcintyre2010">. McIntyre,
                    Dan, and Dawn Archer. <title rend="quotes">A corpus-based approach to mind
                        style.</title>
                    <title rend="italic">Journal of Literary Semantics</title> 39.2 (2010):
                    167-182.</bibl>
                <bibl xml:id="moore2004" label="Moore 2004" key="moore2004">. Moore, Robert C.
                        “<title rend="italic">On log-likelihood-ratios and the significance of rare
                        events.</title>
                    <title rend="italic">Proceedings of the 2004 Conference on Empirical Methods in
                        Natural Language Processing</title>. 2004.</bibl>
                <bibl xml:id="nelson2010" label="Nelson 2010" key="nelson2010b">. Nelson, Robert K.
                        <title rend="quotes">Mining the <emph>Dispatch</emph> –
                        Introduction.</title>
                    <title rend="italic">Mining the <emph>Dispatch</emph> website.</title>
                    <ref target="http://dsl.richmond.edu/dispatch/pages/home">
                        http://dsl.richmond.edu/dispatch/pages/home</ref>.</bibl>
                <bibl xml:id="nelson2012" label="Nelson 2012" key="nelson2012c">. Nelson, Robert K.
                        <title rend="italic">A Conversation with Digital Historians</title>, <title
                        rend="italic">Southern Spaces</title>, 31 January 2012. <ref
                        target="http://www.southernspaces.org/2012/conversation-digital-historians"
                        >www.southernspaces.org/2012/conversation-digital-historians</ref>.</bibl>
                <bibl xml:id="newman2006" label="Newman and Block 2006" key="newman2006"> Newman,
                    David J., and Sharon Block. <title rend="quotes">Probabilistic topic
                        decomposition of an eighteenth‐century American newspaper</title>. <title
                        rend="italic">Journal of the American Society for Information Science and
                        Technology</title> 57.6 (2006): 753-767.</bibl>
                <bibl xml:id="obrien2010" label="O'Brien 2010" key="obrien2010">. O’Brien,
                        Geoffrey. <title rend="italic">The Fall of the House of Walworth: A Tale of
                        Madness and Murder in Gilded Age America</title>. New York: Henry Holt and
                    Company, 2010. </bibl>
                <bibl xml:id="paquot2009" label="Paquot and Bestgen 2009" key="paquot2009">. Paquot,
                    Magali, and Yves Bestgen. <title rend="quotes">Distinctive words in academic
                        writing: A comparison of three statistical tests for keyword
                        extraction</title>. <title rend="italic">Language and Computers</title> 68.1
                    (2009): 247-269.</bibl>
                <bibl xml:id="powell2004" label="Powell 2004" key="powell2004"> Powell, Kerry.
                        <title rend="italic">The Cambridge Companion to Victorian and Edwardian
                        Theatre</title>. Cambridge: Cambridge University Press, 2004. </bibl>
                <bibl xml:id="rayson2000" label="Rayson and Garside 2000" key="rayson2000">. Rayson,
                    Paul, and Roger Garside. <title rend="quotes">Comparing corpora using frequency
                        profiling</title>. <title rend="italic">Proceedings of the workshop on
                        Comparing Corpora</title>. Association for Computational Linguistics,
                    2000.</bibl>
                <bibl xml:id="rice1993" label="Rice et al. 1993" key="rice1993"> Rice, Stephen V.,
                    Junichi Kanai, and Thomas A. Nartker. <title rend="quotes">An Evaluation of OCR
                        Accuracy</title>. Information Science Research Institute, 1993 Annual
                    Research Report (1993): 9-20.</bibl>
                <bibl xml:id="stein2006" label="Stein et al. 2006" key="stein2006">. Stein, Sterling
                    Stuart, Shlomo Argamon, and Ophir Frieder. “<title rend="quotes">The effect of
                        OCR errors on stylistic text classification</title>.” <title rend="italic"
                        >Proceedings of the 29th annual international ACM SIGIR conference on
                        Research and development in information retrieval</title>. ACM, 2006.</bibl>
                <bibl xml:id="strange2010" label="Strange 2010" key="strange2010">. Strange,
                    Carolyn. <title rend="quotes">The Unwritten Law of Executive Justice: Pardoning
                        Patricide in Reconstruction-Era New York</title>, <title rend="italic">Law
                        and History Review</title> 28. 4 (2010): 891-30.</bibl>
                <bibl xml:id="strapparava2008"
                    label="Strapparava and Mihalcea                     2008" key="strapparava2008">
                    Strapparava, Carlo and Rada Mihalcea. <title rend="quotes">Learning to Identify
                        Emotions in Texts</title>
                    <title rend="italic">SAC: Proceedings of the 2008 ACM symposium on Applied
                        computing</title> (2008): 1556-1560.</bibl>
                <bibl xml:id="svensson2010" label="Svensson 2010" key="svensson2010"> Svensson,
                    Patrik. “<title rend="quotes">The Landscape of the Digital
                        Humanities</title>.<title rend="italic">Digital Humanities Quarterly</title>
                    4.1 (2010).</bibl>
                <bibl xml:id="tanner2009" label="Tanner et al. 2009" key="tanner2009"> Tanner,
                    Simon, Trevor Muñoz, and Pich Hemy Ros. <title rend="quotes">Measuring Mass Text
                        Digitization Quality and Usefulness</title>. <title rend="italic">D-Lib
                        Magazine </title>15.7/8 (2009).</bibl>
                <bibl xml:id="walker2010" label="Walker et al. 2010" key="walker2010"> Walker,
                    Daniel D., William B. Lund and Eric K. Ringger. <title rend="quotes">Evaluating
                        Models of Latent Document Semantics in the Presence of OCR Errors</title>.
                        <title rend="italic">Proceedings of the 2010 Conference on Empirical Methods
                        in Natural Language Processing,</title> Boston: Association for
                    Computational Linguistics, October 2010. (2010): 240-50. </bibl>
                <bibl xml:id="wiebe2005" label="Wiebe 2005" key="wiebe2005">Wiebe, Janyce, Theresa
                    Wilson, and Claire Cardie. <title rend="quotes">Annotating Expressions of
                        Opinions and Emotions in Language</title>. <title rend="italic">Language
                        Resources and Evaluation</title>39.2/3 (2005): 165-210.</bibl>
                <bibl xml:id="williams2011" label="Williams 2011" key="williams2011"> Williams,
                    Jeffrey J. <title rend="quotes">The Statistical Turn in Literary
                    Studies</title>. <title rend="italic">The Chronicle Review</title> 57.18 (2011):
                    B14-15.</bibl>
                <bibl xml:id="yang2011" label="Yang et al. 2011" key="yang2011"> Yang, Tze-I.,
                    Andrew J. Torget, and Rada Mihalcea. “Topic modelling on historical
                        newspapers.” <title rend="italic">Proceedings of the 5th ACL-HLT Workshop on
                        Language Technology for Cultural Heritage, Social Sciences, and
                        Humanities</title>. Association for Computational Linguistics, 2011. </bibl>

            </listBibl>
        </back>
    </text>
</TEI>
