<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI
  xmlns="http://www.tei-c.org/ns/1.0"
  xmlns:cc="http://web.resource.org/cc/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">

  <!-- BEGIN TEI HEADER ELEMENTS -->
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title type="article" xml:lang="en">Advances in Digital Music
          Iconography: Benchmarking the detection of musical instruments in
          unrestricted, non-photorealistic images from the artistic
          domain</title>
        <dhq:authorInfo>
          <dhq:author_name>Matthia <dhq:family>Sabatelli</dhq:family></dhq:author_name>
          <dhq:affiliation>Montefiore Institute</dhq:affiliation>
          <email>m.sabatelli@uliege.be</email>
          <dhq:bio><p>Matthia Sabatelli is a Ph.D. candidate in Machine Learning
            at the Department of Electrical Engineering and Computer Science of
            the University of Liège where he is supervised by Dr. Pierre Geurts.
            His main research interests revolve around the transferability and
            scalability of deep neural networks whose generalization properties
            are studied under the lens of different machine learning paradigms
            ranging from computer vision to reinforcement learning.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Nikolay <dhq:family>Banar</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>nicolae.banari@uantwerpen.be</email>
          <dhq:bio><p>Nikolay Banar is a Ph.D candidate at the University of
            Antwerp (Belgium). His scientific interests lie with the
            intersection of machine learning and humanities.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Marie <dhq:family>Cocriamont</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Museums of Art and History</dhq:affiliation>
          <email>marie_cocriamont@hotmail.com</email>
          <dhq:bio><p>Marie Cocriamont obtained her Masters degree in
            Musicology at the University of Ghent in 2016. She specialized in
            the comparison of didactic methods in Classical Arabic music.
            In 2019 she started working as a scientific assistant at the Royal
            Museums of Art and History, where she mainly works as an annotator
            for the research project INSIGHT (Intelligent Neural Systems as
            InteGrated Heritage Tools).</p></dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Eva <dhq:family>Coudyzer</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Museums of Art and History</dhq:affiliation>
          <email>eva.coudyzer@kikirpa.be</email>
          <dhq:bio><p>Eva Coudyzer obtained a Master in Art History and
            Archaeology in 2004 at the Vrije Universiteit Brussel. She worked
            in documentation centers and collection management services in
            several cultural organizations in Belgium. In 2009 she started
            working as a scientific assistant at the Royal Museums of Art and
            History, specializing in collection management systems. She was
            coordinator and partner in several national and international
            digitization projects with a main focus on linking and publishing
            collections with the use of standardized controlled vocabularies.
            She currently works as a scientific assistant at the information
            center of the <ref target="http://www.kikirpa.be">Royal Institute for Cultural Heritage</ref>
            where she participates in the development of the collection
            management system and the valorization of the collection in
            digitization projects.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Karine <dhq:family>Lasaracina</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Museums of Fine Arts of Belgium, Brussels</dhq:affiliation>
          <email>karine.lasaracina@fine-arts-museum.be</email>
          <dhq:bio><p>Karine Lasaracina has master degrees in art history and
            journalism. She joined the RMFAB in 1999 and is now head of the
            Digital Museum unit. From the very beginning of her career, she has
            been interested in the concept of digital management of heritage
            data. A current focus is the development of digital applications
            that can support enriched visitor experiences in the museum through
            the implementation of various innovative technological solutions,
            for example virtual reality tools, multimedia narratives and virtual
            exhibitions. Promoter of various ongoing research projects, she
            also works on Data Interoperability, Open Science, the development
            of Artificial Intelligence to serve the museums, as well as
            innovation in the field of images of artworks (reproduction,
            storage, preservation and sharing).</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Walter <dhq:family>Daelemans</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>walter.daelemans@uantwerpen.be</email>
          <dhq:bio><p>Walter Daelemans is professor of Computational
            Linguistics at the University of Antwerp and research director of
            the CLiPS (Computational Linguistics, Psycholinguistics and
            Sociolinguistics) research centre. His expertise is in Natural
            Language Processing and Machine Learning and applications in
            automatic text analysis and computational stylometry.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Pierre <dhq:family>Geurts</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Liège</dhq:affiliation>
          <email>p.geurts@uliege.be</email>
          <dhq:bio><p>Pierre Geurts is professor in computer science at the
            University of Liège. His research interests concern the design,
            the empirical, and the theoretical analyses of machine learning
            algorithms, with emphasis on scalability, interpretability, and
            usability of these algorithms. He develops real-world applications
            of these algorithms in various domains, including computational and
            systems biology, computer vision, and digital humanities.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Mike <dhq:family>Kestemont</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>mike.kestemont@uantwerpen.be</email>
          <dhq:bio><p>Mike Kestemont is research professor in Digital Text
            Analysis at the University of Antwerp (Belgium). His expertise
            lies in the application of computational methods to the Humanities,
            in particular premodern literature. With F. Karsdorp and A. Riddell,
            he has co-authored the monograph <title rend="italic">Humanities
            Data Analysis: Case Studies with Python</title>, which will appear
            with Princeton University Press in early 2021.</p>
          </dhq:bio>
        </dhq:authorInfo>
      </titleStmt>
      <publicationStmt>
        <publisher>Alliance of Digital Humanities Organizations</publisher>
        <publisher>Association for Computers and the Humanities</publisher>
        <idno type="DHQarticle-id">000517</idno>
        <idno type="volume">014</idno>
        <idno type="issue">4</idno>
        <date></date>
        <dhq:articleType>article</dhq:articleType>
        <availability>
          <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <p>This is the source</p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <classDecl>
      <taxonomy xml:id="dhq_keywords">
        <bibl>DHQ classification scheme; full list available at<ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
      </taxonomy>
      <taxonomy xml:id="authorial_keywords">
        <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
      </taxonomy>
      </classDecl>
    </encodingDesc>
      <profileDesc>
        <langUsage>
          <language ident="en" extent="original"/>
          </langUsage>
          <textClass>
            <keywords scheme="#dhq_keywords">
              <list type="simple">
                <item></item>
              </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
              <list type="simple">
                <item></item>
              </list>
            </keywords>
          </textClass>
      </profileDesc>
      <revisionDesc>
        <change when="2020-09-18" who="Taylor Arnold">Created file</change>
      </revisionDesc>
    </teiHeader>
    <!-- END TEI HEADER ELEMENTS -->

    <!-- BEGIN TEXT -->
    <text xml:lang="en" type="original">
      <!-- FRONT TEXT -->
      <front>
        <dhq:abstract>
          <p>In this paper, we present MINERVA, the first benchmark dataset for
            the detection of musical instruments in non-photorealistic,
            unrestricted image collections from the realm of the visual arts.
            This effort is situated against the scholarly background of music
            iconography, an interdisciplinary field at the intersection of
            musicology and art history. We benchmark a number of
            state-of-the-art systems for image classification and object
            detection. Our results demonstrate the feasibility of the task
            but also highlight the significant challenges which this artistic
            material poses to computer vision. We evaluate the system to an
            out-of-sample collection and offer an interpretive discussion of
            the false positives detected. The error analysis yields a number
            of unexpected insights into the contextual cues that trigger the
            detector. The iconography surrounding children and musical
            instruments, for instance, shares some core properties, such as an
            intimacy in body language.</p>
        </dhq:abstract>
        <dhq:teaser>
          <p>In this paper, we present MINERVA, the first benchmark dataset for
            the detection of musical instruments in non-photorealistic,
            unrestricted image collections from the realm of the visual arts.</p>
        </dhq:teaser>
      </front>

      <!-- BODY TEXT -->
      <body>
        <div><head>Introduction: the era of the pixel</head>
        <p>The Digital Humanities constitute an intersectional community of praxis,
        in which the application of computing technologies in various subdisciplines
        in the <emph>Geisteswissenschaften</emph> plays a significant role. Surveys
        of the history of the field
        <ptr target="#hockey2004"/>
         have stressed that most of the
        seminal applications of computing technology were heavily, if not exclusively,
        text-oriented: due to the hardware and software limitations of the time,
        analyses of image data (but also audio or video data) remained elusive and out
        of practical reach until relatively late, certainly at a larger scale. In the
        past decade, the application of deep neural networks has significantly pushed
        the state of the art in computer vision, leading to impressive advances in
        tasks such as image classification or object detection <ptr target="#lecun2015"/>
        <ptr target="#schmidhuber2015"/>. Even more recently, improvements in the field of computer
        vision have started to find practical applications in study domains outside of
        strict machine learning, such as physics, medicine or even astrology. Supported
        by this technology's (at times rather naive) coverage in the popular media,
        the communis opinio has been eager to herald the advent of the "Era of the
        Pixel".</p>
        <p>In the Digital Humanities too, the potential of computer vision is nowadays
        increasingly recognized. A programmatic duet of two recent articles on "distant
        viewing" in the field's flagship journal <ptr target="#wevers2020"/>
        <ptr target="#arnold2019"/> leads the way in this respect, emphasizing the privileged role
        these new methodologies can play in the exploration of large data collections
        in the Humanities. The present paper too is situated in a multidisciplinary
        project in which we investigate how modern artificial intelligence can support
        GLAM institutions (galleries, libraries, archives, and museums) in cataloguing
        and curating their rapidly expanding digital assets. As a case study, we shall
        work with non-photorealistic depictions of musical instruments in the artistic
        domain.</p>
        <p>The structure of this paper is as follows. First, we motivate and
        contextualize our case study of musical instruments from within the scholarly
        framework of music iconography and computer vision, but also from the more
        pragmatic context of the research project from which this focus has emerged.
        We go on to describe the construction and characteristics of an annotated
        benchmark dataset, the MINERVA dataset, that
        will be released together with this paper, through which we hope to stimulate
        further research in this area. Using this benchmark data, we stress-test the
        available technology for the identification and detection of objects in images
        and discuss the current limitations of systems. To illustrate the broader
        relevance of our approach, we apply the trained benchmark system 'in the wild',
        on unseen and out-of-sample heritage data, followed by a quantitative and
        qualitative evaluation of the results. Finally, we identify what seem to be the
        most relevant directions for future research.</p>
        </div>
        <div><head>Motivation</head>
        <div><head>Music iconography</head>
        <p>The present paper must be understood against the wider scholarly
        background of music iconography, a Humanities field of inquiry with a rich,
        interdisciplinary history in its own right. <ptr target="#buckley1998"/> concisely defined
        music iconography as a field being "concerned with the study of the visual
        representation of musical topics. Its primary materials include portraits
        of performers and composers, illustrations of instruments, occasions of
        music-making, and the use of musical imagery for purposes of metaphorical
        or allegorical allusion". Because of this wide range of topics, at the
        intersection of art history and musicology <ptr target="#baldassarre2007"/>
        <ptr target="#baldassarre2008"/>, the field takes pride of its interdisciplinarity.</p>
        <p>Music iconography deliberately adopts a "methodological
        plurality" <ptr target="#baldassarre2007"/> which is increasingly complemented with digital
        approaches. A major achievement in this respect has been the establishment
        (in 1971) and continued expansion and curation of an international digital
        inventory for musical iconography, the <emph>Répertoire International
        d'Iconographie Musicale</emph> (RIDIM). Now publicly available as an online
        web resource (<ref target="https://ridim.org/">https://ridim.org/</ref>), RIDIM functions as a reference image
        database, designed to facilitate the efficient yet powerful description and
        discovery of music-related art works <ptr target="#green2013"/>. The need for
        such an international inventory has been acknowledged as early as 1929 and
        its significant scope facilitates the international study of music-related
        phenomena and their depiction across the visual arts.</p>
        <p>Music iconography has an important tradition of focused studies targeting
        the deep, interpretive analysis of individual artworks or small collections
        of them. Such hermeneutic case studies have the advantage of depth, but
        understandably lack a more panoramic perspective on the phenomena of interest
        and, for instance, diachronic or synchronic trends and shifts therein. The
        large-scale, "serial" study of musical instruments as depicted across the
        visual arts remains a desideratum in the field and has the potential of
        bringing a macroscopic perspective to historical developments. In the present
        paper, we explore the feasibility of applying methods from present-day computer
        vision, in an attempt to scale up current approaches. The primary motivation
        of this endeavour is that digital music iconography – or "Distant" music
        iconography, in an analogy to similar developments in literary studies
        <ptr target="#wevers2020"/> <ptr target="#arnold2019"/> – in principle has much to gain
        from such methods, at least if they are carefully applied and in continuous
        interaction with experts in the domain. Our focal point is the automated
        identification and detection of individual musical instruments in unrestricted,
        digitized materials from the realm of the visual arts.</p>
        <p>This scholarly initiative is embedded in the collaborative research
        project INSIGHT (Intelligent Neural Systems as InteGrated Heritage Tools),
        which aims to stimulate the application of Artificial Intelligence to the
        rapidly expanding digital collections of a selection of federal museum
        clusters in Belgium.<note>See <ref
        target="https://hosting.uantwerpen.be/insight/">https://hosting.uantwerpen.be/insight/</ref>.
        This project is generously funded by the Belgian Federal Research Agency
        BELSPO under the BRAIN-be program.</note> One important, transcommunal aspect
        to Belgium's cultural history relates to music and musical history, with
        the invention of the saxophone by Adolphe Sax as an iconic example. An
        additional factor is the presence of the Musical Instruments Museum in the
        capital (Brussels) that contributed significantly to international research
        projects in this area (and which is a partner in the INSIGHT project). This
        contextualization, finally, is also important to understand our specific choice
        for the topic of musical instruments, as a representative and worthwhile case
        study on the application of modern machine learning technology in digital
        heritage studies.</p>
        </div></div>
        <div><head>Computer vision</head>
        <p>The methodology for the present paper largely derives from machine learning
        and more specifically computer vision, a field concerned with computational
        algorithms that can mimic the perceptual abilities of humans and their capacity
        to construct high-level interpretations from raw visual stimuli <ptr target="#ballard1982"/>.
        In the past decade, this field has gone through a remarkable
        renaissance, following the emergence of powerful learning techniques based on
        so-called neural networks. In particular the advent of "convolutional" networks
        <ptr target="#lecun2015"/> has led to dramatic advances in the state of the art for
        a number of standard applications, including image classification ("Is this an
        image of a cat or a dog?") and object detection ("Draw a bounding box around
        any cats in this image"). For some of these tasks, modern computer systems have
        even been shown to rival the performance of humans <ptr target="#russakovsky2015"/>.
        In spite of the impressive advances in recent computer vision research, it is
        generally acknowledged that the state of the art is still confronted with a
        number of major, as yet unsolved, challenges. In this section we highlight four
        concrete issues that are especially pressing, given the focus of this paper on
        image collections in the artistic domain. These challenges motivate our work
        from the point of view of computer vision, rather than art history.</p>
        <div><head>Photo-realism</head>
        <p>One major hurdle is that computer vision nowadays strongly gravitates
        towards so-called photo-realistic material, i.e. digitized or born-digital
        versions of photographs that do not actively attempt to distort the reality
        they depict. The best example in this respect is the influential ImageNet
        dataset <ptr target="#russakovsky2015"/>, that offers highly realistic photographic
        renderings of everyday concepts drawn from WordNet's lexical database. While
        some more recent heritage collections of course abound in such photo-realistic
        material (e.g. advertisements in historic newspapers), traditional photography
        does not take us further back in time than the nineteenth century <ptr target="#hertzmann2018"/>.
        Additionally, the Humanities study many other visual arts that
        prioritize much less photorealistic representation and focus even on completely
        'fictional' renderings of (potentially imagined or historical) realities. While
        there has been some encouraging and worthwhile prior work into the application
        of computer vision to non-photorealistic depictions, this work is generally
        more scattered and the results (understandably) less advanced than those
        reported for the photorealistic domain. Inspiring recent studies in this area
        include
        <ptr target="#crowley2014"/>
        <ptr target="#van2015"/>
        <ptr target="#seguin2018"/>
        <ptr target="#bell2019"/>.</p>
        </div>
        <div><head>Data scarcity</head>
        <p>It is a well-known limitation that convolutional neural networks require
        large amounts of manually annotated example data (or training data) in order to
        perform well. To address this issue, the community has released several public
        datasets over the years
        <ptr target="#xiang2014"/>
        <ptr target="#russakovsky2015"/>
        <ptr target="#mensink2014"/>
        <ptr target="#strezoski2017"/>
        <ptr target="#lin2014"/>
        which has
        allowed the successful training of a large set of neural architectures
        <ptr target="#he2016"/>
        <ptr target="#szegedy2015"/>
        <ptr target="#simonyan2014"/>. However, the
        nature of the images included in these datasets is mostly photo-realistic,
        also because such images are relatively straightforward to obtain and annotate.
        These image collections are very different in terms of texture, content and
        availability from the sort of data that can nowadays be found in the digital
        heritage domain.</p>
        <p>Computer vision researchers interested in the artistic domain have
        attempted to alleviate the relative dearth of training data by either releasing
        domain-specific datasets
        <ptr target="#mensink2014"/>
        <ptr target="#strezoski2017"/>
        or through the application of transfer learning <ptr target="#sabatelli2018"/>,
        a machine learning paradigm which allows the application of neural
        networks to domains where training data is scarce. For image classification,
        for instance, these efforts have indeed greatly contributed to overall
        feasibility of applying computer vision outside the photo-realistic domain
        <ptr target="#sabatelli2018"/>. Both approaches, however, have limitations when it
        comes to the complementary task of object detection. Popular datasets such as
        the Rijksmuseum collection <ptr target="#mensink2014"/> or the more recent
        OmniArt dataset <ptr target="#strezoski2017"/> do not come with the metadata
        required for object-detection problems.</p>
        <p>With this work, we make one step forward in addressing these limitations.
        Firstly, the MINERVA dataset that we present
        below, specifically tackles the problem of object detection within the
        broader heritage domain of the visual arts, introducing a novel benchmark for
        researchers working at the intersection of computer vision and art history.
        Secondly, we present a number of baseline results on the newly introduced
        dataset. The results are reported for a representative set of common
        architectures, which were pre-trained on photo-realistic images. This allows
        us to investigate to what extent these methods can be reused when tested on
        artistic images.</p>
        </div>
        <div><head>Irrelevant training categories</head>
        <p>Previous studies have demonstrated the feasibility of "pretraining": with
        this approach, networks are first trained on (large) photorealistic collections
        (i.e. the source domain) and then applied downstream (or further fine-tuned)
        on an out-of-sample target domain, that has much less annotated data available.
        While generally useful, this approach is still confronted with the problem
        that the annotation labels or categories attested in the source domain are
        often of little interest within the target domain (i.e. art history, in the
        present case). The popular Pascal-VOC dataset <ptr target="#everingham2010"/>,
        for instance, tackles the detection of 20 classes, out of which more than a
        third constitute different kinds of transportation systems, such as trains,
        boats, motorcycles and cars. Naturally, these means of transportation are very
        unlikely to be represented in artworks that date back to the premodern period.
        The more complex MS-COCO dataset <ptr target="#lin2014"/> presents similar problems:
        even though the amount of classes increases to 80, most of the objects which
        should be detected are again unlikely to be represented within historical
        works of art, since they correspond to objects which have only been relatively
        recently invented such as "microwave", "cell-phone", "tv-monitor", "laptop",
        or "remote", and the like. This poses a serious constraint when it comes to
        the use of pre-trained object-detectors for artistic collections. As with most
        supervised learning algorithms, models trained on these collections will only
        perform well on the sort of data on which they have been explicitly trained.
        To illustrate this model bias, we report some (nonsensical) detections in the
        first row of images presented in <ref target="#figure01">Figure 1a</ref>.</p>
        </div>
        <div><head>Robustness of the models</head>
        <p>Popular object detectors such as YOLO <ptr target="#redmon2018"/> and
        Fast R-CNN <ptr target="#ren2017"/> have been designed to perform well on the
        above-mentioned photo-realistic datasets. However, the variance of the samples
        denoting a specific class within these datasets is usually much smaller when
        compared to that in artistic collections. As an example, we refer to a number
        of images representing the person class within the Pascal-VOC dataset: we
        can observe from the two leftmost images of the bottom row of
        <ref target="#figure01">Figure 1</ref> that
        the representation of a 'person' is overall relatively unambiguous and hardly
        distorted. As a result, the person class is usually easily detected by e.g.
        the YOLO architecture. However, we can see that this task already becomes
        harder when a person has to be detected within a painting (potentially with a
        highly distorted representation of the humans in the scene). As shown by the
        two rightmost images of the bottom row (<ref target="#figure01">Figure 1b</ref>), a YOLO-V3 model does not
        see most of the persons represented in the paintings and misclassifies them as
        non-human beings (e.g. "bear").</p>
        <figure xml:id="figure01">
          <head>Examples showing the limitations that occur when a standard object detector trained on photo-realistic images is tested in the domain of the visual arts. Figure 1a: Four anecdotal examples showing that the "person” class is usually reasonably detected by the YOLO architecture, although other, non-sensical detections frequently occur. Figure 1b: the two images on the left show that the variation in depiction of people is limited in photorealistic material, in comparison to the artistic representations of people (two examples to the right).</head>
          <graphic url="resources/images/figure01.png"/>
        </figure>
        <p>All examples in <ref target="#figure01">Figure 1</ref> come from a
        pretrained YOLO-V3 model <ptr target="#redmon2018"/> which has been originally
        trained on the COCO dataset <ptr target="#lin2014"/> and then tested on artworks
        coming from <ptr target="#mensink2014"/> and <ptr target="#strezoski2017"/>.
        The images presented in the first row
        illustrate that the network is biased towards making detections which are very
        unlikely to appear in premodern depictions. These detections correspond to
        the identification of objects such as "suitcase", "umbrella" or "frisbee" and
        "banana". The two last images presented in the second row show that standard
        models fail to properly recognize a simple class such as person. In fact, they
        fully fail in detecting most of the persons that are present in the artworks
        due to these representations being highly different from the persons that are
        present in the Pascal-VOC dataset (first two images of the second row).</p>
        </div></div>
        <div><head>MINERVA: dataset description</head>
        <p>In this section, we describe MINERVA,
        the annotated dataset in the field of object detection that is presented
        in this work. This novel benchmark dataset will be released jointly
        with this paper.<note>All code used in this
        paper is publicly available from this repository: <ref
        target="https://github.com/paintception/MINeRVA">https://github.com/paintception/MINeRVA</ref>.
        Likewise, the MINERVA dataset can be
        obtained from this DOI on Zenodo: 10.5281/zenodo.3732580.</note>
        The main task under scrutiny here is the
        detection of musical instruments in non-photorealistic, unrestricted image
        collections from the artistic domain. We have named the dataset with the
        acronym MINERVA, which stands for 'Musical
        INstrumEnts Represented in the Visual Arts', after the Roman goddess of the
        arts (amongst many other things).</p>
        <div><head>Data Sources</head>
        <p>The base data for our annotation effort was assembled in a series of
        'concentric' collection campaigns, where we started from smaller, but
        high-quality datasets and gradually expanded into larger, albeit less well
        curated data sources.</p>
        <list type="ordered">
        <item><label>RIDIM</label>: We harvested a collection of high-quality images
        from the RIDIM database, in those cases where the database entries provided
        an unambiguous hyperlink to a publicly accessible image. These records were
        already assigned MIMO codes by a community of domain experts, which provided
        important support to our in-house annotators (especially during the first
        experimental rounds of annotations).</item>
        <item><label>RMFAB/RMAH</label>: We expanded on the core RIDIM data by
        including (midrange resolution) images from the digital collections of two
        federal museums in Brussels: the RMFAB (Royal Museums of Fine Arts of Belgium,
        Brussels) and the RMAH (Royal Museums of Art and History, Brussels). These
        images were selected on the basis of previous annotations that suggested they
        included depictions of musical instruments, although no more specific labels
        (e.g. MIMO codes) were available for these records at this stage. Copyrighted
        artworks could not be included for obvious reasons (copyright lasts for 70
        years from the death of the creator under Belgian intellectual law).</item>
        <item><label>Flickr</label>: To scale up our annotation efforts,
        finally, we collected a larger dataset of images from the well-known image
        hosting service 'Flickr' (www.flickr.com). We harvested all images from a
        community-curated collection of depictions of musical instruments in the
        visual arts pre-dating
        1800.<note><ref target="https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1">https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1</ref></note>
        This third campaign yielded much more data
        than the former two, but these were more noisy and contained a variety of false
        positives that had to be manually deleted during the annotation phase.</item>
        </list>
        <p>Our collection efforts were inclusive, and the resulting dataset should
        be considered as "unrestricted", covering a variety of periods, genres and
        materials (although it was not feasible to include more precise metadata about
        these aspects in the dataset). Note that, exactly because of this highly mixed
        data origin, the distribution in MINERVA does
        not give a faithful representation of any kind of historic reality: music
        iconography gives a highly colored perspective on "popular" instruments in
        art history and some instruments may not often have been depicted, even though
        they were popular at the time. Likewise, other instruments are likely to be
        over-represented in iconography.</p>
        </div>
        <div><head>Vocabulary</head>
        <p>To increase the interoperability of the dataset, individual instruments
        have been unambiguously identified using their MIMO codes. The MIMO (Musical
        Instrument Museums Online) initiative is an international consortium, well
        known for its online database of musical instruments, aggregating data and
        metadata from multiple heritage institutions <ptr target="#dolan2017"/>.<note>
        <ref target="https://web.archive.org/save/https://www.mimo-international.com/MIMO/">https://web.archive.org/save/https://www.mimo-international.com/MIMO/</ref></note> An
        important contribution is their development of a uniform metadata documentation
        standard for the field, including a (multilingual) vocabulary to identify
        musical instruments in an interoperable manner. The MIMO ontology is
        hierarchical, meaning that each individual leaf node in their concept tree
        (e.g. 'viola') is a hyponym of a wider instrument category (e.g. 'viola' ∈
        'string instruments'). <ref target="#table01">Table 1</ref> shows examples of annotation labels from this
        ontology. Our dataset provides a spreadsheet that allows for the easy mapping
        of individual instruments to their instrument category. Below, we shall report
        experiments for the more fine-grained and less granular, hypernym versions of
        the categorization task.</p>
        <table xml:id="table01">
          <head>Examples of annotation labels from the MIMO ontology (not all were encountered in MINERVA).</head>
          <row>
            <cell><hi rend="bold">Instrument hypernym</hi></cell>
            <cell><p><hi rend="bold">Stringed</hi></p>
          <p><hi rend="bold">Instruments</hi></p></cell>
            <cell><hi rend="bold">Wind instruments</hi></cell>
            <cell><hi rend="bold">Percussion instruments</hi></cell>
            <cell><hi rend="bold">Keyboard instruments</hi></cell>
            <cell><hi rend="bold">Electronic instruments</hi></cell>
          </row>
          <row>
            <cell>Example instruments</cell>
            <cell>Lute, psaltery, fiddle, viola da gamba, cittern</cell>
            <cell>Transverse flute, end-blown trumpet, horn, shawm, bagpipe</cell>
            <cell>Tambourine, cylindrical drum, frame drum, friction drum, bell</cell>
            <cell>Pianoforte, virginal, portative organ, harpsichord, clavichord</cell>
            <cell>Electric guitar, synthesizer, theremin, vocoder, mellotron</cell>
          </row>
        </table>
        </div>
        <div><head>Annotation process</head>
        <p>Using the conventional method of rectangular bounding boxes, we have
        manually annotated 16,142 musical instruments (of which 172 unique) in
        a collection of 11,765 images, within the open-source <ref target="https://cytomine.be">Cytomine</ref> software
        environment <ptr target="#marée2016"/>. Often multiple instruments
        appeared within the same images and bounding boxes were therefore allowed to
        overlap. Example annotations and a screenshot of the annotation environment are
        presented in <ref target="#figure02">Figure 2</ref>.</p>
        <p>The dataset contains artistic objects from diverse periods and of various
        types, ranging from paintings, sculptures, drawings, to decorative arts,
        manuscript illuminations and stained-glass windows. Thus, they involve a
        daunting diversity of media, techniques and modes. Whereas in some cases the
        images were straightforward to annotate (e.g. an image representing a bell in
        full frame), several obstacles occurred on a recurrent basis. These obstacles
        can be linked to three parameters:</p>
        <list type="ordered">
        <item><label>Representation</label>: A challenging aspect was the
        variety of artistic depiction modes represented in the dataset, ranging
        from photo-realistic renderings to heavily stylized depictions from
        specific art-historical movements (e.g. impressionism, pointillism, fauvism,
        cubism, ...) (<ref target="#figure03">Figure 3a</ref>). Additionally,
        visibility could be low due to a proportionally small instrument depiction or
        the profusion of details (<ref target="#figure03">Figure 3b</ref>). In some
        instances, the state of the depicted object and its medium made
        the detection of the instrument difficult, e.g. a damaged medieval tympanon
        (<ref target="#figure03">Figure 3b</ref>).</item>
        <item><label>Quality</label>: Other, more pragmatic issues arose from
        the images themselves. Occasionally, the quality of the images was too low
        to be able to detect the instruments (e.g. low resolution or compression
        defects) (<ref target="#figure03">Figure 3c</ref>). A great deal of the images
        did not meet international
        quality standards for heritage reproduction photography (uniform and neutral
        environment and lighting, frontal point of view), which implies that the
        instruments were even more difficult to detect.</item>
        <item><label>Boxes</label>: The use of a rectangular shape for the
        bounding boxes sometimes has limitations and implied a certain lack of
        precision, e.g. in the case of a diagonally positioned flute, or in the case
        of overlapping instruments (<ref target="#figure03">Figure 3d</ref>). For some
        instruments which consist of several parts, e.g. a violin and its bow, only the
        main part (the violin) was annotated.</item>
        </list>
        <figure xml:id="figure02">
          <head>Illustration of the annotation interface in Cytomine <ptr target="#marée2016"/>.</head>
          <graphic url="resources/images/figure02.png"/>
        </figure>
        <figure xml:id="figure03">
          <head>Examples of difficulties encountered when annotating images.</head>
          <graphic url="resources/images/figure03.png"/>
        </figure>
        </div>
        <div><head>Characteristics</head>
        <p>An important share of the annotations which we collected were singletons,
        i.e. instruments that were only encountered once or twice. Although we release
        the full dataset, we shall from now on only consider instruments that occurred
        at least three times that allow for a conventional machine learning setup (with
        non-overlapping train, validation and test sets, that include at least one
        instance of each label). Whereas the full MIMO vocabulary covers over 2,000
        vocabulary terms for individual instruments, only a fraction of these were
        attested in the 4,183 images which we use below (overview in <ref target="#table01">Table 1</ref>). Note
        that this table shows a considerable drop in the original number of images
        that we annotated, because we only included images that (a) actually contained
        an instrument and (b) images depicting instruments that occurred at least
        thrice.</p>
        <p>Our annotators only encountered 93 different instrument categories. A
        visualization of the heavily skewed distribution of the different instruments
        can be seen in <ref target="#figure04">Figure 4</ref>,
        where each instrument is represented together with
        its corresponding MIMO code (between parentheses). This distribution exposes
        two core aspects of this dataset (but also of music iconography in general):
        (i) its strong Western-European bias, which has been historically acknowledged,
        and which scholars are actively trying to correct nowadays, but which is a
        slow process; (ii) the 'heavy-tail' distribution associated with cultural
        data in general; i.e. only a fraction of instruments, such as the lute, harp
        and violin, are depicted with a high frequency, the rest occurs much more
        sparsely.</p>
        <figure xml:id="figure04">
          <head>Distribution of the instrument types in the full MINERVA dataset.</head>
          <graphic url="resources/images/figure04.png"/>
        </figure>
        </div>
        <div><head>Versions and splits</head>
        <p>The label imbalance described in the previous paragraph is a significant
        issue for machine learning methods. We therefore experiment with the data
        in five versions (that are available from the repository) that correspond to
        object detection tasks of varying complexity. We start by exploring whether
        it is possible to just detect the presence of an instrument in the different
        artworks, without the additional need of also predicting the class of the
        detected instrument. We refer to this benchmark as single-instrument object
        detection. We then move to three more challenging tasks in which we also
        aim at correctly classifying the content of the detected bounding boxes. We
        include data for this detection task for the top-5, the top-10 and top-20 most
        frequently occurring instruments, a customary practice in the field. Finally,
        we also repeat this task for all images, but with the "hypernym" labels of the
        instrument categories (see <ref target="#figure05">Figure 5</ref>).</p>
        <p>Each version of the dataset comes with its own training, development
        and testing splits, where we offer the guarantee that at least one of
        the instrument classes in the task is represented in each of the splits.
        Additionally, the splits are stratified so that the class distribution is
        approximately the same in each split. The number of images per split in each
        version is summarized in <ref target="#table02">Table 2</ref>. The hypernym version of the dataset is
        not reported in this table as it shares the same images and splits as the
        single-instrument version (they both contain all instruments). We used a
        standard implementation <ptr target="#pedregosa2011"/> for a randomized and shuffled
        split at the level of images and the following, approximate proportions: 1/2
        train, 1/4 dev, and 1/4 test. Images may contain multiple instruments, so that
        the actual number of instruments (as opposed to images) may vary relatively
        strongly across splits.</p>
        <table xml:id="table02">
          <head>Image and instruments distributions of the training, development and test sets for the four different benchmarks presented in this paper (single instrument, top-5 instruments, top-10 instruments and top-20 instruments).</head>
          <row>
            <cell></cell>
            <cell><hi rend="bold">Training-set</hi></cell>
            <cell></cell>
            <cell><hi rend="bold">Dev-set</hi></cell>
            <cell></cell>
            <cell><hi rend="bold">Test-set</hi></cell>
            <cell></cell>
            <cell><hi rend="bold">Total</hi></cell>
            <cell></cell>
          </row>
          <row>
            <cell></cell>
            <cell>Imag</cell>
            <cell>Inst</cell>
            <cell>Imag</cell>
            <cell>Inst</cell>
            <cell>Imag</cell>
            <cell>Inst</cell>
            <cell>Imag</cell>
            <cell>Inst</cell>
          </row>
          <row>
            <cell>Single inst</cell>
            <cell>1857</cell>
            <cell>4243</cell>
            <cell>1137</cell>
            <cell>2288</cell>
            <cell>1189</cell>
            <cell>2102</cell>
            <cell>4183</cell>
            <cell>8633</cell>
          </row>
          <row>
            <cell>Top-5 inst</cell>
            <cell>952</cell>
            <cell>1589</cell>
            <cell>540</cell>
            <cell>852</cell>
            <cell>724</cell>
            <cell>1173</cell>
            <cell>2216</cell>
            <cell>3614</cell>
          </row>
          <row>
            <cell>Top-10 inst</cell>
            <cell>1227</cell>
            <cell>2147</cell>
            <cell>680</cell>
            <cell>1127</cell>
            <cell>898</cell>
            <cell>1506</cell>
            <cell>2805</cell>
            <cell>4780</cell>
          </row>
          <row>
            <cell>Top-20 inst</cell>
            <cell>1471</cell>
            <cell>2915</cell>
            <cell>860</cell>
            <cell>1543
          </cell>
            <cell>1047</cell>
            <cell>1838</cell>
            <cell>3378</cell>
            <cell>6296</cell>
          </row>
        </table>
        <figure xml:id="figure05">
          <head>Distribution of the 5 hypernym categories over the three splits in the MINERVA dataset.</head>
          <graphic url="resources/images/figure05.png"/>
        </figure>
        </div></div>
        <div><head>Benchmark experiments</head>
        <div><head>Classification</head>
        <p>In the first benchmark experiment, we start by investigating whether
        convolutional neural networks are able to correctly classify the different
        instruments that are present in the dataset. That means that we focus on the
        image classification task and postpone the task of object detection to the
        next section. To this end, we have extracted the various patches delineated
        by the bounding boxes in the detection dataset as stand-alone instances. Note,
        however, that patches from the same images always ended in the same split,
        to avoid information leakage across the splits. Example patches are shown in
        <ref target="#figure06">Figure 6</ref>.</p>
        <figure xml:id="figure06">
          <head>Examples of the patches delineated by the bounding boxes, extracted from MINERVA images for the classification experiment.</head>
          <graphic url="resources/images/figure06.png"/>
        </figure>
        <p>Next, we tackled this task as a standard machine-learning classification
        problem for which we applied a representative selection of established neural
        network architectures. All of these networks were pretrained on the Rijksmuseum
        dataset <ptr target="#mensink2014"/>, for which the weights are publicly
        available <ptr target="#sabatelli2018"/>. The tested architectures are: VGG19
        <ptr target="#simonyan2014"/>, Inception-V3 <ptr target="#szegedy2015"/> and ResNet
        <ptr target="#he2016"/>. This approach is motivated by previous work <ptr target="#sabatelli2018"/>
        which shows that when it comes to the classification images from
        the domain of cultural heritage, popular neural architectures which have been
        trained on the large Rijksmuseum collection, can outperform the same kind
        of architectures that are pre-trained on ImageNet only. In order to maximize
        the final classification performance, all network parameters get fine-tuned,
        using the Adam optimizer <ptr target="#kingma2014"/> and minimizing the conventional
        categorical cross-entropy loss function over mini-batches of 32 samples.
        Additionally, we try 3 different learning rates: 0.001, 0.0001, 0.00001. In
        order to handle the skewed distribution of the classes, we try settings with
        and without oversampling. The training regime is interrupted as soon the
        validation loss does not decrease for five epochs in a row.</p>
        <p>In <ref target="#table01">Table 2</ref> and <ref target="#table03">Table 3</ref>
        we report the results in terms of Accuracy and F1-score
        for the MINERVA test sets. For the individual
        instruments, we do so for four versions of the dataset of increasing
        complexity: the top-5 instruments, top-10 instruments, top-20 instruments
        and the entire dataset. Analogously we report the scores for a classification
        experiment where the object detector is trained on the instrument hypernyms as
        class labels.</p>
        <table xml:id="table03">
          <head>Classification results on the MINERVA test set for the three architectures (best results in bold).</head>
          <row>
            <cell></cell>
            <cell>Top-5 inst</cell>
            <cell></cell>
            <cell>Top-10 inst</cell>
            <cell></cell>
            <cell>Top-20 inst</cell>
            <cell></cell>
            <cell>All inst</cell>
            <cell></cell>
            <cell>Hypernyms</cell>
            <cell></cell>
          </row>
          <row>
            <cell>CNN</cell>
            <cell>Acc.</cell>
            <cell>F1</cell>
            <cell>Acc.</cell>
            <cell>F1</cell>
            <cell>Acc.</cell>
            <cell>F1</cell>
            <cell>Acc.</cell>
            <cell>F1</cell>
            <cell>Acc.</cell>
            <cell>F1</cell>
          </row>
          <row>
            <cell>R-Net</cell>
            <cell>68.71</cell>
            <cell>64.10</cell>
            <cell>52.85</cell>
            <cell>41.55</cell>
            <cell>30.73</cell>
            <cell>8.45</cell>
            <cell>26.36</cell>
            <cell>2.08</cell>
            <cell>72.26</cell>
            <cell>52.66 </cell>
          </row>
          <row>
            <cell>V3</cell>
            <cell><hi rend="bold">73.66</hi></cell>
            <cell><hi rend="bold">70.29</hi></cell>
            <cell><hi rend="bold">55.51</hi></cell>
            <cell><hi rend="bold">44.77</hi></cell>
            <cell><hi rend="bold">36.51</hi></cell>
            <cell><hi rend="bold">19.06</hi></cell>
            <cell><hi rend="bold">27.02</hi></cell>
            <cell><hi rend="bold">6.67</hi></cell>
            <cell><hi rend="bold">75.80</hi></cell>
            <cell><hi rend="bold">57.03</hi></cell>
          </row>
          <row>
            <cell>V19</cell>
            <cell>48.33</cell>
            <cell>35.92</cell>
            <cell>37.52</cell>
            <cell>15.22</cell>
            <cell>33.41</cell>
            <cell>9.87</cell>
            <cell>20.17</cell>
            <cell>1.72</cell>
            <cell>66.41</cell>
            <cell>40.35</cell>
          </row>
        </table>
        <table xml:id="table04">
          <head>Confusion matrix for the classification experiment with ResNet on the MINERVA test set (the top-10 most frequently occurring instruments).</head>
          <row>
            <cell><hi rend="bold">Predicted label / Gold label</hi></cell>
            <cell><hi rend="bold">Bagpipe</hi></cell>
            <cell><hi rend="bold">E-b trumpet</hi></cell>
            <cell><hi rend="bold">Harp</hi></cell>
            <cell><hi rend="bold">Horn</hi></cell>
            <cell><hi rend="bold">Lute</hi></cell>
            <cell><hi rend="bold">Lyre</hi></cell>
            <cell><hi rend="bold">Por. organ</hi></cell>
            <cell><hi rend="bold">Rebec</hi></cell>
            <cell><hi rend="bold">Shawm</hi></cell>
            <cell><hi rend="bold">Violin</hi></cell>
          </row>
          <row>
            <cell><hi rend="bold">Bagpipe</hi></cell>
            <cell><hi rend="bold">31</hi></cell>
            <cell>0</cell>
            <cell>10</cell>
            <cell>6</cell>
            <cell>8</cell>
            <cell>1</cell>
            <cell>2</cell>
            <cell>0</cell>
            <cell>7</cell>
            <cell>17</cell>
          </row>
          <row>
            <cell><hi rend="bold">E-b trumpet</hi></cell>
            <cell>4</cell>
            <cell><hi rend="bold">72</hi></cell>
            <cell>19</cell>
            <cell>2</cell>
            <cell>14</cell>
            <cell>1</cell>
            <cell>3</cell>
            <cell>1</cell>
            <cell>38</cell>
            <cell>21</cell>
          </row>
          <row>
            <cell><hi rend="bold">Harp</hi></cell>
            <cell>8</cell>
            <cell>2</cell>
            <cell><hi rend="bold">227</hi></cell>
            <cell>1</cell>
            <cell>10</cell>
            <cell>3</cell>
            <cell>11</cell>
            <cell>0</cell>
            <cell>10</cell>
            <cell>19</cell>
          </row>
          <row>
            <cell><hi rend="bold">Horn</hi></cell>
            <cell>7</cell>
            <cell>5</cell>
            <cell>14</cell>
            <cell><hi rend="bold">9</hi></cell>
            <cell>16</cell>
            <cell>9</cell>
            <cell>1</cell>
            <cell>2</cell>
            <cell>5</cell>
            <cell>14</cell>
          </row>
          <row>
            <cell><hi rend="bold">Lute</hi></cell>
            <cell>6</cell>
            <cell>10</cell>
            <cell>17</cell>
            <cell>6</cell>
            <cell><hi rend="bold">199</hi></cell>
            <cell>6</cell>
            <cell>5</cell>
            <cell>1</cell>
            <cell>5</cell>
            <cell>42</cell>
          </row>
          <row>
            <cell><hi rend="bold">Lyre</hi></cell>
            <cell>3</cell>
            <cell>0</cell>
            <cell>19</cell>
            <cell>1</cell>
            <cell>13</cell>
            <cell><hi rend="bold">5</hi></cell>
            <cell>2</cell>
            <cell>0</cell>
            <cell>3</cell>
            <cell>11</cell>
          </row>
          <row>
            <cell><hi rend="bold">Por. organ</hi></cell>
            <cell>3</cell>
            <cell>0</cell>
            <cell>10</cell>
            <cell>1</cell>
            <cell>0</cell>
            <cell>0</cell>
            <cell><hi rend="bold">57</hi></cell>
            <cell>0</cell>
            <cell>1</cell>
            <cell>4</cell>
          </row>
          <row>
            <cell><hi rend="bold">Rebec</hi></cell>
            <cell>5</cell>
            <cell>2</cell>
            <cell>14</cell>
            <cell>0</cell>
            <cell>9</cell>
            <cell>0</cell>
            <cell>4</cell>
            <cell><hi rend="bold">7</hi></cell>
            <cell>1</cell>
            <cell>23</cell>
          </row>
          <row>
            <cell><hi rend="bold">Shawm</hi></cell>
            <cell>4</cell>
            <cell>11</cell>
            <cell>25</cell>
            <cell>2</cell>
            <cell>11</cell>
            <cell>2</cell>
            <cell>4</cell>
            <cell>6</cell>
            <cell><hi rend="bold">40</hi></cell>
            <cell>13</cell>
          </row>
          <row>
            <cell><hi rend="bold">Violin</hi></cell>
            <cell>6</cell>
            <cell>12</cell>
            <cell>29</cell>
            <cell>4</cell>
            <cell>35</cell>
            <cell>4</cell>
            <cell>7</cell>
            <cell>11</cell>
            <cell>6</cell>
            <cell><hi rend="bold">202</hi></cell>
          </row>
        </table>
        </div>
        <div><head>Detection</head>
        <p>For the second benchmark experiment we report the results that we have
        obtained on the four of the five detection benchmarks introduced in the
        previous section. The way the different instruments are distributed in their
        respective test sets is visually represented in the first image of each row
        of <ref target="#figure07">Figure 7</ref>. For our experiments, we use the popular YOLO-V3
        <ptr target="#redmon2018"/> architecture which we fully fine-tune during training. To explore
        the benefits that transfer learning could bring to the artistic domain, we
        initialize the network with the weights that are obtained after training the
        model on the MS-COCO dataset <ptr target="#lin2014"/>. The network gets then trained
        either with the Adam optimizer <ptr target="#kingma2014"/> or
        RMSprop<note>There is no officially
        published reference for RMSprop, but scholars commonly refer
        to this lecture from Geoffrey Hinton and colleagues:
        <ref target="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</ref></note>
        over mini-batches of 8 images.</p>
        <p>To assess the performance of the neural network, we follow the same
        evaluation protocol that characterizes object detection problems in CV <ptr target="#lin2014"/>.
        Each detected bounding box is compared to the bounding box which
        has been annotated on the Cytomine platform. We only consider bounding boxes
        for which the confidence level is ≥ 0.05, following the protocol established in
        <ptr target="#everingham2010"/>. We then compute the "Intersection over Union" (IoU)
        for measuring how much the detected bounding-boxes differ from the ground-truth
        ones. To assess whether a prediction can be considered as a "true positive"
        or a "false positive", we define two, increasingly restrictive metrics: first,
        IoU ≥ 10 and, secondly, IoU ≥ 50. This approach is again inspired by
        <ptr target="#gonthier2018"/>, where the authors report additional results with an IoU ≥ 10 on
        their IconArt dataset. <ref target="#table05">Table 5</ref> lists precision, recall and average precision
        (AP) scores for each detected class of each data version and <ref target="#figure07">Figure 7</ref> visually
        shows the number of true and false positive predictions in all cases. Examples
        of correct detections are shown in <ref target="#figure08">Figure 8</ref>.</p>
        <table xml:id="table05">
          <head>A quantitative analysis of the results obtained on the four localization benchmarks introduced in this work. To distinguish different benchmarks in the table we separate them by a double line. We report the precision, recall and average-precision scores for each detected class.</head>
          <row>
            <cell><hi rend="bold">Instrument ≥ IoU</hi></cell>
            <cell><hi rend="bold">Precision</hi></cell>
            <cell><hi rend="bold">Recall</hi></cell>
            <cell><hi rend="bold">AP</hi></cell>
          </row>
          <row>
            <cell><p>Single-instrument ≥ 10</p>
          <p>Single-instrument ≥ 50</p></cell>
            <cell><p>0.63</p>
          <p>0.47</p></cell>
            <cell><p>0.42</p>
          <p>0.31</p></cell>
            <cell><p>0.35</p>
          <p>0.22</p></cell>
          </row>
          <row>
            <cell><p>Stringed-Instruments ≥ 10</p>
          <p>Stringed-Instruments ≥ 50</p></cell>
            <cell><p>0.65</p>
          <p>0.53</p></cell>
            <cell><p>0.36</p>
          <p>0.29</p></cell>
            <cell><p>0.28</p>
          <p>0.20</p></cell>
          </row>
          <row>
            <cell><p>Wind-Instruments ≥ 10</p>
          <p>Wind-Instruments ≥ 50</p></cell>
            <cell><p>0.43</p>
          <p>0.32</p></cell>
            <cell><p>0.07</p>
          <p>0.05</p></cell>
            <cell><p>0.04</p>
          <p>0.02</p></cell>
          </row>
          <row>
            <cell><p>Percussion-Instruments ≥ 10</p>
          <p>Percussion-Instruments ≥ 50</p></cell>
            <cell><p>0.32</p>
          <p>0.21</p></cell>
            <cell><p>0.04</p>
          <p>0.03</p></cell>
            <cell><p>0.02</p>
          <p>0.01</p></cell>
          </row>
          <row>
            <cell><p>Keyboard-Instruments ≥ 10</p>
          <p>Keyboard-Instruments ≥ 50</p></cell>
            <cell>
          <p>0.61</p>
          
          <p>0.45</p></cell>
            <cell>
          <p>0.11</p>
          
          <p>0.08</p></cell>
            <cell>
          <p>0.07</p>
          
          <p>0.04</p></cell>
          </row>
          <row>
            <cell><p>Electronic-Instruments ≥ 10</p>
          <p>Electronic-Instruments ≥ 50</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
          </row>
          <row>
            <cell><p>Harp ≥ 10</p>
          <p>Harp ≥ 50</p></cell>
            <cell><p>0.68</p>
          <p>0.60</p></cell>
            <cell><p>0.62</p>
          <p>0.54</p></cell>
            <cell><p>0.55</p>
          <p>0.46</p></cell>
          </row>
          <row>
            <cell><p>Lute ≥ 10</p>
          <p>Lute ≥ 50</p></cell>
            <cell><p>0.57</p>
          <p>0.47</p></cell>
            <cell><p>0.43</p>
          <p>0.35</p></cell>
            <cell><p>0.36</p>
          <p>0.26</p></cell>
          </row>
          <row>
            <cell><p>Violin ≥ 10</p>
          <p>Violin ≥ 50</p></cell>
            <cell><p>0.37</p>
          <p>0.26</p></cell>
            <cell><p>0.22</p>
          <p>0.16</p></cell>
            <cell><p>0.12</p>
          <p>0.07</p></cell>
          </row>
          <row>
            <cell><p>Shawm ≥ 10</p>
          <p>Shawm ≥ 50</p></cell>
            <cell><p>0.13</p>
          <p>0.08</p></cell>
            <cell><p>0.04</p>
          <p>0.02</p></cell>
            <cell><p>0.01</p>
          <p>0.00</p></cell>
          </row>
          <row>
            <cell><p>End-blown trumpet ≥ 10</p>
          <p>End-blown trumpet ≥ 50</p></cell>
            <cell><p>0.28</p>
          <p>0.24</p></cell>
            <cell><p>0.04</p>
          <p>0.03</p></cell>
            <cell><p>0.01</p>
          <p>0.01</p></cell>
          </row>
          <row>
            <cell><p>Harp ≥ 10</p>
          <p>Harp ≥ 50</p></cell>
            <cell><p>0.62</p>
          <p>0.56</p></cell>
            <cell><p>0.56</p>
          <p>0.51</p></cell>
            <cell><p>0.46</p>
          <p>0.39</p></cell>
          </row>
          <row>
            <cell><p>Lute ≥ 10</p>
          <p>Lute ≥ 50</p></cell>
            <cell><p>0.55</p>
          <p>0.47</p></cell>
            <cell><p>0.42</p>
          <p>0.36</p></cell>
            <cell><p>0.33</p>
          <p>0.25</p></cell>
          </row>
          <row>
            <cell><p>Violin ≥ 10</p>
          <p>Violin ≥ 50</p></cell>
            <cell><p>0.26</p>
          <p>0.20</p></cell>
            <cell><p>0.19</p>
          <p>0.14</p></cell>
            <cell><p>0.06</p>
          <p>0.04</p></cell>
          </row>
          <row>
            <cell><p>Shawm ≥ 10</p>
          <p>Shawm ≥ 50</p></cell>
            <cell><p>0.17</p>
          <p>0.17</p></cell>
            <cell><p>0.03</p>
          <p>0.01</p></cell>
            <cell><p>0.00</p>
          <p>0.00</p></cell>
          </row>
          <row>
            <cell><p>End-blown trumpet ≥ 10</p>
          <p>End-blown trumpet ≥ 50</p></cell>
            <cell><p>0.67</p>
          <p>0.17</p></cell>
            <cell><p>0.02</p>
          <p>0.03</p></cell>
            <cell><p>0.01</p>
          <p>0.00</p></cell>
          </row>
          <row>
            <cell><p>Bagpipe ≥ 10</p>
          <p>Bagpipe ≥ 50</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
          </row>
          <row>
            <cell><p>Portative-Organ ≥ 10</p>
          <p>Portative-Organ ≥ 50</p></cell>
            <cell><p>0.24</p>
          <p>0.24</p></cell>
            <cell><p>0.13</p>
          <p>0.13</p></cell>
            <cell><p>0.06</p>
          <p>0.06</p></cell>
          </row>
          <row>
            <cell><p>Horn ≥ 10</p>
          <p>Horn ≥ 50</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
            <cell><p>0</p>
          <p>0</p></cell>
          </row>
          <row>
            <cell><p>Rebec ≥ 10</p>
          <p>Rebec ≥ 50</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
          </row>
          <row>
            <cell><p>Lyre ≥ 10</p>
          <p>Lyre ≥ 50</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>-</p>
          <p>-</p></cell>
            <cell><p>--</p>
          <p>-</p></cell>
          </row>
        </table>
        <figure xml:id="figure07">
          <head>A visual representation of how many instruments should be detected in the testing sets of the four MINERVA benchmarks that are introduced in this paper (first plot of each row). The second and third plots represent the true and false detections that we have obtained with a fully fine-tuned YOLO network. Results are computed with respect to an IoU ≥ 10 and an IoU ≥ 50.</head>
          <graphic url="resources/images/figure07.png"/>
        </figure>
        <figure xml:id="figure08">
          <head>Sample visualizations of the detections obtained on the MINERVA test set for a fully fine-tuned YOLO architecture. The first three rows report the detection of any kind of instrument within the images (single-instrument task), while the last three rows also report the correct classification of the detected bounding boxes.</head>
          <graphic url="resources/images/figure08.png"/>
        </figure>
        </div>
        <div><head>Additional experiments</head>
        <p>As an additional stress-test, we have applied a trained object detector
        to two external data sets, in order to assess how valid and performant our
        approach is when applied "in the wild". We have considered two out-of-sample
        datasets:</p>
        <list type="unordered">
        <item><label>RMFAB/RMAH</label>: 428 out-of-sample images from the digital
        assets of both museum collections that are not included in annotated material
        (and which are thus not included the train and validation material of the
        applied detector), because the available metadata did not explicitly specify
        that they contained depictions of musical instruments. (This collection cannot
        be shared due to copyright restrictions.)</item>
        <item><label>IconArt</label>: a generic collection of
        6,528 artistic images, collected from the community-curated
        platform <emph>WikiArt: Visual Art Encyclopedia</emph> (<ref
        target="https://www.wikiart.org/">https://www.wikiart.org/</ref>).
        The IconArt subcollection was previously
        redistributed by <ptr target="#gonthier2018"/>: <ref
        target="https://wsoda.telecom-paristech.fr/downloads/dataset/">https://wsoda.telecom-paristech.fr/downloads/dataset/</ref>.</item>
        </list>
        <p>Note that both external datasets differ in crucial aspects:
        <label>RMFAB/RMAH</label> can be considered "out-of-sample", but
        "in-collection", in the sense that these images derive from the
        same digital collections as many of the images represented in MINERVA.
        Additionally, we can expect extremely low
        detection rates for this dataset, because the presence of musical instruments
        will already have been flagged in a large majority of cases by the museum's
        staff. Thus, the application of <label>RMFAB/RMAH</label> should be viewed as
        a rather conservative stress test or sanity check, mainly checking for images
        that might have been missed by annotators in the past. The IconArt dataset
        is "out-of-sample" and "out-of-collection", in the sense that these images
        derive from a variety of other sources. It is therefore fully unrestricted,
        and this test can be considered a curiosity-driven validation of the method "in
        the wild". Importantly, IconArt was not collected with specific attention for
        musical instruments, so here too, we can anticipate a rather low detection rate
        (since many works of art simply do not feature any instruments). For all these
        reasons, we only evaluate the results on these external datasets in terms of
        precision (as recall is much less meaningful in this context).</p>
        <p>Following these differences, we have applied the single-instrument detector
        to the <label>RMFAB/RMAH</label> data and the hypernym detector to IconArt.
        Keeping an eye on the feasibility of the manual inspection, we have limited
        the number of instances returned by only allowing detections with a confidence
        score ≥ 0.20 (which is a rather generous threshold). Next, the results have
        then been evaluated in terms of precision, i.e. the number of returned image
        regions that actually represent musical instruments. The results are presented
        in <ref target="#table06">Table 6</ref>. <ref target="#figure10">Figure 10</ref>
        showcases a number of cherry-picked successful examples
        of detections from the out-of-collection IconArt images.</p>
        <table xml:id="table06">
          <head>Quantitative evaluation of the method on two out-of-sample datasets in terms of precision, restricted to detections with a confidence score ≥ 0.20.</head>
          <row>
            <cell><hi rend="bold">Collection</hi></cell>
            <cell><hi rend="bold">Total images</hi></cell>
            <cell><hi rend="bold">Detections</hi></cell>
            <cell><hi rend="bold">True positives</hi></cell>
          </row>
          <row>
            <cell><hi rend="bold">RMFAB/RMAH</hi></cell>
            <cell>428</cell>
            <cell>162</cell>
            <cell>6</cell>
          </row>
          <row>
            <cell><hi rend="bold">IconArt</hi></cell>
            <cell>6528</cell>
            <cell>118</cell>
            <cell>42</cell>
          </row>
        </table>
        </div></div>
        <div><head>Discussion</head>
        <div><head>Skewed results</head>
        <p>First and foremost, we can observe that the scores obtained across all
        benchmarks are generally much lower than those reported for other datasets
        in computer vision (outside of the strict artistic domain). This drop in
        performance was to be expected and can be attributed to both the smaller size
        of the training data and the higher variance in the representation spectrum of
        musical instruments (across periods, materials, modes and, artists). Secondly,
        one can observe large fluctuations in the identifiability and detectability of
        individual instrument categories across both tasks. Not all of the fluctuations
        are easy to account for.</p>
        <p>We first consider the classification results. The confusion matrix reported
        in <ref target="#table04">Table 4</ref> clearly shows that the classes representing the top-4 of instruments
        (harp, lute, violin, and portative organ) can be learned rather successfully,
        but that the performance rapidly breaks down for instrument categories at
        lower frequency ranks. Thus, while the accuracies for the top-5 experiments are
        relatively satisfying, especially in terms of accuracy (V3: <emph>acc=73.66;
        F1=70.29</emph>), the performance rapidly degrades for the more difficult
        setups. The results for the "all" classification experiment, where every
        instrument category is included no matter its frequency, are nothing less
        than dramatic (V3: <emph>acc=27.02; F1=6.67</emph>) and call for in-depth
        further research. The significant divergence between accuracy scores and F1
        scores demonstrate that class imbalance is thus another aspect in which
        MINERVA presents a more challenging benchmark than its
        photorealistic counterparts.</p>
        <p>The skewness of the class distribution in MINERVA is representative of the long-tail
        distribution that we commonly encounter in cultural data. This imbalance is
        somewhat alleviated in the hypernym setup, where the labels are of course much
        better distributed over a much smaller number of classes (<emph>n=5</emph>).
        The general feasibility of this specific task is demonstrated by the
        encouraging scores that can be reported for the Inception-V3 architecture
        on this task (<emph>acc=75.80; F1=57.03</emph>). Note, additionally, that
        the "Electronic instruments" hypernym is included for completeness in this
        task, although the label is very infrequent and inevitably pulls down the
        (macro-averaged) F1-score in this respect. Overall, we notice than the
        Inception-V3 architecture yields the highest performance on average for the
        classification task.</p>
        <p>Similar trends can be observed for the musical instrument detection
        task. First of all, we should emphasize the encouraging scores for the
        "single-instrument" detection task that simply aims to detect musical
        instruments (no matter their type). Here, a relatively high precision score
        is obtained (<emph>prec=0.63</emph> for <emph>IoU ≥ 10</emph>), which seems on
        par with comparable object categories for modern photo-realistic collections
        <ptr target="#ren2017"/>. Thus, this algorithm might not be fully apt at
        retrieving every single instrument from an unseen collection, but when it
        detects an instrument, we can be relatively sure that the detection deserves
        further inspection by a domain expert. Equally heartening scores are evident
        for most of the instrument hypernyms (with the notable exception of the
        under-represented "Electronic instruments" hypernym). While these detection
        tasks are of course relatively coarse, this observation nevertheless entails
        that this sort of detection technology can already find useful applications in
        the field (see below).</p>
        <p>When making our way down the frequency list in <ref target="#table05">Table 5</ref>, we again
        observe how the results break down dramatically for less common instrument
        categories. The fact that an over-represented category like harps can be
        reasonably well detected (<emph>AP(IoU ≥ 10)=0.55; AP(IoU ≥ 50)=0.46</emph>),
        should not lead the attention away from the fact that a state of the art
        object detector, such as YOLO, fails miserably at detecting a number of
        iconographically highly salient instruments, such as lyres and end-blown
        trumpets. At this stage, it is unclear whether this is caused by mere class
        imbalance or by the higher variance in the iconographic depiction of specific
        instruments. Bagpipes, for instance, occur frequently across images in
        MINERVA but might display much more depiction variance
        than, for instance, a harp.</p>
        </div>
        <div><head>Saliency maps</head>
        <p>The results from the previous question call into question which visual
        properties the neural networks find useful to exploit in the identification of
        instruments. Importantly, the characteristic features exploited by a machine
        learning algorithm need not coincide with the properties that are judged
        most relevant by human experts and the comparison of both types of relevance
        judgements is worthwhile. In this section, we therefore perform model criticism
        or "network introspection" on the basis of the so-called "saliency maps" that
        can be extracted from a trained model <ptr target="#boyarski2017"/>. These saliency
        maps make visible to which regions in the original image the network paid
        most attention to, before arriving at its final classification decision. All
        examples discussed below come from the experiments on the hypernym dataset for
        the VGG19 network. <ref target="#figure09">Figure 9</ref> shows a series of manually selected, insightful
        examples, including the original image (as inputted into the network after
        preprocessing), as well as the saliency map obtained for it. We limit these
        examples to the representative hypernyms 'Stringed instruments' and 'Wind
        instruments.'</p>
        <p>The maps in <ref target="#figure09">Figure 9</ref> vividly illustrate that the network focuses on
        two broad types of regions: properties of the instruments itself (which
        was expected) but also the immediate context of the instruments, and more
        specifically the way they are operated, handled or presented by people, c.q.
        musicians. The characteristics of the salient regions in the examples in
        <ref target="#figure09">Figure 9</ref> could be described as:</p>
        <p><label>Stringed instruments:</label></p>
        <list type="simple">
        <item>(a) Focus on the neck of the stringed instrument, as well as the
        characteristic presence of tuning pins at the end of the neck;</item>
        <item>(b) Sensitive to the presence of stretched fingers in an unnatural
        position;</item>
        <item>(c) Typical conic shape of a lyre, with outward pointing ends connected by a
        bridge;</item>
        </list>
        <p><label>Wind instruments:</label></p>
        <list type="simple">
        <item>(d) Symmetric presence of tone holes in the areophone;</item>
        <item>(e) Elongated, cylindric shape of the main body of the areophone with wider
        end;</item>
        <item>(f) Mirrored placement of fingers and hands (close to one another).</item>
        </list>
        <p>These characteristics strongly suggest that the way an instrument is
        handled (i.e. its immediate iconographic neighborhood) is potentially of equal
        importance as the shape of the actual instrument, an insight that we will
        further expand on below.</p>
        <table>
        
        <row>
        <cell>Stringed instruments</cell>
        <cell>(a)</cell>
        <cell><graphic url="./media/image40.jpg"
        style="width:4.46875in;height:1.90625in" /></cell>
        </row>
        <row>
        <cell></cell>
        <cell>(b)</cell>
        <cell><graphic url="./media/image41.jpg" style="width:4.5in;height:1.96875in" /></cell>
        </row>
        <row>
        <cell></cell>
        <cell>(c)</cell>
        <cell><graphic url="./media/image42.jpg"
        style="width:4.47917in;height:2.05208in" /></cell>
        </row>
        <row>
        <cell>Wind instruments</cell>
        <cell>(d)</cell>
        <cell><graphic url="./media/image43.jpg"
        style="width:4.47917in;height:1.90625in" /></cell>
        </row>
        <row>
        <cell></cell>
        <cell>(e)</cell>
        <cell><graphic url="./media/image44.jpg"
        style="width:4.47917in;height:1.9375in" /></cell>
        </row>
        <row>
        <cell></cell>
        <cell>(f)</cell>
        <cell><graphic url="./media/image45.jpg"
        style="width:4.47917in;height:1.94792in" /></cell>
        </row>
        
        </table>
        <figure xml:id="figure09">
          <head>Saliency maps for several stringed (subfigures (a) to (c)) and wind (subfigures (d) to (f)) instruments.</head>
          <graphic url="resources/images/figure09.png"/>
        </figure>
        </div>
        <div><head>Error analysis: false positives</head>
        <p>In this section, we offer a qualitative discussion of the false positives
        from the out-of-sample tests reported in the previous section, i.e. instances
        where the detectors erroneously thought to have detected an instrument. This
        eagerness is a known problem of object detectors: a system that is trained
        to recognize "sheep" will be inclined to see "sheep" everywhere. Anecdotally,
        people have noted how misleading contextual cues can indeed be a confounding
        factor in image analysis. One blog post for instances noted how a major
        image labeling service tagged photographs of green fields with the "sheep"
        label, although no sheep whatsoever were present in the
        images.<note><ref target="https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep">https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep</ref></note>
        Eagerness-to-detect or over-association is therefore a clear first shortcoming
        of this method when applied in the wild, mainly because it was only trained
        in images that actually contain musical instruments. Interestingly, the false
        positives come in clusters that shed an interesting light on this issue. Below
        we list a representative number of error clusters:</p>
        <figure xml:id="figure10">
          <head>Examples of successful detections in IconArt for "stringed instruments”.</head>
          <graphic url="resources/images/figure10.png"/>
        </figure>
        <figure xml:id="figure11">
          <head>Anecdotal examples of false positive detection, divided in 7 interpretive clusters (numbered a-g).</head>
          <graphic url="resources/images/figure11.png"/>
        </figure>
        <p>The above categorization illustrates that the false positives are
        rather insightful, mainly because the absence of an instrument highlights
        the contextual clues that are at work. Of particular relevance is the
        observation that the iconography surrounding children closely resembles that
        of instruments. This seems related to the intimate and caring body language
        of both the caretakers and musicians in such compositions. The immediate
        iconographic neighborhood of children clearly reminds the detector of the
        delicacy and reverence with which instruments are portrayed and presented
        in historical artworks. This delicacy and intimacy in body language can be
        specifically related to the foregrounding of fingers, the prominent portrayal
        of which invariably triggers the detector, also in the absence of children.
        Some of these phenomena invite closer inspection by domain experts in music
        iconography and suggest that serial or panoramic analyses are a worthwhile
        endeavour in this field, also from the point of view of more hermeneutically
        oriented scholars.</p>
        </div></div>
        <div><head>Conclusions and future research</head>
        <p>In this paper, we have introduced MINERVA,
        to our knowledge the first sizable benchmark dataset for the identification and
        detection of individual musical instruments in unrestricted, digitized images
        from the realm of the visual arts. Our benchmark experiments have highlighted
        the feasibility of a number of tasks but also, and perhaps primarily, the
        significant challenges that state-of-the-art machine learning systems are
        still confronted with on this data, such as the "long-tail" of the instruments'
        distribution and the staggering variance in depiction across the images in the
        dataset. We therefore hope that this work will inspire new (and much-needed)
        research in this area. At the end of this paper, we wish to formulate some
        advice and concerns in this respect.</p>
        <p>One evident direction from future research is more advanced transfer
        learning, where algorithms make more efficient use of the wealth of
        photorealistic data that is provided, for instance, by MIMO <ptr target="#dolan2017"/>.
        The main issue with the MIMO data in this respect is that the bulk of these
        photographs are context-free (i.e. the instruments are photographed in
        isolation, against a white or neutral background), which is almost never the
        case in the artistic domain. Preliminary research demonstrated that this a
        major hurdle to established pretraining scenarios. Cascaded approaches, where
        instruments are detected first and only classified in a second stage might be
        promising avenue here.</p>
        <p>One crucial final remark is that AI has an amply attested tendency not
        only to be sensitive to biases in the input data but also to amplify them
        <ptr target="#zou2018"/>. Whereas the computational methods presented here
        have the potential to scale up dramatically the scope of current research
        in music iconography, it also comes with ideological dangers. The technology
        could further strengthen the bias on specific canonical regions and periods
        in art history and lead the attention even further away from artistic and
        iconographic cultures that are already in specific need of reappraisal. The
        community will therefore have to think carefully about bias correction and
        mitigation. Collecting training data in a diverse and inclusive manner, with
        ample attention for resource-lower cultures should be a key strategy in future
        data collection campaigns.</p>
        </div>
        <div><head>Acknowledgements</head>
        <p>We wish to thank Remy Vandaele for the help with Cytomine and for the
        fruitful discussions related to computer vision and object detection. Special
        thanks go out to our annotators and other (former) colleagues in museums:
        Cedric Feys, Odile Keromnes, Lies Van De Cappelle and Els Angenon. Our
        gratitude also goes out to Rodolphe Bailly for his support and advice regarding
        MIMO. Finally, we wish to credit our former project member dr. Ellen van Keer
        with the original idea of applying object detection to musical instruments.
        This project is generously funded by the Belgian Federal Research Agency BELSPO
        under the BRAIN-be program.</p>
        </div>
      </body>

      <!-- BACK TEXT -->
      <back>
        <listBibl>
          <bibl xml:id="arnold2019" label="Arnold and Tilton 2019">Arnold, T., and Tilton, L., <title rend="quotes">Distant viewing: analyzing large visual corpora.</title><title rend="italic">Digital Scholarship in the Humanities</title>, 34 (2019), i3-i16.</bibl>
          <bibl xml:id="baldassarre2007" label="Baldassarre 2007">Baldassarre, A. <title rend="quotes">Quo vadis music iconography? The Repertoire International d'Iconographie Musicale as a case study</title> <title rend="italic">Fontes Artis Musicae</title>, 54 (2007), 440-452.</bibl>
          <bibl xml:id="baldassarre2008" label="Baldassarre 2008">Baldassarre, A. <title rend="quotes">Music Iconography: What is it all about? Some remarks and considerations with a selected bibliography</title> <title rend="italic">Ictus: Periódico do Programa de Pós-Graduação em Música da UFBA</title>, 9 (2008), 55-95.</bibl>
          <bibl xml:id="ballard1982" label="Ballard and Brown 1982">Ballard, D. H., and Christopher M. Brown, C. M. <title rend="italic">Computer Vision</title>, Upper Saddle River (1982).</bibl>
          <bibl xml:id="bell2019" label="Bell and Impett 2019">Bell, P., and Impett, L. <title rend="quotes">Ikonographie und Interaktion. Computergestützte Analyse von Posen in Bildern der Heilsgeschichte</title> <title rend="italic">Das Mittelalter</title>, 24 (2019): 31–53.</bibl>
          <bibl xml:id="boyarski2017" label="Boyarski et al. 2017">Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry J. Ackel, Urs Muller, Philip Yeres, Karol Zieba, <title rend="quotes">VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving</title> <title rend="italic">Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)</title>, 2018, 4701-4708. DOI: 10.1109/ICRA.2018.8461053.</bibl>
          <bibl xml:id="buckley1998" label="Buckley 1998">Buckley, A. <title rend="quotes">Music Iconography and the Semiotics of Visual Representation</title> <title rend="italic">Music in Art</title>, 23 (1998), 5-10.</bibl>
          <bibl xml:id="crowley2014" label="Crowley and Zisserman 2014">Crowley, E., and Zisserman, A. <title rend="quotes">The State of the Art: Object Retrieval in Paintings using Discriminative Regions</title> In Valstar, M., French, A., and Pridmore, T. (eds), <title rend="italic">Proceedings of the British Machine Vision Conference</title>, Nottingham (2014), s.p.</bibl>
          <bibl xml:id="dolan2017" label="Dolan 2017">Dolan, E. I. <title rend="quotes">Review: MIMO: Musical Instrument Museums Online</title> <title rend="italic">Journal of the American Musicological Society</title>, 70 (2017): 555-565.</bibl>
          <bibl xml:id="everingham2010" label="Everingham et al. 2010">Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. <title rend="quotes">The Pascal visual object classes (VOC) challenge</title> In <title rend="italic">International journal of computer vision</title>, 88(2) (2010): 303–338.</bibl>
          <bibl xml:id="gonthier2018" label="Gonthier et al. 2018">Gonthier, N., Gousseau, Y., Ladjal, S. and Bonfait, O. <title rend="quotes">Weakly supervised object detection in artworks</title> In <title rend="italic">Proceedings of the European Conference on Computer Vision (ECCV)</title> (2018): 692–709.</bibl>
          <bibl xml:id="green2013" label="Green and Ferguson 2013">Green, A., and Ferguson, S. <title rend="quotes">RIDIM: Cataloguing music iconography since 1971</title> <title rend="italic">Fontes Artis Musicae</title>, 60 (2013), 1-8.</bibl>
          <bibl xml:id="he2016" label="He et al. 2016">He, K., Zhang, X., Ren, S., and Sun, J. <title rend="quotes">Deep residual learning for image recognition.</title>In <title rend="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</title>, pages 770–778, 2010.</bibl>
          <bibl xml:id="hertzmann2018" label="Hertzmann 2018">Hertzmann, A. <title rend="quotes">Can Computers Create Art?</title> Arts, 7 (2018) doi:10.3390/arts7020018.</bibl>
          <bibl xml:id="hockey2004" label="Hockey 2004">Hockey, S. <title rend="quotes">A History of Humanities Computing.</title>In S. Schreibman, R. Siemens, and J. Unsworth (eds.), <title rend="italic">A Companion to Digital Humanities</title>, Oxford (2004), pp. 3–19.</bibl>
          <bibl xml:id="huang2017" label="Huang et al. 2017">Huang, G., Zhuang, L., Van Der Maaten, L., and Weinberger, K. <title rend="quotes">Densely connected convolutional networks</title>In <title rend="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</title> (2017), pp. 4700– 4708.</bibl>
          <bibl xml:id="kingma2014" label="Kingma and Ba 2014">Kingma, D. P., and Ba, J. <title rend="quotes">A method for stochastic optimization</title> <title rend="italic">arXiv preprint arXiv:1412.6980</title>, 2014.</bibl>
          <bibl xml:id="lecun2015" label="LeCun et al. 2015">LeCun, J., Bengio, Y., and Hinton, G., <title rend="quotes">Deep Learning</title> <title rend="italic">Nature</title>, 521 (2015): 436–444.</bibl>
          <bibl xml:id="lin2014" label="Lin et al. 2014">Lin T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P and Zitnick, C. L. <title rend="quotes">Microsoft COCO: Common objects in context</title> In <title rend="italic">European conference on computer vision</title>, pages 740–755. Springer, 2014.</bibl>
          <bibl xml:id="marée2016" label="Marée et al. 2016">Marée, R., Rollus, L. Stévens, B., Hoyoux, R., Louppe, G., Vandaele, R., Begon, J., Kainz, P., Geurts, P., and Wehenkel <title rend="quotes">Collaborative analysis of multi-gigapixel imaging data using Cytomine</title> <title rend="italic">Bioinformatics</title>, 32 (2016): 1395–1401.</bibl>
          <bibl xml:id="mensink2014" label="Mensink and Van Gemert 2014">Mensink, T. and Van Gemert, J. <title rend="quotes">The Rijksmuseum challenge: Museum-centered visual recognition</title> In <title rend="italic">Proceedings of International Conference on Multimedia Retrieval</title>, page 451. ACM, 2014.</bibl>
          <bibl xml:id="pedregosa2011" label="Pedregosa et al. 2011">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R. and Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. <title rend="quotes">Scikit-learn: Machine Learning in Python</title> <title rend="italic">Journal of Machine Learning Research</title>, 12 (2011): 2825-2830.</bibl>
          <bibl xml:id="redmon2018" label="Redmon and Farhadi 2018">Redmon, J. and Farhadi, A. <title rend="quotes">Yolov3: An incremental improvement</title> <title rend="italic">arXiv preprint arXiv:1804.02767</title>, 2018.</bibl>
          <bibl xml:id="ren2017" label="Ren et al. 2017">S. Ren, K. He, R. Girshick, and J. Sun, <title rend="quotes">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title> <title rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>, 39 (2017), 1137-1149.</bibl>
          <bibl xml:id="russakovsky2015" label="Russakovsky et al. 2015">Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, K., Khosla, A., Bernstein, M. et al. <title rend="quotes">Imagenet large scale visual recognition challenge</title> <title rend="italic">International journal of computer vision</title>, 115(3) (2015), 211–252.</bibl>
          <bibl xml:id="sabatelli2018" label="Sabatelli et al. 2018">Sabatelli, M., Kestemont, M., Daelemans, W. and Geurts, P. <title rend="quotes">Deep transfer learning for art classification problems</title> In <title rend="italic">Proceedings of the European Conference on Computer Vision (ECCV)</title>, pages 631–646, 2018.</bibl>
          <bibl xml:id="schmidhuber2015" label="Schmidhuber 2015">, Schmidhuber, J. <title rend="quotes">Deep Learning in Neural Networks: An Overview</title> <title rend="italic">Neural Networks</title>, 61 (2015), 85-117.</bibl>
          <bibl xml:id="seguin2018" label="Seguin 2018">Seguin, B. <title rend="quotes">The Replica Project: Building a visual search engine for art historians</title> XRDS: Crossroads, <title rend="italic">The ACM Magazine for Students - Computers and Art</title>, 24 (2018), 24-29.</bibl>
          <bibl xml:id="simonyan2014" label="Simonyan and Zisserman 2014">Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. <title rend="italic">arXiv preprint arXiv:1409.1556</title>, 2014.</bibl>
          <bibl xml:id="strezoski2017" label="Strezoski and Worring 2017">Strezoski, G. and Worring, M. <title rend="quotes">Omniart: multi-task deep learning for artistic data analysis</title> <title rend="italic">arXiv preprint arXiv:1708.00684</title>, 2017.</bibl>
          <bibl xml:id="szegedy2015" label="Szegedy et al. 2015">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, S., Erhan, D., Vanhoucke, V., and Rabinovich, A. <title rend="quotes">Going deeper with convolutions</title> In <title rend="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</title> (2015), pp. 1–9.</bibl>
          <bibl xml:id="van2015" label="Van et al. 2015">Van Noord, N., Hendriks, E., and Postma, E., <title rend="quotes">Toward Discovery of the Artist's Style: Learning to recognize artists by their artworks</title> <title rend="italic">IEEE Signal Processing Magazine</title>, 32 (2015), 46-54.</bibl>
          <bibl xml:id="wevers2020" label="Wevers and Smits 2020">Wevers M., and Smits, T. <title rend="quotes">The visual digital turn: Using neural networks to study historical images</title> <title rend="italic">Digital Scholarship in the Humanities</title>, 35 (2020), 194–207.</bibl>
          <bibl xml:id="xiang2014" label="Xiang et al. 2014">Xiang, Y., Mottaghi, R., and Savarese, S. <title rend="quotes">Beyond pascal: A benchmark for 3d object detection in the wild</title> In <title rend="italic">IEEE Winter Conference on Applications of Computer Vision</title>, pages 75–82. IEEE, 2014.</bibl>
          <bibl xml:id="zou2018" label="Zou and Schiebinger 2018">Zou, J., and Schiebinger, L. <title rend="quotes">AI can be sexist and racist — it's time to make it fair</title> <title rend="italic">Nature</title>, 559 (2018): 324-326.</bibl>
        </listBibl>
      </back>
    </text>
    <!-- END TEXT -->

</TEI>
