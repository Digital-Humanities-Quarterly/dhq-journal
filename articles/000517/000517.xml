<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">

  <!-- BEGIN TEI HEADER ELEMENTS -->
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title type="article" xml:lang="en">Advances in Digital Music Iconography: Benchmarking the
          detection of musical instruments in unrestricted, non-photorealistic images from the
          artistic domain</title>
        <dhq:authorInfo>
          <dhq:author_name>Matthia <dhq:family>Sabatelli</dhq:family></dhq:author_name>
          <dhq:affiliation>Montefiore Institute</dhq:affiliation>
          <email>m.sabatelli@uliege.be</email>
          <dhq:bio>
            <p>Matthia Sabatelli is a Ph.D. candidate in Machine Learning at the Department of
              Electrical Engineering and Computer Science of the University of Liège where he is
              supervised by Dr. Pierre Geurts. His main research interests revolve around the
              transferability and scalability of deep neural networks whose generalization
              properties are studied under the lens of different machine learning paradigms ranging
              from computer vision to reinforcement learning.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Nikolay <dhq:family>Banar</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>nicolae.banari@uantwerpen.be</email>
          <dhq:bio>
            <p>Nikolay Banar is a Ph.D candidate at the University of Antwerp (Belgium). His
              scientific interests lie with the intersection of machine learning and humanities.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Marie <dhq:family>Cocriamont</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Museums of Art and History, Brussels</dhq:affiliation>
          <email>marie_cocriamont@hotmail.com</email>
          <dhq:bio>
            <p>Marie Cocriamont obtained her Masters degree in Musicology at the University of Ghent
              in 2016. She specialized in the comparison of didactic methods in Classical Arabic
              music. In 2019 she started working as a scientific assistant at the Royal Museums of
              Art and History, where she mainly works as an annotator for the research project
              INSIGHT (Intelligent Neural Systems as InteGrated Heritage Tools).</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Eva <dhq:family>Coudyzer</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Institute for Cultural Heritage</dhq:affiliation>
          <email>eva.coudyzer@kikirpa.be</email>
          <dhq:bio>
            <p>Eva Coudyzer obtained a Master in Art History and Archaeology in 2004 at the Vrije
              Universiteit Brussel. She worked in documentation centers and collection management
              services in several cultural organizations in Belgium. In 2009 she started working as
              a scientific assistant at the Royal Museums of Art and History, specializing in
              collection management systems. She was coordinator and partner in several national and
              international digitization projects with a main focus on linking and publishing
              collections with the use of standardized controlled vocabularies. She currently works
              as a scientific assistant at the information center of the <ref
                target="http://www.kikirpa.be">Royal Institute for Cultural Heritage</ref> where she
              participates in the development of the collection management system and the
              valorization of the collection in digitization projects.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Karine <dhq:family>Lasaracina</dhq:family></dhq:author_name>
          <dhq:affiliation>Royal Museums of Fine Arts of Belgium, Brussels</dhq:affiliation>
          <email>karine.lasaracina@fine-arts-museum.be</email>
          <dhq:bio>
            <p>Karine Lasaracina has master degrees in art history and journalism. She joined the
              RMFAB in 1999 and is now head of the Digital Museum unit. From the very beginning of
              her career, she has been interested in the concept of digital management of heritage
              data. A current focus is the development of digital applications that can support
              enriched visitor experiences in the museum through the implementation of various
              innovative technological solutions, for example virtual reality tools, multimedia
              narratives and virtual exhibitions. Promoter of various ongoing research projects, she
              also works on Data Interoperability, Open Science, the development of Artificial
              Intelligence to serve the museums, as well as innovation in the field of images of
              artworks (reproduction, storage, preservation and sharing).</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Walter <dhq:family>Daelemans</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>walter.daelemans@uantwerpen.be</email>
          <dhq:bio>
            <p>Walter Daelemans is professor of Computational Linguistics at the University of
              Antwerp and research director of the CLiPS (Computational Linguistics,
              Psycholinguistics and Sociolinguistics) research centre. His expertise is in Natural
              Language Processing and Machine Learning and applications in automatic text analysis
              and computational stylometry.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Pierre <dhq:family>Geurts</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Liège</dhq:affiliation>
          <email>p.geurts@uliege.be</email>
          <dhq:bio>
            <p>Pierre Geurts is professor in computer science at the University of Liège. His
              research interests concern the design, the empirical, and the theoretical analyses of
              machine learning algorithms, with emphasis on scalability, interpretability, and
              usability of these algorithms. He develops real-world applications of these algorithms
              in various domains, including computational and systems biology, computer vision, and
              digital humanities.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Mike <dhq:family>Kestemont</dhq:family></dhq:author_name>
          <dhq:affiliation>University of Antwerp</dhq:affiliation>
          <email>mike.kestemont@uantwerpen.be</email>
          <dhq:bio>
            <p>Mike Kestemont is research professor in Digital Text Analysis at the University of
              Antwerp (Belgium). His expertise lies in the application of computational methods to
              the Humanities, in particular premodern literature. With F. Karsdorp and A. Riddell,
              he has co-authored the monograph <title rend="italic">Humanities Data Analysis: Case
                Studies with Python</title>, which will appear with Princeton University Press in
              early 2021.</p>
          </dhq:bio>
        </dhq:authorInfo>
      </titleStmt>
      <publicationStmt>
        <publisher>Alliance of Digital Humanities Organizations</publisher>
        <publisher>Association for Computers and the Humanities</publisher>
        <idno type="DHQarticle-id">000517</idno>
        <idno type="volume">015</idno>
        <idno type="issue">1</idno>
        <date when="2021-03-05">05 March 2021</date>
        <dhq:articleType>article</dhq:articleType>
        <availability status="CC-BY-ND">
          <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <p>This is the source</p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <classDecl>
        <taxonomy xml:id="dhq_keywords">
          <bibl>DHQ classification scheme; full list available at<ref
              target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
              >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
        </taxonomy>
        <taxonomy xml:id="authorial_keywords">
          <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
        </taxonomy>
      </classDecl>
    </encodingDesc>
    <profileDesc>
      <langUsage>
        <language ident="en" extent="original"/>
      </langUsage>
      <textClass>
        <keywords scheme="#dhq_keywords">
          <list type="simple">
            <item/>
          </list>
        </keywords>
        <keywords scheme="#authorial_keywords">
          <list type="simple">
            <item/>
          </list>
        </keywords>
      </textClass>
    </profileDesc>
    <revisionDesc>
      <change when="2020-09-18" who="Taylor Arnold">Created file</change>
    </revisionDesc>
  </teiHeader>
  <!-- END TEI HEADER ELEMENTS -->

  <!-- BEGIN TEXT -->
  <text xml:lang="en" type="original">
    <!-- FRONT TEXT -->
    <front>
      <dhq:abstract>
        <p>In this paper, we present MINERVA, the first benchmark dataset for the detection of
          musical instruments in non-photorealistic, unrestricted image collections from the realm
          of the visual arts. This effort is situated against the scholarly background of music
          iconography, an interdisciplinary field at the intersection of musicology and art history.
          We benchmark a number of state-of-the-art systems for image classification and object
          detection. Our results demonstrate the feasibility of the task but also highlight the
          significant challenges which this artistic material poses to computer vision. We evaluate
          the system to an out-of-sample collection and offer an interpretive discussion of the
          false positives detected. The error analysis yields a number of unexpected insights into
          the contextual cues that trigger the detector. The iconography surrounding children and
          musical instruments, for instance, shares some core properties, such as an intimacy in
          body language.</p>
      </dhq:abstract>
      <dhq:teaser>
        <p>In this paper, we present MINERVA, the first benchmark dataset for the detection of
          musical instruments in non-photorealistic, unrestricted image collections from the realm
          of the visual arts.</p>
      </dhq:teaser>
    </front>

    <!-- BODY TEXT -->
    <body>
      <div>
        <head>Introduction: the era of the pixel</head>
        <p>The Digital Humanities constitute an intersectional community of praxis, in which the
          application of computing technologies in various subdisciplines in the
            <emph>Geisteswissenschaften</emph> plays a significant role. Surveys of the history of
          the field <ptr target="#hockey2004"/> have stressed that most of the seminal applications
          of computing technology were heavily, if not exclusively, text-oriented: due to the
          hardware and software limitations of the time, analyses of image data (but also audio or
          video data) remained elusive and out of practical reach until relatively late, certainly
          at a larger scale. In the past decade, the application of deep neural networks has
          significantly pushed the state of the art in computer vision, leading to impressive
          advances in tasks such as image classification or object detection <ptr
            target="#lecun2015"/>
          <ptr target="#schmidhuber2015"/>. Even more recently, improvements in the field of
          computer vision have started to find practical applications in study domains outside of
          strict machine learning, such as physics, medicine or even astrology. Supported by this
          technology's (at times rather naive) coverage in the popular media, the communis opinio
          has been eager to herald the advent of the "Era of the Pixel".</p>
        <p>In the Digital Humanities too, the potential of computer vision is nowadays increasingly
          recognized. A programmatic duet of two recent articles on "distant viewing" in the field's
          flagship journal <ptr target="#wevers2020"/>
          <ptr target="#arnold2019"/> leads the way in this respect, emphasizing the privileged role
          these new methodologies can play in the exploration of large data collections in the
          Humanities. The present paper too is situated in a multidisciplinary project in which we
          investigate how modern artificial intelligence can support GLAM institutions (galleries,
          libraries, archives, and museums) in cataloguing and curating their rapidly expanding
          digital assets. As a case study, we shall work with non-photorealistic depictions of
          musical instruments in the artistic domain.</p>
        <p>The structure of this paper is as follows. First, we motivate and contextualize our case
          study of musical instruments from within the scholarly framework of music iconography and
          computer vision, but also from the more pragmatic context of the research project from
          which this focus has emerged. We go on to describe the construction and characteristics of
          an annotated benchmark dataset, the MINERVA dataset, that will be released together with
          this paper, through which we hope to stimulate further research in this area. Using this
          benchmark data, we stress-test the available technology for the identification and
          detection of objects in images and discuss the current limitations of systems. To
          illustrate the broader relevance of our approach, we apply the trained benchmark system
          'in the wild', on unseen and out-of-sample heritage data, followed by a quantitative and
          qualitative evaluation of the results. Finally, we identify what seem to be the most
          relevant directions for future research.</p>
      </div>
      <div>
        <head>Motivation</head>
        <div>
          <head>Music iconography</head>
          <p>The present paper must be understood against the wider scholarly background of music
            iconography, a Humanities field of inquiry with a rich, interdisciplinary history in its
            own right. <ptr target="#buckley1998"/> concisely defined music iconography as a field
            being "concerned with the study of the visual representation of musical topics. Its
            primary materials include portraits of performers and composers, illustrations of
            instruments, occasions of music-making, and the use of musical imagery for purposes of
            metaphorical or allegorical allusion". Because of this wide range of topics, at the
            intersection of art history and musicology <ptr target="#baldassarre2007"/>
            <ptr target="#baldassarre2008"/>, the field takes pride of its interdisciplinarity.</p>
          <p>Music iconography deliberately adopts a "methodological plurality" <ptr
              target="#baldassarre2007"/> which is increasingly complemented with digital
            approaches. A major achievement in this respect has been the establishment (in 1971) and
            continued expansion and curation of an international digital inventory for musical
            iconography, the <emph>Répertoire International d'Iconographie Musicale</emph> (RIDIM).
            Now publicly available as an online web resource (<ref target="https://ridim.org/"
              >https://ridim.org/</ref>), RIDIM functions as a reference image database, designed to
            facilitate the efficient yet powerful description and discovery of music-related art
            works <ptr target="#green2013"/>. The need for such an international inventory has been
            acknowledged as early as 1929 and its significant scope facilitates the international
            study of music-related phenomena and their depiction across the visual arts.</p>
          <p>Music iconography has an important tradition of focused studies targeting the deep,
            interpretive analysis of individual artworks or small collections of them. Such
            hermeneutic case studies have the advantage of depth, but understandably lack a more
            panoramic perspective on the phenomena of interest and, for instance, diachronic or
            synchronic trends and shifts therein. The large-scale, "serial" study of musical
            instruments as depicted across the visual arts remains a desideratum in the field and
            has the potential of bringing a macroscopic perspective to historical developments. In
            the present paper, we explore the feasibility of applying methods from present-day
            computer vision, in an attempt to scale up current approaches. The primary motivation of
            this endeavour is that digital music iconography – or "Distant" music iconography, in an
            analogy to similar developments in literary studies <ptr target="#wevers2020"/>
            <ptr target="#arnold2019"/> – in principle has much to gain from such methods, at least
            if they are carefully applied and in continuous interaction with experts in the domain.
            Our focal point is the automated identification and detection of individual musical
            instruments in unrestricted, digitized materials from the realm of the visual arts.</p>
          <p>This scholarly initiative is embedded in the collaborative research project INSIGHT
            (Intelligent Neural Systems as InteGrated Heritage Tools), which aims to stimulate the
            application of Artificial Intelligence to the rapidly expanding digital collections of a
            selection of federal museum clusters in Belgium.<note>See <ref
                target="https://hosting.uantwerpen.be/insight/"
                >https://hosting.uantwerpen.be/insight/</ref>. This project is generously funded by
              the Belgian Federal Research Agency BELSPO under the BRAIN-be program.</note> One
            important, transcommunal aspect to Belgium's cultural history relates to music and
            musical history, with the invention of the saxophone by Adolphe Sax as an iconic
            example. An additional factor is the presence of the Musical Instruments Museum in the
            capital (Brussels) that contributed significantly to international research projects in
            this area (and which is a partner in the INSIGHT project). This contextualization,
            finally, is also important to understand our specific choice for the topic of musical
            instruments, as a representative and worthwhile case study on the application of modern
            machine learning technology in digital heritage studies.</p>
        </div>
      </div>
      <div>
        <head>Computer vision</head>
        <p>The methodology for the present paper largely derives from machine learning and more
          specifically computer vision, a field concerned with computational algorithms that can
          mimic the perceptual abilities of humans and their capacity to construct high-level
          interpretations from raw visual stimuli <ptr target="#ballard1982"/>. In the past decade,
          this field has gone through a remarkable renaissance, following the emergence of powerful
          learning techniques based on so-called neural networks. In particular the advent of
          "convolutional" networks <ptr target="#lecun2015"/> has led to dramatic advances in the
          state of the art for a number of standard applications, including image classification
          ("Is this an image of a cat or a dog?") and object detection ("Draw a bounding box around
          any cats in this image"). For some of these tasks, modern computer systems have even been
          shown to rival the performance of humans <ptr target="#russakovsky2015"/>. In spite of the
          impressive advances in recent computer vision research, it is generally acknowledged that
          the state of the art is still confronted with a number of major, as yet unsolved,
          challenges. In this section we highlight four concrete issues that are especially
          pressing, given the focus of this paper on image collections in the artistic domain. These
          challenges motivate our work from the point of view of computer vision, rather than art
          history.</p>
        <div>
          <head>Photo-realism</head>
          <p>One major hurdle is that computer vision nowadays strongly gravitates towards so-called
            photo-realistic material, i.e. digitized or born-digital versions of photographs that do
            not actively attempt to distort the reality they depict. The best example in this
            respect is the influential ImageNet dataset <ptr target="#russakovsky2015"/>, that
            offers highly realistic photographic renderings of everyday concepts drawn from
            WordNet's lexical database. While some more recent heritage collections of course abound
            in such photo-realistic material (e.g. advertisements in historic newspapers),
            traditional photography does not take us further back in time than the nineteenth
            century <ptr target="#hertzmann2018"/>. Additionally, the Humanities study many other
            visual arts that prioritize much less photorealistic representation and focus even on
            completely 'fictional' renderings of (potentially imagined or historical) realities.
            While there has been some encouraging and worthwhile prior work into the application of
            computer vision to non-photorealistic depictions, this work is generally more scattered
            and the results (understandably) less advanced than those reported for the
            photorealistic domain. Inspiring recent studies in this area include <ptr
              target="#crowley2014"/>
            <ptr target="#van2015"/>
            <ptr target="#seguin2018"/>
            <ptr target="#bell2019"/>.</p>
        </div>
        <div>
          <head>Data scarcity</head>
          <p>It is a well-known limitation that convolutional neural networks require large amounts
            of manually annotated example data (or training data) in order to perform well. To
            address this issue, the community has released several public datasets over the years
              <ptr target="#xiang2014"/>
            <ptr target="#russakovsky2015"/>
            <ptr target="#mensink2014"/>
            <ptr target="#strezoski2017"/>
            <ptr target="#lin2014"/> which has allowed the successful training of a large set of
            neural architectures <ptr target="#he2016"/>
            <ptr target="#szegedy2015"/>
            <ptr target="#simonyan2014"/>. However, the nature of the images included in these
            datasets is mostly photo-realistic, also because such images are relatively
            straightforward to obtain and annotate. These image collections are very different in
            terms of texture, content and availability from the sort of data that can nowadays be
            found in the digital heritage domain.</p>
          <p>Computer vision researchers interested in the artistic domain have attempted to
            alleviate the relative dearth of training data by either releasing domain-specific
            datasets <ptr target="#mensink2014"/>
            <ptr target="#strezoski2017"/> or through the application of transfer learning <ptr
              target="#sabatelli2018"/>, a machine learning paradigm which allows the application of
            neural networks to domains where training data is scarce. For image classification, for
            instance, these efforts have indeed greatly contributed to overall feasibility of
            applying computer vision outside the photo-realistic domain <ptr target="#sabatelli2018"
            />. Both approaches, however, have limitations when it comes to the complementary task
            of object detection. Popular datasets such as the Rijksmuseum collection <ptr
              target="#mensink2014"/> or the more recent OmniArt dataset <ptr
              target="#strezoski2017"/> do not come with the metadata required for object-detection
            problems.</p>
          <p>With this work, we take one step forward in addressing these limitations. Firstly, the
            MINERVA dataset that we present below, specifically tackles the problem of object
            detection within the broader heritage domain of the visual arts, introducing a novel
            benchmark for researchers working at the intersection of computer vision and art
            history. Secondly, we present a number of baseline results on the newly introduced
            dataset. The results are reported for a representative set of common architectures,
            which were pre-trained on photo-realistic images. This allows us to investigate to what
            extent these methods can be reused when tested on artistic images.</p>
        </div>
        <div>
          <head>Irrelevant training categories</head>
          <p>Previous studies have demonstrated the feasibility of "pretraining": with this
            approach, networks are first trained on (large) photorealistic collections (i.e. the
            source domain) and then applied downstream (or further fine-tuned) on an out-of-sample
            target domain, that has much less annotated data available. While generally useful, this
            approach is still confronted with the problem that the annotation labels or categories
            attested in the source domain are often of little interest within the target domain
            (i.e. art history, in the present case). The popular Pascal-VOC dataset <ptr
              target="#everingham2010"/>, for instance, tackles the detection of 20 classes, out of
            which more than a third constitute different kinds of transportation systems, such as
            trains, boats, motorcycles and cars. Naturally, these means of transportation are very
            unlikely to be represented in artworks that date back to the premodern period. The more
            complex MS-COCO dataset <ptr target="#lin2014"/> presents similar problems: even though
            the amount of classes increases to 80, most of the objects which should be detected are
            again unlikely to be represented within historical works of art, since they correspond
            to objects which have only been relatively recently invented such as "microwave",
            "cell-phone", "tv-monitor", "laptop", or "remote", and the like. This poses a serious
            constraint when it comes to the use of pre-trained object-detectors for artistic
            collections. As with most supervised learning algorithms, models trained on these
            collections will only perform well on the sort of data on which they have been
            explicitly trained. To illustrate this model bias, we report some (nonsensical)
            detections in the first row of images presented in <ref target="#figure01">Figure
              1a</ref>.</p>
        </div>
        <div>
          <head>Robustness of the models</head>
          <p>Popular object detectors such as YOLO <ptr target="#redmon2018"/> and Fast R-CNN <ptr
              target="#ren2017"/> have been designed to perform well on the above-mentioned
            photo-realistic datasets. However, the variance of the samples denoting a specific class
            within these datasets is usually much smaller when compared to that in artistic
            collections. As an example, we refer to a number of images representing the person class
            within the Pascal-VOC dataset: we can observe from the two leftmost images of the bottom
            row of <ref target="#figure01">Figure 1</ref> that the representation of a 'person' is
            overall relatively unambiguous and hardly distorted. As a result, the person class is
            usually easily detected by e.g. the YOLO architecture. However, we can see that this
            task already becomes harder when a person has to be detected within a painting
            (potentially with a highly distorted representation of the humans in the scene). As
            shown by the two rightmost images of the bottom row (<ref target="#figure01">Figure
              1b</ref>), a YOLO-V3 model does not see most of the persons represented in the
            paintings and misclassifies them as non-human beings (e.g. "bear").</p>
          <figure xml:id="figure01">
            <head>Examples showing the limitations that occur when a standard object detector
              trained on photo-realistic images is tested in the domain of the visual arts. Figure
              1a: Four anecdotal examples showing that the "person” class is usually reasonably
              detected by the YOLO architecture, although other, non-sensical detections frequently
              occur. Figure 1b: the two images on the left show that the variation in depiction of
              people is limited in photorealistic material, in comparison to the artistic
              representations of people (two examples to the right).</head>
            <graphic url="resources/images/figure01.png"/>
          </figure>
          <p>All examples in <ref target="#figure01">Figure 1</ref> come from a pretrained YOLO-V3
            model <ptr target="#redmon2018"/> which has been originally trained on the COCO dataset
              <ptr target="#lin2014"/> and then tested on artworks coming from <ptr
              target="#mensink2014"/> and <ptr target="#strezoski2017"/>. The images presented in
            the first row illustrate that the network is biased towards making detections which are
            very unlikely to appear in premodern depictions. These detections correspond to the
            identification of objects such as "suitcase", "umbrella" or "frisbee" and "banana". The
            two last images presented in the second row show that standard models fail to properly
            recognize a simple class such as person. In fact, they fully fail in detecting most of
            the persons that are present in the artworks due to these representations being highly
            different from the persons that are present in the Pascal-VOC dataset (first two images
            of the second row).</p>
        </div>
      </div>
      <div>
        <head>MINERVA: dataset description</head>
        <p>In this section, we describe MINERVA, the annotated dataset in the field of object
          detection that is presented in this work. This novel benchmark dataset will be released
          jointly with this paper.<note>All code used in this paper is publicly available from this
            repository: <ref target="https://github.com/paintception/MINeRVA"
              >https://github.com/paintception/MINeRVA</ref>. Likewise, the MINERVA dataset can be
            obtained from this DOI on Zenodo: 10.5281/zenodo.3732580.</note> The main task under
          scrutiny here is the detection of musical instruments in non-photorealistic, unrestricted
          image collections from the artistic domain. We have named the dataset with the acronym
          MINERVA, which stands for 'Musical INstrumEnts Represented in the Visual Arts', after the
          Roman goddess of the arts (amongst many other things).</p>
        <div>
          <head>Data Sources</head>
          <p>The base data for our annotation effort was assembled in a series of 'concentric'
            collection campaigns, where we started from smaller, but high-quality datasets and
            gradually expanded into larger, albeit less well curated data sources.</p>
          <list type="ordered">
            <item><label>RIDIM</label>: We harvested a collection of high-quality images from the
              RIDIM database, in those cases where the database entries provided an unambiguous
              hyperlink to a publicly accessible image. These records were already assigned MIMO
              codes by a community of domain experts, which provided important support to our
              in-house annotators (especially during the first experimental rounds of
              annotations).</item>
            <item><label>RMFAB/RMAH</label>: We expanded on the core RIDIM data by including
              (midrange resolution) images from the digital collections of two federal museums in
              Brussels: the RMFAB (Royal Museums of Fine Arts of Belgium, Brussels) and the RMAH
              (Royal Museums of Art and History, Brussels). These images were selected on the basis
              of previous annotations that suggested they included depictions of musical
              instruments, although no more specific labels (e.g. MIMO codes) were available for
              these records at this stage. Copyrighted artworks could not be included for obvious
              reasons (copyright lasts for 70 years from the death of the creator under Belgian
              intellectual law).</item>
            <item><label>Flickr</label>: To scale up our annotation efforts, finally, we collected a
              larger dataset of images from the well-known image hosting service 'Flickr'
              (www.flickr.com). We harvested all images from a community-curated collection of
              depictions of musical instruments in the visual arts pre-dating 1800.<note><ref
                  target="https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1"
                  >https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1</ref></note>
              This third campaign yielded much more data than the former two, but these were more
              noisy and contained a variety of false positives that had to be manually deleted
              during the annotation phase.</item>
          </list>
          <p>Our collection efforts were inclusive, and the resulting dataset should be considered
            as "unrestricted", covering a variety of periods, genres and materials (although it was
            not feasible to include more precise metadata about these aspects in the dataset). Note
            that, exactly because of this highly mixed data origin, the distribution in MINERVA does
            not give a faithful representation of any kind of historic reality: music iconography
            gives a highly colored perspective on "popular" instruments in art history and some
            instruments may not often have been depicted, even though they were popular at the time.
            Likewise, other instruments are likely to be over-represented in iconography.</p>
        </div>
        <div>
          <head>Vocabulary</head>
          <p>To increase the interoperability of the dataset, individual instruments have been
            unambiguously identified using their MIMO codes. The MIMO (Musical Instrument Museums
            Online) initiative is an international consortium, well known for its online database of
            musical instruments, aggregating data and metadata from multiple heritage institutions
              <ptr target="#dolan2017"/>.<note>
              <ref target="https://web.archive.org/save/https://www.mimo-international.com/MIMO/"
                >https://web.archive.org/save/https://www.mimo-international.com/MIMO/</ref></note>
            An important contribution is their development of a uniform metadata documentation
            standard for the field, including a (multilingual) vocabulary to identify musical
            instruments in an interoperable manner. The MIMO ontology is hierarchical, meaning that
            each individual leaf node in their concept tree (e.g. 'viola') is a hyponym of a wider
            instrument category (e.g. 'viola' ∈ 'string instruments'). <ref target="#table01">Table
              1</ref> shows examples of annotation labels from this ontology. Our dataset provides a
            spreadsheet that allows for the easy mapping of individual instruments to their
            instrument category. Below, we shall report experiments for the more fine-grained and
            less granular, hypernym versions of the categorization task.</p>
          <table xml:id="table01">
            <head>Examples of annotation labels from the MIMO ontology (not all were encountered in
              MINERVA).</head>
            <row>
              <cell><hi rend="bold">Instrument hypernym</hi></cell>
              <cell><hi rend="bold">Stringed instruments</hi></cell>
              <cell><hi rend="bold">Wind instruments</hi></cell>
              <cell><hi rend="bold">Percussion instruments</hi></cell>
              <cell><hi rend="bold">Keyboard instruments</hi></cell>
              <cell><hi rend="bold">Electronic instruments</hi></cell>
            </row>
            <row>
              <cell><hi rend="italic">Example instruments</hi></cell>
              <cell>Lute, psaltery, fiddle, viola da gamba, cittern</cell>
              <cell>Transverse flute, end-blown trumpet, horn, shawm, bagpipe</cell>
              <cell>Tambourine, cylindrical drum, frame drum, friction drum, bell</cell>
              <cell>Pianoforte, virginal, portative organ, harpsichord, clavichord</cell>
              <cell>Electric guitar, synthesizer, theremin, vocoder, mellotron</cell>
            </row>
          </table>
        </div>
        <div>
          <head>Annotation process</head>
          <p>Using the conventional method of rectangular bounding boxes, we have manually annotated
            16,142 musical instruments (of which 172 unique) in a collection of 11,765 images,
            within the open-source <ref target="https://cytomine.be">Cytomine</ref> software
            environment <ptr target="#marée2016"/>. Often multiple instruments appeared within the
            same images and bounding boxes were therefore allowed to overlap. Example annotations
            and a screenshot of the annotation environment are presented in <ref target="#figure02"
              >Figure 2</ref>.</p>
          <p>The dataset contains artistic objects from diverse periods and of various types,
            ranging from paintings, sculptures, drawings, to decorative arts, manuscript
            illuminations and stained-glass windows. Thus, they involve a daunting diversity of
            media, techniques and modes. Whereas in some cases the images were straightforward to
            annotate (e.g. an image representing a bell in full frame), several obstacles occurred
            on a recurrent basis. These obstacles can be linked to three parameters:</p>
          <list type="ordered">
            <item><label>Representation</label>: A challenging aspect was the variety of artistic
              depiction modes represented in the dataset, ranging from photo-realistic renderings to
              heavily stylized depictions from specific art-historical movements (e.g.
              impressionism, pointillism, fauvism, cubism, ...) (<ref target="#figure03">Figure
                3a</ref>). Additionally, visibility could be low due to a proportionally small
              instrument depiction or the profusion of details (<ref target="#figure03">Figure
                3b</ref>). In some instances, the state of the depicted object and its medium made
              the detection of the instrument difficult, e.g. a damaged medieval tympanon (<ref
                target="#figure03">Figure 3b</ref>).</item>
            <item><label>Quality</label>: Other, more pragmatic issues arose from the images
              themselves. Occasionally, the quality of the images was too low to be able to detect
              the instruments (e.g. low resolution or compression defects) (<ref target="#figure03"
                >Figure 3c</ref>). A great deal of the images did not meet international quality
              standards for heritage reproduction photography (uniform and neutral environment and
              lighting, frontal point of view), which implies that the instruments were even more
              difficult to detect.</item>
            <item><label>Boxes</label>: The use of a rectangular shape for the bounding boxes
              sometimes has limitations and implied a certain lack of precision, e.g. in the case of
              a diagonally positioned flute, or in the case of overlapping instruments (<ref
                target="#figure03">Figure 3d</ref>). For some instruments which consist of several
              parts, e.g. a violin and its bow, only the main part (the violin) was
              annotated.</item>
          </list>
          <figure xml:id="figure02">
            <head>Illustration of the annotation interface in Cytomine <ptr target="#marée2016"
              />.</head>
            <graphic url="resources/images/figure02.png"/>
          </figure>
          <figure xml:id="figure03">
            <head>Examples of difficulties encountered when annotating images.</head>
            <graphic url="resources/images/figure03.png"/>
          </figure>
        </div>
        <div>
          <head>Characteristics</head>
          <p>An important share of the annotations which we collected were singletons, i.e.
            instruments that were only encountered once or twice. Although we release the full
            dataset, we shall from now on only consider instruments that occurred at least three
            times that allow for a conventional machine learning setup (with non-overlapping train,
            validation and test sets, that include at least one instance of each label). Whereas the
            full MIMO vocabulary covers over 2,000 vocabulary terms for individual instruments, only
            a fraction of these were attested in the 4,183 images which we use below (overview in
              <ref target="#table01">Table 1</ref>). Note that this table shows a considerable drop
            in the original number of images that we annotated, because we only included images that
            (a) actually contained an instrument and (b) images depicting instruments that occurred
            at least thrice.</p>
          <p>93 different instrument categories appear at least thrice in the dataset. A
            visualization of the heavily skewed distribution of the different instruments can be
            seen in <ref target="#figure04">Figure 4</ref>, where each instrument is represented
            together with its corresponding MIMO code (between parentheses). This distribution
            exposes two core aspects of this dataset (but also of music iconography in general): (i)
            its strong Western-European bias, which has been historically acknowledged, and which
            scholars are actively trying to correct nowadays, but which is a slow process; (ii) the
            'heavy-tail' distribution associated with cultural data in general; i.e. only a fraction
            of instruments, such as the lute, harp and violin, are depicted with a high frequency,
            the rest occurs much more sparsely.</p>
          <figure xml:id="figure04">
            <head>Distribution of the instrument types in the full MINERVA dataset.</head>
            <graphic url="resources/images/figure04.png"/>
          </figure>
        </div>
        <div>
          <head>Versions and splits</head>
          <p>The label imbalance described in the previous paragraph is a significant issue for
            machine learning methods. We therefore experiment with the data in five versions (that
            are available from the repository) that correspond to object detection tasks of varying
            complexity. We start by exploring whether it is possible to just detect the presence of
            an instrument in the different artworks, without the additional need of also predicting
            the class of the detected instrument. We refer to this benchmark as single-instrument
            object detection. We then move to three more challenging tasks in which we also aim at
            correctly classifying the content of the detected bounding boxes. We include data for
            this detection task for the top-5, the top-10 and top-20 most frequently occurring
            instruments, a customary practice in the field. Finally, we also repeat this task for
            all images, but with the "hypernym" labels of the instrument categories (see <ref
              target="#figure05">Figure 5</ref>).</p>
          <p>Each version of the dataset comes with its own training, development and testing
            splits, where we offer the guarantee that at least one of the instrument classes in the
            task is represented in each of the splits. Additionally, the splits are stratified so
            that the class distribution is approximately the same in each split. The number of
            images per split in each version is summarized in <ref target="#table02">Table 2</ref>.
            The hypernym version of the dataset is not reported in this table as it shares the same
            images and splits as the single-instrument version (they both contain all instruments).
            We used a standard implementation <ptr target="#pedregosa2011"/> for a randomized and
            shuffled split at the level of images and the following, approximate proportions: 1/2
            train, 1/4 dev, and 1/4 test. Images may contain multiple instruments, so that the
            actual number of instruments (as opposed to images) may vary relatively strongly across
            splits.</p>
          <table xml:id="table02">
            <head>Image and instruments distributions of the training, development and test sets for
              the four different benchmarks presented in this paper (single instrument, top-5
              instruments, top-10 instruments and top-20 instruments).</head>
            <row>
              <cell/>
              <cell><hi rend="bold">Training-set</hi></cell>
              <cell/>
              <cell><hi rend="bold">Dev-set</hi></cell>
              <cell/>
              <cell><hi rend="bold">Test-set</hi></cell>
              <cell/>
              <cell><hi rend="bold">Total</hi></cell>
              <cell/>
            </row>
            <row>
              <cell/>
              <cell>Imag</cell>
              <cell>Inst</cell>
              <cell>Imag</cell>
              <cell>Inst</cell>
              <cell>Imag</cell>
              <cell>Inst</cell>
              <cell>Imag</cell>
              <cell>Inst</cell>
            </row>
            <row>
              <cell>Single inst</cell>
              <cell>1857</cell>
              <cell>4243</cell>
              <cell>1137</cell>
              <cell>2288</cell>
              <cell>1189</cell>
              <cell>2102</cell>
              <cell>4183</cell>
              <cell>8633</cell>
            </row>
            <row>
              <cell>Top-5 inst</cell>
              <cell>952</cell>
              <cell>1589</cell>
              <cell>540</cell>
              <cell>852</cell>
              <cell>724</cell>
              <cell>1173</cell>
              <cell>2216</cell>
              <cell>3614</cell>
            </row>
            <row>
              <cell>Top-10 inst</cell>
              <cell>1227</cell>
              <cell>2147</cell>
              <cell>680</cell>
              <cell>1127</cell>
              <cell>898</cell>
              <cell>1506</cell>
              <cell>2805</cell>
              <cell>4780</cell>
            </row>
            <row>
              <cell>Top-20 inst</cell>
              <cell>1471</cell>
              <cell>2915</cell>
              <cell>860</cell>
              <cell>1543 </cell>
              <cell>1047</cell>
              <cell>1838</cell>
              <cell>3378</cell>
              <cell>6296</cell>
            </row>
          </table>
          <figure xml:id="figure05">
            <head>Distribution of the 5 hypernym categories over the three splits in the MINERVA
              dataset.</head>
            <graphic url="resources/images/figure05.png"/>
          </figure>
        </div>
      </div>
      <div>
        <head>Benchmark experiments</head>
        <div>
          <head>Classification</head>
          <p>In the first benchmark experiment, we start by investigating whether convolutional
            neural networks are able to correctly classify the different instruments that are
            present in the dataset. That means that we focus on the image classification task and
            postpone the task of object detection to the next section. To this end, we have
            extracted the various patches delineated by the bounding boxes in the detection dataset
            as stand-alone instances. Note, however, that patches from the same images always ended
            in the same split, to avoid information leakage across the splits. Example patches are
            shown in <ref target="#figure06">Figure 6</ref>.</p>
          <figure xml:id="figure06">
            <head>Examples of the patches delineated by the bounding boxes, extracted from MINERVA
              images for the classification experiment.</head>
            <graphic url="resources/images/figure06.png"/>
          </figure>
          <p>Next, we tackled this task as a standard machine-learning classification problem for
            which we applied a representative selection of established neural network architectures.
            All of these networks were pretrained on the Rijksmuseum dataset <ptr
              target="#mensink2014"/>, for which the weights are publicly available <ptr
              target="#sabatelli2018"/>. The tested architectures are: VGG19 <ptr
              target="#simonyan2014"/>, Inception-V3 <ptr target="#szegedy2015"/> and ResNet <ptr
              target="#he2016"/>. This approach is motivated by previous work <ptr
              target="#sabatelli2018"/> which shows that when it comes to the classification images
            from the domain of cultural heritage, popular neural architectures which have been
            trained on the large Rijksmuseum collection, can outperform the same kind of
            architectures that are pre-trained on ImageNet only. In order to maximize the final
            classification performance, all network parameters get fine-tuned, using the Adam
            optimizer <ptr target="#kingma2014"/> and minimizing the conventional categorical
            cross-entropy loss function over mini-batches of 32 samples. Additionally, we applied 3
            different learning rates: 0.001, 0.0001, 0.00001. In order to handle the skewed
            distribution of the classes, we experimented with models including and excluding
            oversampling. The training regime is interrupted as soon the validation loss does not
            decrease for five epochs in a row.</p>
          <p>In <ref target="#table01">Table 2</ref> and <ref target="#table03">Table 3</ref> we
            report the results in terms of Accuracy and F1-score for the MINERVA test sets. For the
            individual instruments, we do so for four versions of the dataset of increasing
            complexity: the top-5 instruments, top-10 instruments, top-20 instruments and the entire
            dataset. Analogously we report the scores for a classification experiment where the
            object detector is trained on the instrument hypernyms as class labels.</p>
          <table xml:id="table03">
            <head>Classification results on the MINERVA test set for the three architectures (best
              results in bold).</head>
            <row>
              <cell/>
              <cell>Top-5 inst</cell>
              <cell/>
              <cell>Top-10 inst</cell>
              <cell/>
              <cell>Top-20 inst</cell>
              <cell/>
              <cell>All inst</cell>
              <cell/>
              <cell>Hypernyms</cell>
              <cell/>
            </row>
            <row>
              <cell>CNN</cell>
              <cell>Acc.</cell>
              <cell>F1</cell>
              <cell>Acc.</cell>
              <cell>F1</cell>
              <cell>Acc.</cell>
              <cell>F1</cell>
              <cell>Acc.</cell>
              <cell>F1</cell>
              <cell>Acc.</cell>
              <cell>F1</cell>
            </row>
            <row>
              <cell>R-Net</cell>
              <cell>68.71</cell>
              <cell>64.10</cell>
              <cell>52.85</cell>
              <cell>41.55</cell>
              <cell>30.73</cell>
              <cell>8.45</cell>
              <cell>26.36</cell>
              <cell>2.08</cell>
              <cell>72.26</cell>
              <cell>52.66 </cell>
            </row>
            <row>
              <cell>V3</cell>
              <cell><hi rend="bold">73.66</hi></cell>
              <cell><hi rend="bold">70.29</hi></cell>
              <cell><hi rend="bold">55.51</hi></cell>
              <cell><hi rend="bold">44.77</hi></cell>
              <cell><hi rend="bold">36.51</hi></cell>
              <cell><hi rend="bold">19.06</hi></cell>
              <cell><hi rend="bold">27.02</hi></cell>
              <cell><hi rend="bold">6.67</hi></cell>
              <cell><hi rend="bold">75.80</hi></cell>
              <cell><hi rend="bold">57.03</hi></cell>
            </row>
            <row>
              <cell>V19</cell>
              <cell>48.33</cell>
              <cell>35.92</cell>
              <cell>37.52</cell>
              <cell>15.22</cell>
              <cell>33.41</cell>
              <cell>9.87</cell>
              <cell>20.17</cell>
              <cell>1.72</cell>
              <cell>66.41</cell>
              <cell>40.35</cell>
            </row>
          </table>
          <table xml:id="table04">
            <head>Confusion matrix for the classification experiment with ResNet on the MINERVA test
              set (the top-10 most frequently occurring instruments).</head>
            <row>
              <cell><hi rend="bold">Predicted label / Gold label</hi></cell>
              <cell><hi rend="bold">Bagpipe</hi></cell>
              <cell><hi rend="bold">E-b trumpet</hi></cell>
              <cell><hi rend="bold">Harp</hi></cell>
              <cell><hi rend="bold">Horn</hi></cell>
              <cell><hi rend="bold">Lute</hi></cell>
              <cell><hi rend="bold">Lyre</hi></cell>
              <cell><hi rend="bold">Por. organ</hi></cell>
              <cell><hi rend="bold">Rebec</hi></cell>
              <cell><hi rend="bold">Shawm</hi></cell>
              <cell><hi rend="bold">Violin</hi></cell>
            </row>
            <row>
              <cell><hi rend="bold">Bagpipe</hi></cell>
              <cell><hi rend="bold">31</hi></cell>
              <cell>0</cell>
              <cell>10</cell>
              <cell>6</cell>
              <cell>8</cell>
              <cell>1</cell>
              <cell>2</cell>
              <cell>0</cell>
              <cell>7</cell>
              <cell>17</cell>
            </row>
            <row>
              <cell><hi rend="bold">E-b trumpet</hi></cell>
              <cell>4</cell>
              <cell><hi rend="bold">72</hi></cell>
              <cell>19</cell>
              <cell>2</cell>
              <cell>14</cell>
              <cell>1</cell>
              <cell>3</cell>
              <cell>1</cell>
              <cell>38</cell>
              <cell>21</cell>
            </row>
            <row>
              <cell><hi rend="bold">Harp</hi></cell>
              <cell>8</cell>
              <cell>2</cell>
              <cell><hi rend="bold">227</hi></cell>
              <cell>1</cell>
              <cell>10</cell>
              <cell>3</cell>
              <cell>11</cell>
              <cell>0</cell>
              <cell>10</cell>
              <cell>19</cell>
            </row>
            <row>
              <cell><hi rend="bold">Horn</hi></cell>
              <cell>7</cell>
              <cell>5</cell>
              <cell>14</cell>
              <cell><hi rend="bold">9</hi></cell>
              <cell>16</cell>
              <cell>9</cell>
              <cell>1</cell>
              <cell>2</cell>
              <cell>5</cell>
              <cell>14</cell>
            </row>
            <row>
              <cell><hi rend="bold">Lute</hi></cell>
              <cell>6</cell>
              <cell>10</cell>
              <cell>17</cell>
              <cell>6</cell>
              <cell><hi rend="bold">199</hi></cell>
              <cell>6</cell>
              <cell>5</cell>
              <cell>1</cell>
              <cell>5</cell>
              <cell>42</cell>
            </row>
            <row>
              <cell><hi rend="bold">Lyre</hi></cell>
              <cell>3</cell>
              <cell>0</cell>
              <cell>19</cell>
              <cell>1</cell>
              <cell>13</cell>
              <cell><hi rend="bold">5</hi></cell>
              <cell>2</cell>
              <cell>0</cell>
              <cell>3</cell>
              <cell>11</cell>
            </row>
            <row>
              <cell><hi rend="bold">Por. organ</hi></cell>
              <cell>3</cell>
              <cell>0</cell>
              <cell>10</cell>
              <cell>1</cell>
              <cell>0</cell>
              <cell>0</cell>
              <cell><hi rend="bold">57</hi></cell>
              <cell>0</cell>
              <cell>1</cell>
              <cell>4</cell>
            </row>
            <row>
              <cell><hi rend="bold">Rebec</hi></cell>
              <cell>5</cell>
              <cell>2</cell>
              <cell>14</cell>
              <cell>0</cell>
              <cell>9</cell>
              <cell>0</cell>
              <cell>4</cell>
              <cell><hi rend="bold">7</hi></cell>
              <cell>1</cell>
              <cell>23</cell>
            </row>
            <row>
              <cell><hi rend="bold">Shawm</hi></cell>
              <cell>4</cell>
              <cell>11</cell>
              <cell>25</cell>
              <cell>2</cell>
              <cell>11</cell>
              <cell>2</cell>
              <cell>4</cell>
              <cell>6</cell>
              <cell><hi rend="bold">40</hi></cell>
              <cell>13</cell>
            </row>
            <row>
              <cell><hi rend="bold">Violin</hi></cell>
              <cell>6</cell>
              <cell>12</cell>
              <cell>29</cell>
              <cell>4</cell>
              <cell>35</cell>
              <cell>4</cell>
              <cell>7</cell>
              <cell>11</cell>
              <cell>6</cell>
              <cell><hi rend="bold">202</hi></cell>
            </row>
          </table>
        </div>
        <div>
          <head>Detection</head>
          <p>For the second benchmark experiment we report the results that we have obtained on the
            four of the five detection benchmarks introduced in the previous section. The way the
            different instruments are distributed in their respective test sets is visually
            represented in the first image of each row of <ref target="#figure07">Figure 7</ref>.
            For our experiments, we use the popular YOLO-V3 <ptr target="#redmon2018"/> architecture
            which we fully fine-tune during training. To explore the benefits that transfer learning
            could bring to the artistic domain, we initialize the network with the weights that are
            obtained after training the model on the MS-COCO dataset <ptr target="#lin2014"/>. The
            network gets then trained either with the Adam optimizer <ptr target="#kingma2014"/> or
              RMSprop<note>There is no officially published reference for RMSprop, but scholars
              commonly refer to this lecture from Geoffrey Hinton and colleagues: <ref
                target="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
                >https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</ref></note>
            over mini-batches of 8 images.</p>
          <p>To assess the performance of the neural network, we follow the same evaluation protocol
            that characterizes object detection problems in CV <ptr target="#lin2014"/>. Each
            detected bounding box is compared to the bounding box which has been annotated on the
            Cytomine platform. We only consider bounding boxes for which the confidence level is ≥
            0.05, following the protocol established in <ptr target="#everingham2010"/>. We then
            compute the "Intersection over Union" (IoU) for measuring how much the detected
            bounding-boxes differ from the ground-truth ones. To assess whether a prediction can be
            considered as a "true positive" or a "false positive", we define two, increasingly
            restrictive metrics: first, IoU ≥ 10 and, secondly, IoU ≥ 50. This approach is again
            inspired by <ptr target="#gonthier2018"/>, where the authors report additional results
            with an IoU ≥ 10 on their IconArt dataset. <ref target="#table05">Table 5</ref> lists
            precision, recall and average precision (AP) scores for each detected class of each data
            version and <ref target="#figure07">Figure 7</ref> visually shows the number of true and
            false positive predictions in all cases. Examples of correct detections are shown in
              <ref target="#figure08">Figure 8</ref>.</p>
          <table xml:id="table05">
            <head>A quantitative analysis of the results obtained on the four localization
              benchmarks introduced in this work. To distinguish different benchmarks in the table
              we separate them by a double line. We report the precision, recall and
              average-precision scores for each detected class.</head>
            <row>
              <cell><hi rend="bold">Instrument ≥ IoU</hi></cell>
              <cell><hi rend="bold">Precision</hi></cell>
              <cell><hi rend="bold">Recall</hi></cell>
              <cell><hi rend="bold">AP</hi></cell>
            </row>
            <row>
              <cell>Single-instrument ≥ <lb/>10 Single-instrument ≥ 50</cell>
              <cell>0.63 <lb/>0.47</cell>
              <cell>0.42 <lb/>0.31</cell>
              <cell>0.35 <lb/>0.22</cell>
            </row>
            <row>
              <cell>Stringed-Instruments ≥ 10 <lb/>Stringed-Instruments ≥ 50</cell>
              <cell>0.65 <lb/>0.53</cell>
              <cell>0.36 <lb/>0.29</cell>
              <cell>0.28 <lb/>0.20</cell>
            </row>
            <row>
              <cell>Wind-Instruments ≥ 10 <lb/>Wind-Instruments ≥ 50</cell>
              <cell>0.43 <lb/>0.32</cell>
              <cell>0.07 <lb/>0.05</cell>
              <cell>0.04 <lb/>0.02</cell>
            </row>
            <row>
              <cell>Percussion-Instruments ≥ 10 <lb/>Percussion-Instruments ≥ 50</cell>
              <cell>0.32 <lb/>0.21</cell>
              <cell>0.04 <lb/>0.03</cell>
              <cell>0.02 <lb/>0.01</cell>
            </row>
            <row>
              <cell>Keyboard-Instruments ≥ 10 <lb/>Keyboard-Instruments ≥ 50</cell>
              <cell> 0.61 <lb/>0.45</cell>
              <cell> 0.11 <lb/>0.08</cell>
              <cell> 0.07 <lb/>0.04</cell>
            </row>
            <row>
              <cell>Electronic-Instruments ≥ 10 <lb/>Electronic-Instruments ≥ 50</cell>
              <cell>- <lb/>-</cell>
              <cell>- <lb/>-</cell>
              <cell>- <lb/>-</cell>
            </row>
            <row>
              <cell>Harp ≥ 10 <lb/>Harp ≥ 50</cell>
              <cell>0.68 <lb/>0.60</cell>
              <cell>0.62 <lb/>0.54</cell>
              <cell>0.55 <lb/>0.46</cell>
            </row>
            <row>
              <cell>Lute ≥ 10 <lb/>Lute ≥ 50</cell>
              <cell>0.57 <lb/>0.47</cell>
              <cell>0.43 <lb/>0.35</cell>
              <cell>0.36 <lb/>0.26</cell>
            </row>
            <row>
              <cell>Violin ≥ 10 <lb/>Violin ≥ 50</cell>
              <cell>0.37 <lb/>0.26</cell>
              <cell>0.22 <lb/>0.16</cell>
              <cell>0.12 <lb/>0.07</cell>
            </row>
            <row>
              <cell>Shawm ≥ 10 <lb/>Shawm ≥ 50</cell>
              <cell>0.13 <lb/>0.08</cell>
              <cell>0.04 <lb/>0.02</cell>
              <cell>0.01 <lb/>0.00</cell>
            </row>
            <row>
              <cell>End-blown trumpet ≥ 10 <lb/>End-blown trumpet ≥ 50</cell>
              <cell>0.28 <lb/>0.24</cell>
              <cell>0.04 <lb/>0.03</cell>
              <cell>0.01 <lb/>0.01</cell>
            </row>
            <row>
              <cell>Harp ≥ 10 <lb/>Harp ≥ 50</cell>
              <cell>0.62 <lb/>0.56</cell>
              <cell>0.56 <lb/>0.51</cell>
              <cell>0.46 <lb/>0.39</cell>
            </row>
            <row>
              <cell>Lute ≥ 10 <lb/>Lute ≥ 50</cell>
              <cell>0.55 <lb/>0.47</cell>
              <cell>0.42 <lb/>0.36</cell>
              <cell>0.33 <lb/>0.25</cell>
            </row>
            <row>
              <cell>Violin ≥ 10 <lb/>Violin ≥ 50</cell>
              <cell>0.26 <lb/>0.20</cell>
              <cell>0.19 <lb/>0.14</cell>
              <cell>0.06 <lb/>0.04</cell>
            </row>
            <row>
              <cell>Shawm ≥ 10 <lb/>Shawm ≥ 50</cell>
              <cell>0.17 <lb/>0.17</cell>
              <cell>0.03 <lb/>0.01</cell>
              <cell>0.00 <lb/>0.00</cell>
            </row>
            <row>
              <cell>End-blown trumpet ≥ 10 <lb/>End-blown trumpet ≥ 50</cell>
              <cell>0.67 <lb/>0.17</cell>
              <cell>0.02 <lb/>0.03</cell>
              <cell>0.01 <lb/>0.00</cell>
            </row>
            <row>
              <cell>Bagpipe ≥ 10 <lb/>Bagpipe ≥ 50</cell>
              <cell>0 <lb/>0</cell>
              <cell>0 <lb/>0</cell>
              <cell>0 <lb/>0</cell>
            </row>
            <row>
              <cell>Portative-Organ ≥ 10 <lb/>Portative-Organ ≥ 50</cell>
              <cell>0.24 <lb/>0.24</cell>
              <cell>0.13 <lb/>0.13</cell>
              <cell>0.06 <lb/>0.06</cell>
            </row>
            <row>
              <cell>Horn ≥ 10 <lb/>Horn ≥ 50</cell>
              <cell>0 <lb/>0</cell>
              <cell>0 <lb/>0</cell>
              <cell>0 <lb/>0</cell>
            </row>
            <row>
              <cell>Rebec ≥ 10 <lb/>Rebec ≥ 50</cell>
              <cell>- <lb/>-</cell>
              <cell>- <lb/>-</cell>
              <cell>- <lb/>-</cell>
            </row>
            <row>
              <cell>Lyre ≥ 10 <lb/>Lyre ≥ 50</cell>
              <cell>- <lb/>-</cell>
              <cell>- <lb/>-</cell>
              <cell>-- <lb/>-</cell>
            </row>
          </table>
          <figure xml:id="figure07">
            <head>A visual representation of how many instruments should be detected in the testing
              sets of the four MINERVA benchmarks that are introduced in this paper (first plot of
              each row). The second and third plots represent the true and false detections that we
              have obtained with a fully fine-tuned YOLO network. Results are computed with respect
              to an IoU ≥ 10 and an IoU ≥ 50.</head>
            <graphic url="resources/images/figure07.png"/>
          </figure>
          <figure xml:id="figure08">
            <head>Sample visualizations of the detections obtained on the MINERVA test set for a
              fully fine-tuned YOLO architecture. The first three rows report the detection of any
              kind of instrument within the images (single-instrument task), while the last three
              rows also report the correct classification of the detected bounding boxes.</head>
            <graphic url="resources/images/figure08.png"/>
          </figure>
        </div>
        <div>
          <head>Additional experiments</head>
          <p>As an additional stress-test, we have applied a trained object detector to two external
            data sets, in order to assess how valid and performant our approach is when applied "in
            the wild". We have considered two out-of-sample datasets:</p>
          <list type="unordered">
            <item><label>RMFAB/RMAH</label>: 428 out-of-sample images from the digital assets of
              both museum collections that are not included in the annotated material (and which are
              thus not included the train and validation material of the applied detector), because
              the available metadata did not explicitly specify that they contained depictions of
              musical instruments. (This collection cannot be shared due to copyright
              restrictions.)</item>
            <item><label>IconArt</label>: a generic collection of 6,528 artistic images, collected
              from the community-curated platform <emph>WikiArt: Visual Art Encyclopedia</emph>
                (<ref target="https://www.wikiart.org/">https://www.wikiart.org/</ref>). The IconArt
              subcollection was previously redistributed by <ptr target="#gonthier2018"/>: <ref
                target="https://wsoda.telecom-paristech.fr/downloads/dataset/"
                >https://wsoda.telecom-paristech.fr/downloads/dataset/</ref>.</item>
          </list>
          <p>Note that both external datasets differ in crucial aspects: <label>RMFAB/RMAH</label>
            can be considered "out-of-sample", but "in-collection", in the sense that these images
            derive from the same digital collections as many of the images represented in MINERVA.
            Additionally, we can expect extremely low detection rates for this dataset, because the
            presence of musical instruments will already have been flagged in a large majority of
            cases by the museum's staff. Thus, the application of <label>RMFAB/RMAH</label> should
            be viewed as a rather conservative stress test or sanity check, mainly checking for
            images that might have been missed by annotators in the past. The IconArt dataset is
            "out-of-sample" and "out-of-collection", in the sense that these images derive from a
            variety of other sources. It is therefore fully unrestricted, and this test can be
            considered a curiosity-driven validation of the method "in the wild". Importantly,
            IconArt was not collected with specific attention for musical instruments, so here too,
            we can anticipate a rather low detection rate (since many works of art simply do not
            feature any instruments). For all these reasons, we only evaluate the results on these
            external datasets in terms of precision (as recall is much less meaningful in this
            context).</p>
          <p>Following these differences, we have applied the single-instrument detector to the
              <label>RMFAB/RMAH</label> data and the hypernym detector to IconArt. Keeping an eye on
            the feasibility of the manual inspection, we have limited the number of instances
            returned by only allowing detections with a confidence score ≥ 0.20 (which is a rather
            generous threshold). Next, the results have then been evaluated in terms of precision,
            i.e. the number of returned image regions that actually represent musical instruments.
            The results are presented in <ref target="#table06">Table 6</ref>. <ref
              target="#figure10">Figure 10</ref> showcases a number of cherry-picked successful
            examples of detections from the out-of-collection IconArt images.</p>
          <table xml:id="table06">
            <head>Quantitative evaluation of the method on two out-of-sample datasets in terms of
              precision, restricted to detections with a confidence score ≥ 0.20.</head>
            <row>
              <cell><hi rend="bold">Collection</hi></cell>
              <cell><hi rend="bold">Total images</hi></cell>
              <cell><hi rend="bold">Detections</hi></cell>
              <cell><hi rend="bold">True positives</hi></cell>
            </row>
            <row>
              <cell><hi rend="bold">RMFAB/RMAH</hi></cell>
              <cell>428</cell>
              <cell>162</cell>
              <cell>6</cell>
            </row>
            <row>
              <cell><hi rend="bold">IconArt</hi></cell>
              <cell>6528</cell>
              <cell>118</cell>
              <cell>42</cell>
            </row>
          </table>
        </div>
      </div>
      <div>
        <head>Discussion</head>
        <div>
          <head>Skewed results</head>
          <p>First and foremost, we can observe that the scores obtained across all benchmarks are
            generally much lower than those reported for other datasets in computer vision (outside
            of the strict artistic domain). This drop in performance was to be expected and can be
            attributed to both the smaller size of the training data and the higher variance in the
            representation spectrum of musical instruments (across periods, materials, modes and,
            artists). Secondly, one can observe large fluctuations in the identifiability and
            detectability of individual instrument categories across both tasks. Not all of the
            fluctuations are easy to account for.</p>
          <p>We first consider the classification results. The confusion matrix reported in <ref
              target="#table04">Table 4</ref> clearly shows that the classes representing the top-4
            of instruments (harp, lute, violin, and portative organ) can be learned rather
            successfully, but that the performance rapidly breaks down for instrument categories at
            lower frequency ranks. Thus, while the accuracies for the top-5 experiments are
            relatively satisfying, especially in terms of accuracy (V3: <emph>acc=73.66;
              F1=70.29</emph>), the performance rapidly degrades for the more difficult setups. The
            results for the "all" classification experiment, where every instrument category is
            included no matter its frequency, are nothing less than dramatic (V3: <emph>acc=27.02;
              F1=6.67</emph>) and call for in-depth further research. The significant divergence
            between accuracy scores and F1 scores demonstrate that class imbalance is thus another
            aspect in which MINERVA presents a more challenging benchmark than its photorealistic
            counterparts.</p>
          <p>The skewness of the class distribution in MINERVA is representative of the long-tail
            distribution that we commonly encounter in cultural data. This imbalance is somewhat
            alleviated in the hypernym setup, where the labels are of course much better distributed
            over a much smaller number of classes (<emph>n=5</emph>). The general feasibility of
            this specific task is demonstrated by the encouraging scores that can be reported for
            the Inception-V3 architecture on this task (<emph>acc=75.80; F1=57.03</emph>). Note,
            additionally, that the "Electronic instruments" hypernym is included for completeness in
            this task, although the label is very infrequent and inevitably pulls down the
            (macro-averaged) F1-score in this respect. Overall, we notice than the Inception-V3
            architecture yields the highest performance on average for the classification task.</p>
          <p>Similar trends can be observed for the musical instrument detection task. First of all,
            we should emphasize the encouraging scores for the "single-instrument" detection task
            that simply aims to detect musical instruments (no matter their type). Here, a
            relatively high precision score is obtained (<emph>prec=0.63</emph> for <emph>IoU ≥
              10</emph>), which seems on par with comparable object categories for modern
            photo-realistic collections <ptr target="#ren2017"/>. Thus, this algorithm might not be
            fully apt at retrieving every single instrument from an unseen collection, but when it
            detects an instrument, we can be relatively sure that the detection deserves further
            inspection by a domain expert. Equally heartening scores are evident for most of the
            instrument hypernyms (with the notable exception of the under-represented "Electronic
            instruments" hypernym). While these detection tasks are of course relatively coarse,
            this observation nevertheless entails that this sort of detection technology can already
            find useful applications in the field (see below).</p>
          <p>When making our way down the frequency list in <ref target="#table05">Table 5</ref>, we
            again observe how the results break down dramatically for less common instrument
            categories. The fact that an over-represented category like harps can be reasonably well
            detected (<emph>AP(IoU ≥ 10)=0.55; AP(IoU ≥ 50)=0.46</emph>), should not lead the
            attention away from the fact that a state of the art object detector, such as YOLO,
            fails miserably at detecting a number of iconographically highly salient instruments,
            such as lyres and end-blown trumpets. At this stage, it is unclear whether this is
            caused by mere class imbalance or by the higher variance in the iconographic depiction
            of specific instruments. Bagpipes, for instance, occur frequently across images in
            MINERVA but might display much more depiction variance than, for instance, a harp.</p>
        </div>
        <div>
          <head>Saliency maps</head>
          <p>The results from the previous question call into question which visual properties the
            neural networks find useful to exploit in the identification of instruments.
            Importantly, the characteristic features exploited by a machine learning algorithm need
            not coincide with the properties that are judged most relevant by human experts and the
            comparison of both types of relevance judgements is worthwhile. In this section, we
            therefore perform model criticism or "network introspection" on the basis of the
            so-called "saliency maps" that can be extracted from a trained model <ptr
              target="#boyarski2017"/>. These saliency maps make visible to which regions in the
            original image the network paid most attention to, before arriving at its final
            classification decision. All examples discussed below come from the experiments on the
            hypernym dataset for the VGG19 network. <ref target="#figure09">Figure 9</ref> shows a
            series of manually selected, insightful examples, including the original image (as
            inputted into the network after preprocessing), as well as the saliency map obtained for
            it. We limit these examples to the representative hypernyms 'Stringed instruments' and
            'Wind instruments.'</p>
          <p>The maps in <ref target="#figure09">Figure 9</ref> vividly illustrate that the network
            focuses on two broad types of regions: properties of the instruments itself (which was
            expected) but also the immediate context of the instruments, and more specifically the
            way they are operated, handled or presented by people, c.q. musicians. The
            characteristics of the salient regions in the examples in <ref target="#figure09">Figure
              9</ref> could be described as:</p>
          <p><label>Stringed instruments:</label></p>
          <list type="simple">
            <item>(a) Focus on the neck of the stringed instrument, as well as the characteristic
              presence of tuning pins at the end of the neck;</item>
            <item>(b) Sensitive to the presence of stretched fingers in an unnatural
              position;</item>
            <item>(c) Typical conic shape of a lyre, with outward pointing ends connected by a
              bridge;</item>
          </list>
          <p><label>Wind instruments:</label></p>
          <list type="simple">
            <item>(d) Symmetric presence of tone holes in the areophone;</item>
            <item>(e) Elongated, cylindric shape of the main body of the areophone with wider
              end;</item>
            <item>(f) Mirrored placement of fingers and hands (close to one another).</item>
          </list>
          <p>These characteristics strongly suggest that the way an instrument is handled (i.e. its
            immediate iconographic neighborhood) is potentially of equal importance as the shape of
            the actual instrument, an insight that we will further expand on below.</p>
          <figure xml:id="figure09">
            <head>Saliency maps for several stringed (subfigures (a) to (c)) and wind (subfigures
              (d) to (f)) instruments.</head>
            <graphic url="resources/images/figure09.png"/>
          </figure>
        </div>
        <div>
          <head>Error analysis: false positives</head>
          <p>In this section, we offer a qualitative discussion of the false positives from the
            out-of-sample tests reported in the previous section, i.e. instances where the detectors
            erroneously thought to have detected an instrument. This eagerness is a known problem of
            object detectors: a system that is trained to recognize "sheep" will be inclined to see
            "sheep" everywhere. Anecdotally, people have noted how misleading contextual cues can
            indeed be a confounding factor in image analysis. One blog post for instances noted how
            a major image labeling service tagged photographs of green fields with the "sheep"
            label, although no sheep whatsoever were present in the images.<note><ref
                target="https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep"
                >https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep</ref></note>
            Eagerness-to-detect or over-association is therefore a clear first shortcoming of this
            method when applied in the wild, mainly because it was only trained in images that
            actually contain musical instruments. Interestingly, the false positives come in
            clusters that shed an interesting light on this issue. Below we list a representative
            number of error clusters:</p>
          <figure xml:id="figure10">
            <head>Examples of successful detections in IconArt for "stringed instruments”.</head>
            <graphic url="resources/images/figure10.png"/>
          </figure>
          <figure xml:id="figure11">
            <head>Anecdotal examples of false positive detection, divided in 7 interpretive clusters
              (numbered a-g).</head>
            <graphic url="resources/images/figure11.png"/>
          </figure>
          <p>The above categorization illustrates that the false positives are rather insightful,
            mainly because the absence of an instrument highlights the contextual clues that are at
            work. Of particular relevance is the observation that the iconography surrounding
            children closely resembles that of instruments. This seems related to the intimate and
            caring body language of both the caretakers and musicians in such compositions. The
            immediate iconographic neighborhood of children clearly reminds the detector of the
            delicacy and reverence with which instruments are portrayed and presented in historical
            artworks. This delicacy and intimacy in body language can be specifically related to the
            foregrounding of fingers, the prominent portrayal of which invariably triggers the
            detector, also in the absence of children. Some of these phenomena invite closer
            inspection by domain experts in music iconography and suggest that serial or panoramic
            analyses are a worthwhile endeavour in this field, also from the point of view of more
            hermeneutically oriented scholars.</p>
        </div>
      </div>
      <div>
        <head>Conclusions and future research</head>
        <p>In this paper, we have introduced MINERVA, to our knowledge the first sizable benchmark
          dataset for the identification and detection of individual musical instruments in
          unrestricted, digitized images from the realm of the visual arts. Our benchmark
          experiments have highlighted the feasibility of a number of tasks but also, and perhaps
          primarily, the significant challenges that state-of-the-art machine learning systems are
          still confronted with on this data, such as the "long-tail" of the instruments'
          distribution and the staggering variance in depiction across the images in the dataset. We
          therefore hope that this work will inspire new (and much-needed) research in this area. At
          the end of this paper, we wish to formulate some advice and concerns in this respect.</p>
        <p>One evident direction from future research is more advanced transfer learning, where
          algorithms make more efficient use of the wealth of photorealistic data that is provided,
          for instance, by MIMO <ptr target="#dolan2017"/>. The main issue with the MIMO data in
          this respect is that the bulk of these photographs are context-free (i.e. the instruments
          are photographed in isolation, against a white or neutral background), which is almost
          never the case in the artistic domain. Preliminary research demonstrated that this a major
          hurdle to established pretraining scenarios. Cascaded approaches, where instruments are
          detected first and only classified in a second stage might be a promising avenue here.</p>
        <p>One crucial final remark is that AI has an amply attested tendency not only to be
          sensitive to biases in the input data but also to amplify them <ptr target="#zou2018"/>.
          Whereas the computational methods presented here have the potential to scale up
          dramatically the scope of current research in music iconography, it also comes with
          ideological dangers. The technology could further strengthen the bias on specific
          canonical regions and periods in art history and lead the attention even further away from
          artistic and iconographic cultures that are already in specific need of reappraisal. The
          community will therefore have to think carefully about bias correction and mitigation.
          Collecting training data in a diverse and inclusive manner, with ample attention for
          resource-lower cultures should be a key strategy in future data collection campaigns.</p>
      </div>
      <div>
        <head>Acknowledgements</head>
        <p>We wish to thank Remy Vandaele for the help with Cytomine and for the fruitful
          discussions related to computer vision and object detection. Special thanks go out to our
          annotators and other (former) colleagues in the museums involved: Cedric Feys, Odile
          Keromnes, Lies Van De Cappelle and Els Angenon. Our gratitude also goes out to Rodolphe
          Bailly for his support and advice regarding MIMO. Finally, we wish to credit our former
          project member dr. Ellen van Keer with the original idea of applying object detection to
          musical instruments. This project is generously funded by the Belgian Federal Research
          Agency BELSPO under the BRAIN-be program.</p>
      </div>
    </body>

    <!-- BACK TEXT -->
    <back>
      <listBibl>
        <bibl xml:id="arnold2019" label="Arnold and Tilton 2019">Arnold, T., and Tilton, L., <title
            rend="quotes">Distant viewing: analyzing large visual corpora.</title><title
            rend="italic">Digital Scholarship in the Humanities</title>, 34 (2019), i3-i16.</bibl>
        <bibl xml:id="baldassarre2007" label="Baldassarre 2007">Baldassarre, A. <title rend="quotes"
            >Quo vadis music iconography? The Repertoire International d'Iconographie Musicale as a
            case study</title>
          <title rend="italic">Fontes Artis Musicae</title>, 54 (2007), 440-452.</bibl>
        <bibl xml:id="baldassarre2008" label="Baldassarre 2008">Baldassarre, A. <title rend="quotes"
            >Music Iconography: What is it all about? Some remarks and considerations with a
            selected bibliography</title>
          <title rend="italic">Ictus: Periódico do Programa de Pós-Graduação em Música da
            UFBA</title>, 9 (2008), 55-95.</bibl>
        <bibl xml:id="ballard1982" label="Ballard and Brown 1982">Ballard, D. H., and Christopher M.
          Brown, C. M. <title rend="italic">Computer Vision</title>, Upper Saddle River
          (1982).</bibl>
        <bibl xml:id="bell2019" label="Bell and Impett 2019">Bell, P., and Impett, L. <title
            rend="quotes">Ikonographie und Interaktion. Computergestützte Analyse von Posen in
            Bildern der Heilsgeschichte</title>
          <title rend="italic">Das Mittelalter</title>, 24 (2019): 31–53.</bibl>
        <bibl xml:id="boyarski2017" label="Boyarski et al. 2017">Mariusz Bojarski, Anna Choromanska,
          Krzysztof Choromanski, Bernhard Firner, Larry J. Ackel, Urs Muller, Philip Yeres, Karol
          Zieba, <title rend="quotes">VisualBackProp: Efficient Visualization of CNNs for Autonomous
            Driving</title>
          <title rend="italic">Proceedings of the 2018 IEEE International Conference on Robotics and
            Automation (ICRA)</title>, 2018, 4701-4708. DOI: 10.1109/ICRA.2018.8461053.</bibl>
        <bibl xml:id="buckley1998" label="Buckley 1998">Buckley, A. <title rend="quotes">Music
            Iconography and the Semiotics of Visual Representation</title>
          <title rend="italic">Music in Art</title>, 23 (1998), 5-10.</bibl>
        <bibl xml:id="crowley2014" label="Crowley and Zisserman 2014">Crowley, E., and Zisserman, A.
            <title rend="quotes">The State of the Art: Object Retrieval in Paintings using
            Discriminative Regions</title> In Valstar, M., French, A., and Pridmore, T. (eds),
            <title rend="italic">Proceedings of the British Machine Vision Conference</title>,
          Nottingham (2014), s.p.</bibl>
        <bibl xml:id="dolan2017" label="Dolan 2017">Dolan, E. I. <title rend="quotes">Review: MIMO:
            Musical Instrument Museums Online</title>
          <title rend="italic">Journal of the American Musicological Society</title>, 70 (2017):
          555-565.</bibl>
        <bibl xml:id="everingham2010" label="Everingham et al. 2010">Everingham, M., Van Gool, L.,
          Williams, C. K., Winn, J., and Zisserman, A. <title rend="quotes">The Pascal visual object
            classes (VOC) challenge</title> In <title rend="italic">International journal of
            computer vision</title>, 88(2) (2010): 303–338.</bibl>
        <bibl xml:id="gonthier2018" label="Gonthier et al. 2018">Gonthier, N., Gousseau, Y., Ladjal,
          S. and Bonfait, O. <title rend="quotes">Weakly supervised object detection in
            artworks</title> In <title rend="italic">Proceedings of the European Conference on
            Computer Vision (ECCV)</title> (2018): 692–709.</bibl>
        <bibl xml:id="green2013" label="Green and Ferguson 2013">Green, A., and Ferguson, S. <title
            rend="quotes">RIDIM: Cataloguing music iconography since 1971</title>
          <title rend="italic">Fontes Artis Musicae</title>, 60 (2013), 1-8.</bibl>
        <bibl xml:id="he2016" label="He et al. 2016">He, K., Zhang, X., Ren, S., and Sun, J. <title
            rend="quotes">Deep residual learning for image recognition.</title>In <title
            rend="italic">Proceedings of the IEEE conference on computer vision and pattern
            recognition</title>, pages 770–778, 2010.</bibl>
        <bibl xml:id="hertzmann2018" label="Hertzmann 2018">Hertzmann, A. <title rend="quotes">Can
            Computers Create Art?</title> Arts, 7 (2018) doi:10.3390/arts7020018.</bibl>
        <bibl xml:id="hockey2004" label="Hockey 2004">Hockey, S. <title rend="quotes">A History of
            Humanities Computing.</title>In S. Schreibman, R. Siemens, and J. Unsworth (eds.),
            <title rend="italic">A Companion to Digital Humanities</title>, Oxford (2004), pp.
          3–19.</bibl>
        <bibl xml:id="huang2017" label="Huang et al. 2017">Huang, G., Zhuang, L., Van Der Maaten,
          L., and Weinberger, K. <title rend="quotes">Densely connected convolutional
            networks</title>In <title rend="italic">Proceedings of the IEEE conference on computer
            vision and pattern recognition</title> (2017), pp. 4700– 4708.</bibl>
        <bibl xml:id="kingma2014" label="Kingma and Ba 2014">Kingma, D. P., and Ba, J. <title
            rend="quotes">A method for stochastic optimization</title>
          <title rend="italic">arXiv preprint arXiv:1412.6980</title>, 2014.</bibl>
        <bibl xml:id="lecun2015" label="LeCun et al. 2015">LeCun, J., Bengio, Y., and Hinton, G.,
            <title rend="quotes">Deep Learning</title>
          <title rend="italic">Nature</title>, 521 (2015): 436–444.</bibl>
        <bibl xml:id="lin2014" label="Lin et al. 2014">Lin T.-Y., Maire, M., Belongie, S., Hays, J.,
          Perona, P., Ramanan, D., Dollar, P and Zitnick, C. L. <title rend="quotes">Microsoft COCO:
            Common objects in context</title> In <title rend="italic">European conference on
            computer vision</title>, pages 740–755. Springer, 2014.</bibl>
        <bibl xml:id="marée2016" label="Marée et al. 2016">Marée, R., Rollus, L. Stévens, B.,
          Hoyoux, R., Louppe, G., Vandaele, R., Begon, J., Kainz, P., Geurts, P., and Wehenkel
            <title rend="quotes">Collaborative analysis of multi-gigapixel imaging data using
            Cytomine</title>
          <title rend="italic">Bioinformatics</title>, 32 (2016): 1395–1401.</bibl>
        <bibl xml:id="mensink2014" label="Mensink and Van Gemert 2014">Mensink, T. and Van Gemert,
          J. <title rend="quotes">The Rijksmuseum challenge: Museum-centered visual
            recognition</title> In <title rend="italic">Proceedings of International Conference on
            Multimedia Retrieval</title>, page 451. ACM, 2014.</bibl>
        <bibl xml:id="pedregosa2011" label="Pedregosa et al. 2011">Pedregosa, F., Varoquaux, G.,
          Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss,
          R. and Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M.,
          and Duchesnay, E. <title rend="quotes">Scikit-learn: Machine Learning in Python</title>
          <title rend="italic">Journal of Machine Learning Research</title>, 12 (2011):
          2825-2830.</bibl>
        <bibl xml:id="redmon2018" label="Redmon and Farhadi 2018">Redmon, J. and Farhadi, A. <title
            rend="quotes">Yolov3: An incremental improvement</title>
          <title rend="italic">arXiv preprint arXiv:1804.02767</title>, 2018.</bibl>
        <bibl xml:id="ren2017" label="Ren et al. 2017">S. Ren, K. He, R. Girshick, and J. Sun,
            <title rend="quotes">Faster R-CNN: Towards Real-Time Object Detection with Region
            Proposal Networks</title>
          <title rend="italic">IEEE Transactions on Pattern Analysis and Machine
            Intelligence</title>, 39 (2017), 1137-1149.</bibl>
        <bibl xml:id="russakovsky2015" label="Russakovsky et al. 2015">Russakovsky, O., Deng, J.,
          Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, K., Khosla, A., Bernstein,
          M. et al. <title rend="quotes">Imagenet large scale visual recognition challenge</title>
          <title rend="italic">International journal of computer vision</title>, 115(3) (2015),
          211–252.</bibl>
        <bibl xml:id="sabatelli2018" label="Sabatelli et al. 2018">Sabatelli, M., Kestemont, M.,
          Daelemans, W. and Geurts, P. <title rend="quotes">Deep transfer learning for art
            classification problems</title> In <title rend="italic">Proceedings of the European
            Conference on Computer Vision (ECCV)</title>, pages 631–646, 2018.</bibl>
        <bibl xml:id="schmidhuber2015" label="Schmidhuber 2015">, Schmidhuber, J. <title
            rend="quotes">Deep Learning in Neural Networks: An Overview</title>
          <title rend="italic">Neural Networks</title>, 61 (2015), 85-117.</bibl>
        <bibl xml:id="seguin2018" label="Seguin 2018">Seguin, B. <title rend="quotes">The Replica
            Project: Building a visual search engine for art historians</title> XRDS: Crossroads,
            <title rend="italic">The ACM Magazine for Students - Computers and Art</title>, 24
          (2018), 24-29.</bibl>
        <bibl xml:id="simonyan2014" label="Simonyan and Zisserman 2014">Simonyan, K. and Zisserman,
          A. Very deep convolutional networks for large-scale image recognition. <title
            rend="italic">arXiv preprint arXiv:1409.1556</title>, 2014.</bibl>
        <bibl xml:id="strezoski2017" label="Strezoski and Worring 2017">Strezoski, G. and Worring,
          M. <title rend="quotes">Omniart: multi-task deep learning for artistic data
            analysis</title>
          <title rend="italic">arXiv preprint arXiv:1708.00684</title>, 2017.</bibl>
        <bibl xml:id="szegedy2015" label="Szegedy et al. 2015">Szegedy, C., Liu, W., Jia, Y.,
          Sermanet, P., Reed, S., Anguelov, S., Erhan, D., Vanhoucke, V., and Rabinovich, A. <title
            rend="quotes">Going deeper with convolutions</title> In <title rend="italic">Proceedings
            of the IEEE conference on computer vision and pattern recognition</title> (2015), pp.
          1–9.</bibl>
        <bibl xml:id="van2015" label="Van et al. 2015">Van Noord, N., Hendriks, E., and Postma, E.,
            <title rend="quotes">Toward Discovery of the Artist's Style: Learning to recognize
            artists by their artworks</title>
          <title rend="italic">IEEE Signal Processing Magazine</title>, 32 (2015), 46-54.</bibl>
        <bibl xml:id="wevers2020" label="Wevers and Smits 2020">Wevers M., and Smits, T. <title
            rend="quotes">The visual digital turn: Using neural networks to study historical
            images</title>
          <title rend="italic">Digital Scholarship in the Humanities</title>, 35 (2020),
          194–207.</bibl>
        <bibl xml:id="xiang2014" label="Xiang et al. 2014">Xiang, Y., Mottaghi, R., and Savarese, S.
            <title rend="quotes">Beyond pascal: A benchmark for 3d object detection in the
            wild</title> In <title rend="italic">IEEE Winter Conference on Applications of Computer
            Vision</title>, pages 75–82. IEEE, 2014.</bibl>
        <bibl xml:id="zou2018" label="Zou and Schiebinger 2018">Zou, J., and Schiebinger, L. <title
            rend="quotes">AI can be sexist and racist — it's time to make it fair</title>
          <title rend="italic">Nature</title>, 559 (2018): 324-326.</bibl>
      </listBibl>
    </back>
  </text>
  <!-- END TEXT -->

</TEI>
