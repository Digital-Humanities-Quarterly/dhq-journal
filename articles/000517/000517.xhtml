<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 015 Number 1
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol/15/1/000517.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol/15/1/000517/000517.html#disqus_thread" data-disqus-identifier="000517">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Advances in Digital Music Iconography: Benchmarking the
                  detection of musical instruments in unrestricted, non-photorealistic images from the
                  artistic domain
               </h1>
               
               <div class="author"><span style="color: grey">Matthia Sabatelli</span> &lt;<a href="mailto:m_dot_sabatelli_at_uliege_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('m_dot_sabatelli_at_uliege_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('m_dot_sabatelli_at_uliege_dot_be'); return false;">m_dot_sabatelli_at_uliege_dot_be</a>&gt;, Montefiore Institute
               </div>
               
               <div class="author"><span style="color: grey">Nikolay Banar</span> &lt;<a href="mailto:nicolae_dot_banari_at_uantwerpen_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('nicolae_dot_banari_at_uantwerpen_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('nicolae_dot_banari_at_uantwerpen_dot_be'); return false;">nicolae_dot_banari_at_uantwerpen_dot_be</a>&gt;, University of Antwerp
               </div>
               
               <div class="author"><span style="color: grey">Marie Cocriamont</span> &lt;<a href="mailto:marie_cocriamont_at_hotmail_dot_com" onclick="javascript:window.location.href='mailto:'+deobfuscate('marie_cocriamont_at_hotmail_dot_com'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('marie_cocriamont_at_hotmail_dot_com'); return false;">marie_cocriamont_at_hotmail_dot_com</a>&gt;, Royal Museums of Art and History, Brussels
               </div>
               
               <div class="author"><span style="color: grey">Eva Coudyzer</span> &lt;<a href="mailto:eva_dot_coudyzer_at_kikirpa_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('eva_dot_coudyzer_at_kikirpa_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('eva_dot_coudyzer_at_kikirpa_dot_be'); return false;">eva_dot_coudyzer_at_kikirpa_dot_be</a>&gt;, Royal Museums of Art and History, Brussels
               </div>
               
               <div class="author"><span style="color: grey">Karine Lasaracina</span> &lt;<a href="mailto:karine_dot_lasaracina_at_fine-arts-museum_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('karine_dot_lasaracina_at_fine-arts-museum_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('karine_dot_lasaracina_at_fine-arts-museum_dot_be'); return false;">karine_dot_lasaracina_at_fine-arts-museum_dot_be</a>&gt;, Royal Museums of Fine Arts of Belgium, Brussels
               </div>
               
               <div class="author"><span style="color: grey">Walter Daelemans</span> &lt;<a href="mailto:walter_dot_daelemans_at_uantwerpen_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('walter_dot_daelemans_at_uantwerpen_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('walter_dot_daelemans_at_uantwerpen_dot_be'); return false;">walter_dot_daelemans_at_uantwerpen_dot_be</a>&gt;, University of Antwerp
               </div>
               
               <div class="author"><span style="color: grey">Pierre Geurts</span> &lt;<a href="mailto:p_dot_geurts_at_uliege_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('p_dot_geurts_at_uliege_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('p_dot_geurts_at_uliege_dot_be'); return false;">p_dot_geurts_at_uliege_dot_be</a>&gt;, University of Liège
               </div>
               
               <div class="author"><span style="color: grey">Mike Kestemont</span> &lt;<a href="mailto:mike_dot_kestemont_at_uantwerpen_dot_be" onclick="javascript:window.location.href='mailto:'+deobfuscate('mike_dot_kestemont_at_uantwerpen_dot_be'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('mike_dot_kestemont_at_uantwerpen_dot_be'); return false;">mike_dot_kestemont_at_uantwerpen_dot_be</a>&gt;, University of Antwerp
               </div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Advances%20in%20Digital%20Music%20Iconography%3A%20Benchmarking%20the%20detection%20of%20musical%20instruments%20in%20unrestricted,%20non-photorealistic%20images%20from%20the%20artistic%20domain&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=015&amp;rft.issue=1&amp;rft.aulast=Sabatelli&amp;rft.aufirst=Matthia&amp;rft.au=Matthia%20Sabatelli&amp;rft.au=Nikolay%20Banar&amp;rft.au=Marie%20Cocriamont&amp;rft.au=Eva%20Coudyzer&amp;rft.au=Karine%20Lasaracina&amp;rft.au=Walter%20Daelemans&amp;rft.au=Pierre%20Geurts&amp;rft.au=Mike%20Kestemont"> </span></div>
            
            
            
            
            <div id="DHQtext">
               
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>In this paper, we present MINERVA, the first benchmark dataset for the detection of
                     musical instruments in non-photorealistic, unrestricted image collections from the
                     realm
                     of the visual arts. This effort is situated against the scholarly background of music
                     iconography, an interdisciplinary field at the intersection of musicology and art
                     history.
                     We benchmark a number of state-of-the-art systems for image classification and object
                     detection. Our results demonstrate the feasibility of the task but also highlight
                     the
                     significant challenges which this artistic material poses to computer vision. We evaluate
                     the system to an out-of-sample collection and offer an interpretive discussion of
                     the
                     false positives detected. The error analysis yields a number of unexpected insights
                     into
                     the contextual cues that trigger the detector. The iconography surrounding children
                     and
                     musical instruments, for instance, shares some core properties, such as an intimacy
                     in
                     body language.
                  </p>
                  
               </div>
               
               
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Introduction: the era of the pixel</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">The Digital Humanities constitute an intersectional community of praxis, in which
                     the
                     application of computing technologies in various subdisciplines in the
                     <em class="emph">Geisteswissenschaften</em> plays a significant role. Surveys of the history of
                     the field [<a class="ref" href="#hockey2004">Hockey 2004</a>] have stressed that most of the seminal applications
                     of computing technology were heavily, if not exclusively, text-oriented: due to the
                     hardware and software limitations of the time, analyses of image data (but also audio
                     or
                     video data) remained elusive and out of practical reach until relatively late, certainly
                     at a larger scale. In the past decade, the application of deep neural networks has
                     significantly pushed the state of the art in computer vision, leading to impressive
                     advances in tasks such as image classification or object detection [<a class="ref" href="#lecun2015">LeCun et al. 2015</a>]
                     [<a class="ref" href="#schmidhuber2015">Schmidhuber 2015</a>]. Even more recently, improvements in the field of
                     computer vision have started to find practical applications in study domains outside
                     of
                     strict machine learning, such as physics, medicine or even astrology. Supported by
                     this
                     technology's (at times rather naive) coverage in the popular media, the communis opinio
                     has been eager to herald the advent of the "Era of the Pixel".
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">In the Digital Humanities too, the potential of computer vision is nowadays increasingly
                     recognized. A programmatic duet of two recent articles on "distant viewing" in the
                     field's
                     flagship journal [<a class="ref" href="#wevers2020">Wevers and Smits 2020</a>]
                     [<a class="ref" href="#arnold2019">Arnold and Tilton 2019</a>] leads the way in this respect, emphasizing the privileged role
                     these new methodologies can play in the exploration of large data collections in the
                     Humanities. The present paper too is situated in a multidisciplinary project in which
                     we
                     investigate how modern artificial intelligence can support GLAM institutions (galleries,
                     libraries, archives, and museums) in cataloguing and curating their rapidly expanding
                     digital assets. As a case study, we shall work with non-photorealistic depictions
                     of
                     musical instruments in the artistic domain.
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The structure of this paper is as follows. First, we motivate and contextualize our
                     case
                     study of musical instruments from within the scholarly framework of music iconography
                     and
                     computer vision, but also from the more pragmatic context of the research project
                     from
                     which this focus has emerged. We go on to describe the construction and characteristics
                     of
                     an annotated benchmark dataset, the MINERVA dataset, that will be released together
                     with
                     this paper, through which we hope to stimulate further research in this area. Using
                     this
                     benchmark data, we stress-test the available technology for the identification and
                     detection of objects in images and discuss the current limitations of systems. To
                     illustrate the broader relevance of our approach, we apply the trained benchmark system
                     'in the wild', on unseen and out-of-sample heritage data, followed by a quantitative
                     and
                     qualitative evaluation of the results. Finally, we identify what seem to be the most
                     relevant directions for future research.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Motivation</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Music iconography</h2>
                     
                     <div class="counter"><a href="#p4">4</a></div>
                     <div class="ptext" id="p4">The present paper must be understood against the wider scholarly background of music
                        iconography, a Humanities field of inquiry with a rich, interdisciplinary history
                        in its
                        own right. [<a class="ref" href="#buckley1998">Buckley 1998</a>] concisely defined music iconography as a field
                        being "concerned with the study of the visual representation of musical topics. Its
                        primary materials include portraits of performers and composers, illustrations of
                        instruments, occasions of music-making, and the use of musical imagery for purposes
                        of
                        metaphorical or allegorical allusion". Because of this wide range of topics, at the
                        intersection of art history and musicology [<a class="ref" href="#baldassarre2007">Baldassarre 2007</a>]
                        [<a class="ref" href="#baldassarre2008">Baldassarre 2008</a>], the field takes pride of its interdisciplinarity.
                     </div>
                     
                     <div class="counter"><a href="#p5">5</a></div>
                     <div class="ptext" id="p5">Music iconography deliberately adopts a "methodological plurality" [<a class="ref" href="#baldassarre2007">Baldassarre 2007</a>] which is increasingly complemented with digital
                        approaches. A major achievement in this respect has been the establishment (in 1971)
                        and
                        continued expansion and curation of an international digital inventory for musical
                        iconography, the <em class="emph">Répertoire International d'Iconographie Musicale</em> (RIDIM).
                        Now publicly available as an online web resource (<a href="https://ridim.org/" onclick="window.open('https://ridim.org/'); return false" class="ref">https://ridim.org/</a>), RIDIM functions as a reference image database, designed to
                        facilitate the efficient yet powerful description and discovery of music-related art
                        works [<a class="ref" href="#green2013">Green and Ferguson 2013</a>]. The need for such an international inventory has been
                        acknowledged as early as 1929 and its significant scope facilitates the international
                        study of music-related phenomena and their depiction across the visual arts.
                     </div>
                     
                     <div class="counter"><a href="#p6">6</a></div>
                     <div class="ptext" id="p6">Music iconography has an important tradition of focused studies targeting the deep,
                        interpretive analysis of individual artworks or small collections of them. Such
                        hermeneutic case studies have the advantage of depth, but understandably lack a more
                        panoramic perspective on the phenomena of interest and, for instance, diachronic or
                        synchronic trends and shifts therein. The large-scale, "serial" study of musical
                        instruments as depicted across the visual arts remains a desideratum in the field
                        and
                        has the potential of bringing a macroscopic perspective to historical developments.
                        In
                        the present paper, we explore the feasibility of applying methods from present-day
                        computer vision, in an attempt to scale up current approaches. The primary motivation
                        of
                        this endeavour is that digital music iconography – or "Distant" music iconography,
                        in an
                        analogy to similar developments in literary studies [<a class="ref" href="#wevers2020">Wevers and Smits 2020</a>]
                        [<a class="ref" href="#arnold2019">Arnold and Tilton 2019</a>] – in principle has much to gain from such methods, at least
                        if they are carefully applied and in continuous interaction with experts in the domain.
                        Our focal point is the automated identification and detection of individual musical
                        instruments in unrestricted, digitized materials from the realm of the visual arts.
                     </div>
                     
                     <div class="counter"><a href="#p7">7</a></div>
                     <div class="ptext" id="p7">This scholarly initiative is embedded in the collaborative research project INSIGHT
                        (Intelligent Neural Systems as InteGrated Heritage Tools), which aims to stimulate
                        the
                        application of Artificial Intelligence to the rapidly expanding digital collections
                        of a
                        selection of federal museum clusters in Belgium.<a class="noteRef" href="#d4e370">[1]</a> One
                        important, transcommunal aspect to Belgium's cultural history relates to music and
                        musical history, with the invention of the saxophone by Adolphe Sax as an iconic
                        example. An additional factor is the presence of the Musical Instruments Museum in
                        the
                        capital (Brussels) that contributed significantly to international research projects
                        in
                        this area (and which is a partner in the INSIGHT project). This contextualization,
                        finally, is also important to understand our specific choice for the topic of musical
                        instruments, as a representative and worthwhile case study on the application of modern
                        machine learning technology in digital heritage studies.
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Computer vision</h1>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">The methodology for the present paper largely derives from machine learning and more
                     specifically computer vision, a field concerned with computational algorithms that
                     can
                     mimic the perceptual abilities of humans and their capacity to construct high-level
                     interpretations from raw visual stimuli [<a class="ref" href="#ballard1982">Ballard and Brown 1982</a>]. In the past decade,
                     this field has gone through a remarkable renaissance, following the emergence of powerful
                     learning techniques based on so-called neural networks. In particular the advent of
                     "convolutional" networks [<a class="ref" href="#lecun2015">LeCun et al. 2015</a>] has led to dramatic advances in the
                     state of the art for a number of standard applications, including image classification
                     ("Is this an image of a cat or a dog?") and object detection ("Draw a bounding box
                     around
                     any cats in this image"). For some of these tasks, modern computer systems have even
                     been
                     shown to rival the performance of humans [<a class="ref" href="#russakovsky2015">Russakovsky et al. 2015</a>]. In spite of the
                     impressive advances in recent computer vision research, it is generally acknowledged
                     that
                     the state of the art is still confronted with a number of major, as yet unsolved,
                     challenges. In this section we highlight four concrete issues that are especially
                     pressing, given the focus of this paper on image collections in the artistic domain.
                     These
                     challenges motivate our work from the point of view of computer vision, rather than
                     art
                     history.
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Photo-realism</h2>
                     
                     <div class="counter"><a href="#p9">9</a></div>
                     <div class="ptext" id="p9">One major hurdle is that computer vision nowadays strongly gravitates towards so-called
                        photo-realistic material, i.e. digitized or born-digital versions of photographs that
                        do
                        not actively attempt to distort the reality they depict. The best example in this
                        respect is the influential ImageNet dataset [<a class="ref" href="#russakovsky2015">Russakovsky et al. 2015</a>], that
                        offers highly realistic photographic renderings of everyday concepts drawn from
                        WordNet's lexical database. While some more recent heritage collections of course
                        abound
                        in such photo-realistic material (e.g. advertisements in historic newspapers),
                        traditional photography does not take us further back in time than the nineteenth
                        century [<a class="ref" href="#hertzmann2018">Hertzmann 2018</a>]. Additionally, the Humanities study many other
                        visual arts that prioritize much less photorealistic representation and focus even
                        on
                        completely 'fictional' renderings of (potentially imagined or historical) realities.
                        While there has been some encouraging and worthwhile prior work into the application
                        of
                        computer vision to non-photorealistic depictions, this work is generally more scattered
                        and the results (understandably) less advanced than those reported for the
                        photorealistic domain. Inspiring recent studies in this area include [<a class="ref" href="#crowley2014">Crowley and Zisserman 2014</a>]
                        [<a class="ref" href="#van2015">Van et al. 2015</a>]
                        [<a class="ref" href="#seguin2018">Seguin 2018</a>]
                        [<a class="ref" href="#bell2019">Bell and Impett 2019</a>].
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Data scarcity</h2>
                     
                     <div class="counter"><a href="#p10">10</a></div>
                     <div class="ptext" id="p10">It is a well-known limitation that convolutional neural networks require large amounts
                        of manually annotated example data (or training data) in order to perform well. To
                        address this issue, the community has released several public datasets over the years
                        [<a class="ref" href="#xiang2014">Xiang et al. 2014</a>]
                        [<a class="ref" href="#russakovsky2015">Russakovsky et al. 2015</a>]
                        [<a class="ref" href="#mensink2014">Mensink and Van Gemert 2014</a>]
                        [<a class="ref" href="#strezoski2017">Strezoski and Worring 2017</a>]
                        [<a class="ref" href="#lin2014">Lin et al. 2014</a>] which has allowed the successful training of a large set of
                        neural architectures [<a class="ref" href="#he2016">He et al. 2016</a>]
                        [<a class="ref" href="#szegedy2015">Szegedy et al. 2015</a>]
                        [<a class="ref" href="#simonyan2014">Simonyan and Zisserman 2014</a>]. However, the nature of the images included in these
                        datasets is mostly photo-realistic, also because such images are relatively
                        straightforward to obtain and annotate. These image collections are very different
                        in
                        terms of texture, content and availability from the sort of data that can nowadays
                        be
                        found in the digital heritage domain.
                     </div>
                     
                     <div class="counter"><a href="#p11">11</a></div>
                     <div class="ptext" id="p11">Computer vision researchers interested in the artistic domain have attempted to
                        alleviate the relative dearth of training data by either releasing domain-specific
                        datasets [<a class="ref" href="#mensink2014">Mensink and Van Gemert 2014</a>]
                        [<a class="ref" href="#strezoski2017">Strezoski and Worring 2017</a>] or through the application of transfer learning [<a class="ref" href="#sabatelli2018">Sabatelli et al. 2018</a>], a machine learning paradigm which allows the application of
                        neural networks to domains where training data is scarce. For image classification,
                        for
                        instance, these efforts have indeed greatly contributed to overall feasibility of
                        applying computer vision outside the photo-realistic domain [<a class="ref" href="#sabatelli2018">Sabatelli et al. 2018</a>]. Both approaches, however, have limitations when it comes to the complementary task
                        of object detection. Popular datasets such as the Rijksmuseum collection [<a class="ref" href="#mensink2014">Mensink and Van Gemert 2014</a>] or the more recent OmniArt dataset [<a class="ref" href="#strezoski2017">Strezoski and Worring 2017</a>] do not come with the metadata required for object-detection
                        problems.
                     </div>
                     
                     <div class="counter"><a href="#p12">12</a></div>
                     <div class="ptext" id="p12">With this work, we take one step forward in addressing these limitations. Firstly,
                        the
                        MINERVA dataset that we present below, specifically tackles the problem of object
                        detection within the broader heritage domain of the visual arts, introducing a novel
                        benchmark for researchers working at the intersection of computer vision and art
                        history. Secondly, we present a number of baseline results on the newly introduced
                        dataset. The results are reported for a representative set of common architectures,
                        which were pre-trained on photo-realistic images. This allows us to investigate to
                        what
                        extent these methods can be reused when tested on artistic images.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Irrelevant training categories</h2>
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">Previous studies have demonstrated the feasibility of "pretraining": with this
                        approach, networks are first trained on (large) photorealistic collections (i.e. the
                        source domain) and then applied downstream (or further fine-tuned) on an out-of-sample
                        target domain, that has much less annotated data available. While generally useful,
                        this
                        approach is still confronted with the problem that the annotation labels or categories
                        attested in the source domain are often of little interest within the target domain
                        (i.e. art history, in the present case). The popular Pascal-VOC dataset [<a class="ref" href="#everingham2010">Everingham et al. 2010</a>], for instance, tackles the detection of 20 classes, out of
                        which more than a third constitute different kinds of transportation systems, such
                        as
                        trains, boats, motorcycles and cars. Naturally, these means of transportation are
                        very
                        unlikely to be represented in artworks that date back to the premodern period. The
                        more
                        complex MS-COCO dataset [<a class="ref" href="#lin2014">Lin et al. 2014</a>] presents similar problems: even though
                        the amount of classes increases to 80, most of the objects which should be detected
                        are
                        again unlikely to be represented within historical works of art, since they correspond
                        to objects which have only been relatively recently invented such as "microwave",
                        "cell-phone", "tv-monitor", "laptop", or "remote", and the like. This poses a serious
                        constraint when it comes to the use of pre-trained object-detectors for artistic
                        collections. As with most supervised learning algorithms, models trained on these
                        collections will only perform well on the sort of data on which they have been
                        explicitly trained. To illustrate this model bias, we report some (nonsensical)
                        detections in the first row of images presented in <a href="#figure01">Figure
                           1a</a>.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Robustness of the models</h2>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">Popular object detectors such as YOLO [<a class="ref" href="#redmon2018">Redmon and Farhadi 2018</a>] and Fast R-CNN [<a class="ref" href="#ren2017">Ren et al. 2017</a>] have been designed to perform well on the above-mentioned
                        photo-realistic datasets. However, the variance of the samples denoting a specific
                        class
                        within these datasets is usually much smaller when compared to that in artistic
                        collections. As an example, we refer to a number of images representing the person
                        class
                        within the Pascal-VOC dataset: we can observe from the two leftmost images of the
                        bottom
                        row of <a href="#figure01">Figure 1</a> that the representation of a 'person' is
                        overall relatively unambiguous and hardly distorted. As a result, the person class
                        is
                        usually easily detected by e.g. the YOLO architecture. However, we can see that this
                        task already becomes harder when a person has to be detected within a painting
                        (potentially with a highly distorted representation of the humans in the scene). As
                        shown by the two rightmost images of the bottom row (<a href="#figure01">Figure
                           1b</a>), a YOLO-V3 model does not see most of the persons represented in the
                        paintings and misclassifies them as non-human beings (e.g. "bear").
                     </div>
                     
                     <div id="figure01" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>Examples showing the limitations that occur when a standard object detector
                           trained on photo-realistic images is tested in the domain of the visual arts. Figure
                           1a: Four anecdotal examples showing that the "person” class is usually reasonably
                           detected by the YOLO architecture, although other, non-sensical detections frequently
                           occur. Figure 1b: the two images on the left show that the variation in depiction
                           of
                           people is limited in photorealistic material, in comparison to the artistic
                           representations of people (two examples to the right).
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">All examples in <a href="#figure01">Figure 1</a> come from a pretrained YOLO-V3
                        model [<a class="ref" href="#redmon2018">Redmon and Farhadi 2018</a>] which has been originally trained on the COCO dataset
                        [<a class="ref" href="#lin2014">Lin et al. 2014</a>] and then tested on artworks coming from [<a class="ref" href="#mensink2014">Mensink and Van Gemert 2014</a>] and [<a class="ref" href="#strezoski2017">Strezoski and Worring 2017</a>]. The images presented in
                        the first row illustrate that the network is biased towards making detections which
                        are
                        very unlikely to appear in premodern depictions. These detections correspond to the
                        identification of objects such as "suitcase", "umbrella" or "frisbee" and "banana".
                        The
                        two last images presented in the second row show that standard models fail to properly
                        recognize a simple class such as person. In fact, they fully fail in detecting most
                        of
                        the persons that are present in the artworks due to these representations being highly
                        different from the persons that are present in the Pascal-VOC dataset (first two images
                        of the second row).
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">MINERVA: dataset description</h1>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">In this section, we describe MINERVA, the annotated dataset in the field of object
                     detection that is presented in this work. This novel benchmark dataset will be released
                     jointly with this paper.<a class="noteRef" href="#d4e526">[2]</a> The main task under
                     scrutiny here is the detection of musical instruments in non-photorealistic, unrestricted
                     image collections from the artistic domain. We have named the dataset with the acronym
                     MINERVA, which stands for 'Musical INstrumEnts Represented in the Visual Arts', after
                     the
                     Roman goddess of the arts (amongst many other things).
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Data Sources</h2>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">The base data for our annotation effort was assembled in a series of 'concentric'
                        collection campaigns, where we started from smaller, but high-quality datasets and
                        gradually expanded into larger, albeit less well curated data sources.
                     </div>
                     
                     <div class="ptext">
                        <ol class="list">
                           <li class="item"><span class="label bold">RIDIM</span>: We harvested a collection of high-quality images from the
                              RIDIM database, in those cases where the database entries provided an unambiguous
                              hyperlink to a publicly accessible image. These records were already assigned MIMO
                              codes by a community of domain experts, which provided important support to our
                              in-house annotators (especially during the first experimental rounds of
                              annotations).
                           </li>
                           <li class="item"><span class="label bold">RMFAB/RMAH</span>: We expanded on the core RIDIM data by including
                              (midrange resolution) images from the digital collections of two federal museums in
                              Brussels: the RMFAB (Royal Museums of Fine Arts of Belgium, Brussels) and the RMAH
                              (Royal Museums of Art and History, Brussels). These images were selected on the basis
                              of previous annotations that suggested they included depictions of musical
                              instruments, although no more specific labels (e.g. MIMO codes) were available for
                              these records at this stage. Copyrighted artworks could not be included for obvious
                              reasons (copyright lasts for 70 years from the death of the creator under Belgian
                              intellectual law).
                           </li>
                           <li class="item"><span class="label bold">Flickr</span>: To scale up our annotation efforts, finally, we collected a
                              larger dataset of images from the well-known image hosting service 'Flickr'
                              (www.flickr.com). We harvested all images from a community-curated collection of
                              depictions of musical instruments in the visual arts pre-dating 1800.<a class="noteRef" href="#d4e557">[3]</a>
                              This third campaign yielded much more data than the former two, but these were more
                              noisy and contained a variety of false positives that had to be manually deleted
                              during the annotation phase.
                           </li>
                        </ol>
                     </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">Our collection efforts were inclusive, and the resulting dataset should be considered
                        as "unrestricted", covering a variety of periods, genres and materials (although it
                        was
                        not feasible to include more precise metadata about these aspects in the dataset).
                        Note
                        that, exactly because of this highly mixed data origin, the distribution in MINERVA
                        does
                        not give a faithful representation of any kind of historic reality: music iconography
                        gives a highly colored perspective on "popular" instruments in art history and some
                        instruments may not often have been depicted, even though they were popular at the
                        time.
                        Likewise, other instruments are likely to be over-represented in iconography.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Vocabulary</h2>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">To increase the interoperability of the dataset, individual instruments have been
                        unambiguously identified using their MIMO codes. The MIMO (Musical Instrument Museums
                        Online) initiative is an international consortium, well known for its online database
                        of
                        musical instruments, aggregating data and metadata from multiple heritage institutions
                        [<a class="ref" href="#dolan2017">Dolan 2017</a>].<a class="noteRef" href="#d4e576">[4]</a>
                        An important contribution is their development of a uniform metadata documentation
                        standard for the field, including a (multilingual) vocabulary to identify musical
                        instruments in an interoperable manner. The MIMO ontology is hierarchical, meaning
                        that
                        each individual leaf node in their concept tree (e.g. 'viola') is a hyponym of a wider
                        instrument category (e.g. 'viola' ∈ 'string instruments'). <a href="#table01">Table
                           1</a> shows examples of annotation labels from this ontology. Our dataset provides a
                        spreadsheet that allows for the easy mapping of individual instruments to their
                        instrument category. Below, we shall report experiments for the more fine-grained
                        and
                        less granular, hypernym versions of the categorization task.
                     </div>
                     
                     <div id="table01" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Instrument hypernym</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">StringedInstruments</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Wind instruments</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Percussion instruments</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Keyboard instruments</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Electronic instruments</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi italic">Example instruments</span></td>
                              
                              <td valign="top" class="cell">Lute, psaltery, fiddle, viola da gamba, cittern</td>
                              
                              <td valign="top" class="cell">Transverse flute, end-blown trumpet, horn, shawm, bagpipe</td>
                              
                              <td valign="top" class="cell">Tambourine, cylindrical drum, frame drum, friction drum, bell</td>
                              
                              <td valign="top" class="cell">Pianoforte, virginal, portative organ, harpsichord, clavichord</td>
                              
                              <td valign="top" class="cell">Electric guitar, synthesizer, theremin, vocoder, mellotron</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>Examples of annotation labels from the MIMO ontology (not all were encountered in
                           MINERVA).
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Annotation process</h2>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20">Using the conventional method of rectangular bounding boxes, we have manually annotated
                        16,142 musical instruments (of which 172 unique) in a collection of 11,765 images,
                        within the open-source <a href="https://cytomine.be" onclick="window.open('https://cytomine.be'); return false" class="ref">Cytomine</a> software
                        environment [<a class="ref" href="#mar%C3%A9e2016">Marée et al. 2016</a>]. Often multiple instruments appeared within the
                        same images and bounding boxes were therefore allowed to overlap. Example annotations
                        and a screenshot of the annotation environment are presented in <a href="#figure02">Figure 2</a>.
                     </div>
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">The dataset contains artistic objects from diverse periods and of various types,
                        ranging from paintings, sculptures, drawings, to decorative arts, manuscript
                        illuminations and stained-glass windows. Thus, they involve a daunting diversity of
                        media, techniques and modes. Whereas in some cases the images were straightforward
                        to
                        annotate (e.g. an image representing a bell in full frame), several obstacles occurred
                        on a recurrent basis. These obstacles can be linked to three parameters:
                     </div>
                     
                     <div class="ptext">
                        <ol class="list">
                           <li class="item"><span class="label bold">Representation</span>: A challenging aspect was the variety of artistic
                              depiction modes represented in the dataset, ranging from photo-realistic renderings
                              to
                              heavily stylized depictions from specific art-historical movements (e.g.
                              impressionism, pointillism, fauvism, cubism, ...) (<a href="#figure03">Figure
                                 3a</a>). Additionally, visibility could be low due to a proportionally small
                              instrument depiction or the profusion of details (<a href="#figure03">Figure
                                 3b</a>). In some instances, the state of the depicted object and its medium made
                              the detection of the instrument difficult, e.g. a damaged medieval tympanon (<a href="#figure03">Figure 3b</a>).
                           </li>
                           <li class="item"><span class="label bold">Quality</span>: Other, more pragmatic issues arose from the images
                              themselves. Occasionally, the quality of the images was too low to be able to detect
                              the instruments (e.g. low resolution or compression defects) (<a href="#figure03">Figure 3c</a>). A great deal of the images did not meet international quality
                              standards for heritage reproduction photography (uniform and neutral environment and
                              lighting, frontal point of view), which implies that the instruments were even more
                              difficult to detect.
                           </li>
                           <li class="item"><span class="label bold">Boxes</span>: The use of a rectangular shape for the bounding boxes
                              sometimes has limitations and implied a certain lack of precision, e.g. in the case
                              of
                              a diagonally positioned flute, or in the case of overlapping instruments (<a href="#figure03">Figure 3d</a>). For some instruments which consist of several
                              parts, e.g. a violin and its bow, only the main part (the violin) was
                              annotated.
                           </li>
                        </ol>
                     </div>
                     
                     <div id="figure02" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 2. </div>Illustration of the annotation interface in Cytomine [<a class="ref" href="#mar%C3%A9e2016">Marée et al. 2016</a>].
                        </div>
                     </div>
                     
                     <div id="figure03" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Examples of difficulties encountered when annotating images.
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Characteristics</h2>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">An important share of the annotations which we collected were singletons, i.e.
                        instruments that were only encountered once or twice. Although we release the full
                        dataset, we shall from now on only consider instruments that occurred at least three
                        times that allow for a conventional machine learning setup (with non-overlapping train,
                        validation and test sets, that include at least one instance of each label). Whereas
                        the
                        full MIMO vocabulary covers over 2,000 vocabulary terms for individual instruments,
                        only
                        a fraction of these were attested in the 4,183 images which we use below (overview
                        in
                        <a href="#table01">Table 1</a>). Note that this table shows a considerable drop
                        in the original number of images that we annotated, because we only included images
                        that
                        (a) actually contained an instrument and (b) images depicting instruments that occurred
                        at least thrice.
                     </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">93 different instrument categories appear at least thrice in the dataset. A
                        visualization of the heavily skewed distribution of the different instruments can
                        be
                        seen in <a href="#figure04">Figure 4</a>, where each instrument is represented
                        together with its corresponding MIMO code (between parentheses). This distribution
                        exposes two core aspects of this dataset (but also of music iconography in general):
                        (i)
                        its strong Western-European bias, which has been historically acknowledged, and which
                        scholars are actively trying to correct nowadays, but which is a slow process; (ii)
                        the
                        'heavy-tail' distribution associated with cultural data in general; i.e. only a fraction
                        of instruments, such as the lute, harp and violin, are depicted with a high frequency,
                        the rest occurs much more sparsely.
                     </div>
                     
                     <div id="figure04" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Distribution of the instrument types in the full MINERVA dataset.
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Versions and splits</h2>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">The label imbalance described in the previous paragraph is a significant issue for
                        machine learning methods. We therefore experiment with the data in five versions (that
                        are available from the repository) that correspond to object detection tasks of varying
                        complexity. We start by exploring whether it is possible to just detect the presence
                        of
                        an instrument in the different artworks, without the additional need of also predicting
                        the class of the detected instrument. We refer to this benchmark as single-instrument
                        object detection. We then move to three more challenging tasks in which we also aim
                        at
                        correctly classifying the content of the detected bounding boxes. We include data
                        for
                        this detection task for the top-5, the top-10 and top-20 most frequently occurring
                        instruments, a customary practice in the field. Finally, we also repeat this task
                        for
                        all images, but with the "hypernym" labels of the instrument categories (see <a href="#figure05">Figure 5</a>).
                     </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">Each version of the dataset comes with its own training, development and testing
                        splits, where we offer the guarantee that at least one of the instrument classes in
                        the
                        task is represented in each of the splits. Additionally, the splits are stratified
                        so
                        that the class distribution is approximately the same in each split. The number of
                        images per split in each version is summarized in <a href="#table02">Table 2</a>.
                        The hypernym version of the dataset is not reported in this table as it shares the
                        same
                        images and splits as the single-instrument version (they both contain all instruments).
                        We used a standard implementation [<a class="ref" href="#pedregosa2011">Pedregosa et al. 2011</a>] for a randomized and
                        shuffled split at the level of images and the following, approximate proportions:
                        1/2
                        train, 1/4 dev, and 1/4 test. Images may contain multiple instruments, so that the
                        actual number of instruments (as opposed to images) may vary relatively strongly across
                        splits.
                     </div>
                     
                     <div id="table02" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Training-set</span></td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Dev-set</span></td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Test-set</span></td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Total</span></td>
                              
                              <td valign="top" class="cell"></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">Imag</td>
                              
                              <td valign="top" class="cell">Inst</td>
                              
                              <td valign="top" class="cell">Imag</td>
                              
                              <td valign="top" class="cell">Inst</td>
                              
                              <td valign="top" class="cell">Imag</td>
                              
                              <td valign="top" class="cell">Inst</td>
                              
                              <td valign="top" class="cell">Imag</td>
                              
                              <td valign="top" class="cell">Inst</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Single inst</td>
                              
                              <td valign="top" class="cell">1857</td>
                              
                              <td valign="top" class="cell">4243</td>
                              
                              <td valign="top" class="cell">1137</td>
                              
                              <td valign="top" class="cell">2288</td>
                              
                              <td valign="top" class="cell">1189</td>
                              
                              <td valign="top" class="cell">2102</td>
                              
                              <td valign="top" class="cell">4183</td>
                              
                              <td valign="top" class="cell">8633</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Top-5 inst</td>
                              
                              <td valign="top" class="cell">952</td>
                              
                              <td valign="top" class="cell">1589</td>
                              
                              <td valign="top" class="cell">540</td>
                              
                              <td valign="top" class="cell">852</td>
                              
                              <td valign="top" class="cell">724</td>
                              
                              <td valign="top" class="cell">1173</td>
                              
                              <td valign="top" class="cell">2216</td>
                              
                              <td valign="top" class="cell">3614</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Top-10 inst</td>
                              
                              <td valign="top" class="cell">1227</td>
                              
                              <td valign="top" class="cell">2147</td>
                              
                              <td valign="top" class="cell">680</td>
                              
                              <td valign="top" class="cell">1127</td>
                              
                              <td valign="top" class="cell">898</td>
                              
                              <td valign="top" class="cell">1506</td>
                              
                              <td valign="top" class="cell">2805</td>
                              
                              <td valign="top" class="cell">4780</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Top-20 inst</td>
                              
                              <td valign="top" class="cell">1471</td>
                              
                              <td valign="top" class="cell">2915</td>
                              
                              <td valign="top" class="cell">860</td>
                              
                              <td valign="top" class="cell">1543 </td>
                              
                              <td valign="top" class="cell">1047</td>
                              
                              <td valign="top" class="cell">1838</td>
                              
                              <td valign="top" class="cell">3378</td>
                              
                              <td valign="top" class="cell">6296</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Image and instruments distributions of the training, development and test sets for
                           the four different benchmarks presented in this paper (single instrument, top-5
                           instruments, top-10 instruments and top-20 instruments).
                        </div>
                     </div>
                     
                     <div id="figure05" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 5. </div>Distribution of the 5 hypernym categories over the three splits in the MINERVA
                           dataset.
                        </div>
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Benchmark experiments</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Classification</h2>
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">In the first benchmark experiment, we start by investigating whether convolutional
                        neural networks are able to correctly classify the different instruments that are
                        present in the dataset. That means that we focus on the image classification task
                        and
                        postpone the task of object detection to the next section. To this end, we have
                        extracted the various patches delineated by the bounding boxes in the detection dataset
                        as stand-alone instances. Note, however, that patches from the same images always
                        ended
                        in the same split, to avoid information leakage across the splits. Example patches
                        are
                        shown in <a href="#figure06">Figure 6</a>.
                     </div>
                     
                     <div id="figure06" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 6. </div>Examples of the patches delineated by the bounding boxes, extracted from MINERVA
                           images for the classification experiment.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">Next, we tackled this task as a standard machine-learning classification problem for
                        which we applied a representative selection of established neural network architectures.
                        All of these networks were pretrained on the Rijksmuseum dataset [<a class="ref" href="#mensink2014">Mensink and Van Gemert 2014</a>], for which the weights are publicly available [<a class="ref" href="#sabatelli2018">Sabatelli et al. 2018</a>]. The tested architectures are: VGG19 [<a class="ref" href="#simonyan2014">Simonyan and Zisserman 2014</a>], Inception-V3 [<a class="ref" href="#szegedy2015">Szegedy et al. 2015</a>] and ResNet [<a class="ref" href="#he2016">He et al. 2016</a>]. This approach is motivated by previous work [<a class="ref" href="#sabatelli2018">Sabatelli et al. 2018</a>] which shows that when it comes to the classification images
                        from the domain of cultural heritage, popular neural architectures which have been
                        trained on the large Rijksmuseum collection, can outperform the same kind of
                        architectures that are pre-trained on ImageNet only. In order to maximize the final
                        classification performance, all network parameters get fine-tuned, using the Adam
                        optimizer [<a class="ref" href="#kingma2014">Kingma and Ba 2014</a>] and minimizing the conventional categorical
                        cross-entropy loss function over mini-batches of 32 samples. Additionally, we applied
                        3
                        different learning rates: 0.001, 0.0001, 0.00001. In order to handle the skewed
                        distribution of the classes, we experimented with models including and excluding
                        oversampling. The training regime is interrupted as soon the validation loss does
                        not
                        decrease for five epochs in a row.
                     </div>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">In <a href="#table01">Table 2</a> and <a href="#table03">Table 3</a> we
                        report the results in terms of Accuracy and F1-score for the MINERVA test sets. For
                        the
                        individual instruments, we do so for four versions of the dataset of increasing
                        complexity: the top-5 instruments, top-10 instruments, top-20 instruments and the
                        entire
                        dataset. Analogously we report the scores for a classification experiment where the
                        object detector is trained on the instrument hypernyms as class labels.
                     </div>
                     
                     <div id="table03" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">Top-5 inst</td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">Top-10 inst</td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">Top-20 inst</td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">All inst</td>
                              
                              <td valign="top" class="cell"></td>
                              
                              <td valign="top" class="cell">Hypernyms</td>
                              
                              <td valign="top" class="cell"></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">CNN</td>
                              
                              <td valign="top" class="cell">Acc.</td>
                              
                              <td valign="top" class="cell">F1</td>
                              
                              <td valign="top" class="cell">Acc.</td>
                              
                              <td valign="top" class="cell">F1</td>
                              
                              <td valign="top" class="cell">Acc.</td>
                              
                              <td valign="top" class="cell">F1</td>
                              
                              <td valign="top" class="cell">Acc.</td>
                              
                              <td valign="top" class="cell">F1</td>
                              
                              <td valign="top" class="cell">Acc.</td>
                              
                              <td valign="top" class="cell">F1</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">R-Net</td>
                              
                              <td valign="top" class="cell">68.71</td>
                              
                              <td valign="top" class="cell">64.10</td>
                              
                              <td valign="top" class="cell">52.85</td>
                              
                              <td valign="top" class="cell">41.55</td>
                              
                              <td valign="top" class="cell">30.73</td>
                              
                              <td valign="top" class="cell">8.45</td>
                              
                              <td valign="top" class="cell">26.36</td>
                              
                              <td valign="top" class="cell">2.08</td>
                              
                              <td valign="top" class="cell">72.26</td>
                              
                              <td valign="top" class="cell">52.66 </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">V3</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">73.66</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">70.29</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">55.51</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">44.77</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">36.51</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">19.06</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">27.02</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">6.67</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">75.80</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">57.03</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">V19</td>
                              
                              <td valign="top" class="cell">48.33</td>
                              
                              <td valign="top" class="cell">35.92</td>
                              
                              <td valign="top" class="cell">37.52</td>
                              
                              <td valign="top" class="cell">15.22</td>
                              
                              <td valign="top" class="cell">33.41</td>
                              
                              <td valign="top" class="cell">9.87</td>
                              
                              <td valign="top" class="cell">20.17</td>
                              
                              <td valign="top" class="cell">1.72</td>
                              
                              <td valign="top" class="cell">66.41</td>
                              
                              <td valign="top" class="cell">40.35</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 3. </div>Classification results on the MINERVA test set for the three architectures (best
                           results in bold).
                        </div>
                     </div>
                     
                     <div id="table04" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Predicted label / Gold label</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Bagpipe</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">E-b trumpet</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Harp</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Horn</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Lute</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Lyre</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Por. organ</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Rebec</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Shawm</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Violin</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Bagpipe</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">31</span></td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">10</td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell">8</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">7</td>
                              
                              <td valign="top" class="cell">17</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">E-b trumpet</span></td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">72</span></td>
                              
                              <td valign="top" class="cell">19</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">14</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">3</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">38</td>
                              
                              <td valign="top" class="cell">21</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Harp</span></td>
                              
                              <td valign="top" class="cell">8</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">227</span></td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">10</td>
                              
                              <td valign="top" class="cell">3</td>
                              
                              <td valign="top" class="cell">11</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">10</td>
                              
                              <td valign="top" class="cell">19</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Horn</span></td>
                              
                              <td valign="top" class="cell">7</td>
                              
                              <td valign="top" class="cell">5</td>
                              
                              <td valign="top" class="cell">14</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">9</span></td>
                              
                              <td valign="top" class="cell">16</td>
                              
                              <td valign="top" class="cell">9</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">5</td>
                              
                              <td valign="top" class="cell">14</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Lute</span></td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell">10</td>
                              
                              <td valign="top" class="cell">17</td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">199</span></td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell">5</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">5</td>
                              
                              <td valign="top" class="cell">42</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Lyre</span></td>
                              
                              <td valign="top" class="cell">3</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">19</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">13</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">5</span></td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">3</td>
                              
                              <td valign="top" class="cell">11</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Por. organ</span></td>
                              
                              <td valign="top" class="cell">3</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">10</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">57</span></td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Rebec</span></td>
                              
                              <td valign="top" class="cell">5</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">14</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">9</td>
                              
                              <td valign="top" class="cell">0</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">7</span></td>
                              
                              <td valign="top" class="cell">1</td>
                              
                              <td valign="top" class="cell">23</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Shawm</span></td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell">11</td>
                              
                              <td valign="top" class="cell">25</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">11</td>
                              
                              <td valign="top" class="cell">2</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">40</span></td>
                              
                              <td valign="top" class="cell">13</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Violin</span></td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell">12</td>
                              
                              <td valign="top" class="cell">29</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell">35</td>
                              
                              <td valign="top" class="cell">4</td>
                              
                              <td valign="top" class="cell">7</td>
                              
                              <td valign="top" class="cell">11</td>
                              
                              <td valign="top" class="cell">6</td>
                              
                              <td valign="top" class="cell"><span class="hi bold">202</span></td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 4. </div>Confusion matrix for the classification experiment with ResNet on the MINERVA test
                           set (the top-10 most frequently occurring instruments).
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Detection</h2>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">For the second benchmark experiment we report the results that we have obtained on
                        the
                        four of the five detection benchmarks introduced in the previous section. The way
                        the
                        different instruments are distributed in their respective test sets is visually
                        represented in the first image of each row of <a href="#figure07">Figure 7</a>.
                        For our experiments, we use the popular YOLO-V3 [<a class="ref" href="#redmon2018">Redmon and Farhadi 2018</a>] architecture
                        which we fully fine-tune during training. To explore the benefits that transfer learning
                        could bring to the artistic domain, we initialize the network with the weights that
                        are
                        obtained after training the model on the MS-COCO dataset [<a class="ref" href="#lin2014">Lin et al. 2014</a>]. The
                        network gets then trained either with the Adam optimizer [<a class="ref" href="#kingma2014">Kingma and Ba 2014</a>] or
                        RMSprop<a class="noteRef" href="#d4e1673">[5]</a>
                        over mini-batches of 8 images.
                     </div>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">To assess the performance of the neural network, we follow the same evaluation protocol
                        that characterizes object detection problems in CV [<a class="ref" href="#lin2014">Lin et al. 2014</a>]. Each
                        detected bounding box is compared to the bounding box which has been annotated on
                        the
                        Cytomine platform. We only consider bounding boxes for which the confidence level
                        is ≥
                        0.05, following the protocol established in [<a class="ref" href="#everingham2010">Everingham et al. 2010</a>]. We then
                        compute the "Intersection over Union" (IoU) for measuring how much the detected
                        bounding-boxes differ from the ground-truth ones. To assess whether a prediction can
                        be
                        considered as a "true positive" or a "false positive", we define two, increasingly
                        restrictive metrics: first, IoU ≥ 10 and, secondly, IoU ≥ 50. This approach is again
                        inspired by [<a class="ref" href="#gonthier2018">Gonthier et al. 2018</a>], where the authors report additional results
                        with an IoU ≥ 10 on their IconArt dataset. <a href="#table05">Table 5</a> lists
                        precision, recall and average precision (AP) scores for each detected class of each
                        data
                        version and <a href="#figure07">Figure 7</a> visually shows the number of true and
                        false positive predictions in all cases. Examples of correct detections are shown
                        in
                        <a href="#figure08">Figure 8</a>.
                     </div>
                     
                     <div id="table05" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Instrument ≥ IoU</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Precision</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Recall</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">AP</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Single-instrument ≥ <br />10 Single-instrument ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.63 <br />0.47
                              </td>
                              
                              <td valign="top" class="cell">0.42 <br />0.31
                              </td>
                              
                              <td valign="top" class="cell">0.35 <br />0.22
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Stringed-Instruments ≥ 10 <br />Stringed-Instruments ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.65 <br />0.53
                              </td>
                              
                              <td valign="top" class="cell">0.36 <br />0.29
                              </td>
                              
                              <td valign="top" class="cell">0.28 <br />0.20
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Wind-Instruments ≥ 10 <br />Wind-Instruments ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.43 <br />0.32
                              </td>
                              
                              <td valign="top" class="cell">0.07 <br />0.05
                              </td>
                              
                              <td valign="top" class="cell">0.04 <br />0.02
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Percussion-Instruments ≥ 10 <br />Percussion-Instruments ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.32 <br />0.21
                              </td>
                              
                              <td valign="top" class="cell">0.04 <br />0.03
                              </td>
                              
                              <td valign="top" class="cell">0.02 <br />0.01
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Keyboard-Instruments ≥ 10 <br />Keyboard-Instruments ≥ 50
                              </td>
                              
                              <td valign="top" class="cell"> 0.61 <br />0.45
                              </td>
                              
                              <td valign="top" class="cell"> 0.11 <br />0.08
                              </td>
                              
                              <td valign="top" class="cell"> 0.07 <br />0.04
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Electronic-Instruments ≥ 10 <br />Electronic-Instruments ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Harp ≥ 10 <br />Harp ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.68 <br />0.60
                              </td>
                              
                              <td valign="top" class="cell">0.62 <br />0.54
                              </td>
                              
                              <td valign="top" class="cell">0.55 <br />0.46
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Lute ≥ 10 <br />Lute ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.57 <br />0.47
                              </td>
                              
                              <td valign="top" class="cell">0.43 <br />0.35
                              </td>
                              
                              <td valign="top" class="cell">0.36 <br />0.26
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Violin ≥ 10 <br />Violin ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.37 <br />0.26
                              </td>
                              
                              <td valign="top" class="cell">0.22 <br />0.16
                              </td>
                              
                              <td valign="top" class="cell">0.12 <br />0.07
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Shawm ≥ 10 <br />Shawm ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.13 <br />0.08
                              </td>
                              
                              <td valign="top" class="cell">0.04 <br />0.02
                              </td>
                              
                              <td valign="top" class="cell">0.01 <br />0.00
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">End-blown trumpet ≥ 10 <br />End-blown trumpet ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.28 <br />0.24
                              </td>
                              
                              <td valign="top" class="cell">0.04 <br />0.03
                              </td>
                              
                              <td valign="top" class="cell">0.01 <br />0.01
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Harp ≥ 10 <br />Harp ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.62 <br />0.56
                              </td>
                              
                              <td valign="top" class="cell">0.56 <br />0.51
                              </td>
                              
                              <td valign="top" class="cell">0.46 <br />0.39
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Lute ≥ 10 <br />Lute ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.55 <br />0.47
                              </td>
                              
                              <td valign="top" class="cell">0.42 <br />0.36
                              </td>
                              
                              <td valign="top" class="cell">0.33 <br />0.25
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Violin ≥ 10 <br />Violin ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.26 <br />0.20
                              </td>
                              
                              <td valign="top" class="cell">0.19 <br />0.14
                              </td>
                              
                              <td valign="top" class="cell">0.06 <br />0.04
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Shawm ≥ 10 <br />Shawm ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.17 <br />0.17
                              </td>
                              
                              <td valign="top" class="cell">0.03 <br />0.01
                              </td>
                              
                              <td valign="top" class="cell">0.00 <br />0.00
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">End-blown trumpet ≥ 10 <br />End-blown trumpet ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.67 <br />0.17
                              </td>
                              
                              <td valign="top" class="cell">0.02 <br />0.03
                              </td>
                              
                              <td valign="top" class="cell">0.01 <br />0.00
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Bagpipe ≥ 10 <br />Bagpipe ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Portative-Organ ≥ 10 <br />Portative-Organ ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0.24 <br />0.24
                              </td>
                              
                              <td valign="top" class="cell">0.13 <br />0.13
                              </td>
                              
                              <td valign="top" class="cell">0.06 <br />0.06
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Horn ≥ 10 <br />Horn ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                              <td valign="top" class="cell">0 <br />0
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Rebec ≥ 10 <br />Rebec ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Lyre ≥ 10 <br />Lyre ≥ 50
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">- <br />-
                              </td>
                              
                              <td valign="top" class="cell">-- <br />-
                              </td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 5. </div>A quantitative analysis of the results obtained on the four localization
                           benchmarks introduced in this work. To distinguish different benchmarks in the table
                           we separate them by a double line. We report the precision, recall and
                           average-precision scores for each detected class.
                        </div>
                     </div>
                     
                     <div id="figure07" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 7. </div>A visual representation of how many instruments should be detected in the testing
                           sets of the four MINERVA benchmarks that are introduced in this paper (first plot
                           of
                           each row). The second and third plots represent the true and false detections that
                           we
                           have obtained with a fully fine-tuned YOLO network. Results are computed with respect
                           to an IoU ≥ 10 and an IoU ≥ 50.
                        </div>
                     </div>
                     
                     <div id="figure08" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 8. </div>Sample visualizations of the detections obtained on the MINERVA test set for a
                           fully fine-tuned YOLO architecture. The first three rows report the detection of any
                           kind of instrument within the images (single-instrument task), while the last three
                           rows also report the correct classification of the detected bounding boxes.
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Additional experiments</h2>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">As an additional stress-test, we have applied a trained object detector to two external
                        data sets, in order to assess how valid and performant our approach is when applied
                        "in
                        the wild". We have considered two out-of-sample datasets:
                     </div>
                     
                     <div class="ptext">
                        <ul class="list">
                           <li class="item"><span class="label bold">RMFAB/RMAH</span>: 428 out-of-sample images from the digital assets of
                              both museum collections that are not included in annotated material (and which are
                              thus not included the train and validation material of the applied detector), because
                              the available metadata did not explicitly specify that they contained depictions of
                              musical instruments. (This collection cannot be shared due to copyright
                              restrictions.)
                           </li>
                           <li class="item"><span class="label bold">IconArt</span>: a generic collection of 6,528 artistic images, collected
                              from the community-curated platform <em class="emph">WikiArt: Visual Art Encyclopedia</em>
                              (<a href="https://www.wikiart.org/" onclick="window.open('https://www.wikiart.org/'); return false" class="ref">https://www.wikiart.org/</a>). The IconArt
                              subcollection was previously redistributed by [<a class="ref" href="#gonthier2018">Gonthier et al. 2018</a>]: <a href="https://wsoda.telecom-paristech.fr/downloads/dataset/" onclick="window.open('https://wsoda.telecom-paristech.fr/downloads/dataset/'); return false" class="ref">https://wsoda.telecom-paristech.fr/downloads/dataset/</a>.
                           </li>
                        </ul>
                     </div>
                     
                     <div class="counter"><a href="#p32">32</a></div>
                     <div class="ptext" id="p32">Note that both external datasets differ in crucial aspects: <span class="label bold">RMFAB/RMAH</span>
                        can be considered "out-of-sample", but "in-collection", in the sense that these images
                        derive from the same digital collections as many of the images represented in MINERVA.
                        Additionally, we can expect extremely low detection rates for this dataset, because
                        the
                        presence of musical instruments will already have been flagged in a large majority
                        of
                        cases by the museum's staff. Thus, the application of <span class="label bold">RMFAB/RMAH</span> should
                        be viewed as a rather conservative stress test or sanity check, mainly checking for
                        images that might have been missed by annotators in the past. The IconArt dataset
                        is
                        "out-of-sample" and "out-of-collection", in the sense that these images derive from
                        a
                        variety of other sources. It is therefore fully unrestricted, and this test can be
                        considered a curiosity-driven validation of the method "in the wild". Importantly,
                        IconArt was not collected with specific attention for musical instruments, so here
                        too,
                        we can anticipate a rather low detection rate (since many works of art simply do not
                        feature any instruments). For all these reasons, we only evaluate the results on these
                        external datasets in terms of precision (as recall is much less meaningful in this
                        context).
                     </div>
                     
                     <div class="counter"><a href="#p33">33</a></div>
                     <div class="ptext" id="p33">Following these differences, we have applied the single-instrument detector to the
                        <span class="label bold">RMFAB/RMAH</span> data and the hypernym detector to IconArt. Keeping an eye on
                        the feasibility of the manual inspection, we have limited the number of instances
                        returned by only allowing detections with a confidence score ≥ 0.20 (which is a rather
                        generous threshold). Next, the results have then been evaluated in terms of precision,
                        i.e. the number of returned image regions that actually represent musical instruments.
                        The results are presented in <a href="#table06">Table 6</a>. <a href="#figure10">Figure 10</a> showcases a number of cherry-picked successful
                        examples of detections from the out-of-collection IconArt images.
                     </div>
                     
                     <div id="table06" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Collection</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Total images</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Detections</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">True positives</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">RMFAB/RMAH</span></td>
                              
                              <td valign="top" class="cell">428</td>
                              
                              <td valign="top" class="cell">162</td>
                              
                              <td valign="top" class="cell">6</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">IconArt</span></td>
                              
                              <td valign="top" class="cell">6528</td>
                              
                              <td valign="top" class="cell">118</td>
                              
                              <td valign="top" class="cell">42</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 6. </div>Quantitative evaluation of the method on two out-of-sample datasets in terms of
                           precision, restricted to detections with a confidence score ≥ 0.20.
                        </div>
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Discussion</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Skewed results</h2>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">First and foremost, we can observe that the scores obtained across all benchmarks
                        are
                        generally much lower than those reported for other datasets in computer vision (outside
                        of the strict artistic domain). This drop in performance was to be expected and can
                        be
                        attributed to both the smaller size of the training data and the higher variance in
                        the
                        representation spectrum of musical instruments (across periods, materials, modes and,
                        artists). Secondly, one can observe large fluctuations in the identifiability and
                        detectability of individual instrument categories across both tasks. Not all of the
                        fluctuations are easy to account for.
                     </div>
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">We first consider the classification results. The confusion matrix reported in <a href="#table04">Table 4</a> clearly shows that the classes representing the top-4
                        of instruments (harp, lute, violin, and portative organ) can be learned rather
                        successfully, but that the performance rapidly breaks down for instrument categories
                        at
                        lower frequency ranks. Thus, while the accuracies for the top-5 experiments are
                        relatively satisfying, especially in terms of accuracy (V3: <em class="emph">acc=73.66;
                           F1=70.29</em>), the performance rapidly degrades for the more difficult setups. The
                        results for the "all" classification experiment, where every instrument category is
                        included no matter its frequency, are nothing less than dramatic (V3: <em class="emph">acc=27.02;
                           F1=6.67</em>) and call for in-depth further research. The significant divergence
                        between accuracy scores and F1 scores demonstrate that class imbalance is thus another
                        aspect in which MINERVA presents a more challenging benchmark than its photorealistic
                        counterparts.
                     </div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">The skewness of the class distribution in MINERVA is representative of the long-tail
                        distribution that we commonly encounter in cultural data. This imbalance is somewhat
                        alleviated in the hypernym setup, where the labels are of course much better distributed
                        over a much smaller number of classes (<em class="emph">n=5</em>). The general feasibility of
                        this specific task is demonstrated by the encouraging scores that can be reported
                        for
                        the Inception-V3 architecture on this task (<em class="emph">acc=75.80; F1=57.03</em>). Note,
                        additionally, that the "Electronic instruments" hypernym is included for completeness
                        in
                        this task, although the label is very infrequent and inevitably pulls down the
                        (macro-averaged) F1-score in this respect. Overall, we notice than the Inception-V3
                        architecture yields the highest performance on average for the classification task.
                     </div>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Similar trends can be observed for the musical instrument detection task. First of
                        all,
                        we should emphasize the encouraging scores for the "single-instrument" detection task
                        that simply aims to detect musical instruments (no matter their type). Here, a
                        relatively high precision score is obtained (<em class="emph">prec=0.63</em> for <em class="emph">IoU ≥
                           10</em>), which seems on par with comparable object categories for modern
                        photo-realistic collections [<a class="ref" href="#ren2017">Ren et al. 2017</a>]. Thus, this algorithm might not be
                        fully apt at retrieving every single instrument from an unseen collection, but when
                        it
                        detects an instrument, we can be relatively sure that the detection deserves further
                        inspection by a domain expert. Equally heartening scores are evident for most of the
                        instrument hypernyms (with the notable exception of the under-represented "Electronic
                        instruments" hypernym). While these detection tasks are of course relatively coarse,
                        this observation nevertheless entails that this sort of detection technology can already
                        find useful applications in the field (see below).
                     </div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">When making our way down the frequency list in <a href="#table05">Table 5</a>, we
                        again observe how the results break down dramatically for less common instrument
                        categories. The fact that an over-represented category like harps can be reasonably
                        well
                        detected (<em class="emph">AP(IoU ≥ 10)=0.55; AP(IoU ≥ 50)=0.46</em>), should not lead the
                        attention away from the fact that a state of the art object detector, such as YOLO,
                        fails miserably at detecting a number of iconographically highly salient instruments,
                        such as lyres and end-blown trumpets. At this stage, it is unclear whether this is
                        caused by mere class imbalance or by the higher variance in the iconographic depiction
                        of specific instruments. Bagpipes, for instance, occur frequently across images in
                        MINERVA but might display much more depiction variance than, for instance, a harp.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Saliency maps</h2>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">The results from the previous question call into question which visual properties
                        the
                        neural networks find useful to exploit in the identification of instruments.
                        Importantly, the characteristic features exploited by a machine learning algorithm
                        need
                        not coincide with the properties that are judged most relevant by human experts and
                        the
                        comparison of both types of relevance judgements is worthwhile. In this section, we
                        therefore perform model criticism or "network introspection" on the basis of the
                        so-called "saliency maps" that can be extracted from a trained model [<a class="ref" href="#boyarski2017">Boyarski et al. 2017</a>]. These saliency maps make visible to which regions in the
                        original image the network paid most attention to, before arriving at its final
                        classification decision. All examples discussed below come from the experiments on
                        the
                        hypernym dataset for the VGG19 network. <a href="#figure09">Figure 9</a> shows a
                        series of manually selected, insightful examples, including the original image (as
                        inputted into the network after preprocessing), as well as the saliency map obtained
                        for
                        it. We limit these examples to the representative hypernyms 'Stringed instruments'
                        and
                        'Wind instruments.'
                     </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">The maps in <a href="#figure09">Figure 9</a> vividly illustrate that the network
                        focuses on two broad types of regions: properties of the instruments itself (which
                        was
                        expected) but also the immediate context of the instruments, and more specifically
                        the
                        way they are operated, handled or presented by people, c.q. musicians. The
                        characteristics of the salient regions in the examples in <a href="#figure09">Figure
                           9</a> could be described as:
                     </div>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41"><span class="label bold">Stringed instruments:</span></div>
                     
                     <div class="ptext">
                        <ul class="list simple">
                           <li class="item">(a) Focus on the neck of the stringed instrument, as well as the characteristic
                              presence of tuning pins at the end of the neck;
                           </li>
                           <li class="item">(b) Sensitive to the presence of stretched fingers in an unnatural
                              position;
                           </li>
                           <li class="item">(c) Typical conic shape of a lyre, with outward pointing ends connected by a
                              bridge;
                           </li>
                        </ul>
                     </div>
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42"><span class="label bold">Wind instruments:</span></div>
                     
                     <div class="ptext">
                        <ul class="list simple">
                           <li class="item">(d) Symmetric presence of tone holes in the areophone;</li>
                           <li class="item">(e) Elongated, cylindric shape of the main body of the areophone with wider
                              end;
                           </li>
                           <li class="item">(f) Mirrored placement of fingers and hands (close to one another).</li>
                        </ul>
                     </div>
                     
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">These characteristics strongly suggest that the way an instrument is handled (i.e.
                        its
                        immediate iconographic neighborhood) is potentially of equal importance as the shape
                        of
                        the actual instrument, an insight that we will further expand on below.
                     </div>
                     
                     <div id="figure09" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 9. </div>Saliency maps for several stringed (subfigures (a) to (c)) and wind (subfigures
                           (d) to (f)) instruments.
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Error analysis: false positives</h2>
                     
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">In this section, we offer a qualitative discussion of the false positives from the
                        out-of-sample tests reported in the previous section, i.e. instances where the detectors
                        erroneously thought to have detected an instrument. This eagerness is a known problem
                        of
                        object detectors: a system that is trained to recognize "sheep" will be inclined to
                        see
                        "sheep" everywhere. Anecdotally, people have noted how misleading contextual cues
                        can
                        indeed be a confounding factor in image analysis. One blog post for instances noted
                        how
                        a major image labeling service tagged photographs of green fields with the "sheep"
                        label, although no sheep whatsoever were present in the images.<a class="noteRef" href="#d4e2471">[6]</a>
                        Eagerness-to-detect or over-association is therefore a clear first shortcoming of
                        this
                        method when applied in the wild, mainly because it was only trained in images that
                        actually contain musical instruments. Interestingly, the false positives come in
                        clusters that shed an interesting light on this issue. Below we list a representative
                        number of error clusters:
                     </div>
                     
                     <div id="figure10" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 10. </div>Examples of successful detections in IconArt for "stringed instruments”.
                        </div>
                     </div>
                     
                     <div id="figure11" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure11.png" rel="external"><img src="resources/images/figure11.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 11. </div>Anecdotal examples of false positive detection, divided in 7 interpretive clusters
                           (numbered a-g).
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">The above categorization illustrates that the false positives are rather insightful,
                        mainly because the absence of an instrument highlights the contextual clues that are
                        at
                        work. Of particular relevance is the observation that the iconography surrounding
                        children closely resembles that of instruments. This seems related to the intimate
                        and
                        caring body language of both the caretakers and musicians in such compositions. The
                        immediate iconographic neighborhood of children clearly reminds the detector of the
                        delicacy and reverence with which instruments are portrayed and presented in historical
                        artworks. This delicacy and intimacy in body language can be specifically related
                        to the
                        foregrounding of fingers, the prominent portrayal of which invariably triggers the
                        detector, also in the absence of children. Some of these phenomena invite closer
                        inspection by domain experts in music iconography and suggest that serial or panoramic
                        analyses are a worthwhile endeavour in this field, also from the point of view of
                        more
                        hermeneutically oriented scholars.
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Conclusions and future research</h1>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">In this paper, we have introduced MINERVA, to our knowledge the first sizable benchmark
                     dataset for the identification and detection of individual musical instruments in
                     unrestricted, digitized images from the realm of the visual arts. Our benchmark
                     experiments have highlighted the feasibility of a number of tasks but also, and perhaps
                     primarily, the significant challenges that state-of-the-art machine learning systems
                     are
                     still confronted with on this data, such as the "long-tail" of the instruments'
                     distribution and the staggering variance in depiction across the images in the dataset.
                     We
                     therefore hope that this work will inspire new (and much-needed) research in this
                     area. At
                     the end of this paper, we wish to formulate some advice and concerns in this respect.
                  </div>
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">One evident direction from future research is more advanced transfer learning, where
                     algorithms make more efficient use of the wealth of photorealistic data that is provided,
                     for instance, by MIMO [<a class="ref" href="#dolan2017">Dolan 2017</a>]. The main issue with the MIMO data in
                     this respect is that the bulk of these photographs are context-free (i.e. the instruments
                     are photographed in isolation, against a white or neutral background), which is almost
                     never the case in the artistic domain. Preliminary research demonstrated that this
                     a major
                     hurdle to established pretraining scenarios. Cascaded approaches, where instruments
                     are
                     detected first and only classified in a second stage might be a promising avenue here.
                  </div>
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">One crucial final remark is that AI has an amply attested tendency not only to be
                     sensitive to biases in the input data but also to amplify them [<a class="ref" href="#zou2018">Zou and Schiebinger 2018</a>].
                     Whereas the computational methods presented here have the potential to scale up
                     dramatically the scope of current research in music iconography, it also comes with
                     ideological dangers. The technology could further strengthen the bias on specific
                     canonical regions and periods in art history and lead the attention even further away
                     from
                     artistic and iconographic cultures that are already in specific need of reappraisal.
                     The
                     community will therefore have to think carefully about bias correction and mitigation.
                     Collecting training data in a diverse and inclusive manner, with ample attention for
                     resource-lower cultures should be a key strategy in future data collection campaigns.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">We wish to thank Remy Vandaele for the help with Cytomine and for the fruitful
                     discussions related to computer vision and object detection. Special thanks go out
                     to our
                     annotators and other (former) colleagues in the museums involved: Cedric Feys, Odile
                     Keromnes, Lies Van De Cappelle and Els Angenon. Our gratitude also goes out to Rodolphe
                     Bailly for his support and advice regarding MIMO. Finally, we wish to credit our former
                     project member dr. Ellen van Keer with the original idea of applying object detection
                     to
                     musical instruments. This project is generously funded by the Belgian Federal Research
                     Agency BELSPO under the BRAIN-be program.
                  </div>
                  
               </div>
               
               
               
               
               
               
               
            </div>
            
            
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e370"><span class="noteRef lang en">[1] See <a href="https://hosting.uantwerpen.be/insight/" onclick="window.open('https://hosting.uantwerpen.be/insight/'); return false" class="ref">https://hosting.uantwerpen.be/insight/</a>. This project is generously funded by
                     the Belgian Federal Research Agency BELSPO under the BRAIN-be program.</span></div>
               <div class="endnote" id="d4e526"><span class="noteRef lang en">[2] All code used in this paper is publicly available from this
                     repository: <a href="https://github.com/paintception/MINeRVA" onclick="window.open('https://github.com/paintception/MINeRVA'); return false" class="ref">https://github.com/paintception/MINeRVA</a>. Likewise, the MINERVA dataset can be
                     obtained from this DOI on Zenodo: 10.5281/zenodo.3732580.</span></div>
               <div class="endnote" id="d4e557"><span class="noteRef lang en">[3] <a href="https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1" onclick="window.open('https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1'); return false" class="ref">https://web.archive.org/save/https://www.flickr.com/groups/1991907@N24/?rb=1</a></span></div>
               <div class="endnote" id="d4e576"><span class="noteRef lang en">[4] 
                     <a href="https://web.archive.org/save/https://www.mimo-international.com/MIMO/" onclick="window.open('https://web.archive.org/save/https://www.mimo-international.com/MIMO/'); return false" class="ref">https://web.archive.org/save/https://www.mimo-international.com/MIMO/</a></span></div>
               <div class="endnote" id="d4e1673"><span class="noteRef lang en">[5] There is no officially published reference for RMSprop, but scholars
                     commonly refer to this lecture from Geoffrey Hinton and colleagues: <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" onclick="window.open('https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf'); return false" class="ref">https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></span></div>
               <div class="endnote" id="d4e2471"><span class="noteRef lang en">[6] <a href="https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep" onclick="window.open('https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep'); return false" class="ref">https://web.archive.org/save/https://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep</a></span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="arnold2019">
                     <!-- close -->Arnold and Tilton 2019</span> Arnold, T., and Tilton, L., “Distant viewing: analyzing large visual corpora.”<cite class="title italic">Digital Scholarship in the Humanities</cite>, 34 (2019), i3-i16.
               </div>
               <div class="bibl"><span class="ref" id="baldassarre2007">
                     <!-- close -->Baldassarre 2007</span> Baldassarre, A. “Quo vadis music iconography? The Repertoire International d'Iconographie Musicale
                  as a
                  case study”
                  <cite class="title italic">Fontes Artis Musicae</cite>, 54 (2007), 440-452.
               </div>
               <div class="bibl"><span class="ref" id="baldassarre2008">
                     <!-- close -->Baldassarre 2008</span> Baldassarre, A. “Music Iconography: What is it all about? Some remarks and considerations with a
                  selected bibliography”
                  <cite class="title italic">Ictus: Periódico do Programa de Pós-Graduação em Música da
                     UFBA</cite>, 9 (2008), 55-95.
               </div>
               <div class="bibl"><span class="ref" id="ballard1982">
                     <!-- close -->Ballard and Brown 1982</span> Ballard, D. H., and Christopher M.
                  Brown, C. M. <cite class="title italic">Computer Vision</cite>, Upper Saddle River
                  (1982).
               </div>
               <div class="bibl"><span class="ref" id="bell2019">
                     <!-- close -->Bell and Impett 2019</span> Bell, P., and Impett, L. “Ikonographie und Interaktion. Computergestützte Analyse von Posen in
                  Bildern der Heilsgeschichte”
                  <cite class="title italic">Das Mittelalter</cite>, 24 (2019): 31–53.
               </div>
               <div class="bibl"><span class="ref" id="boyarski2017">
                     <!-- close -->Boyarski et al. 2017</span> Mariusz Bojarski, Anna Choromanska,
                  Krzysztof Choromanski, Bernhard Firner, Larry J. Ackel, Urs Muller, Philip Yeres,
                  Karol
                  Zieba, “VisualBackProp: Efficient Visualization of CNNs for Autonomous
                  Driving”
                  <cite class="title italic">Proceedings of the 2018 IEEE International Conference on Robotics and
                     Automation (ICRA)</cite>, 2018, 4701-4708. DOI: 10.1109/ICRA.2018.8461053.
               </div>
               <div class="bibl"><span class="ref" id="buckley1998">
                     <!-- close -->Buckley 1998</span> Buckley, A. “Music
                  Iconography and the Semiotics of Visual Representation”
                  <cite class="title italic">Music in Art</cite>, 23 (1998), 5-10.
               </div>
               <div class="bibl"><span class="ref" id="crowley2014">
                     <!-- close -->Crowley and Zisserman 2014</span> Crowley, E., and Zisserman, A.
                  “The State of the Art: Object Retrieval in Paintings using
                  Discriminative Regions” In Valstar, M., French, A., and Pridmore, T. (eds),
                  <cite class="title italic">Proceedings of the British Machine Vision Conference</cite>,
                  Nottingham (2014), s.p.
               </div>
               <div class="bibl"><span class="ref" id="dolan2017">
                     <!-- close -->Dolan 2017</span> Dolan, E. I. “Review: MIMO:
                  Musical Instrument Museums Online”
                  <cite class="title italic">Journal of the American Musicological Society</cite>, 70 (2017):
                  555-565.
               </div>
               <div class="bibl"><span class="ref" id="everingham2010">
                     <!-- close -->Everingham et al. 2010</span> Everingham, M., Van Gool, L.,
                  Williams, C. K., Winn, J., and Zisserman, A. “The Pascal visual object
                  classes (VOC) challenge” In <cite class="title italic">International journal of
                     computer vision</cite>, 88(2) (2010): 303–338.
               </div>
               <div class="bibl"><span class="ref" id="gonthier2018">
                     <!-- close -->Gonthier et al. 2018</span> Gonthier, N., Gousseau, Y., Ladjal,
                  S. and Bonfait, O. “Weakly supervised object detection in
                  artworks” In <cite class="title italic">Proceedings of the European Conference on
                     Computer Vision (ECCV)</cite> (2018): 692–709.
               </div>
               <div class="bibl"><span class="ref" id="green2013">
                     <!-- close -->Green and Ferguson 2013</span> Green, A., and Ferguson, S. “RIDIM: Cataloguing music iconography since 1971”
                  <cite class="title italic">Fontes Artis Musicae</cite>, 60 (2013), 1-8.
               </div>
               <div class="bibl"><span class="ref" id="he2016">
                     <!-- close -->He et al. 2016</span> He, K., Zhang, X., Ren, S., and Sun, J. “Deep residual learning for image recognition.”In <cite class="title italic">Proceedings of the IEEE conference on computer vision and pattern
                     recognition</cite>, pages 770–778, 2010.
               </div>
               <div class="bibl"><span class="ref" id="hertzmann2018">
                     <!-- close -->Hertzmann 2018</span> Hertzmann, A. “Can
                  Computers Create Art?” Arts, 7 (2018) doi:10.3390/arts7020018.
               </div>
               <div class="bibl"><span class="ref" id="hockey2004">
                     <!-- close -->Hockey 2004</span> Hockey, S. “A History of
                  Humanities Computing.”In S. Schreibman, R. Siemens, and J. Unsworth (eds.),
                  <cite class="title italic">A Companion to Digital Humanities</cite>, Oxford (2004), pp.
                  3–19.
               </div>
               <div class="bibl"><span class="ref" id="huang2017">
                     <!-- close -->Huang et al. 2017</span> Huang, G., Zhuang, L., Van Der Maaten,
                  L., and Weinberger, K. “Densely connected convolutional
                  networks”In <cite class="title italic">Proceedings of the IEEE conference on computer
                     vision and pattern recognition</cite> (2017), pp. 4700– 4708.
               </div>
               <div class="bibl"><span class="ref" id="kingma2014">
                     <!-- close -->Kingma and Ba 2014</span> Kingma, D. P., and Ba, J. “A method for stochastic optimization”
                  <cite class="title italic">arXiv preprint arXiv:1412.6980</cite>, 2014.
               </div>
               <div class="bibl"><span class="ref" id="lecun2015">
                     <!-- close -->LeCun et al. 2015</span> LeCun, J., Bengio, Y., and Hinton, G.,
                  “Deep Learning”
                  <cite class="title italic">Nature</cite>, 521 (2015): 436–444.
               </div>
               <div class="bibl"><span class="ref" id="lin2014">
                     <!-- close -->Lin et al. 2014</span> Lin T.-Y., Maire, M., Belongie, S., Hays, J.,
                  Perona, P., Ramanan, D., Dollar, P and Zitnick, C. L. “Microsoft COCO:
                  Common objects in context” In <cite class="title italic">European conference on
                     computer vision</cite>, pages 740–755. Springer, 2014.
               </div>
               <div class="bibl"><span class="ref" id="marée2016">
                     <!-- close -->Marée et al. 2016</span> Marée, R., Rollus, L. Stévens, B.,
                  Hoyoux, R., Louppe, G., Vandaele, R., Begon, J., Kainz, P., Geurts, P., and Wehenkel
                  “Collaborative analysis of multi-gigapixel imaging data using
                  Cytomine”
                  <cite class="title italic">Bioinformatics</cite>, 32 (2016): 1395–1401.
               </div>
               <div class="bibl"><span class="ref" id="mensink2014">
                     <!-- close -->Mensink and Van Gemert 2014</span> Mensink, T. and Van Gemert,
                  J. “The Rijksmuseum challenge: Museum-centered visual
                  recognition” In <cite class="title italic">Proceedings of International Conference on
                     Multimedia Retrieval</cite>, page 451. ACM, 2014.
               </div>
               <div class="bibl"><span class="ref" id="pedregosa2011">
                     <!-- close -->Pedregosa et al. 2011</span> Pedregosa, F., Varoquaux, G.,
                  Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
                  Weiss,
                  R. and Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot,
                  M.,
                  and Duchesnay, E. “Scikit-learn: Machine Learning in Python”
                  <cite class="title italic">Journal of Machine Learning Research</cite>, 12 (2011):
                  2825-2830.
               </div>
               <div class="bibl"><span class="ref" id="redmon2018">
                     <!-- close -->Redmon and Farhadi 2018</span> Redmon, J. and Farhadi, A. “Yolov3: An incremental improvement”
                  <cite class="title italic">arXiv preprint arXiv:1804.02767</cite>, 2018.
               </div>
               <div class="bibl"><span class="ref" id="ren2017">
                     <!-- close -->Ren et al. 2017</span> S. Ren, K. He, R. Girshick, and J. Sun,
                  “Faster R-CNN: Towards Real-Time Object Detection with Region
                  Proposal Networks”
                  <cite class="title italic">IEEE Transactions on Pattern Analysis and Machine
                     Intelligence</cite>, 39 (2017), 1137-1149.
               </div>
               <div class="bibl"><span class="ref" id="russakovsky2015">
                     <!-- close -->Russakovsky et al. 2015</span> Russakovsky, O., Deng, J.,
                  Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, K., Khosla, A., Bernstein,
                  M. et al. “Imagenet large scale visual recognition challenge”
                  <cite class="title italic">International journal of computer vision</cite>, 115(3) (2015),
                  211–252.
               </div>
               <div class="bibl"><span class="ref" id="sabatelli2018">
                     <!-- close -->Sabatelli et al. 2018</span> Sabatelli, M., Kestemont, M.,
                  Daelemans, W. and Geurts, P. “Deep transfer learning for art
                  classification problems” In <cite class="title italic">Proceedings of the European
                     Conference on Computer Vision (ECCV)</cite>, pages 631–646, 2018.
               </div>
               <div class="bibl"><span class="ref" id="schmidhuber2015">
                     <!-- close -->Schmidhuber 2015</span> , Schmidhuber, J. “Deep Learning in Neural Networks: An Overview”
                  <cite class="title italic">Neural Networks</cite>, 61 (2015), 85-117.
               </div>
               <div class="bibl"><span class="ref" id="seguin2018">
                     <!-- close -->Seguin 2018</span> Seguin, B. “The Replica
                  Project: Building a visual search engine for art historians” XRDS: Crossroads,
                  <cite class="title italic">The ACM Magazine for Students - Computers and Art</cite>, 24
                  (2018), 24-29.
               </div>
               <div class="bibl"><span class="ref" id="simonyan2014">
                     <!-- close -->Simonyan and Zisserman 2014</span> Simonyan, K. and Zisserman,
                  A. Very deep convolutional networks for large-scale image recognition. <cite class="title italic">arXiv preprint arXiv:1409.1556</cite>, 2014.
               </div>
               <div class="bibl"><span class="ref" id="strezoski2017">
                     <!-- close -->Strezoski and Worring 2017</span> Strezoski, G. and Worring,
                  M. “Omniart: multi-task deep learning for artistic data
                  analysis”
                  <cite class="title italic">arXiv preprint arXiv:1708.00684</cite>, 2017.
               </div>
               <div class="bibl"><span class="ref" id="szegedy2015">
                     <!-- close -->Szegedy et al. 2015</span> Szegedy, C., Liu, W., Jia, Y.,
                  Sermanet, P., Reed, S., Anguelov, S., Erhan, D., Vanhoucke, V., and Rabinovich, A.
                  “Going deeper with convolutions” In <cite class="title italic">Proceedings
                     of the IEEE conference on computer vision and pattern recognition</cite> (2015), pp.
                  1–9.
               </div>
               <div class="bibl"><span class="ref" id="van2015">
                     <!-- close -->Van et al. 2015</span> Van Noord, N., Hendriks, E., and Postma, E.,
                  “Toward Discovery of the Artist's Style: Learning to recognize
                  artists by their artworks”
                  <cite class="title italic">IEEE Signal Processing Magazine</cite>, 32 (2015), 46-54.
               </div>
               <div class="bibl"><span class="ref" id="wevers2020">
                     <!-- close -->Wevers and Smits 2020</span> Wevers M., and Smits, T. “The visual digital turn: Using neural networks to study historical
                  images”
                  <cite class="title italic">Digital Scholarship in the Humanities</cite>, 35 (2020),
                  194–207.
               </div>
               <div class="bibl"><span class="ref" id="xiang2014">
                     <!-- close -->Xiang et al. 2014</span> Xiang, Y., Mottaghi, R., and Savarese, S.
                  “Beyond pascal: A benchmark for 3d object detection in the
                  wild” In <cite class="title italic">IEEE Winter Conference on Applications of Computer
                     Vision</cite>, pages 75–82. IEEE, 2014.
               </div>
               <div class="bibl"><span class="ref" id="zou2018">
                     <!-- close -->Zou and Schiebinger 2018</span> Zou, J., and Schiebinger, L. “AI can be sexist and racist — it's time to make it fair”
                  <cite class="title italic">Nature</cite>, 559 (2018): 324-326.
               </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>