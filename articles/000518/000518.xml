<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Automated Visual Content Analysis for Film Studies:
               Current Status and Challenges</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Ralph <dhq:family>Ewerth</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Leibniz Information Centre of Science and Technology (TIB),
                  Hannover, Germany; L3S Research Center, Leibniz University Hannover,
                  Germany</dhq:affiliation>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Julian <dhq:family>Sittel</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Institute for Film, Theatre and Empirical Cultural Studies,
                  University of Mainz, Germany</dhq:affiliation>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Kader <dhq:family>Pustu-Iren</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Leibniz Information Centre of Science and Technology (TIB),
                  Hannover, Germany</dhq:affiliation>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Roman <dhq:family>Mauer</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Institute for Film, Theatre and Empirical Cultural Studies,
                  University of Mainz, Germany</dhq:affiliation>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Oksana <dhq:family>Bulgakowa</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Institute for Film, Theatre and Empirical Cultural Studies,
                  University of Mainz, Germany</dhq:affiliation>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association of Computers and the Humanities</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000518</idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date/>
            <dhq:articleType>article</dhq:articleType>
            <availability>
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change who="2020-10-30" when="jmurel">Created file</change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>Lots of approaches for automated video analysis have been suggested since the
               1990ies, which have the potential to support quantitative and qualitative analysis in
               film studies. However, software solutions for the scholarly study of film that
               utilise video analysis algorithms are still relatively rare. In this paper, we aim to
               provide an overview of related work in this field, review current developments in
               computer vision, compare machine and human performance for some visual recognition
               tasks, and outline the requirements for video analysis software that would optimally
               support scholars of film studies.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>


         <div xml:id="section01">
            <head>1 Introduction</head>
            <p>In contrast to the field of computer-assisted research in the arts that has been
               established for several years <ptr target="#anitha2013"/>
               <ptr target="#johnson2008"/>
               <ptr target="#klein2014"/>
               <ptr target="#resig2014"/>, there is a need to catch up in scientific approaches to
               film (represented in the fields of New Film History and Stylometry). An important
               reason is the lack of practical software solutions available to date and the
               incompatibility of quantitative research designs with existing methodologies for film
               studies. In this context, some researchers criticise above all the appropriation of a
               technicistic, unrelated mission statement, which advocates of digital humanities
               apply to their own subject following other principles <ptr target="#liu2012"/>
               <ptr target="#missomelius2014"/>. However, more recent research <ptr
                  target="#heftberger2016"/>
               <ptr target="#sittel2017"/> has shown that qualitative and quantitative analysis are
               by no means mutually exclusive, but can be integrated in order to enrich film studies
               with new impulses.</p>
            <p>The statistical film analysis developed by the physicist Salt thus holds the
               potential of a methodological guideline for quantifying filmic characteristics <ptr
                  target="#salt2006"/>
               <ptr target="#salt2009"/>. This methodology focuses on quantifiable factors in the
               formal structure of a film such as camera shot length, which is considered an
               objective unit because it can be measured over time. The various forms of camera
               usage and movement (such as tracking shots, pans and tilts, camera distance, i.e.,
               shot size) as well as other techniques (such as zoom-in and -out or the use of a
               camera crane) are also relevant for quantification. Casting this set of techniques as
               measuring instruments, it is possible to obtain data that scientists can relate to
               verifiable criteria in terms of film production history and to formulate hypotheses
               that allow conclusions to be drawn about the formal stylistic development of the
               selected films. To this end, Salt's concept allows for complete traceability of the
               measurement results and thus also of the numerical values to a theory set.</p>
            <p>The concept was criticized for its reductionism <ptr target="#fluckiger2011"/>, which
               prevents it from being connected to the qualitative research methods that dominate
               film studies. However, research in digital humanities has shown that quantitative
               parameters such as shot length are a suitable foundation for various analytical tools
               when it comes to the qualitative investigation of data (<ref target="#tsivian2008"
                  >Tsivian 2008</ref>; <ref target="#buckland2009">Buckland 2009</ref>; many
               others). In this way, quantitative research according to Salt makes it possible to
               validate stylistic changes in the work of emigrated European directors due to
               technical opportunities of the American studio system, or even to collect the average
               shot lengths of numerous productions from the beginning of film history to the
               present. Such research allows researchers to draw conclusions about the progressive
               acceleration of editing and thus provides information about the development of film
               technology and changes in our viewing habits. These questions concerning stylistic
               research in film studies cannot be examined without a corresponding quantitative
               research design, although Salt's concept remains too inflexible for broader
               application.</p>
            <p>In this context, <ref target="#korte2010">Korte's work (2010)</ref> is regarded as
               pioneering (especially in German-speaking countries) in transferring quantitative
               methods into the framework of a qualitative analysis immanent in the work. An example
               for this is Rodenberg's analysis of Zabriskie Point (directed by Michelangelo
               Antonioni, 1970) in Korte's introduction to systematic film analysis (2010), which
               graphically depicts the stylistic structure of the film in an innovative way. Thus,
               for a detailed analysis Rodenberg visualises the adaption and alignment of editing
               rhythms to the characterisations of the persons in the narrative, and makes the
               alignment of music and narration comprehensible using diagrams. <ref
                  target="#heftberger2016">Heftberger (2016)</ref>, for example, combines the
               analysis of historical sources such as editing diagrams and tables by the Russian
               director Vertov with computer-aided representations of digital humanities in order to
               provide insights into the filmmaker's working methods. Heftberger starts with copies
               of Vertov's films, which were analysed for structural features using the annotation
               software ANVIL <ptr target="#kipp2001"/>, but also for dissolves or condition-related
               factors (e.g. markings in the film rolls or damage to the film). Within the framework
               of single and overall analyses, the data serve to elucidate the director’s
               intentions.</p>
            <figure>
               <head>Increase and decrease of shot lengths in Antonioni’s Blow Up (1966) using
                  Videana.</head>
               <graphic url="resources/images/figure01.png"/>
            </figure>
            <p><ref target="#sittel2016">Sittel (2016)</ref> investigated the principle of increase
               and decrease of shot lengths in Michelangelo Antonioni’s <title rend="italic">Blow
                  Up</title> (1966). The visualization in Figure 1 was created during this study
               with the video analysis software Videana <ptr target="#ewerth2009"/> and gives an
               insight into this pattern. This technique, which is increasingly used in film
               editing, represents a structuring feature of the second half of the film and can be
               interpreted as a message with regard to the film content. </p>
            <figure>
               <head>Average shot length of all Antonioni films in chronological order.</head>
               <graphic url="resources/images/figure02.png"/>
            </figure>
            <p>Using Salt's paradigm as a guideline, an analysis of all Antonioni films makes it
               possible to identify a clear change in style based on shot sizes and camera
               movements, which extends existing, non-qualitative approaches in a differentiated
               way. Figure 2 shows that the average shot length becomes shorter across nearly all
               films. To this end, a principle of systematic camera movement (longer shots) and thus
               an abandonment of montage gradually gives way to the increasing use of editing.</p>
            <p>Research efforts like this benefit from automated computer-based methods that measure
               basic features of filmic structure. Similar to former work <ptr target="#estrada2017"
               />, we present a comprehensive survey of related software tools for video annotation,
               but particularly focus on methods for visual content analysis for film studies.
               First, we examine major software tools for video analysis with a focus on automated
               analysis algorithms and discuss their advantages and drawbacks. In addition, related
               work that applies automated video analysis to film studies is discussed. Moreover, we
               summarise current progress in computer vision and visual content analysis with a
               focus on deep learning methods. Besides, a comparison of machine vs. human
               performance in annotation tasks that are relevant for video content analysis is
               provided. Finally, we discuss future desirable functionalities for automated analysis
               in software tools for film studies.</p>
            <p>The remainder of the paper is structured as follows. <ref target="#section02">Section 2</ref> reviews existing
               software tools and algorithms for quantitative approaches in film analysis. <ref target="#section03">Section 3</ref>
               discusses recent advancements in the field of video analysis and computer vision. A
               comparison of human and machine performance in different annotation tasks is provided
               in <ref target="#section04">Section 4</ref>. <ref target="#section05">Section 5</ref> describes requirements for software tools in scholarly film
               studies and outlines areas for future work.</p>
         </div>
         <div xml:id="section02">
            <head>2 Software Tools and Algorithms for Quantitative Film Studies </head>
            <p>Researchers who want to utilise quantitative strategies to analyse film as presented
               in Figure 1 and 2 usually have to evaluate larger films or video corpora. However,
               existing software tools so far, with a few exceptions, require a high degree of
               manual annotation. As a consequence, many current film productions are difficult to
               evaluate due to ever shorter shot lengths. This section provides an overview of
               software solutions for film studies and their degree of automation in terms of
               capturing basic filmic features. While there are numerous existing annotation and
               video analysis tools, the focus lies on ready-to-use software applications most
               suitable for quantitative film studies. An overview of the functionalities provided
               by the selected applications and their current status of availability is presented in
               Table 1. We also distinguish between application areas in Table 1, since not all of
               the tools were originally proposed for scholarly film studies as targeted here. </p>
            <p>Table 1. Overview of software applications for quantitative approaches in film
               analysis, characterised by their degree of automation regarding video content
               analysis tasks. While <q>m</q> denotes manual annotation, <q>a</q> refers to
               automated annotation. </p>
            <table>
               <row role="data">
                  <cell>Tool</cell>
                  <cell>Application Area</cell>
                  <cell>Availability</cell>
                  <cell>Shot change detection</cell>
                  <cell>Camera motion </cell>
                  <cell>Video OCR</cell>
                  <cell>Face detection</cell>
                  <cell>Colour analysis</cell>
                  <cell>Annotation level</cell>
                  <cell>Visualisations</cell>
               </row>
               <row role="data">
                  <cell>Advene </cell>
                  <cell>
                     <p>Film studies/</p>
                     <p>teaching platform</p>
                  </cell>
                  <cell>
                     <p>Desktop App </p>
                     <p>(free)</p>
                  </cell>
                  <cell>a</cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>ROI</cell>
                  <cell>
                     <p>Timelines for shots &amp;</p>
                     <p>annotation </p>
                  </cell>
               </row>
               <row role="data">
                  <cell>ANVIL</cell>
                  <cell>
                     <p>Psycholinguistics/</p>
                     <p>social sciences</p>
                  </cell>
                  <cell>
                     <p>Desktop App </p>
                     <p>(free)</p>
                  </cell>
                  <cell>m</cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>Shot</cell>
                  <cell>
                     <p>Timelines for annotation tiers &amp; </p>
                     <p>speech track</p>
                  </cell>
               </row>
               <row role="data">
                  <cell>Cinemetrics</cell>
                  <cell>
                     <p>Film studies</p>
                  </cell>
                  <cell>Web-based crowd-sourcing platform</cell>
                  <cell>m</cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>Shot</cell>
                  <cell>Cut frequency diagram</cell>
               </row>
               <row role="data">
                  <cell>ELAN</cell>
                  <cell>
                     <p>Psycholinguistics/</p>
                     <p>social sciences</p>
                  </cell>
                  <cell>
                     <p>Desktop App </p>
                     <p>(free)</p>
                  </cell>
                  <cell>m</cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>Shot</cell>
                  <cell>
                     <p>Timelines for annotation tiers &amp;</p>
                     <p>speech track segments </p>
                  </cell>
               </row>
               <row role="data">
                  <cell>Ligne de Temps</cell>
                  <cell>Film studies</cell>
                  <cell>
                     <p>Desktop App </p>
                     <p>(free)</p>
                  </cell>
                  <cell>a</cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>m</cell>
                  <cell>Shot</cell>
                  <cell>Timeline for cuts</cell>
               </row>
               <row role="data">
                  <cell>Media-thread</cell>
                  <cell>Teaching platform</cell>
                  <cell>
                     <p>Web App</p>
                     <p>(source code available)</p>
                  </cell>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell/>
                  <cell>ROI</cell>
                  <cell>Hyper video annotations</cell>
               </row>
               <row role="data">
                  <cell>VIAN</cell>
                  <cell>
                     <p>Film studies</p>
                     <p> (colour analysis)</p>
                  </cell>
                  <cell>
                     <p>Desktop App<note> background-character/figure segmentation</note>
                     </p>
                     <p>(not publicly available yet)</p>
                  </cell>
                  <cell>a</cell>
                  <cell/>
                  <cell/>
                  <cell>a¹</cell>
                  <cell>a</cell>
                  <cell>ROI</cell>
                  <cell>Timelines for shots &amp; annotations, colour schemes view, screenshot
                     manager </cell>
               </row>
               <row role="data">
                  <cell>Videana</cell>
                  <cell>
                     <p>Media/</p>
                     <p>film studies</p>
                  </cell>
                  <cell>
                     <p>Desktop App </p>
                     <p>(on request until 2012)</p>
                  </cell>
                  <cell>a</cell>
                  <cell>a</cell>
                  <cell>a</cell>
                  <cell>a</cell>
                  <cell>a</cell>
                  <cell>ROI</cell>
                  <cell>
                     <p>Timelines of detections annotations &amp; cuts, </p>
                     <p>cut frequency diagram, shot list</p>
                  </cell>
               </row>
            </table>
            <div>
               <head>2.1 Cinemetrics</head>
               <p>In the Cinemetrics project <ptr target="#tsivian2009"/>, Yuri and Gunnar Tsivian
                  took up Salt’s methodology and used it for the first time as conceptual guidelines
                  for the implementation of digital analytical instruments, which are freely
                  available as a Web-based platform since 2005.<note><ref
                        target="http://www.cinemetrics.lv/">http://www.cinemetrics.lv/</ref></note>
                  The tool allows the user to synchronously view and annotate video material, or to
                  systematically comb through AVI video files. In the advanced mode, customized
                  annotation criteria can be defined in addition to basic features (e.g. frequency
                  and length of shots). The data sets obtained are then collected within an online
                  database according to the principle of crowdsourcing. With metadata for more than
                  50,000 films to date, it acts as a comprehensive research data archive that
                  documents the development and history of film style. Cinemetrics is the only
                  platform based on Web 2.0 principles that consistently aggregates film data and
                  makes it publicly accessible. However, the analysis of film data is not
                  systematic. Moreover, Cinemetrics relies exclusively on the manual acquisition of
                  data such as shot changes, camera distances, or camera movements. The accurate
                  evaluation of video material such as feature films therefore requires an effort of
                  several hours and is unsuitable for broader studies. Nonetheless, the platform
                  enjoys a high level of visitor traffic. Between 2010 and 2013, the number of users
                  almost doubled (2010: 4,500 clicks per day, 2013: 8,326). The program is regularly
                  used in seminars at the Universities of Chicago, Amsterdam, Taiwan and at the
                  Johannes Gutenberg University of Mainz.</p>
            </div>
            <div>
               <head>2.2 ANVIL, ELAN</head>
               <p>ANVIL <note>Available at <ref target="http://www.anvil-software.org/"
                        >http://www.anvil-software.org/</ref></note>
                  <ptr target="#kipp2001"/> and ELAN (EUDICO Linguistic Annotator)<note>Available at
                        <ref target="https://tla.mpi.nl/tools/tla-tools/elan/"
                        >https://tla.mpi.nl/tools/tla-tools/elan/</ref></note>
                  <ptr target="#sloetjes2008"/> are visual annotation tools, which were originally
                  designed for psycholinguistics and gesture research. They are suitable for the
                  differentiated graphical representation of editing rhythms or for the annotation
                  of previously defined structural features at the shot and sequence level, though
                  it usually requires the export of the data to another statistical software or
                  Microsoft Excel. In contrast to Cinemetrics, ANVIL and ELAN allow the user to
                  directly interact with the video material. They offer a much greater
                  methodological scope, especially through the option of adding several feature
                  dimensions to the video material in the form of tracks. Both systems work
                  according to a similar principle, to which the video segments or the collected
                  shots can be viewed and annotated with metadata. Annotations can also be arranged
                  hierarchically by means of multiple layers which are called tiers. Thus,
                  annotations can be cross-referenced to other annotations or to corresponding shots
                  or video segments, making the programs particularly suitable for a fine-grained
                  analysis of the structure of individual works. However, if several parameters are
                  to be recorded during an evaluation run, repeated viewing of a film is required in
                  order to label the individual tracks with the respective characteristics and
                  features.</p>
               <figure>
                  <head>Screenshot of ANVIL software.</head>
                  <graphic url="resources/images/figure03.png"/>
               </figure>
               <figure>
                  <head>Screenshot of ELAN software.</head>
                  <graphic url="resources/images/figure04.png"/>
               </figure>
            </div>
            <div>
               <head>2.3 Ligne de temps</head>
               <p>The analysis tool Ligne de temps<note><ref
                        target="http://www.iri.centrepompidou.fr/outils/lignes-de-temps-2/"
                        >http://www.iri.centrepompidou.fr/outils/lignes-de-temps-2/</ref></note> was
                  developed between 2007 and 2011 by the Institut de recherche et d’innovation du
                  Centre Pompidou. The tool provides a graphical timeline-based representation of
                  the material and allows for selecting temporal segments in order to annotate
                  different types of modality (video, audio, text) of the corresponding sequence in
                  the movie, or add information in the form of images or external links. Moreover,
                  it is possible to generate colour-coded annotations aligned with the shots by
                  choosing from a range of available RGB colour values. This function can be
                  implicitly used for colour analysis. However, a single colour cannot represent a
                  holistic image or even an entire shot. Ligne de temps enables the automated
                  detection of shot boundaries in the video, as only few of the presented tools do.
                  But there is no information available about the algorithm used and its performance
                  on benchmark data sets.</p>
               <figure>
                  <head>Screenshot of Ligne de Temps.</head>
                  <graphic url="resources/images/figure05.png"/>
               </figure>
            </div>
            <div>
               <head>2.4 Advene, Mediathread</head>
               <p>Advene (Annotate Digital Video, Exchange on the NEt) <ptr target="#aubert2005"/>
                  is an ongoing project for the annotation of digital videos that also provides a
                  format to share annotations. The tool has been developed since 2002 and is freely
                  available as a cross-platform desktop application.<note><ref
                        target="https://www.advene.org">www.advene.org</ref></note> It provides a
                  broader number of functionalities compared with the former tools. In particular,
                  the tool enables textual as well as graphical annotations to augment the video and
                  also provides automatically generated thumbnails for each shot. Moreover, it is
                  possible to edit and visualise hypervideos consisting of both the annotations and
                  the video. Target groups are film scholars, teachers and students who want to
                  exchange multimedia comments and analyses about videos such as movies,
                  conferences, or courses. The tool has also been used for reflexive interviews of
                  museum visitors, or with regard to on-demand video providers and movie-related
                  social networks. However, the automatic analysis options that Advene provides are
                  restricted to shot boundary detection and temporal segmentation of audio tracks. </p>
               <p>Mediathread<note><ref target="https://mediathread.info"
                        >https://mediathread.info</ref></note> was developed by Columbia
                  University’s Center for Teaching and Learning (CTL)<note><ref
                        target="https://ctl.columbia.edu/">https://ctl.columbia.edu/</ref></note>
                  and first launched in 2010. It is a web application for multimedia annotations
                  enabling collaboration on video and image analysis. Similar to Advene, Mediathread
                  primarily serves as a platform for collaboratively working and sharing annotations
                  for multimedia and is therefore actively used in classroom environments at various
                  universities, also including courses on film studies. However, it does not provide
                  automated analysis capabilities such as shot detection. In addition, film studies
                  researches wanting to use the web application need a certain degree of expertise
                  to individually deploy the openly available source code. </p>
               <figure>
                  <head>Screenshot of Advene.</head>
                  <graphic url="resources/images/figure06.png"/>
               </figure>
            </div>
            <div>
               <head>2.5 Videana</head>
               <p>The video analysis software Videana was developed at the University of Marburg
                     <ptr target="#ewerth2009b"/>. Videana is one of the few software tools to offer
                  more than simple analysis functions such as the detection of shot changes <ptr
                     target="#ewerth2004b"/>
                  <ptr target="#ewerth2009a"/>. For example, the software integrates algorithms for
                  text detection <ptr target="#gllavata2004a"/>, video OCR <ptr
                     target="#gllavata2004b"/>, estimation of camera motion <ptr
                     target="#ewerth2004a"/> and of object motion <ptr target="#ewerth2007a"/>, and
                  face detection <ptr target="#viola2004"/>. Further functionalities, which are
                  however not part of the standard version, are the recognition of dominant colour
                  values, person recognition and indexing <ptr target="#ewerth2007b"/>, or the
                  temporal segmentation of the soundtrack. The many available features and the
                  possibility to combine them allow for the flexible formulation of complex research
                  hypotheses and their empirical verification. For example, Videana was used for a
                  media study to investigate user behaviour in Google Earth Tours <ptr
                     target="#abend2011"/>
                  <ptr target="#abend2012"/>. However, the software has not been updated since 2012
                  and therefore does not rely on current state-of-the-art methods in video analysis.
                  Another drawback of the tool is the lack of enhanced visualizations that go beyond
                  simple cut frequency diagrams and event timelines. Finally, the software is not
                  usable through a Web browser, but only available as a desktop software.</p>
               <figure>
                  <head>Screenshot of the main window of Videana (provided in <ref
                        target="#ewerth2009b">Ewerth et al. 2009</ref>).</head>
                  <graphic url="resources/images/figure07.png"/>
               </figure>
            </div>
            <div>
               <head>2.6 VIAN</head>
               <p>Within the project <title rend="quotes">Film Colors - Bridging the Gap Between
                     Technology and Aesthetics</title><note><ref target="https://filmcolors.org/"
                        >https://filmcolors.org/</ref></note> at the University of Zurich, the tool
                  VIAN <ptr target="#fluckiger2017"/>
                  <ptr target="#halter2019"/> for video analysis and annotation is currently being
                  developed with a focus on film colour patterns. In comparison to general-purpose
                  annotation tools like ELAN, VIAN particularly addresses aesthetic analyses of full
                  feature films. The tool is also planned to allow a variety of (semi)-automatic
                  tools for the analysis and visualisation of film colours based on computer vision
                  and deep learning methods such as figure-ground separation and extraction of the
                  corresponding colour schemes. Although the tool is not released yet (announced to
                  be open source), VIAN seems to be a very promising tool with regard to
                  state-of-the-art visual content analysis methods.</p>
               <figure/>
               <p>Figure 8. Screenshot of VIAN temporal segmentation and screenshot manager.</p>
            </div>
            <div>
               <head>2.7 Other Tools and Approaches</head>
               <p>There are some other tools that offer video and film analysis and (to a lesser
                  extent) provide functions similar to the previously introduced applications for
                  automatic film analysis. The Semantic Annotation Tool (SAT) is launched in the
                  context of the Media Ecology Project (MEP) (<ref
                     target="http://mediaecology.dartmouth.edu/sat/"
                     >http://mediaecology.dartmouth.edu/sat/</ref>). It allows for the annotation
                  and sharing of videos on the Web in free-text form, by a controlled set of tags,
                  or polygonal regions in a frame. It targets classroom as well as research
                  environments. The tool itself does not provide (integrated) quantitative measures.
                  However, it can be fed with external machine-generated metadata (by automated
                  video analysis methods). The Distant Viewing Toolkit<note><ref
                        target="https://www.distantviewing.org/labs/"
                        >https://www.distantviewing.org/labs/</ref></note> is a python package that
                  provides computational analysis and visualisation methods for video collections.
                  The software extracts and visualises semantic metadata from videos using standard
                  computer vision methods as well as exploring more high-level patterns such as
                  screen time per character. Another project is eFilms<note><ref
                        target="http://efilms.ushmm.org">http://efilms.ushmm.org</ref></note> that
                  provides a web-based film player that is supplemented with contextual meta
                  information such as date, geolocation or visual events for the footage. Its main
                  use case is the contextualization of historical footage of the Nazi era. The
                  project also provides a film annotation editor to insert the context annotations.
                  However, as for the aforementioned projects as well, the source code needs to be
                  deployed first and thus is no off-the-shelf and ready-to-use solution for film
                  scholars. Furthermore, the SAT and eFilms tools only offer manual annotation.</p>
               <p>Many other works exist that deal with analysis and visualisation of certain filmic
                  aspects. Some early works by Adams et al. transfer concepts of film grammar to
                  computational measures for automated film analysis. <ref target="#adams2000">Adams
                     and Dorai (2000)</ref> introduce a computational measure of movie tempo based
                  on its filmic definition and use it to automatically extract dramatic sections and
                  events from film. Furthermore, <ref target="#adams2001">Adams et al. (2001)</ref>
                  present a computational model for extracting film rhythm by deriving classes for
                  different motion characteristics in order to identify narrative structures and
                  dramatic progression. Another work deals with the extraction of narrative act
                  boundaries, in particular the 3-act-story telling paradigm using a probabilistic
                  model <ptr target="#adams2005"/>. </p>
               <p><ref target="#pause2016">Pause and Walkowski (2016)</ref> address the
                  characterization of dominant colours in film and discuss the limitations of the
                  k-means clustering approach. Furthermore, they propose to proceed according to
                  Itten's method of seven colour contrasts <ref target="#pause1961">(1961)</ref> and
                  outline how it can be implemented algorithmically. <ref target="#burghardt2016"
                     >Burghardt et al. (2016)</ref> present a system that automatically extracts
                  colour and language information using k-means clustering as well as subtitles and
                  propose an interactive visualisation. <ref target="#hoyt2014">Hoyt et al.
                     (2014)</ref> propose a tool to visualise the relationships between characters
                  in a movie. <ref target="#john2017">John et al. (2017)</ref> present a visual
                  analytics approach (for the analysis of single or a set of videos) that combines
                  automatic data analysis and visualisation. The interface supports the
                  representation of so-called semantic frames, simple keywords, hierarchical
                  annotations, as well as keywords for categories and similarities. Hohman et al
                  (2017) propose a method to explore individual videos (entertainment, series) on
                  the basis of colour information (dominant colour, etc.) as well as texts from
                  dialogues and evaluate the approach on the basis of two use cases for the series
                     <title rend="italic">Game of Thrones</title>. </p>
               <p><ref target="#tseng2013b">Tseng (2013b)</ref> provides an analysis of plot
                  cohesion in film by tracking film elements such as characters, objects, settings,
                  and character action. Furthermore, <ref target="#tseng2013a">Tseng (2013a)</ref>
                  distinguishes basic narrative types in visual images by interconnecting salient
                  people, objects and settings within single and across sequential images. <ref
                     target="#bateman2014">Bateman (2014)</ref> reviews empirical, quantitative
                  approaches to the analysis of films and, moreover, suggests employing discourse
                  semantics for more systematic interpretative analysis in order to overcome the
                  difficulty of relating particular technical film features to qualitative
                  interpretations. </p>
            </div>
         </div>
         <div xml:id="section03">
            <head>3 Current Developments in Computer Vision and Video Analysis</head>
            <p>Apart<note>Ich finde der gelöschte Absatz passt als Einleitung nicht. Am Ende wird ja
                  eh noch mal erwähnt das CNNs in allen CV-Bereichen den State-of-the-Art
                  erreichen.</note> from analysing film style (shot and scene segmentation, use of
               camera motion and colours, etc.), film studies are also concerned with question(s)
               such as: <q>Who (or what) did what, when, and where?.</q> To answer such questions,
               algorithms for recognising persons, location and time of shots, etc. are required.
               There are some state-of-the-art approaches that basically target these questions
               (e.g., visual concept detection, geolocation and date estimation of images) and might
               be applicable to film style and content analysis – at least in future work.</p>
            <p>A central question in computer vision approaches is the choice of a feature
               representation for the visual data. Classical approaches rely on hand-crafted feature
               descriptors. While global descriptors like <term>Histogram of Oriented
                  Gradients</term> (HOG) are considered to represent an image holistically,
               descriptors based on <term>Scale Invariant Feature Transform</term> (SIFT) <ptr
                  target="#lowe2004"/> or <term>Speeded Up Robust Features</term> (SURF) <ptr
                  target="#bay2008"/> have proven to be particularly suitable for local features
               since they are invariant to coordinate transformations, and robust to noise as well
               as to illumination changes. With the emergence of deep learning, however, feature
               representations based on convolutional neural networks (CNNs) largely replaced
               hand-crafted low-level features. Nowadays, CNNs and deep features are the
               state-of-the-art for many computer vision tasks <ptr target="#brejcha2017"/>
               <ptr target="#rawat2017"/>
               <ptr target="#wang2018"/>.</p>
            <div>
               <head>3.1 Shot Boundary Detection </head>
               <p>Shot boundary detection (SBD) is an essential prerequisite for video processing
                  and for video content analysis tasks. Typical techniques in SBD <ptr
                     target="#baber2011"/>
                  <ptr target="#lankinen2013"/>
                  <ptr target="#li2010"/> rely on low-level features, consisting of global or local
                  frame features that are used to measure the distance between consecutive frames.
                  One notable approach <ptr target="#apostolidis2014"/> utilises both colour
                  histograms and SURF descriptors <ptr target="#bay2008"/> along with GPU
                  acceleration to identify abrupt and gradual transitions in real time. This
                  approach achieves a F1 accuracy score of 0.902 on a collection of 15 videos
                  gathered from different video archives while being 3x faster than real-time
                  processing on GPU. In order to enhance detection, especially for more subtle
                  gradual transitions, several CNN-based proposals have been introduced. They
                  extract and employ representative deep features for frames <ptr target="#xu2016"/>
                  or train networks that detect shot boundaries directly <ptr target="#gygli2018"/>.
                  Xu et al. conduct experiments on TRECVID 2001 test data and achieve F1 scores of
                  0.988 and 0.968 for cut and gradual transitions, respectively. <ref
                     target="#gygli2018">Gygli (2018)</ref> reports a F1 score of 0.88 on the RAI
                  dataset <ptr target="#baraldi2015b"/> outperforming previous work, while being
                  extremely fast (more than 120x real-time on GPU).</p>
            </div>
            <div>
               <head>3.2 Scene Detection</head>
               <p>Given the shots, it is often desirable to segment a broadcast video into higher
                  level scenes. A scene is considered as a sequence of shots, which are related in a
                  spatio-temporal manner. For this purpose, <ref target="#baraldi2015b">Baraldi et
                     al. (2015b)</ref> detect superordinate scenes describing shots by means of
                  colour histograms and subsequently apply a hierarchical clustering approach. The
                  approach is applied to a collection of ten randomly selected broadcasting videos
                  from the RAI Scuola video archive constituting the RAI dataset. The method
                  achieves a F1 score of 0.70 at 7.7x real-time on CPU. <ref
                     target="#sidiropoulos2011">Sidiropoulos et al. (2011)</ref> suggest an
                  alternative approach, where shots constitute nodes in a graph representation and
                  edges between shots are weighted by shot similarity. Exploiting this
                  representation, scenes can be determined by partitioning the graph. On a test set
                  of six movies as well as on a set of 15 documentary films, the approach obtains a
                  F1 accuracy of 0.869 and 0.890 (F1 score on RAI dataset of <ref
                     target="#baraldi2015b">Baraldi et al. 2015b</ref>: 0.54), respectively. Another
                  solution <ptr target="#baraldi2015a"/> is to apply a multimodal deep neural
                  network to learn a metric for rating the difference between pairs of shots
                  utilizing both visual and textual features from the transcript. Similarity scores
                  of shots are used to segment the video into scenes. Beraldi et al. evaluate the
                  approach on 11 episodes from the BBC educational TV series <title rend="italic"
                     >Planet Earth</title> and report a F1 score of 0.62.</p>
            </div>
            <div>
               <head>3.3 Camera Motion Estimation </head>
               <p>Camera motion is considered as a significant element in film production. Thus,
                  estimating the types of camera motion can be helpful in breaking down a video
                  sequence into shots or for motion analysis of objects. Some techniques for camera
                  motion estimation perform direct optical flow computation <ptr
                     target="#nguyen2010"/>, while others consider motion vectors that are available
                  in compressed video files <ptr target="#ewerth2004a"/>. On four short movie
                  sequences Nguyen et al. individually obtain 94.04% to 98.26% precision (percentage
                  of correct detections). The latter approach is evaluated on a video test set
                  consisting of 32 video sequences including all kinds of motion types. It detects
                  zooming with 99%, tilting (vertical camera movement) with 93% and panning
                  (horizontal camera movement) with 79% precision among other motion types. This
                  approach achieved the best results in the task of zoom detection at TRECVID 2005.
                  More recent works in this field couple the camera motion problem with similar
                  tasks in order to train neural networks in a joint unsupervised framework <ptr
                     target="#zhou2017"/>
                  <ptr target="#yin2018"/>
                  <ptr target="#ranjan2019"/>. </p>
            </div>
            <div>
               <head>3.4 Object Detection and Visual Concept Classification</head>
               <p>Object detection is the task of localising and classifying objects in an image.
                  Motivated by the first application of CNNs to object classification <ptr
                     target="#krizhevsky2012"/>, regions with CNN features (R-CNN) were introduced
                  in order to (localize and) classify objects based on region proposals <ptr
                     target="#girschick2014"/>. However, since this approach is computationally very
                  expensive, several improvements have been proposed. Fast-R-CNNs <ptr
                     target="#girshick2015"/> were designed to jointly train feature extraction,
                  classification and bounding box regression in an unified network. Additionally
                  integrating a region proposal subnetwork enabled Faster-R-CNNs <ptr
                     target="#ren2017"/> to significantly speed up the formerly separate process of
                  generating regions of interest. Thus, Faster-R-CNNs achieve an accuracy of 42.7
                  mAP (mean Average Precision) at a frame rate of 5 fps (frames per second) on the
                  challenging MS COCO detection dataset <ptr target="#lin2014"/> (MSCOCO: Microsoft
                  Common Objects in Context). Furthermore, mask R-CNNs <ptr target="#he2017"/>extend
                  the Faster-R-CNN approach for pixel-level segmentation of object instances by
                  predicting object masks. Running at 5 fps, this approach predicts object boxes as
                  well as segments with an accuracy of 60.3 mAP and 58.0 mAP on the COCO dataset. In
                  contrast to region proposal based methods, <ref target="#redmon2016">Redmon et al.
                     (2016)</ref> introduced YOLO,  a single shot object detector that predicts
                  bounding boxes and associated object classes based on a fixed-grid regression.
                  While YOLO is very fast in terms of inference time, further extensions <ptr
                     target="#redmon2017"/>
                  <ptr target="#redmon2018"/> employ anchor boxes and make several improvements on
                  network design also boosting the overall detection accuracy to 57.9 mAP at 20 fps
                  on the COCO dataset. In order to detect a wide variety of over 9000 object
                  categories, <ref target="#redmon2017">Redmon and Farhadi (2017)</ref> also
                  introduced the real-time system YOLO9000, which was simultaneously trained on the
                  COCO detection dataset as well as the ImageNet classification dataset <ptr
                     target="#deng2009"/>. Apart from detecting objects, there were also many
                  approaches and more importantly datasets introduced for classifying images into
                  concepts. The SUN database <ptr target="#zhou2014"/> provides up to 5400
                  categories of objects and scenes. Current image concept classification approaches
                  typically rely on deep models trained with state-of-the-art architectures <ptr
                     target="#he2016"/>
                  <ptr target="#liu2017"/>
                  <ptr target="#szegedy2016"/>. </p>
            </div>
            <div>
               <head>3.5 Face Recognition and Person Identification </head>
               <p>Motivated by the significant progress in object classification, deep learning
                  methods have also been applied to face recognition. In this context, DeepFace <ptr
                     target="#taigman2014"/> is one of the first approaches that is trained to
                  obtain deep features for face verification and, moreover, enhances face alignment
                  based on explicit 3D modelling of faces. This approach achieves an accuracy of
                  97.35% on the prominent as well as challenging Labeled Faces in the Wild (LFW)
                  benchmark set. While DeepFace uses a cross-entropy loss (cost function) for
                  feature learning, <ref target="#schroff2015">Schroff et al. (2015)</ref>
                  introduced with FaceNet a novel and more sophisticated loss based on training with
                  triplets of roughly aligned matching and nonmatching face patches. On the LFW
                  benchmark, FaceNet obtains an accuracy of 99.63%.</p>
               <p>In the context of broadcast videos, the task of detecting faces and clustering
                  them for person indexing of frames or shots has been widely studied (e.g., <ref
                     target="#ewerth2007b">Ewerth et al. 2007b</ref>). While <ref
                     target="#muller2016">Müller et al. (2016)</ref> present a semi-supervised
                  system for automatically naming characters in TV broadcasts by extending and
                  correcting weakly labelled training data, <ref target="#jin2017">Jin et al.
                     (2017)</ref> both detect faces and cluster them by identity in full-length
                  movies. For content-based video retrieval in broadcast videos face recognition as
                  well as concept detection based on deep learning have also been proven to be
                  effective <ptr target="#muhling2017"/>. </p>
            </div>
            <div>
               <head>3.6 Recognition of Places and Geolocation</head>
               <p>Recognising a place in a frame or shot might yield also useful information for
                  film studies. The Places database contains over 400 unique place categories for
                  scene recognition. Along with the dataset, <ref target="#zhou2014">Zhou et al. (2014;</ref> <ref target="#zhou2018">2018</ref>) provide CNN
                  models trained with various architectures on the Places365 dataset. Using the
                  ResNet architecture <ptr target="#he2016"/>, for example, a top-5 accuracy of 85.1% can
                  be obtained on the Places365 validation set. <ref target="#mallya2018">Mallya and Lazebnik (2018)</ref> introduce
                  PackNet for training multiple tasks in a single model by pruning redundant
                  parameters. Thus, the network is trained on classes of the ImageNet as well as the
                  Places365 dataset. Being trained on multiple tasks, the model yields a top-5
                  classification error of 15.6% for the Places365 classes on the validation set,
                  while the individually trained network by <ref target="#zhou2018">Zhou et al. (2018)</ref> shows a top-5 error
                  rate of 16.1%. <ref target="#hu2018">Hu et al. (2018)</ref> introduce a novel CNN architecture unit called SE
                  block, which enables a network to use global information to selectively emphasise
                  informative features and suppress less useful ones by performing dynamic
                  channel-wise feature recalibration. This approach was trained on the Places365
                  dataset as well and shows a top-5 error rate of 11.0% on the corresponding
                  validation set. </p>
               <p>For the task of earth scale photo geolocation two major directions have been taken
                  so far. Im2GPS, one fundamental proposal for photo geolocation estimation, infers
                  GPS coordinates by matching the query image against a reference database of
                  geotagged images <ptr target="#hays2008"/> <ptr target="#hays2015"/> and was recently enhanced by
                  incorporating deep learning features <ptr target="#vo2017"/>. In this context, the Im2GPS
                  test set consisting of 237 challenging photos (only 5% are depicting touristic
                  sites) was introduced. The latest deep feature based Im2GPS version <ptr target="#vo2017"/> achieves an accuracy of 47.7% at region scale (location error less than 200
                  km). Other major proposals cast the task as a CNN-based classification approach by
                  partitioning the earth into geographical cells <ptr target="#weyand2016"/> and
                  considering combinatorial partitioning of maps, which facilitate more accurate and
                  robust class predictions <ptr target="#seo2018"/>. These frameworks called PlaNet and
                  CPlaNet achieve 37.6% and 42.6% accuracy at region scale on the benchmark,
                  respectively. Current state-of-the-art results (51.9% accuracy at region scale)
                  are achieved by similarly combining hierarchical map partitions and additionally
                  distinguishing three different settings (indoor, urban, and rural) through
                  automatic scene recognition <ptr target="#muller-budack2018"/>. </p>
            </div>
            <div>
               <head>3.7 Image Date Estimation</head>
               <p>While unrestricted photo geolocation estimation is well covered by several
                  studies, the problem of estimating the date of arbitrary (historical) photos was
                  addressed less frequently in the past. The first unrestricted work in this context
                  <ptr target="#palermo2012"/> dates historical colour images from 1930 to 1980 utilizing
                  colour descriptors that model the evolution of colour imaging processes over time.
                  Thus, Palermo et al. report an overall accuracy of 45.7% on a set of 1375 Flickr
                  images which are uniformly distributed across the considered decades. A recent
                  deep learning approach <ptr target="#muller2017"/> dates images from 1930 to 1999
                  considering the task either as a classification or a regression problem. Müller et
                  al. introduce the Date Estimation in the Wild test set consisting of 1120 Flickr
                  images, which cover every year uniformly, and report a mean estimation error of
                  less than 8 years for both the classification and regression models. </p>
            </div>
         </div>
         <div xml:id="section04">
            <head>4 Human and Machine Performance in Annotation Tasks</head>
            <p>When applying computer vision approaches to film studies, the question arises whether
               their accuracy is sufficiently high. In this respect, we provide a comparison of
               human and machine performance based on own previous work <ptr target="#ewerth2017"/> for
               some specific visual recognition tasks: face recognition, geolocation estimation of
               photos, date estimation of photos, as well as visual object and concept detection
               (<ref target="#table02">Table 2</ref>). </p>
            <p>A major field where human and machine performance has been compared frequently is
               visual concept classification. For a long time human annotations were (significantly)
               superior to machine-generated ones, as demonstrated by many studies <ptr target="#jiang2011"/> <ptr target="#parikh2010"/> <ptr target="#xiao2010"/> on datasets like PASCAL VOC or SUN.
               However, the accuracy of machine annotations could be significantly raised by deep
               convolutional neural networks. The error rate of 6.7% reported with GoogLeNet
               <ptr target="#szegedy2014"/> on the ImageNet Large Scale Visual Recognition Challenge
               (ILSVRC) 2014 was already close to that of humans (5.1 %), until in 2015 the error
               rate of neural networks <ptr target="#he2015"/> was slightly lower (error rate: 4.94%) than
               human error on the ImageNet challenge. However, the human error rate is only based on
               a single annotator. Hence, no information about human inter-annotator agreement is
               available.</p>
            <table xml:id="table02">
               <head>Comparison of human and machine performance in visual recognition tasks. </head>
               <row role="data">
                  <cell>Approach</cell>
                  <cell>Challenge/test set </cell>
                  <cell>Error Metric</cell>
                  <cell>Human Performance</cell>
                  <cell>Machine Performance</cell>
               </row>
               <row role="data">
                  <cell>Visual Concept Classification</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>GoogLeNet </p>
                     <p><ptr target="#szegedy2014"/></p>
                  </cell>
                  <cell>ILSVRC’14</cell>
                  <cell>Top-5 test error</cell>
                  <cell>5.1%</cell>
                  <cell>6.66%</cell>
               </row>
               <row role="data">
                  <cell>He et al. 2015</cell>
                  <cell>ImageNet’12 dataset</cell>
                  <cell>Top-5 test error</cell>
                  <cell>5.1%</cell>
                  <cell>4.94%</cell>
               </row>
               <row role="data">
                  <cell>Face Verification</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>DeepFace </p>
                     <p><ptr target="#taigman2014"/></p>
                  </cell>
                  <cell>LFW </cell>
                  <cell>Accuracy</cell>
                  <cell>97.53%</cell>
                  <cell>97.35%</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>FaceNet</p>
                     <p><ptr target="#schroff2015"/></p>
                  </cell>
                  <cell>LFW</cell>
                  <cell>Accuracy</cell>
                  <cell>97.53%</cell>
                  <cell>99.63%</cell>
               </row>
               <row role="data">
                  <cell>Geolocation Estimation</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>PlaNet </p>
                     <p><ptr target="#weyand2017"/></p>
                  </cell>
                  <cell>Im2GPS test set</cell>
                  <cell>
                     <p>Accuracy at </p>
                     <p>region/continent scale</p>
                  </cell>
                  <cell>3.8% / 39.3%</cell>
                  <cell>37.6% / 71.3%</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>Im2GPS</p>
                     <p><ptr target="#vo2017"/></p>
                  </cell>
                  <cell>Im2GPS test set</cell>
                  <cell>
                     <p>Accuracy at </p>
                     <p>region/continent scale</p>
                  </cell>
                  <cell>3.8% / 39.3%</cell>
                  <cell>47.7% / 73.4%</cell>
               </row>
               <row role="data">
                  <cell>
                     <p>CPlaNet</p>
                     <p><ptr target="#seo2018"/></p>
                  </cell>
                  <cell>Im2GPS test set</cell>
                  <cell>
                     <p>Accuracy at </p>
                     <p>region/continent scale</p>
                  </cell>
                  <cell>3.8% / 39.3%</cell>
                  <cell>46.4% / 78.5%</cell>
               </row>
               <row role="data">
                  <cell><ptr target="#muller-budack2018"/></cell>
                  <cell>Im2GPS test set</cell>
                  <cell>
                     <p>Accuracy at </p>
                     <p>region/continent scale</p>
                  </cell>
                  <cell>3.8% / 39.3%</cell>
                  <cell>51.9% / 80.2%</cell>
               </row>
               <row role="data">
                  <cell>Date Estimation</cell>
               </row>
               <row role="data">
                  <cell><ptr target="#palermo2012"/></cell>
                  <cell>Test set crawled from Flickr</cell>
                  <cell>
                     <p>Classification accuracy</p>
                     <p>by decade</p>
                  </cell>
                  <cell>26.0%</cell>
                  <cell>45.7%</cell>
               </row>
               <row role="data">
                  <cell><ptr target="muller2017"/></cell>
                  <cell>
                     <hi rend="italic">Date Estimation in the Wild </hi>test set</cell>
                  <cell>
                     <p>Absolute mean error </p>
                     <p>(in years)</p>
                  </cell>
                  <cell>10.9%</cell>
                  <cell>7.3%</cell>
               </row>
            </table>
            <p>A comparison of face identification capabilities of humans and machines was made for
               the first time in a Face Recognition Vendor Test (FRVT) study in 2006 <ptr target="#phillips2006"/>. Thus, it has been shown - interestingly, already nearly 15 years ago -
               that industry-strength methods can compete with human performance in identifying
               unfamiliar faces under illumination changes. On the more recent as well as more
               challenging Labeled Faces in the Wild (LFW) benchmark, human performance has also
               been reached by several approaches. Among several others, DeepFace <ptr target="#taigman2014"/> and FaceNet <ptr target="#schroff2015"/> are prominent deep learning approaches
               reporting human-level (97.53%) results of 97.35% and 99.63% accuracy on the
               benchmark, practically solving the LFW dataset.</p>
            <p>While humans are relatively good at recognising persons, estimating or guessing the
               location and date of an image imposes a more difficult task for subtle spatial as
               well as temporal differences. Various systems have been proposed for earth scale
               photo geolocation that outperform human performance in this task. The latest of these
               <ptr target="#seo2018"/> <ptr target="#vo2017"/> <ptr target="#weyand2016"/> employ deep learning methods
               consuming up to 90 million training images for an imposed classification task of
               cellular earth regions and/or rely on an extensive retrieval database. Current
               state-of-the-art results are reached by considering hierarchical as well as scene
               information of photos within the established classification approach, while using
               only 5 million training images <ptr target="#muller-budack2018"/>. Since human performance
               reported on the established Im2GPS benchmark <ptr target="#vo2017"/> is relatively poor in
               this task, the system clearly surpasses human ability to guess geolocation by 48.1%
               accuracy within a tolerated distance level of 200 km (predictions at region level). </p>
            <p>The creation date of (historical) photos is very hard to judge for periods of time
               that are quite similar. Therefore, a fundamental work <ptr target="#palermo2012"/> predicts
               the decade of historical colour photos with an accuracy of 45.7% exceeding that of
               untrained humans (26.0% accuracy), <ref target="#muller2017">Müller et al. (2017)</ref> suggest a deep learning
               system that infers the capturing date of images taken between 1930 and 1999 in terms
               of 5-year periods. When comparing human and machine annotations by means of absolute
               mean error in years, the deep learning system achieves better results nearly at all
               periods and improves the overall mean error by more than three years.</p>
            <p>In general, the promising performance of the computer vision approaches compared to
               humans in Table 2 makes a strong case for their use in film studies. Although these
               methods are still prone to errors (to a lesser or greater extent), they can highly
               raise the exploration of media sources and help in finding patterns. In spite of the
               impressive performance of the selected approaches, computer vision approaches still
               have some shortcomings. Such approaches are optimized for specific visual content
               analysis tasks and rely on custom training data. Therefore, they have limited
               flexibility and cannot adapt to arbitrary images across different genres. While they
               perform well in basic computer vision tasks, humans are by far superior in grasping
               and interpreting images in their context, for example, in identifying gradual
               transitions between shots or in captioning images/videos.</p>
         </div>
         <div xml:id="section05">
            <head>5 Conclusions and Future Prospects for Software Tools in Film Studies </head>
            <p>In this paper, we have reviewed off-the-shelf software solutions for film studies.
               Since quantitative approaches to film content analysis are a handy way to effectively
               explore basic filmic features (see <ref target="#section01">Section 1</ref>), we have put a focus on offered
               functionalities regarding automated film style and content analysis. In this respect,
               only the tools Videana and VIAN offer a wider range of automated video analysis
               methods. However, with Videana not being developed anymore and the most recent tool
               VIAN focusing on film colour patterns, the field still lacks available tools that
               provide powerful state-of-the-art methods for visual content analysis. We discuss
               needed functionality in detail in the latter part of this section. Furthermore, we
               have outlined recent advances in the (automated) analysis of film style (shot and
               scene segmentation, use of camera motion or colours) as well as current developments
               and progress in the field of computer vision. As also discussed in the beginning of
               this paper, quantitative approaches are partially criticised for being incompatible
               with qualitative film analysis. To showcase the chances of basic computer vision
               approaches, we have compared machine and human performance in visual annotation
               tasks. We have shown that machine annotations are of similar quality to those of
               humans for some basic tasks like object classification, face recognition or
               geolocation estimation. Even when these methods do not reach human abilities, they
               can build a valid basis for exploring media sources for further manual inspection. </p>
            <p>What kind of basic and extended functionality for automated analysis and
               visualisation should a software tool have in order to support research in film
               studies? Previous practical experience with the basic functions of Videana has shown
               that such software is basically suitable for effectively supporting film research and
               teaching. However, film scholars often need more advanced functions tailored to their
               particular domain that allow the automatic detection of complex stylistic film
               elements such as the shot-reverse-shot technique. This requires, for example,
               detecting a series of alternating close-ups with a dialogue component and can be
               detected through the syntactic interaction of different factors. Starting from the
               smallest discontinuity in film, the cut, it is also possible to detect changing
               colour values or certain structural semantic properties such as the position of an
               object within consecutive shots. These could also allow drawing conclusions about
               changes within the storyline. Such forms of automatic segmentation and creating
               individual segments of events can be relevant in the context of narrative analyses.
               Researchers could be offered an effective structuring and orientation aid, which can
               undergo further manual differentiation based on content-related or motivic aspects.
               Therefore we envision a program that also offers a high degree of flexibility with
               regard to manual annotation opportunities. For a specific film or film corpus,
               individual survey parameters must be generated in order to judge their correlation
               with other parameters - depending on which hypotheses are to be applied to the object
               of investigation or can be formulated on the basis of the film material.</p>
            <p>Considering, for example, a film as a system of information mediation as in
               quantitative suspense research <ptr target="#junkerjurgen2001"/> <ptr target="#weibel2008"/> <ptr target="#weibel2017"/>, individual
               shots and sequences could be detected using automatic methods and manually be
               supplemented with further content parameters. Here, classifications such as the
               degree of information shared between the film character and the viewer (if the film
               viewer knows more or less than the character), the degree of correspondence between
               narrative time (duration of the film) and narrated time (time period covered by the
               filmic narrative), or the degree of narrative relevance (is the event only of
               relevance within the sequence or does it affect the central conflict of the
               narrative) are considered in order to draw conclusions about the suspense potential
               of a sequence. In the outlined framework, these factors favour the exploitation of
               the viewer's anticipation of damage - in particular their emotional connection to a
               film character - through a principle of delay, as primarily applied in battle
               sequences typical of action films. Due to the principle of delaying the resolution of
               a conflict, they have a low information gain in view of the entire narrative. The
               principles of this parameterisation can now be used to check which dramaturgical
               context is characterised by which formal characteristics in comparison with the
               editing parameters of Salt. This concept of data acquisition can be extended by
               further parameters such as the application of digital visual effects in individual
               sequences, which provides a differentiated insight into the internal dynamics and
               proportions of filmic representation systems. Especially with regard to
               computer-generated imagery, which is subject to development processes spanning
               decades, it is possible to examine on a longitudinal scale how this process has had a
               concrete effect on production practices. However, such highly complex interwoven data
               structures require sophisticated statistical models with which these hypotheses can
               be tested.</p>
            <p>Finally, a software tool adapted to the research object should be developed in a
               productive dialogue between computer science and film science. Such software for film
               studies could be based on experiences with software such as Videana or VIAN that
               allows for the evaluation of larger film corpora on a differentiated and reliable
               data basis, which cannot be generated with previous analysis instruments.
               Furthermore, it is desirable to include specific forms of information visualisations
               tailored to the needs of film scholars. Cinemetrics or Ligne de temps, as software
               designed for film studies, are also limited to a graphical representation that cannot
               take into account the requirements of narrative questions. Since an all-in-one
               approach will not fit to all analysis and visualisation requirements, such a tool
               should also provide an interface for plugins that offer additional functionality.
               Using these approaches, a large collection of research data can be gathered and
               exported for further processing, but the lack of a working digital environment for
               media science continues to be a problem. Statistical software or Microsoft Excel were
               not designed for the visualization of editing rhythms or narrative structures, which
               makes it difficult to process the corresponding data. An interdisciplinary
               cooperation could foster research in designing an optimal solution for scholarly film
               studies that allows direct interaction with the automatically determined parameters
               as well as their method-dependent annotation and graphical processing.</p>
            <p>In summary, the development of a comprehensive software solution for scientists who
               systematically carry out film or video analysis would be desirable. This group
               includes media and film scientists, but also scientists from other disciplines (e.g.,
               applications in journalism and media education, analysis of propaganda videos and
               political video works, image and educational films, television programmes). Also,
               empirical studies of media psychology in the field of event indexing require the
               annotation and analysis of structural properties of audiovisual research data.
               Factors such as the duration of an action segment as well as temporal, spatial,
               figure-related or action-related changes within the sequences (four dimension
               models), but also shot lengths are integrated into the research design and are
               prerequisites for the formulation of hypotheses and their empirical validation <ptr target="#huff2014"/>.</p>
            <p>An interdisciplinary all-in-one software tool of this kind should be openly available
               on a web-based platform, intuitive, easy to use and rely on state-of-the-art
               algorithms and technology. On the one hand, by providing automatic methods for the
               quantitative analysis of film material, large film and video corpora could become the
               object of investigation and hypotheses could, for example, be statistically tested;
               on the other hand, the interpretation would be simplified by different possibilities
               of visualisation. Last but not least, legal questions regarding the storage,
               processing and use of moving image material should be clarified and taken into
               account in the technical implementation.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="abend2011" label="Abend et al. 2011">Abend, P., Thielmann, T.,  Ewerth,
               R., Seiler, D., Mühling, M., Döring, J., Grauer, M., &amp; Freisleben, B. <title
                  rend="quotes">Geobrowsing the Globe: A Geovisual Analysis of Google Earth
                  Usage.</title> In: <title rend="italic">Proc. of Linking GeoVisualization with
                  Spatial Analysis and Modeling</title> (GeoViz), Hamburg, (2011).</bibl>
            <bibl xml:id="abend2012" label="Abend et al. 2012">Abend, P., Thielmann, T., Ewerth, R.,
               Seiler, D., Mühling, M., Döring, J., Grauer, M., &amp; Freisleben, B. <title
                  rend="quotes">Geobrowsing Behaviour in Google Earth: A Semantic Video Content
                  Analysis of On-Screen Navigation.</title> In: <title rend="italic">Proc. of
                  Geoinformatics Forum</title>, Salzburg, Österreich, (2012), pp. 2-13. </bibl>
            <bibl xml:id="adams2000" label="Adams et al. 2000">Adams, B., Dorai, C. &amp; Venkatesh,
               S. <title rend="quotes">Towards Automatic Extraction of Expressive Elements from
                  Motion Pictures: Tempo.</title> <title rend="italic">IEEE International Conference
                  on Multimedia and Expo (II)</title> (2000), pp. 641-644.</bibl>
            <bibl xml:id="adams2001" label="Adams et al. 2001">Adams, B., Chitra Dorai, C.
               &amp; Venkatesh, S. <title rend="quotes">Automated Film Rhythm Extraction For Scene
                  Analysis.</title> <title rend="italic">ICME</title> (2001).</bibl>
            <bibl xml:id="adams2005" label="Adams et al. 2005">Adams, B., Venkatesh, S., Bui, H. H.
               &amp; Dorai, C. <title rend="quotes">A Probabilistic Framework for Extracting
                  Narrative Act Boundaries and Semantics in Motion Pictures.</title> <title
                  rend="italic">Multimedia Tools Appl.</title> 27(2) (2005): 195-213.</bibl>
            <bibl xml:id="anitha2013" label="Anitha et al. 2013">Anitha A., Brasoveanu, A., Duarte
               M., Hughes, S., Daubechies, I., Dik, J., Janssens, K., &amp; Alfeld, M. <title
                  rend="quotes">Restoration of X-ray fluorescence images of hidden
                  paintings.</title>
               <title rend="italic">Signal Processing</title>, 93(3) (2013): 592-604.</bibl>
            <bibl xml:id="apostolidis2014" label="Apostolidis and Mezaris 2014">Apostolidis, E.
               &amp; Mezaris, V. <title rend="quotes">Fast shot segmentation combining global and
                  local visual descriptors.</title> In: <title rend="italic">International
                  Conference on Acoustics, Speech and Signal Processing</title>, Florence, Italy
               (2014), pp. 6583-6587.</bibl>
            <bibl xml:id="aubert2005" label="Aubert and Prié 2005">Aubert, O. &amp; Prié, Y. <title
                  rend="quotes">Advene: Active reading through hypervideo.</title> In: <title
                  rend="italic">Proceedings of ACM Hypertext '05</title> (2005), pp. 235-244.</bibl>
            <bibl xml:id="baber2011" label="Baber et al. 2011">Baber, J., Afzulpurkar, N. V.,
               Dailey, M. N., &amp; Bakhtyar, M. <title rend="quotes">Shot boundary detection from
                  videos using entropy and local descriptor.</title> In: <title rend="italic"
                  >Proceedings of the 17th International Conference on Digital Signal
                  Processing</title>, Corfu, Greece (2011), pp. 1-6.</bibl>
            <bibl xml:id="baraldi2015a" label="Baraldi et al. 2015a">Baraldi, L., Grana, C., &amp;
               Cucchiara, R. A <title rend="quotes">Deep Siamese Network for Scene Detection in
                  Broadcast Videos.</title> In <title rend="italic">Proceedings of the 23rd Annual
                  ACM Conference on Multimedia Conference</title>, Brisbane, Australia (2015), pp.
               1199-1202.</bibl>
            <bibl xml:id="baraldi2015b" label="Baraldi et al. 2015b">Baraldi, L., Grana,  C., &amp;
               Cucchiara, R. <title rend="quotes">Shot and scene detection via hierarchical
                  clustering for re-using broadcast video.</title> In: <title rend="italic"
                  >International Conference on Computer Analysis of Images and Patterns</title>
               (2015), pp. 801-811.</bibl>
            <bibl xml:id="bateman2014" label="Bateman 2014">Bateman, J. A. <title rend="quotes"
                  >Looking for what counts in film analysis: A programme of empirical
                  research.</title> In <title rend="italic">Visual Communication</title>, De
               Gruyter, Berlin (2014): 301-330. </bibl>
            <bibl xml:id="bay2008" label="Bay et al. 2008">Bay, H., Ess, A., Tuytelaars, T., &amp;
               Van Gool, L. <title rend="quotes">Speeded-Up Robust Features (SURF).</title>
               <title rend="italic">Computer Vision and Image Understanding</title>, 110(3) (2008):
               346-359.</bibl>
            <bibl xml:id="brejcha2017" label="Brejcha and Cadík 2017">Brejcha, J. &amp; Cadík, M.
                  <title rend="quotes">State-of-the-art in visual geo-localization.</title>
               <title rend="italic">Pattern Analysis and Applications</title>, 20(3) (2017):
               613-637.</bibl>
            <bibl xml:id="buckland2009" label="Buckland 2009">Buckland, W. <title rend="quotes"
                  >Ghost director.</title>
               <title rend="italic">Digital Tools in Media Studies</title>, M. Ross, M. Grauer and
               B. Freisleben (eds.). Bielefeld: transcript Verlag (2009). </bibl>
            <bibl xml:id="burghardt2016" label="Burghardt et al. 2016">Burghardt, M., Kao, M., &amp;
               Wolff, C. <title rend="quotes">Beyond Shot Lengths–Using Language Data and Color
                  Information as Additional Parameters for Quantitative Movie Analysis.</title> In:
                  <title rend="italic">Digital Humanities 2016: Conference Abstracts</title>.
               Jagiellonian University &amp; Pedagogical University, Kraków (2016): 753-755.</bibl>
            <bibl xml:id="deng2009" label="Deng et al. 2009">Deng, J., Dong, W., Socher, R., Li, L.,
               Li, K., &amp; Fei-Fei, L. <title rend="quotes">ImageNet: A large-scale hierarchical
                  image database.</title> In: <title rend="italic">Proceedings of the Conference on
                  Computer Vision and Pattern Recognition</title> (2009), pp. 248–255.</bibl>
            <bibl xml:id="estrada2017" label="Estrada et al. 2017">Estrada, L. M, Hielscher, E.
               Koolen, M., Olesen, C. G., Noordegraaf, J. &amp; Jaap Blom, J. <title rend="quotes"
                  >Film Analysis as Annotation: Exploring Current Tools.</title>
               <title rend="italic">The Moving Image - Special Issue on Digital Humanities and/in
                  Film Archives</title> (Vol 17, no 2) (2017): 40-70.</bibl>
            <bibl xml:id="ewerth2004a" label="Ewerth et al. 2004">Ewerth, R., Schwalb, M., Tessmann,
               P., &amp; Freisleben, B. <title rend="quotes">Estimation of Arbitrary Camera Motion
                  in MPEG Videos.</title> In: <title rend="italic">Proceedings of 17th Int.
                  Conference on Pattern Recognition</title>, (2004), pp. 512–515.</bibl>
            <bibl xml:id="ewerth2004b" label="Ewerth and Freisleben 2004">Ewerth, R. &amp;
               Freisleben, B. <title rend="quotes">Video Cut Detection without Thresholds.</title>
               In: <title rend="italic">Proc. of 11th Workshop on Signals, Systems and Image
                  Processing</title>, Poznan, Poland (2004), pp. 227-230.</bibl>
            <bibl xml:id="ewerth2007a" label="Ewerth et al. 2007a">Ewerth, R., Schwalb, M.,
               Tessmann, P., &amp; Freisleben, B. <title rend="quotes">Segmenting Moving Objects in
                  the Presence of Camera Motion.</title> In: <title rend="italic">Proc. of 14th Int.
                  Conference on Image Analysis and Processing</title>, Modena, Italy (2007), pp.
               819-824.</bibl>
            <bibl xml:id="ewerth2007b" label="Ewerth et al. 2007b">Ewerth, R., Mühling, M., &amp;
               Freisleben, B. <title rend="quotes">Self-Supervised Learning of Face Appearances in
                  TV Casts and Movies.</title> Invited Paper (Best papers from IEEE International
               Symposium on Multimedia ‘06): <title rend="italic">International Journal on Semantic
                  Computing, World Scientific</title> (2007), pp. 185-204.</bibl>
            <bibl xml:id="ewerth2009a" label="Ewerth and Freisleben 2009">Ewerth, R. &amp;
               Freisleben, B. <title rend="quotes">Unsupervised Detection of Gradual Shot Changes
                  with Motion-Based False Alarm Removal.</title> In: <title rend="italic"
                  >Proceedings of 8th International Conference on Advanced Concepts for Intelligent
                  Vision Systems (ACIVS)</title>, Bordeaux, France, Springer (2009), pp.
               253-264.</bibl>
            <bibl xml:id="ewerth2009b" label="Ewerth et al. 2009b">Ewerth, R., Mühling, M.,
               Stadelmann, T., Gllavata, J., Grauer, M., &amp; Freisleben, B. <title rend="quotes"
                  >Videana: A Software Toolkit for Scientific Film Studies.</title>
               <title rend="italic">Digital Tools in Media Studies – Analysis and Research. An
                  Overview.</title> Transcript Verlag, Bielefeld, Germany (2009): 101-116.</bibl>
            <bibl xml:id="ewerth2012" label="Ewerth et al. 2012">Ewerth, R., Ballafkir, K., Mühling,
               M., Seiler, D., &amp; Freisleben, B. <title rend="quotes">Long-Term Incremental
                  Web-Supervised Learning of Visual Concepts via Random Savannas.</title>
               <title rend="italic">IEEE Trans. on Multimedia</title>, 14(4) (2012):
               1008-1020.</bibl>
            <bibl xml:id="ewerth2017" label="Ewerth et al. 2017">Ewerth, R., Springstein, M.,
               Phan-Vogtmann, L. A., &amp; Schütze, J. <title rend="quotes"><q>Are Machines Better
                     in Image Tagging?</q> – A User Study Adds to the Puzzle.</title> In:  <title
                  rend="italic">Proceedings of 39th European Conference on Information Retrieval
                  (ECIR)</title>, Aberdeen, UK (2017), pp. 186-198 </bibl>
            <bibl xml:id="fluckiger2011" label="Flückiger 2011">Flückiger, B. <title rend="quotes"
                  >Die Vermessung ästhetischer Erscheinungen.</title>
               <title rend="italic">ZfM</title> 5 (2011): 44-60.</bibl>
            <bibl xml:id="fluckiger2017" label="Flückiger et al. 2017">Flückiger, B.,  Evirgen, N.,
               Paredes, E. G., Ballester-Ripoll, R.,  &amp; Pajarola, R. <title rend="quotes">Deep
                  Learning Tools for Foreground-Aware Analysis of Film Colors.</title> In: <title
                  rend="italic">Computer Vision in Digital Humanities</title>, Digital Humanities
               Conference, Montreal (2017).</bibl>
            <bibl xml:id="girschick2014" label="Girschick et al. 2014">Girshick R. B., Donahue, J.,
               Darrell, T., &amp; Malik, J. <title rend="quotes">Rich Feature Hierarchies for
                  Accurate Object Detection and Semantic Segmentation.</title> In <title
                  rend="italic">Proceedings of the Conference on Computer Vision and Pattern
                  Recognition</title>, Columbus, OH, USA (2014), pp. 580-587.</bibl>
            <bibl xml:id="girshick2015" label="Girshick 2015">Girshick, R. B. <title rend="quotes"
                  >Fast R-CNN.</title> In <title rend="italic">Proceedings of the International
                  Conference on Computer Vision</title>, Santiago, Chile (2015), pp.
               1440-1448.</bibl>
            <bibl xml:id="gllavata2004a" label="Gllavata et al. 2004a">Gllavata, J., Ewerth, R.,
               &amp; Freisleben, B. <title rend="quotes">Text Detection in Images Based on
                  Unsupervised Classification of High-Frequency Wavelet Coefficients.</title>
               <title rend="italic">ICPR</title> (2004), pp. 425-428.</bibl>
            <bibl xml:id="gllavata2004b" label="Gllavata et al. 2004b">Gllavata, J., Ewerth, R.,
               &amp; Freisleben, B. <title rend="quotes">Tracking text in MPEG videos.</title> In:
                  <title rend="italic">ACM Multimedia</title> (2004), pp. 240-243.</bibl>
            <bibl xml:id="gygli2018" label="Gygli 2018">Gygli, M. <title rend="quotes">Ridiculously
                  Fast Shot Boundary Detection with Fully Convolutional Neural Networks.</title> In:
                  <title rend="italic">International Conference on Content-Based Multimedia
                  Indexing</title>, La Rochelle, France (2018), pp. 1-4.</bibl>
            <bibl xml:id="hays2008" label="Hays and Efros 2008">Hays, J. &amp; Efros, A. A. <title
                  rend="quotes">IM2GPS: estimating geographic information from a single
                  image.</title> In <title rend="italic">Proceedings of the Conference on Computer
                  Vision and Pattern Recognition</title>, Anchorage, Alaska, USA (2008). </bibl>
            <bibl xml:id="hays2015" label="Hays and Efros 2015">Hays, J. &amp; Efros, A. A. <title
                  rend="quotes">Large-Scale Image Geolocalization.</title>
               <title rend="italic">Multimodal Location Estimation of Videos and Images</title>
               (2015): 41-62.</bibl>
            <bibl xml:id="he2015" label="He et al. 2015">He, K., Zhang, X., Ren, S., &amp; Sun, J.
                  <title rend="quotes">Delving Deep into Rectifiers: Surpassing Human-Level
                  Performance on ImageNet Classification.</title> In <title rend="italic"
                  >Proceedings of ICCV</title> (2015), pp. 1026-1034.</bibl>
            <bibl xml:id="he2016" label="He et al. 2016">He, K., Zhang, X.,  Ren, S., &amp; Sun, J.
                  <title rend="quotes">Deep Residual Learning for Image Recognition.</title> In
                  <title rend="italic">Proceedings of CVPR</title> (2016), pp. 770-778.</bibl>
            <bibl xml:id="he2018" label="He et al. 2018">He, K., Gkioxari, G., Dollár, P., &amp;
               Girshick, R. B. <title rend="quotes">Mask R-CNN.</title> In <title rend="italic"
                  >International Conference on Computer Vision</title>, Venice, Italy (2018), pp.
               2980-2988.</bibl>
            <bibl xml:id="heftberger2016" label="Heftberger 2016">Heftberger, A. <title
                  rend="italic">Kollision der Kader: Dziga Vertovs Filme, die Visualisierung ihrer
                  Strukturen und die Digital Humanities</title>, München: edition text+kritik
               (2016).</bibl>
            <bibl xml:id="hohman2017" label="Hohman et al. 2017">Hohman, F., Soni, S., Stewart, I.,
               &amp; Stasko, J. <title rend="quotes">A Viz of Ice and Fire: Exploring Entertainment
                  Video Using Color and Dialogue.</title> In <title rend="italic">2nd Workshop on
                  Visualization for the Digital Humanities</title>, Phoenix, Arizona, USA
               (2017).</bibl>
            <bibl xml:id="hoyt2014" label="Hoyt et al. 2014">Hoyt, E., Ponot , K . and Roy, C.
                  <title rend="quotes">Visualizing and Analyzing the Hollywood Screenplay with
                  ScripThreads.</title>
               <title rend="italic">Digital Humanities Quarterly</title>, 8(4) (2014).</bibl>
            <bibl xml:id="hu2018" label="Hu et al. 2018">Hu, J., Shen, L., &amp; Sun, G. <title
                  rend="quotes">Squeeze-and-Excitation Networks.</title> In: <title rend="italic"
                  >Proceedings of the Conference on Computer Vision and Machine Learning</title>
               (2018), pp. 7132-7141.</bibl>
            <bibl xml:id="huff2014" label="Huff et al. 2014">Huff, M., Meitz, T., &amp; Papenmeier,
               F. <title rend="quotes">Changes in Situation Models Modulate Processes of Event
                  Perception in Audiovisual Narratives.</title>
               <title rend="italic">Journal of Experimental Psychology - Learning, Memory, and
                  Cognition</title>, 40(5) (2014): 1377-1388.</bibl>
            <bibl xml:id="itten1961" label="Itten 1961">Itten, J. <title rend="italic">Kunst der
                  Farbe</title>. Ravensburg: Otto Maier Verlag (1961).</bibl>
            <bibl xml:id="jiang2011" label="Jiang et al. 2011">Jiang, Y. G., Ye, G., Chang, S. F.,
               Ellis, D., &amp; Loui, A. C. <title rend="quotes">Consumer video understanding: a
                  benchmark database and an evaluation of human and machine performance.</title> In:
                  <title rend="italic">Proceedings of the International Conference on Multimedia
                  Retrieval</title> (2011), p. 29.</bibl>
            <bibl xml:id="jin2017" label="Jin et al. 2017">Jin, S., Su, H., Stauffer, C., &amp;
               Learned-Miller, E. G. <title rend="quotes">End-to-End Face Detection and Cast
                  Grouping in Movies Using Erdös-Rényi Clustering.</title> In: <title rend="italic"
                  >Proceedings of the International Conference on Computer Vision</title>, Venice,
               Italy (2017), pp. 5286-5295.</bibl>
            <bibl xml:id="john2017" label="John et al. 2017">John, M., Kurzhals, K., Koch, S., &amp;
               Weiskopf, D. <title rend="quotes">A Visual Analytics Approach for Semantic
                  Multi-Video Annotation.</title> In: <title rend="italic">2nd Workshop on
                  Visualization for the Digital Humanities</title>, Phoenix, Arizona, USA (2017). </bibl>
            <bibl xml:id="johnson2008" label="Johnson et al. 2008">Johnson, C. R., Hendriks, E.,
               Berezhnoy, I. J., Brevdo, E., Hughes, S. M., Daubechies, I., &amp; Wang, J. Z. <title
                  rend="quotes">Image processing for artist identification.</title>
               <title rend="italic">IEEE Signal Processing Magazine</title>, 25(4) (2008):
               37-48.</bibl>
            <bibl xml:id="junkerjurgen2001" label="Junkerjürgen 2001">Junkerjürgen, R. <title
                  rend="italic">Spannung – narrative Verfahrenweisen der Leseraktivierung: eine
                  Studie am Beispiel der Reiseromane von Jules Verne.</title> Frankfurt am Main;
               Berlin; Bern; Bruxelles; New York; Oxford; Wien: Lang (2001).</bibl>
            <bibl xml:id="kipp2001" label="Kipp 2001">Kipp, M. (2001). <title rend="quotes">Anvil -
                  A Generic Annotation Tool for Multimodal Dialogue.</title> In: <title
                  rend="italic">Proceedings of the 7th European Conference on Speech Communication
                  and Technology</title> (Eurospeech) (2001), pp. 1367-1370. </bibl>
            <bibl xml:id="klein2014" label="Klein et al. 2014">Klein C., Betz J., Hirschbuehl M.,
               Fuchs C., Schmiedtová B., Engelbrecht M., Mueller-Paul, J., &amp; Rosenberg, R.
                  <title rend="quotes">Describing Art – An Interdisciplinary Approach to the Effects
                  of Speaking on Gaze Movements during the Beholding of Paintings.</title>
               <title rend="italic">PLoS ONE</title> 9(12) (2014). </bibl>
            <bibl xml:id="korte2010" label="Korte 2010">Korte, H. <title rend="italic">Einführung in
                  die systematische Filmanalyse</title>. Berlin: Schmidt (2010).</bibl>
            <bibl xml:id="krizhevsky2012" label="Krizhevsky et al. 2012">Krizhevsky, A., Sutskever,
               I., &amp; Hinton, G. E. <title rend="quotes">ImageNet Classification with Deep
                  Convolutional Neural Networks.</title> In: <title rend="italic">Proc. of 26th
                  Conf. on Neural Information Processing Systems 2012</title>. Lake Tahoe, Nevada,
               United States (2012), pp. 1106–1114.</bibl>
            <bibl xml:id="lankinen2013" label="Lankinen and Kämäräinen 2013">Lankinen, J., &amp;
               Kämäräinen, J. <title rend="quotes">Video Shot Boundary Detection Using Visual
                  Bag-of-Words.</title> In: <title rend="italic">Proceedings of the International
                  Conference on Computer Vision Theory and Applications</title> (1), Barcelona,
               Spain (2013), pp. 788-791.</bibl>
            <bibl xml:id="li2010" label="Li et al. 2010">Li, J., Ding, Y., Shi, Y., &amp; Li, W.
                  <title rend="quotes">A divide-and-rule scheme for shot boundary detection based on
                  sift.</title>
               <title rend="italic">Journal of Digital Content Technology and its
                  Applications</title> (2010): 202–214.</bibl>
            <bibl xml:id="lin2014" label="Lin et al. 2014">Lin, T., Maire, M., Belongie, S. J.,
               Hays, H., Perona, P., Ramanan, D., Dollár, P., &amp; Zitnick, L. <title rend="quotes"
                  >Microsoft COCO: Common Objects in Context.</title> In: <title rend="italic"
                  >Proceedings of ECCV</title> (2014), pp. 740-755.</bibl>
            <bibl xml:id="liu2012" label="Liu 2012">Liu, A. <title rend="quotes">Where Is Cultural
                  Criticism in the Digital Humanities?</title> [Online] (2012). Available at: <ref
                  target="http://dhdebates.gc.cuny.edu/debates/text/20"
                  >http://dhdebates.gc.cuny.edu/debates/text/20</ref> (Accessed: 19 December
               2018)</bibl>
            <bibl xml:id="liu2017" label="Liu et al. 2017">Liu, C., Zoph, B., Shlens, J., Hua, W.,
               Li, L., Fei-Fei, L., Yuille, A., Huang, J., &amp; Murphy, K. <title rend="italic"
                  >Progressive Neural Architecture Search</title> (2017).</bibl>
            <bibl xml:id="lowe2004" label="Lowe 2004">Lowe, D. G. <title rend="quotes">Distinctive
                  Image Features from Scale-Invariant Keypoints.</title>
               <title rend="italic">International Journal of Computer Vision</title>, 60(2) (2004):
               91–110.  </bibl>
            <bibl xml:id="mallya2018" label="Mallya and Lazebnik 2018">Mallya, A. &amp; Lazebnik, S.
                  <title rend="quotes">PackNet: Adding Multiple Tasks to a Single Network by
                  Iterative Pruning.</title> In <title rend="italic">Proceedings of the Conference
                  on Computer Vision and Pattern Recognition</title>, Salt Lake City, UT, USA
               (2018), pp. 7765-7773.</bibl>
            <bibl xml:id="missomelius2014" label="Missomelius 2014">Missomelius, P. <title
                  rend="quotes">Medienbildung und Digital Humanities: Die Medienvergessenheit
                  technisierter Geisteswissenschaften.</title> In: Ortner, H., Pfurtscheller, D.,
               Rizzolli, M, &amp; Wiesinger, A. (Hg.): <title rend="italic">Datenflut und
                  Informationskanäle</title>. Innsbruck: Innsbruck UP (2014), 101-112. </bibl>
            <bibl xml:id="muhling2017" label="Mühling et al. 2017">Mühling, M., Korfhage, N.,
               Müller, E., Otto, C., Springstein, M., Langelage, T., Veith, U., Ewerth, R., &amp;
               Freisleben, B. <title rend="quotes">Deep learning for content-based video retrieval
                  in film and television production.</title>
               <title rend="italic">Multimedia Tools Appl.</title> 76(21) (2017):
               22169-22194.</bibl>
            <bibl xml:id="muller2016" label="Müller et al. 2016">Müller, E., Otto, C., &amp; Ewerth,
               R. <title rend="quotes">Semi-supervised Identification of Rarely Appearing Persons in
                  Video by Correcting Weak Labels.</title> In <title rend="italic">Proceedings of
                  the ACM on International Conference on Multimedia Retrieval</title>, New York, New
               York, USA (2016), pp. 381-384.</bibl>
            <bibl xml:id="muller2017" label="Müller et al. 2017">Müller, E., Springstein, M., &amp;
               Ewerth, R. <title rend="quotes"><q>When Was This Picture Taken?</q> - Image Date
                  Estimation in the Wild.</title> In: <title rend="italic">Proceedings of the
                  European Conference on IR Research</title>, Aberdeen, UK (2017), pp.
               619-625.</bibl>
            <bibl xml:id="muller-budack2018" label="Müller-Budack et al. 2018">Müller-Budack, E.,
               Pustu-Iren, K., &amp; Ewerth, R. <title rend="quotes">Geolocation Estimation of
                  Photos Using a Hierarchical Model and Scene Classification.</title> In: <title
                  rend="italic">Proceedings of the European Conference on Computer Vision</title>,
               Munich, Germany (2018), pp. 575-592.</bibl>
            <bibl xml:id="nguyen2010" label="Nguyen et al. 2010">Nguyen, B. T., Laurendeau, D.,
               &amp; Albu, A. B. <title rend="quotes">A robust method for camera motion estimation
                  in movies based on optical flow.</title>
               <title rend="italic">IJISTA</title>, 9(3/4) (2010): 228-238.</bibl>
            <bibl xml:id="palermo2012" label="Palermo et al. 2012">Palermo, F., Hays, J., &amp;
               Efros, A. A. <title rend="quotes">Dating Historical Color Images.</title> In: <title
                  rend="italic">Proceedings of the European Conference on Computer Vision</title>,
               Florence, Italy (2012), pp. 499-512.</bibl>
            <bibl xml:id="parikh2010" label="Parikh and Zitnick 2010">Parikh, D. &amp; Zitnick, C.
               L. <title rend="quotes">The role of features, algorithms and data in visual
                  recognition.</title> In: <title rend="italic">Conference on Computer Vision and
                  Pattern Recognition</title> (2010), pp. 2328–2335.</bibl>
            <bibl xml:id="pause2016" label="Pause and Walkowski 2016">Pause, J. &amp; Walkowski, N.
               The Colorized Dead: Computerunterstützte Analysen der Farblichkeit von Filmen in den
               Digital Humanities am Beispiel von Zombiefilmen (2016). <ref
                  target="http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:kobv:b4-opus4-25910"
                  >http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:kobv:b4-opus4-25910</ref>
            </bibl>
            <bibl xml:id="phillips2006" label="Phillips et al. 2006">Phillips, P. J., Scruggs, W.
               T., O’Toole, A. J., Flynn, P. J., Bowyer, K. W., Schott, C. L., &amp; Sharpe, M. FRVT
               2006 and ICE 2006 Large-Scale Results (2006). </bibl>
            <bibl xml:id="ranjan2019" label="Ranjan et al. 2019">Ranjan, A., Jampani, V., Balles,
               L., Kim, K., Sun, D., Wulff, J. &amp; Black, M. J. <title rend="quotes">Competitive
                  Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow
                  and Motion Segmentation.</title> In: <title rend="italic">Proceedings of the
                  Conference on Computer Vision and Pattern Recognition</title> (2019),
               pp.12240-12249.</bibl>
            <bibl xml:id="rawat2017" label="Rawat and Wang 2017">Rawat, W. &amp; Wang, Z. <title
                  rend="quotes">Deep Convolutional Neural Networks for Image Classification: A
                  Comprehensive Review.</title>
               <title rend="italic">Neural Computation</title> 29(9) (2017): 2352-2449.</bibl>
            <bibl xml:id="redmon2016" label="Redmon et al. 2016">Redmon, J., Divvala, S. K.,
               Girshick, R. B., Farhadi, A. <title rend="quotes">You Only Look Once: Unified,
                  Real-Time Object Detection.</title> In: <title rend="italic">Proceedings of the
                  Conference on Computer Vision and Pattern Recognition</title>, Las Vegas, NV, USA
               (2016), pp. 779-788.</bibl>
            <bibl xml:id="redmon2017" label="Redmon and Farhadi 2017">Redmon, J. &amp; Farhadi, A.
                  <title rend="quotes">YOLO9000: Better, Faster, Stronger.</title> In: <title
                  rend="italic">Proceedings of the Conference on Computer Vision and Pattern
                  Recognition</title>, Honolulu, HI, USA (2017), pp. 6517-6525.</bibl>
            <bibl xml:id="redmon2018" label="Redmon and Farhadi 2018">Redmon, J. &amp; Farhadi, A.
                  <title rend="quotes">YOLOv3: An Incremental Improvement.</title> CoRR
               abs/1804.02767 (2018).</bibl>
            <bibl xml:id="ren2017" label="Ren et al. 2017">Ren, S., He, K., Girshick, R., B., &amp;
               Sun, J. <title rend="quotes">Faster R-CNN: Towards Real-Time Object Detection with
                  Region Proposal Networks.</title>
               <title rend="italic">Transactions on Pattern Analysis and Machine
                  Intelligence</title>, 39(6) (2017): 1137-1149. </bibl>
            <bibl xml:id="resig2014" label="Resig 2014">Resig, J. <title rend="quotes">Using
                  computer vision to increase the research potential of photo archives.</title>
               <title rend="italic">Journal of Digital Humanities</title>, 3(2) (2014): 33.</bibl>
            <bibl xml:id="rodenberg2010" label="Rodenberg 2010">Rodenberg, H.-P. <title
                  rend="quotes">Historischer Kontext und der zeitgenössische Zuschauer: Michelangelo
                  Antonionis ZABRISKIE POINT</title> (1969). In: Korte, Helmut (Hg.): <title
                  rend="italic">Einführung in die systematische Filmanalyse</title>. Berlin: Schmidt
               (2010), pp. 5-118.</bibl>
            <bibl xml:id="salt2006" label="Salt 2006">Salt, B . <title rend="italic">Moving into
                  Pictures</title>. London: Starword (2006).</bibl>
            <bibl xml:id="salt2009" label="Salt 2009">Salt, B. <title rend="italic">Film Style and
                  Technology: History and Analysis</title>. London: Starword (2009).</bibl>
            <bibl xml:id="schroff2015" label="Schroff et al. 2015">Schroff, F., Kalenichenko, D.,
               &amp; Philbin, J. <title rend="quotes">FaceNet: A unified embedding for face
                  recognition and clustering.</title> In: <title rend="italic">Conference on
                  Computer Vision and Pattern Recognition</title>, Boston, MA, USA (2015), pp.
               815-823. </bibl>
            <bibl xml:id="seo2018" label="Seo et al. 2018">Seo, P. H., Weyand, T., Sim, J., &amp;
               Han, B. <title rend="quotes">CPlaNet: Enhancing Image Geolocalization by
                  Combinatorial Partitioning of Maps.</title> In: <title rend="italic">Proceedings
                  of the European Conference on Computer Vision</title>, Munich, Germany (2018), pp.
               544-560. </bibl>
            <bibl xml:id="sidiropoulos2011" label="Sidiropoulos et al. 2011">Sidiropoulos, P.,
               Mezaris, V., Kompatsiaris, I., Meinedo, H., Bugalho, M., &amp; Trancoso, I. <title
                  rend="quotes">Temporal video segmentation to scenes using high-level audiovisual
                  features.</title>
               <title rend="italic">Trans. Circuits Syst. Video Technol.</title>, 21(8) (2011):
               1163–1177. </bibl>
            <bibl xml:id="sittel2016" label="Sittel 2016">Sittel, J. Die systematische Anwendung
               computergestützter Verfahren in der Filmwissenschaft (2016).<ref
                  target="https://www.glk.uni-mainz.de/files/2019/07/MASTERARBEIT-JulianSittel.pdf"
                  >https://www.glk.uni-mainz.de/files/2019/07/MASTERARBEIT-JulianSittel.pdf</ref>
            </bibl>
            <bibl xml:id="sittel2017" label="Sittel 2017">Sittel, J. <title rend="quotes">Digital
                  Humanities in der Filmwissenschaft.</title> In: <title rend="italic">ZfM 4</title>
               (2017), 472-489.</bibl>
            <bibl xml:id="sloetjes2008" label="Sloetjes and Wittenburg 2008">Sloetjes, H., &amp;
               Wittenburg, P. <title rend="quotes">Annotation by category – ELAN and ISO
                  DCR.</title> In: <title rend="italic">Proceedings of the 6th International
                  Conference on Language Resources and Evaluation</title> (2008).</bibl>
            <bibl xml:id="springstein2016" label="Springstein and Ewerth 2016">Springstein, M. &amp;
               Ewerth, R. <title rend="quotes">On the Effects of Spam Filtering and Incremental
                  Learning for Web-supervised Visual Concept Classification.</title> In: <title
                  rend="italic">ACM Int. Conf. on Multimedia Retrieval</title>, New York (2016), pp.
               377-380.</bibl>
            <bibl xml:id="szegedy2015" label="Szegedy et al. 2015">Szegedy, C., Liu, W., Jia, Y.,
               Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, &amp; V., Rabinovich, A.
                  <title rend="quotes">Going deeper with convolutions.</title> In <title
                  rend="italic">Proceedings of the Conference on Computer Vision and Pattern
                  Recognition</title> (2015), pp. 1–9.</bibl>
            <bibl xml:id="szegedy2016" label="Szegedy et al. 2016">Szegedy, C., Ioffe, S., &amp;
               Vanhoucke, V. <title rend="quotes">Inception-v4, Inception-ResNet and the Impact of
                  Residual Connections on Learning.</title> CoRR abs/1602.07261 (2016).</bibl>
            <bibl xml:id="taigman2014" label="Taigman et al. 2014">Taigman, Y., Yang, M., Ranzato,
               M., &amp; Wolf, L. <title rend="quotes">DeepFace: Closing the Gap to Human-Level
                  Performance in Face Verification.</title> In: <title rend="italic">Proceedings of
                  the Conference on Computer Vision and Pattern Recognition</title>, Columbus, OH,
               USA (2014), pp. 1701-1708.</bibl>
            <bibl xml:id="tseng2013a" label="Tseng 2013a">Tseng, C.-I. <title rend="quotes"
                  >Analyzing Characters’ Actions in Filmic Text: A Functional-Semiotic
                  Approach.</title>
               <title rend="italic">Social Semiotics</title> 23 (2013): 587–605.</bibl>
            <bibl xml:id="tseng2013b" label="Tseng 2013b">Tseng, C.-I. <title rend="italic">Cohesion
                  in film: Tracking film elements</title>. Basingstoke: Palgrave Macmillan
               (2013).</bibl>
            <bibl xml:id="tsivian2008" label="Tsivian 2008">Tsivian, Y. <title rend="quotes"><q>What
                     Is Cinema?</q> An Agnostic Answer.</title> In: <title rend="italic">Critical
                  Inquiry</title>. Vol. 34, No. 4, the University of Chicago Press (2008).</bibl>
            <bibl xml:id="tsivian2009" label="Tsivian 2009">Tsivian, Y. <title rend="quotes"
                  >Cinemetrics, Part of the Humanities’ Cyberinfrastructure.</title> In: Michael
               Ross, Manfred Grauer, Bernd Freisleben (eds.), <title rend="italic">Digital Tools in
                  Media Studies</title> 9, Bielefeld: Transcript Verlag (2009): 93-100.</bibl>
            <bibl xml:id="viola2004" label="Viola and Jones 2004">Viola, P. &amp; Jones, M. <title
                  rend="quotes">Robust Real-Time Face Detection.</title>
               <title rend="italic">Int. Journal of Computer Vision</title>, 57(2) (2004):
               137–154.</bibl>
            <bibl xml:id="wang2018" label="Wang and Deng 2018">Wang, M. &amp; Deng, W. <title
                  rend="quotes">Deep Face Recognition: A Survey.</title> CoRR abs/1804.06655
               (2018).</bibl>
            <bibl xml:id="weibel2008" label="Weibel 2008">Weibel, A. <title rend="italic">Spannung
                  bei Hitchcock. Zur Funktionsweise auktorialer Suspense</title>. Würzburg:
               Königshausen &amp; Neumann (2008).</bibl>
            <bibl xml:id="weibel2017" label="Weibel 2017">Weibel, A. <title rend="italic">Suspense
                  im Animationsfilm Band I Methodik: Grundlagen der quantitativen Spannungsanalyse.
                  Studienbeisipiel Ice Age 3</title>. Norderstedt: Books on Demand (2017).</bibl>
            <bibl xml:id="weyand2016" label="Weyand et al. 2016">Weyand, T., Kostrikov, I., &amp;
               Philbin, J. <title rend="quotes">PlaNet - Photo Geolocation with Convolutional Neural
                  Networks.</title> In: <title rend="italic">Proceedings of the European Conference
                  on Computer Vision</title>, Amsterdam, The Netherlands (2016), pp. 37-55.</bibl>
            <bibl xml:id="xiao2010" label="Xiao et al. 2010">Xiao, J., Hays, J., Ehinger, K. A.,
               Oliva, A., &amp; Torralba, A. <title rend="quotes">SUN database: large-scale scene
                  recognition from abbey to zoo.</title> In: <title rend="italic">Conference on
                  Computer Vision and Pattern Recognition</title> (2010), pp. 3485–3492.</bibl>
            <bibl xml:id="xu2016" label="Xu et al. 2016">Xu, J., Song, L., &amp; Xie, R. <title
                  rend="quotes">Shot boundary detection using convolutional neural networks.</title>
               In: <title rend="italic">Proceedings of the International Conference on Visual
                  Communications and Image Processing</title> (2016), pp. 1-4.</bibl>
            <bibl xml:id="yin2018" label="Yin and Shi 2018">Yin, Z. &amp; Shi, J. <title
                  rend="quotes">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and
                  Camera Pose.</title> In: <title rend="italic">Conference on Computer Vision and
                  Pattern Recognition</title> (2018), pp. 1983-1992.</bibl>
            <bibl xml:id="zhou2014" label="Zhou et al. 2014">Zhou, B., Lapedriza, A., Xiao, J.,
                Torralba, A., &amp; Oliva, A. <title rend="quotes">Learning Deep Features for Scene
                  Recognition using Places Database.</title> In: <title rend="italic">NIPS
                  Proceedings</title>, Montreal, Quebec, Canada (2014), pp. 487-495.</bibl>
            <bibl xml:id="zhou2017" label="Zhou et al. 2017">Zhou, T., Brown, B., Snavely, N. &amp;
               Lowe, D. G. <title rend="quotes">Unsupervised learning of depth and ego-motion from
                  video.</title> In: <title rend="italic">Conference on Computer Vision and Pattern
                  Recognition</title> (2017), pp. 6612-6619.</bibl>
            <bibl xml:id="zhou2018" label="Zhou et al. 2018">Zhou, B., Lapedriza, A., Khosla, A.,
               Oliva, A. &amp; Torralba, A. <title rend="quotes">Places: A 10 Million Image Database
                  for Scene Recognition.</title>
               <title rend="italic">IEEE Trans. Pattern Anal. Mach. Intell.</title> 40(6) (2018):
               1452-1464.</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
