<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en">"I was painted by...": A Case Study on the Use of CNNs for Image Classification in the Humanities </title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Marta <dhq:family>Kipke</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0003-0130-261X</idno>
                    <dhq:affiliation>Institut für Digital Humanities, Georg-August-Universität Göttingen</dhq:affiliation>
                    <email>marta.kipke@uni-goettingen.de</email>
                    <dhq:bio><p></p></dhq:bio>
                </dhq:authorInfo>
                
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Lukas <dhq:family>Brinkmeyer</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0001-5754-1746</idno>
                    <dhq:affiliation>Information Systems and Machine Learning Lab, Stiftung Universität Hildesheim</dhq:affiliation>
                    <email>lukas.brinkmeyer@ismll.de</email>
                    <dhq:bio><p></p></dhq:bio>
                </dhq:authorInfo>
                
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Martin <dhq:family>Langer</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0002-6109-4878</idno>
                    <dhq:affiliation>Institut für Digital Humanities, Georg-August-Universität Göttingen</dhq:affiliation>
                    <email>martin.langner@uni-goettingen.de</email>
                    <dhq:bio><p></p></dhq:bio>
                </dhq:authorInfo>
                
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Lars <dhq:family> Schmidt-Thieme </dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0001-5729-6023</idno>
                    <dhq:affiliation>Information Systems and Machine Learning Lab, Stiftung Universität Hildesheim</dhq:affiliation>
                    <email>schmidt-thieme@ismll.uni-hildesheim.de</email>
                    <dhq:bio><p></p></dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt><publisher>Alliance of Digital Humanities Organizations</publisher>
<publisher>Association for Computers and the Humanities</publisher>
            	
            	<!-- This information should be added when the file is created -->
                <idno type="DHQarticle-id">000789</idno>

            	
            	<!-- This information will be completed at publication -->
                <idno type="volume"><!-- volume number, with leading zeroes as needed to make 3 digits: e.g. 006 --></idno>
                <idno type="issue"><!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date><!-- include @when with ISO date and also content in the form 23 February 2024 --></date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
<!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->                    
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            	<taxonomy xml:id="project_keywords">
            		<bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
            	</taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->
                	
                	<!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp=""/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item></item>
                    </list>
                </keywords>
            	<keywords scheme="#project_keywords">
            		<list type="simple">
            			<item></item>
            		</list>
            	</keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
        	<!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
        	<change>The version history for this file can be found on <ref target=
        		"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000789/000789.xml">GitHub
        	</ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
               
                <p>EGRAPHSEN is a case study on image classification in the humanities, specifically on painter attribution on Attic vase paintings. 
                    The goal of this study was to explore the new perspective that artificial intelligence (AI) can provide us with when we study traditional methods and heterogenous domains. 
                    When we translate the task (painter attribution), we have to consider the idiosyncrasies of the data domain (Attic vase paintings). This is challenging for both, classical archaeologists and computer scientists. 
                    In this paper, we address how to approach the challenges in the creation of the dataset. We carefully selected and prepared the data, 
                    reflected on potential biases and trained a convolutional neural network (CNN) accordingly. Specifically, we developed sampling criteria to 
                    combat the biases and a hierarchical labelling system to segment the images into details. Our model architecture was designed to 
                    process sets of images instead of only one individual image, which enables us to experiment with different combinations of image segments. 
                    This forms the basis for an analysis framework, which allows us to go beyond mere painter attribution and to explore the ambiguity of 
                    image similarity itself.
                </p>
            </dhq:abstract>
            <dhq:teaser>
                <p>In this case study the authors explore Attic vase painters using CNNs, investigating image similarity through both archaeological and computational perspectives.</p>
            </dhq:teaser>
        </front>
         <body>
            <head></head>
                <div>
                    <head>1. Introduction</head>
                    <p>When visitors look at a painting in a museum, they might expect a clear answer to the question “who painted it?”. Often a painting contains a signature, which makes the answer to this question seem pretty obvious. 
                        However, the answer is not always straightforward. Sometimes the painter cannot be pinpointed to one specific individual, and our knowledge might be limited to a timeframe, a region, a workshop. Even well-known paintings were not always signed. 
                        A good example is the <title rend="italic">Sleeping Venus</title>. It was first thought to be from the school of <name>Titian</name> and was later identified to be by <name>Giorgione</name> <ptr target="#anderson1996"/>; <ptr target="#lüdemann2005"/>; <ptr target="#ugloq2014"/>. 
                        Identifying a painter is a riddle, waiting to be solved by Art Historians and Classical Archaeologists. In our project EGRAPHSEN 
                        
                        <note>This work was supported by the Lower Saxony Ministry for Science and Culture (MWK), Germany, and was funded through the programme “<title rend="italic">Geistes- und Kulturwissenschaften – digital</title>”. This project was a cooperation between the Institute for Digital Humanities Göttingen, 
                            Germany and the Information Systems and Machine Learning Lab (ISMLL), Stiftung Universität Hildesheim, Germany.</note>
                        
                        we view this question from the perspective of artificial intelligence (AI): Can AI help solving this riddle? What does this reveal about the task itself? And more importantly: How can artificial intelligence help us go beyond that and answer fundamental questions regarding
                        image similarity and variation? How exactly does an artificial neural network differ from a human scholar? Painter attribution – the task of identifying painters on unsigned images – 
                        is a complex field with many differing opinions, which poses a challenge to the operationalisation of the task from the data selection up to the interpretation of the results. 
                    </p>
                    
                    <p>The overall situation is complicated: It was not always common practice to sign paintings. And even a famous painter may have left some paintings unsigned, for reasons which we can only speculate about <ptr target="#hegener_horsthemke2013"/>; <ptr target="#burg2007"/>. 
                        Nor is an existing signature necessarily a confirmation of authorship, as it may have been forged for profit <ptr target="#jones_craddock1990"/>. Due to atelier workflows we cannot take real historical signatures at face value either: 
                        Was it the actual painter who signed a work, or the leader of the workshop <ptr target="#liedtke2004"/>, <ptr target="#büttner2017"/>? Overall, painter attribution can be understood as a tool which paves the way for further research on historical contexts, 
                        workshop processes and style in general.
                    </p>
                    
                    <p>In EGRAPHSEN we approach this task at the very core: Our main focus lies in understanding painter attribution as a method, especially its manifestation in Classical archaeology. Our use case are Attic vase paintings from Greek antiquity, 
                        which pose an even greater challenge than later canvas paintings, as the former are only rarely signed at all. With this, other hints to solving this riddle become increasingly important, 
                        which in this case are primarily found in the visual comparison of stylistic phenomena. By studying the artists styles a scholar can <quote rend="inline">develop an eye</quote> for the respective artists. 
                        As fundamental as this method is, this skill is subjective and not always verifiable. The expert’s eye has to be trained and for an untrained eye the arguments might seem elusive and presumptuous, begging the question if they hold any value at all. 
                        Throughout the times the value of connoisseurship has been questioned in both Art History and Classical Archaeology (for the classical archaeological perspective see: <ptr target="#graepler2016"/>; <ptr target="#sparkes2013" loc="90–113"/>. 
                        The discussion is extensive and while Painter attribution has its defenders <ptr target="#oakley1998"/>, it is still controversial and challenging to navigate. Despite of this, painter attribution is still practiced in major publications recently.
                        
                        <note>For example, in the Corpus Vasorum Antiquorum (CVA, <hi rend="italic">Corpus Vasorum Antiquorum</hi> 1922-2023), which is a major project cataloguing 
                            vases of museums all over the world.</note>
                        
                        The insight it offers is too valuable, making it a good use case to revise and discuss traditional methods through the lens of a novel computational approach. 
                    </p>
                    
                    <p>With this, we are also researching the role of AI in in humanities disciplines, including its metrics and its dangers. Humans are complex, as is humanities research and the material we study. Can a computer <emph>understand</emph> humans or art? 
                        This is a controversial question, which leads to heated discussions between computer scientists and art historians <ptr target="#wasielewski2023"/>. Our work adds a case study which emphasizes the complexity of humanities research questions 
                        and the necessity of an informed and reflective approach, shaping the data compilation closely to our research questions. 
                    </p>
                    
                    <p>Our use case is particularly suitable because of its dual nature: On the one hand it is strictly about shapes and lines: the style of a painting. We call them <hi rend="bold">formal characteristics</hi>, with <quote rend="inline">form</quote> meaning the lines and shapes, 
                        the design, arrangement and style. This describes a phenomenological approach to images that should not be confused with formalism as a scholarly direction of the late 20th century <ptr target="#pinotti2012"/>. 
                        These characteristics ultimately help answer the question: <quote rend="inline"><hi rend="bold">How</hi> is a painting painted?</quote> in contrast to <hi rend="bold">what</hi> is painted. It seems that neither a human nor a computer needs additional information beyond the painting itself to be able to <quote rend="inline">learn</quote> connoisseurship. 
                        On the other hand, painter attribution includes subjectivity and visual association and therefore uncertainty, fuzziness and ambiguity. These concepts are difficult to grasp with computational methods, but they are essential for the method of 
                        painter attribution as well as many research questions in the humanities in general. At first glance, it seems that painter attribution could be easily translated into a machine learning classification set-up, with the different painters representing the classes. 
                        However, human connoisseurship is inherently instinctive and algorithms require specificity. A neural network needs distinct and well-defined categories to function, but the data is subject to biases and distortion. 
                        In the end, additional information beyond the painting might prove more important than one might anticipate.
                    </p>
                    <p>This research promises new insights on both sides: As Classical Archaeologists we re-evaluate our view on painter attribution and the underlying concept of image similarity through the lens of AI, while Computer Scientists re-evaluate their algorithms using our complex and heterogenic data. 
                        Thus, our research goal is as follows: We aim to evaluate and develop methods for image-related research questions with the help of artificial intelligence. We do not aim at recreating painter attribution through computational means; instead we re-evaluate it critically and beyond its surface. 
                        In EGRAPHSEN we developed methods to (1) compile our data in a meaningful way and (2) explore image similarity with computational means. In this paper we are going to present the first part of our research and provide a preliminary insight into our results.
                    </p>
                    
                    <p>Working with AI presents major challenges when it comes to the translation of humanities knowledge into data – and conversely, it is also difficult to generate knowledge from experiment results. For the purpose of this paper, our research question can be divided into the following sub-questions:
                        <list type="unordered">
                            <item>How do we operationalize the traditional method painter attribution and how does it relate to the algorithms used?</item>
                            <item>What biases must be taken into account when compiling the dataset, and to what extent can we control the input in order to find an adequate data representation for the purpose of attribution?</item>
                        </list>
                        Overall, these questions set the baseline for the interpretation and exploration of the model in regards to uncertainty and fuzziness of visual references and relations. 
                    </p>
                    <p>In a first step we need to contextualize our research questions, both for the traditional method (2.1) as well as in regards to other computational painter attribution projects (2.2) 
                        and machine learning in general (2.3). Doing so, we want to demonstrate the methodological demands of the task. The second step concerns the sampling strategies on different levels of input – 
                        the selection of painters (3) and the selection of input segments (4). After a short interim conclusion (5), we present our experimental set-up and model architecture (6), where we outline the main challenges of the algorithms. 
                        We then provide a first analysis of our results, which outlines our model’s performance and first insights into the similarity networks between the painters (7). 
                        A final conclusion explores possibilities for further research, including an outlook on the analysis part of our work (8).
                    </p>
                   </div>
             <div>
                 <head>2. Painter Attribution in Context</head>
                 <div>
                     <head>2.1 Beazley and Painter Attribution in Classical Archaeology</head>
                     <p>In order better to understand the skill of connoisseurship and the visual relations between the paintings and their styles, we first need to outline the traditional method. How do Classical Archaeologists attribute painters on Attic vases?
                     </p>
                    
                     <p>We study an era so long ago that some aspects of the culture have been distorted and lost. Yet others have been carefully preserved. The ancient world has fascinated many people throughout
                         the times, sometimes even taking on an exemplary character. This includes all kinds of cultural property: history, philosophy and literature. As Classical Archaeologists we study the appearance, execution, creation and contexts of <hi rend="italic">material culture</hi>, 
                         developing methods to describe the developments of imagery, objects and style. For this purpose, we have to correlate them with each other, which results in intricate chronological and typological systems (on the dating of vases specifically see <ptr target="#schmidt2022"/>. 
                         One interest of painter attribution lies in the creation of such an overarching system based on individual style, but also covering chronological aspects. 
                     </p>
                    
                     <p>The interest in the objects themselves is manifold due to the insight they give us into the realities and ideas of life in ancient Greek society. One aspect is their use in everyday life, for example as grave goods <ptr target="#tsingarida2009"/>. Another aspect concerns the imagery they depict. 
                         The themes range from mythological stories to portrayals of everyday Athenian citizens <ptr target="#bérard_vernant1985"/>. This gives us an insight into the values, ideas and culture of that time. Research on Attic vase paintings is vast, 
                         as witnessed by numerous introductory handbooks on the subject <ptr target="#vierneisel1990"/>; <ptr target="#robertson1992a"/>; <ptr target="#scheibler1995"/>; <ptr target="#boardman2001"/>; <ptr target="#mannack2012"/>; <ptr target="#cook2013"/>; <ptr target="#sparkes2013"/>. 
                         Another interest lies in artisan production and craftsmanship <ptr target="#coulson_etal1997"/>; <ptr target="#palagia_oakley1997"/>; <ptr target="#oakley2014"/>; <ptr target="#eschbach_schmidt2016"/>. 
                     </p>
                    
                     <p>While some artists’ names, for example those of sculptors, are known from historical sources
                         
                         <note>Pausanias for example, described and named many sculptors in his travel report on Greece <ptr target="#pausanias1973"/></note>
                         
                         , we do not know much about vase painters historically. Sometimes, however, there is a Greek name painted on a vase, accompanied by the phrase EΓΡAΦΣEN (I was painted by), 
                         meaning that the person with that name painted that particular vase <ptr target="#osborne2010"/>; <ptr target="#pevinck2010"/>; <ptr target="#hurwit2015"/>. These signatures were tempting clues that fuelled scholars to study the painters of the vases as if they were great masters. 
                         The deep appreciation for the paintings and the challenge of analysing their stylistic details led to decades of research on painter attribution in 19th-20th century Classical Archaeology <ptr target="#schmidt2016"/>. The question <quote rend="inline"><hi rend="italic">Who painted this vase?</hi></quote> became hard to resist.
                     </p>
                     
                     <p>One particular scholar, <name>Sir John Davidson Beazley</name> (1885-1970), emerged as an authority in this field. <name>Beazley</name> developed his skills at the height of painter attribution research on Attic vases during the first half of the 20th century <ptr target="#rouet2001"/>. 
                         But his method is often retraced to earlier studies of Renaissance paintings and the specific approach developed by the art historian and doctor of human anatomy <name>Giovanni Morelli</name> (1816-1891, see Kurtz 1986). <name>Morelli</name> theorized about painter attribution and developed a specific approach for this task. 
                         It was his approach, that led to the <title rend="italic">Sleeping Venus</title> being recognized as a painting by <name>Giorgione</name>. <name>Morelli</name> prided himself on being a anatomist and his method consisted of a systematic and refined comparison of anatomical details between the alleged paintings of an artist. 
                         Specifically those details that might seem insignificant at first glance turn out to be particularly important, because this is where the individuality of the artists is most clearly expressed <ptr target="wollheim1973"/>; <ptr target="#ginzburg_davin1980"/>; <ptr target="#vakkari2001"/>; <ptr target="#hinojosa2009"/>; <ptr target="#uglow2014"/>. 
                         Regardless of whether <name>Beazley</name> actually followed <name>Morelli</name> specifically, it is conceivable that Attic vase painters could be distinguished by such characteristics as well. The craftsmen produced multiple paintings per day and were routinely handling their tools, making shapes flow easily from their hands.
                     </p>
                     
                     <p>By studying the signed works of a painter, scholars were able to grasp a painter’s <emph>handwriting</emph>, with details like eyes, hands or ankles being the <emph>letters</emph>. Looking at a signed cup (Figure 1 
                         
                         <note>For each vase painting we discuss we are giving first the BAPD and then the museum and the inventory number or the number in the monograph where the vase was published. The BAPD number is the identifier in the online available Beazley Archive Pottery Database (<ref target="https://www.carc.ox.ac.uk/carc/pottery">https://www.carc.ox.ac.uk/carc/pottery</ref>). </note>
                         
                         ), we are able gain a first impression of the painter <name>Douris</name>. He is one of the painters with the most signatures and as such his style 
                         is well studied and established <ptr target="#buitron_oliver1995"/>. Looking at the pictures, we are often drawn to the faces first. It is possible to discern the similarity between eyes or mouths, for example. Their context is also important: How are the features arranged in the face? How big or small are they? 
                         Are they scrunched together or more spaced out? With this, we look beyond mere <emph>letters</emph>, we also consider the context; the words they are used in. Scholars have of course examined not only faces, but other body parts as well. In fact, the monograph on Douris features a section on the renderings of 
                         larger body parts of naked male figures, such as the chest and the legs <ptr target="#buitron_oliver1995"/>. 
                     </p>
                     
                     <p>Studying the paintings in great detail gives a good impression of a painter’s idiosyncrasies. But it also reveals the <hi rend="bold">variation</hi> of their ‘handwriting’. <name>Beazley</name> was one of the first to shift the focus from the signed works and to unsigned works on a larger scale <ptr target="#neer1997"/>; <ptr target="#sparkes2013"/>; <ptr target="#oakley2009"/>; <ptr target="#pérez2018"/>; <ptr target="#driscoll2019"/>. 
                         Not only was he able to identify unsigned paintings from known painters, he even identified painters with no known signatures at all. He named them after a topic they painted (e. g. <quote rend="inline">painter of the Paris Gigantomachy</quote>) or the city of the museum with their most well-known work (e. g. “Berlin Painter"). 
                         This way, he created an extensive structure with hundreds of painters. 
                     </p>
                     
                     <figure>
                         <head>Faces from a signed cup by Douris. By studying the details and their variations scholars like Beazley are able to
                         recognize the painters handwriting. BAPD 275972, Los Angeles Getty Museum of Art, 86.AE.29</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/fig1.jpg"></graphic>
                     </figure>
                     <!-- figire 1 -->
                     
                     <p>To understand how he was able achieved that, we turn our attention to one of these painters. Figure 2 shows a few examples of the way the Berlin Painter drew faces. <name>Beazley</name> was especially fascinated by this painter, as attested by several publications <ptr target="#beazley1922"/><ptr target="#beazley1930"/>; <ptr target="#beazley1911"/>. In the first publication he describes the linework of 
                         a handful of vases in great detail. He not only looked at faces, but placed great importance on the rendering of the body and especially the drapery of the clothes.
                         
                         <note><quote rend="inline"><hi rend="italic">I would draw attention to the bounding lines of the breasts, with the curvilinear triangle at the pit of the stomach; to the omission of the off clavicle; to the line of the hither clavicle, recurving at the pit of the neck without touching the median line of the breast; to the curved line which runs down from about hem-way along 
                             the line of the clavicle, separating shoulder and breast; to the smaller arc in the middle of the deltoid; to the indication of the trapezius between neck and shoulder; to the pair of curved lines on the upper right arm; to the projection of the wrist when the position of the hand requires it; to the two brown lines on the neck, 
                             indicating the sterno-mastoid; to the marking on the body between the lower boundary of the breast and the himation; to the form of the black lines indicating the ankle; to the pair of brown lines running from each ankle up the leg; to the forward contour of left leg and knee showing through the himation; in the himation, 
                             to the peaked folds on the left upper arm, the loose fold in the region of the navel, and the triangle where the inside of the garment shows at the shoulder</hi></quote><ptr target="#beazley1922"/>.</note>
                         
                         A combination of all the features, including their deviations, led him to the conclusion that these particular vases must come from one hand. 
                         He writes <quote rend="inline"><hi rend="italic">A <hi rend="bold">system</hi> so definite, coherent, distinctive, and in some respects so wilful, is most easily intelligible as a personal system" and "the child, above all else, of one man's brain and will</hi></quote> <ptr target="#beazley1922"/>. He describes the painters’ style as <quote rend="inline"><hi rend="bold"><hi rend="italic">system of renderings</hi></hi></quote>, which helps to understand his approach: 
                         <name>Beazley</name> compared not only stylistic details, but their overall interconnection, allowing him to not only identify known painters, but systems themselves. Beazley trained his eye well during his long-lasting career and sometimes he changed his mind about certain attributions. 
                         The more pictures he studied, the better his connoisseurship became. 
                     </p>
                     
                     <p><name>Beazley</name>’s success was unparalleled and many of his observations, especially those regarding the Berlin Painter, still hold value today <ptr target="#sparkes2013"/>; <ptr target="#oakley2017"/>. His most important contribution remain the volumes of his Attic vase painter series, where he simply lists hundreds of thousands of vases, sorted by painter <ptr target="#beazley1963"/>; <ptr target="#beazley1956"/>. 
                         His approach itself became subject of study as well <ptr target="#kurtz1986"/>; <ptr target="#neer1997"/>. This demonstrates the ambivalent relationship of Classical Archaeology towards painter attribution.
                     </p>
                     
                     <!-- figure 2 -->
                     <figure>
                         <head>Cut out faces from vases attributed to the Berlin Painter.
                             Upper row: 201879B (New Haven, Yale University 1913.133), 201836A (London, British Museum 1843,1103.51, Image Credit: © The Trustees of the British Museum CC BY-NC-SA 4.0, Assetnumber: 1613148360), 201811B (New York, Metropolitan Museum 56.171.38), 275093 (New York, Metropolitan Museum 65.11.12 )
                             Lower row: 275093 (New York, Metropolitan Museum 65.11.12 ), 201836B (London, British Museum 1843,1103.51, Image Credit: © The Trustees of the British Museum CC BY-NC-SA 4.0, Assetnumber: 1613148365) 201829B (London, British Museum 1843,1103.74, Image Credit: © The Trustees of the British Museum CC BY-NC-SA 4.0, Assetnumber: 283160001), 202004 (New York, Metropolitan Museum 22.139.32)
                         </head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure2.jpg"></graphic>
                     </figure>
                     
                     <p>While <name>Beazley</name>’s contribution to Classical Archaeology is undeniable, he and especially his successors were harshly criticized, especially in the 1990s and early 2000s. Many aspects, but also the very existence of this approach were criticized for many reasons, some of them valid, others just coming from a different place of interest <ptr target="#sparkes2013"/>; <ptr target="#driscoll2019"/>; <ptr target="graepler2016"/>; <ptr target="#arrington_2017"/>. 
                         Some of the criticism is aimed at the language of painter attribution, which can come across as imprecise and judgemental. Beazley's early observations on the development of <name>Douris</name> for example reads as follows: <quote rend="inline"><hi rend="italic">Douris the youth [meaning in his earlier period] is a lively and graceful character: Douris the man is scrupulously neat, highly accomplished, sleek, decent and dull</hi></quote> <ptr target="#beazley1918" loc="97"/>. 
                         Calling a painter dull might seem insulting, but for us, even such statements are useful, as we can still extract formal observations from them. The term “dull” might indicate a certain repetitive nature in his works, which is a valuable observation on the variation of the renderings.
                     </p>
                     
                     <p><name>Beazley</name>’s extensive work is fundamental for our own research, but not because we want to re-attribute works or re-analyse the works of certain painters. We are first and foremost interested in understanding these “systems of renderings” as a tool for categorizing visual similarity. At its core, <name>Beazley</name>'s method is comparing one system of renderings against another, 
                         considering the variation within a painter’s hand. In the end, it comes down to two questions: How much can a style vary to still belong to one painter’s hand and how do we interpret specific kinds of similarity between paintings?
                     </p>
                 </div><!-- div 2.1 -->
                 
                 <div>
                     <head>2.2 Painter Attribution: A Classification Task</head>
                     <p>Connoisseurship is often criticized for focusing too much on the shapes in an image, instead of its content. However, this is actually an advantage for machine learning purposes, since a strictly descriptive approach is something an algorithm can usually easily process. So of course, we are the first to attempt and reflect painter attribution with computational methods. Most such attempts
                         are rooted in Art Historical research of later time periods however, which come with different approaches to data and painting styles. Paintings in later periods are usually canvases covered in brushstrokes. Although vase painters used a brush as well, usually only the contours of the figures are painted and there are rarely large areas covered in a close pattern of brushstrokes. 
                         In this sense they are closer to drawings than post-antique paintings. Apart from this, the task of painter attribution is the same in its basic structure. 
                         
                         <note>The only other study of machine learning methods on Attic vase paintings is the one conducted by <name>Adrian J. Ryan</name> <ptr target="#ryan2009"/>. His approach differs greatly from ours however, as he focused on only one painter and artificially enhanced his dataset, 
                             rendering it useless for the analysis of style.</note>
                         
                         Therefore, other projects might offer useful insights into sampling strategies and other challenges. 
                     </p>
                     
                     <p>There have been large-scale approaches to  train classifiers on individual or periodical style based on large datasets like WikiArt as early as 2011 (for a review see <ptr target="#santos_etal2021"/>. However, as <name>Bell</name> and <name>Offert</name> pointed out, the definition of style in such projects is not always in line with the definitions by Art Historians and Classical Archaeologists <ptr target="#bell_offert2021"/>. 
                         This is a phenomenon apparent in many projects, but it becomes less prevalent the more precise the research questions are and the stronger they connect to traditional Art History research. Such in-depth projects often revolve around painters who either have controversial bodies of work or who are subjected to forgeries due to their popularity. 
                         While forgery detection is a common goal for many scholars in that field <ptr target="#levy_etal2014"/>; <ptr target="#narvaez_etal2022"/>, there are also examples of more profound Art Historical research questions beyond that, for example on <name>Leonardo da Vinci</name> and the heavily restored <name>Salvador Mundi</name> <ptr target="#lewis2019"/>; <ptr target="#frank_frank2021"/>; <ptr target="#langmead_etal2021"/>; on the <name>Salvador Mundi</name> see: <ptr target="#lewis2019"/> 
                         or <name>Vincent van Gogh</name> and his distinct brushstroke patterns (<ptr target="#li_etal2011"/>; <ptr target="#folego_etal2016"/>.
                     </p>
                     
                     <p>Despite varying degrees of success and methodological discussion, these approaches show that painter attribution translates especially well into supervised machine learning (SML) algorithms. It is a classification problem and SML algorithms treat given problems as a classification task. SML includes many methods, from statistical approaches to deep learning <ptr target="#elmrabet_etal2021"/>. 
                         For the purpose of image pattern recognition, deep convolutional neural networks (CNN) prove to be the most successful approach.
                     </p>
                     
                     <p>In SML, a model is trained on a set of labelled data to recognize patterns and make predictions on new, unseen data. In our case, the labelled data would be a set of images of ancient vases with known painter attribution, and the model would be trained to recognize the stylistic characteristics that distinguish each painter's work. Once the model is trained, it can be used to analyse new images of 
                         vases and predict the most likely painter based on the similarities with the previously labelled data. In order to ensure that the neural network is accurately identifying the work of individual painters and not just picking up on general trends, it is important to test the model's ability to generalize. This can be done by withholding a portion of the images from the training set and using them to test 
                         the model's ability to correctly identify the painter. If the model is able to accurately identify the painters based on images it has not seen before, it suggests that the distinct classes are representative of stylistic similarities. The first key challenge stemming from this, is the model’s frame of reference. Everything it learns is framed through the class selection and has the be interpreted in relation to that. 
                         Therefore, selection of classes plays a major role in our approach, as elaborated in section 3.
                     </p>
                     
                     <p>Once verified through a model performance, the next step is to understand how the model is making its decisions. This process is often interpretative, as it involves inferring the decision-making processes from the classifications. However, there are also methods that provide insight into the model itself, for example with the use of heat maps <ptr target="#selvaraju_etal2017"/>; <ptr target="#seifert_etal2017"/>. 
                     </p>
                     
                     <p>Our focus lies not in explaining the decisions behind our model in detail, but the implications they have for our own analysis. For this purpose, the experiments have to be as transparent as possible from the beginning. We can achieve this by controlling our dataset and the receptive field of the classification task. Limiting and controlling the input, for example by omitting or including specific details, 
                         controls the model’s decision-making from the beginning, rather than explaining it after training and validation. Since the painter attribution proves to be focused on the details of the paintings, laying a strong focus on them in the training process is not only preferable, but necessary.             
                     </p>
                     
                     <p>This has been a key challenge for other projects as well. Some solutions include an automatic segmentation into tiles, as performed by <name>Frank</name> and <name>Frank</name> <ptr target="#frank_frank2020"/>. They use the segments with the highest amount of information for their training. Another approach includes brushstroke-segmentation, which works particularly well for painters like <name>van Gogh</name>, who has a very distinct brushstroke-pattern <ptr target="#li_etal2011"/>. 
                         Both these approaches are not suitable for our goal of painter attribution. The lines of vase paintings do not contain repetitive patterns such as <name>van Gogh</name>. Additionally, we are looking for more complex <emph>combinations</emph> of brushstrokes which constitute the motifs: Comparing hands to hands, eyes to eyes. Other scholars, such as <name>Bell</name> and <name>Offert</name> <ptr target="#bell_offert2021"/> and <name>Langmead</name> et al <ptr target="#langmead_etal2021"/> acknowledge this as well. 
                         This is why they use pre-existing models such as keypoint RCNN or the Google API for a semantic segmentation via object detection and face recognition. These pre-trained models are trained on real-world data however and do not work well with vase paintings. So instead of these approaches, we choose to annotate the images manually. Our annotation strategy is presented in section 4. 
                     </p>
                     
                 </div> <!-- div 2.2 -->
                 
                 <div>
                     <head>2.3 Functionality of Convolutional Neural Networks (CNNs)</head>
                     <p>Lastly, we want to give some technical background on the functionality of CNNs, because they also present us with certain specifics to consider in our data compilation. CNNs have become the standard for image classification tasks in the recent years, with various models and model architectures being proposed (introductory works see <ptr target="#oshea_nash2015"/>; <ptr target="#venkatesan_li2017"/>; 
                         <ptr target="#khan_etal2018"/>; <ptr target="#li_etal2021"/>. The success of AlexNet (<ptr target="#krizhevsky_etal2012"/> and the introduction of ImageNet <ptr target="#deng_etal2009"/>, the first large-scale annotated image dataset, have played a significant role in promoting the use of CNNs for image classification. 
                     </p>
                     
                     <p>Since then, various extensions have been proposed to enhance the representational power of convolutional architectures. 
                         
                         <note>One common approach is to increase the depth of the network by adding more layers and parameters <ptr target="#simonyan_zisserman2014"/>. By doing so, the receptive field, which represents the region of the input image that influences a given feature in the network, can be enlarged. This is achieved by using smaller kernels per convolution, allowing the network to capture both fine-grained and global information. In addition to depth, other components have been introduced to improve the performance of convolutional networks. 
                             For example, the inception module <ptr target="#szegedy_etal2015"/> incorporates multiple filter sizes within a single layer, enabling the network to capture diverse feature scales effectively. Another notable advancement is the introduction of residual connections (He et al. 2016) and densely connected blocks <ptr target="#huang_etal2017"/>. These techniques address the challenge of training very deep networks which are much harder to optimize and subsequent prone to becoming stuck in the optimization process.</note>
                         
                         At its core, a neural network is a complex mathematical model compromised of multiple layers. Each layer is a parameterized mathematical function. In the case of CNNs, the defining layer is the convolutional layer, where the mathematical operation of the same name is used. 
                         In this operation the input image is combined with a set of filters, resulting in feature maps. From a human point of view these feature maps look like a filtered image, with specific characteristics emphasized. By using different filters, the neural network can recognize patterns and determine them as features for the specific class. This is the same concept used in Gaussian blurring and edge detection. 
                         Using a blurring filter the CNN would recognize features regarding the overall shapes, while an edge detecting filter would emphasize the lines. By stacking multiple convolutional layers, the network can learn representations of increasing complexity. Convolutional layers are usually coupled with a pooling layer to condense the features in the feature maps, 
                         and an activation function to introduce non-linearity to the model.               
                         
                         <note>One common example and the activation function used throughout this work is the Rectified Linear Unit (ReLU) activation <ptr target="#agarap_etal2018"/> which is represented by: <hi rend="italic">f(x)</hi> =max⁡(0,<hi rend="italic">x</hi>).</note>
                         
                     </p>
                     
                     <p>When we train a neural network, our goal is to find the best parameters for the functions in the layers by making them fit our training data as closely as possible. We have a multitude of parameters across the layers, making it impossible to compute an exact solution. This is where
                         backpropagation comes into play. These iterative technique helps us fine-tune the network's parameters, by gradually reducing prediction errors. 
                         
                         <note>The optimization is achieved through the gradient descent method, with the equation: <formula>\(θ_new=θ_old-a⋅∇Jθ\)</formula>; where <hi rend="italic">θ</hi> represents the model's parameters, α is the learning rate which controls the step size of the iterative updates, and ∇J(θ) is the gradient of the loss function with respect to the parameters.</note>
                                                  
                         During training, the neural network learns and optimizes the parameters, such as weights and biases, that can then be applied to new, unseen data. 
                         To put it simply, it basically involves backtracking through the model and optimizing the parameters to reduce errors in the classification.
                     </p>

                     <p>The theoretical basis of CNNs has a major repercussions for our research: Everything in an image is fair game, which, as widely known, creates a perceptual bias in the model <ptr target="#offert_bell2021"/>. Because the convolution is applied on the whole image, global properties are more dominant <ptr target="#islam_etal2021"/>, 
                         making overall textures easier to learn than specific details.
                    </p>
                 
                 </div><!-- 2.3 -->
             </div> <!-- div 2 -->
             
             <div>
                 <head>3. Painter Selection and Biases</head>
                
                <div>
                    <head>3.1 Biases in the Image Selection</head>
                    <p>As the first step of our data compilation strategy, we want to address the class selection. Selecting the classes for our use case means choosing which painters we want to use as examples in our study. 
                        Furthermore, it also involves the question which paintings to choose and which painter attributions by which scholars to consider when assigning a specific painting to a class. 
                        Overall, our painter selection is influenced by both humanities and machine learning factors, with archaeological interests being the leading force. 
                        For the algorithms to work, some statistical requirements have to be considered as well: 
                    </p>
                    
                    <p>First, the algorithms call for a sufficient number of samples per painter, which should also be in a similar range for all painters to avoid a skewed distribution. Second, we have to keep our dataset diverse enough in every aspect that may possibly be relevant, otherwise the model will not be able to generalize to unseen images. 
                        This means that we have to choose paintings representative for the variety within a class. Creating such a dataset is not a problem for most machine learning applications as long as they work with real-world data. It is always easy to produce more data to balance or enlarge the dataset. With vase paintings however, 
                        we ‘only’ have the preserved images and simply cannot create more. Projects in the Digital Humanities often deal with the bottleneck of too few or too diverse data. In other projects on vase paintings there have been attempts to enhance the datasets artificially, for example by transferring the style of vase paintings 
                        onto real-world images and therefore creating more images in the same style <ptr target="#madhu_etal2022"/>. <name>Adrian Ryan</name> even drew new paintings by his own hand <ptr target="#ryan2009"/>. However, we have refrained from artificially enhancing our dataset, because it usually introduces new diversity into the data which might create another bias. 
                        Also, our research question is focused on individual renderings, which cannot be re-created faithfully. So at least for the initial set-up, we had to focus on painters with a naturally large body of work. 
                    </p>
                    
                    <p>However, this requirement raises another important question: By which criteria does a painting belong to a class? As discussed above, painter attribution on Attic vases is often done ambiguously and intuitively, making it difficult to objectify the attributions and put them into distinct classes. 
                        <name>Beazley</name>’s attributions are so vast in number that they offer a solid basis for machine learning algorithms. Nonetheless, not all of his attributions are undisputed. 
                        
                        <note>A good example for a dispute among connoisseurs is the highly controversial cup in the Agora Museum in Athens (<hi rend="bold">202142</hi>, P24113). For the different opinions see <ptr target="#robertson1958"/>, <ptr target="#cardon1979"/> and <ptr target="#kurtz1983a"/>. In other cases scholars focusing on specific painters and their œuvre also re-attribute works by <name>Beazley</name>, for example 
                            <name>John H. Oakley</name> for the <name>Phiale Painter</name> (<hi rend="bold">214303</hi>, Gela, Museo Archeologico N66 <ptr target="#oakley1980" loc="Nr. 122"/> or <name>Diana Buitron-Oliver</name> for the <name>Painter of London</name> E 55 (<hi rend="bold">205103</hi>, Florence, Museo Archeologico Etrusco  V48 <ptr target="#buitron_oliver1995" loc="Nr. E1 117"/>.</note>
                        
                        The only factual evidence nobody can dispute are signatures. 
                        They are the real ground truth, even if we add the attributions of scholars who have a life-time of visual experience. Also, we do not want to limit ourselves to merely replicating Beazleys approach. We wanted to study connoisseurship and vase paintings in general. 
                        So, we also had to consider the number of signatures and potential controversies in regarding the painter’s bodies of works to avoid too much disagreement within the classes. Nevertheless, we did include some controversial paintings in the dataset, because we wanted to study how they would perform. 
                        They could give us an insight into which features might make it hard for human scholars to agree on an attribution and therefore add to our understanding of the method.
                    </p>
                    
                    <p>Adding to these criteria, Attic vase paintings also confronts us with inherent biases. They are a complex medium with dense information, pertaining to both formal characteristics and content. We are mostly interested in the  formal characteristics, as interpretative information is not relevant for painter attribution. 
                        The categories can correlate however. To identify biases in this labyrinth of features, we defined different levels of formal characteristics, going from general and external to specific and deep. General characteristics are, for example, the composition of a painting, or its motif. 
                        These are dominant characteristics that take up a lot of literal space in the painting. A deep characteristic, for example, is the personal style, which is found within the details in specific areas of the painting. Such details might be overshadowed by more general characteristics and potentially be “buried” underneath them.
                    </p>
                    
                    <p>Above these levels of image-inherent characteristics, there is an even more general level. It concerns external criteria, such as the image properties and image quality: If the model is expected to perform on images with different angles, lighting conditions and other variations, then these factors need to be accounted for in the training data as well. 
                        So, the images we chose had to be diverse in this respect as well, which is further limited by the natural occurrence of vase painting publications, meaning printed books and online databases. We included scanned and de-rasterized photographs from printed publications, photographs on Wikimedia commons (<ref target="https://commons.wikimedia.org">https://commons.wikimedia.org</ref>) 
                        and photographs from major museum databases online, especially the British Museum in London (<ref target="https://www.britishmuseum.org/collection">https://www.britishmuseum.org/collection</ref>), the Metropolitan Museum in New York (https://www.metmuseum.org/art/the-collection), the J. Paul Getty Museum in  Los Angeles (<ref target="https://www.getty.edu/art/collection/">https://www.getty.edu/art/collection/</ref>). While keeping the dataset diverse, 
                        it is also important not to oversample images with certain characteristics for a specific painter, as they contain easy-to-learn textural qualities. If we chose only medium quality scans of book publications for one painter and high-resolution camera pictures from Wikimedia for another, the model would generalize unwanted features. 
                        Thus, it is important to consider the overall quality within the classes carefully to ensure that it is representative of the dataset and free from external biases. 
                    </p>
                    
                    <p>Going deeper into the levels of image information we can proceed from the more general to the more detailed levels. For a human it is easier to look for specific details, because we can control our visual attention. A neural network on the other hand, might get stuck on the more general shapes if they are generalizable, due to the nature of convolution. 
                        This makes it even more urgent that we carefully consider these levels and their connections between each other. Sometimes, they are hard to separate, with the following characteristics being worth exploring further:
                        
                        <list type="unordered">
                            <item>The first characteristic concerns the context of the painting, meaning its placement and size. Vases offer many differently shaped surfaces for painting, which can affect the basic structure of the picture. Again, as with handwriting, the linework might vary depending on the available space and on the vessel’s curvature. 
                                If a painter preferred certain shapes, this might lead to an unwanted bias.</item>
                            <item>The next characteristics include motivic features, for example composition,  scene, motif or the topic depicted. Those characteristics are often the first thing we notice in a picture, even if we reduce the depiction significantly. 
                                A neural network might learn those features before it learns anything else. If a painter always prefers certain motifs, the neural 
                                network might generalize this characteristic over the personal style, due to the space they take up in a painting.</item>
                            <item>The final characteristic is found at the same “depth” as personal style: The Z<hi rend="italic">Zeitstil</hi> (period or temporal style). As in all time periods, certain visual features are typical – "in fashion" – for their time. A good example of this is the clothing and the way the folds are painted. They are very distinctive in different time periods <ptr target="#langlotz1920"/>. 
                                Both of these stylistic features are contained in the same details, which makes it hard to separate them.</item>
                        </list>
                    </p>
                    
                    <p>Temporal style and personal style get confused sometimes and a study including different time periods might lead to a classification based on art periods or technical differences, rather than personal styles. The model might perform its task correctly at first glance, but the features it learns are not the ones intendent. 
                        Still, the temporal bias is the easiest to resolve, at least in our initial set-up: By focusing on only one art period with its immediate predecessors and successors we can eradicate a potential temporal bias and teach the neural network to recognize subtle differences between personal styles instead. 
                        This approach was also considered in some of the projects mentioned above <ptr target="#frank_frank2022" loc="840"/>; <ptr target="#bell_offert2021"/>. 
                    </p>
                    
                    <p>The time frame we choose is the early 5th century BC, the late Archaic / early Classical times. 
                        The production of vase paintings in Athens spans over several centuries, with many stylistic and technical developments. Our chosen time frame takes place after the transition from black-figure to red-figure technique. 
                        In black-figure technique the figures were applied in a translucent slip onto the terracotta surface, which became black after firing the vessel. The details were then etched on with a sharp tool. 
                        Red-figure vases are the opposite: The vase was covered in black slip and the figures are blank areas that appear in the red colour of the terracotta. The contours and details are painted with a brush before firing the vase 
                        <ptr target="#noble1988" loc="99–117"/>. In our chosen time-period, the red-figure style had just become firmly established, while the generation before that is considered more experimental <ptr target="#boardman_1975" loc="89–91"/>. 
                        It is also a time frame where it was common to sign the vases, which offers a good foundation for our dataset as well <ptr target="#bolmarcich_muskett2017"/>; <ptr target="#sapirstein2013"/>.
                    </p>
                    
                    <p>The vase size and shape, as well as the depicted motif, pose a challenge that cannot be solved this easily, because they may differ from painter to painter, even within a certain period. Between these two, size and shape are easier to control: 
                        They represent parameters that depend on the kind of vase onto which the image was painted on. According to <name>Beazley</name>, vase painters of that period can be roughly divided into cup painters and pot painters <ptr target="#beazley1961"/>. <hi rend="italic">Cups</hi> in this terminology means smaller vessels 
                        from which one can drink, for example kylikes (Figure 3). They have a round, shallow body and are decorated on the curved outside as well as the plane inside. Meaning there are two kinds surfaces with different curvatures. But there are also other vases, such as skyphoi, 
                        that are closer in shape to the kind of cups we use today. <hi rend="italic">Pots</hi> include large vessels, often with a large body, a small foot, and a shoulder connecting to the neck of the vessel. At the top they are either curved inwards, or curved outwards. 
                        The body is usually broad, offering a slightly curved surface for the paintings. However, the curvature at the top of the painting, and the size of the vessels differ greatly from pot-shape to pot-shape, as seen in the examples in Figure 4. 
                        There are also many exceptions, for example lekythoi, which have a narrow and high body, or decorative zones featuring smaller figures on the necks of larger vessels. Moreover, this distinction between cup and pot painters does not mean that 
                        they only painted these specific vessels. There are exceptions to the case, which adds to the variety within the classes. The cup painter <name>Douris</name>, for example, also painted a larger vessel, a psykter, as indicated by his signature (BAPD 205309, London, British Museum E768). 
                        Other pots can also be attributed to his body of work on a stylistic basis, such as an amphora in St. Petersburg (BAPD 205310, State Hermitage Museum B5576.) and another one in Paris (BAPD 205311, Louvre S3853).
                    </p>
                    
                    <p>Possible solutions against this bias would include focusing only on pot (or cup) painters or creating sub-classes per painter. Only focusing on one kind of painter would take too much complexity from the task, however, and due to the high variety and the uneven distribution of shapes, 
                        creating subclasses is impossible. This diversity of vase shapes causes a bias with unpredictable extent. Therefore, contrary to our approach with the temporal bias, we decided to include both pot and cup painters. 
                        We specifically included vessels representing exceptions to their usual preference of vase shapes, to emphasize the variation of linework in our dataset.</p>
                
                    <!-- figure 3 -->
                    <figure>
                        <head>: Cups by Makron. 
                            Top: BAPD 204734, New York Metropolitan Museum (MET) 06.1152; Bottom: BAPD 6917, MET 1979.11.9
                        </head>
                        <figDesc></figDesc>
                        <graphic url=" resources/images/figure03.jpg"></graphic>
                    </figure>
                
                    <p>A similar reasoning was used on the variety of the motifs. The depictions range from mythological fight scenes, to athletes doing sports, to women in their chamber. There is great variety in the topics depicted, but certain types of scenes or figures, such as mantle figures <ptr target="#franceschini2018"/>, are often reused. 
                        As with the vase shapes, it seems that certain painters preferred certain motifs. The Berlin Painter for example frequently depicted musicians with stringed instruments (<ptr target="#padgett2017" loc="4, 13, 15–16, 18–20, 28, 33, 39, 55"/>. This observation can lead to circular reasoning, with attributions being based on the motifs. 
                        Since the dataset is limited, we cannot simply avoid having too many examples of a painter's favourite motifs. We can only ensure as much diversity as possible per class. Another strategy against this bias consists of stripping the details from their compositional and motivic context by segmentation, as explained in Section 4.
                </p>
      
                </div><!-- 3.1 -->
                 
                 <div>
                     <head>3.2 Selection of Painters and their Circles</head>
                     <p>By reducing these biases, our data selection was shaped in a certain way, either by focusing on certain characteristic or by ensuring diversity so that the model would be able to generalize. The final selection was further determined by 
                         painters’ potential for answering research questions and uncovering similarities and connections between them. With all this in mind, we chose four painters from around 500 to 470 BC: The <hi rend="bold">Berlin Painter</hi> <ptr target="#beazley1963"/>; <ptr target="#padgett2017"/>, 
                         <name><hi rend="bold">Douris</hi></name> <ptr target="#beazley1963" loc="425ff"/>; <ptr target="#buitron_oliver1995"/>, <name><hi rend="bold">Makron</hi></name> <ptr target="#beazley1963" loc="458ff"/>; <ptr target="#kunisch1997"/> and the <hi rend="bold">Brygos Painter</hi> <ptr target="#beazley1963"/>; <ptr target="#pace2019"/>. Additionally, we also added <name><hi rend="bold">Oltos</hi></name> <ptr target="#beazley1963"/>; <ptr target="#bruhn1943"/>; <ptr target="#dehoffmeyer1943"/>. He was active a little earlier 
                         than the other painters and painted many cups, but also other vase shapes. His performance in training and evaluation should give us a deeper insight into the two potential temporal biases and vase shape biases throughout the experiments. 
                     </p>
                     
                     <!-- figure 4 -->
                     
                     <figure>
                         <head>Pots by the Berlin Painter. Top: Neckamphora: BAPD 201878, MET 07.286.69; BAPD 201837, Los Angeles Getty Museum of Art 86.AE.187; BAPD 201888, MET 41.162.17; Amphora: BAPD 201811, MET 56.171.38; 
                             Bottom: Stamnos: BAPD 201960, MET 1988.40; Hydria: BAPD 201987, MET 10.210.19; Oinochoe: BAPD 202004, MET 22.139.32; Lekythos: BAPD 202022, MET 21.88.163.</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure4.jpg"></graphic>
                     </figure>
                     
                     <p>Three of these painters - the Berlin Painter, the <name>Brygos Painter</name> and <name>Oltos</name> – were put into relation to other painters by different scholars. Sometimes, when talking how painters might relate to each other, 
                         scholars have been overly interpretative with their assertions based on visual similarity. To avoid misunderstandings regarding the interpretation of our data in relation to the history of research in Classical Archaeology, 
                         we have to be very careful in our choice of words. Firstly, we will use the word 'relationship' only when talking about clearly definable relationships between painters. To this, we add the terms “relate to” and “connection”. 
                         With connection we mean visual similarity, which includes the possibility that there might not be a historical relationship at all. Second, we call the three painters “core painters” (= training classes) and will describe their 
                         groups of proximal painters as a “circles” (= test classes). Beazley used the word “circle” to describe an extended range of relationships beyond workshops or groups <ptr target="#robertson1989"/>. In contrast, we use this term to include all 
                         kinds of relation(ship)s and connections that archaeological research claimed between certain painters and their styles of drawing. We neither agree nor disagree with this research and its interpretations. 
                         We simply use these circles to explore and further define the visual relations they are based on.
                     </p>
                     
                     <p>The three circles are based on different dimensions of argumentation. The first is the circle around <name>Oltos</name>. He signed his works frequently and many of his vases were also signed by their potters, which was used before to form a network of workshop relationships 
                         <ptr target="#cline_hasaki2019"/>. The second is the circle of the <name>Brygos Painter</name>. We do not know his real name, because there are no signed works. However, <name>Beazley</name> believed that he potentially worked together with a group of stylistically similar painters 
                         <ptr target="#beazley1963" loc="368, 400–405"/>. His assertions are supported by a potter debris found in Athens with fragments of some of the visually similar painters <ptr target="#tzachou_alexandri1968"/>; <ptr target="#baziotopoulou_valavani1994"/>; <ptr target="#monaco2000"/>; <ptr target="#maffre2001"/>; <ptr target="#williams2017" loc="168"/>. 
                         The fragments are also unsigned and it is therefore important to be wary of circular reasoning when discussing this group. Both Oltos and Brygos may give us insight into workshop processes and how the paintings of fellow craftsmen might relate to 
                         each other stylistically. 
                     </p>
                     
                     <p>The third circle surrounds the <name>Berlin Painter</name> and is based on purely stylistic observations. His circle is a prime example of historical interpretations of visual similarity, because it contains what connoisseurs label <quote rend="inline">teacher and student relationships.</quote> 
                         It is impossible to reconstruct such relationships based on straightforward evidence, instead it is based on the similarities between the painter’s styles and vase shapes <ptr target="#oakley1997" loc="97"/>. Hypotheses such as these have been criticized in Classical Archaeology, 
                         because the arguments are circumstantial and too much historical information is inferred from visual observations <ptr target="#neer2009" loc="45"/>. Nevertheless, we want to study the <name>Berlin Painter</name>’s circle in particular, because it represents the peak of connoisseurship 
                         intuition and association: Connoisseurs have seen distinct and nuanced similarities between the paintings that led them to label them with this specific interpretation. Our goal is to strip out all the historical assumptions and to re-examine said similarities. 
                         We will continue to use quotation marks when discussing their relationship, since they are only inferred. 
                     </p>
                     
                     <table>
                         <head><hi rend="italic">Painters in our data-set and number of instances. The arguments for their inclusion are based on a literature of supposed relations. 
                             For the <name>Brygos P.</name>, this includes fragments in the archaeological find and their attribution, for the <name>Berlin Painter</name> the labels given by research and for 
                             Oltos the number of signatures on the works by the same potters</hi>.</head>
                         
                         <row role="label">
                             <cell></cell>
                             <cell>Supposed Relationship</cell>
                             <cell>Instances</cell>
                         </row>
                         
                         <row role="data">
                             <cell><hi rend="bold">Berlin P.</hi></cell>
                             <cell><hi rend="bold">Core Painter</hi></cell>
                             <cell><hi rend="bold">230</hi></cell>
                         </row>
                         
                         <row role="data">
                             <cell>Providence P.</cell>
                             <cell>"Student"</cell>
                             <cell>84</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Achilles P.</cell>
                             <cell>"Student"</cell>
                             <cell>164</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Hermonax</cell>
                             <cell>"Student"</cell>
                             <cell>97</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Phiale P.</cell>
                             <cell>"Student" of the Achilles P.</cell>
                             <cell>150</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Oinokles</cell>
                             <cell>"Follower" of the Providence P.</cell>
                             <cell>48</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Phintias</cell>
                             <cell>"Teacher"</cell>
                             <cell>43</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Euthymides</cell>
                             <cell>"Teacher"</cell>
                             <cell>40</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Kleophrades P.</cell>
                             <cell>"Student" of Phintias and Euthymides</cell>
                             <cell>305</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Myson</cell>
                             <cell>"Student" of Phintias</cell>
                             <cell>42</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Persephone P.</cell>
                             <cell>"akin in spirit" to the Achilles Painter</cell>
                             <cell>35</cell>
                         </row> 
                     </table>
                     
                     <table>
                         <row role="label">
                             <cell>Additional Painters to diversify the data-set</cell>
                        <cell></cell>
                         </row>
                         
                         <row role="data">
                             <cell>Douris</cell>
                             <cell></cell>
                             <cell>208</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Makron</cell>
                             <cell></cell>
                             <cell>314</cell>
                         </row>
                     </table>
                     
                     <table>
                         <row role="label">
                             <cell></cell>
                             <cell>Shared Potters</cell>
                             <cell>Instances</cell>
                         </row>
                         
                         <row role="data">
                             <cell><hi rend="bold">Oltos</hi></cell>
                             <cell><hi rend="bold">Core Painter</hi></cell>
                             <cell><hi rend="bold">150</hi></cell>
                         </row>
                         
                         <row role="data">
                             <cell>Euphronios</cell>
                             <cell>2 (4 Signed, 8 Attributed)</cell>
                             <cell>94</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Epiktetos</cell>
                             <cell>2 (4 Signed, 3 Attributed)</cell>
                             <cell>119</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Onesimos</cell>
                             <cell>1 (1 Signed, 9 Attributed)</cell>
                             <cell>129</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Nikosthenes</cell>
                             <cell>2 (15 Attributed)</cell>
                             <cell>31</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Pistoxenos</cell>
                             <cell>1 (1 Attributed)</cell>
                             <cell>28</cell>
                         </row>
                     </table>
                     
                     <table>
                         <row role="label">
                             <cell></cell>
                             <cell>Supposed Relationship</cell>
                             <cell>Fragments</cell>
                             <cell>Instances</cell>
                         </row>
                         
                         <row role="data">
                             <cell><hi rend="bold">Brygos P.</hi></cell>
                             <cell><hi rend="bold">Core Painter</hi></cell>
                             <cell><hi rend="bold">✓ (14)</hi></cell>
                             <cell><hi rend="bold">197</hi></cell>
                         </row>
                         
                         <row role="data">
                             <cell>Briseis P.</cell>
                             <cell>"Mild-Brygan-Group"</cell>
                             <cell>✓ (6)</cell>
                             <cell>100</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Dokimasia P.</cell>
                             <cell>"Mild-Brygan-Group"</cell>
                             <cell>✓ (3)</cell>
                             <cell>62</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Foundry P.</cell>
                             <cell>Visually Related by Beazley</cell>
                             <cell>? (1)</cell>
                             <cell>120</cell>
                         </row>
                         
                         <row role="data">
                             <cell>P. of the Paris G.</cell>
                             <cell>Visually Related by Beazley</cell>
                             <cell>? (1)</cell>
                             <cell>75</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Onesimos</cell>
                             <cell>"Teacher" of the Foundry P.</cell>
                             <cell>? (1)</cell>
                             <cell>129</cell>
                         </row>
                     </table>
                     
                     <p>Table 1 shows the distribution of instances per painter in our dataset. An instance is a single figure in a painting. Asking <quote rend="inline">who painted this figure?</quote> instead of <quote rend="inline">who painted this vase?</quote> has two advantages: First, it allows us to disregard the overall 
                         image composition, therefore reducing the potential motif bias; and second, it leads to a more nuanced assessment of painter attribution and the role individual figures may play when the model attributes a painting. 
                     </p>
                     
                     <p>In total, we carefully curated a dataset consisting of 2785 individual figures extracted from 1353 vase paintings created by the five core painters along with 19 other painters in their associated circles. 
                         Our total dataset includes 37 painters, 9,362 figures, and a total of 428,960 annotated objects. The distribution displays a natural imbalance, with a significant variation in the number of existing paintings across different painters. 
                         This inherent imbalance poses potential challenges for the training process, particularly when it means that classes are underrepresented because they have a low number of paintings. 
                         Such underrepresentation may adversely affect the model's performance, especially when assessing these painters during the testing phase. Fortunately, there are various methods to improve training when dealing with an 
                         imbalanced data source, which we will describe in Section 6 below.
                     </p>
                 </div><!-- 3.2 -->
             </div><!-- 3 -->
             
             <div>
                 <head>4. Image Annotation and Segmentation</head>
                 <p>With the painters selected we can now move onto the input-segments. Segmenting the images means that our model is not trained on the single figure, but on the (combination of) details per figure. As discussed above, the established methods for automating semantic segmentation do not work for vase paintings. 
                     Instead, we opted for a semi-automated workflow with manually annotated images as the basis for our own object-detection model. Adding to the need of transparency, our strategy also aims at reducing the biases of motif and vase shape, thereby focusing on the details of a painting.
                 </p>
                 
                 <!-- figure 5 -->
                 <figure>
                     <head><hi rend="italic">The annotation process involves annotating the figures at different levels of detail. The parts the figures are connected via parent-child-connections (for example the flute belongs to the hand, the hand to the arm, the arm to the woman). 
                         Vase depicted: BAPD 204117, MET 24.97.28</hi></head>
                     <figDesc></figDesc>
                     <graphic url="resources/images/figure5.jpg"></graphic>
                 </figure>
                 
                 <p>For this purpose, we have developed annotation guidelines with a controlled vocabulary. It was important to have an agreement between the annotators on what exactly the labels represent and which part of the painting they encompass. It is also important that they do justice to the painting’s complexity and can be used not only our project, but also for future research. 
                     This requires detailed, comprehensive and explicit annotation guidelines, which we developed during our project. These are the three core principles:
                     
                     <list type="ordered">
                         <item>We want to compare different levels of detail, in which each label has to be connected to their corresponding higher-level-label (for example, an eye belongs to the head, a hand to an arm). For this, adapted a <hi rend="bold">hierarchical system</hi>, in which the annotations get more detailed with each successive level, while the segments stay connected via parent-child connections. 
                             Specifically, this means that when we annotate a figure, we begin with the highest level: The figure itself. This can be either a man, a youth, a woman or a creature. Then we annotate the main body parts of the figure (for example the head or the arm). These body parts then get divided into their sub-parts. For the arm this would be the upper arm, the forearm, the elbow and the hand 
                             (s. Figure 3) The same goes for clothes: First, the specific garment is named; then the drape of the hem folds and the surface folds. This way we have three levels of detail: The figure, the larger body parts and the smaller details.</item>
                         
                         <item>For this purpose, we need a controlled vocabulary with a focus on the formal characteristics of a painting. A neural network is unable to <emph>understand</emph> the image. It learns from our categories which therefore must be as clear as possible. This is an important consideration for our controlled vocabulary. On the one hand, it is composed of specific terms. 
                             These are usually the names of the body parts, but sometimes, if necessary, we used technical terms from Classical Archaeology, for example to describe the clothes. This is because the specific types of clothing come with the same folds within their class. A chiton has fine folds at the bottom, a himation has large folds. On the other hand, our vocabulary also goes beyond simply determining semantic segments: 
                             It also has a <hi rend="bold">visual component</hi>, describing the composition of brushstrokes associated with each label. The hip, for example, consists of two lines going from the lower side of a figure to the genitals. The shoulder (Figure 6) is described by the concave shape from the neck to the upper arm. The goal of the vocabulary is to clearly 
                             define the combinations of brushstrokes associated with each segment.</item>
                     
                         <item>Still, some segments vary depending on the context they of their depiction, even if they are clearly defined. Therefore, we had to include <hi rend="bold">additional information concerning the variation of the labels</hi>. 
                             This additional information had to follow certain rules as well, to avoid overlapping and mix-ups between the strict form and levels of additional information. For example, we annotated whether or not a figure or 
                             their body parts are clothed, because the folds of the clothing change the depiction in a specific way. We also added information on the action the figure or their body parts are performing. A closed hand is a different 
                             shape than an open hand, for instance. We also added some interpretative information, if it comes with specific shapes, such as the iconographical identification of gods or heroes. It is usually based on the figure’s attributes, 
                             and thus contains information about its shape: Athena usually wears a large helmet, Eros has wings, Herakles wears a lion’s skin, and so on. This way, a figure is identifiable using a collection of features 
                             within the image, which adds to the identification of a potential motif-bias.                              
                         </item>
                     </list>
                 </p>
                 
                 <p>Most of the annotated information was not specifically used for training, but was used for describing the shapes contained in an image beyond that. It also opens up the possibility for motif-dependent de-biasing both before and after training, for example if we decided to exclude all 
                     hands playing instruments from the dataset. This way we can exclude motif-heavy labels from the beginning, reducing the bias efficiently. We also wanted to pave the way for future research on the contents of the image, 
                     such as iconographical analysis, which greatly profits from such a detailed and universal annotation.
                 </p>
                 
                 <!-- figure 6 -->
                 <figure>
                     <head><hi rend="italic">Annotation of the shoulder. Our polygons encompass clearly defined accumulations of brushstrokes, which do not always align with anatomical definitions of body parts, but rather the domain specific depiction thereof. 
                         From left to right: BAPD 201878, MET 07.286.69; BAPD 204497, MET 06.1021.188; BAPD 213926, MET 17.230.13.</hi></head>
                     <figDesc></figDesc>
                     <graphic url="resources/images/figure6.jpg"></graphic>
                 </figure>
                 
                 <p>The annotation team consisted of student assistants with a background in Classical Archaeology. To avoid an annotator’s bias, we also split each painter between them. After annotating a large amount of images we used the manual annotations to implement our own object-detection model, which was trained to annotate the most common labels and also their the parent-child connections. 
                     We used our hierarchical system to make the process even more efficient: Our annotations are not nearly as numerous as real-world data. We resolved this problem by adding a control instance through a human annotator and initiating the detection in the same hierarchical order as our annotations: firstly, 
                     the human annotator selects the figure, which becomes the area for the regions-of-interest of the first hierarchical level (body parts). The process is then repeated for the sub-parts. This means, that the next level of annotation is only being searched for in the relevant regions, e. g. only looking for hands in the area of the arm. 
                     This made our detection very accurate despite the smaller training set <ptr target="#kipke_etal2022"/>.
                 </p>
                 
                 <figure>
                     <head><hi rend="italic">Frequency of the object classes for the segments extracted from each figure</hi></head>
                     <figDesc></figDesc>
                     <graphic url="resources/images/figure7.jpg"></graphic>
                 </figure>
                 
                 <p>For training and analysis, we used only the 21 most common object classes. We excluded the surroundings of the figures, as well as any objects they hold in their hands. Not only are those instances few, they also strongly indicate the motif of the image. In the end this annotation process resulted in a total of 45,442 components recorded across the paintings included in the experiments. 
                     This results in 16.3± 4.4 segments per figure (Figure 5).</p>
             </div><!-- div 4 -->
             
             <div>
                 <head>5. Interim Conclusion: What goes into the neural network?</head>
                 <p>In the end it comes down to the question of what we want the network to learn and which biases we plant into the training data, whether unknowingly or deliberately. This challenge begins with the choice of painters, continues in the selection of images and culminates in the segmentation of the individual images. In our selection we aimed at balancing potential biases using different strategies:
                 <list type="ordered">
                     <item><hi rend="bold">Limiting</hi> the painter selection to a specific characteristic (the training classes represent only two generations, which helped us to avoid a to temporal bias)</item>
                     <item><hi rend="bold">Maintaining</hi> heterogeneity with respect to the characteristics affected by potential biases on the same level as individual style, so as not to reduce the complexity of the task.</item>
                     <item><hi rend="bold">Reducing biases</hi> concerning full image characteristics (such as motif or composition) by <hi rend="bold">segmenting the image and excluding labels</hi> that carry characteristics of this bias 
                         (attributes, surroundings).</item>
                 </list>
                 </p>
                 
                 <p>It boils down to a question of representability. We want our data-set to represent the variation of the linework, while ignoring other image characteristics. Our decisions are based on archaeological criteria, but with machine learning algorithms in mind. This means that our data already underwent several layers of processing, which differentiates them from the original pictures a Classical archaeologist might look at:
                     <list type="ordered">
                         <item>The very photography of the vase is the first layer of distortion. By publishing vases as photographs, we are transforming the paintings from a three-dimensional surface into two dimensions, which results in details being distorted depending 
                             on the angle from which the picture was taken.</item>
                         <item>By sorting and compiling the data we contextualize the images in such a way, that the neural network has to assume common features within a class. This creates a framework, in which it can only “see” generalizable features of the images.</item>
                         <item>Our image annotation cuts each image into pieces. This way, we try to control which aspects add to the training of the model. We are trying to re-create the way an archaeologist is able to focus on certain details, even if it is just an approximation and not possible for every detail to the same extent.</item>
                     </list>
                 </p>
             </div><!-- div 5 -->
             
             <div>
                 <head>6. Experimental Set-Up</head>
                 <div>
                     <head>6.1 Experimental Details</head>
                     <p>With the data sampling laid down, we now come to our machine learning experiments. First, we want to elaborate on some of the experimental details, because they set the baseline for the analysis down the line. 
                         Addressing the imbalanced nature of our dataset, where the number of figures per painter varies drastically, we also employed standard data augmentation and overfitting-avoidance strategies.  
                         
                         <note>This involves creating modified versions of the images (such as rotations, translations, or zooming) to artificially expand and diversify the training dataset. Furthermore, samples were weighted in our loss function based on their class frequency, ensuring that underrepresented classes receive more attention during the training process. Finally, dropout was applied on the learned embeddings, a regularization technique where randomly 
                             selected neurons are ignored during training, which helps in preventing over-dependence on specific neurons and promotes generalization. It is also worth noting that the model is trained for 1000 epochs, and during each epoch, the model is presented with each figure in the training data. We implemented early stopping based on hold-out validation data, 
                             which is a technique that halts training when the model's performance ceases to improve on a set aside validation dataset, preventing potential overfitting to the training data.</note>
                         
                     </p>
                     
                     <p>To assess the performance of our models, we employed a 10-fold cross-validation methodology <ptr target="#berrar_etal2019"/>. This is a method to make use of the data in smaller datasets as efficiently as possible. In this approach, we divided the images of each painter into ten equal parts (folds). For each of the ten experiments, one fold from each painter was assigned to the test set, 
                         while the remaining data was used for training and validation. This way different parts of the dataset rotated through the training / validation / test split and added to the model’s ability to generalize. This means, that looking at each fold we get an insight in how well an instance performs not only as training, but also as test and validation data.
                     </p>
                     
                     <p>We also employed transfer learning techniques. In many use cases, the neural network is not trained from scratch. In the lower layers it learns basic shapes and edges, which stay the same between different data domains. That is why many projects rely on transfer learning, where a pre-trained neural network is adapted to another data domain. 
                         In our work we employed the large feature extractor ResNet-50, which is pre-trained on ImageNet <ptr target="#he_etal2016"/>. ResNet-50 is the appropriate size for our experiments, because smaller versions resulted in a loss of performance and larger versions did not offer a significant lift. 
                         On top of the ResNet-50 model, we added layers with our own classification task. The pre-trained model is subject to its own data biases, through the adaption to our own data domain they do become mostly irrelevant however. 
                     </p>
                 </div><!-- div 6.1 -->
                 
                 <div>
                     <head>6.2 Image and Image Set Classification</head>
                     <p>As discussed above, there are many arguments in favour of a model trained on the details of the paintings. However, to confirm the efficacy of this approach, we trained models on other potential scenarios as well. In this experiment, we distinguish three scenarios, each of which gave us unique insights into the process. These approaches differ in their methodology and underlying assumptions, providing us with a comprehensive understanding of the task at hand. 
                       Figure 9 shows examples of the three scenarios:
                 <list type="ordered">
                     <item>The first approach involves a prediction based on the complete vase painting images. This encompasses not only the figures but also the overall scene, motif, ornaments and parts of the vase. We wanted to include the biases discussed above to analyse how they affect the model. </item>
                     <item>In the second approach, our focus shifted to detecting the painter based on a single figure extracted from the entire painting. This approach aims to eliminate some of the biases such as figure’s surroundings, ornaments etc. We wanted to see what remains and how much this approach differs from (1) and (3), 
                         since it borrowed from both, but has the advantages of neither.</item>
                     <item>The third approach constitutes our preferred strategy. Here, instead of presenting the model with the figure as a single image, we represented it as a collection of its sub-components, such as the arm, hand, and eye.</item>
                 </list>
                     </p>
                     
                     <p>This meant that we worked not only with different data inputs, but also with different classification set-ups. While the first two scenarios follow standard image classification protocols
                     
                         <note>We are given a dataset <formula>\(D≔(x_1,y_1 )…,(x_N,y_N )\)</formula> sampled from an unknown distribution <hi rend="italic">p</hi> that consists of Npairs of an image <formula>\(x∈R^(H×W×C)\)</formula>, with height H, width Wand C channels, and target <formula>\(y∈R^L\)</formula> for a problem with L distinct classes. The channels typically follow the RGB format. 
                             Given a loss function <formula>\(l:R^NxL×R^NxL→R\)</formula>, we wanted to learn a model <formula>\(y ̂:R^(N×H×W×C)→R^NxL\)</formula> with minimal expected loss over the data: <formula>\(E_((x,y)∼p) l(y,y ̂(x))\)</formula>.</note>, 
                      
                      the third scenario comes with the additional challenge of controlling the segments. The complexity of this task calls for an extension of the standard set-up to handle a <hi rend="bold">varying number of images</hi>, with each image representing a segment of the original painting. While the overall objective remained the same, our model now had to process sets of images, while maintaining invariance to the order and number
                         of elements in each set.
                     </p>
                     
                     <p>Recent studies have investigated the concept of invariant and equivariant layers from various perspectives. Maron et al. <ptr target="#maron_etal2018"/> introduced a methodology that involves a set of learned basis functions, while the set-transformer approach <ptr target="#lee_etal2019"/> employs straightforward self-attention mechanisms. <name>Zaheer</name> et al. proposed equivariant and invariant layers capable of operating on a set of elements by building upon the Kolmogorov-Arnold representation 
                         theorem as discussed by <ptr target="#zaheer_etal2017"/>. This theorem asserts that any multivariate continuous function ϕ can be expressed as a finite composition of continuous functions of a single variable and the binary operation of addition. 
                         
                         <note>Consequently, this insight leads to the development of the following formalized deep-set layer <formula>\(ϕ(x)=g(1/I ∑_(i=1)^I▒f(x_i ) )\)</formula>, where an inner function <hi rend="italic">f</hi> is applied to each element of the set x, before the outer function g is applied to the aggregation of all inner embeddings. This formalization is permutation-invariant to the order of 
                             elements in x due to the summation, while representing a simple architecture for an image-set classification task.</note>  
                         
                         This means specifically, that our network consists of two components. The first component functions as an inner function f. After aggregating the images through summation, 
                         a second model g consisting of four dense layers is employed to compute the painter prediction for the entire set. Thus, the networks f and g are trained on the precomputed embeddings of our paintings to classify the actual painter. In this study, we have chosen to utilize a deep sets architecture due to its inherent simplicity, which makes it suitable for addressing problems characterized by limited available data.  
                     
                         <note>Related work and benchmarks include the following: <ptr target="#liu_etal2019"/>; <ptr target="#afrasiyabi_etal2022"/> (similarity recognition and few-shot classification); <ptr target="#maron_etal2020"/> and <ptr target="#zhong_etal2018"/> 
                             (similarity recognition and few-shot classification).</note>
                     </p>
                     
                     <!-- figure 8 -->
                     <figure>
                         <head><hi rend="italic">Example of the three different scenarios for processing the full painting, a single figure, and the set of annotations. Vase depicted: BAPD 201960, MET 1988.40</hi></head>
                         <figDesc></figDesc>
                        <graphic url="resources/images/figure8.jpg"></graphic>
                     </figure>
                     
                     <p>It is also important to note that the experiments involving the full figure and the figure set employed the same split of figures, while the full image scenario involved a different random split across all paintings. Additionally, we ensured that figures or paintings from the same vase were not present in both the training and test sets simultaneously. 
                         Apart from these distinctions, the general experimental set-up and pre-processing across these three variants remained consistent.                    
                </p>
                 </div><!-- 6.2 -->
                 
                 
            <div>
                <head>6.3 Pre-Processing and Image Distortion</head>
                <p>Another point to address is the input distortion. Not only are the images scaled down for training, they have to be normalized to a fixed aspect ratio as well. Since we relied on a pre-trained deep feature extractor, the size of the input image is already predetermined to be 224 x 224 px by the pretraining dataset, even if we annotated high resolution images. 
                    Operating on full high-resolution images, which can be up to 4000 x 4000 px, would limit the number of images we can process simultaneously, potentially slowing or even impairing the training process.</p>
                    
                <!-- figure 9 -->
                <figure>
                    <head><hi rend="italic">The arm and hem folds scaled (left) and padded (right). With scaling the linework gets distorted significantly, especially for narrow and long labels. Left: BAPD 200751, MET 09.221.47; Right: BAPD 201811, MET 56.171.38</hi></head>
                    <figDesc></figDesc>
                    <graphic url="resources/images/figure9.png"></graphic>
                </figure>
            
                <p>Although these methods are advantageous due to their simplicity and low computational cost, they have the drawback that they distort the image significantly. Every segment is scaled without respect to their ratio, making small details larger and vertically compressing labels such as the arms or hem folds (Figure 8). While this distortion is considered negligible for standard computer vision tasks <ptr target="#hashemi2019"/>, it was crucial to investigate this factor in our research, since the specific linework observed is assumed to have a significant impact on painter attribution. 
                    Does the contortion pose a disadvantage for the task or is the model able to compensate for it?
            </p>
                
                <table>
                    <head><hi rend="italic">Comparison of the performance between padded images and scaled images. 
                        Results are given as accuracy aggregated across the 10-fold cross-validation</hi></head>                    
                    <row role="label">
                        <cell>Accuracy with standard deviation training with….</cell>
                    </row>
                    
                    <row role="label">
                        <cell></cell>
                        <cell>...paded images</cell>
                        <cell>...scaled images</cell>  
                    </row>
                    
                    <row role="data">
                        <cell>Berlin P.</cell>
                        <cell>61.7 ± 5.1</cell>
                        <cell>71.3 ± 4.2</cell>
                    </row>
                    
                    <row role="data">
                        <cell>Brygos P.</cell>
                        <cell>54.3 ± 5.9</cell>
                        <cell>53.3 ± 5.6</cell>
                    </row>
                    
                    <row role="data">
                        <cell>Douris</cell>
                        <cell>50.8 ± 8.7</cell>
                        <cell>62.1 ± 7.5</cell>
                    </row>
                    
                    <row role="data">
                        <cell>Makron</cell>
                        <cell>66.5 ± 8.9</cell>
                        <cell>78.9 ± 5.2</cell>
                    </row>
                    
                    <row role="data">
                        <cell>Oltos</cell>
                        <cell>56.1 ± 2.8</cell>
                        <cell>54.7 ± 9.1</cell>
                    </row>
                                        
                    <row role="data">
                        <cell>Average</cell>
                        <cell>58.9 ± 2.9</cell>
                        <cell>66.3 ± 3.4</cell>
                    </row>          
                </table>
                
                <p>We tested this by training three models on different pre-processing scenarios. In the first variation we kept the distorting normalisation, in the second scenario we scaled the image to their longest side and added black padding to the empty space around it (Figure 8). In the third variation we refrained from up-scaling smaller segments, as this results in different line thicknesses for different labels depending on their size. As expected, however, the third scenario did not 
                    create satisfactory results due to the small size. The comparison between the first two other scenarios can be found in Table 2. 
                    Contrary to our concerns, the distorted images do in fact offered better results. This may be because scaling the images maximizes the input 
                    information, while padding the images adds large areas of black background, which results in unused space. 
                    In the case of the hem-folds and the arm the distortion might even be helpful. For the hem-folds it emphasizes the linework and rhythm of the folds and for the arms the focus 
                    lies on the direction changes of the lines rather than their length.
                </p>

                <p>Additionally to the input-distortion, the images are often naturally distorted due to their curvature. Especially the hydria in Figure 4 shows clearly, how some paintings are difficult to photograph. 
                    This difficulty is specific to certain vase shapes and brings us back to the problem of a potential vessel shape bias.
            </p>
                
                <p>The problem of vase painting photography has been addressed on other occasions and with un-warping-algorithms (f. e. <ptr target="#felicisimo_etal2019"/>). However, they add another level of distortion in the pre-processing level, skewing the dataset even further. Due to the heterogeneity of our 
                    image selection concerning vase shapes and the focus on details rather than the whole vase, it would be impossible to create a workflow that addressed all of them. The findings of the padding vs. scaling experiment support our decision, as distortion 
                    in itself does not seem to affect the outcome of the experiment negatively, except for a potential shape bias. However, this should of course continue to be tested in further research and the methods should be adjusted if necessary.</p>
            </div>  <!-- div 6.3 --> 
                 
                 <div>
                     <head>6.4	Robustness</head>
                     <p>Finally, we also want to address about model robustness. “Robustness”, in this context, refers to the stability and consistency of a model's predictions when trained multiple times. Neural networks exhibit both determinism and non-determinism in their behaviour. 
                         When fully trained, (non-bayesian) neural networks operate deterministically, consistently producing the same outputs for a given input, ensuring their reliability. However, since the parameters of a neural network are typically initialized randomly before training, 
                         model performance might vary across multiple training runs, even with the same architecture and data. It is important to quantify this variation to ensure that the results hold ground, not only regarding the robustness of classes, but also the single instances.</p>
                 
                 <table>
                     <head><hi rend="italic">Standard deviation over the test-training folds and over then re-trainings with the same set-up</hi></head>
                     <row role="label">
                         <cell></cell>
                         <cell></cell>
                         <cell>Standard deviation over…</cell>
                     </row>
                     
                     <row role="label">
                         <cell></cell>
                         <cell>Accuracy</cell>
                         <cell>...folds</cell>
                         <cell>...runs</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Berlin P.</cell>
                         <cell>73.9</cell>
                         <cell>± 4.2</cell>
                         <cell>± 1.5</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Brygos P.</cell>
                         <cell>61.5</cell>
                         <cell>± 5.6</cell>
                         <cell>± 3.0</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Douris</cell>
                         <cell>58.6</cell>
                         <cell>± 7.5</cell>
                         <cell>± 3.2</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Makron</cell>
                         <cell>79.3</cell>
                         <cell>± 5.2</cell>
                         <cell>± 1.9</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Oltos</cell>
                         <cell>65.3</cell>
                         <cell>± 9.1</cell>
                         <cell>± 2.5</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Average</cell>
                         <cell>67.7</cell>
                         <cell>± 6.32</cell>
                         <cell>± 2.42</cell>
                     </row>
                 </table>
                     
                     <table>
                         <head><hi rend="italic">Classification accuracy of each painter along with the 2nd and 3rd most frequently assigned classes</hi></head>
                         <row role="label">
                             <cell></cell>
                             <cell>Most frequent class</cell>
                             <cell>Second frequent class</cell>
                             <cell>Third frequent class</cell>
                         </row>

                    <row role="data">
                        <cell>Berlin P.</cell>
                        <cell>Berlin P. (73.9%)</cell>
                        <cell>Brygos P. (11.4%)</cell>
                        <cell>Makron (8.1%)</cell>
                            </row>   
                         
                         <row role="data">
                             <cell>Brygos P.</cell>
                             <cell>Brygos P. (61.5%)</cell>
                             <cell>Makron (9.9%)</cell>
                             <cell>Douris	 (8.7%)</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Douris</cell>
                             <cell>Douris (58.6%)</cell>
                             <cell>Makron (13.8)</cell>
                             <cell>Brygos	(11.3%)</cell>
                         </row>
                         
                         
                         <row role="data">
                             <cell>Makron</cell>
                             <cell>Makron (79.3%)</cell>
                             <cell>Brygos P. (14.7%)</cell>
                             <cell>Berlin	 (7.1%)</cell>
                         </row>
                         
                         <row role="data">
                             <cell>Oltos</cell>
                             <cell>Oltos 	(65.3%)</cell>
                             <cell>Berlin (7.8%)</cell>
                             <cell>Brygos	 (7.0%)</cell>
                         </row>
                     </table>
                     
                     <p>This investigation primarily focused on two key dimensions: First, the ability of the model's predictions to withstand repeated executions on identical but permuted inputs; and second, the possible alignment of a high confidence level in the model's class scores with the robustness of its predictions. 
                         For this purpose, we repeated our main experiment with the same 10-fold cross-validation image sets 10 times. We then analysed the specific class assignments across these 10 runs. 
                         As shown in Table 4, the accuracy over runs varies only up to ± 2.42%, meaning our best model achieves an overall accuracy of 70.12% ± standard deviation.</p>
                 
                 <!-- figure 10 -->
                     <figure>
                         <head><hi rend="italic">Class distribution over 10 repeated training runs with random weight initialization. The scatter plots show the relation between mean assigned class probability for the correct class of an instance and its standard deviation</hi></head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure10.jpg"></graphic>
                 </figure>
                 
                     <p>Two more experiments give us a better insight into the different aspects of the robustness. First, we examined the instances for each painter and analysed the correlation of the standard deviation and prediction value. The prediction value is the raw model output and represents the level of certainty for the attribution of an instance. 
                         The scatter plot in Figure 10 shows the mean of the assigned class probability over the 10 repeated runs for each figure, plotted against the standard deviation over these runs. It shows a curve, in which a high mean probability corresponds with a low standard deviation. Similarly, a low score also corresponds with a low standard deviation, 
                         meaning strong predictions come with a high confidence and predictions in the middle field tend to vary over the runs. This suggests that the predictions with a very high, but also with very low prediction values are especially stable, meaning that they might provide good case studies for studying painter attribution.
                 </p>
                     
                     <p>This became even more apparent looking at each assigned class ordered by its frequency. For this, we analysed the class prediction with the highest probability and evaluated the frequencies over assigned class: Did the model assigned the same class 10 out of 10 times, or did the final prediction change frequently? The boxplot in Figure 11 shows these frequencies. 
                         The mean/centre of each boxplot corresponds to the relative frequency over predicted classes. Over 10 runs, the model would assign the most frequent class for an instance 70% of the time; conversely, the second most frequent class was assigned at a rate of only 20%.                      </p>
                 
                 <!-- figure 11 -->
                     <figure>
                         <head><hi rend="italic">Class distribution over 10 repeated training runs with random weight initialization. Left to right shows the boxplot over the relative frequency of the class assigned the most, then second most, etc., up the class least likely to be assigned.</hi></head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure11.jpg" style="height: 700px"></graphic>
                     </figure>
                 
                     <p>This means that the model is fairly sure about the overall classification, even when it misclassifies an instance and, in that regard, it is stable when making attributions, even if they are wrong. In further analysis, the second highest class might still be relevant, as it attests which painters get confused more easily than others. 
                         The second and third frequent classes can be found in Table 5. Overall, these experiments add an important perspective for the analysis of peculiar attributions, especially if they are confidently wrong. </p>
                 
                 </div><!-- 6.4 -->
             </div><!-- div 6 -->
             
             <div>
                 <head>7. A First Look at the Results</head>            
                 
                 <div>
                     <head>7.1	Image and Image-Set Classification in Comparison</head>
                     <p>Finally, we turn to the results of our experiments, starting with the comparison of training scenarios regarding the input images (table 3). The difference in accuracy between full vase paintings segment sets is surprisingly close. 
                         Considering the fact that the data is highly complex and that the task requires deep and specific knowledge (in human experts as well) the numbers up to 70% accuracy with 5 classes is within expected parameters. 
                         The full figures have an accuracy about 17% lower than the other scenario.                     
                 </p>
                     
                           <table>
                               <head><hi rend="italic">Accuracy with standard deviation of each scenario described above in Section 5.2. 
                                   Results are given as accuracy aggregated across 10 folds.</hi></head>
                               <row role="label">
                                   <cell></cell>
                                   <cell>(1) Full Image</cell>
                                   <cell>(2) Single Figure</cell>
                                   <cell>(3) Segment-Sets</cell>
                               </row>
                               
                               <row role="data">
                                   <cell>Berlin P.</cell>
                                   <cell>86.6 ± 6.6</cell>
                                   <cell>56.9 ± 2.7</cell>
                                   <cell>73.9 ± 4.2</cell>
                               </row>
                           
                     <row role="data">
                         <cell>Brygos P.</cell>
                         <cell>56.6 ± 5.5</cell>
                         <cell>47.2 ± 3.6</cell>
                         <cell>61.5 ± 5.6</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Douris</cell>
                         <cell>55.7 ± 3.0</cell>
                         <cell>42.8 ± 5.1</cell>
                         <cell>58.6 ± 7.5</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Makron</cell>
                         <cell>73.7 ± 3.7</cell>
                         <cell>63.4 ± 2.9</cell>
                         <cell>79.3 ± 5.2</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Oltos</cell>
                         <cell>66.2 ± 5.8</cell>
                         <cell>42.1 ± 8.6</cell>
                         <cell>65.3 ± 9.1</cell>
                     </row>
                     
                     <row role="data">
                         <cell>Average</cell>
                         <cell>67.9 ± 4.9</cell>
                         <cell>50.5 ± 4.5</cell>
                         <cell>67.7 ± 6.32</cell>
                     </row>
                           </table>
                   
                     <p>When comparing the numbers, we have to be mindful of the major differences in scaling. In the case of scenarios (1) and (2), most elements within the figure, like the eyes, were exceedingly small. 
                         This, in turn, means that the linework of the details cannot be a factor for attribution. 
                         The difference between full vase paintings and the other two scenarios demonstrates the impact of 
                         various factors beyond the distinctive linework. This includes elements such as the overall composition of the painting, the placement of figures and the vessel shape. 
                         Because this confirms that the model is able to generalize other characteristics of a painting between 
                         the painters and make correct attributions on that basis. In comparison to the traditional method, this is not inherently misleading. 
                         Archaeological scholars do not disregard composition and arrangement when making painter attributions either. 
                         <name>Beazley</name> for example emphasizes aspects of composition <ptr target="#beazley1930" loc="12"/> 
                         and the overall impression of the figures <ptr target="#beazley1933" loc="7"/>. 
                         This does, however, mean, that we are restrained to only general characteristics with this model.
                   </p>
                     
                     <p>It is also noticeable, that the accuracies vary significantly between the classes. Some are easier to learn in either of the scenarios, others pertaining to their full images or single figures. 
                         <name>Douris</name> and <name>Makron</name> have a higher accuracy with segment sets, which is not surprising, considering their datasets included mostly the outsides of the cups as well as the insides. 
                         In terms of vase shape and composition the classes have as much in common with each other as they do within the class. Therefore, they are more distinguishable by their details. 
                         The Berlin Painter, on the other hand, has a higher accuracy for full images, which indicates, that his class might be more coherent in composition and motif. 
                     </p>

                     <p>The second scenario, which involves only full figures, combines the weaknesses of the other scenarios: Cutting out the figures reduced the stronger foreground characteristics of (1) 
                         and focusing on the whole figure makes it impossible to recognize details such as in (3). Therefore, the significant drop in accuracy is not surprising: Neither the overall 
                         characteristics of the image nor the details of the handwriting help the model to recognize the painters. This is an important observation: The biases apparent in (1) are significantly reduced when focusing on a figure. 
                         Thus, we feel validated in focusing on the details of the figures, which leads to a significant performance enhancement in (3). This can be attributed to the model's 
                         ability to process the information in the details at a much higher resolution, leading to a superior recognition of individual lines and details neglected in the other two scenarios.            
                </p>

                     <p>In addition to the accuracy, we also took the standard deviation into account, which is slightly larger in (3) than in (1), especially for specific painters. 
                         The standard deviation is calculated across the cross-validation, meaning that a higher standard deviation indicates that the model struggled to generalize across the varying training and validation data. 
                         Oltos was exceptional in this sense, with a standard deviation of ±9.1. There are three reasons for this: First, he is the painter with the least instances, making him prone to misclassification. 
                         The difference is not very high however and there are more reasons found in the composition of his œuvre. Therefore: Second, his oeuvre is quite heterogenous, making it difficult to generalize learned 
                         features across the images. And third, he is the earliest painter. The model difficulties with him confirm our hypothesis regarding the model’s focus on the personal style 
                         of painters from our main period. Oltos stands on a different level, as he should. Another painter with a higher standard deviation is Douris (7.5). His case is more difficult to explain. 
                         His work is often divided into distinct phases (Buitron-Oliver 1995 2-3). Arguments vary from figural style to his specific use of ornaments. It is possible that his class is more heterogeneous 
                         and that his development therefore posed a challenge for our model. 
                    </p>
                     
                     <p>This experiment supports us in our proposed approach in two aspects: First, it corroborated the assessment of potential biases and the strategies to reduce them. 
                         When we compared the performance between (1) and (3) we noticed that our approach works well, specifically for the cup painters. Second, the combination of details allows the model 
                         to capture both larger elements and smaller details in a much more nuanced manner. It also proves to be invaluable for our aim of understanding the specific criteria contributing to each painter's attribution, 
                         and aligns with our primary goal of challenging and questioning established methods. However, it also indicates that the full image classification might provide an additional insight into our dataset 
                         composition and possible biases in further experiments. Therefore, it is a valuable addition, even though we are aware that this is not a recognition of ‘style’ in the sense of handwriting. 
                     </p>
                 </div> <!-- 7.1 -->
                 
                 <div>
                     <head>7.2	Attribution of Painter Circles</head>
                     <p>Next, we want to take a first look into the attribution of the core painter’s circles. Since our model has only been trained to differentiate between the five core painters, testing it on other painters gives us an 
                         insight into their stylistic nearness to the training classes. For an additional perspective on the level of similarity we considered the full image model as well as the model trained on segment sets (table 5).</p>
                 
                     <p>First, the results vary significantly for the respective circles. As explained in Section 3, there are different underlying assumptions for each circle. The model worked best for the Berlin Painter, whose circle is defined by stylistic similarities 
                         and supposed "teacher-student-relationships". In the Brygos Painter’s circle, the model classified some painters relatively well, especially the Briseis painter <ptr target="#beazley1963" loc="406"/> and the Dokimasia painter <ptr target="#beazley1963" loc="412"/> who are considered to be a “milder” 
                         variation of the <name>Brygos Painter</name> <ptr target="#beazley1963" loc="400"/>; <ptr target="giudice_giudice2011"/>; <ptr target="#williams2017" loc="166"/>. The Foundry painter and the painter of the Paris Gigantomachy both have high prediction values in full figure, but not in the set of annotations. 
                         <name>Beazley</name> described the style of the Foundry painter as follows: <quote rend="inline"><hi rend="italic">with his forcible, sometimes even brutal style he often equals the Brygos Painter</hi></quote> and the Painter of the Paris Gigantomachy <quote rend="inline"><hi rend="italic">stands nearer to the Foundry Painter than to the 
                             <name>Brygos Painter</name> himself</hi></quote> <ptr target="#beazley1963" loc="400"/>. It may be that our model classifies them as less similar to the Brygos Painter because the connection is based on the general impression more so than on their stylistic details.
                 </p>
                     
                     <p>This was also a similar trend in the circle of Oltos. His circle is based on workshop connections, as indicated by potter signatures. The painters in his circle have low prediction values for his class; the numbers are overall low, 
                         suggesting that the neural network is very insecure concerning the attribution. This exposes the signatures as a fickle and uncertain argument, as they might have worked in the same workshops at different points of their career. 
                         However, we must also bear in mind that the model generally had more trouble recognising the Brygos Painter and Oltos compared to the <name>Berlin Painter</name>. Overall, the predictions seem to align with stylistic similarity especially well, 
                         which allows us to analyse the Berlin Painter’s circle in more depth.  
                     </p>
                     
                     <p>The painters in his circle are related to him in various ways and we are going to analyse them chronologically. For a long time, scholars argued that <name><hi rend="bold">Euthymides</hi></name> <ptr target="#beazley1963" loc="26"/>; <ptr target="#neils1995"/> and <name><hi rend="bold">Phintias</hi></name> <ptr target="#beazley1963" loc="22"/>; <ptr target="#pécasse1992"/> have been the Berlin Painters “teachers”. 
                         They belong to the earlier generation and belong to a group called the “Pioneers” <ptr target="#robertson1992b"/>; <ptr target="#williams1991"/>. Different arguments have  been proposed for each possible “teacher”.  Beazley wrote that the Berlin Painter <quote rend="inline">issues from their group</quote> <ptr target="#beazley1963" loc="196"/>, 
                         but also that the <quote rend="inline">subtle flow of his lines and his fine sense of composition place him nearer Euthymides</quote> <ptr target="#beazley1918" loc="38"/>. This last judgement reflects in our model as well: Regardless of who his “teacher” may have been, our model’s prediction 
                         places him closer to Euthymides, both in linework and composition.                          
                     </p>
                     
                     <p>The <hi rend="bold">Kleophrades painter</hi> <ptr target="beazley1933"/>; <ptr target="#beazley1963" loc="181"/>; <ptr target="#williams2017" loc="153-156"/>, and <name><hi rend="bold">Myson</hi></name> <ptr target="#beazley1963" loc="237"/>; <ptr target="#williams2017" loc="156-157"/> are also associated with the Pioneers’ circle. <name>Kleophrades</name> is considered a “rival” to the <name>Berlin Painter</name> <ptr target="#williams2017" loc="153"/> 
                         and <name>Myson</name> is supposedly a “student” of <name>Phintias</name>. While the <name>Kleophrades</name> painter’s prediction value is high in his full paintings, the number is lower for his actual linework, which reflects him supplying a similar market, but in a different individual style. 
                         <name>Myson</name>’s prediction values are lower for the full paintings, but higher for the linework, maybe reflecting their common grounds regarding their “teacher”. We have to be careful to draw too many conclusions from the numbers however, 
                         as they merely express our models “next best guess”. 
                     </p>


                    <table>
                        <head><hi rend="italic">Painter attribution results on the painter’s friends for the model trained on full images (a) and (b) on a segment sets.</hi></head>
                        <row role="label">
                            <cell>(a) Full Image</cell>
                            <cell></cell>
                            <cell></cell>
                            <cell></cell>
                            <cell></cell>
                            <cell></cell>
                            <cell>(b) Segment Set</cell>
                        </row>
                       
                        <row role="label">
                            <cell></cell>
                            <cell>Berlin</cell>
                            <cell>Brygos</cell>
                            <cell>Douris</cell>
                            <cell>Makron</cell>
                            <cell>Oltos</cell>
                            
                            <cell></cell>
                            
                            <cell>Berlin</cell>
                            <cell>Brygos</cell>
                            <cell>Douris</cell>
                            <cell>Makron</cell>
                            <cell>Oltos</cell>
                        </row>

                <row role="label">
                    <cell>Berlin P.</cell>
                </row>
                        
                        <row role="data">
                            <cell>Achilles P.</cell>
                            <cell><hi rend="bold">68.3</hi></cell>
                            <cell>18.8</cell>
                            <cell>11.0</cell>
                            <cell>0.0</cell>
                            <cell>1.8</cell>
                            
                            <cell></cell>
                            <cell><hi rend="bold">61.6</hi></cell>
                            <cell>6.1</cell>
                            <cell>6.7</cell>
                            <cell>22.0</cell>
                            <cell>3.7</cell>
                        </row>
                        
                        <row role="data">
                            <cell>Euthymides</cell>
                            <cell><hi rend="bold">75.0</hi></cell>
                            <cell>2.5</cell>
                            <cell>17.5</cell>
                            <cell>0.0</cell>
                            <cell>5.0</cell>
                            
                            <cell></cell>
                            <cell><hi rend="bold">72.5</hi></cell>
                            <cell>7.5</cell>
                            <cell>5.0</cell>
                            <cell>7.5</cell>
                            <cell>7.5</cell>
                  </row>
                        
                        <row role="data">
                            <cell>Hermonax</cell>
                            <cell>60.8</cell>
                            <cell>5.2</cell>
                            <cell>13.4</cell>
                            <cell>3.1</cell>
                            <cell>17.5</cell>
                            
                            <cell></cell>
                            <cell><hi rend="bold">36.1</hi></cell>
                            <cell>29.9</cell>
                            <cell>2.1</cell>
                            <cell>27.8</cell>
                            <cell>4.1</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Kleophrades P.</cell>
                            <cell><hi rend="bold">61.0</hi></cell>
                            <cell>15.7</cell>
                            <cell>14.8</cell>
                            <cell>5.6</cell>
                            <cell>3.0</cell>
                            
                            <cell></cell>
                            <cell><hi rend="bold">43.3</hi></cell>
                            <cell>15.7</cell>
                            <cell>6.2</cell>
                            <cell>28.2</cell>
                            <cell>6.6</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Myson</cell>
                            <cell><hi rend="bold">42.9</hi></cell>
                            <cell>23.8</cell>
                            <cell>21.4</cell>
                            <cell>9.5</cell>
                            <cell>2.4</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">54.8</hi></cell>
                            <cell>16.7</cell>
                            <cell>0.0</cell>
                            <cell>16.7</cell>
                            <cell>11.9</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Oionokles</cell>
                            <cell><hi rend="bold">77.1</hi></cell>
                            <cell>4.2</cell>
                            <cell>6.2</cell>
                            <cell>0.0</cell>
                            <cell>12.5</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">60.4</hi></cell>
                            <cell>25.0</cell>
                            <cell>4.2</cell>
                            <cell>6.2</cell>
                            <cell>4.2</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Persephone P.</cell>
                            <cell>20.0</cell>
                            <cell><hi rend="bold">37.1</hi></cell>
                            <cell>11.4</cell>
                            <cell>20.0</cell>
                            <cell>11.4</cell>
                            
                            <cell></cell>

                            <cell><hi rend="bold">60.0</hi></cell>
                            <cell>14.3</cell>
                            <cell>11.4</cell>
                            <cell>11.4</cell>
                            <cell>2.9</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Phiale P.</cell>
                            <cell><hi rend="bold">60.0</hi></cell>
                            <cell>10.7</cell>
                            <cell>20.0</cell>
                            <cell>3.3</cell>
                            <cell>6.0</cell>

                            <cell></cell>

                            <cell><hi rend="bold">30.0</hi></cell>
                            <cell>20.0</cell>
                            <cell>12.7</cell>
                            <cell>9.3</cell>
                            <cell>28.0</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Phintias</cell>
                            <cell><hi rend="bold"></hi>39.5</cell>
                            <cell>16.3</cell>
                            <cell>25.6</cell>
                            <cell>0.0</cell>
                            <cell>18.6</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">34.9</hi></cell>
                            <cell>14.0</cell>
                            <cell>11.6</cell>
                            <cell>25.6</cell>
                            <cell>14.0</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Providence P.</cell>
                            <cell><hi rend="bold">69.0</hi></cell>
                            <cell>2.4</cell>
                            <cell>13.1</cell>
                            <cell>1.2</cell>
                            <cell>14.3</cell>
                            
                            <cell></cell>

                            <cell><hi rend="bold">75.0</hi></cell>
                            <cell>15.5</cell>
                            <cell>0.0</cell>
                            <cell>7.1</cell>
                            <cell>2.4</cell>
                            
                            
                        </row>
                        
                        <row role="label">
                            <cell>Brygos P.</cell>
                            <cell></cell>
                        </row>
                        
                        <row role="data">
                            <cell>Briseis P.</cell>
                            <cell>12.0</cell>
                            <cell><hi rend="bold">36.0</hi></cell>
                            <cell>17.0</cell>
                            <cell>18.0</cell>
                            <cell>17.0</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">39.0</hi></cell>
                            <cell><hi rend="bold">39.0</hi></cell>
                            <cell>2.0</cell>
                            <cell>16.0</cell>
                            <cell>4.0</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Dokimasia P.</cell>
                            <cell>1.6</cell>
                            <cell>29.0</cell>
                            <cell>17.7</cell>
                            <cell>21.0</cell>
                            <cell><hi rend="bold">30.6</hi></cell>
                            
                            <cell></cell>
                            
                            <cell>25.8</cell>
                            <cell><hi rend="bold">35.5</hi></cell>
                            <cell>3.2</cell>
                            <cell>30.6</cell>
                            <cell>4.8</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Foundry P.</cell>
                            <cell>8.3</cell>
                            <cell><hi rend="bold">34.2</hi></cell>
                            <cell>17.5</cell>
                            <cell>17.5</cell>
                            <cell>22.5</cell>
                            
                            <cell></cell>

                            <cell>15.5</cell>
                            <cell>36.7</cell>
                            <cell>5.8</cell>
                            <cell><hi rend="bold">38.3</hi></cell>
                            <cell>4.2</cell>
                        </row>
                        
                        <row role="data">
                            <cell>P. of the Paris G.</cell>
                            <cell>8.0</cell>
                            <cell><hi rend="bold">40.0</hi></cell>
                            <cell>10.7</cell>
                            <cell>20.0</cell>
                            <cell>21.3</cell>
                            
                            <cell></cell>
                            
                            <cell>18.7</cell>
                            <cell>22.7</cell>
                            <cell>6.7</cell>
                            <cell><hi rend="bold">45.3</hi></cell>
                            <cell>6.7</cell>
                        </row>
                        
                        <row role="label">
                            <cell>Oltos</cell>
                            <cell></cell>
                        </row>
                        
                        <row role="data">
                            <cell>Epiktetos</cell>
                            <cell>6.4</cell>
                            <cell>9.1</cell>
                            <cell>20.9</cell>
                            <cell>14.5</cell>
                            <cell><hi rend="bold">49.1</hi></cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">38.2</hi></cell>
                            <cell>6.4</cell>
                            <cell>3.6</cell>
                            <cell>21.8</cell>
                            <cell>30.0</cell>                            
                        </row>
                        
                        <row role="data">
                            <cell>Nikosthenes P.</cell>
                            <cell>6.5</cell>
                            <cell>12.9</cell>
                            <cell><hi rend="bold">38.7</hi></cell>
                            <cell>9.7</cell>
                            <cell>32.3</cell>
                            
                            <cell></cell>
                            
                            <cell>25.8</cell>
                            <cell>16.1</cell>
                            <cell>6.5</cell>
                            <cell><hi rend="bold">38.7</hi></cell>
                            <cell>12.9</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Onesimos</cell>
                            <cell>9.3</cell>
                            <cell><hi rend="bold">24.8</hi></cell>
                            <cell><hi rend="bold">24.8</hi></cell>
                            <cell>20.9</cell>
                            <cell>20.2</cell>
                            
                            <cell></cell>
                            
                            <cell>17.1</cell>
                            <cell>17.8</cell>
                            <cell>4.7</cell>
                            <cell><hi rend="bold">53.5</hi></cell>
                            <cell>7.0</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Pistoxenos P.</cell>
                            <cell><hi rend="bold">21.4</hi></cell>
                            <cell>25.0</cell>
                            <cell>14.3</cell>
                            <cell>25.0</cell>
                            <cell>14.3</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">21.4</hi></cell>
                            <cell><hi rend="bold">21.4</hi></cell>
                            <cell>17.9</cell>
                            <cell><hi rend="bold">21.4</hi></cell>
                            <cell>17.9</cell>
                            
                        </row>
                        
                        <row role="data">
                            <cell>Euphronios</cell>
                            <cell><hi rend="bold">47.27</hi></cell>
                            <cell>6.36</cell>
                            <cell>4.54</cell>
                            <cell>15.45</cell>
                            <cell>26.36</cell>
                            
                            <cell></cell>
                            
                            <cell><hi rend="bold">36.6</hi></cell>
                            <cell>10.0</cell>
                            <cell>7.2</cell>
                            <cell>21.8</cell>
                            <cell>24.5</cell>
                            
                        </row>           
                    </table>

                     <p>More distinct conclusions can be drawn about painters with a closer connection, his supposed “students” (Figure 12): <hi rend="bold">Hermonax</hi> <ptr target="#beazley1963" loc="483"/>; <ptr target="#oakley2017" loc="72-74"/>Oakley 2017, the <hi rend="bold">Achilles Painter</hi> <ptr target="#beazley1963" loc="986"/>; <ptr target="#oakley1997"/>; <ptr target="oakley2017" loc="75-78"/> and the <hi rend="bold">Providence painter</hi>
                         <ptr target="#beazley1963" loc="635"/>; <ptr target="#oakley2017" loc="69-72"/>. In the full vase classification, our model sees a strong resemblance between the <name>Berlin Painter</name>, and both the <name>Achilles Painter</name> and <name>Hermonax</name>. Their values are lower in the segment-set classification – for the <name>Achilles Painter</name> by a little, 
                         for <name>Hermonax</name> significantly. According to our model, the closest student is the Providence painter, who even features higher values for the annotation sets than the full image classification. This indicates, that the similarity is not only present in his composition and motifs, 
                         but also in stylistic details such as delicately rendered hands or facial proportions. A focus on such details could make the two painters seem like the same person, and only in a closer inspection the differences become apparent as well. His other supposed 
                         "students" also share similarities with the Berlin Painter, but they often have their own twist on certain renditions of similar shapes and the similarity is often limited to specific features.    
            </p>
                   <!-- figure 12 -->  
                     <figure>
                         <head><hi rend="italic">Paintings by painters from the Berlin Painters circle, images from the Metropolitan Museum Collection, New York. 
                             First row from left to right: Providence painter (<hi rend="bold">207429</hi>, 41.162.18), Achilles Painter (<hi rend="bold">214079</hi>, 41.162.139), Hermonax (<hi rend="bold">205498</hi>, 41.162.19). Second row: Oionokles (<hi rend="bold">207552</hi>, 28.57.11), Phiale Painter (<hi rend="bold">214188</hi>, 41.162.142)</hi>
                         </head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure12.jpg"></graphic>
                     </figure>


                     <p>The last group of test classes features only distant connections to the <name>Berlin Painter</name>. The <name>Phiale Painter</name> is considered to be a "student" of the <hi rend="bold">Achilles Painter</hi> <ptr target="#beazley1963" loc="1014"/>; <ptr target="#oakley1980" loc="58-63"/>; <ptr target="#oakley2017" loc="78-81"/> and <hi rend="bold">Oionokles</hi> 
                         <ptr target="#beazley1963" loc="646"/>; <ptr target="#serbeti1989"/>; <ptr target="#oakley2017" loc="72"/> is considered a "follower" of the Providence painter. The <hi rend="bold">Persephone</hi> painter was described as “akin in spirit” to the Achilles Painter (Beazley 1963, 1012; Oakley 1997, 102). 
                    </p>
                     
                     <p>Since the Providence painter is close to the Berlin Painter, his “follower” Oionokles also has high prediction values for the Berlin Painter’s class. Conversely, the Phiale Painter has lower prediction values, but still a higher value for the full figure classification. 
                         It may be that he would be classified closer to his own teacher, the Achilles Painter, than to the Berlin Painter, from whom his teacher may have eventually deviated. Then again, his paintings are from a much later generation. Yet the Persephone painter, who was also active much later, 
                         still shows high prediction values in the segment-set classification. This variation may therefore expose a temporal bias for later periods, as the Berlin Painters class also contains more later works than the other classes.                         
                     </p>
                     
                     <p>We cannot draw conclusions regarding the specific relationship between the painters based on the prediction values, but the alignment of the model’s results with scholarly opinion suggests that there are underlying similarities which we have yet to interpret. 
                         While we do not go as far so to argue that the scholarly interpretations are correct, we recognize that there may be distinct similarities underlying their observations, which also get picked up by our model. Our experiments suggest that the 
                         “teacher-students-relationships” are based on complex and multi-layered features. Thus, further investigations into the details of our model are necessary to characterize the similarities and re-contextualize the scholars’ interpretations.
                     </p>
            
                     <p>Overall, the prediction values indicate that our model is able to recognize very nuanced kinds of similarities between the paintings as observed by connoisseurs. This is a good foundation for further discussion about the criteria that lead scholars to assert their propinquity in the first place. 
                         Furthermore, this experiment demonstrates the importance of the entire vase painting in addition to the segment sets since motivic and compositional congruence allow the human researcher as well as the neural network to infer similarities, just as stylistic features do. 
                         Sometimes the former might be even stronger than the latter, such as in the case of Hermonax, which may perhaps reveal a human bias in turn.
            </p>

                 </div><!-- 7.2 -->
             </div><!-- 7 -->
             
             <div>
                 <head>8. Conclusion and Outlook</head>
                 
                 <p>The previous chapter is only a 'first look' at the results, as the analysis still reveals many peculiarities and has led to exciting case studies, which go too deep to discuss in this paper.  Therefore, we would like to end our paper with a characterization of the data sampling in regards to our analysis framework, 
                     which we will be presented as brief outlook as well.</p>
                 
                 <p>At its core, the individual style of painter can be described with Beazley’s term “system of renderings”; a combination of certain characteristics and their variation. With this in mind, painter attribution boils down the drawing lines between the variation of one painter against the variation of the others. 
                     To operationalize this approach, we had to take several distortions of the data into account. Being aware of these transformations, controlling them, and assessing their effect, is necessary for the creation of an analysis framework. In some instances, the distortion was deliberate, in order to emphasize important details, 
                     the “<hi rend="bold">renderings</hi>” in Beazley’s “system”. In other instances, the potential biases and the workings of computational methods did not allow for anything else. For painter attribution, potential biases are found in general image characteristics and can be mitigated through careful painter selection and through image segmentation, 
                     intentionally skewing the representation of the original paintings to emphasize the features necessary. During this process, our annotation guidelines ensured comparability between the segments and our model architecture ensured that the segments were equally and invariantly weighted as to represent the 
                     “<hi rend="bold">"systems</hi>” of Beazley’s “renderings”.</p>
                 
                 <p>This extensive effort was necessary to ensure a comparability between the model and human researchers and to tackle the core uncertainty, fuzziness and ambiguity of the traditional method. In most cases we cannot know exactly who painted a vase. But this is not what our research is about. 
                     The work of Beazley and other scholars’ hints at a phenomenon that is even more fundamental and goes beyond the question of "who painted a picture?": The phenomenon of (stylistic) variation and similarity. Where do we draw the lines? Where does one painter end and where does the other begin? 
                     The methods of machine learning offer us a new approach to these questions by simulating the referential framework and creating an exploratory space, where uncertainty and fuzziness become tangible.</p>
                 
                 <p>To analyse these phenomena, our framework builds upon our observations in the baseline experiments and even our overall experiment design. Each data test/training data split works as a simulation of a single instances’ learnability. It shows how well a model can learn a painter’s characteristic by the examples provided. 
                     Some work better than others, which points us to the uncertainties of painter attribution. Additionally, the certainty of the model and the robustness of certain instances also allows us to determine the scope of our observations. Some outliers may come with remarkable values, but turn out to be instable, 
                     while other instances may not appear very remarkable, but they appear regularly. In our further research we investigate cases of both. For this purpose, we also use dimensionality reduction techniques and their visualisations such as t-SNE (Van der Maaten and Hinton 2008) and UMAP (McInnes, Healy, and Melville 2018), which adds to the perspectivity of our approach. 
                     It adds a visual component and allows us to take the overall distribution of the data into account. All in all, we were able to re-create some nuances of painter attribution faithfully, as already shown by the first look at the results. This allows for an in-depth investigation of our own model’s 
                     ‘connoisseurship’ in comparison with human researchers.                  
             </p>
                 
                 <p>In any case, our first look at the results already reveals key insights into connoisseurship as a method of educated association. This adds to our understanding of image comprehension as a highly complex scholarly method, where dense information is derived from the very details of a painting. 
                     By creating a specific frame of reference with our AI-experiments, we can simulate different associative frames of scholars to explore their interpretation of similarity even deeper. Another key to understanding lies in the difference between the model’s performance and human scholars. 
                     In which way do the models 30% “errors” differ from the human approach? The answer might lie in what Richard Neer calls “focused seeing”. The human is able to make an educated decisions on where to look, while a CNN blindly compares everything against everything, at least within the frame we created. 
                     Our case studies showed, that in some instances an attribution is obvious for a human expert, because the artist reveals themselves in only few, but highly identical details. Cases such as these might get lost in the statistics of machine learning.</p>
             
                 <p>Therefore, it is obvious, that AI is not able to replace the human expert. The relationship between the computational methods and human researchers is, on the contrary, defined by a constant interaction instead of automation (see also Bell and Offert 2021). Human experts have invaluable domain knowledge and contextual visual experience that cannot be adequately represented in the simulation. 
                     So, on the one hand, we have to translate and communicate our research interests to the neural network. On the other hand, it “communicates” its processing of the task back to us via evaluation metrics, prediction scores and visualisations. The “researcher” and the “artificial intelligence” are deeply intertwined, and in the end, the researchers and their interests are the key actors in the beginning and in the end of the process.</p>
                 
                 <p>Another important takeaway from our research is that it is not always possible to distinctly isolate stylistic details from other image characteristics. Sometimes stylistic similarity correlates with general shapes, the composition and the motif of a painting. Thinking beyond painter attribution, this is a central aspect of any artistic image domain. It shows, how standard approaches might not encompass the images in all their complexity. All in all, our case study of painter attribution in Classical Archaeology provides insights into the very foundation of humanities research: 
                     Uncertainty. Ambiguity. Variation. Complexity.</p>
             
             </div><!-- div 8 -->
             
        </body>
        <back>
            <listBibl>
                <bibl xml:id="afrasiyabi_etal2022" label="Afrasiyabi, Larochelle, Lalonde, and Gagné (2022)">Afrasiyabi, A., Larochelle, H., Lalonde, J-F., and Gagné, C. (2022) <title rend="quotes">Matching feature sets for few-shot image classification</title>, in <title rend="italic">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</title>, 9014-24. Los Alamitos: IEEE Computer Society.</bibl>
                
                <bibl xml:id="agarap_2018" label="Agarap 2018">Agarap, A. F. (2018) <title rend="quotes">Deep learning using rectified linear units (Relu)</title>, arXiv Preprint. Available at: <ref target="https://doi.org/10.48550/arXiv.1803.08375">https://doi.org/10.48550/arXiv.1803.08375</ref>.</bibl>
          
                <bibl xml:id="anderson1996" label="Anderson 1996">Anderson, J. (1996) <title rend="italic">Giorgione: Peintre de La" brièveté poétique"; Catalogue raisonné.</title> Paris: Lagune.</bibl>
                
                <bibl xml:id="arrington_2017" label="Arrington 2017">Arrington, N. (2017) <title rend="quotes">Connoisseurship, vases and Greek art and archaeology</title>, in <title rend="italic">(Padgett 2017)</title>, 21–39. Princeton University Art Museum.</bibl>
                
                <bibl xml:id="baziotopoulou_valavani_1994" label="Baziotopoulou-Valavani 1994">Baziotopoulou-Valavani, E. (1994) <title rend="quotes">Ανασκαφές Σε Αθηναϊκά Κεραμικά Εργαστήρια Των Αρχαϊκών Και Κλασικών Χρόνων</title>, in <title rend="italic">The archaeology of Athens and Attica under the democracy: Proceedings of an international conference celebrating 2500 years since the birth of democracy in Greece, held at the American School of Classical Studies at Athens, December 4-6, 1992</title>, 37:45–54. Oxbow Monograph. Oxford: Oxbow Books.</bibl>
                
                <bibl xml:id="beazley1911" label="Beazley 1911">Beazley, J. D. (1911) <title rend="quotes">The master of the Berlin amphora</title>, <title rend="italic">The Journal of Hellenic Studies</title>, 31:276–95.</bibl>
                
                <bibl xml:id="beazley1918" label="Beazley 1918">Beazley, J. D. (1918) <title rend="italic">Attic red-figured vases in American museums</title>. Cambridge: Harvard University Press.</bibl>
                
                <bibl xml:id="beazley1922" label="Beazley 1922">Beazley, J. D.(1922) <title rend="quotes">Citharoedus</title>, <title rend="italic">The Journal of Hellenic Studies</title>, 42 (1): 70–98.</bibl>
                
                <bibl xml:id="beazley1930" label="beazley1930">Beazley, J. D. (1930) <title rend="italic">Der Berliner Maler</title>. Vol. 2. Forschungen Zur Antiken Keramik 1. Berlin: Keller.</bibl>
                
                <bibl xml:id="beazley1933" label="Beazley 1933">Beazley, J. D. (1933) <title rend="italic">The Kleophrades painter</title>. Vol. 6. Forschungen Zur Antiken Keramik 1. Berlin: Keller.</bibl>
                
                <bibl xml:id="beazley1956" label="Beazley 1956">Beazley, J. D. (1956) <title rend="italic">Attic black-figure vase-painters.</title> Oxford: Clarendon Press.</bibl>
                
                <bibl xml:id="beazley1961" label="Beazley 1961">Beazley, J. D. (1961) <title rend="quotes">An amphora by the Berlin painter</title>, <title rend="italic">Antike Kunst</title>, 4 (2): 49–67.</bibl>
                
                <bibl xml:id="beazley1963" label="Beazley 1963">Beazley, J. D. (1963) <title rend="italic">Attic red-figure vase-painters</title>. Oxford: Clarendon Press.</bibl>
                
                <bibl xml:id="bell_offert2021" label="Bell and Offert 2021">Bell, P. and Offert, F. (2021) <title rend="quotes">Reflections on connoisseurship and computer vision</title>, <title rend="italic">Journal of Art Historiography</title>, 24:1–10.</bibl>
                
                <bibl xml:id="bérard_vernant1985" label="Bérard and Vernant 1985">Bérard, C. and Vernant J. P. (1985) <title rend="italic">Die bilderwelt der Griechen: Schlüssel zu Einer "Fremden" Kultur</title>. Translated by Ursula Sturzenegger. Vol. 31. Kulturgeschichte Der Antiken Welt. München: Verlag Philipp von Zabern.</bibl>
                
                <bibl xml:id="boardman_1975" label="Boardman 1975">Boardman, J. (1975) <title rend="italic">Athenian red figure vases: The Archaic Period</title>. Oxford University Press.</bibl>
                
                <bibl xml:id="boardman2001" label="Boardman 2001">Boardman, J. (2001) <title rend="italic">The history of Greek vases: Potters, painters and pictures</title>. London: Thames &amp; Hudson.</bibl>
                
                <bibl xml:id="bolmarcich_muskett2017" label="Bolmarcich and Muskett 2017">Bolmarcich, S. and Muskett, G. (2017) <title rend="quotes">Artists’ signatures on Archaic Greek Vases from Athens</title>, in <title rend="italic">Artists and Artistic Production in Ancient Greece</title>, 154–76. Cambridge: Cambridge University Press.</bibl>
                
                <bibl xml:id="bruhn1943" label="Bruhn 1943">Bruhn, A. (1943) <title rend="italic">Oltos and early red-figure vase painting</title>. Kopenhagen: Busck.</bibl>
                
                <bibl xml:id="buitron_oliver1995" label="Buitron-Oliver 1995">Buitron-Oliver, D. (1995) <title rend="italic">Douris: A master-painter of Athenian red-figure vase</title>. Forschungen Zur Antiken Keramik. II Reihe, Kerameus 9. Mainz: Verlag Philipp von Zabern.</bibl>
                
                <bibl xml:id="burg2007" label="Burg 2007">Burg, T. (2007) <title rend="italic">Die signatur: Formen und funktionen vom mittelalter bis zum 17. Jahrhundert</title>. Münster: LIT Verlag.</bibl>
                
                <bibl xml:id="cardon1979" label="Cardon 1979">Cardon, C. M. (1979) <title rend="quotes">The Gorgos Cup</title>, <title rend="italic">American Journal of Archaeology</title>, 83 (2): 169–73.</bibl>
                
                <bibl xml:id="cline_hasaki2019" label="Cline and Hasaki 2019">Cline, D. and Hasaki, E. (2019) <title rend="quotes">The connected world of potters in ancient Athens: Collaborations, connoisseurship, and social network analysis</title>, <title rend="italic">CHS Research Bulletin</title> 7. Available at: <ref target="https://research-bulletin.chs.harvard.edu/2019/03/19/connected-world-of-potters/">https://research-bulletin.chs.harvard.edu/2019/03/19/connected-world-of-potters/</ref>.</bibl>
                
                <bibl xml:id="cook2013" label="Cook 2013 ed.">Cook, R. M. (2013 ed.) <title rend="italic">Greek painted pottery</title>. London: Routledge.</bibl>
                
                <bibl xml:id="corpus" label="Corpus Vasorum Antiquorum">No author (1922) <title rend="italic">Corpus vasorum antiquor</title>. Available at: <ref target="https://www.cvaonline.org/cva/Home">https://www.cvaonline.org/cva/Home</ref>.</bibl>
                
                <bibl xml:id="deng_etal2009" label="Deng et al. 2009">Deng, J., Dong, W., Socher, R., Li, L. J., Fei-Fei, L. (2009) <title rend="quotes">Imagenet: A large-scale hierarchical image database</title>, in <title rend="italic">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>, 248–55. Los Alamitos: IEEE Computer Society.</bibl>
                
                <bibl xml:id="driscoll2019" label="Driscoll 2019">Driscoll, E. (2019) <title rend="quotes">Beazley’s connoisseurship: Aesthetics, natural history, and artistic development</title>, <title rend="italic">Natural History, and Artistic Development. Metis</title>, 17:101–20. Available at: <ref target="https://doi.org/10.4000/books.editionsehess.13689">https://doi.org/10.4000/books.editionsehess.13689</ref>.</bibl>
                
                <bibl xml:id="elmrabet_etal2021">El Mrabet, M. A., El Makkaoui, K., and Faize, A. (2021) <title rend="quotes">Supervised machine learning. A survey</title>, in <title rend="italic">2021 4th International Conference on Advanced Communication Technologies and Networking (CommNet)</title>, 1–10. Los Alamitos: IEEE Computer Society.</bibl>
                
                <bibl xml:id="eschbach_schmidt2016" label="Eschbah and Schmidt 2016">Eschbach, N. and Schmidt S. (eds.) (2016) <title rend="italic">Töpfer Maler Werkstatt: Zuschreibungen in der griechischen vasenmalerei und die organisation Antiker Keramikproduktion</title>. CVA Beihefte 7. München: CH Beck.</bibl>
                
                <bibl xml:id="felicisimo_etal2019" label="Felicisimo et al. 2019">Felicisimo, Á. M., Polo, M. E., Durán, G., Tortosa, T., and Rodero, A. (2019) <title rend="quotes">Rollout archaeological photography for the graphic documentation of cultural heritage</title>, in Wien: Museen der Stadt Wien.</bibl>
                
                <bibl xml:id="folego_etal2016" label="Folego et al. 2016">Folego, G., Gomes, O., and Rocha, A. (2016) <title rend="quotes">From Impressionism to Expressionism: Automatically identifying van Gogh’s paintings</title>, in <title rend="italic">2016 IEEE International Conference on Image Processing (ICIP)</title>, 141–45. Los Alamitos, (CA): IEEE Computer Society.</bibl>
                
                <bibl xml:id="franceschini2018" label="Franceschini 2018">Franceschini, M. (2018) <title rend="italic">ttische Mantelfiguren. Relevanz Eines Standardisierten Motivs Der Rotfigurigen Vasenmalerei</title>. Zürcher Archäologische Forschungen 5. Rahden: VML Verlag Marie Leidorf.</bibl>
                
                <bibl xml:id="frank_frank2020" label="Frank and Frank 2020">Frank, S. J. and Frank, A. M. (2020) <title rend="quotes">Salient slices: Improved neural network training and performance with image entropy</title>. <title rend="italic">Neural Computation</title> 32 (6): 1222–37.</bibl>
                
                <bibl xml:id="frank_frank2021" label="Frank and Frank 2021">Frank, S. J. and Frank, A. M. (2021) <title rend="quotes">A neural network looks at Leonardo’s(?) Salvator Mundi.</title> <title rend="italic">Leonardo</title> 54 (6): 619–24. Available at: <ref target="https://doi.org/10.1162/leon_a_02004">https://doi.org/10.1162/leon_a_02004</ref>.</bibl>
                
                <bibl xml:id="frank_frank2022" label="Frank and Frank 2022">Frank, S. J. and Frank, A. M. (2022) <title rend="quotes">Complementing connoisseurship with artificial intelligence</title>. <title rend="italic"> Curator: The Museum Journal</title> 65 (4): 835–68.</bibl>
                
                <bibl xml:id="ginzburg_davin1980" label="Ginzburg and Davin 1980">Ginzburg, C. and Davin, A. (1980) <title rend="quotes">Morelli, Freud and Sherlock Holmes: Clues and scientific method</title>. <title rend="italic"> History Workshop Journal</title> 9:5–36.</bibl>
                
                <bibl xml:id="giudice_giudice2011" label="Giudice and Giudice 2011">Giudice, G. and Giudice, E. (2011) <title rend="quotes">Una madre in fuga su una lekythos siciliana</title></bibl>
                
                <bibl xml:id="graepler2016" label="Graepler 2016">Graepler, D. (2016) <title rend="quotes">Künstlerhand Und Kennerauge. Die Zuschreibung Als Archäologisches Methodenproblem</title>.In <title rend="italic">Eschbach - Schmidt (2016)</title>, edited by Norbert Eschbach and Stefan Schmidt, 7:14-24. Corpus Vasorum Antiquorum. Deutschland. Beihefte Zum Corpus Vasorum Antiquorum. München: Verlag C.H. Beck.</bibl>
                
                <bibl xml:id="hashemi2019" label="Hashemmi 2019">Hashemi, M. (2019) <title rend="quotes">Enlarging smaller images before inputting into convolutional neural network: Zero-padding vs. interpolation</title>, <title rend="italic">Journal of Big Data</title> 6 (1): 1–13.</bibl>
                
                <bibl xml:id="he_etal2016" label="He, Zhang, Ren, and Sun 2016">He, K., Zhang, X., Ren, S., and Sun, J. (2016) <title rend="quotes">Deep residual learning for image recognition</title>. In <title rend="italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>, 770–78. o. O.: IEEE Computer Society. Available at: <ref target="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</ref>.</bibl>
                
                <bibl xml:id="hegener_horsthemke2013" label="Hegener and Horsthemke 2013">Hegener, N. and Horsthemke F. (2013) <title rend="quotes">Künstler-Signaturen von Der Antike Bis Zur Gegenwart</title>. In. Petersberg: Imhof.</bibl>
                
                <bibl xml:id="hinojosa2009" label="Hinojosa 2009">Hinojosa, L. W. (2009) <title rend="italic">The Renaissance, English Cultural Nationalism, and Modernism, 1860–1920</title>. New York: Palgrave Macmillan.</bibl>
                
                <bibl xml:id="hoffmeyer1943" label="Hoffmeyer 1943">Hoffmeyer, A. B. (1943) <title rend="italic">Oltos and early red-figure vase painting</title>.  Kopenhagen: Busk.</bibl>
                
                <bibl xml:id="huang_etal2017" label="Huang, Liu, Van Der Maaten, and Weinberger 2017">Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017) <title rend="quotes">Densely connected convolutional networks</title>. <title rend="italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>, 4700–4708. o. O.: IEEE Computer Society. Available at: <ref target="https://doi.org/10.1109/CVPR.2017.243">https://doi.org/10.1109/CVPR.2017.243</ref>.</bibl>
                
                <bibl xml:id="hurwit2015" label="Hurwit 2015">Hurwit, J.M. (2015) <title rend="italic">Artists and signatures in Ancient Greece</title>. New York: Cambridge University Press.</bibl>
                
                <bibl xml:id="islam_etal2021" label="Islam et al. 2021">Islam, Md.A., Kowal, M., Esser, P., Jia, S., Ommer, B., Derpanis, K.G., and Bruce, N. (2021) <title rend="quotes">Shape or texture. Understanding discriminative features in CNNs</title>. <title rend="italic">arXiv Preprint</title>, 1–15. Available at: <ref target="https://doi.org/10.48550/arXiv.2101.11604">https://doi.org/10.48550/arXiv.2101.11604</ref>.</bibl>
                
                <bibl xml:id="jones_craddock1990" label="Jones and Craddock 1990">Jones and Craddock (1990) <title rend="italic">Fake? The art of deception</title>. Berkeley: University of California Press.</bibl>
                
                <bibl xml:id="khan_etal2018" label="Khan et al. 2018">Khan, S., Rahmani, H., Ali Shah, S. A., Bennamoun, M., Medioni, G., and Dickinson, S. (2018) <title rend="italic">A guide to convolutional neural networks for computer vision</title>. Cham: Springer. Available at: <ref target="https://doi.org/10.1007/978-3-031-01821-3">https://doi.org/10.1007/978-3-031-01821-3</ref>.</bibl>
                
                <bibl xml:id="kipke_etal2022" label="Kipke et al. 2022">Kipke, M., Brinkmeyer, L., Bagayoko, S., Schmidt-Thieme, L., and Lanngner, M. (2022) <title rend="quotes">Deep level annotation for painter attribution on Greek vases utilizing object detection</title>. In <title rend="italic">SUMAC ’22: Proceedings of the 4th ACM International Workshop on Structuring and Understanding of Multimedia heritAge Contents</title>, 23–31. o. O.: Association for Computing Machinery. Available at: <ref target="https://doi.org/10.1145/3552464.355568">https://doi.org/10.1145/3552464.355568</ref>.</bibl>
                
                <bibl xml:id="krizhevsky_etal2012" label="Krizhevsky et al. 2012">Krizhevsky, A., Sutskever, I., and Hinton, G.E. (2012) <title rend="quotes">ImageNet classification with deep convolutional neural networks</title>.Edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. <title rend="italic">Advances in Neural Information Processing Systems</title> 25:1–9.</bibl>
                
                <bibl xml:id="kunisch1997" label="Kunisch 1997">Kunisch (1997) <title rend="italic">Makron</title>. Vol. 10. Forschungen Zur Antiken Keramik 2. Mainz: Verlag Philipp von Zabern.</bibl>
                
                <bibl xml:id="kurtz1983" label="Kurtz 1983">Kurtz, D.C. (1983) <title rend="quotes">Gorgos’ Cup: An essay in connoisseurship</title>. <title rend="italic">The Journal of Hellenic Studies</title>, 103:68–86.</bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
                
                <bibl></bibl>
            
            </listBibl>
           
        </back>
    </text>
</TEI>
