<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
	xmlns:mml="http://www.w3.org/1998/Math/MathML">
	<teiHeader>
		<fileDesc>
			<titleStmt>
				<!--Author should supply the title and personal information-->
				<title type="article" xml:lang="en"><!--article title in English-->The Banality
					of Big Data: A Review of <title rend="italic">Discriminating
					Data</title></title>
				<!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
				<dhq:authorInfo>
					<!--Include a separate <dhq:authorInfo> element for each author-->
					<dhq:author_name>Amanda <dhq:family>Furiasse</dhq:family>
					</dhq:author_name>
					<idno type="ORCID"
						><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
					<dhq:affiliation>Nova Southeastern University</dhq:affiliation>
					<email>afuriasse@gmail.com</email>
					<dhq:bio>
						<p>Amanda Furiasse, PhD, is Assistant Professor of Digital Humanities
							at Nova Southeastern University. Her research unfolds at the
							intersection of religion, AI, and medicine. She is co-founder of
							the Religion, Art, and Technology Lab and co-host of the
							Political Theology Network’s Assembly Podcast.</p>
					</dhq:bio>
				</dhq:authorInfo>
			</titleStmt>
			<publicationStmt>
				<publisher>Alliance of Digital Humanities Organizations</publisher>
				<publisher>Association for Computers and the Humanities</publisher>
				<!--This information will be completed at publication-->
				<idno type="DHQarticle-id"
					><!--including leading zeroes: e.g. 000110-->000656</idno>
				<idno type="volume"
					><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->016</idno>
				<idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->4</idno>
				<date when="2022-10-14">14 October 2022</date>
				<dhq:articleType>article</dhq:articleType>
				<availability status="CC-BY-ND">
					<!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
					<cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
				</availability>
			</publicationStmt>
			<sourceDesc>
				<p>This is the source</p>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<classDecl>
				<taxonomy xml:id="dhq_keywords">
					<bibl>DHQ classification scheme; full list available at <ref
							target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
							>http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
					</bibl>
				</taxonomy>
				<taxonomy xml:id="authorial_keywords">
					<bibl>Keywords supplied by author; no controlled vocabulary</bibl>
				</taxonomy>
			</classDecl>
		</encodingDesc>
		<profileDesc>
			<langUsage>
				<language ident="en" extent="original"/>
				<!--add <language> with appropriate @ident for any additional languages-->
			</langUsage>
			<textClass>
				<keywords scheme="#dhq_keywords">
					<!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
					<list type="simple">
						<item/>
					</list>
				</keywords>
				<keywords scheme="#authorial_keywords">
					<!--Authors may include one or more keywords of their choice-->
					<list type="simple">
						<item/>
					</list>
				</keywords>
			</textClass>
		</profileDesc>
		<revisionDesc>
			<change>The version history for this file can be found on <ref
					target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000656/000656.xml"
					>GitHub </ref>
			</change>
		</revisionDesc>
	</teiHeader>
	<text xml:lang="en" type="original">
		<front>
			<dhq:abstract>
				<!--Include a brief abstract of the article-->
				<p>This review critically interrogates Wendy Chun’s book <title rend="italic"
						>Discriminating Data: Correlation, Neighborhoods, and the New Politics
						of Recognition</title> (MIT Press, 2021) from the perspective of the
					digital medical or health humanities. The monograph’s exploration of
					predictive machine learning and big data’s propensity to encode segregation
					through their default assumptions about correlation raises important
					questions about machine learning’s growing uses in fields, such as medicine
					and pharmacology, where the stakes of such digital experimentation are
					particularly high. Chun’s exploration of the predictive processes by which
					data analytics replicates 20th-century eugenics discourses makes an
					important contribution to the field of digital medical ethics and also
					offers unique insight into the mechanisms by which digital humanities
					scholars can disrupt and challenge the use and application of such
					predictive programs.</p>
			</dhq:abstract>
			<dhq:teaser>
				<!--Include a brief teaser, no more than a phrase or a single sentence-->
				<p/>
			</dhq:teaser>
		</front>
		<body>
	<div>
			<head>Introduction</head>
			<p>On April 14, 2003, the International Human Genome Sequencing Consortium, a
				collaborative project in the United States between the National Human Genome
				Research Institute and the Department of Energy, announced the successful
				completion of the Human Genome Project. It entailed developing a complete
				sequence of the Human Genome, a task made possible by 800 interconnected Compaq
				Alpha-based computers performing 250 billion sequence comparisons an hour
				<ptr target="#bergeron2004"/>. The torrent of data produced by the project not only
				revealed the incredible complexity animating human life but also and more
				importantly, demonstrated how machinic systems could be used to both reveal and
				even reverse engineer the patterns and trends driving such large data sets,
				giving humanity an almost god-like power over nature. As President Bill Clinton
				announced: <quote rend="inline">Today we are learning the language in which God created life. We are
				gaining ever more awe for the complexity, the beauty, the wonder of God’s most
				divine and sacred gift. With this profound new knowledge, humankind is on the
				verge of gaining immense new power to heal</quote> <ptr target="#newyorktimes2000"/>. Thus
				began the era now known as <q>big data</q> and with it the assumption that the
				analysis of large data sets would allow humanity to reveal and fundamentally
				alter the patterns and trends driving the natural world. Nearly two decades
				after the Genome Project’s success, the promise of big data, however, seems more
				like a nightmare. In her new book <title rend="italic">Discriminating Data:
					Correlation, Neighborhoods, and the New Politics of Recognition</title>, Wendy
				Hui Kyong Chun delves into this nightmare and argues that heightened
				polarization during the era of big data is not an error or flaw within the
				system <ptr target="#chun2021"/>. Rather, Chun makes the argument that discrimination is in
				fact big data’s primary product. </p></div>
			<div>
			<head>Correlating the Data</head>
			<p>As with Chun’s previous contributions, <title rend="italic">Control and Freedom</title>
				(2008), <title rend="italic">Programmed Visions </title>(2011), and <title rend="italic"
					>Updating to Remain the Same</title> (2017), Chun investigates a fundamental
				paradox and contradictory tension at the heart of new digital technologies.
				Machine learning algorithms are often marketed as free from human bias and
				prejudice, yet they simultaneously encode segregation into the very logic by
				which they promise to reverse engineer life. As Chun explains, <quote rend="inline">We need to
				understand how machine learning and other algorithms have been embedded with
				human prejudice and discrimination, not simply at the level of data, but also at
				the levels of procedure, prediction, and logic</quote> <ptr target="#chun2021" loc="16"/>. Prejudice then
				is not a problem that results from human error, but it structures the basic
				logic animating our machinic systems. </p>
			<p>The monograph’s argument unfolds around four foundational concepts in computing:
				correlation, homophily, authenticity and recognition. Correlation remains among
				the most crucial concepts undergirding nearly every aspect of existing AI
				systems. According to Chun, correlation is not just a conceptual category, but
				it constitutes an everyday practice whereby people are lumped into <quote rend="inline">categories
				based on their being <q>like</q> one another amplifying the effects of historical
					inequalities</quote> <ptr target="#chun2021" loc="58"/>. These inequalities are in turn naturalized with
				data organization systems making it appear as though they are innate or <term>sui generis </term>categories which already preexist in the
				world. As Chun warns, <quote rend="inline">correlation contains within it the seeds of manipulation,
					segregation and misrepresentation</quote> <ptr target="#chun2021" loc="59"/>. As a result of their
				reliance on correlation, social networks create <q>microidentities</q> by default
				which instrumentalize and weaponize individual differences. Data analytics
				consequently reimagines eugenics discourses within a big data future where
				correlations are not only assumed to be predictive of future outcomes, but
				surveillance is assumed to be a necessary component of every human institution
				and one which will allow humanity to improve nearly every component of daily
				life.</p>
			<p>All three other foundational concepts are conceptually linked to correlation.
				Homophily, for example, plays a crucial role in naturalizing correlations,
				making likeness seem like an obvious and innate way to group and organize data.
				The assumption that people gravitate toward things that are like them
				consequently becomes <quote rend="inline">a tool for discovering bias while simultaneously
				perpetuating those very biases in the name of <q>comfort,</q> predictability, and
					common sense</quote> <ptr target="#chun2021" loc="85"/>. Chun in turn stresses how the very feeling of comfort
				which homophily generates naturalizes these acts of discrimination. Like
				homophily, authenticity has become automated in that authentication of one’s
				identity within systems of validation are assumed to be a necessary and
				nonnegotiable component of digitization. Before you log in, you must
				authenticate yourself and essentially prove yourself to be real. Algorithmic
				authenticity is rarely if ever challenged. Instead, we are <quote rend="inline">trained to be
					transparent</quote> <ptr target="#chun2021" loc="24"/>. Finally, recognition makes discrimination
				possible, since it encompasses the process whereby we come to accept someone or
				something’s status and authority as authentic.</p></div>
			<div>
			<head>Assessing the Costs of Algorithmic Performativity </head>
			<p>Although Chun’s four foundational concepts might organize all computational
				systems, they are ultimately dependent upon our performance of them. Throughout
				the monograph, Chun emphasizes that data analytics requires our performance of
				it, since it is only through our performance of the roles that data analytics
				assigns us that it comes to take on its meaning. Here, Chun makes the most
				significant and deeply troubling argument in the monograph. Through our active
				participation in it, data analytics transforms the world into a research
				laboratory where we are its main research subjects. The rhythms of life are
				consequently transformed into significant patterns that reveal some sort of
				deeper order that can be reconfigured to improve social outcomes. People who
				fail to follow their algorithmically-assigned roles are either dismissed as
				meaningless outliers or at worse categorized as deviant actors and subject to
				swift disciplinary action. However, deviance, as Chun warns, is also
				pre-scripted and performed, since the internet requires offensive and deviant
				content to hold people’s attention. Deviations are consequently the very way by
				which users come to be authenticated and clustered <ptr target="#chun2021" loc="332"/>. </p>
			<p>For Chun, the transformation of society into an experimental lab is one which
				bears explicit similarities to one of the darkest and most troubling eras of
				human history: eugenics. While often dismissed as a relic of a bygone era in
				human history, eugenics is in fact increasingly coming to undergird contemporary
				institutions. From medicine to education, the assumption that people can be
				divided into certain groups based on certain perceived behavioral or assumed
				physical characteristics and then those groupings can be used to evaluate and
				predict future behaviors and outcomes is now the underlying logic which guides
				human decision-making and institutional structures. As Chun warns, <quote rend="inline">If
				twentieth-century eugenicists however defended their work against accusations
				that it experimented on humans, twenty-first-century data scientists openly
				embrace experimentation</quote> <ptr target="#chun2021" loc="158"/>. Their unyielding belief in the
				inherent benevolent power of experimentation is why firms like Cambridge
				Analytica have been able to experiment on the public relatively free of
				oversight or regulatory pressures, despite the fact that the firm’s claim that
				they were able to predict and shape voting patterns in the 2016 election was
				never verified and no evidence was ever provided to verify their claims of
				success.</p>
			<p>While Cambridge Analytica might have not been able to prove their claims, that has
				not stopped nor mitigated data metrics’ use in other fields such as medicine
				where the stakes of such reverse engineering experiments are particularly high.
				Postmarket surveillance, for example, was once a practice unique to digital
				technology companies but in recent years has become a standardized practice by
				which medical and pharmaceutical companies can expedite the lengthy and costly
				process of developing new drugs and increase their profitability. Chun echoes
				Olivia Banner’s cautionary tale about medicine’s increasing reliance on this
				practice in <title rend="italic">Communicative Biocapitalism</title> <ptr target="#banner2017"/>. Once relegated to decades of costly and lengthy
				trials, today new drugs are commonly studied only after they have been approved
				for the public, leading to drugs later being pulled from the market after
				causing significant and lasting damage to patients. The adverse effects of
				OxyContin, for example, were only discovered years after its approval to the
				general public. Such examples demonstrate how our growing faith in the power of
				big data analytics in fields such as medicine is ultimately predicated on our
				underlying faith that the benefits of experimentation always outweigh its
				potential costs.</p></div>
			<div>
			<head>Rethinking the Role of the Digital Humanities</head>
			<p>Chun’s description of the apparent parallels between eugenics and big data
				surveillance mechanisms make the monograph a particularly important read for
				digital humanities scholars, particularly those who work in the digital medical
				or health humanities. In particular, Chun’s five-step program to counter
				data-informed discrimination and repair <quote rend="inline">the mistakes of our discriminatory
					past</quote> <ptr target="#chun2021" loc="2"/> offers a possible set of responses for scholars interested
				in working toward more socially and culturally conscious ways of designing and
				working with machinic systems. According to Chun, being more socially and
				culturally conscious ultimately entails critical interrogation of the historical
				and political conditions which propel and inform the logic underlying our
				algorithms and data structures. This process of interrogation, however, hinges
				on embracing our moral responsibility in participating in these experiments. </p>
			<p>As Hannah Arendt once argued, eugenics was able to find a home in German society,
				because it was so banal and deflected individual moral responsibility to
				institutional structures <ptr target="#arendt1963"/>. In the case of Nazi officers like
				Eichmann, Arendt explains that they deflected moral accountability for their
				active participation in the murder and torture of millions of people by claiming
				that they were just following administrative commands and orders from higher up.
				Yet, Arendt aptly explains that one’s moral responsibility only increases in
				such situations where you might not know the victim nor personally be the one
				who carried out the execution. Responding to Eichmann’s defense, the judgment of
				the court ruled <quote rend="inline">On the contrary in general the degree of responsibility
				increases as we draw further away from the man who uses the fatal instrument
				with his own hands</quote> <ptr target="#arendt1963" loc="247"/>.
				Likewise, are we just following the data? Put differently, if a decision is made
				on the basis of data metrics, who exactly bears moral responsibility for its
				outcomes? </p>
			<p>Although machine learning and AI systems might have ushered in a new historical
				era constituted by the promise of reverse engineering social systems, this era
				is one which carries with it many of the same moral problems of the one that
				preceded it. Perhaps more is needed than just critical interrogation and
				awareness of the propensity for algorithms to reorganize social institutions
				around polarizing divides. Such new technological innovations reveal the need
				for a new moral system, one with the potential to hold individuals and
				organizations accountable for the outcomes of their digital experiments on
				social, political, and economic institutions. If board members and executives of
				a company were held morally responsible for the outcomes of their use and
				application of data metrics, would they be more likely to critically interrogate
				the assumptions behind those metrics? How exactly can scholars break up this
				synergistic nexus between the corporate boardroom, Congress, and laboratory? </p>
			<p>Ultimately, Chun’s monograph exposes the urgency for more investment in the
				digital humanities, particularly when it comes to the study of ethics. As the
				digital humanities comes to define itself as a field, ethics must remain a
				nonnegotiable and fundamental component of the field’s central task and purpose.
				The development, creation, and implementation of a moral system which holds
				people accountable for the outcomes of their data metrics, modeling, and social
				experiments must take precedence in the field as the digital humanities works
				toward the development and application of more socially and culturally conscious
				ways of using and applying digital technologies. We might not know what those
				moral systems look like in practice today, but the field is already increasingly
				coalescing around critical assessments and interrogations of the material and
				human costs of digital technologies as awareness and evidence of digital media’s
				costs continues to accumulate. The era of big data might have given humanity
				god-like abilities, yet perhaps it is the task of the digital humanities to
				expose how those god-like powers have made us even more flawed and vulnerable
				than we were before. </p>
			</div>
			
		</body>
		<back>
			<listBibl>
				<bibl xml:id="arendt1963" label="Arendt 1963"> Arendt, H. 
						<title rend="italic">Eichmann in Jerusalem: A Report on the Banality of
							Evil</title>. Penguin (1963).
				</bibl>
				<bibl xml:id="banner2017" label="Banner 2017"> Banner, O. 
						<title rend="italic">Communicative Biocapitalism: The Voice of the Patient in
							Digital Health and the Health Humanities</title>. University of
						Michigan Press, Grand Rapids (2017).
				</bibl>
				<bibl xml:id="bergeron2004" label="Bergeron and Chan 2004"> Bergeron, B. and P. Chan.
						<title rend="italic">Biotech Industry: A Global, Economic, and Financing
							Overview</title>. John Wiley &amp;
						Sons, Hoboken (2004).</bibl>
				<bibl xml:id="chun2021" label="Chun 2021"> Chun, W. H. K. <title rend="italic">Discriminating Data: Correlation, Neighborhoods, and the
							New Politics of Recognition</title>. MIT Press,
					Cambridge
						(2021).</bibl>
				<bibl xml:id="newyorktimes2000" label="The New York Times 2000"> The New York Times. <title rend="quotes">Text of the White House Statements on the Human Genome Project.</title> New York
					(27 June 2000).
					<ref target="https://archive.nytimes.com/www.nytimes.com/library/national/science/062700sci-genome-text.html">https://archive.nytimes.com/www.nytimes.com/library/national/science/062700sci-genome-text.html</ref>.</bibl>
			</listBibl>
		</back>
	</text>
</TEI>
