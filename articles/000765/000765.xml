<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="../../common/schema/DHQauthor-TEI.rng"    type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0" ?>
<?xml-model href="../../common/schema/DHQauthor-TEI.isosch" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-model href="../../common/schema/dhqTEI-ready.sch"     type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<TEI xmlns=      "http://www.tei-c.org/ns/1.0"
     xmlns:cc=   "http://web.resource.org/cc/"
     xmlns:dhq=  "http://www.digitalhumanities.org/ns/dhq"
     xmlns:html= "http://www.w3.org/1999/xhtml"
     xmlns:mml=  "http://www.w3.org/1998/Math/MathML"
     xmlns:rdf=  "http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Introducing Booksnake: A Scholarly App for
               Transforming Existing Digitized Archival Materials into Life-Size Virtual Objects for
               Embodied Interaction in Physical Space, using IIIF and Augmented
               Reality<!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->


            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Sean <dhq:family>Fraga</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-6804-4486<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>sfraga@usc.edu</email>
               <dhq:bio>
                  <p>Sean Fraga is the creator of and project director for Booksnake, which he
                     developed as an Andrew W. Mellon postdoctoral fellow with the Humanities in a
                     Digital World program at the University of Southern California. He is currently
                     an Assistant Professor (Teaching) of Environmental Studies and History at
                     USC.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Christy <dhq:family>Ye</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>christyspace1731@gmail.com</email>
               <dhq:bio>
                  <p>Christy Ye is a game developer and designer who creates interactive experiences
                     around meaningful play and narratives in projects across a variety of media.
                     She holds an MFA in Interactive Media and Game Design from the USC School of
                     Cinematic Arts.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Henry <dhq:family>Huang</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>henry2423@gmail.com </email>
               <dhq:bio>
                  <p>Shih-Hsuan (Henry) Huang is an experienced software engineer with a Masterâ€™s
                     degree in Computer Science from the University of Southern California. He
                     commenced his professional journey at Apple, where he has dedicated his career
                     to developing innovative solutions that positively impact the world.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Zack <dhq:family>Sai</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>saidane@usc.edu</email>
               <dhq:bio>
                  <p>Zack Sai is an undergraduate student majoring in Computer Science, with a minor
                     in mobile app development, in the USC Viterbi School of Engineering. </p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Michael <dhq:family>Hughes</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>hughesmr@usc.edu</email>
               <dhq:bio>
                  <p>Michael Hughes is an undergraduate student majoring in Computer Science in the
                     USC Viterbi School of Engineering. </p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>April <dhq:family>Yao</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>siyuyao@usc.edu</email>
               <dhq:bio>
                  <p>April Yao is a Master's student in Computer Science at the University of
                     Southern California.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Samir <dhq:family>Ghosh</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0009-0004-4575-4954<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of California, Santa Cruz</dhq:affiliation>
               <email>samir.ghosh@ucsc.edu</email>
               <dhq:bio>
                  <p>Samir Ghosh is a PhD Student in the Department of Computational Media at UC
                     Santa Cruz. He researches virtual reality interfaces at the Social Emotional
                     Technology Lab.</p>
               </dhq:bio>
            </dhq:authorInfo>


         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000765<!--including leading zeroes: e.g. 000110--></idno>
            <idno type="volume">019<!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue">1<!--issue number, without leading zeroes: e.g. 2--></idno>
            <date when="2025-04-04">4 April 2025<!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp="#ar"/>
               <term corresp="#digital_libraries"/>
               <term corresp="#mobile"/>
               <term corresp="#code_studies"/>

            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item>Augmented reality</item>
                  <item>IIIF</item>
                  <item>Digital Collections</item>
                  <item>Humanities software</item>
                  <item>Mobile app development</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000765/000765.xml">GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>

         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>We introduce Booksnake, a new mobile app that makes it feel like digitized archival
               items are physically present in a userâ€™s real-world surroundings by using the
               augmented reality (AR) technology in consumer smartphones and tablets. Unlike
               humanities projects that use virtual reality (VR) or AR to publish custom content,
               Booksnake is a general-purpose, content-agnostic tool compatible with existing online
               collections that support the International Image Interoperability Framework (IIIF).
               In this article, we critique existing flat-screen image viewers and discuss the
               benefits of embodied interaction with archival materials. We contextualize Booksnake
               within the broader landscape of immersive technologies for cultural heritage. We
               detail the technical pipeline by which Booksnake transforms existing digitized
               archival materials into custom life-size virtual objects for interaction in physical
               space. We conclude with a brief discussion of the future of the immersive
               humanities.</p>
         </dhq:abstract>

         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>Learn about Booksnake, a new app for bringing digitized cultural heritage materials
               into physical space via augmented reality, creating a novel spatial interface for
               embodied research with digitized materials.</p>
         </dhq:teaser>
      </front>

      <body>




         <div>

            <head>INTRODUCTION</head>

            <p>Close engagement with primary sources is foundational to humanities research,
               teaching, and learning <ptr target="#falbo_2000"/>, <ptr target="#schmiesing_hollis_2002"/>, <ptr target="#toner_1993"/>. We are fortunate
               to live in an age of digital abundance: Over the past three decades, galleries,
               libraries, archives, and museums (collectively known as GLAM institutions) have
               undertaken initiatives to digitize their collection holdings, making millions of
               primary-source archival materials freely available online and more accessible to more
               people than ever before <ptr target="#rosenzweig_2003"/>,<ptr target="#solberg_2012"/>, <ptr target="#ramsay_2014"/><ptr target="#putnam_2016"/>. Increasingly, the
               worldâ€™s myriad cultural heritage materials â€” architectural plans, books, codices,
               correspondence, drawings, ephemera, manuscripts, maps, newspapers, paintings,
               paperwork, periodicals, photographs, postcards, posters, prints, sheet music,
               sketches, slides, and more â€” are now only a click away.</p>

            <p>But interacting with digitized archival materials in a Web browser can be a
               frustrating experience â€” one that fails to replicate the embodied engagement possible
               during in-person research. As users, we have become resigned to the limitations of
               Web-based image viewers. How we have to fiddle with <hi rend="quotes">Zoom In</hi>
               and <hi rend="quotes">Zoom Out</hi> buttons to awkwardly jump between fixed levels of
               magnification. How we must laboriously click and drag, click and drag, over and over,
               to follow a line of text across a page, or read the length of a newspaper column, or
               trace a river over a map. How we canâ€™t ever quite tell how big or small something is.
               We have long accepted these frictions as necessary compromises to quickly and easily
               view materials online instead of making an inconvenient trip to a physical archive
               itself â€” even as we also understand that clicking around in a Web viewer pales in
               comparison to the rich, fluid, embodied experience of interacting with a physical
               item in a museum gallery or library reading room.</p>

            <figure xml:id="figure01">
               <head>Booksnake transforms existing digitized archival materials, like this 1909
                  birds-eye-view map of Los Angeles, into custom virtual objects for embodied
                  exploration in physical space, by using the augmented reality technology in
                  consumer smartphones and tablets. The virtual object remains anchored to the table
                  surface even as the user and device move.</head>
               <figDesc/>
               <graphic url="resources/images/figure01.jpg" style="width: 700px"/>
            </figure>

            <p>As a step toward moving past these limitations, this research team presents a new
               kind of image viewer. Booksnake is a scholarly app for iPhones and iPads that enables
               users to interact with existing digitized archival materials at life size in physical
               space (see figure 1). Instead of displaying digital images on a flat screen for
               indirect manipulation, Booksnake uses augmented reality, the process of overlaying a
               virtual object on physical space <ptr target="#azuma_etal_2001"/>, to bring digitized
               items into the physical world for embodied exploration. Existing image viewers
               display image files as a means of conveying visual information about an archival
               item. In contrast, Booksnake converts image files into virtual objects in order to
               create the feeling of being in the item's presence. To do this, Booksnake
               automatically transforms digital images of archival materials into custom,
               size-accurate virtual objects, then dynamically inserts these virtual objects into
               the live camera view on a smartphone or tablet for interaction in physical space (see
               figure 2). Booksnake is available as a free app for iPhone and iPad on Apple's App
               Store: <ref target="https://apps.apple.com/us/app/booksnake/id1543247176.">https://apps.apple.com/us/app/booksnake/id1543247176.</ref></p>


            <figure xml:id="figure02">
               <head>This video shows how a Booksnake user can freely search, browse, and filter the
                  Library of Congress (LOC) digitized collections, then download LOC items to their
                  Booksnake library. When the user selects an item and taps "View in Your Space,"
                  Booksnake automatically transforms the item's digital image into a custom,
                  life-size virtual object, then displays the virtual object in the user's physical
                  surroundings.</head>
               <figDesc/>
               <media mimeType="video/mp4" url="https://www.youtube.com/embed/k9ccB3GNEvo?si=VkpQoUQMXjmNJ8fG" style="width: 700px"/>
            </figure>

            <p>Booksnake's use of AR makes it feel like a digitized item is physically present in a
               userâ€™s real-world environment, enabling closer and richer engagement with digitized
               materials. A Booksnake user aims their phone or tablet at a flat surface (like a
               table, wall, bed, or floor) and taps the screen to anchor the virtual object to the
               physical surface. As the user moves, Booksnake uses information from the deviceâ€™s
               cameras and sensors to continually adjust the virtual objectâ€™s relative position,
               orientation, and size in the camera view, such that the item appears to remain
               stationary in physical space as the user and device move. A user thus treats
               Booksnake like a lens, looking <hi rend="italic">through</hi> their deviceâ€™s screen
               at a virtual object overlaid on the physical world.</p>

            <p>The project takes its name from book snakes, the weighted strings used by archival
               researchers to hold fragile physical materials in place. Similarly, Booksnake enables
               users to keep virtual materials in place on physical surfaces in the real world. By
               virtualizing the experience of embodied interaction, Booksnake makes it possible for
               people who cannot otherwise visit an archive (due to cost, schedule, distance,
               disability, or other reasons) to physically engage with digitized materials.</p>

            <p>Booksnake represents a proof-of-concept, demonstrating that it is possible to
               automatically transform existing digital image files of cultural heritage materials
               into life-size virtual objects on demand. We have designed Booksnake to easily link
               with existing digitized collections via the International Image Interoperability
               Framework (or IIIF, pronounced <hi rend="quotes">triple-eye-eff</hi>), a
               widely-supported set of open Web protocols for digitally accessing archival materials
               and related metadata, developed and maintained by the GLAM community <ptr target="#cramer_2011"/>, <ptr target="#cramer_2015"/>, <ptr target="#snydman_sanderson_cramer_2015"/>. The first version of Booksnake uses
               IIIF to access Library of Congress digitized collections, including the Chronicling
               America collection of historical newspapers. Our goal is for Booksnake to be able to
               display any image file accessible through IIIF. At this point, the considerable
               variability in how different institutions record dimensional metadata means that we
               will have to customize Booksnake's links with each digitized collection, as we
               discuss further below. </p>

            <p>Our approach represents a novel use of AR for humanistic research, teaching, and
               learning. While humanists have begun to explore the potential of immersive
               technologies â€” primarily virtual reality (VR) and AR â€” they have largely approached
               these technologies as publication platforms: ways of hosting custom-built experiences
               containing visual, textual, spatial, and/or aural content that is manually created or
               curated by subject-matter experts. Examples include reconstructions of
               twelfth-century Angkor Wat <ptr target="#chandler_etal_2017"/>, a Renaissance
               studiolo <ptr target="#shemek_etal_2018"/>, events from the Underground Railroad <ptr target="#roth_fisher_2019"/>, a medieval Irish castle <ptr target="#herron_2020"/>, and an 18th-century Paris theatre <ptr target="#francois_etal_2021"/>. These are
               rich, immersive experiences, but they are typically limited to a specific site,
               narrative, or set of material. The work involved in manually creating these virtual
               models can be considerable, lengthy, and expensive.</p>

            <p>In contrast, Booksnake is designed as a general-purpose AR tool for archival
               exploration. Booksnake's central technical innovation is the automatic transformation
               of existing digital images of archival cultural heritage materials into
               dimensionally-accurate virtual objects. We aim to enable users to freely select
               materials from existing digitized cultural heritage collections for embodied
               interaction. Unlike custom-built immersive humanities projects, Booksnake makes it
               possible for a user to create virtual objects easily, quickly, and freely. While
               Booksnake fundamentally differs from existing Web-based image viewers, it is, like
               them, an empty frame, waiting for a user to fill it with something interesting. </p>

            <p>This article presents a conceptual and technical overview of Booksnake. We first
               critique the accepted method of viewing digitized archival materials in Web-based
               image viewers on flat screens and discuss the benefits of embodied interaction with
               archival materials, then describe results from initial user testing and potential use
               cases. Next, we contextualize Booksnake, as an AR app for mobile devices, within the
               broader landscape of immersive technologies for cultural heritage. We then detail the
               technical pipeline by which Booksnake transforms digitized archival materials into
               virtual objects for interaction in physical space. Ultimately, Booksnake's use of
               augmented reality demonstrates the potential of spatial interfaces and embodied
               interaction to improve accessibility to archival materials and activate digitized
               collections, while its presentation as a mobile app is an argument for the untapped
               potential of mobile devices to support humanities research, teaching, and
               learning.</p>

            <p>Booksnake is designed and built by a multidisciplinary team at the University of
               Southern California. It represents the combined efforts of humanities scholars,
               librarians, interactive media designers, and computer scientists â€” most of them
               students or early-career scholars â€” extending over four years.<note> Sean Fraga
                  researched and designed the project's technical architecture and user interface
                  affordances; Fraga also prepared the article manuscript. Christy Ye developed a
                  method for transforming digital images into virtual objects. Samir Ghosh ideated
                  and refined Booksnake's UI/UX fundamentals. Henry Huang built a means of
                  organizing and persistently storing item metadata and digital images using Appleâ€™s
                  Core Data framework. Fraga developed and Huang implemented a method for creating
                  size-accurate virtual objects from Library of Congress metadata. Fraga, Ye, and
                  Huang jointly designed support for compound objects, and Ye and Huang collaborated
                  to implement support for compound objects; this work was supported by an NEH
                  Digital Humanities Advancement Grant (HAA-287859-22). Zack Sai is building support
                  for the new IIIF v3.0 APIs. Michael Hughes is expanding the range of interactions
                  possible with virtual objects in AR. Siyu (April) Yao is building technical links
                  to additional digitized collections. Additionally, Peter Mancall serves as senior
                  advisor to the project. Curtis Fletcher provides strategic guidance, and Mats
                  Borges provides guidance on user testing and feedback.</note> The project has been
               financially supported by the Humanities in a Digital World program (under grants from
               the Mellon Foundation) in USC's Dornsife College of Letters, Arts, and Sciences; and
               by the Ahmanson Lab, a scholarly innovation lab in the Sydney Harman Academy for
               Polymathic Studies in USC Libraries. The research described in this article was
               supported by a Digital Humanities Advancement Grant (Level II) from the National
               Endowment for the Humanities (HAA-287859-22). The projectâ€™s next phase will be
               supported by a second NEH Digital Humanities Advancement Grant (Level II)
               (HAA-304169-25).</p>
         </div>


         <div>
            <head>FROM FLAT SCREENS TO EMBODIED EXPLORATION</head>

            <p>Flat-screen image viewers have long been hiding in plain sight. Each image viewer
               sits at the end of an institution's digitization pipeline and offer an interface
               through which users can access, view, and interact with digital image files of
               collection materials. These image viewers' fundamental characteristic is their
               transparency: The act of viewing a digital image of an archival item on a flat screen
               has become so common within contemporary humanities practices that we have
               unthinkingly naturalized it.</p>

            <p>But there is nothing <hi rend="quotes">natural</hi> about using flat-screen image
               viewers to apprehend archival materials, because everything about the digitization
               process is novel. The ability to instantly view a digitized archival item on a
               computer screen rests on technological developments from the last sixty years: the
               high-resolution digital cameras that capture archival material; the digital asset
               management software that organizes it; the cheap and redundant cloud storage that
               houses it; the high-speed networking infrastructure that delivers it; the Web
               browsers with which we access it; the high-resolution, full-color screens that
               display it. Even the concept of a graphical user interface, with its representative
               icons and mouse-based input, is a modern development, first publicly demonstrated by
               Douglas Engelbart in 1968 and first commercialized by Apple, with the Lisa and the
               Macintosh, in the early 1980s <ptr target="#fisher_2018" loc="17â€“26, 85â€“93, 104â€“117"/>. Put another way, our now-familiar ways of using a mouse or trackpad to interact
               with digitized archival materials in a Web-based flat-screen image viewer would be
               incomprehensibly alien to the people who originally created and used many of these
               materials â€” and, in many cases, even to the curators, librarians, and archivists who
               first acquired and accessioned these items. What does this mean for our relationship
               with these archival materials? </p>

            <p> Despite this, and although Web-based flat-screen image viewers sustain a robust
               technical development community, they have been largely overlooked by most digital
               humanists. A notable exception is manuscript scholars, for whom the relationship
               between text, object, and digital image is particularly important. Indeed, manuscript
               scholars have led the charge in identifying flat-screen image viewers as sites of
               knowledge creation and interpretation â€” often by expressing their frustration with
               these viewers' limited affordances or with the contextual information these viewers
               shear away <ptr target="#nolan_2013"/>, <ptr target="#szpiech_2014"/>, <ptr target="#kropf_2017"/>, <ptr target="#almas_etal_2018"/>, <ptr target="#porter_2018"/>, <ptr target="#van_zundert_2018"/>, <ptr target="#van_lit_2020"/>.</p>

            <p> To see flat-screen image viewers more clearly, it helps to understand them within
               the context of archival digitization practices. Digitization is usually understood as
               the straightforward conversion of physical objects into digital files, but this
               process is never simple and always involves multiple interpretive decisions.<note> We
                  focus here on the decisions involved in digitizing an item itself, rather than the
                  decisions about what items get digitized and why, which also, of course, inform
                  the development and structure of a given digitized collection.</note> As Johanna
               Drucker writes, <quote rend="inline">the way artifacts are [digitally] encoded
                  depends on the parameters set for scanning and photography. These already embody
                  interpretation, since the resolution of an image, the conditions of lighting under
                  which it is produced, and other factors, will alter the outcome</quote>
               <ptr target="#drucker_2013" loc="8"/>. Some of these encoding decisions are
               structured by digitization guidelines, such as the U.S. Federal Agency Digitization
               Guidelines Initiative <ptr target="#fadgi_2023"/> standards (2023), while other
               decisions, such as how to light an item or how to color-correct a digital image,
               depend on the individual training and judgment of digitization professionals.</p>

            <p> A key digitization convention is to render an archival item from an idealized
               perspective, that of an observer perfectly centered before the item. To achieve this,
               a photographer typically places the item being digitized perpendicular to the
               camera's optical axis, centers the item within the camera's view, and orthogonally
               aligns the item with the viewfinder's edges. During post-processing, a digitization
               professional can then crop, straighten, and de-skew the resulting image, or stitch
               together multiple images of a given item into a single cohesive whole. These physical
               and digital activities produce an observer-independent interpretation of an archival
               item. Put another way, archival digitization is what Donna Haraway calls a god trick,
               the act of <quote rend="inline">seeing everything from nowhere</quote>
               <ptr target="#haraway_1988" loc="582"/>.</p>

            <p> Taking these many decisions together, Drucker argues that <quote rend="inline"><emph>digitization is not representation but interpretation</emph></quote>
               <ptr target="#drucker_2013" loc="12"/> (emphasis original). Understanding
               digitization as a continuous interpretive process, rather than a simple act of
               representation, helps us see how this process extends past the production of digital
               image files and into how these files are presented to human users via into
               traditional flat-screen image viewers. </p>

            <p> Flat-screen image viewers encode a set of decisions about how we can (or should)
               interact with a digitized item. Just as decisions about resolution, lighting, and
               file types serve to construct a digitized interpretation of a physical object, so too
               do decisions about interface affordances for an image viewer serve to construct an
               interpretive space. Following Drucker, image viewers do not simply <hi rend="italic">represent </hi>digital image files to a user, they <hi rend="italic">interpret</hi> them. Decisions by designers and developers about an image
               viewerâ€™s interface affordances (how to zoom, turn pages, create annotations, etc.)
               structure the conditions of possibility for a user's interactions with a digitized
               item. </p>

            <p> The implications of these decisions are particularly visible when comparing
               different image viewers. For example, Mirador, a leading IIIF-based image viewer,
               enables users to compare two images from different repositories side-by-side <ptr target="#project_mirador_n.d."/>. A different IIIF-based image viewer,
               OpenSeadragon, is instead optimized for viewing individual <quote rend="inline">high-resolution zoomable images</quote>
               <ptr target="#openseadragon_n.d."/>. Mirador encourages juxtaposition, while
               OpenSeadragon emphasizes attention to detail. These two viewers each represent a
               particular set of assumptions, goals, decisions, and compromises, which in turn shape
               how their respective users encounter and read a given item, the interpretations those
               users form, and the knowledge they create. </p>

            <p> Flat-screen image viewers generally share three attributes that collectively
               structure a user's interactions with a digitized item. First, and most importantly,
               flat-screen image viewers directly reproduce the idealized <quote rend="inline">view
                  from nowhere</quote> delivered by the digitization pipeline. Flat-screen image
               viewers could present digital images in any number of ways â€” upside down, canted at
               an angle away from the viewer, obscured by a digital curtain. But instead,
               flat-screen image viewers play the god trick. Second, flat-screen image viewers rely
               on indirect manipulation via a mouse or trackpad. To zoom, pan, rotate, or otherwise
               navigate a digitized item, a user must repeatedly click buttons or click and drag,
               positioning and re-positioning the digital image in order to apprehend its content.
               These interaction methods create friction between the user and the digitized item,
               impeding discovery <ptr target="#sundar_etal_2013"/>. Finally, flat-screen image
               viewers arbitrarily scale digital images to fit a userâ€™s computer screen. <quote rend="inline">Digitisation doesn't make everything equal, it just makes everything
                  the same size,</quote> writes <ptr target="#crane_2021"/>. In a flat-screen image
               viewer, a monumental painting and its postcard reproduction appear to be the same
               size, giving digitized materials a false homogeneity and disregarding the contextual
               information conveyed by an item's physical dimensions. In sum, flat-screen image
               viewers are observer-independent interfaces for indirect manipulation of arbitrarily
               scaled digitized materials. </p>

            <p> In contrast, Booksnake is an observer-dependent interface for embodied interaction
               with life-size digitized materials. When we encounter physical items, we do so
               through our bodies, from our individual point of view. Humans are not simply <quote rend="inline">two eyeballs attached by stalks to a brain computer,</quote> as
               Catherine D'Ignazio and Lauren Klein write in their discussion of data
               visceralization <ptr target="#dignazio_klein_2020" loc="Â§3"/>. We strain toward the
               far corners of maps. We pivot back and forth between the pages of newspapers. We curl
               ourselves over small objects like daguerreotypes, postcards, or brochures. These
               embodied, situated, perspectival experiences are inherent to our interactions with
               physical objects. By using augmented reality to pin life-size digitized items to
               physical surfaces, Booksnake enables and encourages this kind of embodied
               exploration. With Booksnake, you can move around an item to see it from all sides,
               step back to see it in its totality, or get in close to focus on fine details.
               Integral to Booksnake is Haraway's idea of <quote rend="inline">the particularity and
                  embodiment of all vision</quote>
               <ptr target="#haraway_1988" loc="582"/>. Put another way, Booksnake lets you break
               out of the god view and see an object as only you can. As an image viewer with a
               spatial interface, Booksnake is an argument for a way of seeing that prioritizes
               embodied interaction with digitized archival materials at real-world size. In this,
               Booksnake is more than a technical critique of existing flat-screen image viewers. It
               is also an <hi rend="italic">intellectual</hi> critique of how these image viewers
               foreclose certain types of knowledge creation and interpretation. Booksnake thus
               offers <hi rend="italic">a new way of looking</hi> at digitized materials.</p>

            <p> This is an interpretive choice in our design of Booksnake. Again, image viewers do
               not simply <hi rend="italic">represent </hi>digital image files to a user, they <hi rend="italic">interpret</hi> them. Booksnake relies on the same digital image
               files as do flat-screen image viewers, and these files are just as mediated in
               Booksnake as they are when viewed in a flat-screen viewer. (<quote rend="inline">There is no unmediated photograph,</quote> Haraway writes <ptr target="#haraway_1988" loc="583"/>. Indeed, Booksnake even displays these image
               files on the flat screen of a smartphone or tablet â€” but its use of AR creates the
               illusion that the object is physically present. Where existing flat-screen image
               viewers foreground the digital-ness of digitized objects, Booksnake instead recovers
               and foregrounds their object-ness, their materiality. Drucker writes that <quote rend="inline">information spaces drawn from a point of view, rather than as if
                  they were observer independent, reinsert the subjective standpoint of their
                  creation</quote>
               <ptr target="#drucker_2011" loc="Â¶20"/>. Drucker was writing about data
               visualization, but her point holds for image viewers (which, after all, represent a
               kind of data visualization). Our design decisions aim to create an interpretive space
               grounded in individual perspective, with the goal of helping a Booksnake user get
               closer to the <quote rend="inline">subjective standpoint</quote> of an archival
               item's original creators and users. Even as the technology underpinning Booksnake is
               radically new, it enables methods of embodied looking that are very old, closely
               resembling in form and substance physical, pre-digitized ways of looking. </p>
         </div>



         <div>

            <head>INITIAL TESTING RESULTS AND USE CASES</head>

            <p> Booksnake's emphasis on embodied interaction gives it particular potential as a tool
               for humanities education. Embodied interaction is a means of accessing situated
               knowledges <ptr target="#haraway_1988"/> and is key to apprehending cultural heritage
               materials in their full complexity. As museum scholars and educators have
               demonstrated, this is true both for physical objects <ptr target="#kai-kee_latina_sadoyan_2020"/> and for virtual replicas <ptr target="#kenderdine_yip_2019"/>. Meanwhile, systematic reviews show AR can support
               student learning gains, motivation, and knowledge transfer <ptr target="#bacca_etal_2014"/>. By using AR to make embodied interaction possible
               with digitized items, Booksnake supports student learning through movement,
               perspective, and scale. For example, a student could use Booksnake to physically
               follow an explorer's track across a map, watch a paintingâ€™s details emerge as she
               moves closer, or investigate the relationship between a posterâ€™s size and its message
               â€” interactions impossible with a flat-screen viewer. Here, we briefly highlight
               themes that have emerged in user and classroom testing, then discuss potential use
               cases.</p>

            <p> A common theme in classroom testing was that Booksnake's presentation of life-size
               virtual objects made students feel like they were closer to digitized sources. A
               colleague's students used Booksnake as part of an in-class exercise to explore
               colonial-era Mesoamerican maps and codices, searching for examples of cultural
               syncretism. In a post-activity survey, students repeatedly described a feeling of
               presence. Booksnake gave one student <quote rend="inline">the feeling of actually
                  seeing the real thing up close.</quote> Another student wrote that <quote rend="inline">I felt that I was actually working with the codex.</quote> A third
               wrote that <quote rend="inline">it was cool to see the resource, something I will
                  probably never get to flip through, and get to flip through and examine
                  it.</quote> Students also commented on how Booksnake represented these items'
               size. One student wrote that Booksnake <quote rend="inline">gave me the opportunity
                  to get a better idea of the scale of the pieces.</quote> Another wrote that <quote rend="inline">I liked being able to see the material <q>physically</q> and see the
                  scale of the drawings on the page to see what they emphasized and how they took up
                  space.</quote> These comments suggest that Booksnake has the most potential to
               support embodied learning activities that ask students to engage with an item's
               physical features (such as an object's size or proportions), or with the relationship
               between these features and the item's textual and visual content.</p>

            <p> During one-on-one user testing sessions, two history Ph.D. students each separately
               described feeling closer to digitized sources. One tester described Booksnake as
               opening <quote rend="inline">a middle ground</quote> between digital and physical
               documents by offering the <quote rend="inline">flexibility</quote> of digital
               materials, but the <quote rend="inline">sensorial experience of closeness</quote>
               with archival documents. The other tester said that Booksnake brought <quote rend="inline">emotional value</quote> to archival materials. <quote rend="inline">Being able to stand up and lean over it [the object] brought it to life a little
                  more,</quote> this tester said. <quote rend="inline">You canâ€™t assign research
                  utility to that, but it was more immersive and interactive, and in a way
                  satisfying.</quote> Their comments suggest that Booksnake can enrich engagement
               with digitized materials, especially by producing the feeling of physical
               presence.</p>

            <p> As a virtualization technology, Booksnake makes it possible to present archival
               materials in new contexts, beyond the physical site of the archive itself. Jeremy
               Bailenson argues that virtualization technologies are especially effective for
               scenarios that would otherwise be rare, impractical, destructive, or expensive <ptr target="#bailenson_2018"/>. Here, we use these four characteristics to describe
               potential Booksnake use cases. First, it is rare to have an individual, embodied
               interaction with one-of-a-kind materials, especially for people who do not work at
               GLAM institutions. A key theme in student comments from the classroom testing
               described above, for example, was that Booksnake gave students a feeling of direct
               engagement with unique Mesoamerican codices. Second, it is often logistically
               impractical for a class to travel to an archive (especially for larger classes) or to
               bring archival materials into classrooms outside of library or museum buildings. The
               class described above, for example, had around eighty students, and Booksnake made it
               possible for each student to individually interact with these codices, during
               scheduled class time and in their existing classrooms. Third, the physical fragility
               and the rarity or uniqueness of many archival materials typically limits who can
               handle them or where they can be examined. Booksnake makes it possible to engage with
               virtual archival materials in ways that would cause damage to physical originals. For
               example, third-grade students could use Booksnake to explore a historic painting by
               walking atop a virtual copy placed on their classroom floor, or architectural
               historians could use Booksnake to bring virtual archival blueprints into a physical
               site. Finally, it is expensive (in both money and time) to physically bring together
               archival materials that are held by two different institutions. With Booksnake, a
               researcher could juxtapose digitized items held by one institution with physical
               items held by a different institution, for purposes of comparison (such as comparing
               a preparatory sketch to a finished painting) or reconstruction (such as reuniting
               manuscript pages that had been separated). In each of these scenarios, Booksnake's
               ability to produce virtual replicas of physical objects lowers barriers to embodied
               engagement with archival materials and opens new possibilities for research,
               teaching, and learning. </p>
         </div>


         <div>

            <head>IMMERSIVE TECHNOLOGIES FOR CULTURAL HERITAGE</head>

            <p> Booksnake is an empty frame. It leverages AR to extend the exploratory freedom that
               users associate with browsing online collections into physical space. In doing so,
               Booksnake joins a small collection of digital tools and projects using immersive
               technologies as the basis for interacting with cultural heritage materials and
               collections.</p>

            <p> A dedicated and technically adept user could use 3D modeling software (such as
               Blender, Unity, Maya, or Reality Composer) to manually transform digital images into
               virtual objects. This can be done with any digital image, but is time-intensive and
               breaks links between images and metadata. Booksnake automates this process, making it
               more widely accessible, and preserves metadata, supporting humanistic inquiry.</p>

            <p> The Google Arts &amp; Culture app offers the most comparable use of humanistic AR.
               Like Booksnake, Arts &amp; Culture is a mobile app for smartphones and tablets that
               offers AR as an interface for digitized materials. A user can activate the appâ€™s
                  <quote rend="inline">Art Projector</quote> feature to <quote rend="inline">display
                  life-size artworks, wherever you are</quote> by placing a digitized artwork in
               their physical surroundings <ptr target="#luo_2019"/>. But there are three key
               differences between Googleâ€™s app and Booksnake. First, AR is one of many possible
               interaction methods in Googleâ€™s app, which is crowded with stories, exhibits, videos,
               games, and interactive experiences. In contrast, Booksnake emphasizes AR as its
               primary interface, foregrounding the embodied experience. Second, Googleâ€™s app
               focuses on visual art (such as paintings and photographs), while Booksnake can
               display a broader range of archival and cultural heritage materials, making it more
               useful for humanities scholars. (Booksnake can also display paginated materials, as
               we discuss further below, while Google's app cannot.) Finally â€” and most importantly
               â€” Googleâ€™s app relies on a centralized database model. Google requires participating
               institutions to upload their collection images and metadata to Googleâ€™s own servers,
               so that Google can format and serve these materials to users <ptr target="#google_n.d."/>. In contrast, Booksnakeâ€™s use of IIIF enables institutions
               to retain control over their digitized collections and expands the capabilities of an
               open humanities software ecosystem.</p>

            <p>Another set of projects approach immersive technologies as tools for designing and
               delivering exhibition content. Some use immersive technologies to enhance physical
               exhibits, such as Veholder, a project exploring technologies and methods for
               juxtaposing 3D virtual and physical objects in museum settings <ptr target="#haynes_2018"/>, <ptr target="#haynes_2019"/>. Others are tools for using
               immersive technologies to create entirely virtual spaces. Before its discontinuation,
               many GLAM institutions and artists adapted Mozilla Hubs, a general-purpose tool for
               building 3D virtual spaces that could be accessed using a flat-screen Web browser or
               a VR headset, to build virtual exhibition spaces, although users were required to
               manually import digitized materials and construct virtual replicas <ptr target="#cool_2022"/>. Another project, Diomira Galleries, is a prototype tool for
               building VR exhibition spaces with IIIF-compliant resources <ptr target="#bifrost_2023"/>. Like Booksnake, Diomira uses IIIF to import digital
               images of archival materials, but Diomira arbitrarily scales these images onto
               template canvases that do not correspond to an itemâ€™s physical dimensions. As with
               Booksnake, these projects demonstrate the potential of immersive technologies for new
               research interactions and collection activation with digitized archival
               materials.</p>

            <p>Finally, we are building Booksnake as an AR application for existing consumer mobile
               devices as a way of lowering barriers to immersive engagement with cultural heritage
               materials. Most VR projects require expensive special-purpose headsets, which has
               limited adoption and access <ptr target="#greengard_2019"/>. In contrast, AR-capable
               smartphones are ubiquitous, enabling a mobile app to tap the potential of a large
               existing user base, and positioning such an app to potentially mitigate racial and
               socioeconomic digital divides in the United States. More Americans own smartphones
               than own laptop or desktop computers <ptr target="#pew_2021"/>. And while Black and
               Hispanic adults in the United States are less likely to own a laptop or desktop
               computer than white adults, Pew researchers have found <quote rend="inline">no
                  statistically significant racial and ethnic differences when it comes to
                  smartphone or tablet ownership</quote>
               <ptr target="#atske_perrin_2021"/>. Similarly, Americans with lower household incomes
               are more likely to rely on smartphones for Internet access <ptr target="#vogels_2021"/>. Smartphones are thus a key digital platform for engaging and including the
               largest and most diverse audience. Developing an Android version of Booksnake will
               enable us to more fully deliver on this potential. </p>
         </div>



         <div>

            <head>AUTOMATICALLY TRANSFORMING DIGITAL IMAGES INTO VIRTUAL OBJECTS</head>

            <p>Booksnakeâ€™s central technical innovation is automatically transforming existing
               digital images of archival cultural heritage materials into dimensionally-accurate
               virtual objects. To make this possible, Booksnake connects existing software
               frameworks in a new way. </p>

            <p>First, Booksnake uses IIIF to download images and metadata.<note> The first version
                  of Booksnake supports the IIIF v2.1 APIs. We are currently building support for
                  the new IIIF v3.0 APIs.</note> IIIF was proposed in 2011 and developed over the
               early 2010s. Today, the IIIF Consortium is composed of sixty-five global GLAM
               institutions, from the British Library to Yale University <ptr target="#iiif_n.d."/>,
               while dozens more institutions offer access to their collections through IIIF because
               several common digital asset management (DAM) platforms, including CONTENTdm, LUNA,
               and Orange DAM, support IIIF <ptr target="#oclc_n.d."/>
               <ptr target="#l_i_2018"/>
               <ptr target="#orangelogic_n.d."/>. This widespread use of IIIF means that Booksnake
               is readily compatible with many existing digitized collections. By using IIIF,
               Booksnake embraces and extends the capabilities of a robust humanities software
               ecosystem. By demonstrating a novel method to transform existing IIIF-compliant
               resources for interaction in augmented reality, we hope that Booksnake will drive
               wider IIIF adoption and standardization. </p>

            <p>Next, Booksnake uses a pair of Apple software frameworks, ARKit and RealityKit.
               ARKit, introduced in 2017, interprets and synthesizes data from an iPhone or iPad's
               cameras and sensors to understand a user's physical surroundings and to anchor
               virtual objects to horizontal and vertical surfaces <ptr target="#apple_n.d._a"/>.
               RealityKit, introduced in 2019, is a framework for rendering and displaying virtual
               objects, as well as managing a user's interactions with them (for example, by
               interpreting a userâ€™s on-screen touch gestures) <ptr target="#apple_n.d._b"/>. Both
               ARKit and RealityKit are built into the device operating system, enabling us to rely
               on these frameworks to create virtual objects and to initiate and manage AR sessions. </p>

            <p>Developing Booksnake as a native mobile app, rather than a Web-based tool, makes it
               possible for Booksnake to take advantage of the powerful camera and sensor
               technologies in mobile devices. We are developing Booksnakeâ€™s first version for
               iPhone and iPad because Appleâ€™s tight integration of hardware and software supports
               rapid AR development and ensures consistency in user experience across devices. We
               plan to extend our work by next developing an Android version of Booksnake, improving
               accessibility. Another development approach, WebXR, a cross-platform Web-based AR/VR
               framework currently in development, lacks the features to support our project
               goals.</p>

            <figure xml:id="figure03">
               <head>This overview summarizes Booksnake's technical pipeline. Booksnake uses the
                  International Image Interoperability Framework (IIIF) to download item metadata
                  and digital images, then uses Apple's RealityKit to construct a custom virtual
                  object, and finally uses Apple's ARKit to display the custom virtual object in
                  physical space.</head>
               <figDesc/>
               <graphic url="resources/images/figure03.pdf" style="width: 700px"/>
            </figure>

            <p>Booksnake thus links IIIF with RealityKit and ARKit to produce a novel result: an
               on-demand pipeline for automatically transforming existing digital images of archival
               materials into custom virtual objects that replicate the physical originalâ€™s
               real-world proportions, dimensions, and appearance, as well as an AR interface for
               interacting with these virtual objects in physical space. (See figure 3.) How does
               Booksnake do this?</p>

            <p>Booksnake starts with the most humble of Internet components: the URL. A Booksnake
               user first searches and browses an institutionâ€™s online catalog through an in-app Web
               view. Booksnake offers an <q>Add</q> button on catalog pages for individual items.
               When the user taps this button to add an item to their Booksnake library, Booksnake
               retrieves the item pageâ€™s URL. Because of Appleâ€™s privacy restrictions and
               application sandboxing, this is the only information that Booksnake can read from a
               given Web page; it cannot directly access content on the page itself. Instead,
               Booksnake translates the item page URL into the corresponding IIIF manifest URL.</p>

            <p>An IIIF manifest is a JSON file â€” a highly structured, computer-readable text file â€”
               that contains a version of the itemâ€™s catalog record, including metadata and URLs for
               associated images. The exact URL translation process varies depending on how an
               institution has implemented IIIF, but in many cases it is as simple as appending
                  <code>"/manifest.json"</code> to the item URL. For example, the item URL for the
               Library of Congressâ€™s 1858 <quote rend="inline">Chart of the submarine Atlantic
                  Telegraph,</quote> is <ref target="https://www.loc.gov/item/2013593216/">https://www.loc.gov/item/2013593216/</ref>, and the itemâ€™s IIIF manifest URL is
                  <ref target="https://www.loc.gov/item/2013593216/manifest.json">https://www.loc.gov/item/2013593216/manifest.json</ref>. In other cases,
               Booksnake may extract a unique item identifier from the item URL, then use that
               unique identifier to construct the appropriate IIIF manifest URL. Booksnake then
               downloads and parses the itemâ€™s IIIF manifest.</p>

            <p>First, Booksnake extracts item metadata from the IIIF manifest. Booksnake uses this
               metadata to construct an item page in the appâ€™s Library tab, enabling a user to view
               much of the same item-level metadata visible in the host institutionâ€™s online
               catalog. An IIIF manifest presents metadata in key-value pairs, with each pair
               containing a general label (or key) and a corresponding entry (or value). For
               example, the IIIF manifest for the 1858 Atlantic telegraph map mentioned above
               contains the key <q>Contributors</q>, representing the catalog-level field listing an
               itemâ€™s authors or creators, and the corresponding item-level value <quote rend="inline">Barker, Wm. J. (William J.) (Surveyor),</quote> identifying the
               creator of this specific item. Importantly, while the key-value pair structure is
               generally consistent across IIIF manifests from different institutions, the key names
               themselves are not. The "Contributors" key at one institution may be named
                  <q>Creators</q> at another institution, and <q>Authors</q> at a third. The current
               version of Booksnake simply displays the key-value metadata as provided in the IIIF
               manifest. As Booksnake adds support for additional institutions, we plan to identify
               and link different keys representing the same metadata categories (such as
                  <q>Contributors,</q>
               <q>Creators,</q> and <q>Authors</q>). This will enable users, for example, to sort
               items from different institutions by common categories like <q>Author</q> or <q>Date
                  created,</q> or to search within a common category.</p>

            <p>Second, Booksnake uses image URLs contained in the IIIF manifest to download the
               digital images (typically JPEG or JPEG 2000 files) associated with an item's catalog
               record. Helpfully, IIIF image URLs are structured so that certain requests â€” like the
               imageâ€™s size, its rotation, even whether it should be displayed in color or
               black-and-white â€” can be encoded in the URL itself. Booksnake leverages this
               affordance to request images that are sufficiently detailed for virtual object
               creation, which sometimes means requesting images larger than what the institution
               serves by default. For example, Library of Congress typically serves images that are
               25% of the original size, but Booksnake modifies the IIIF image URL to request images
               at 50% of original size. Our testing indicates that this level of resolution produces
               sufficiently detailed virtual objects, without visible pixelation, for Library of
               Congress collections.<note> To avoid pixelated virtual objects, the host institution
                  must serve through IIIF a sufficiently high-resolution digital image.</note> We
               anticipate customizing this value for other institutions. Having downloaded the
               itemâ€™s metadata and digital images, Booksnake can now create a virtual object
               replicating the physical original. </p>

            <figure xml:id="figure04">
               <head>These tables show the relationship between an item's physical dimensions, its
                  digitization resolution, and the pixel dimensions of the resulting digital image.
                  Pixel dimensions are the product of the itemâ€™s physical dimensions and its
                  digitization resolution, and can vary considerably depending on the resolution at
                  which it is digitized. As figure 3a shows, a 10 inch by 10 inch item digitized at
                  300 ppi would produce a 3,000 by 3,000 pixel digital image (10 x 300 = 3,000). The
                  same item digitized at 600 ppiâ€”twice the resolutionâ€”would produce a digital image
                  measuring 6,000 by 6,000 pixels (10 x 600 = 6,000). The same item digitized at
                  1,200 ppi would produce a digital image measuring 12,000 by 12,000 pixels (10 x
                  1,200 = 12,000). Importantly, there is not a consistent relationship between pixel
                  dimensions and physical dimensions, as figure 3b shows. Digitization is a one-way
                  street: Without knowing an itemâ€™s digitization resolution, it is not
                  mathematically possible to convert pixel dimensions back into physical dimensions.
                  A 6000 x 6000 pixel image may represent a 20 x 20 inch item digitized at 300 ppi,
                  or a 10 x 10 inch item digitized at 600 ppi, or a 5 x 5 inch item digitized at
                  1,200 ppi. For this reason, Booksnake requires either an itemâ€™s physical
                  dimensions, or both the itemâ€™s pixel dimensions and its digitization resolution. </head>
               <figDesc/>
               <graphic url="resources/images/figure04.pdf" style="width: 700px"/>
            </figure>

            <p>Our initial goal was for Booksnake to display any image resource available through
               IIIF, but we quickly discovered that differences in how institutions record
               dimensional metadata meant that we would have to adapt Booksnake to different
               digitized collections. To produce size-accurate virtual objects, Booksnake requires
                  <hi rend="italic">either</hi> an itemâ€™s physical dimensions in a computer-readable
               format, or <hi rend="italic">both </hi>the itemâ€™s pixel dimensions and its
               digitization resolution, from which it can calculate the item's physical dimensions
               (see figures 4a and 4b). Institutions generally take one of three approaches to
               providing dimensional metadata.</p>

            <p>The simplest approach is to list an itemâ€™s physical dimensions as metadata in its
               IIIF manifest. Some archives, such as the David Rumsey Map Collection, provide
               separate fields for an itemâ€™s height and width, each labeled with units of measure.
               This formatting provides the itemâ€™s dimensions in a computer-readable format, making
               it straightforward to create a virtual object of the appropriate size. Alternatively,
               an institution may use the IIIF Physical Dimension service, an optional service that
               provides the scale relationship between an itemâ€™s physical and pixel dimensions,
               along with the physical units of measure <ptr target="#iiif_2015"/>. But we are
               unaware of any institution that has implemented this service for its
                  collections.<note> David Newberry's <quote rend="inline">IIIF for Dolls</quote>
                  web tool makes inventive use of the IIIF Physical Dimension service to rescale
                  digitized materials <ptr target="#newberry_2023"/>.</note>
            </p>

            <p>A more common approach is to provide an itemâ€™s physical dimensions in a format that
               is not immediately computer-readable. The Huntington Digital Library, for example,
               typically lists an itemâ€™s dimensions as part of a text string in the <q>physical
                  description</q> field. <ref target="https://hdl.huntington.org/digital/collection/p9539coll1/id/12262/rec/2">This c.1921 steamship poster</ref>, for example, is described as: <quote rend="inline">Print ; image 60.4 x 55 cm (23 3/4 x 21 5/8 in.) ; overall 93.2 x 61
                  cm (36 11/16 x 24 in.)</quote> [spacing <hi rend="italic">sic</hi>]. To interpret
               this text string and convert it into numerical dimensions, a computer program like
               Booksnake requires additional guidance. Which set of dimensions to use, <q>image</q>
               or <q>overall</q>? Which units of measure, centimeters or inches? And what if the
               string includes additional descriptors, such as <q>folded</q> or <q>unframed</q>? We
               are currently collaborating with the Huntington to develop methods to parse
               dimensional metadata from textual descriptions, with extensibility to other
               institutions and collections.</p>

            <p>Finally, an institution may not provide <hi rend="italic">any</hi> dimensional
               metadata in its IIIF manifests. This is the case with the Library of Congress (LOC),
               which lists physical dimensions, where available, in an itemâ€™s catalog record, but
               does not provide this information in the itemâ€™s IIIF manifest.<note> Library of
                  Congress uses MARC data to construct IIIF manifests, but this is sometimes a
                     <q>lossy</q> process <ptr target="#woodward_2021"/>.</note> This presented us
               with a problem: How to create dimensionally-accurate virtual objects without the
               itemâ€™s physical dimensions? After much research and troubleshooting, we hit upon a
               solution. We initially dismissed pixel dimensions as a source of dimensional metadata
               because there is no consistent relationship between physical and pixel dimensions.
               And yet, during early-stage testing, Booksnake consistently created life-size virtual
               maps from LOC, even as items in other LOC collections resulted in virtual objects
               with wildly incorrect sizing. This meant there <hi rend="italic">was</hi> a
               relationship, at least for one collection â€” we just had to find it. </p>

            <p>LOC digitizes items to FADGI standards, which specify a digitization resolution for
               different item types. For example, FADGI standards specify a target resolution of 600
               ppi for prints and photographs, and 400 ppi for books.<note> Both ppi requirements
                  are for FADGIâ€™s highest performance level, 4 Star.</note> We then discovered that
               LOC scales down item images in some collections. (For example, map images are scaled
               down by a factor of four.) We then combined digitization resolution and scaling
               factors for each item type into a pre-coded reference table. When Booksnake imports
               an LOC item, it consults the itemâ€™s IIIF manifest to determine the item type, then
               consults the reference table to determine the appropriate factor for converting the
               itemâ€™s pixel dimensions to physical dimensions. This solution is similar to a
               client-side version of the IIIF Physical Dimension service, customized to LOCâ€™s
               digital collections. </p>

            <p>As this discussion suggests, determining the physical dimensions of a digitized item
               is a seemingly simple problem that can quickly become complicated. Developing robust
               methods for parsing different types of dimensional metadata is a key research area
               because these methods will allow us to expand the range of institutions and materials
               with which Booksnake is compatible. While IIIF makes it straightforward to access
               digitized materials held by different institutions, the differences in how each
               institution presents dimensional metadata mean that we will currently have to adapt
               Booksnake to use each institution's metadata schema.<note> More information about
                  Booksnake's metadata requirements is available at <ref target="https://booksnake.app/glam/">https://booksnake.app/glam/</ref>.</note>
            </p>

            <p>Booksnake then uses this information to create virtual objects on demand. When a user
               taps <q>View in Your Space</q> to initiate an AR session, Booksnake uses RealityKit
               to transform the itemâ€™s digital image into a custom virtual object suitable for
               interaction in physical space. First, Booksnake creates a blank two-dimensional
               virtual plane sized to match the itemâ€™s physical dimensions. Next, Booksnake applies
               the downloaded image to this virtual plane as a texture, scaling the image to match
               the size of the plane. This results in a custom virtual object that matches the
               original itemâ€™s physical dimensions, proportions, and appearance. This process is
               invisible to the user â€” Booksnake <q>just works.</q> This process is straightforward
               for flat digitized items like maps or posters, which are typically digitized from a
               single perspective, of their recto (front) side.</p>

            <p>Booksnakeâ€™s virtual object creation process is more complex for compound objects,
               which have multiple images linked to a single item record. Compound objects can
               include books, issues of periodicals or newspapers, diaries, scrapbooks, photo
               albums, and postcards. The simplest compound objects, such as postcards, have two
               images showing the objectâ€™s recto (front) and verso (back) sides. Nineteenth-century
               newspapers may have four or eight pages, while books may run into hundreds of pages,
               with each page typically captured and stored as a single image. </p>

            <figure xml:id="figure05">
               <head>This and the following figures illustrate how Booksnake constructs virtual
                  compound objects from multiple individual virtual objects. In each figure, the top
                  image shows an annotated screenshot of the live camera view, and the bottom images
                  show what images are stored in device memory. First, Booksnake converts the item's
                  first image into a virtual object (indicated with the red arrow), and creates a
                  matching blank (untextured) virtual object, which acts as an invisible "page zero"
                  (indicated with the yellow dashed line). Booksnake places the invisible "page
                  zero" object to the left of the page one object, and links the two objects, as if
                  along a central spine.</head>
               <figDesc/>
               <graphic url="resources/images/figure05.jpeg" style="width: 700px"/>
            </figure>


            <figure xml:id="figure06">
               <head>When a user turns the page of a virtual compound object, Booksnake loads the
                  next two item images and converts them into individual virtual objects. Booksnake
                  aligns the page two object (indicated with the light blue arrow) on the reverse
                  side of the page one object (indicated with the red arrow) and positions the page
                  three object (indicated with the dark blue arrow) directly beneath the page one
                  object. Booksnake then animates a page turn by rotating pages one and two
                  together, over one hundred and eighty degrees. </head>
               <figDesc/>
               <graphic url="resources/images/figure06.jpeg" style="width: 700px"/>
            </figure>


            <figure xml:id="figure07">
               <head>When the animation is complete, Booksnake has replaced the original virtual
                  objects representing pages zero and one with the new virtual objects representing
                  pages two and three (indicated with the light blue and dark blue arrows,
                  respectively), and discards the virtual objects representing page zero and page
                  one. This on-demand virtual object creation process optimizes for memory and
                  performance.</head>
               <figDesc/>
               <graphic url="resources/images/figure07.jpeg" style="width: 700px"/>
            </figure>

            <p>Booksnake handles compound objects by creating multiple individual virtual objects,
               one for each item image, then arranging and animating these objects to support the
               illusion of a cohesive compound object. Our initial implementation is modeled on a
               generic paginated codex, with multiple pages around a central spine. As with flat
               objects, this creation process happens on demand, when a user starts an AR session.
               Booksnake uses a compound objectâ€™s first image as the virtual objectâ€™s cover or first
               page (more on this below). Booksnake creates a virtual object for this first image,
               then creates a matching invisible virtual object, which acts as an invisible <q>page
                  zero</q> (see figure 5). The user sees the first page of a newspaper, for example,
               sitting and waiting to be opened. When the user swipes from right to left across the
               object edge to "turn" the virtual page, Booksnake retrieves the next two images,
               transforms them into virtual objects representing pages two and three, then animates
               a page turn with the objectâ€™s <q>spine</q> serving as the rotation axis (see figure
               6). Once the animation is complete, pages two and three have replaced the invisible
                  <q>page zero</q> and page one, and Booksnake discards those virtual objects (see
               figure 7). This on-demand process means that Booksnake only needs to load a maximum
               of four images into AR space, optimizing for memory and performance. By using a swipe
               gesture to turn pages, Booksnake leverages a navigation affordance with which users
               are already familiar from interactions with physical paginated items, supporting
               immersion and engagement. The page-turn animation, paired with a page-turn sound
               effect, further enhances the realism of the virtual experience.</p>

            <p>A key limitation in our initial approach to creating virtual paginated objects is
               that our generic codex model is based on one particular type of object, the
               newspaper. Specifically, we used LOC's Chronicling America collection of historical
               newspapers as a testbed to develop the pipeline for creating virtual paginated
               objects, as well as the user interface and methods for interacting with virtual
               paginated objects in physical space. While the newspaper is broadly representative of
               the codex form's physical features and affordances, making it readily extensible to
               other paginated media, there are important differences in how different media are
               digitized and presented in online collections. For example, while Chronicling America
               newspapers have one page per image, some digitized books held by LOC have two pages
               per image. We have adapted Booksnake's object creation pipeline to identify
               double-page images, split the image in half, and wrap each resulting image onto
               individual facing pages.<note> Another example: For some digitized books held by LOC,
                  the first image is of the book spine, not the book cover. Booksnake incorporates
                  logic to identify and ignore book spine images.</note> There are also important
               cultural differences in codex interaction methods: We plan to further extend the
               capabilities of this model by building support for IIIF's "right-to-left" reading
               direction flag, which will enable Booksnake to correctly display paginated materials
               in languages like Arabic, Chinese, Hebrew, and Japanese.</p>

            <p>A further limitation of our initial approach is our assumption that all compound
               objects represent paginated material in codex form, which is not the case. Booksnake
               cannot yet realistically display items like scrolls or Mesoamerican screenfold books
               (which are often digitized by page, but differ in physical construction and
               interaction methods from Western books). In other cases, a compound object comprises
               a collection of individual items that are stored together but not physically bound to
               each other. One example is <ref target="https://www.loc.gov/item/pldec.110/"><quote rend="inline">Polish Declarations of Admiration and Friendship for the United
                     States</quote></ref>, held by LOC, which consists of 181 unbound sheets. Or an
               institution may use a compound object to store multiple different images of a single
               object. For example, the Yale Center for British Art (YCBA) often provides several
               images for paintings in its collections, as with<ref target="https://collections.britishart.yale.edu/catalog/tms:52309"> the 1769
                  George Stubbs painting <quote rend="inline">Water Spaniel</quote>.</ref> YCBA
               provides four images: of the framed painting, the unframed painting, the unframed
               image cropped to show just the canvas, and the framed paintingâ€™s verso side. We plan
               to continue refining Booksnakeâ€™s compound object support by using collection- and
               item-level metadata to differentiate between paginated and non-paginated compound
               objects, in order to realistically display different types of materials.</p>

            <p>Having constructed a custom virtual object, Booksnake next opens the live camera view
               so that the user can interact with the object in physical space. As the user moves
               their device to scan their surroundings, Booksnake overlays a transparent image of
               the object, outlined in red, over horizontal and vertical surfaces, giving the user a
               preview of how the object will fit in their space. Tapping the transparent preview
               anchors the object, which turns opaque, to a physical surface. As the user and device
               move, Booksnake uses camera and sensor data to maintain the object's relative
               location in physical space, opening the object to embodied exploration. The user can
               crouch down and peer at the object from the edge of the table, pan across the
               object's surface, or zoom into the key details by moving closer to the object. The
               user can also enlarge or shrink an object by using the familiar pinch-to-zoom
               gestureâ€”although in this case, the user is not zooming in or out, but re-scaling the
               virtual object itself. The object remains in place even when out of view of the
               device: A user can walk away from an anchored object, then turn back to look at it
               from across the room. </p>

            <p>Again, an object viewed with Booksnake is just as mediated as one viewed in a
               traditional Web-based viewer. Throughout the development process, we have repeatedly
               faced interpretive questions at the intersection of technology and the humanities.
               One early question, for example: Should users be allowed to re-scale objects? The
               earliest versions of Booksnake lacked this affordance, to emphasize that a given
               virtual object was presented at life size. But one of our advisory board members,
               Philip J. Ethington, argued that digital technology is powerful because it can enable
               interactions that aren't possible in the real world. And so we built a way for users
               to re-scale objects by pinching them, while also displaying a changing percentage
               indicator to show users how much larger or smaller the virtual object is when
               compared to the original. In approaching this and similar questions, our goal is for
               virtual objects to behave in realistic and familiar ways, to closely mimic the
               experience of embodied interaction with physical materials.</p>
         </div>


         <div>

            <head>CONCLUSION</head>

            <p><quote rend="inline">What kind of interface exists after the screen goes
                  away?</quote> asks Johanna Drucker. <quote rend="inline">I touch the surface of my
                  desk and it opens to the library of the world? My walls are display points,
                  capable of offering the inventory of masterworks from the worldâ€™s museums and
                  collections into view?</quote>
               <ptr target="#drucker_2014" loc="195"/>. These questions point toward spatial
               interfaces, and this is what Booksnake is building toward. Futurists and
               science-fiction authors have long positioned virtual reality as a means of
               transporting users away from their day-to-day humdrum reality into persistent
               interactive three-dimensional virtual worlds, sometimes called the metaverse <ptr target="#gibson_1984"/>, <ptr target="#stephenson_1992"/>, <ptr target="#cline_2011"/>, <ptr target="#ball_2022"/>. Booksnake takes a different
               approach, bringing life-size virtual representations of real objects into a user's
               physical surroundings for embodied interaction. The recent emergence of mixed-reality
               headsets, such as the Apple Vision Pro, only underscores the potential of spatial
               computing for humanities work, especially for affordances like hands-on interaction
               with virtual objects.</p>

            <p>The first version of Booksnake represents a proof of concept, both of the pipeline to
               transform digitized items into virtual objects and of AR as an interface for
               research, teaching, and learning with digitized archival materials. In addition to
               researching development of an Android version, our next steps are to refine
               Booksnake's object creation pipeline and AR interface. One major research focus is
               improving how Booksnake identifies, ingests, and interprets different types of
               dimensional metadata, to support as many different metadata schemas and existing
               online collections as possible. Another research focus is expanding the range of
               interactions possible with virtual objects â€” for example, making it possible for
               users to annotate a virtual object with text or graphics, or to adjust the
               transparency of a virtual object (to better support comparison between virtual and
               physical items). In the longer term, once the IIIF Consortium finalizes protocols for
               3D archival data and metadata <ptr target="#iiifc_2022"/>, we anticipate that future
               versions of Booksnake will be able to download and display 3D objects. Ongoing user
               testing and feedback will further inform Booksnake's continuing development. </p>

            <p>Our technical work enables Booksnake users to bring digitized cultural heritage
               materials out of flat screens and onto their desks and walls, using a tool â€” the
               consumer smartphone or tablet â€” that they already own. Meanwhile, our conceptual work
               to construct size-accurate virtual objects from existing images and available
               metadata will help to make a large, culturally significant, and ever-expanding body
               of digitized materials available for use in other immersive technology projects where
               the dimensional accuracy of virtual objects is important, such as virtual museum
               exhibits or manuscript viewers. To put it another way, itâ€™s taken the better part of
               three decades to <hi rend="italic">digitize</hi> millions of cultural heritage
               materials â€” and that's barely the tip of the iceberg. How much more time will it take
                  <hi rend="italic">virtualize</hi> these cultural heritage materials, that is, to
               create accurate virtual replicas? Booksnake's automatic transformation method offers
               a way to repurpose existing resources toward this goal. </p>

            <p>Weâ€™re building Booksnake to enable digital accessibility and connect more people with
               primary sources. At its core, Booksnake is very simple. It is a tool for transforming
               digital images of cultural heritage materials into life-size virtual objects. In
               doing so, Booksnake leverages augmented reality as a means of interacting with
               archival materials that is radically new, but that feels deeply familiar. Booksnake
               is a way to virtualize the reading room or museum experience â€” and thereby
               democratize it,Â making embodied interaction with cultural heritage materials in
               physical space more accessible to more people in more places.</p>
         </div>



      </body>



      <back>
         <listBibl>
            <bibl xml:id="almas_etal_2018" label="Almas et al. 2018"> Almas, Bridget, et al. (2018)
                  <title rend="quotes">Manuscript Study in Digital Spaces: The State of the Field
                  and New Ways Forward</title>, <title rend="italic">Digital Humanities
                  Quarterly</title>, 12(2). Available at: <ref target="http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html">http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html</ref>. </bibl>


            <bibl xml:id="apple_n.d._a" label="Apple n.d._a"> Apple. (N.d.) <title rend="italic">ARKit</title> [Online]. Apple Developer. Available at: <ref target="https://developer.apple.com/documentation/arkit/">https://developer.apple.com/documentation/arkit/</ref>. </bibl>


            <bibl xml:id="apple_n.d._b" label="Apple n.d._b"> Apple. (N.d.) <title rend="italic">RealityKit</title> [Online]. Apple Developer. Available at: <ref target="https://developer.apple.com/documentation/realitykit/">https://developer.apple.com/documentation/realitykit/</ref>. </bibl>


            <bibl xml:id="atske_perrin_2021" label="Atske and Perrin 2021"> Atske, Sara, and Perrin,
               Andrew. (2021) <title rend="quotes">Home broadband adoption, computer ownership vary
                  by race, ethnicity in the U.S.</title> Pew Research Center. Available at: <ref target="https://pewrsr.ch/3Bdn6tW">https://pewrsr.ch/3Bdn6tW</ref>. </bibl>


            <bibl xml:id="azuma_etal_2001" label="Azuma et al. 2001"> Azuma, Ronald, et al. (2001)
                  <title rend="quotes">Recent advances in augmented reality</title>, <title rend="italic">IEEE Computer Graphics and Applications</title>, 21(6), 34â€“47.
               Available at: <ref target="https://doi.org/10.1109/38.963459">https://doi.org/10.1109/38.963459</ref>. </bibl>


            <bibl xml:id="bacca_etal_2014" label="Bacca et al. 2014"> Bacca, Jorge, et al. (2014)
                  <title rend="quotes">Augmented Reality Trends in Education: A Systematic Review of
                  Research and Applications</title>, <title rend="italic">Educational Technology
                  &amp; Society</title>, 17(4), pp. 133â€“149. Available at: <ref target="https://www.jstor.org/stable/jeductechsoci.17.4.133">https://www.jstor.org/stable/jeductechsoci.17.4.133</ref>. </bibl>


            <bibl xml:id="bailenson_2018" label="Bailenson 2018"> Bailenson, Jeremy. (2018) <title rend="italic">Experience on Demand: What Virtual Reality is, How it Works, and
                  What it Can Do</title>. New York: W. W. Norton &amp; Co. </bibl>


            <bibl xml:id="ball_2022" label="Ball 2022"> Ball, Matthew. (2022) <title rend="italic">The Metaverse: And How It Will Revolutionize Everything</title>. New York:
               Liveright. </bibl>


            <bibl xml:id="bifrost_2023" label="Bifrost Consulting Group 2023"> Bifrost Consulting
               Group. (2023) <title rend="quotes">Let's build cultural spaces in the
                  metaverse</title>. Available at: <ref target="https://diomira.ca/">https://diomira.ca/</ref>. </bibl>


            <bibl xml:id="chandler_etal_2017" label="Chandler et al. 2017"> Chandler, Tom, et al.
               (2017) <title rend="quotes">A New Model of Angkor Wat: Simulated Reconstruction as a
                  Methodology for Analysis and Public Engagement</title>, <title rend="italic">Australian and New Zealand Journal of Art</title>, 17(2), pp. 182â€“194. Available
               at: <ref target="https://doi.org/10.1080/14434318.2017.1450063">https://doi.org/10.1080/14434318.2017.1450063</ref>. </bibl>


            <bibl xml:id="cline_2011" label="Cline 2011"> Cline, Ernest. (2011) <title rend="italic">Ready Player One</title>. New York: Broadway Books. </bibl>


            <bibl xml:id="cool_2022" label="Cool 2022"> Cool, Matt. (2022) <title rend="quotes">5
                  Inspiring Galleries Built with Hubs</title> [Blog]. Mozilla Hubs Creator Labs.
               Available at: <ref target="https://hubs.mozilla.com/labs/5-incredible-art-galleries/">https://hubs.mozilla.com/labs/5-incredible-art-galleries/</ref>. </bibl>


            <bibl xml:id="cramer_2011" label="Cramer 2011"> Cramer, Tom. (2011) <title rend="quotes">The International Image Interoperability Framework (IIIF): Laying the Foundation
                  for Common Services, Integrated Resources and a Marketplace of Tools for Scholars
                  Worldwide</title>, Coalition for Networked Information Fall 2011 Membership
               Meeting, Arlington, Virginia, December 12â€“13. Available at: <ref target="https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework">https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework</ref>. </bibl>


            <bibl xml:id="cramer_2015" label="Cramer 2015"> Cramer, Tom. (2015) <title rend="quotes">IIIF Consortium Formed</title> [Online]. International Image Interoperability
               Framework. Available at: <ref target="https://iiif.io/news/2015/06/17/iiif-consortium/">https://iiif.io/news/2015/06/17/iiif-consortium/</ref>. </bibl>


            <bibl xml:id="crane_2021" label="Crane 2021"> Crane, Tom. (2021) <title rend="quotes">On
                  being the right size</title> [Online]. Canvas Panel. Available at: <ref target="https://canvas-panel.digirati.com/developer-stories/rightsize.html">https://canvas-panel.digirati.com/developer-stories/rightsize.html</ref>. </bibl>


            <bibl xml:id="dignazio_klein_2020" label="D'Ignazio and Klein 2020"> D'Ignazio,
               Catherine, and Klein, Lauren F. (2020) <title rend="italic">Data Feminism</title>.
               Cambridge, Mass.: The MIT Press. Available at: <ref target="https://data-feminism.mitpress.mit.edu/">https://data-feminism.mitpress.mit.edu/</ref>. </bibl>


            <bibl xml:id="drucker_2014" label="Drucker 2014"> Drucker, Johanna. (2014) <title rend="italic">Graphesis: Visual Forms of Knowledge Production</title>. Cambridge,
               Mass.: Harvard University Press. </bibl>


            <bibl xml:id="drucker_2013" label="Drucker 2013"> Drucker, Johanna. (2013) <title rend="quotes">Is There a 'Digital' Art History?</title>, <title rend="italic">Visual Resources</title>, 29(1-2). Available at: <ref target="https://doi.org/10.1080/01973762.2013.761106">https://doi.org/10.1080/01973762.2013.761106</ref>. </bibl>


            <bibl xml:id="drucker_2011" label="Drucker 2011"> Drucker, Johanna. (2011) <title rend="quotes">Humanities Approaches to Graphical Display</title>, <title rend="italic">Digital Humanities Quarterly</title>, 5(1). Available at: <ref target="https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</ref>. </bibl>


            <bibl xml:id="falbo_2000" label="Falbo 2000">Falbo, Bianca. (2000) <title rend="quotes">Teaching from the Archives</title>,Â <title rend="italic">RBM: A Journal of Rare
                  Books, Manuscripts, and Cultural Heritage</title>,Â 1(1), pp. 33â€“35. Available at:
                  <ref target="https://doi.org/10.5860/rbm.1.1.173">https://doi.org/10.5860/rbm.1.1.173</ref>.</bibl>


            <bibl xml:id="fadgi_2023" label="Federal Agency Digitization Guidelines Initiative 2023">Federal Agency Digitization Guidelines Initiative. (2023) <title rend="italic">Technical Guidelines for Digitizing Cultural Heritage Materials: Third
                  Edition</title>. [Washington, DC: Federal Agency Digitization Guidelines
               Initiative.] Available at: <ref target="https://www.digitizationguidelines.gov/guidelines/digitize-technical.html">https://www.digitizationguidelines.gov/guidelines/digitize-technical.html</ref>.</bibl>


            <bibl xml:id="fisher_2018" label="Fisher 2019">Fisher, Adam. (2018) <title rend="italic">Valley of Genius: The Uncensored History of Silicon Valley</title>. New York:
               Twelve.</bibl>


            <bibl xml:id="francois_etal_2021" label="FranÃ§ois et al. 2021">FranÃ§ois, Paul, et al.
               (2021) <title rend="quotes">Virtual reality as a versatile tool for research,
                  dissemination and mediation in the humanities,</title>
               <title rend="italic">Virtual Archaeology Review</title> 12(25), pp. 1â€“15. Available
               at: <ref target="https://doi.org/10.4995/var.2021.14880">https://doi.org/10.4995/var.2021.14880</ref>.</bibl>


            <bibl xml:id="gibson_1984" label="Gibson 1984">Gibson, William. (1984) <hi rend="italic">Neuromancer. </hi>Reprint, New York: Ace, 2018.</bibl>


            <bibl xml:id="google_n.d." label="Google n.d.">Google. (N.d.) <title rend="italic">Add
                  items</title> [Online]. Google Arts &amp; Culture Platform Help. Available at:
                  <ref target="https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA">https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA</ref>
            </bibl>


            <bibl xml:id="greengard_2019" label="Greengard 2019">Greengard, Samuel. (2019) <title rend="italic">Virtual Reality</title>. Cambridge, Mass.: The MIT Press.</bibl>


            <bibl xml:id="haraway_1988" label="Haraway 1988">Haraway, Donna. (1988) <title rend="quotes">Situated Knowledges: The Science Question in Feminism and the
                  Privilege of Partial Perspective,</title>
               <title rend="italic">Feminist Studies </title>14(3), pp. 575â€“599. Available at: <ref target="https://www.jstor.org/stable/3178066">https://www.jstor.org/stable/3178066</ref>.</bibl>


            <bibl xml:id="haynes_2018" label="Haynes 2018">Haynes, Ronald. (2018) <title rend="quotes">Eye of the Veholder: AR Extending and Blending of Museum Objects and
                  Virtual Collections</title> in Jung, T., and tom Dieck, M. C. (eds.) <title rend="italic">Augmented Reality and Virtual Reality: Empowering Human, Place and
                  Business.</title> Cham, Switzerland: Springer, pp. 79â€“91. Available at: <ref target="https://doi.org/10.1007/978-3-319-64027-3_6">https://doi.org/10.1007/978-3-319-64027-3_6</ref>.</bibl>


            <bibl xml:id="haynes_2019" label="Haynes 2010">Haynes, Ronald.(2019) <title rend="quotes">To Have and Vehold: Marrying Museum Objects and Virtual Collections
                  via AR</title> in tom Dieck, M. C., and Jung, T. (eds.) <title rend="italic">Augmented Reality and Virtual Reality: The Power of AR and VR for
                  Business.</title> Cham, Switzerland: Springer, pp. 191â€“202. Available at: <ref target="https://doi.org/10.1007/978-3-030-06246-0_14">https://doi.org/10.1007/978-3-030-06246-0_14</ref>.</bibl>


            <bibl xml:id="herron_2020" label="Herron 2020">Herron, Thomas. (2020) <title rend="italic">Recreating Spenser: The Irish Castle of an English Poet</title>.
               Greenville, N.C.: Author.</bibl>


            <bibl xml:id="iiifc_2022" label="IIIF Consortium 2022">IIIF Consortium staff. (2022)
                  <title rend="italic">New 3D Technical Specification Group</title> [Online].
               Available at: <ref target="https://iiif.io/news/2022/01/11/new-3d-tsg/">https://iiif.io/news/2022/01/11/new-3d-tsg/</ref>.</bibl>


            <bibl xml:id="iiif_n.d." label="International Image Interoperability Framework n.d.">International Image Interoperability Framework. (N.d.) <title rend="italic">Consortium</title>
               <title rend="italic">Members</title> [Online]. Available at: <ref target="https://iiif.io/community/consortium/members/">https://iiif.io/community/consortium/members/</ref>.</bibl>


            <bibl xml:id="iiif_2015" label="International Image Interoperability Framework 2015">International Image Interoperability Framework. (2015) <title rend="quotes">Physical
                  Dimensions</title> in Albritton, Benjamin, et al. (eds.) <title rend="italic">Linking to External Services </title>[Online]. Available at: <ref target="https://iiif.io/api/annex/services/%23physical-dimensions">https://iiif.io/api/annex/services/#physical-dimensions</ref>.</bibl>


            <bibl xml:id="kai-kee_latina_sadoyan_2020" label="Kai-Kee, Latina, Sadoyan 2020">Kai-Kee, E., Latina, L., and Sadoyan, L. (2020) <title rend="italic">Activity-based
                  Teaching in the Art Museum: Movement, Embodiment, Emotion</title>. Los Angeles:
               Getty Museum. </bibl>


            <bibl xml:id="kenderdine_yip_2019" label="Kenderdine and Yip 2019">Kenderdine, Sarah,
               and Yip, Andrew (2019). <title rend="quotes">The Proliferation of Aura: Facsimiles,
                  Authenticity and Digital Objects</title> in Drtoner, K., et al. (eds.) <title rend="italic">The Routledge Handbook of Museums, Media and Communication</title>.
               London and New York: Routledge, pp. 274â€“289. Available at: <ref target="https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip">https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip</ref>
            </bibl>


            <bibl xml:id="kropf_2017" label="Kropf 2017">Kropf, Evyn. (2017) <title rend="quotes">Will that Surrogate Do?: Reflections on Material Manuscript Literacy in the
                  Digital Environment from Islamic Manuscripts at the University of Michigan
                  Library,</title>
               <title rend="italic">Manuscript Studies</title>, 1(1), pp. 52â€“70. Available at: <ref target="https://doi.org/10.1353/mns.2016.0007">https://doi.org/10.1353/mns.2016.0007</ref>.</bibl>


            <bibl xml:id="l_i_2018" label="Luna 2018">Luna Imaging. (2018) <title rend="italic">LUNA
                  and IIIF</title> [Online]. Available at: <ref target="http://www.lunaimaging.com/iiif">http://www.lunaimaging.com/iiif</ref>.</bibl>


            <bibl xml:id="luo_2019" label="Luo 2019">Luo, Michelle. (2019) <title rend="quotes">Explore art and culture through a new lens [Blog]</title>
               <title rend="italic">The Keyword</title>. Available at: <ref target="https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/">https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/</ref>
            </bibl>


            <bibl xml:id="newberry_2023" label="Newberry 2023">Newberry, David. (2023) <title rend="italic">IIIF for Dolls</title> [Online]. Available at: <ref target="https://iiif-for-dolls.davidnewbury.com/">https://iiif-for-dolls.davidnewbury.com/</ref>.</bibl>


            <bibl xml:id="nolan_2013" label="Nolan 2013">Nolan, Maura. (2013) <title rend="quotes">Medieval Habit, Modern Sensation: Reading Manuscripts in the Digital Age,</title>
               <title rend="italic">The Chaucer Review</title>, 47(4), pp. 465â€“476. Available at:
                  <ref target="https://doi.org/10.5325/chaucerrev.47.4.0465">https://doi.org/10.5325/chaucerrev.47.4.0465</ref>.</bibl>


            <bibl xml:id="oclc_n.d." label="OCLC n.d.">OCLC. (N.d.) <title rend="italic">CONTENTdm
                  and the International Image Interoperability Framework (IIIF)</title> [Online].
               Available at: <ref target="https://www.oclc.org/en/contentdm/iiif.html">https://www.oclc.org/en/contentdm/iiif.html</ref>.</bibl>


            <bibl xml:id="orangelogic_n.d." label="orangelogic n.d.">orangelogic. (N.d.) <title rend="italic">Connect to your favorite systems: Get DAM integrations that fit your
                  existing workflows</title> [Online]. Available at: <ref target="https://www.orangelogic.com/features/integrations">https://www.orangelogic.com/features/integrations</ref>.</bibl>


            <bibl xml:id="openseadragon_n.d." label="OpenSeadragon n.d.">OpenSeadragon. (N.d.)
                  <title rend="italic">OpenSeadragon</title>. Available at: <ref target="http://openseadragon.github.io/">http://openseadragon.github.io/</ref>. </bibl>
            <bibl xml:id="pew_2021" label="Pew Research Center 2021">Pew Research Center. (2021)
               Mobile Fact Sheet. Available at: <ref target="http://pewrsr.ch/2ik6Ux9">http://pewrsr.ch/2ik6Ux9</ref></bibl>


            <bibl xml:id="porter_2018" label="Porter 2018">Porter, Dot. (2018) <title rend="quotes">Zombie Manuscripts: Digital Facsimiles in the Uncanny Valley</title>.
               Presentation at the International Congress on Medieval Studies, Western Michigan
               University, Kalamazoo, Michigan, 12 May 2018. Available at: <ref target="https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/">https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/</ref>
            </bibl>


            <bibl xml:id="project_mirador_n.d." label="Project Mirador n.d.">Project Mirador. (N.d.)
                  <title rend="italic">Mirador</title>. Available at: <ref target="https://projectmirador.org/">https://projectmirador.org/</ref>.</bibl>


            <bibl xml:id="putnam_2016" label="Putnam 2016">Putnam, Lara. (2016) <title rend="quotes">The Transnational and the Text-Searchable: Digitized Sources and the Shadows They
                  Cast,</title>
               <title rend="italic">The American Historical Review</title>, 121(2), pp. 377â€“402.
               Available at: <ref target="https://doi.org/10.1093/ahr/121.2.377">https://doi.org/10.1093/ahr/121.2.377</ref>.</bibl>


            <bibl xml:id="ramsay_2014" label="Ramsay 2014">Ramsay, Stephen. (2014) <title rend="quotes">The Hermeneutics of Screwing Around; or What You Do With a Million
                  Books</title> in Kee, Kevin (ed.) <title rend="italic">Pastplay: Teaching and
                  Learning History with Technology</title>. Ann Arbor: University of Michigan Press,
               pp. 111â€“120. Available at: <ref target="https://doi.org/10.1353/book.29517">https://doi.org/10.1353/book.29517</ref>.</bibl>


            <bibl xml:id="rosenzweig_2003" label="Rosenzweig 2003">Rosenzweig, Roy. (2003) <title rend="quotes">Scarcity or Abundance? Preserving the Past in a Digital Era,</title>
               <title rend="italic">The American Historical Review</title> 108 (3), pp. 735â€“762.
               Available at: <ref target="https://doi.org/10.1086/ahr/108.3.735">https://doi.org/10.1086/ahr/108.3.735</ref>.</bibl>


            <bibl xml:id="roth_fisher_2019" label="Roth and Fisher 2019">Roth, A., and Fisher, C.
               (2019) <title rend="quotes">Building Augmented Reality Freedom Stories: A Critical
                  Reflection,</title> in Kee, Kevin, and Compeau, Timothy (eds.) <title rend="italic">Seeing the Past with Computers: Experiments with Augmented Reality
                  and Computer Vision for History.</title> Ann Arbor: University of Michigan Press,
               pp. 137â€“157. Available at: <ref target="https://www.jstor.org/stable/j.ctvnjbdr0.11">https://www.jstor.org/stable/j.ctvnjbdr0.11</ref></bibl>


            <bibl xml:id="schmiesing_hollis_2002" label="Schmiesing and Hollis 2002">Schmiesing, A.,
               and Hollis, D. (2002) <title rend="quotes">The Role of Special Collections
                  Departments in Humanities Undergraduate and Graduate Teaching: A Case
                  Study,</title>Â  <title rend="italic">Libraries and the Academy</title>Â 2(3), pp.
               465â€“480. Available at: <ref target="https://doi.org/10.1353/pla.2002.0065">https://doi.org/10.1353/pla.2002.0065</ref>.</bibl>


            <bibl xml:id="shemek_etal_2018" label="Shemek 2018">Shemek, D., et al. (2018) <title rend="quotes">Renaissance Remix. Isabella d'Este: Virtual Studiolo,</title>
               <title rend="italic">Digital Humanties Quarterly</title>, 12(4). Available at: <ref target="http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html">http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html</ref>.</bibl>


            <bibl xml:id="snydman_sanderson_cramer_2015" label="Snydman, Sanderson, and Cramer 2015">Snydman, S., Sanderson, R., &amp; Cramer, T. (2015) <title rend="quotes">The
                  International Image Interoperability Framework (IIIF): A community &amp;
                  technology approach for web-based images.</title> In D. Walls (Ed), <title rend="italic">Archiving 2015: Final Program and Proceedings</title> (pp. 16â€“21).
               Society for Imaging Science and Technology. Available at: <ref target="https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf">https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf</ref></bibl>


            <bibl xml:id="solberg_2012" label="Solberg 2012">Solberg, Janine. (2012) <title rend="quotes">Googling the Archive: Digital Tools and the Practice of
                  History.</title>
               <title rend="italic">Advances in the History of Rhetoric</title> 15(1), pp. 53â€“76.
               Available at: <ref target="https://doi.org/10.1080/15362426.2012.657052">https://doi.org/10.1080/15362426.2012.657052</ref>.</bibl>


            <bibl xml:id="stephenson_1992" label="Stephenson 1992">Stephenson, Neal. (1992) <title rend="italic">Snow Crash</title>. Reprint, New York: Bantam, 2000.</bibl>


            <bibl xml:id="sundar_etal_2013" label="Sundar et.al 2013">Sundar, S., Bellur,
               Saraswathi, Oh, Jeeyun, Xu, Qian, &amp; Jia, Haiyan. (2013) <title rend="quotes">User
                  Experience of On-Screen Interaction Techniques: An Experimental Investigation of
                  Clicking, Sliding, Zooming, Hovering, Dragging, and Flipping.</title>
               <title rend="italic">Humanâ€“Computer Interaction</title> 29 (2), pp. 109â€“152.
               doi:10.1080/07370024.2013.789347 </bibl>


            <bibl xml:id="szpiech_2014" label="Szpiech 2014">Szpiech, Ryan. (2014) <title rend="quotes">Cracking the Code: Reflections on Manuscripts in the Age of Digital
                  Books.</title>
               <title rend="italic">Digital Philology: A Journal of Medieval Cultures</title> 3(1),
               pp. 75â€“100. Available at: <ref target="https://doi.org/10.1353/dph.2014.0010">https://doi.org/10.1353/dph.2014.0010</ref>.</bibl>


            <bibl xml:id="toner_1993" label="Toner 1993">Toner, Carol. (1993) <title rend="quotes">Teaching Students to be Historians: Suggestions for an Undergraduate Research
                  Seminar,</title>Â <title rend="italic">The History Teacher</title>Â 27(1), pp.
               37â€“51. Available at: <ref target="https://doi.org/10.2307/494330">https://doi.org/10.2307/494330</ref>.</bibl>


            <bibl xml:id="van_lit_2020" label="van Lit 2020">van Lit, L. W. C. (2020) <title rend="italic">Among Digitized Manuscripts: Philology, Codicology, Paleography in a
                  Digital World</title>. Leiden: Brill.</bibl>


            <bibl xml:id="van_zundert_2018" label="van Zundert 2018">van Zundert, Joris. (2018)
                  <title rend="quotes">On Not Writing a Review about Mirador: Mirador, IIIF, and the
                  Epistemological Gains of Distributed Digital Scholarly Resources.</title>
               <title rend="italic">Digital Medievalist</title> 11(1). Available at: <ref target="http://doi.org/10.16995/dm.78">http://doi.org/10.16995/dm.78</ref>.</bibl>


            <bibl xml:id="vogels_2021" label="Vogels 2021">Vogels, Emily A. (2021) <title rend="quotes">Digital divide persists even as Americans with lower incomes make
                  gains in tech adoption</title>
               <title rend="italic">Pew Research Center</title>. Available at: <ref target="https://pewrsr.ch/2TRM7cP">https://pewrsr.ch/2TRM7cP</ref></bibl>


            <bibl xml:id="woodward_2021" label="Woodward 2021">Woodward, Dave. (2021) Email to Sean
               Fraga, February 2.</bibl>
            <bibl/>


         </listBibl>
      </back>
   </text>
</TEI>