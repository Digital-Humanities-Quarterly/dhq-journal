<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt><!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>first name(s) <dhq:family>family name</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation/>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110--></idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date><!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND"><!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords"><!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors--><!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp=""/>
            </keywords>
            <keywords scheme="#authorial_keywords"><!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc><!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/NNNNNN/NNNNNN.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract><!--Include a brief abstract of the article-->
            <p/>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>
         <p>Introducing Booksnake: A Scholarly App for Transforming Existing Digitized Archival Materials into Life-Size Virtual Objects for Embodied Interaction in Augmented Reality with IIIF</p>
         <p>By AUTHOR1, AUTHOR2, AUTHOR3, AUTHOR4, AUTHOR5, and AUTHOR6<note> The authors gratefully acknowledge the support and guidance of PERSON1, PERSON2, and the PROGRAM1 program in the UNIVERSITY DIVISION 1; and of PERSON3, PERSON4, and the PROGRAM2 of UNIVERSITY DIVISION 2. We thank PERSON5, PERSON6, PERSON7, and PERSON8 for their service on the project's advisory board. We thank PERSON9 and PERSON10 for testing Booksnake with their students. We thank PERSON11 and PERSON12 at LIBRARY1 for their technical guidance. </note>
         </p>
         <p>INTRODUCTION</p>
         <p>	Close engagement with primary sources is foundational to humanities research, teaching, and learning (Falbo 2000) (Schmiesing and Hollis 2002) (Toner 1993). We are fortunate to live in an age of digital abundance. Over the past three decades, galleries, libraries, archives, and museums (collectively known as GLAM institutions) have undertaken initiatives to digitize their collection holdings, making millions of primary-source archival materials freely available online and more accessible to more people than ever before (Rosenzweig 2003) (Solberg 2012) (Ramsay 2014) (Putnam 2016). Increasingly, the world’s myriad cultural heritage materials — architectural plans, books, codices, correspondence, drawings, ephemera, manuscripts, maps, newspapers, paintings, paperwork, periodicals, photographs, postcards, posters, prints, sheet music, sketches, slides, and more — are now only a click away.</p>
         <p>	But interacting with digitized archival materials in a Web browser can be a frustrating experience — one that fails to replicate the close engagement possible during in-person research. We have become resigned to the limitations of flat-screen image viewers. How we have to fiddle with "Zoom In" and "Zoom Out" buttons to awkwardly jump between fixed levels of magnification. How we must laboriously click and drag, click and drag, over and over, to follow a line of text across a page, or read the length of a newspaper column, or trace a river over a map. How we can’t ever quite tell how big or small something is. We have long accepted these frictions as necessary compromises to quickly and easily view materials that would otherwise require an inconvenient trip to an archive itself — even as we also understand that clicking around in a Web viewer pales in comparison to the rich, fluid, embodied experience of interacting with a physical item in a museum gallery or library reading room.</p>
         <p>	To solve these problems, we present Booksnake, a scholarly app that enables users to interact with existing digitized archival materials in physical space, thereby virtualizing the physical archival encounter. Instead of displaying digital images on a flat screen for indirect manipulation, Booksnake brings digitized items into the physical world for embodied exploration. To do so, Booksnake automatically transforms existing digitized archival materials into custom, size-accurate virtual objects, then dynamically inserts these virtual objects into the live camera view on a mobile phone or tablet for interaction in physical space. The process of overlaying a virtual object on physical space in this way is known as augmented reality, or AR (Azuma et al. 2001).</p>
         <p>	Booksnake uses AR to make it feel as though a digitized item is physically present in a user’s real-world environment, enabling closer and richer engagement with digitized materials. A Booksnake user aims their phone or tablet at a flat surface (like a table, wall, bed, or floor) and taps the screen to anchor the virtual object to the physical surface. As the user moves, Booksnake uses information from the device’s cameras and sensors to continually adjust the virtual object’s relative position, orientation, and size in the camera view, such that the item appears to remain stationary in physical space as the user and device move. A user thus treats Booksnake like a lens, looking <hi rend="italic">through</hi> their device’s screen at a virtual object overlaid on the physical world. (See <figure>
               <head/>
               <graphic url="media/image1.jpeg"/>
            </figure>�<note>unable to handle picture here, no embed or link</note>Figure 1. Booksnake transforms existing digitized archival materials, like this 1909 birds-eye-view map of Los Angeles, into custom virtual objects for embodied exploration in physical space by using the augmented reality technology in consumer smartphones and tablets. The virtual object remains anchored to the table surface even as the user and device move. figure 1.) The project takes its name from book snakes, the weighted strings used by archival researchers to hold fragile physical materials in place. Similarly, Booksnake enables users to keep virtual materials in place on physical surfaces in the real world. By virtualizing the experience of embodied interaction, Booksnake makes it possible for people who cannot otherwise visit an archive (due to cost, schedule, distance, disability, or other reasons) to physically engage with digitized materials. The first version of Booksnake will s�<note>unable to handle picture here, no embed or link</note>upport Library of Congress digital collections, including the Chronicling America collection of historical newspapers, and we are actively working to build support for additional archives and collections.      </p>
         <p>	Booksnake’s central technical innovation is the automatic production of dimensionally-accurate virtual objects from existing digital images of archival cultural heritage materials. To make this possible, Booksnake connects existing software frameworks in a new way. Booksnake first uses the International Image Interoperability Framework (or IIIF, pronounced "triple-eye-eff") to retrieve digitized archival materials and related metadata from online repositories, then uses Apple ’s RealityKit and ARKit frameworks to transform these materials into custom virtual objects and manage interactions with them in physical space. Developing Booksnake as a native mobile app, rather than a Web-based tool, makes it possible for Booksnake to take advantage of the powerful camera and sensor technologies in mobile devices. We are developing Booksnake’s first version for iPhone and iPad because Apple’s tight integration of hardware and software supports rapid AR development and ensures consistency in user experience across devices. We plan to extend our work by next developing an Android version of Booksnake, improving accessibility and making Booksnake a more useful classroom tool by enabling more students to use Booksnake on their existing personal devices. Another development approach, WebXR, a Web-based technology currently in development, lacks the features to support our project goals.</p>
         <p>	Booksnake’s approach represents a novel use of AR for humanistic research, teaching, and learning. While humanists have begun to explore the potential of virtual reality (VR) and AR, they have largely approached these technologies as publication platforms: ways of hosting custom-built experiences containing visual, textual, spatial, and/or aural content that is manually created or curated by subject-matter experts. Examples include reconstructions of twelfth-century Angkor Wat (Chandler et al. 2017), a Renaissance studiolo (Shemek et al. 2018), events from the Underground Railroad (Roth and Fisher 2019), a medieval Irish castle (Herron 2020), and an 18th-century Paris theatre (François et al. 2021). These are rich, immersive experiences, but they are limited to a specific site, narrative, or question. In contrast, Booksnake is a general-purpose, content-agnostic AR tool. It enables a user to freely select materials from existing digitized cultural heritage collections for embodied interaction. Although Booksnake fundamentally differs from existing flat-screen image viewers, it is, like them, an empty frame, waiting for a user to fill it with something interesting. Booksnake's use of augmented reality demonstrates the potential of spatial interfaces and embodied interaction to activate cultural heritage collections and improve accessibility to archival materials, while its presentation as a mobile app argues for the untapped potential of mobile devices to support humanities research, teaching, and learning.</p>
         <p>	Booksnake is designed and built by a multidisciplinary team at the UNIVERSITY1. It represents the combined efforts of humanities scholars, librarians, interactive media designers, and computer scientists — most of them students or early-career scholars — extending over three years.<note> AUTHOR1 researched and designed the project's technical architecture and user interface affordances; AUTHOR1 also prepared the article manuscript. AUTHOR2 developed a method for transforming digital images into virtual objects. AUTHOR6 ideated and refined Booksnake's UI/UX fundamentals. AUTHOR3 built a means of organizing and persistently storing item metadata and digital images using Apple’s Core Data framework. AUTHOR1 developed and AUTHOR3 implemented a method for creating size-accurate virtual objects from Library of Congress metadata. AUTHOR1, AUTHOR2, and AUTHOR3 jointly designed support for compound objects, and AUTHOR2 and AUTHOR3 collaborated to implement support for compound objects; this work was supported by an NEH Digital Humanities Advancement Grant (HAA-REDACTED). AUTHOR4 is currently building support for the new IIIF v3.0 APIs and building technical links to additional archives. AUTHOR5 is expanding the range of interactions possible with virtual objects in AR. Additionally, PERSON1 serves as senior advisor to the project. PERSON3 provides strategic guidance, and PERSON4 provides guidance on user testing and feedback.</note> The project has been financially supported by the PROGRAM1 program (under grants from the FOUNDATION1 Foundation) in UNIVERSITY1's DIVISION1; and by the PROGRAM2, a scholarly innovation lab in the PROGRAM3 in UNIVERSITY1 DIVISION2. The project is also funded by a Digital Humanities Advancement Grant (Level II) from the National Endowment for the Humanities (HAA-REDACTED).</p>
         <p>	This article presents a conceptual and technical overview of Booksnake. We first critique the accepted method of viewing digitized archival materials in Web-based image viewers on flat screens and discuss the benefits of embodied interaction with archival materials. Next, we contextualize Booksnake, as an AR app for mobile devices, within the broader landscape of immersive technologies for cultural heritage. We then detail the technical pipeline by which Booksnake transforms digitized archival materials for interaction in physical space. We conclude with a brief discussion of the future of the immersive humanities. </p>
         <p>FROM FLAT SCREENS TO EMBODIED EXPLORATION</p>
         <p>	Flat-screen image viewers have long been hiding in plain sight. Sitting at the end of a digitization pipeline, they offer an interface through which users can access, view, and interact with digital image files. These image viewers' fundamental characteristic is their transparency: The act of viewing a digital image of an archival item on a flat screen has become so common within contemporary humanities practices as to escape notice. Although Web-based flat-screen image viewers sustain a robust technical development community, they have been largely overlooked by most digital humanists. A notable exception is manuscript scholars, for whom the relationship between text, object, and digital image is particularly important. Indeed, manuscript scholars have led the charge in identifying flat-screen image viewers as sites of knowledge creation and interpretation — often by expressing their frustration with these viewers' limited affordances or the contextual information these viewers shear away (Nolan 2013) (Szpiech 2014) (Kropf 2017) (Almas et al. 2018) (van Zundert 2018) (van Lit 2020). As an image viewer with a spatial interface, Booksnake is an argument for a way of seeing that prioritizes embodied interaction with digitized archival materials at real-world size. In this, Booksnake is more than a technical critique of existing flat-screen image viewers. It is also an <hi rend="italic">intellectual</hi> critique of how these image viewers foreclose certain types of knowledge creation and interpretation.</p>
         <p>	To see flat-screen image viewers more clearly, it helps to understand them within the context of archival digitization practices. Digitization is usually understood as the straightforward conversion of physical objects into digital files, but this process is never simple and always involves multiple interpretive decisions.<note> We focus here on the decisions involved in digitizing an item itself, rather than the decisions about what get items digitized and why, which also, of course, inform the development and structure of a given online collection.</note> As Johanna Drucker writes, "the way artifacts are [digitally] encoded depends on the parameters set for scanning and photography. These already embody interpretation, since the resolution of an image, the conditions of lighting under which it is produced, and other factors, will alter the outcome" (2013, p. 8). Some of these encoding decisions are structured by digitization guidelines, such as the U.S. Federal Agency Digitization Guidelines Initiative (FADGI) standards (2023), while other decisions, such as how to light an item or how to color-correct a digital image, depend on the individual training and judgment of digitization professionals.</p>
         <p>	A key digitization convention is to render an archival item from an idealized perspective, that of an observer perfectly centered before the item. To achieve this, a photographer typically places the item being digitized perpendicular to the camera's plane of vision, centers the item within the camera's view, and orthogonally aligns the item with the viewfinder's edges. During post-processing, a digitization professional can then crop, straighten, and de-skew the resulting image, or stitch together multiple images of a given item into a single cohesive whole. These physical and digital activities produce an observer-independent interpretation of an archival item. Put another way, archival digitization is what Donna Haraway calls a god trick, the act of "seeing everything from nowhere" (1988, p. 582).  </p>
         <p>	Taking these many decisions together, Drucker argues that "<hi rend="italic">digitization is not representation but interpretation</hi>" (emphasis original) (2013, p. 12). Understanding digitization as a continuous interpretive process, rather than a simple act of representation, helps us see how this process extends past the production of digital image files and into how these files are presented to people. </p>
         <p>	Flat-screen image viewers encode a set of decisions about how we can (or should) interact with a digitized item. Just as decisions about resolution, lighting, and file types serve to construct a digitized interpretation of a physical object, so too do decisions about interface affordances for an image viewer serve to construct an interpretive space. Following Drucker, image viewers do not simply <hi rend="italic">represent </hi>digital image files to a user, they <hi rend="italic">interpret</hi> them. Decisions by designers and developers about an image viewer’s interface affordances (how to zoom, turn pages, create annotations, etc.) structure the conditions of possibility for a user's interactions with a digitized item. These decisions are particularly visible when comparing different image viewers. For example, Mirador, a leading IIIF-based image viewer, enables users to compare two images from different repositories side-by-side (Project Mirador no date). A different IIIF-based image viewer, OpenSeadragon, is instead optimized for viewing "high-resolution zoomable images" one at a time (no date). Mirador encourages juxtaposition, while OpenSeadragon emphasizes attention to detail. These two viewers each represent a particular set of assumptions, goals, decisions, and compromises, which in turn shape how their respective users encounter and read a given item, the interpretations those users form, and the knowledge they create. </p>
         <p>	Flat-screen image viewers generally share three attributes that collectively structure a user's interactions with a digitized item. First, and most importantly, flat-screen image viewers directly reproduce the idealized "view from nowhere" delivered by the digitization pipeline. Flat-screen image viewers could present digital images in any number of ways — upside down, canted at an angle away from the viewer, obscured by a digital curtain. But instead, flat-screen image viewers play the god trick. Second, flat-screen image viewers rely on indirect manipulation via a mouse or trackpad. To zoom, pan, rotate, or otherwise navigate a digitized item, a user must repeatedly click buttons or click and drag, positioning and re-positioning the digital image in order to apprehend its content. These interaction methods create friction between the user and the digitized item, impeding discovery. Finally, flat-screen image viewers arbitrarily scale digital images to fit a user’s computer screen. "Digitisation [<hi rend="italic">sic</hi>] doesn't make everything equal, it just makes everything the same size," writes Tom Crane (2021). In a flat-screen image viewer, a monumental painting and its postcard reproduction appear to be the same size, giving digitized materials a false homogeneity and disregarding the contextual information conveyed by an item's physical dimensions. In sum, flat-screen image viewers are observer-independent interfaces for indirect manipulation of arbitrarily scaled digitized materials. </p>
         <p>	In contrast, Booksnake is an observer-dependent interface for direct interaction with size-accurate digitized materials. Booksnake thus offers <hi rend="italic">a new way of looking</hi> at digitized materials. Importantly, everything about digitization is novel. The ability to instantly view a digitized archival item on a computer screen rests on technological developments from the last sixty years: the high-resolution digital cameras that capture archival material; the digital asset management software that organizes it; the cheap and redundant cloud storage that houses it; the high-speed networking infrastructure that delivers it; the Web browsers with which we access it; the high-resolution, full-color monitors that display it. Even the concept of a graphical user interface, with its representative icons and mouse-based input, is a modern development, first publicly demonstrated by Douglas Engelbart in 1968 and first commercialized by Apple, with the Lisa and the Macintosh, in the early 1980s (Fisher 2018, pp. 17–26, 85–93, 104–117). Put another way, our now-familiar ways of interacting with digitized archival materials via flat-screen image viewers would be incomprehensibly alien to the people who originally created and used many of these materials — and, in many cases, even to the curators, librarians, and archivists who first acquired and accessioned these items. Even as the technology underpinning Booksnake is radically new, the methods of embodied looking that it enables are very old, closely resembling in form and substance physical, pre-digitized ways of looking.</p>
         <p>	When we encounter physical items, we do so through our bodies, from our individual point of view. Humans are not simply "two eyeballs attached by stalks to a brain computer," as Catherine D'Ignazio and Lauren Klein write in their discussion of data visceralization (2020, §3). We strain toward the far corners of maps. We pivot back and forth between the pages of newspapers. We curl ourselves over small objects like daguerreotypes, postcards, brochures. These embodied, situated, perspectival experiences are inherent to our interactions with physical objects. By using augmented reality to pin life-size digitized items to physical surfaces, Booksnake enables and encourages this kind of embodied exploration. With Booksnake, you can move around an item to see it from all sides, step back to see it in its totality, or get in close to focus on fine details. Integral to Booksnake is what Haraway terms "the particularity and embodiment of all vision" (1988, p. 582). Put another way, Booksnake lets you break out of the god view and see an object as only you can. </p>
         <p>	This is an interpretive choice in our design of Booksnake. Again, image viewers do not simply <hi rend="italic">represent </hi>digital image files to a user, they <hi rend="italic">interpret</hi> them. Booksnake relies on the same digital image files as do flat-screen image viewers, and these files are just as mediated in Booksnake as when they are viewed in a flat-screen viewer. ("There is no unmediated photograph," Haraway writes [1988, p. 583].) But where existing flat-screen image viewers foreground the digital-ness of digitized objects, Booksnake instead recovers and foregrounds their object-ness. Drucker writes that "information spaces drawn from a point of view, rather than as if they were observer independent, reinsert the subjective standpoint of their creation" (2011, ¶20). Drucker was writing about data visualization, but her point holds for image viewers (which, after all, represent a kind of data visualization). Our design decisions create an interpretive space grounded in individual perspective to help a Booksnake user get closer to the "subjective standpoint" of an archival item's original creators and users. </p>
         <p>	Booksnake's emphasis on embodied interaction gives it particular potential as a tool for humanities education. Embodied interaction is a means of accessing situated knowledges (Haraway 1988) and is key to apprehending cultural heritage materials in their full complexity — both physical objects (Kai-Kee, Latina &amp; Sadoyan 2020) and virtual replicas (Kenderdine and Yip 2019). Systematic reviews show AR can support student learning gains, motivation, and knowledge transfer (Bacca et al. 2014). By using AR to make embodied interaction possible with digitized items, Booksnake supports student learning through movement, perspective, and scale. For example, a student could use Booksnake to physically follow an explorer's track across a map, watch a painting’s details emerge as she moves closer, or investigate the relationship between a poster’s size and its message — interactions impossible with a flat-screen viewer. As a virtualization technology, Booksnake is especially effective for educational scenarios that would otherwise be rare, impractical, destructive, or expensive (Bailenson 2018), making it possible to present archival materials in new contexts without risking damage to fragile physical originals. For example, third-grade students could use Booksnake to explore a historic painting by walking atop a digital copy placed on their classroom floor. High school students could use Booksnake to freely flip through the digital pages of a rare codex. College students could use Booksnake to juxtapose physical items held in their institution's library with digitized items held elsewhere. As an educational tool, Booksnake can bring students closer to digitized sources, widening access to archival collections. </p>
         <p>IMMERSIVE TECHNOLOGIES FOR CULTURAL HERITAGE</p>
         <p>	Booksnake is an empty frame. It leverages AR to extend the exploratory freedom that users associate with browsing online collections into physical space. In doing so, Booksnake joins a small collection of digital projects using immersive technologies as the basis for interacting with cultural heritage materials and collections.</p>
         <p>	The Google Arts &amp; Culture app offers the most comparable use of humanistic AR. Like Booksnake, Arts &amp; Culture is a mobile app for smartphones and tablets that offers AR as an interface for digitized materials. A user can activate the app’s "Art Projector" feature to "display life-size artworks, wherever you are" by placing a digitized artwork in their physical surroundings (Luo 2019). But there are three key differences between Google’s app and Booksnake. First, AR is one of many possible interaction methods in Google’s app, which is crowded with stories, exhibits, videos, games, and interactive experiences. In contrast, Booksnake emphasizes AR as its primary interface, foregrounding the embodied experience. Second, Google’s app focuses on visual art (such as paintings, photographs, and sculpture), while Booksnake can display any type of digitized cultural heritage material, making it more broadly useful for humanities scholars. (Booksnake can also display paginated materials, as we discuss further below, while Google's app cannot.) Finally, Google’s app relies on a centralized database model. Google requires participating institutions to upload their collection images and metadata to Google’s own servers, so that Google can format and serve these materials to users (Google no date). In contrast, Booksnake’s use of IIIF enables institutions to retain control over their digitized collections and supports an open humanities software ecosystem.</p>
         <p>	Another set of projects approach immersive technologies as tools for designing and delivering exhibition content. Some use immersive technologies to enhance physical exhibits, such as Veholder, a project exploring technologies and methods for juxtaposing 3D virtual and physical objects in museum settings (Haynes 2018, Haynes 2019). Others are tools for using immersive technologies to create entirely virtual spaces. Many GLAM institutions and artists have adapted Mozilla Hubs, a general-purpose tool for building 3D virtual spaces that can be accessed using a flat-screen Web browser or a VR headset, to build virtual exhibition spaces, although users must manually import digitized materials and construct virtual replicas (Cool 2022). Another project, Diomira Galleries, is a prototype tool for building VR exhibition spaces with IIIF-compliant resources (Bifrost Consulting Group 2023). Like Booksnake, Diomira uses IIIF to import digital images of archival materials, but Diomira arbitrarily scales these images onto template canvases that do not represent an item ’s physical dimensions. As with Booksnake, these projects demonstrate the potential of immersive technologies for new research interactions and collection activation with digitized archival materials.</p>
         <p>	Finally, we are building Booksnake as an AR application for existing consumer mobile devices as a way of lowering barriers to immersive engagement with cultural heritage materials. Most VR projects require expensive special-purpose headsets, which has limited adoption and access (Greengard 2019). In contrast, AR-capable smartphones are ubiquitous, enabling a mobile app to tap the potential of a large existing user base, and positioning such an app to potentially mitigate racial and socioeconomic digital divides in the United States. More Americans own smartphones than own laptop or desktop computers (Pew 2021). And while Black and Hispanic adults in the United States are less likely to own a laptop or desktop computer than white adults, Pew researchers have found "no statistically significant racial and ethnic differences when it comes to smartphone or tablet ownership" (Atske and Perrin 2021). Similarly, Americans with lower household incomes are more likely to rely on smartphones for Internet access (Vogels 2021). Smartphones are thus a key digital platform for engaging and including the largest and most diverse audience. Developing an Android version of Booksnake will enable us to more fully deliver on this potential. </p>
         <p>AUTOMATICALLY TRANSFORMING DIGITAL IMAGES INTO VIRTUAL OBJECTS</p>
         <p>	Booksnake builds on two recently developed technologies. The first is IIIF, an open Web framework designed and built by GLAM institutions to share digitized materials and related metadata (Cramer 2011) (Cramer 2015). By using IIIF, Booksnake builds on and contributes to a robust humanities software ecosystem. IIIF was proposed in 2011 and developed over the early 2010s. Today, the IIIF Consortium is composed of sixty-five global GLAM institutions, from the British Library to Yale University (no date). Dozens more institutions offer access to their collections through IIIF because several common digital asset management (DAM) platforms, including CONTENTdm, LUNA, and Orange DAM, support IIIF (OCLC no date) (Luna Imaging 2018) (orangelogic no date). Widespread IIIF support means that Booksnake is readily compatible with many existing online collections. Booksnake currently supports the IIIF v2.0 APIs and we are building support for the new IIIF v3.0 APIs. Importantly, IIIF is a framework, not a standard. This gives each institution substantial flexibility to customize its implementation of IIIF, but also means that there can be considerable variation in how IIIF is deployed at different institutions (the implications of which we discuss further below). By demonstrating a novel method to transform existing IIIF-compliant resources for interaction in augmented reality, we hope that Booksnake will drive wider IIIF adoption and standardization.  </p>
         <p>	The second technology is a pair of Apple software frameworks, ARKit and RealityKit. ARKit, introduced in 2017, interprets and synthesizes data from an iPhone or iPad's cameras and sensors to understand a user's physical surroundings and to anchor virtual objects to horizontal and vertical surfaces (Apple no date a). RealityKit, introduced in 2019, is a�<note>unable to handle picture here, no embed or link</note>User specifies item for download.Convert item page URL into IIIF manifest URL. Download and parse IIIF manifest. Download digital images linked within IIIF manifest.When user initiates an AR session, use item metadata and images to generate custom virtual object with RealityKit.Display custom virtual object in physical space with ARKit.  Figure 2. This overview summarizes Booksnake's technical pipeline. Booksnake uses IIIF to download item metadata and digital images, then uses RealityKit to construct a custom virtual object, and finally uses ARKit to display the custom virtual object in physical space. framework for rendering and displaying virtual objects, as well as managing a user's interactions with them (for example, by interpreting a user’s on-screen touch gestures) (Apple no date b). Both ARKit and RealityKit are built into the device operating system, enabling us to rely on these frameworks to initiate and manage an AR session. </p>
         <p>	Booksnake thus links IIIF with RealityKit and ARKit to produce a novel result: a pipeline for automatically transforming existing digital images of archival materials into custom virtual objects that replicate the physical original’s real-world proportions, dimensions, and appearance, as well as an AR interface for interacting with these virtual objects in physical space. (See figure 2.) How does Booksnake do this?</p>
         <p>	Booksnake starts with the most humble of Internet components: the URL. A Booksnake user first searches and browses an institution  ’s online catalog through an in-app Web view. Booksnake offers an "Add" button on catalog pages for individual items. When the user taps this button to add an item to their Booksnake library, Booksnake retrieves the item page’s URL. Because of Apple’s privacy restrictions and application sandboxing, this is the only information that Booksnake can read from a given Web page; it cannot directly access content on the page itself. Instead, Booksnake translates the item page’s URL into the URL for the corresponding IIIF manifest.</p>
         <p>	An IIIF manifest is a JSON file — a highly structured, computer-readable text file — that contains a version of the item’s catalog record, including metadata and URLs for associated images. The exact URL translation process varies depending on how an archive has implemented IIIF, but in many cases it is as simple as appending "/manifest.json" to the item URL. For example, the item URL for the Library of Congress’s 1858 "Chart of the submarine Atlantic Telegraph," is  <ref target="https://www.loc.gov/item/2013593216/">https://www.loc.gov/item/2013593216/</ref>, and the item’s IIIF manifest URL is <ref target="https://www.loc.gov/item/2013593216/manifest.json">https://www.loc.gov/item/2013593216/manifest.json</ref>. In other cases, Booksnake may extract a unique item identifier from the item URL, then use that unique identifier to construct the appropriate IIIF manifest URL. Booksnake then downloads and parses the item ’s IIIF manifest.</p>
         <p>	First, Booksnake extracts item metadata from the IIIF manifest. Booksnake uses this metadata to construct an item page in the app ’s Library tab, enabling a user to view much of the same item-level metadata visible in the host archive ’s online catalog. An IIIF manifest presents metadata in key-value pairs, with each pair containing a general label (or key) and a corresponding entry (or value). For example, the IIIF manifest for the 1858 Atlantic telegraph map mentioned above contains the key "Contributors," representing the catalog-level field listing an item’s authors or creators, and the corresponding item-level value "Barker, Wm. J. (William J.) (Surveyor)," identifying the creator of this specific item. Importantly, while the key-value pair structure is generally consistent across IIIF manifests from different institutions, the key names themselves are not. The "Contributors" key at one archive may be named "Creators" at another institution, and "Authors" at a third. The current version of Booksnake simply displays the key-value metadata as provided in the IIIF manifest. A limitation of this approach is that metadata from different archives remains siloed within Booksnake. As Booksnake adds support for additional archives, we plan to identify and link different keys representing the same metadata categories (such as "Contributors," "Creators," and "Authors"). This will enable users, for example, to sort items from different archives by common categories like "Author" or "Date created," or to search within a common category.</p>
         <p>	Second, Booksnake uses image URLs contained in the IIIF manifest to download the digital images associated with an item's catalog record. Helpfully, IIIF image URLs are structured so that certain requests — like the image’s size, its rotation, even whether it should be displayed in color or black-and-white — can be encoded in the URL itself. Booksnake leverages this affordance to request images that are sufficiently detailed for virtual object creation, which sometimes means requesting images larger than what the archive serves by default. For example, Library of Congress typically serves images that are 25% of the original size, but Booksnake modifies the IIIF image URL to request images at 50% of original size. Our testing indicates that this level of resolution produces sufficiently detailed virtual objects, without visible pixelation, for Library of Congress collections.<note> To avoid pixelated virtual objects, the host archive must serve through IIIF a sufficiently high-resolution digital image.</note> We anticipate customizing this value for other institutions. Having downloaded the item’s metadata and digital images, Booksnake can now create a virtual object replicating the physical original. </p>
         <p>	The digitization process involves three pieces of metadata relevant to Booksnake's creation of a size-accurate virtual object. First are the item’s physical dimensions — its height and width — which directly measure the item’s physical size. Second is the resolution at which the item has been digitized. This resolution varies by item type and the reason an item is being digitized, but is typically between 300 and 600 ppi, or pixels per inch. (Pixels are the individual blocks that compose a digital image.) Third are the item’s pixel dimensions, which are the product of the item’s physical dimensions and its digitization resolution. An item’s pixel dimensions can vary considerably depending on the resolution at which it is digitized. For example, a 10 inch by 10 inch item digitized at 300 ppi would produce a 3000 by 3000 pixel image (10 x 300 = 3000), while same item digitized at 600 ppi — twice the resolution — would produce an image measuring 6000 by 6000 pixels (10 x 600 = 6000). Digitization is a one-way street: Without knowing an item’s digitization resolution, it is not mathematically possible to convert pixel dimensions back into physical dimensions. A 6000 x 6000 pixel image may represent a 10 x 10 inch item digitized at 600 ppi. Or it may represent a 20 x 20 inch item digitized at 300 ppi. Or a 5 x 5 inch item digitized at 1200 ppi. As these examples illustrate, there is not a consistent relationship between pixel dimensions and physical dimensions.</p>
         <p>	Here, then, is Booksnake’s central problem: To produce size-accurate virtual objects, Booksnake must be able to access <hi rend="italic">either</hi> an item’s physical dimensions, or <hi rend="italic">both </hi>the item’s pixel dimensions and its digitization resolution. But archives vary widely in the type and quality of dimensional metadata they provide — both across different institutions, and across different collections at the same institution. In general, archives take one of three approaches to providing dimensional metadata.</p>
         <p>	The simplest approach is to list an item’s physical dimensions as metadata in its IIIF manifest. Some archives, such as the David Rumsey Map Collection, provide separate fields for an item’s height and width, each labeled with units of measure. This formatting provides the item’s dimensions in a computer-readable format, making it straightforward to create a virtual object of the appropriate size. Alternatively, an archive may use the IIIF Physical Dimension service, an optional service that provides the scale relationship between an item’s physical and pixel dimensions, along with the physical units of measure (International Image Interoperability Framework 2015). But we are unaware of any institution that has implemented this service for its collections.<note> David Newberry's "IIIF for Dolls" web tool makes inventive use of the Physical Dimension service to rescale digitized materials (Newberry [2023]).</note>
         </p>
         <p>	A more common approach is to provide an item’s physical dimensions in a format that is not immediately computer-readable. The Huntington Digital Library, for example, typically lists an item’s dimensions as part of a text string in the "physical description" field. <ref target="https://hdl.huntington.org/digital/collection/p9539coll1/id/12262/rec/2">This c.1921 steamship poster</ref>, for example, is described as: "Print ; image 60.4 x 55 cm (23 3/4 x 21 5/8 in.) ; overall 93.2 x 61 cm (36 11/16 x 24 in.)" [spacing <hi rend="italic">sic</hi>]. To interpret this text string and convert it into numerical dimensions, a computer program like Booksnake requires additional guidance. Which set of dimensions to use, "image" or "overall"? Which units of measure, centimeters or inches? And what if the string includes additional descriptors, such as "folded" or "unframed"? We are currently collaborating with the Huntington to develop methods to parse dimensional metadata from textual descriptions like this one, with extensibility to other institutions and collections.</p>
         <p>	Finally, an archive may not provide <hi rend="italic">any</hi> dimensional metadata in its IIIF manifests. This is the case with the Library of Congress (LOC), which lists physical dimensions, where available, in an item’s catalog record, but does not provide this information in the item’s IIIF manifest.<note> Library of Congress uses MARC data to construct IIIF manifests, but this is sometimes a "lossy" process (Woodward 2021).</note> This presented us with a problem: How to create dimensionally-accurate virtual objects without access to the item’s original physical dimensions? After much research and troubleshooting, we hit upon a solution. IIIF manifests contain the pixel dimensions of an item’s images. We initially dismissed this as a source of dimensional metadata because, again, there is no consistent relationship between physical and pixel dimensions. And yet, during early-stage testing, Booksnake consistently created virtual objects that matched the listed physical dimensions for maps held by LOC, even as items from other LOC collections resulted in virtual objects with wildly incorrect sizing. This meant there <hi rend="italic">was</hi> a relationship, at least for one collection — we just had to find it. LOC digitizes items to FADGI standards, which specify performance requirements for digitization, including target resolutions. Crucially, FADGI standards vary by item type. For example, FADGI standards specify a target resolution of 600 ppi for prints and photographs, and 400 ppi for books.<note> Both ppi requirements are for FADGI’s highest performance level, 4 Star.</note> This meant we could associate every LOC item type with a target ppi, which made pixel dimensions in IIIF manifests potentially useful. Through trial-and-error research, we also discovered that LOC scales down item images in some collections. (For example, map images are scaled down by a factor of four.) We then combined digitization resolution and scaling factors into a pre-coded reference table. When Booksnake imports an item from Library of Congress, it consults the item’s IIIF manifest to determine the item type, then consults the reference table to determine the appropriate factor for converting the item’s pixel dimensions to physical dimensions in meters, the standard unit of measurement for AR and VR environments. This solution is similar to a client-side version of the IIIF Physical Dimension service, customized to LOC’s digital collections. </p>
         <p>	As these examples suggest, determining an item’s physical dimensions is a seemingly simple problem that can quickly become complicated. Developing robust methods for parsing different types of dimensional metadata is a key research area because these methods will allow us to expand the range of archives and materials with which Booksnake is compatible. </p>
         <p>	Booksnake uses this information to create virtual objects on demand. When a user taps "View in Your Space" to initiate an AR session, Booksnake uses RealityKit to transform the item’s digital image into a custom virtual object suitable for interaction in physical space. First, Booksnake creates a blank two-dimensional virtual plane sized to match the item’s physical dimensions. Next, Booksnake applies the downloaded image to this virtual plane as a texture, scaling the image to match the size of the plane. This results in a custom virtual object that matches the original item’s physical dimensions, proportions, and appearance. This process is invisible to the user — Booksnake "just works." This process is straightforward for flat digitized items like maps or posters, which are typically digitized from a single perspective, of their recto (front) side.</p>
         <p>	Booksnake’s virtual object creation process is more complicated for compound objects, which have multiple images linked to a single item record. Examples of compound objects include books, issues of periodicals or newspapers, diaries, scrapbooks, photo albums, and postcards. The simplest compound objects, such as postcards, have two images showing the object’s recto (front) and verso (back) sides. Nineteenth-century newspapers may have four or eight pages, while books and bound manuscripts may run into hundreds of pages, with each page typically captured and stored as a single image. </p>
         <p>	Booksnake handles compound objects by creating multiple individual virtual objects, one for each item image, then arranging and animating these objects to support the illusion of a cohesive compound object. Our initial implementation is modeled on a generic paginated codex, with multiple pages folded around or bound to a central spine. As with flat objects, this creation process happens on demand, when a user starts an AR session. Booksnake assumes that a compound object’s first image depicts the object’s cover or first page (more on this below). Booksnake creates a virtual object for this first image, then creates an invisible virtual object with the same dimensions, which acts as an invisible "page zero." Booksnake links these two planes, as if along a central spine, placing the invisible "page zero" to the left of page one. The user sees the first page of a newspaper or the cover of a book, sitting alone, waiting to be opened. When the user swipes from right to left across the object edge to "turn" the virtual page, Booksnake retrieves the next two images and transforms them into virtual objects representing pages two and three. Booksnake aligns page two on the reverse side of page one, and positions page three directly underneath page one. Booksnake then animates a page turn by rotating pages one and two together, over one hundred and eighty degrees, with the object’s "spine" serving as the rotation axis. Once the animation is complete, pages two and three have replaced the invisible "page zero" and page one, and Booksnake discards the virtual objects representing pages zero and one. This on-demand virtual compound object creation process means that Booksnake only needs to load a maximum of four images into AR space, optimizing for memory and performance. By using a swipe gesture to turn pages, Booksnake leverages navigation affordances with which users are already familiar from interactions with physical paginated items, supporting immersion and engagement. The page-turn animation, paired with a page-turn sound effect, further enhances the realism of the virtual experience.</p>
         <p>	A key limitation in our initial approach to creating virtual paginated objects is that our generic codex model is based on one particular type of object, the newspaper. Specifically, we used LOC's Chronicling America collection of historical newspapers as a testbed to develop the pipeline for creating virtual paginated objects, as well as the user interface and methods for interacting with virtual paginated objects in physical space. While the newspaper is broadly representative of the codex form's physical features and affordances, making it readily extensible to other paginated media, there are important differences in how different media are digitized and presented in online collections. For example, while Chronicling America newspapers have one page per image, some digitized books held by LOC have two pages per image. We are adapting Booksnake's object creation pipeline to identify double-page images, split the image in half, and wrap each resulting image onto individual facing pages.<note> Another example: For some digitized books held by LOC, the first image is of the book spine, not the book cover. Booksnake incorporates logic to identify and ignore book spine images.</note> There are also important cultural differences in codex interaction methods: We plan to further extend the capabilities of this model by supporting right-to-left books in languages like Arabic, Chinese, Hebrew, and Japanese.<note> Reading direction is rarely recorded in item metadata, so we are investigating reliable item-level proxies (such as place of publication) or collection-level metadata (e.g., a collection of Japanese rare books) that Booksnake could use to determine an item's reading direction.</note>
         </p>
         <p>	A further limitation of our initial approach is our assumption that all compound objects represent paginated material organized as a codex, which is not the case. Booksnake cannot yet realistically display items like scrolls or Mesoamerican screenfold books (which are often digitized by page, but differ in physical construction and interaction methods from Western books). In other cases, a compound object often comprises a collection of individual items that are stored together but not physically bound to each other. One example is <ref target="https://www.loc.gov/item/pldec.110/">"Polish Declarations of Admiration and Friendship for the United States"</ref>, held by LOC, which consists of 181 unbound sheets. Or an archive may use a compound object to store multiple different images of a single object. For example, the Yale Center for British Art (YCBA) often provides several images for paintings in its collections, as with<ref target="https://collections.britishart.yale.edu/catalog/tms:52309"> the 1769 George Stubbs painting "Water Spaniel,"</ref> for which YCBA provides four images: of the framed painting, the unframed painting, the unframed image cropped to show just the canvas, and the framed painting’s verso side. We plan to continue refining Booksnake’s compound object support by using collection- and item-level metadata to differentiate between paginated and non-paginated compound objects, in order to realistically display different types of materials.</p>
         <p>	Having constructed a custom virtual object, Booksnake next opens the live camera view so that the user can interact with the object in physical space. As the user moves their device to scan their surroundings, Booksnake overlays a transparent image of the object, outlined in red, over horizontal and vertical surfaces, giving the user a preview of how the object will fit in their space. Tapping the transparent preview anchors the object, which turns opaque, to a physical surface. As the user and device move, Booksnake uses camera and sensor data to maintain the object's relative location in physical space, opening the object to embodied exploration. The user can crouch down and peer at the object from the edge of the table, pan across the object's surface, or zoom into the key details by moving closer to the object. The user can also enlarge or shrink an object by using the familiar pinch-to-zoom gesture. The object remains in place even when out of view of the device: A user can walk away from an anchored object, then turn back to look at it from across the room. </p>
         <p>	The first version of Booksnake represents a proof of concept, of both the pipeline to transform digitized items into virtual objects and of AR as an interface for research, teaching, and learning with digitized archival materials. In addition to researching development of an Android version, our next steps are to refine Booksnake's pipeline and AR interface. One major research focus is improving how Booksnake identifies, ingests, and interprets different types of dimensional metadata, to support as many existing online collections as possible. Another is to expand the range of interactions possible with virtual objects — for example, making it possible for users to annotate a virtual object with text or graphics, or to adjust the transparency of a virtual object (to better support comparison between virtual and physical items). We are also working to build support for additional archives and collections. In the longer term, once the IIIF Consortium finalizes protocols for 3D archival data and metadata (IIIF consortium staff 2022), we anticipate that future versions of Booksnake will be able to download and display 3D objects. Ongoing user testing and feedback will further inform Booksnake's continuing development. </p>
         <p>	Our technical work enables Booksnake users to bring digitized cultural heritage materials out of flat screens and onto their desks and walls, using a tool — the consumer smartphone or tablet — that they already own. Meanwhile, our conceptual work to construct size-accurate virtual objects from available metadata will help to make a large, culturally significant, and ever-growing body of digitized materials available for use in other immersive technology projects where the dimensional accuracy of virtual objects is important, such as virtual museum exhibits or manuscript viewers. </p>
         <p>CONCLUSION</p>
         <p>	"What kind of interface exists after the screen goes away?" asks Johanna Drucker. "I touch the surface of my desk and it opens to the library of the world? My walls are display points, capable of offering the inventory of masterworks from the world’s museums and collections into view?" (2014, p. 195). These questions point toward spatial interfaces, and this is what Booksnake is building toward. Futurists and science-fiction authors have long positioned virtual reality as a means of transporting users away from their day-to-day humdrum reality into persistent interactive three-dimensional virtual worlds, sometimes called the metaverse (Gibson 1984) (Stephenson 1992) (Cline 2011) (Ball 2022). Booksnake takes a different approach, bringing life-size virtual representations of real objects into a user's physical surroundings for embodied interaction.</p>
         <p>	We’re building Booksnake to enable digital accessibility and connect more people with primary sources. At its core, Booksnake is very simple. It is a tool for transforming digital images of cultural heritage materials into size-accurate virtual objects. In doing so, Booksnake leverages augmented reality as a means of interacting with archival materials that is radically new, but that feels deeply familiar. Booksnake is a way to virtualize the reading room or museum experience — and thereby democratize it, making embodied interaction with cultural heritage materials in physical space more accessible to more people in more places.</p>
         <p/>
         <p>REFERENCES</p>
         <p>Almas, Bridget, et al. (2018) 'Manuscript Study in Digital Spaces: The State of the Field and New Ways Forward', <hi rend="italic">Digital Humanities Quarterly</hi> 12(2). Available at: <ref target="http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html">http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html</ref>.</p>
         <p>Apple. (N.d.a) <hi rend="italic">ARKit</hi> [Online]. Apple Developer. Available at: <ref target="https://developer.apple.com/documentation/arkit/">https://developer.apple.com/documentation/arkit/</ref>.</p>
         <p>Apple. (N.d.b) <hi rend="italic">RealityKit</hi> [Online]. Apple Developer. Available at: <ref target="https://developer.apple.com/documentation/realitykit/">https://developer.apple.com/documentation/realitykit/</ref>.</p>
         <p>Atske, Sara, and Perrin, Andrew. (2021) 'Home broadband adoption, computer ownership vary by race, ethnicity in the U.S.' <hi rend="italic">Pew Research Center</hi>. Available at: <ref target="https://pewrsr.ch/3Bdn6tW">https://pewrsr.ch/3Bdn6tW</ref>.</p>
         <p>Azuma, R., et al. (2001) 'Recent advances in augmented reality,' <hi rend="italic">IEEE Computer Graphics and Applications</hi> 21(6), 34–47. Available at: <ref target="https://doi.org/10.1109/38.963459">https://doi.org/10.1109/38.963459</ref>.</p>
         <p>Bacca, Jorge, et al. (2014) 'Augmented Reality Trends in Education: A Systematic Review of Research and Applications,' <hi rend="italic">Educational Technology &amp; Society</hi>, 17(4), pp. 133–149. Available at: <ref target="https://www.jstor.org/stable/jeductechsoci.17.4.133">https://www.jstor.org/stable/jeductechsoci.17.4.133</ref>.</p>
         <p>Bailenson, Jeremy. (2018) <hi rend="italic">Experience on Demand: What Virtual Reality is, How it Works, and What it Can Do. </hi>New York: W. W. Norton &amp; Co.</p>
         <p>Ball, Matthew. (2022) <hi rend="italic">The Metaverse: And How It Will Revolutionize Everything. </hi>New York: Liveright.</p>
         <p>Bifrost Consulting Group. (2023) Let's build cultural spaces in the metaverse. Available at: <ref target="https://diomira.ca/">https://diomira.ca/</ref>.</p>
         <p>Chandler, Tom, et al. (2017) 'A New Model of Angkor Wat: Simulated Reconstruction as a Methodology for Analysis and Public Engagement,' <hi rend="italic">Australian and New Zealand Journal of Art</hi> 17(2), pp. 182–194. Available at: <ref target="https://doi.org/10.1080/14434318.2017.1450063">https://doi.org/10.1080/14434318.2017.1450063</ref>.</p>
         <p>Cline, Ernest. (2011) <hi rend="italic">Ready Player One</hi>. New York: Broadway Books.</p>
         <p>Cool, Matt. (2022) '5 Inspiring Galleries Built with Hubs' [Blog]. <hi rend="italic">Mozilla Hubs Creator Labs</hi>. Available at: <ref target="https://hubs.mozilla.com/labs/5-incredible-art-galleries/">https://hubs.mozilla.com/labs/5-incredible-art-galleries/</ref>.</p>
         <p>Cramer, Tom. (2011) 'The International Image Interoperability Framework (IIIF): Laying the Foundation for Common Services, Integrated Resources and a Marketplace of Tools for Scholars Worldwide,' Coalition for Networked Information Fall 2011 Membership Meeting, Arlington, Virginia, December 12–13. Available at: <ref target="https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework">https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework</ref>.</p>
         <p>–––––. (2015) <hi rend="italic">IIIF Consortium Formed</hi> [Online]. International Image Interoperability Framework. Available at: <ref target="https://iiif.io/news/2015/06/17/iiif-consortium/">https://iiif.io/news/2015/06/17/iiif-consortium/</ref>.</p>
         <p>Crane, Tom. (2021) <hi rend="italic">On being the right size </hi>[Online]. Canvas Panel<hi rend="italic">.</hi> Available at: <ref target="https://canvas-panel.digirati.com/developer-stories/rightsize.html">https://canvas-panel.digirati.com/developer-stories/rightsize.html</ref>.</p>
         <p>D'Ignazio, Catherine, and Klein, Lauren F. (2020) <hi rend="italic">Data Feminism</hi>. Cambridge, Mass.: The MIT Press. Available at: <ref target="https://data-feminism.mitpress.mit.edu/">https://data-feminism.mitpress.mit.edu/</ref>.</p>
         <p>Drucker, Johanna. (2014) <hi rend="italic">Graphesis: Visual Forms of Knowledge Production</hi>. Cambridge, Mass.: Harvard University Press. </p>
         <p>–––––. (2013) 'Is There a “Digital” Art History?', <hi rend="italic">Visual Resources </hi>29(1-2). Available at: <ref target="https://doi.org/10.1080/01973762.2013.761106">https://doi.org/10.1080/01973762.2013.761106</ref>.</p>
         <p>–––––. (2011) 'Humanities Approaches to Graphical Display', <hi rend="italic">Digital Humanities Quarterly </hi>5(1). Available at: <ref target="https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</ref>.</p>
         <p>Falbo, Bianca. (2000) 'Teaching from the Archives,' <hi rend="italic">RBM: A Journal of Rare Books, Manuscripts, and Cultural Heritage</hi>, 1(1), pp. 33–35. Available at: <ref target="https://doi.org/10.5860/rbm.1.1.173">https://doi.org/10.5860/rbm.1.1.173</ref>.</p>
         <p>Federal Agency Digitization Guidelines Initiative. (2023) <hi rend="italic">Technical Guidelines for Digitizing Cultural Heritage Materials: Third Edition</hi>. [Washington, DC: Federal Agency Digitization Guidelines Initiative.] Available at: <ref target="https://www.digitizationguidelines.gov/guidelines/digitize-technical.html">https://www.digitizationguidelines.gov/guidelines/digitize-technical.html</ref>.</p>
         <p>Fisher, Adam. (2018) <hi rend="italic">Valley of Genius: The Uncensored History of Silicon Valley</hi>. New York: Twelve.</p>
         <p>François, Paul, et al. (2021) 'Virtual reality as a versatile tool for research, dissemination and mediation in the humanities,' <hi rend="italic">Virtual Archaeology Review</hi> 12(25), pp. 1–15. Available at: <ref target="https://doi.org/10.4995/var.2021.14880">https://doi.org/10.4995/var.2021.14880</ref>.</p>
         <p>Gibson, William. (1984) <hi rend="italic">Neuromancer. </hi>Reprint, New York: Ace, 2018.</p>
         <p>Google. (N.d.) <hi rend="italic">Add items</hi> [Online]. Google Arts &amp; Culture Platform Help. Available at: <ref target="https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA">https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA</ref>
         </p>
         <p>Greengard, Samuel. (2019) <hi rend="italic">Virtual Reality</hi>. Cambridge, Mass.: The MIT Press.</p>
         <p>Haraway, Donna. (1988) 'Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective,' <hi rend="italic">Feminist Studies </hi>14(3), pp. 575–599. Available at: <ref target="https://www.jstor.org/stable/3178066">https://www.jstor.org/stable/3178066</ref>.</p>
         <p>Haynes, Ronald. (2018) 'Eye of the Veholder: AR Extending and Blending of Museum Objects and Virtual Collections' in Jung, T., and tom Dieck, M. C. (eds.) <hi rend="italic">Augmented Reality and Virtual Reality: Empowering Human, Place and Business.</hi> Cham, Switzerland: Springer, pp. 79–91. Available at: <ref target="https://doi.org/10.1007/978-3-319-64027-3_6">https://doi.org/10.1007/978-3-319-64027-3_6</ref>.</p>
         <p>–––––. (2019) 'To Have and Vehold: Marrying Museum Objects and Virtual Collections via AR' in tom Dieck, M. C., and Jung, T. (eds.) <hi rend="italic">Augmented Reality and Virtual Reality: The Power of AR and VR for Business. </hi>Cham, Switzerland: Springer, pp. 191–202. Available at: <ref target="https://doi.org/10.1007/978-3-030-06246-0_14">https://doi.org/10.1007/978-3-030-06246-0_14</ref>.</p>
         <p>Herron, Thomas. (2020) <hi rend="italic">Recreating Spenser: The Irish Castle of an English Poet</hi>. Greenville, N.C.: Author. </p>
         <p>IIIF consortium staff. (2022) <hi rend="italic">New 3D Technical Specification Group </hi>[Online]. Available at: <ref target="https://iiif.io/news/2022/01/11/new-3d-tsg/">https://iiif.io/news/2022/01/11/new-3d-tsg/</ref>.</p>
         <p>International Image Interoperability Framework. (N.d.) <hi rend="italic">Consortium Members</hi> [Online]. Available at: <ref target="https://iiif.io/community/consortium/members/">https://iiif.io/community/consortium/members/</ref>.</p>
         <p>–––––. (2015) 'Physical Dimensions' in Albritton, Benjamin, et al. (eds.) <hi rend="italic">Linking to External Services </hi>[Online]. Available at: <ref target="https://iiif.io/api/annex/services/#physical-dimensions">https://iiif.io/api/annex/services/#physical-dimensions</ref>.</p>
         <p>Kai-Kee, Elliott, Latina, Lissa, and Sadoyan, Lilit. (2020) <hi rend="italic">Activity-based Teaching in the Art Museum: Movement, Embodiment, Emotion</hi>. Los Angeles: Getty Museum. </p>
         <p>Kenderdine, Sarah, and Yip, Andrew. 'The Proliferation of Aura: Facsimiles, Authenticity and Digital Objects' in Drtoner, K., et al. (eds.) <hi rend="italic">The Routledge Handbook of Museums, Media and Communication</hi>. London and New York: Routledge, pp. 274–289. Available at: <ref target="https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip">https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip</ref>
         </p>
         <p>Kropf, Evyn. (2017) 'Will that Surrogate Do?: Reflections on Material Manuscript Literacy in the Digital Environment from Islamic Manuscripts at the University of Michigan Library,' <hi rend="italic">Manuscript Studies</hi>, 1(1), pp. 52–70. Available at: <ref target="https://doi.org/10.1353/mns.2016.0007">https://doi.org/10.1353/mns.2016.0007</ref>.</p>
         <p>Luna Imaging. (2018) <hi rend="italic">LUNA and IIIF</hi> [Online]. Available at: <ref target="http://www.lunaimaging.com/iiif">http://www.lunaimaging.com/iiif</ref>.</p>
         <p>Luo, Michelle. (2019) Explore art and culture through a new lens [Blog] <hi rend="italic">The Keyword</hi>. Available at: <ref target="https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/">https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/</ref>
         </p>
         <p>Newberry, David. [(2023)] <hi rend="italic">IIIF for Dolls</hi> [Online]. Available at: <ref target="https://iiif-for-dolls.davidnewbury.com/">https://iiif-for-dolls.davidnewbury.com/</ref>. </p>
         <p>Nolan, Maura. (2013) 'Medieval Habit, Modern Sensation: Reading Manuscripts in the Digital Age,' <hi rend="italic">The Chaucer Review</hi>, 47(4), pp. 465–476. Available at: <ref target="https://doi.org/10.5325/chaucerrev.47.4.0465">https://doi.org/10.5325/chaucerrev.47.4.0465</ref>.</p>
         <p>OCLC. (N.d.) <hi rend="italic">CONTENTdm and the International Image Interoperability Framework (IIIF)</hi> [Online]. Available at: <ref target="https://www.oclc.org/en/contentdm/iiif.html">https://www.oclc.org/en/contentdm/iiif.html</ref>.</p>
         <p>orangelogic. (N.d.) <hi rend="italic">Connect to your favorite systems: Get DAM integrations that fit your existing workflows </hi>[Online]. Available at: <ref target="https://www.orangelogic.com/features/integrations">https://www.orangelogic.com/features/integrations</ref>.</p>
         <p>OpenSeadragon. (N.d.) <hi rend="italic">OpenSeadragon</hi>. Available at: <ref target="http://openseadragon.github.io/">http://openseadragon.github.io/</ref>.</p>
         <p>Pew Research Center. (2021) Mobile Fact Sheet. Available at: <ref target="http://pewrsr.ch/2ik6Ux9">http://pewrsr.ch/2ik6Ux9</ref>.</p>
         <p>Project Mirador. (N.d.) <hi rend="italic">Mirador</hi>. Available at: <ref target="https://projectmirador.org/">https://projectmirador.org/</ref>.</p>
         <p>Putnam, Lara. (2016) 'The Transnational and the Text-Searchable: Digitized Sources and the Shadows They Cast,' <hi rend="italic">The American Historical Review</hi>, 121(2), pp. 377–402. Available at: <ref target="https://doi.org/10.1093/ahr/121.2.377">https://doi.org/10.1093/ahr/121.2.377</ref>.</p>
         <p>Ramsay, Stephen. (2014) 'The Hermeneutics of Screwing Around; or What You Do With a Million Books' in Kee, Kevin (ed.) <hi rend="italic">Pastplay: Teaching and Learning History with Technology</hi>. Ann Arbor: University of Michigan Press, pp. 111–120. Available at: <ref target="https://doi.org/10.1353/book.29517">https://doi.org/10.1353/book.29517</ref>.</p>
         <p>Rosenzweig, Roy. (2003). 'Scarcity or Abundance? Preserving the Past in a Digital Era,' <hi rend="italic">The American Historical Review</hi> 108 (3), pp. 735–762. Available at: <ref target="https://doi.org/10.1086/ahr/108.3.735">https://doi.org/10.1086/ahr/108.3.735</ref>.</p>
         <p>Roth, Andrew, and Fisher, Caitlin. (2019) 'Building Augmented Reality Freedom Stories: A Critical Reflection,' in Kee, Kevin, and Compeau, Timothy (eds.)  <hi rend="italic">Seeing the Past with Computers: Experiments with Augmented Reality and Computer Vision for History.</hi> Ann Arbor: University of Michigan Press, pp. 137–157. Available at: <ref target="https://www.jstor.org/stable/j.ctvnjbdr0.11">https://www.jstor.org/stable/j.ctvnjbdr0.11</ref>
         </p>
         <p>Schmiesing, Ann, and Hollis, Deborah. (2002). 'The Role of Special Collections Departments in Humanities Undergraduate and Graduate Teaching: A Case Study,' <hi rend="italic">Libraries and the Academy</hi> 2(3), pp. 465–480. Available at: <ref target="https://doi.org/10.1353/pla.2002.0065">https://doi.org/10.1353/pla.2002.0065</ref>.</p>
         <p>Shemek, Deanna, et al. (2018) 'Renaissance Remix. Isabella d'Este: Virtual Studiolo,' <hi rend="italic">Digital Humanties Quarterly</hi>, 12(4). Available at: <ref target="http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html">http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html</ref>.</p>
         <p>Solberg, Janine. (2012) 'Googling the Archive: Digital Tools and the Practice of History.' <hi rend="italic">Advances in the History of Rhetoric</hi> 15(1), pp. 53–76. Available at: <ref target="https://doi.org/10.1080/15362426.2012.657052">https://doi.org/10.1080/15362426.2012.657052</ref>.</p>
         <p>Stephenson, Neal. (1992) <hi rend="italic">Snow Crash</hi>. Reprint, New York: Bantam, 2000.</p>
         <p>Szpiech, Ryan. (2014) 'Cracking the Code: Reflections on Manuscripts in the Age of Digital Books.' <hi rend="italic">Digital Philology: A Journal of Medieval Cultures</hi> 3(1), pp. 75–100. Available at: <ref target="https://doi.org/10.1353/dph.2014.0010">https://doi.org/10.1353/dph.2014.0010</ref>.</p>
         <p>Toner, C. (1993). 'Teaching Students to be Historians: Suggestions for an Undergraduate Research Seminar,' <hi rend="italic">The History Teacher</hi> 27(1), pp. 37–51. Available at: <ref target="https://doi.org/10.2307/494330">https://doi.org/10.2307/494330</ref>.</p>
         <p>van Lit, L. W. C. (2020). <hi rend="italic">Among Digitized Manuscripts: Philology, Codicology, Paleography in a Digital World</hi>. Leiden: Brill.</p>
         <p>van Zundert, Joris. (2018) 'On Not Writing a Review about Mirador: Mirador, IIIF, and the Epistemological Gains of Distributed Digital Scholarly Resources.' <hi rend="italic">Digital Medievalist</hi> 11(1). Available at: <ref target="http://doi.org/10.16995/dm.78">http://doi.org/10.16995/dm.78</ref>.</p>
         <p>Vogels, Emily A. (2021) 'Digital divide persists even as Americans with lower incomes make gains in tech adoption' <hi rend="italic">Pew Research Center</hi>. Available at: <ref target="https://pewrsr.ch/2TRM7cP">https://pewrsr.ch/2TRM7cP</ref>
         </p>
         <p>Woodward, Dave. (2021) Email to AUTHOR1, February 2.</p>
      </body>
      <back>
         <listBibl>
            <bibl/>
         </listBibl>
      </back>
   </text>
</TEI>
