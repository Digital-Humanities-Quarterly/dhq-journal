<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Introducing Booksnake: A Scholarly App for
               Transforming Existing Digitized Archival Materials into Life-Size Virtual Objects for
               Embodied Interaction in Augmented Reality with
               IIIF<!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->


            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Sean <dhq:family>Fraga</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  >https://orcid.org/0000-0002-6804-4486<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>sfraga@usc.edu</email>
               <dhq:bio>
                  <p>Sean Fraga is the creator of and project director for Booksnake, which he
                     developed as an Andrew W. Mellon postdoctoral fellow with the Humanities in a
                     Digital World program at the University of Southern California. He is currently
                     an Assistant Professor (Teaching) of Environmental Studies and History at
                     USC.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Christy <dhq:family>Ye</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>christyspace1731@gmail.com</email>
               <dhq:bio>
                  <p>Christy Ye is a game developer and designer who creates interactive experiences
                     around meaningful play and narratives in projects across a variety of media.
                     She holds an MFA in Interactive Media and Game Design from the USC School of
                     Cinematic Arts.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Henry <dhq:family>Huang</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>henry2423@gmail.com </email>
               <dhq:bio>
                  <p>Shih-Hsuan (Henry) Huang is an experienced software engineer with a Master’s
                     degree in Computer Science from the University of Southern California. He
                     commenced his professional journey at Apple, where he has dedicated his career
                     to developing innovative solutions that positively impact the world.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Zack <dhq:family>Sai</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>saidane@usc.edu</email>
               <dhq:bio>
                  <p>Zack Sai is an undergraduate student majoring in Computer Science, with a minor
                     in mobile app development, in the USC Viterbi School of Engineering. </p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Michael <dhq:family>Hughes</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>hughesmr@usc.edu</email>
               <dhq:bio>
                  <p>Michael Hughes is an undergraduate student majoring in Computer Science in the
                     USC Viterbi School of Engineering. </p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>April <dhq:family>Yao</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Southern California</dhq:affiliation>
               <email>siyuyao@usc.edu</email>
               <dhq:bio>
                  <p>April Yao is a Master's student in Computer Science at the University of
                     Southern California.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Samir <dhq:family>Ghosh</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of California, Santa Cruz</dhq:affiliation>
               <email>samir.ghosh@ucsc.edu</email>
               <dhq:bio>
                  <p>Samir Ghosh is a PhD Student in the Department of Computational Media at UC
                     Santa Cruz. He researches virtual reality interfaces at the Social Emotional
                     Technology Lab.</p>
               </dhq:bio>
            </dhq:authorInfo>


         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000765<!--including leading zeroes: e.g. 000110--></idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date><!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/projects.xml"
                     >http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp="#ar"/>
               <term corresp="#digital_libraries"/>
               <term corresp="#mobile"/>
               <term corresp="#code_studies"/>

            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item>Augmented reality</item>
                  <item>IIIF</item>
                  <item>Digital Collections</item>
                  <item>Humanities software</item>
                  <item>Mobile app development</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref
               target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000765/000765.xml"
               >GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>

         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>We introduce Booksnake, a new mobile app that makes it feel like digitized archival
               items are physically present in a user’s real-world surroundings by using the
               augmented reality (AR) technology in consumer smartphones and tablets. Unlike
               humanities projects that use virtual reality (VR) or AR to publish custom content,
               Booksnake is a general-purpose, content-agnostic tool compatible with existing online
               collections that support the International Image Interoperability Framework (IIIF).
               In this article, we critique existing flat-screen image viewers and discuss the
               benefits of embodied interaction with archival materials. We contextualize Booksnake
               within the broader landscape of immersive technologies for cultural heritage. We
               detail the technical pipeline by which Booksnake transforms existing digitized
               archival materials into custom life-size virtual objects for interaction in physical
               space. We conclude with a brief discussion of the future of the immersive
               humanities.</p>
         </dhq:abstract>

         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>Learn about Booksnake, a new app for bringing digitized cultural heritage materials
               into physical space via augmented reality, creating a novel spatial interface for
               embodied research with digitized materials.</p>
         </dhq:teaser>
      </front>

      <body>




         <div>

            <head>INTRODUCTION</head>

            <p>Close engagement with primary sources is foundational to humanities research,
               teaching, and learning <ptr target="#falbo_2000"/>, <ptr
                  target="#schmiesing_hollis_2002"/>, <ptr target="#toner_1993"/>. We are fortunate
               to live in an age of digital abundance: Over the past three decades, galleries,
               libraries, archives, and museums (collectively known as GLAM institutions) have
               undertaken initiatives to digitize their collection holdings, making millions of
               primary-source archival materials freely available online and more accessible to more
               people than ever before <ptr target="#rosenzweig_2003"/>,<ptr target="#solberg_2012"
               />, <ptr target="#ramsay_2014"/><ptr target="#putnam_2016"/>. Increasingly, the
               world’s myriad cultural heritage materials — architectural plans, books, codices,
               correspondence, drawings, ephemera, manuscripts, maps, newspapers, paintings,
               paperwork, periodicals, photographs, postcards, posters, prints, sheet music,
               sketches, slides, and more — are now only a click away.</p>

            <p>But interacting with digitized archival materials in a Web browser can be a
               frustrating experience — one that fails to replicate the embodied engagement possible
               during in-person research. We have become resigned to the limitations of Web-based
               image viewers. How we have to fiddle with <hi rend="quotes">Zoom In</hi> and <hi
                  rend="quotes">Zoom Out</hi> buttons to awkwardly jump between fixed levels of
               magnification. How we must laboriously click and drag, click and drag, over and over,
               to follow a line of text across a page, or read the length of a newspaper column, or
               trace a river over a map. How we can’t ever quite tell how big or small something is.
               We have long accepted these frictions as necessary compromises to quickly and easily
               view materials instead of making an inconvenient trip to an archive itself — even as
               we also understand that clicking around in a Web viewer pales in comparison to the
               rich, fluid, embodied experience of interacting with a physical item in a museum
               gallery or library reading room.</p>

            <p>As a step toward moving past these limitations, we present a new kind of image
               viewer. Booksnake is a scholarly app for iPhones and iPads that enables users to
               interact with existing digitized archival materials at life size in physical space
               (see figure 1). Instead of displaying digital images on a flat screen for indirect
               manipulation, Booksnake uses augmented reality, the process of overlaying a virtual
               object on physical space <ptr target="#azuma_etal_2001"/>, to bring digitized items
               into the physical world for embodied exploration. Existing image viewers display
               image files as a means of conveying visual information about an archival item. In
               contrast, Booksnake converts image files into virtual objects in order to create the
               feeling of being in the item's presence. To do this, Booksnake automatically
               transforms digital images of archival materials into custom, size-accurate virtual
               objects, then dynamically inserts these virtual objects into the live camera view on
               a smartphone or tablet for interaction in physical space (see figure 2). Booksnake is
               currently available as a free public beta (<ref target="https://booksnake.app/beta/"
                  >https://booksnake.app/beta/</ref>) and will be available as a free app on Apple's
               App Store by the end of 2024.</p>

            <p>Booksnake's use of AR makes it feel like a digitized item is physically present in a
               user’s real-world environment, enabling closer and richer engagement with digitized
               materials. A Booksnake user aims their phone or tablet at a flat surface (like a
               table, wall, bed, or floor) and taps the screen to anchor the virtual object to the
               physical surface. As the user moves, Booksnake uses information from the device’s
               cameras and sensors to continually adjust the virtual object’s relative position,
               orientation, and size in the camera view, such that the item appears to remain
               stationary in physical space as the user and device move. A user thus treats
               Booksnake like a lens, looking <hi rend="italic">through</hi> their device’s screen
               at a virtual object overlaid on the physical world.</p>

            <p>The project takes its name from book snakes, the weighted strings used by archival
               researchers to hold fragile physical materials in place. Similarly, Booksnake enables
               users to keep virtual materials in place on physical surfaces in the real world. By
               virtualizing the experience of embodied interaction, Booksnake makes it possible for
               people who cannot otherwise visit an archive (due to cost, schedule, distance,
               disability, or other reasons) to physically engage with digitized materials.</p>

            <p>Booksnake represents a proof-of-concept, demonstrating that it is possible to
               automatically transform existing digital image files of cultural heritage materials
               into life-size virtual objects on demand. We have designed Booksnake to easily link
               with existing digitized collections via the International Image Interoperability
               Framework (or IIIF, pronounced <hi rend="quotes">triple-eye-eff</hi>), a
               widely-supported set of open Web protocols for digitally accessing archival materials
               and related metadata, developed and maintained by the GLAM community <ptr
                  target="#cramer_2011"/>, <ptr target="#cramer_2015"/>, <ptr
                  target="#snydman_sanderson_cramer_2015"/>. The first version of Booksnake uses
               IIIF to access Library of Congress digitized collections, including the Chronicling
               America collection of historical newspapers. Our goal is for Booksnake to be able to
               display any image file accessible through IIIF. At this point, the considerable
               variability in how different institutions record dimensional metadata means that we
               will have to customize Booksnake's links with each digitized collection, as we
               discuss further below. </p>

            <p>Our approach represents a novel use of AR for humanistic research, teaching, and
               learning. While humanists have begun to explore the potential of immersive
               technologies — primarily virtual reality (VR) and AR — they have largely approached
               these technologies as publication platforms: ways of hosting custom-built experiences
               containing visual, textual, spatial, and/or aural content that is manually created or
               curated by subject-matter experts. Examples include reconstructions of
               twelfth-century Angkor Wat <ptr target="#chandler_etal_2017"/>, a Renaissance
               studiolo <ptr target="#shemek_etal_2018"/>, events from the Underground Railroad <ptr
                  target="#roth_fisher_2019"/>, a medieval Irish castle <ptr target="#herron_2020"
               />, and an 18th-century Paris theatre <ptr target="#francois_etal_2021"/>. These are
               rich, immersive experiences, but they are typically limited to a specific site,
               narrative, or set of material. The work involved in manually creating these virtual
               models can be considerable, lengthy, and expensive.</p>

            <p>In contrast, Booksnake is designed as a general-purpose AR tool for archival
               exploration. Booksnake's central technical innovation is the automatic transformation
               of existing digital images of archival cultural heritage materials into
               dimensionally-accurate virtual objects. We aim to enable users to freely select
               materials from existing digitized cultural heritage collections for embodied
               interaction. Unlike custom-built immersive humanities projects, Booksnake makes it
               possible for a user to create virtual objects easily, quickly, and freely. While
               Booksnake fundamentally differs from existing Web-based image viewers, it is, like
               them, an empty frame, waiting for a user to fill it with something interesting. </p>

            <p>This article presents a conceptual and technical overview of Booksnake. We first
               critique the accepted method of viewing digitized archival materials in Web-based
               image viewers on flat screens and discuss the benefits of embodied interaction with
               archival materials, then describe results from initial user testing and potential use
               cases. Next, we contextualize Booksnake, as an AR app for mobile devices, within the
               broader landscape of immersive technologies for cultural heritage. We then detail the
               technical pipeline by which Booksnake transforms digitized archival materials into
               virtual objects for interaction in physical space. Ultimately, Booksnake's use of
               augmented reality demonstrates the potential of spatial interfaces and embodied
               interaction to improve accessibility to archival materials and activate digitized
               collections, while its presentation as a mobile app is an argument for the untapped
               potential of mobile devices to support humanities research, teaching, and
               learning.</p>

            <p>Booksnake is designed and built by a multidisciplinary team at the University of
               Southern California. It represents the combined efforts of humanities scholars,
               librarians, interactive media designers, and computer scientists — most of them
               students or early-career scholars — extending over four years.<note> Sean Fraga
                  researched and designed the project's technical architecture and user interface
                  affordances; Fraga also prepared the article manuscript. Christy Ye developed a
                  method for transforming digital images into virtual objects. Samir Ghosh ideated
                  and refined Booksnake's UI/UX fundamentals. Henry Huang built a means of
                  organizing and persistently storing item metadata and digital images using Apple’s
                  Core Data framework. Fraga developed and Huang implemented a method for creating
                  size-accurate virtual objects from Library of Congress metadata. Fraga, Ye, and
                  Huang jointly designed support for compound objects, and Ye and Huang collaborated
                  to implement support for compound objects; this work was supported by an NEH
                  Digital Humanities Advancement Grant (HAA-287859-22). Zack Sai is building support
                  for the new IIIF v3.0 APIs. Michael Hughes is expanding the range of interactions
                  possible with virtual objects in AR. Siyu (April) Yao is building technical links
                  to additional digitized collections. Additionally, Peter Mancall serves as senior
                  advisor to the project. Curtis Fletcher provides strategic guidance, and Mats
                  Borges provides guidance on user testing and feedback.</note> The project has been
               financially supported by the Humanities in a Digital World program (under grants from
               the Mellon Foundation) in USC's Dornsife College of Letters, Arts, and Sciences; and
               by the Ahmanson Lab, a scholarly innovation lab in the Sydney Harman Academy for
               Polymathic Studies in USC Libraries. The project is also funded by a Digital
               Humanities Advancement Grant (Level II) from the National Endowment for the
               Humanities (HAA-287859-22).</p>
         </div>


         <div>
            <head>FROM FLAT SCREENS TO EMBODIED EXPLORATION</head>

            <p>Flat-screen image viewers have long been hiding in plain sight. Each image viewer
               sits at the end of an institution's digitization pipeline and offer an interface
               through which users can access, view, and interact with digital image files of
               collection materials. These image viewers' fundamental characteristic is their
               transparency: The act of viewing a digital image of an archival item on a flat screen
               has become so common within contemporary humanities practices that we have
               unthinkingly naturalized it.</p>

            <p>But there is nothing <hi rend="quotes">natural</hi> about using flat-screen image
               viewers to apprehend archival materials, because everything about the digitization
               process is novel. The ability to instantly view a digitized archival item on a
               computer screen rests on technological developments from the last sixty years: the
               high-resolution digital cameras that capture archival material; the digital asset
               management software that organizes it; the cheap and redundant cloud storage that
               houses it; the high-speed networking infrastructure that delivers it; the Web
               browsers with which we access it; the high-resolution, full-color screens that
               display it. Even the concept of a graphical user interface, with its representative
               icons and mouse-based input, is a modern development, first publicly demonstrated by
               Douglas Engelbart in 1968 and first commercialized by Apple, with the Lisa and the
               Macintosh, in the early 1980s <ptr target="#fisher_2018" loc="17–26, 85–93, 104–117"
               />. Put another way, our now-familiar ways of using a mouse or trackpad to interact
               with digitized archival materials in a Web-based flat-screen image viewer would be
               incomprehensibly alien to the people who originally created and used many of these
               materials — and, in many cases, even to the curators, librarians, and archivists who
               first acquired and accessioned these items. What does this mean for our relationship
               with these archival materials? </p>

            <p> Despite this, and although Web-based flat-screen image viewers sustain a robust
               technical development community, they have been largely overlooked by most digital
               humanists. A notable exception is manuscript scholars, for whom the relationship
               between text, object, and digital image is particularly important. Indeed, manuscript
               scholars have led the charge in identifying flat-screen image viewers as sites of
               knowledge creation and interpretation — often by expressing their frustration with
               these viewers' limited affordances or with the contextual information these viewers
               shear away <ptr target="#nolan_2013"/>, <ptr target="#szpiech_2014"/>, <ptr
                  target="#kropf_2017"/>, <ptr target="#almas_etal_2018"/>, <ptr
                  target="#porter_2018"/>, <ptr target="#van_zundert_2018"/>, <ptr
                  target="#van_lit_2020"/>.</p>

            <p> To see flat-screen image viewers more clearly, it helps to understand them within
               the context of archival digitization practices. Digitization is usually understood as
               the straightforward conversion of physical objects into digital files, but this
               process is never simple and always involves multiple interpretive decisions.<note> We
                  focus here on the decisions involved in digitizing an item itself, rather than the
                  decisions about what items get digitized and why, which also, of course, inform
                  the development and structure of a given digitized collection.</note> As Johanna
               Drucker writes, <quote rend="inline">the way artifacts are [digitally] encoded
                  depends on the parameters set for scanning and photography. These already embody
                  interpretation, since the resolution of an image, the conditions of lighting under
                  which it is produced, and other factors, will alter the outcome</quote>
               <ptr target="#drucker_2013" loc="8"/>. Some of these encoding decisions are
               structured by digitization guidelines, such as the U.S. Federal Agency Digitization
               Guidelines Initiative (FADGI) standards (2023), while other decisions, such as how to
               light an item or how to color-correct a digital image, depend on the individual
               training and judgment of digitization professionals.</p>

            <p> A key digitization convention is to render an archival item from an idealized
               perspective, that of an observer perfectly centered before the item. To achieve this,
               a photographer typically places the item being digitized perpendicular to the
               camera's optical axis, centers the item within the camera's view, and orthogonally
               aligns the item with the viewfinder's edges. During post-processing, a digitization
               professional can then crop, straighten, and de-skew the resulting image, or stitch
               together multiple images of a given item into a single cohesive whole. These physical
               and digital activities produce an observer-independent interpretation of an archival
               item. Put another way, archival digitization is what Donna Haraway calls a god trick,
               the act of <quote rend="inline">seeing everything from nowhere</quote>
               <ptr target="#haraway_1988" loc="582"/>.</p>

            <p> Taking these many decisions together, Drucker argues that <quote rend="inline"
                  style="italic">digitization is not representation but interpretation</quote>
               <ptr target="#drucker_2013" loc="12"/> (emphasis original). Understanding
               digitization as a continuous interpretive process, rather than a simple act of
               representation, helps us see how this process extends past the production of digital
               image files and into how these files are presented to human viewers into traditional
               flat-screen image viewers. </p>

            <p> Flat-screen image viewers encode a set of decisions about how we can (or should)
               interact with a digitized item. Just as decisions about resolution, lighting, and
               file types serve to construct a digitized interpretation of a physical object, so too
               do decisions about interface affordances for an image viewer serve to construct an
               interpretive space. Following Drucker, image viewers do not simply <hi rend="italic"
                  >represent </hi>digital image files to a user, they <hi rend="italic"
                  >interpret</hi> them. Decisions by designers and developers about an image
               viewer’s interface affordances (how to zoom, turn pages, create annotations, etc.)
               structure the conditions of possibility for a user's interactions with a digitized
               item. </p>

            <p> The implications of these decisions are particularly visible when comparing
               different image viewers. For example, Mirador, a leading IIIF-based image viewer,
               enables users to compare two images from different repositories side-by-side <ptr
                  target="#project_mirador_nd"/>. A different IIIF-based image viewer,
               OpenSeadragon, is instead optimized for viewing individual <quote rend="inline"
                  >high-resolution zoomable images</quote>
               <ptr target="#openseadragon_nd"/>. Mirador encourages juxtaposition, while
               OpenSeadragon emphasizes attention to detail. These two viewers each represent a
               particular set of assumptions, goals, decisions, and compromises, which in turn shape
               how their respective users encounter and read a given item, the interpretations those
               users form, and the knowledge they create. </p>

            <p> Flat-screen image viewers generally share three attributes that collectively
               structure a user's interactions with a digitized item. First, and most importantly,
               flat-screen image viewers directly reproduce the idealized <quote rend="inline">view
                  from nowhere</quote> delivered by the digitization pipeline. Flat-screen image
               viewers could present digital images in any number of ways — upside down, canted at
               an angle away from the viewer, obscured by a digital curtain. But instead,
               flat-screen image viewers play the god trick. Second, flat-screen image viewers rely
               on indirect manipulation via a mouse or trackpad. To zoom, pan, rotate, or otherwise
               navigate a digitized item, a user must repeatedly click buttons or click and drag,
               positioning and re-positioning the digital image in order to apprehend its content.
               These interaction methods create friction between the user and the digitized item,
               impeding discovery <ptr target="#sundar_etal_2013"/>. Finally, flat-screen image
               viewers arbitrarily scale digital images to fit a user’s computer screen. <quote
                  rend="inline">Digitisation doesn't make everything equal, it just makes everything
                  the same size,</quote> writes <ptr target="#crane_2021"/>. In a flat-screen image
               viewer, a monumental painting and its postcard reproduction appear to be the same
               size, giving digitized materials a false homogeneity and disregarding the contextual
               information conveyed by an item's physical dimensions. In sum, flat-screen image
               viewers are observer-independent interfaces for indirect manipulation of arbitrarily
               scaled digitized materials. </p>

            <p> In contrast, Booksnake is an observer-dependent interface for embodied interaction
               with life-size digitized materials. When we encounter physical items, we do so
               through our bodies, from our individual point of view. Humans are not simply <quote
                  rend="inline">two eyeballs attached by stalks to a brain computer,</quote> as
               Catherine D'Ignazio and Lauren Klein write in their discussion of data
               visceralization <ptr target="#dignazio_klein_2020" loc="§3"/>. We strain toward the
               far corners of maps. We pivot back and forth between the pages of newspapers. We curl
               ourselves over small objects like daguerreotypes, postcards, or brochures. These
               embodied, situated, perspectival experiences are inherent to our interactions with
               physical objects. By using augmented reality to pin life-size digitized items to
               physical surfaces, Booksnake enables and encourages this kind of embodied
               exploration. With Booksnake, you can move around an item to see it from all sides,
               step back to see it in its totality, or get in close to focus on fine details.
               Integral to Booksnake is Haraway's idea of <quote rend="inline">the particularity and embodiment of all
               vision</quote> <ptr target="#haraway_1988" loc="582"/>. Put another way, Booksnake lets you break out of the god view
               and see an object as only you can. As an image viewer with a spatial interface,
               Booksnake is an argument for a way of seeing that prioritizes embodied interaction
               with digitized archival materials at real-world size. In this, Booksnake is more than
               a technical critique of existing flat-screen image viewers. It is also an <hi
                  rend="italic">intellectual</hi> critique of how these image viewers foreclose
               certain types of knowledge creation and interpretation. Booksnake thus offers <hi
                  rend="italic">a new way of looking</hi> at digitized materials.</p>

            <p> This is an interpretive choice in our design of Booksnake. Again, image viewers do
               not simply <hi rend="italic">represent </hi>digital image files to a user, they <hi
                  rend="italic">interpret</hi> them. Booksnake relies on the same digital image
               files as do flat-screen image viewers, and these files are just as mediated in
               Booksnake as they are when viewed in a flat-screen viewer. (<quote rend="inline">There is no unmediated
                  photograph,</quote> " Haraway writes <ptr target="#haraway_1988" loc="583"/>. Indeed, Booksnake even displays these
               image files on the flat screen of a smartphone or tablet — but our use of AR creates
               the illusion that the object is physically present. Where existing flat-screen image
               viewers foreground the digital-ness of digitized objects, Booksnake instead recovers
               and foregrounds their object-ness, their materiality. Drucker writes that
               <quote rend="inline">information spaces drawn from a point of view, rather than as if they were observer
                  independent, reinsert the subjective standpoint of their creation</quote> <ptr target="#drucker_2011" loc="¶20"/>.
               Drucker was writing about data visualization, but her point holds for image viewers
               (which, after all, represent a kind of data visualization). Our design decisions aim
               to create an interpretive space grounded in individual perspective, with the goal of
               helping a Booksnake user get closer to the <quote rend="inline">subjective standpoint</quote> of an archival
               item's original creators and users. Even as the technology underpinning Booksnake is
               radically new, it enables methods of embodied looking that are very old, closely
               resembling in form and substance physical, pre-digitized ways of looking. </p>
         </div>



         <div>

            <head>BOOKSNAKE USE CASES</head>

            <p> Booksnake's emphasis on embodied interaction gives it particular potential as a tool
               for humanities education. Embodied interaction is a means of accessing situated
               knowledges <ptr target="#haraway_1988"/> and is key to apprehending cultural heritage materials in
               their full complexity. As museum scholars and educators have demonstrated, this is
               true both for physical objects <ptr target="#kai-kee_latina_sadoyan_2020"/> and for virtual
               replicas <ptr target="#kenderdine_yip_2019"/>. Meanwhile, systematic reviews show AR can support
               student learning gains, motivation, and knowledge transfer <ptr target="#bacca_etal_2014"/> (). By
               using AR to make embodied interaction possible with digitized items, Booksnake
               supports student learning through movement, perspective, and scale. For example, a
               student could use Booksnake to physically follow an explorer's track across a map,
               watch a painting’s details emerge as she moves closer, or investigate the
               relationship between a poster’s size and its message — interactions impossible with a
               flat-screen viewer. Here, we briefly highlight themes that have emerged in user and
               classroom testing, then discuss potential use cases.</p>

            <p> A common theme in classroom testing was that Booksnake's presentation of life-size
               objects made students feel like they were closer to digitized sources. A colleague's
               students used Booksnake as part of an in-class exercise to explore colonial-era
               Mesoamerican maps and codices, searching for examples of cultural syncretism. In a
               post-activity survey, students repeatedly described a feeling of presence. Booksnake
               gave one student <quote rend="inline">the feeling of actually seeing the real thing up close.</quote> Another
               student wrote that <quote rend="inline">I felt that I was actually working with the codex.</quote> A third wrote
               that <quote rend="inline">it was cool to see the resource, something I will probably never get to flip
               through, and get to flip through and examine it.</quote> Students also commented on how
               Booksnake represented these items' size. One student wrote that Booksnake <quote rend="inline">gave me
               the opportunity to get a better idea of the scale of the pieces.</quote> Another wrote that
               <quote rend="inline">I liked being able to see the material <q>physically</q> and see the scale of the
               drawings on the page to see what they emphasized and how they took up space.</quote> These
               comments suggest that Booksnake has the most potential to support embodied learning
               activities that ask students to engage with an item's physical features (such as an
               object's size or proportions), or with the relationship between these features and
               the item's textual and visual content.</p>

            <p> During one-on-one user testing sessions, two history Ph.D. students each similarly
               described feeling closer to digitized sources. One tester described Booksnake as
               opening <quote rend="inline">a middle ground</quote> between digital and physical documents by offering the
               <quote rend="inline">flexibility</quote> of digital materials, but the "sensorial experience of closeness" with
               archival documents. The other tester said that Booksnake brought <quote rend="inline">emotional value</quote> to
               archival materials. <quote rend="inline">Being able to stand up and lean over it [the object] brought it
               to life a little more</quote>, this tester said. <quote rend="inline">You can’t assign research utility to that,
               but it was more immersive and interactive, and in a way satisfying.</quote> Their comments
               suggest that Booksnake can enrich engagement with digitized materials, especially by
               producing the feeling of physical presence.</p>

            <p> As a virtualization technology, Booksnake makes it possible to present archival
               materials in new contexts, beyond the physical site of the archive itself. Jeremy
               Bailenson argues that virtualization technologies are especially effective for
               scenarios that would otherwise be rare, impractical, destructive, or expensive <ptr target="#bailenson_2018"/>. Here, we use these four characteristics to describe potential Booksnake use
               cases. First, it is rare to have an individual, embodied interaction with
               one-of-a-kind materials, especially for people who do not work at GLAM institutions.
               A key theme in student comments from the classroom testing described above, for
               example, was that Booksnake gave students a feeling of direct engagement with unique
               Mesoamerican codices. Second, it is often logistically impractical for a class to
               travel to an archive (especially for larger classes) or to bring archival materials
               into classrooms outside of library or museum buildings. The class described above,
               for example, had around eighty students, and Booksnake made it possible for each
               student to individually interact with these codices, during scheduled class time and
               in their existing classrooms. Third, the physical fragility and the rarity or
               uniqueness of many archival materials typically limits who can handle them or where
               they can be examined. Booksnake makes it possible to engage with virtual archival
               materials in ways that would cause damage to physical originals. For example,
               third-grade students could use Booksnake to explore a historic painting by walking
               atop a virtual copy placed on their classroom floor, or architectural historians
               could use Booksnake to bring virtual archival blueprints into a physical site.
               Finally, it is expensive (in both money and time) to physically bring together
               archival materials that are held by two different institutions. With Booksnake, a
               researcher could juxtapose digitized items held by one institution with physical
               items held by a different institution, for purposes of comparison (such as comparing
               a preparatory sketch to a finished painting) or reconstruction (such as reuniting
               manuscript pages that had been separated). In each of these scenarios, Booksnake's
               ability to produce virtual replicas of physical objects lowers barriers to embodied
               engagement with archival materials and opens new possibilities for research,
               teaching, and learning. </p>
         </div>


         <div>

            <head>IMMERSIVE TECHNOLOGIES FOR CULTURAL HERITAGE</head>

            <p> Booksnake is an empty frame. It leverages AR to extend the exploratory freedom that
               users associate with browsing online collections into physical space. In doing so,
               Booksnake joins a small collection of digital tools and projects using immersive
               technologies as the basis for interacting with cultural heritage materials and
               collections.</p>

            <p> A dedicated and technically adept user could use 3D modeling software (such as
               Blender, Unity, Maya, or Reality Composer) to manually transform digital images into
               virtual objects. This can be done with any digital image, but is time-intensive and
               breaks links between images and metadata. Booksnake automates this process, making it
               more widely accessible, and preserves metadata, supporting humanistic inquiry.</p>

            <p> The Google Arts &amp; Culture app offers the most comparable use of humanistic AR.
               Like Booksnake, Arts &amp; Culture is a mobile app for smartphones and tablets that
               offers AR as an interface for digitized materials. A user can activate the app’s <quote rend="inline">Art
               Projector</quote> feature to <quote rend="inline">display life-size artworks, wherever you are</quote> by placing a
               digitized artwork in their physical surroundings <ptr target="#luo_2019"/>. But there are three key
               differences between Google’s app and Booksnake. First, AR is one of many possible
               interaction methods in Google’s app, which is crowded with stories, exhibits, videos,
               games, and interactive experiences. In contrast, Booksnake emphasizes AR as its
               primary interface, foregrounding the embodied experience. Second, Google’s app
               focuses on visual art (such as paintings and photographs), while Booksnake can
               display a broader range of archival and cultural heritage materials, making it more
               useful for humanities scholars. (Booksnake can also display paginated materials, as
               we discuss further below, while Google's app cannot.) Finally — and most importantly
               — Google’s app relies on a centralized database model. Google requires participating
               institutions to upload their collection images and metadata to Google’s own servers,
               so that Google can format and serve these materials to users <ptr target="#google_nd"/>. In
               contrast, Booksnake’s use of IIIF enables institutions to retain control over their
               digitized collections and expands the capabilities of an open humanities software
               ecosystem.</p>

            <p>Another set of projects approach immersive technologies as tools for designing and
               delivering exhibition content. Some use immersive technologies to enhance physical
               exhibits, such as Veholder, a project exploring technologies and methods for
               juxtaposing 3D virtual and physical objects in museum settings <ptr target="#haynes_2018"/>, <ptr target="#haynes_2019"/>. Others are tools for using immersive technologies to create entirely virtual
               spaces. Before its discontinuation, many GLAM institutions and artists adapted
               Mozilla Hubs, a general-purpose tool for building 3D virtual spaces that could be
               accessed using a flat-screen Web browser or a VR headset, to build virtual exhibition
               spaces, although users were required to manually import digitized materials and
               construct virtual replicas <ptr target="#cool_2022"/>. Another project, Diomira Galleries, is a
               prototype tool for building VR exhibition spaces with IIIF-compliant resources
               <ptr target="#bifrost_2023"/>. Like Booksnake, Diomira uses IIIF to import digital
               images of archival materials, but Diomira arbitrarily scales these images onto
               template canvases that do not correspond to an item’s physical dimensions. As with
               Booksnake, these projects demonstrate the potential of immersive technologies for new
               research interactions and collection activation with digitized archival
               materials.</p>

            <p>Finally, we are building Booksnake as an AR application for existing consumer mobile
               devices as a way of lowering barriers to immersive engagement with cultural heritage
               materials. Most VR projects require expensive special-purpose headsets, which has
               limited adoption and access <ptr target="#greengard_2019"/>. In contrast, AR-capable smartphones are
               ubiquitous, enabling a mobile app to tap the potential of a large existing user base,
               and positioning such an app to potentially mitigate racial and socioeconomic digital
               divides in the United States. More Americans own smartphones than own laptop or
               desktop computers <ptr target="#pew_2021"/>. And while Black and Hispanic adults in the United
               States are less likely to own a laptop or desktop computer than white adults, Pew
               researchers have found <quote rend="inline">no statistically significant racial and ethnic differences
                  when it comes to smartphone or tablet ownership</quote> <ptr target="#atske_perrin_2021"/>. Similarly,
               Americans with lower household incomes are more likely to rely on smartphones for
               Internet access <ptr target="#vogels_2021"/>. Smartphones are thus a key digital platform for
               engaging and including the largest and most diverse audience. Developing an Android
               version of Booksnake will enable us to more fully deliver on this potential. </p>
         </div>



         <div>

            <head>AUTOMATICALLY TRANSFORMING DIGITAL IMAGES INTO VIRTUAL OBJECTS</head>

            <p>Booksnake’s central technical innovation is automatically transforming existing
               digital images of archival cultural heritage materials into dimensionally-accurate
               virtual objects. To make this possible, Booksnake connects existing software
               frameworks in a new way. </p>

            <p>First, Booksnake uses IIIF to download images and metadata.<note> The first version
                  of Booksnake supports the IIIF v2.1 APIs. We are currently building support for
                  the new IIIF v3.0 APIs.</note> IIIF was proposed in 2011 and developed over the
               early 2010s. Today, the IIIF Consortium is composed of sixty-five global GLAM
               institutions, from the British Library to Yale University <ptr target="#iiif_nd"/>, while dozens
               more institutions offer access to their collections through IIIF because several
               common digital asset management (DAM) platforms, including CONTENTdm, LUNA, and
               Orange DAM, support IIIF <ptr target="#oclc_nd"/> <ptr target="#l_i_2018"/> <ptr target="#orangelogic_nd"/>.
               This widespread use of IIIF means that Booksnake is readily compatible with many
               existing digitized collections. By using IIIF, Booksnake embraces and extends the
               capabilities of a robust humanities software ecosystem. By demonstrating a novel
               method to transform existing IIIF-compliant resources for interaction in augmented
               reality, we hope that Booksnake will drive wider IIIF adoption and standardization. </p>

            <p>Next, Booksnake uses a pair of Apple software frameworks, ARKit and RealityKit.
               ARKit, introduced in 2017, interprets and synthesizes data from an iPhone or iPad's
               cameras and sensors to understand a user's physical surroundings and to anchor
               virtual objects to horizontal and vertical surfaces <ptr target="#apple_nd_a"/>. RealityKit,
               introduced in 2019, is a framework for rendering and displaying virtual objects, as
               well as managing a user's interactions with them (for example, by interpreting a
               user’s on-screen touch gestures) <ptr target="#apple_nd_b"/>. Both ARKit and RealityKit are
               built into the device operating system, enabling us to rely on these frameworks to
               create virtual objects and to initiate and manage AR sessions. </p>

            <p>Developing Booksnake as a native mobile app, rather than a Web-based tool, makes it
               possible for Booksnake to take advantage of the powerful camera and sensor
               technologies in mobile devices. We are developing Booksnake’s first version for
               iPhone and iPad because Apple’s tight integration of hardware and software supports
               rapid AR development and ensures consistency in user experience across devices. We
               plan to extend our work by next developing an Android version of Booksnake, improving
               accessibility. Another development approach, WebXR, a cross-platform Web-based AR/VR
               framework currently in development, lacks the features to support our project
               goals.</p>

            <p>Booksnake thus links IIIF with RealityKit and ARKit to produce a novel result: an
               on-demand pipeline for automatically transforming existing digital images of archival
               materials into custom virtual objects that replicate the physical original’s
               real-world proportions, dimensions, and appearance, as well as an AR interface for
               interacting with these virtual objects in physical space. (See figure 3.) How does
               Booksnake do this?</p>

            <p>Booksnake starts with the most humble of Internet components: the URL. A Booksnake
               user first searches and browses an institution’s online catalog through an in-app Web
               view. Booksnake offers an <q>Add</q> button on catalog pages for individual items. When
               the user taps this button to add an item to their Booksnake library, Booksnake
               retrieves the item page’s URL. Because of Apple’s privacy restrictions and
               application sandboxing, this is the only information that Booksnake can read from a
               given Web page; it cannot directly access content on the page itself. Instead,
               Booksnake translates the item page URL into the corresponding IIIF manifest URL.</p>

            <p>An IIIF manifest is a JSON file — a highly structured, computer-readable text file —
               that contains a version of the item’s catalog record, including metadata and URLs for
               associated images. The exact URL translation process varies depending on how an
               institution has implemented IIIF, but in many cases it is as simple as appending
               <quote rend="inline">/manifest.json</quote> to the item URL. For example, the item URL for the Library of
               Congress’s 1858 <quote rend="inline">Chart of the submarine Atlantic Telegraph,</quote> is <ref
                  target="https://www.loc.gov/item/2013593216/"
                  >https://www.loc.gov/item/2013593216/</ref>, and the item’s IIIF manifest URL is
                  <ref target="https://www.loc.gov/item/2013593216/manifest.json"
                  >https://www.loc.gov/item/2013593216/manifest.json</ref>. In other cases,
               Booksnake may extract a unique item identifier from the item URL, then use that
               unique identifier to construct the appropriate IIIF manifest URL. Booksnake then
               downloads and parses the item’s IIIF manifest.</p>

            <p>First, Booksnake extracts item metadata from the IIIF manifest. Booksnake uses this
               metadata to construct an item page in the app’s Library tab, enabling a user to view
               much of the same item-level metadata visible in the host institution’s online
               catalog. An IIIF manifest presents metadata in key-value pairs, with each pair
               containing a general label (or key) and a corresponding entry (or value). For
               example, the IIIF manifest for the 1858 Atlantic telegraph map mentioned above
               contains the key <q>Contributors</q>, representing the catalog-level field listing an
               item’s authors or creators, and the corresponding item-level value <quote rend="inline">Barker, Wm. J.
               (William J.) (Surveyor),</quote> identifying the creator of this specific item. Importantly,
               while the key-value pair structure is generally consistent across IIIF manifests from
               different institutions, the key names themselves are not. The "Contributors" key at
               one institution may be named <q>Creators</q> at another institution, and <q>Authors</q> at a
               third. The current version of Booksnake simply displays the key-value metadata as
               provided in the IIIF manifest. As Booksnake adds support for additional institutions,
               we plan to identify and link different keys representing the same metadata categories
               (such as <q>Contributors,</q> <q>Creators,</q> and <q>Authors</q>). This will enable users, for
               example, to sort items from different institutions by common categories like <q>Author</q>
               or <q>Date created,</q> or to search within a common category.</p>

            <p>Second, Booksnake uses image URLs contained in the IIIF manifest to download the
               digital images (typically JPEG or JPEG 2000 files) associated with an item's catalog
               record. Helpfully, IIIF image URLs are structured so that certain requests — like the
               image’s size, its rotation, even whether it should be displayed in color or
               black-and-white — can be encoded in the URL itself. Booksnake leverages this
               affordance to request images that are sufficiently detailed for virtual object
               creation, which sometimes means requesting images larger than what the institution
               serves by default. For example, Library of Congress typically serves images that are
               25% of the original size, but Booksnake modifies the IIIF image URL to request images
               at 50% of original size. Our testing indicates that this level of resolution produces
               sufficiently detailed virtual objects, without visible pixelation, for Library of
               Congress collections.<note> To avoid pixelated virtual objects, the host institution
                  must serve through IIIF a sufficiently high-resolution digital image.</note> We
               anticipate customizing this value for other institutions. Having downloaded the
               item’s metadata and digital images, Booksnake can now create a virtual object
               replicating the physical original. </p>

            <p>Our initial goal was for Booksnake to display any image resource available through
               IIIF, but we quickly discovered that differences in how institutions record
               dimensional metadata meant that we would have to adapt Booksnake to different
               digitized collections. To produce size-accurate virtual objects, Booksnake requires
                  <hi rend="italic">either</hi> an item’s physical dimensions in a computer-readable
               format, or <hi rend="italic">both </hi>the item’s pixel dimensions and its
               digitization resolution, from which we can calculate the item's physical dimensions
               (see figures 4a and 4b). Institutions generally take one of three approaches to
               providing dimensional metadata.</p>

            <p>The simplest approach is to list an item’s physical dimensions as metadata in its
               IIIF manifest. Some archives, such as the David Rumsey Map Collection, provide
               separate fields for an item’s height and width, each labeled with units of measure.
               This formatting provides the item’s dimensions in a computer-readable format, making
               it straightforward to create a virtual object of the appropriate size. Alternatively,
               an institution may use the IIIF Physical Dimension service, an optional service that
               provides the scale relationship between an item’s physical and pixel dimensions,
               along with the physical units of measure <ptr target="#iiif_2015"/>. But we are unaware of any institution that has implemented this
               service for its collections.<note> David Newberry's <quote>IIIF for Dolls</quote> web tool makes
                  inventive use of the IIIF Physical Dimension service to rescale digitized
                  materials <ptr target="#newberry_2023"/>.</note>
            </p>

            <p>A more common approach is to provide an item’s physical dimensions in a format that
               is not immediately computer-readable. The Huntington Digital Library, for example,
               typically lists an item’s dimensions as part of a text string in the <q>physical
               description</q> field. <ref
                  target="https://hdl.huntington.org/digital/collection/p9539coll1/id/12262/rec/2"
                  >This c.1921 steamship poster</ref>, for example, is described as: "<quote rend="inline">Print ; image
               60.4 x 55 cm (23 3/4 x 21 5/8 in.) ; overall 93.2 x 61 cm (36 11/16 x 24 in.)</quote>"
               [spacing <hi rend="italic">sic</hi>]. To interpret this text string and convert it
               into numerical dimensions, a computer program like Booksnake requires additional
               guidance. Which set of dimensions to use, <q>image</q> or <q>overall</q>? Which units of
               measure, centimeters or inches? And what if the string includes additional
               descriptors, such as <q>folded</q> or <q>unframed</q>? We are currently collaborating with the
               Huntington to develop methods to parse dimensional metadata from textual
               descriptions, with extensibility to other institutions and collections.</p>

            <p>Finally, an institution may not provide <hi rend="italic">any</hi> dimensional
               metadata in its IIIF manifests. This is the case with the Library of Congress (LOC),
               which lists physical dimensions, where available, in an item’s catalog record, but
               does not provide this information in the item’s IIIF manifest.<note> Library of
                  Congress uses MARC data to construct IIIF manifests, but this is sometimes a
                  <q>lossy</q> process <ptr target="#woodward_2021"/>.</note> This presented us with a problem: How to
               create dimensionally-accurate virtual objects without the item’s physical dimensions?
               After much research and troubleshooting, we hit upon a solution. We initially
               dismissed pixel dimensions as a source of dimensional metadata because there is no
               consistent relationship between physical and pixel dimensions. And yet, during
               early-stage testing, Booksnake consistently created life-size virtual maps from LOC,
               even as items in other LOC collections resulted in virtual objects with wildly
               incorrect sizing. This meant there <hi rend="italic">was</hi> a relationship, at
               least for one collection — we just had to find it. </p>

            <p>LOC digitizes items to FADGI standards, which specify a digitization resolution for
               different item types. For example, FADGI standards specify a target resolution of 600
               ppi for prints and photographs, and 400 ppi for books.<note> Both ppi requirements
                  are for FADGI’s highest performance level, 4 Star.</note> We then discovered that
               LOC scales down item images in some collections. (For example, map images are scaled
               down by a factor of four.) We then combined digitization resolution and scaling
               factors for each item type into a pre-coded reference table. When Booksnake imports
               an LOC item, it consults the item’s IIIF manifest to determine the item type, then
               consults the reference table to determine the appropriate factor for converting the
               item’s pixel dimensions to physical dimensions. This solution is similar to a
               client-side version of the IIIF Physical Dimension service, customized to LOC’s
               digital collections. </p>

            <p>As this discussion suggests, determining the physical dimensions of a digitized item
               is a seemingly simple problem that can quickly become complicated. Developing robust
               methods for parsing different types of dimensional metadata is a key research area
               because these methods will allow us to expand the range of institutions and materials
               with which Booksnake is compatible. While IIIF makes it straightforward to access
               digitized materials held by different institutions, the differences in how each
               institution presents dimensional metadata mean that we will currently have to adapt
               Booksnake to use each institution's metadata schema.<note> More information about
                  Booksnake's metadata requirements is available at <ref
                     target="https://booksnake.app/glam/">https://booksnake.app/glam/</ref>.</note>
            </p>

            <p>Booksnake then uses this information to create virtual objects on demand. When a user
               taps <q>View in Your Space</q> to initiate an AR session, Booksnake uses RealityKit to
               transform the item’s digital image into a custom virtual object suitable for
               interaction in physical space. First, Booksnake creates a blank two-dimensional
               virtual plane sized to match the item’s physical dimensions. Next, Booksnake applies
               the downloaded image to this virtual plane as a texture, scaling the image to match
               the size of the plane. This results in a custom virtual object that matches the
               original item’s physical dimensions, proportions, and appearance. This process is
               invisible to the user — Booksnake <q>just works.</q> This process is straightforward for
               flat digitized items like maps or posters, which are typically digitized from a
               single perspective, of their recto (front) side.</p>

            <p>Booksnake’s virtual object creation process is more complex for compound objects,
               which have multiple images linked to a single item record. Compound objects can
               include books, issues of periodicals or newspapers, diaries, scrapbooks, photo
               albums, and postcards. The simplest compound objects, such as postcards, have two
               images showing the object’s recto (front) and verso (back) sides. Nineteenth-century
               newspapers may have four or eight pages, while books may run into hundreds of pages,
               with each page typically captured and stored as a single image. </p>

            <p>Booksnake handles compound objects by creating multiple individual virtual objects,
               one for each item image, then arranging and animating these objects to support the
               illusion of a cohesive compound object. Our initial implementation is modeled on a
               generic paginated codex, with multiple pages around a central spine. As with flat
               objects, this creation process happens on demand, when a user starts an AR session.
               Booksnake uses a compound object’s first image as the virtual object’s cover or first
               page (more on this below). Booksnake creates a virtual object for this first image,
               then creates a matching invisible virtual object, which acts as an invisible <q>page
               zero</q> (see figure 5). The user sees the first page of a newspaper, for example,
               sitting and waiting to be opened. When the user swipes from right to left across the
               object edge to "turn" the virtual page, Booksnake retrieves the next two images,
               transforms them into virtual objects representing pages two and three, then animates
               a page turn with the object’s <q>spine</q> serving as the rotation axis (see figure 6).
               Once the animation is complete, pages two and three have replaced the invisible <q>page
               zero</q> and page one, and Booksnake discards those virtual objects (see figure 7). This
               on-demand process means that Booksnake only needs to load a maximum of four images
               into AR space, optimizing for memory and performance. By using a swipe gesture to
               turn pages, Booksnake leverages a navigation affordance with which users are already
               familiar from interactions with physical paginated items, supporting immersion and
               engagement. The page-turn animation, paired with a page-turn sound effect, further
               enhances the realism of the virtual experience.</p>

            <p>A key limitation in our initial approach to creating virtual paginated objects is
               that our generic codex model is based on one particular type of object, the
               newspaper. Specifically, we used LOC's Chronicling America collection of historical
               newspapers as a testbed to develop the pipeline for creating virtual paginated
               objects, as well as the user interface and methods for interacting with virtual
               paginated objects in physical space. While the newspaper is broadly representative of
               the codex form's physical features and affordances, making it readily extensible to
               other paginated media, there are important differences in how different media are
               digitized and presented in online collections. For example, while Chronicling America
               newspapers have one page per image, some digitized books held by LOC have two pages
               per image. We have adapted Booksnake's object creation pipeline to identify
               double-page images, split the image in half, and wrap each resulting image onto
               individual facing pages.<note> Another example: For some digitized books held by LOC,
                  the first image is of the book spine, not the book cover. Booksnake incorporates
                  logic to identify and ignore book spine images.</note> There are also important
               cultural differences in codex interaction methods: We plan to further extend the
               capabilities of this model by building support for IIIF's "right-to-left" reading
               direction flag, which will enable Booksnake to correctly display paginated materials
               in languages like Arabic, Chinese, Hebrew, and Japanese.</p>

            <p>A further limitation of our initial approach is our assumption that all compound
               objects represent paginated material in codex form, which is not the case. Booksnake
               cannot yet realistically display items like scrolls or Mesoamerican screenfold books
               (which are often digitized by page, but differ in physical construction and
               interaction methods from Western books). In other cases, a compound object comprises
               a collection of individual items that are stored together but not physically bound to
               each other. One example is <ref target="https://www.loc.gov/item/pldec.110/"><quote rend="inline">Polish
                  Declarations of Admiration and Friendship for the United States</quote></ref>, held by
               LOC, which consists of 181 unbound sheets. Or an institution may use a compound
               object to store multiple different images of a single object. For example, the Yale
               Center for British Art (YCBA) often provides several images for paintings in its
               collections, as with<ref
                  target="https://collections.britishart.yale.edu/catalog/tms:52309"> the 1769
                  George Stubbs painting <quote>Water Spaniel</quote>.</ref> YCBA provides four images: of the
               framed painting, the unframed painting, the unframed image cropped to show just the
               canvas, and the framed painting’s verso side. We plan to continue refining
               Booksnake’s compound object support by using collection- and item-level metadata to
               differentiate between paginated and non-paginated compound objects, in order to
               realistically display different types of materials.</p>

            <p>Having constructed a custom virtual object, Booksnake next opens the live camera view
               so that the user can interact with the object in physical space. As the user moves
               their device to scan their surroundings, Booksnake overlays a transparent image of
               the object, outlined in red, over horizontal and vertical surfaces, giving the user a
               preview of how the object will fit in their space. Tapping the transparent preview
               anchors the object, which turns opaque, to a physical surface. As the user and device
               move, Booksnake uses camera and sensor data to maintain the object's relative
               location in physical space, opening the object to embodied exploration. The user can
               crouch down and peer at the object from the edge of the table, pan across the
               object's surface, or zoom into the key details by moving closer to the object. The
               user can also enlarge or shrink an object by using the familiar pinch-to-zoom
               gesture—although in this case, the user is not zooming in or out, but re-scaling the
               virtual object itself. The object remains in place even when out of view of the
               device: A user can walk away from an anchored object, then turn back to look at it
               from across the room. </p>

            <p>Again, an object viewed with Booksnake is just as mediated as one viewed in a
               traditional Web-based viewer. Throughout the development process, we have repeatedly
               faced interpretive questions at the intersection of technology and the humanities.
               One early question, for example: Should users be allowed to re-scale objects? The
               earliest versions of Booksnake lacked this affordance, to emphasize that a given
               virtual object was presented at life size. But one of our advisory board members,
               Philip J. Ethington, argued that digital technology is powerful because it can enable
               interactions that aren't possible in the real world. And so we built a way for users
               to re-scale objects by pinching them, while also displaying a changing percentage
               indicator to show users how much larger or smaller the virtual object is when
               compared to the original. In approaching this and similar questions, our goal is for
               virtual objects to behave in realistic and familiar ways, to closely mimic the
               experience of embodied interaction with physical materials.</p>
         </div>


         <div>

            <head>CONCLUSION</head>

            <p><quote rend="inline">What kind of interface exists after the screen goes away?</quote> asks Johanna Drucker. <quote rend="inline">I
               touch the surface of my desk and it opens to the library of the world? My walls are
               display points, capable of offering the inventory of masterworks from the world’s
               museums and collections into view?</quote> <ptr target="#drucker_2014" loc="195"/>. These questions point toward
               spatial interfaces, and this is what Booksnake is building toward. Futurists and
               science-fiction authors have long positioned virtual reality as a means of
               transporting users away from their day-to-day humdrum reality into persistent
               interactive three-dimensional virtual worlds, sometimes called the metaverse <ptr target="#gibson_1984"/>, <ptr target="#stephenson_1992"/>, <ptr target="#cline_2011"/>, <ptr target="#ball_2022"/>. Booksnake takes a different
               approach, bringing life-size virtual representations of real objects into a user's
               physical surroundings for embodied interaction. The recent emergence of mixed-reality
               headsets, such as the Apple Vision Pro, only underscores the potential of spatial
               computing for humanities work, especially for affordances like hands-on interaction
               with virtual objects.</p>

            <p>The first version of Booksnake represents a proof of concept, both of the pipeline to
               transform digitized items into virtual objects and of AR as an interface for
               research, teaching, and learning with digitized archival materials. In addition to
               researching development of an Android version, our next steps are to refine
               Booksnake's object creation pipeline and AR interface. One major research focus is
               improving how Booksnake identifies, ingests, and interprets different types of
               dimensional metadata, to support as many different metadata schemas and existing
               online collections as possible. Another research focus is expanding the range of
               interactions possible with virtual objects — for example, making it possible for
               users to annotate a virtual object with text or graphics, or to adjust the
               transparency of a virtual object (to better support comparison between virtual and
               physical items). In the longer term, once the IIIF Consortium finalizes protocols for
               3D archival data and metadata <ptr target="#iiifc_2022"/>, we anticipate that future
               versions of Booksnake will be able to download and display 3D objects. Ongoing user
               testing and feedback will further inform Booksnake's continuing development. </p>

            <p>Our technical work enables Booksnake users to bring digitized cultural heritage
               materials out of flat screens and onto their desks and walls, using a tool — the
               consumer smartphone or tablet — that they already own. Meanwhile, our conceptual work
               to construct size-accurate virtual objects from existing images and available
               metadata will help to make a large, culturally significant, and ever-expanding body
               of digitized materials available for use in other immersive technology projects where
               the dimensional accuracy of virtual objects is important, such as virtual museum
               exhibits or manuscript viewers. To put it another way, it’s taken the better part of
               three decades to <hi rend="italic">digitize</hi> millions of cultural heritage
               materials — and that's barely the tip of the iceberg. How much more time will it take
                  <hi rend="italic">virtualize</hi> these cultural heritage materials, that is, to
               create accurate virtual replicas? Booksnake's automatic transformation method offers
               a way to repurpose existing resources toward this goal. </p>

            <p>We’re building Booksnake to enable digital accessibility and connect more people with
               primary sources. At its core, Booksnake is very simple. It is a tool for transforming
               digital images of cultural heritage materials into life-size virtual objects. In
               doing so, Booksnake leverages augmented reality as a means of interacting with
               archival materials that is radically new, but that feels deeply familiar. Booksnake
               is a way to virtualize the reading room or museum experience — and thereby
               democratize it, making embodied interaction with cultural heritage materials in
               physical space more accessible to more people in more places.</p>
         </div>



      </body>



      <back>
         <listBibl>
            <bibl xml:id="almas_etal_2018" label="Almas et al. 2018"> Almas, Bridget, et al. (2018)
                  <title rend="quotes">Manuscript Study in Digital Spaces: The State of the Field
                  and New Ways Forward</title>, <title rend="italic">Digital Humanities
                  Quarterly</title>, 12(2). Available at: <ref
                  target="http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html"
                  >http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html</ref>. </bibl>


            <bibl xml:id="apple_nd_a" label="Apple N.d."> Apple. (N.d.) <title rend="italic"
                  >ARKit</title> [Online]. Apple Developer. Available at: <ref
                  target="https://developer.apple.com/documentation/arkit/"
                  >https://developer.apple.com/documentation/arkit/</ref>. </bibl>


            <bibl xml:id="apple_nd_b" label="Apple N.d."> Apple. (N.d.) <title rend="italic"
                  >RealityKit</title> [Online]. Apple Developer. Available at: <ref
                  target="https://developer.apple.com/documentation/realitykit/"
                  >https://developer.apple.com/documentation/realitykit/</ref>. </bibl>


            <bibl xml:id="atske_perrin_2021" label="Atske and Perrin 2021"> Atske, Sara, and Perrin,
               Andrew. (2021) <title rend="quotes">Home broadband adoption, computer ownership vary
                  by race, ethnicity in the U.S.</title> Pew Research Center. Available at: <ref
                  target="https://pewrsr.ch/3Bdn6tW">https://pewrsr.ch/3Bdn6tW</ref>. </bibl>


            <bibl xml:id="azuma_etal_2001" label="Azuma et al. 2001"> Azuma, Ronald, et al. (2001)
                  <title rend="quotes">Recent advances in augmented reality</title>, <title
                  rend="italic">IEEE Computer Graphics and Applications</title>, 21(6), 34–47.
               Available at: <ref target="https://doi.org/10.1109/38.963459"
                  >https://doi.org/10.1109/38.963459</ref>. </bibl>


            <bibl xml:id="bacca_etal_2014" label="Bacca et al. 2014"> Bacca, Jorge, et al. (2014)
                  <title rend="quotes">Augmented Reality Trends in Education: A Systematic Review of
                  Research and Applications</title>, <title rend="italic">Educational Technology
                  &amp; Society</title>, 17(4), pp. 133–149. Available at: <ref
                  target="https://www.jstor.org/stable/jeductechsoci.17.4.133"
                  >https://www.jstor.org/stable/jeductechsoci.17.4.133</ref>. </bibl>


            <bibl xml:id="bailenson_2018" label="Bailenson 2018"> Bailenson, Jeremy. (2018) <title
                  rend="italic">Experience on Demand: What Virtual Reality is, How it Works, and
                  What it Can Do</title>. New York: W. W. Norton &amp; Co. </bibl>


            <bibl xml:id="ball_2022" label="Ball 2022"> Ball, Matthew. (2022) <title rend="italic"
                  >The Metaverse: And How It Will Revolutionize Everything</title>. New York:
               Liveright. </bibl>


            <bibl xml:id="bifrost_2023" label="Bifrost Consulting Group 2023"> Bifrost Consulting
               Group. (2023) <title rend="quotes">Let's build cultural spaces in the
                  metaverse</title>. Available at: <ref target="https://diomira.ca/"
                  >https://diomira.ca/</ref>. </bibl>


            <bibl xml:id="chandler_etal_2017" label="Chandler et al. 2017"> Chandler, Tom, et al.
               (2017) <title rend="quotes">A New Model of Angkor Wat: Simulated Reconstruction as a
                  Methodology for Analysis and Public Engagement</title>, <title rend="italic"
                  >Australian and New Zealand Journal of Art</title>, 17(2), pp. 182–194. Available
               at: <ref target="https://doi.org/10.1080/14434318.2017.1450063"
                  >https://doi.org/10.1080/14434318.2017.1450063</ref>. </bibl>


            <bibl xml:id="cline_2011" label="Cline 2011"> Cline, Ernest. (2011) <title rend="italic"
                  >Ready Player One</title>. New York: Broadway Books. </bibl>


            <bibl xml:id="cool_2022" label="Cool 2022"> Cool, Matt. (2022) <title rend="quotes">5
                  Inspiring Galleries Built with Hubs</title> [Blog]. Mozilla Hubs Creator Labs.
               Available at: <ref target="https://hubs.mozilla.com/labs/5-incredible-art-galleries/"
                  >https://hubs.mozilla.com/labs/5-incredible-art-galleries/</ref>. </bibl>


            <bibl xml:id="cramer_2011" label="Cramer 2011"> Cramer, Tom. (2011) <title rend="quotes"
                  >The International Image Interoperability Framework (IIIF): Laying the Foundation
                  for Common Services, Integrated Resources and a Marketplace of Tools for Scholars
                  Worldwide</title>, Coalition for Networked Information Fall 2011 Membership
               Meeting, Arlington, Virginia, December 12–13. Available at: <ref
                  target="https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework"
                  >https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework</ref>. </bibl>


            <bibl xml:id="cramer_2015" label="Cramer 2015"> Cramer, Tom. (2015) <title rend="quotes"
                  >IIIF Consortium Formed</title> [Online]. International Image Interoperability
               Framework. Available at: <ref
                  target="https://iiif.io/news/2015/06/17/iiif-consortium/"
                  >https://iiif.io/news/2015/06/17/iiif-consortium/</ref>. </bibl>


            <bibl xml:id="crane_2021" label="Crane 2021"> Crane, Tom. (2021) <title rend="quotes">On
                  being the right size</title> [Online]. Canvas Panel. Available at: <ref
                  target="https://canvas-panel.digirati.com/developer-stories/rightsize.html"
                  >https://canvas-panel.digirati.com/developer-stories/rightsize.html</ref>. </bibl>


            <bibl xml:id="dignazio_klein_2020" label="D'Ignazio and Klein 2020"> D'Ignazio,
               Catherine, and Klein, Lauren F. (2020) <title rend="italic">Data Feminism</title>.
               Cambridge, Mass.: The MIT Press. Available at: <ref
                  target="https://data-feminism.mitpress.mit.edu/"
                  >https://data-feminism.mitpress.mit.edu/</ref>. </bibl>


            <bibl xml:id="drucker_2014" label="Drucker 2014"> Drucker, Johanna. (2014) <title
                  rend="italic">Graphesis: Visual Forms of Knowledge Production</title>. Cambridge,
               Mass.: Harvard University Press. </bibl>


            <bibl xml:id="drucker_2013" label="Drucker 2013"> Drucker, Johanna. (2013) <title
                  rend="quotes">Is There a 'Digital' Art History?</title>, <title rend="italic"
                  >Visual Resources</title>, 29(1-2). Available at: <ref
                  target="https://doi.org/10.1080/01973762.2013.761106"
                  >https://doi.org/10.1080/01973762.2013.761106</ref>. </bibl>


            <bibl xml:id="drucker_2011" label="Drucker 2011"> Drucker, Johanna. (2011) <title
                  rend="quotes">Humanities Approaches to Graphical Display</title>, <title
                  rend="italic">Digital Humanities Quarterly</title>, 5(1). Available at: <ref
                  target="https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html"
                  >https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</ref>. </bibl>


            <bibl xml:id="falbo_2000" label="Falbo 2000">Falbo, Bianca. (2000) <title rend="quotes"
                  >Teaching from the Archives</title>, <title rend="italic">RBM: A Journal of Rare
                  Books, Manuscripts, and Cultural Heritage</title>, 1(1), pp. 33–35. Available at:
                  <ref target="https://doi.org/10.5860/rbm.1.1.173"
                  >https://doi.org/10.5860/rbm.1.1.173</ref>.</bibl>


            <bibl xml:id="fadgi_2023" label="Federal Agency Digitization Guidelines Initiative 2023"
               >Federal Agency Digitization Guidelines Initiative. (2023) <title rend="italic"
                  >Technical Guidelines for Digitizing Cultural Heritage Materials: Third
                  Edition</title>. [Washington, DC: Federal Agency Digitization Guidelines
               Initiative.] Available at: <ref
                  target="https://www.digitizationguidelines.gov/guidelines/digitize-technical.html"
                  >https://www.digitizationguidelines.gov/guidelines/digitize-technical.html</ref>.</bibl>


            <bibl xml:id="fisher_2018" label="Fisher 2019">Fisher, Adam. (2018) <title rend="italic"
                  >Valley of Genius: The Uncensored History of Silicon Valley</title>. New York:
               Twelve.</bibl>


            <bibl xml:id="francois_etal_2021" label="François et al. 2021">François, Paul, et al.
               (2021) <title rend="quotes">Virtual reality as a versatile tool for research,
                  dissemination and mediation in the humanities,</title>
               <title rend="italic">Virtual Archaeology Review</title> 12(25), pp. 1–15. Available
               at: <ref target="https://doi.org/10.4995/var.2021.14880"
                  >https://doi.org/10.4995/var.2021.14880</ref>.</bibl>


            <bibl xml:id="gibson_1984" label="Gibson 1984">Gibson, William. (1984) <hi rend="italic"
                  >Neuromancer. </hi>Reprint, New York: Ace, 2018.</bibl>


            <bibl xml:id="google_nd" label="Google nd">Google. (N.d.) <title rend="italic">Add
                  items</title> [Online]. Google Arts &amp; Culture Platform Help. Available at:
                  <ref
                  target="https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA"
                  >https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA</ref>
            </bibl>


            <bibl xml:id="greengard_2019" label="Greengard 2019">Greengard, Samuel. (2019) <title
                  rend="italic">Virtual Reality</title>. Cambridge, Mass.: The MIT Press.</bibl>


            <bibl xml:id="haraway_1988" label="Haraway 1988">Haraway, Donna. (1988) <title
                  rend="quotes">Situated Knowledges: The Science Question in Feminism and the
                  Privilege of Partial Perspective,</title>
               <title rend="italic">Feminist Studies </title>14(3), pp. 575–599. Available at: <ref
                  target="https://www.jstor.org/stable/3178066"
                  >https://www.jstor.org/stable/3178066</ref>.</bibl>


            <bibl xml:id="haynes_2018" label="Haynes 2018">Haynes, Ronald. (2018) <title
                  rend="quotes">Eye of the Veholder: AR Extending and Blending of Museum Objects and
                  Virtual Collections</title> in Jung, T., and tom Dieck, M. C. (eds.) <title
                  rend="italic">Augmented Reality and Virtual Reality: Empowering Human, Place and
                  Business.</title> Cham, Switzerland: Springer, pp. 79–91. Available at: <ref
                  target="https://doi.org/10.1007/978-3-319-64027-3_6"
                  >https://doi.org/10.1007/978-3-319-64027-3_6</ref>.</bibl>


            <bibl xml:id="haynes_2019" label="Haynes 2010">Haynes, Ronald.(2019) <title
                  rend="quotes">To Have and Vehold: Marrying Museum Objects and Virtual Collections
                  via AR</title> in tom Dieck, M. C., and Jung, T. (eds.) <title rend="italic"
                  >Augmented Reality and Virtual Reality: The Power of AR and VR for
                  Business.</title> Cham, Switzerland: Springer, pp. 191–202. Available at: <ref
                  target="https://doi.org/10.1007/978-3-030-06246-0_14"
                  >https://doi.org/10.1007/978-3-030-06246-0_14</ref>.</bibl>


            <bibl xml:id="herron_2020" label="Herron 2020">Herron, Thomas. (2020) <title
                  rend="italic">Recreating Spenser: The Irish Castle of an English Poet</title>.
               Greenville, N.C.: Author.</bibl>


            <bibl xml:id="iiifc_2022" label="IIIF Consortium 2022">IIIF Consortium staff. (2022)
                  <title rend="italic">New 3D Technical Specification Group</title> [Online].
               Available at: <ref target="https://iiif.io/news/2022/01/11/new-3d-tsg/"
                  >https://iiif.io/news/2022/01/11/new-3d-tsg/</ref>.</bibl>


            <bibl xml:id="iiif_nd" label="International Image Interoperability Framework n.d."
               >International Image Interoperability Framework. (N.d.) <title rend="italic"
                  >Consortium</title>
               <title rend="italic">Members</title> [Online]. Available at: <ref
                  target="https://iiif.io/community/consortium/members/"
                  >https://iiif.io/community/consortium/members/</ref>.</bibl>


            <bibl xml:id="iiif_2015" label="International Image Interoperability Framework 2015"
               >International Image Interoperability Framework. (2015) <title rend="quotes">Physical
                  Dimensions</title> in Albritton, Benjamin, et al. (eds.) <title rend="italic"
                  >Linking to External Services </title>[Online]. Available at: <ref
                  target="https://iiif.io/api/annex/services/%23physical-dimensions"
                  >https://iiif.io/api/annex/services/#physical-dimensions</ref>.</bibl>


            <bibl xml:id="kai-kee_latina_sadoyan_2020" label="Kai-kee, Latina, Sadoyan 2020"
               >Kai-Kee, E., Latina, L., and Sadoyan, L. (2020) <title rend="italic">Activity-based
                  Teaching in the Art Museum: Movement, Embodiment, Emotion</title>. Los Angeles:
               Getty Museum. </bibl>


            <bibl xml:id="kenderdine_yip_2019" label="Kenderdine and Yip 2019">Kenderdine, Sarah, and Yip,
               Andrew (2019). <title rend="quotes">The Proliferation of Aura: Facsimiles,
                  Authenticity and Digital Objects</title> in Drtoner, K., et al. (eds.) <title
                  rend="italic">The Routledge Handbook of Museums, Media and Communication</title>.
               London and New York: Routledge, pp. 274–289. Available at: <ref
                  target="https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip"
                  >https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip</ref>
            </bibl>


            <bibl xml:id="kropf_2017" label="Kropf 2017">Kropf, Evyn. (2017) <title rend="quotes"
                  >Will that Surrogate Do?: Reflections on Material Manuscript Literacy in the
                  Digital Environment from Islamic Manuscripts at the University of Michigan
                  Library,</title>
               <title rend="italic">Manuscript Studies</title>, 1(1), pp. 52–70. Available at: <ref
                  target="https://doi.org/10.1353/mns.2016.0007"
                  >https://doi.org/10.1353/mns.2016.0007</ref>.</bibl>


            <bibl xml:id="l_i_2018" label="Luna 2018">Luna Imaging. (2018) <title rend="italic">LUNA
                  and IIIF</title> [Online]. Available at: <ref
                  target="http://www.lunaimaging.com/iiif"
               >http://www.lunaimaging.com/iiif</ref>.</bibl>


            <bibl xml:id="luo_2019" label="Luo 2019">Luo, Michelle. (2019) <title rend="quotes"
                  >Explore art and culture through a new lens [Blog]</title>
               <title rend="italic">The Keyword</title>. Available at: <ref
                  target="https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/"
                  >https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/</ref>
            </bibl>


            <bibl xml:id="newberry_2023" label="Newberry 2023">Newberry, David. (2023) <title
                  rend="italic">IIIF for Dolls</title> [Online]. Available at: <ref
                  target="https://iiif-for-dolls.davidnewbury.com/"
                  >https://iiif-for-dolls.davidnewbury.com/</ref>.</bibl>


            <bibl xml:id="nolan_2013" label="Nolan 2013">Nolan, Maura. (2013) <title rend="quotes"
                  >Medieval Habit, Modern Sensation: Reading Manuscripts in the Digital Age,</title>
               <title rend="italic">The Chaucer Review</title>, 47(4), pp. 465–476. Available at:
                  <ref target="https://doi.org/10.5325/chaucerrev.47.4.0465"
                  >https://doi.org/10.5325/chaucerrev.47.4.0465</ref>.</bibl>


            <bibl xml:id="oclc_nd" label="OCLC n.d.">OCLC. (N.d.) <title rend="italic">CONTENTdm and
                  the International Image Interoperability Framework (IIIF)</title> [Online].
               Available at: <ref target="https://www.oclc.org/en/contentdm/iiif.html"
                  >https://www.oclc.org/en/contentdm/iiif.html</ref>.</bibl>


            <bibl xml:id="orangelogic_nd" label="orangelogic n.d.">orangelogic. (N.d.) <title
                  rend="italic">Connect to your favorite systems: Get DAM integrations that fit your
                  existing workflows</title> [Online]. Available at: <ref
                  target="https://www.orangelogic.com/features/integrations"
                  >https://www.orangelogic.com/features/integrations</ref>.</bibl>


            <bibl xml:id="openseadragon_nd" label="OpenSeadragon n.d.">OpenSeadragon. (N.d.) <title
                  rend="italic">OpenSeadragon</title>. Available at: <ref
                  target="http://openseadragon.github.io/">http://openseadragon.github.io/</ref>.
               </bibl>
            <bibl xml:id="pew_2021" label="Pew Research Center 2021">Pew Research Center. (2021) Mobile Fact Sheet. Available at: <ref
               target="http://pewrsr.ch/2ik6Ux9">http://pewrsr.ch/2ik6Ux9</ref></bibl>


            <bibl xml:id="porter_2018" label="Porter 2018">Porter, Dot. (2018) <title rend="quotes"
                  >Zombie Manuscripts: Digital Facsimiles in the Uncanny Valley</title>.
               Presentation at the International Congress on Medieval Studies, Western Michigan
               University, Kalamazoo, Michigan, 12 May 2018. Available at: <ref
                  target="https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/"
                  >https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/</ref>
            </bibl>


            <bibl xml:id="project_mirador_nd" label="Project Mirador n.d.">Project Mirador. (N.d.)
                  <title rend="italic">Mirador</title>. Available at: <ref
                  target="https://projectmirador.org/">https://projectmirador.org/</ref>.</bibl>


            <bibl xml:id="putnam_2016" label="Putnam 2016">Putnam, Lara. (2016) <title rend="quotes"
                  >The Transnational and the Text-Searchable: Digitized Sources and the Shadows They
                  Cast,</title>
               <title rend="italic">The American Historical Review</title>, 121(2), pp. 377–402.
               Available at: <ref target="https://doi.org/10.1093/ahr/121.2.377"
                  >https://doi.org/10.1093/ahr/121.2.377</ref>.</bibl>


            <bibl xml:id="ramsay_2014" label="Ramsay 2014">Ramsay, Stephen. (2014) <title
                  rend="quotes">The Hermeneutics of Screwing Around; or What You Do With a Million
                  Books</title> in Kee, Kevin (ed.) <title rend="italic">Pastplay: Teaching and
                  Learning History with Technology</title>. Ann Arbor: University of Michigan Press,
               pp. 111–120. Available at: <ref target="https://doi.org/10.1353/book.29517"
                  >https://doi.org/10.1353/book.29517</ref>.</bibl>


            <bibl xml:id="rosenzweig_2003" label="Rosenzweig 2003">Rosenzweig, Roy. (2003) <title
                  rend="quotes">Scarcity or Abundance? Preserving the Past in a Digital Era,</title>
               <title rend="italic">The American Historical Review</title> 108 (3), pp. 735–762.
               Available at: <ref target="https://doi.org/10.1086/ahr/108.3.735"
                  >https://doi.org/10.1086/ahr/108.3.735</ref>.</bibl>


            <bibl xml:id="roth_fisher_2019" label="Roth and Fisher 2019">Roth, A., and Fisher, C.
               (2019) <title rend="quotes">Building Augmented Reality Freedom Stories: A Critical
                  Reflection,</title> in Kee, Kevin, and Compeau, Timothy (eds.) <title
                  rend="italic">Seeing the Past with Computers: Experiments with Augmented Reality
                  and Computer Vision for History.</title> Ann Arbor: University of Michigan Press,
               pp. 137–157. Available at: <ref target="https://www.jstor.org/stable/j.ctvnjbdr0.11"
                  >https://www.jstor.org/stable/j.ctvnjbdr0.11</ref></bibl>


            <bibl xml:id="schmiesing_hollis_2002" label="Schmiesing and Hollis 2002">Schmiesing, A.,
               and Hollis, D. (2002) <title rend="quotes">The Role of Special Collections
                  Departments in Humanities Undergraduate and Graduate Teaching: A Case
                  Study,</title>  <title rend="italic">Libraries and the Academy</title> 2(3), pp.
               465–480. Available at: <ref target="https://doi.org/10.1353/pla.2002.0065"
                  >https://doi.org/10.1353/pla.2002.0065</ref>.</bibl>


            <bibl xml:id="shemek_etal_2018" label="Shemek 2018">Shemek, D., et al. (2018) <title
                  rend="quotes">Renaissance Remix. Isabella d'Este: Virtual Studiolo,</title>
               <title rend="italic">Digital Humanties Quarterly</title>, 12(4). Available at: <ref
                  target="http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html"
                  >http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html</ref>.</bibl>


            <bibl xml:id="snydman_sanderson_cramer_2015" label="Snydman, Sanderson, and Cramer 2015"
               >Snydman, S., Sanderson, R., &amp; Cramer, T. (2015) <title rend="quotes">The
                  International Image Interoperability Framework (IIIF): A community &amp;
                  technology approach for web-based images.</title> In D. Walls (Ed), <title
                  rend="italic">Archiving 2015: Final Program and Proceedings</title> (pp. 16–21).
               Society for Imaging Science and Technology. Available at: <ref
                  target="https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf"
                  >https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf</ref></bibl>


            <bibl xml:id="solberg_2012" label="Solberg 2012">Solberg, Janine. (2012) <title
                  rend="quotes">Googling the Archive: Digital Tools and the Practice of
                  History.</title>
               <title rend="italic">Advances in the History of Rhetoric</title> 15(1), pp. 53–76.
               Available at: <ref target="https://doi.org/10.1080/15362426.2012.657052"
                  >https://doi.org/10.1080/15362426.2012.657052</ref>.</bibl>


            <bibl xml:id="stephenson_1992" label="Stephenson 1992">Stephenson, Neal. (1992) <title
                  rend="italic">Snow Crash</title>. Reprint, New York: Bantam, 2000.</bibl>


            <bibl xml:id="sundar_etal_2013" label="Sundar et.al 2013">Sundar, S., Bellur,
               Saraswathi, Oh, Jeeyun, Xu, Qian, &amp; Jia, Haiyan. (2013) <title rend="quotes">User
                  Experience of On-Screen Interaction Techniques: An Experimental Investigation of
                  Clicking, Sliding, Zooming, Hovering, Dragging, and Flipping.</title>
               <title rend="italic">Human–Computer Interaction</title> 29 (2), pp. 109–152.
               doi:10.1080/07370024.2013.789347 </bibl>


            <bibl xml:id="szpiech_2014" label="Szpiech 2014">Szpiech, Ryan. (2014) <title
                  rend="quotes">Cracking the Code: Reflections on Manuscripts in the Age of Digital
                  Books.</title>
               <title rend="italic">Digital Philology: A Journal of Medieval Cultures</title> 3(1),
               pp. 75–100. Available at: <ref target="https://doi.org/10.1353/dph.2014.0010"
                  >https://doi.org/10.1353/dph.2014.0010</ref>.</bibl>


            <bibl xml:id="toner_1993" label="Toner 1993">Toner, Carol. (1993) <title rend="quotes"
                  >Teaching Students to be Historians: Suggestions for an Undergraduate Research
                  Seminar,</title> <title rend="italic">The History Teacher</title> 27(1), pp.
               37–51. Available at: <ref target="https://doi.org/10.2307/494330"
                  >https://doi.org/10.2307/494330</ref>.</bibl>


            <bibl xml:id="van_lit_2020" label="van Lit 2020">van Lit, L. W. C. (2020) <title
                  rend="italic">Among Digitized Manuscripts: Philology, Codicology, Paleography in a
                  Digital World</title>. Leiden: Brill.</bibl>


            <bibl xml:id="van_zundert_2018" label="van Zundert 2018">van Zundert, Joris. (2018)
                  <title rend="quotes">On Not Writing a Review about Mirador: Mirador, IIIF, and the
                  Epistemological Gains of Distributed Digital Scholarly Resources.</title>
               <title rend="italic">Digital Medievalist</title> 11(1). Available at: <ref
                  target="http://doi.org/10.16995/dm.78">http://doi.org/10.16995/dm.78</ref>.</bibl>


            <bibl xml:id="vogels_2021" label="Vogels 2021">Vogels, Emily A. (2021) <title
                  rend="quotes">Digital divide persists even as Americans with lower incomes make
                  gains in tech adoption</title>
               <title rend="italic">Pew Research Center</title>. Available at: <ref
                  target="https://pewrsr.ch/2TRM7cP">https://pewrsr.ch/2TRM7cP</ref></bibl>


            <bibl xml:id="woodward_2021" label="Woodward 2021">Woodward, Dave. (2021) Email to Sean
               Fraga, February 2.</bibl>
            <bibl/>


         </listBibl>
      </back>
   </text>
</TEI>
