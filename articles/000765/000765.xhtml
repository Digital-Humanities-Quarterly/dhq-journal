<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Introducing Booksnake: A Scholarly App for
                  Transforming Existing Digitized Archival Materials into Life-Size Virtual Objects
                  for
                  Embodied Interaction in Physical Space, using IIIF and Augmented
                  Reality</h1>
               
               
               
               
               <div class="author"><span style="color: grey">Sean Fraga
                     </span> &lt;<a href="mailto:sfraga_at_usc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('sfraga_at_usc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('sfraga_at_usc_dot_edu'); return false;">sfraga_at_usc_dot_edu</a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">Christy Ye
                     </span> &lt;<a href="mailto:christyspace1731_at_gmail_dot_com" onclick="javascript:window.location.href='mailto:'+deobfuscate('christyspace1731_at_gmail_dot_com'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('christyspace1731_at_gmail_dot_com'); return false;">christyspace1731_at_gmail_dot_com</a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">Henry Huang
                     </span> &lt;<a href="mailto:henry2423_at_gmail_dot_com " onclick="javascript:window.location.href='mailto:'+deobfuscate('henry2423_at_gmail_dot_com '); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('henry2423_at_gmail_dot_com '); return false;">henry2423_at_gmail_dot_com </a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">Zack Sai
                     </span> &lt;<a href="mailto:saidane_at_usc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('saidane_at_usc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('saidane_at_usc_dot_edu'); return false;">saidane_at_usc_dot_edu</a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">Michael Hughes
                     </span> &lt;<a href="mailto:hughesmr_at_usc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('hughesmr_at_usc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('hughesmr_at_usc_dot_edu'); return false;">hughesmr_at_usc_dot_edu</a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">April Yao
                     </span> &lt;<a href="mailto:siyuyao_at_usc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('siyuyao_at_usc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('siyuyao_at_usc_dot_edu'); return false;">siyuyao_at_usc_dot_edu</a>&gt;, University of Southern California</div>
               
               
               <div class="author"><span style="color: grey">Samir Ghosh
                     </span> &lt;<a href="mailto:samir_dot_ghosh_at_ucsc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('samir_dot_ghosh_at_ucsc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('samir_dot_ghosh_at_ucsc_dot_edu'); return false;">samir_dot_ghosh_at_ucsc_dot_edu</a>&gt;, University of California, Santa Cruz</div>
               
               
               
               <div class="author"><span style="color: grey">April Yao
                     </span> &lt;<a href="mailto:siyuyao_at_usc_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('siyuyao_at_usc_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('siyuyao_at_usc_dot_edu'); return false;">siyuyao_at_usc_dot_edu</a>&gt;, University of Southern California</div>
               
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Introducing%20Booksnake%3A%20A%20Scholarly%20App%20for%20Transforming%20Existing%20Digitized%20Archival%20Materials%20into%20Life-Size%20Virtual%20Objects%20for%20Embodied%20Interaction%20in%20Physical%20Space,%20using%20IIIF%20and%20Augmented%20Reality&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Fraga&amp;rft.aufirst=Sean&amp;rft.au=Sean%20Fraga&amp;rft.au=Christy%20Ye&amp;rft.au=Henry%20Huang&amp;rft.au=Zack%20Sai&amp;rft.au=Michael%20Hughes&amp;rft.au=April%20Yao&amp;rft.au=Samir%20Ghosh&amp;rft.au=April%20Yao"> </span></div>
            
            <div id="DHQtext">
               
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>We introduce Booksnake, a new mobile app that makes it feel like digitized archival
                     items are physically present in a user’s real-world surroundings by using the
                     augmented reality (AR) technology in consumer smartphones and tablets. Unlike
                     humanities projects that use virtual reality (VR) or AR to publish custom content,
                     Booksnake is a general-purpose, content-agnostic tool compatible with existing online
                     collections that support the International Image Interoperability Framework (IIIF).
                     In this article, we critique existing flat-screen image viewers and discuss the
                     benefits of embodied interaction with archival materials. We contextualize Booksnake
                     within the broader landscape of immersive technologies for cultural heritage. We
                     detail the technical pipeline by which Booksnake transforms existing digitized
                     archival materials into custom life-size virtual objects for interaction in physical
                     space. We conclude with a brief discussion of the future of the immersive
                     humanities.</p>
                  </div>
               
               
               
               
               
               
               
               
               
               
               <div class="div div0">
                  
                  
                  <h1 class="head">INTRODUCTION</h1>
                  
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">Close engagement with primary sources is foundational to humanities research,
                     teaching, and learning [<a class="ref" href="#falbo_2000">Falbo 2000</a>], [<a class="ref" href="#schmiesing_hollis_2002">Schmiesing and Hollis 2002</a>], [<a class="ref" href="#toner_1993">Toner 1993</a>]. We are fortunate
                     to live in an age of digital abundance: Over the past three decades, galleries,
                     libraries, archives, and museums (collectively known as GLAM institutions) have
                     undertaken initiatives to digitize their collection holdings, making millions of
                     primary-source archival materials freely available online and more accessible to more
                     people than ever before [<a class="ref" href="#rosenzweig_2003">Rosenzweig 2003</a>],[<a class="ref" href="#solberg_2012">Solberg 2012</a>], [<a class="ref" href="#ramsay_2014">Ramsay 2014</a>][<a class="ref" href="#putnam_2016">Putnam 2016</a>]. Increasingly, the
                     world’s myriad cultural heritage materials — architectural plans, books, codices,
                     correspondence, drawings, ephemera, manuscripts, maps, newspapers, paintings,
                     paperwork, periodicals, photographs, postcards, posters, prints, sheet music,
                     sketches, slides, and more — are now only a click away.</div>
                  
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">But interacting with digitized archival materials in a Web browser can be a
                     frustrating experience — one that fails to replicate the embodied engagement possible
                     during in-person research. As users, we have become resigned to the limitations of
                     Web-based image viewers. How we have to fiddle with <span class="hi quotes">Zoom In</span>
                     and <span class="hi quotes">Zoom Out</span> buttons to awkwardly jump between fixed levels of
                     magnification. How we must laboriously click and drag, click and drag, over and over,
                     to follow a line of text across a page, or read the length of a newspaper column,
                     or
                     trace a river over a map. How we can’t ever quite tell how big or small something
                     is.
                     We have long accepted these frictions as necessary compromises to quickly and easily
                     view materials online instead of making an inconvenient trip to a physical archive
                     itself — even as we also understand that clicking around in a Web viewer pales in
                     comparison to the rich, fluid, embodied experience of interacting with a physical
                     item in a museum gallery or library reading room.</div>
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure01.jpg" rel="external"><img src="resources/images/figure01.jpg" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 1. </div>Booksnake transforms existing digitized archival materials, like this 1909
                        birds-eye-view map of Los Angeles, into custom virtual objects for embodied
                        exploration in physical space, by using the augmented reality technology in
                        consumer smartphones and tablets. The virtual object remains anchored to the table
                        surface even as the user and device move.</div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">As a step toward moving past these limitations, this research team presents a new
                     kind of image viewer. Booksnake is a scholarly app for iPhones and iPads that enables
                     users to interact with existing digitized archival materials at life size in physical
                     space (see figure 1). Instead of displaying digital images on a flat screen for
                     indirect manipulation, Booksnake uses augmented reality, the process of overlaying
                     a
                     virtual object on physical space [<a class="ref" href="#azuma_etal_2001">Azuma et al. 2001</a>], to bring digitized
                     items into the physical world for embodied exploration. Existing image viewers
                     display image files as a means of conveying visual information about an archival
                     item. In contrast, Booksnake converts image files into virtual objects in order to
                     create the feeling of being in the item's presence. To do this, Booksnake
                     automatically transforms digital images of archival materials into custom,
                     size-accurate virtual objects, then dynamically inserts these virtual objects into
                     the live camera view on a smartphone or tablet for interaction in physical space (see
                     figure 2). Booksnake is available as a free app for iPhone and iPad on Apple's App
                     Store: <a href="https://apps.apple.com/us/app/booksnake/id1543247176." onclick="window.open('https://apps.apple.com/us/app/booksnake/id1543247176.'); return false" class="ref">https://apps.apple.com/us/app/booksnake/id1543247176.</a></div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div><iframe src="https://www.youtube.com/watch?v=k9ccB3GNEvo" frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" width="640" height="524"><!--Gimme some comment!--></iframe></div>
                     
                     <div class="caption">
                        <div class="label">Figure 2. </div>This video shows how a Booksnake user can freely search, browse, and filter the
                        Library of Congress (LOC) digitized collections, then download LOC items to their
                        Booksnake library. When the user selects an item and taps "View in Your Space,"
                        Booksnake automatically transforms the item's digital image into a custom,
                        life-size virtual object, then displays the virtual object in the user's physical
                        surroundings.</div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">Booksnake's use of AR makes it feel like a digitized item is physically present in
                     a
                     user’s real-world environment, enabling closer and richer engagement with digitized
                     materials. A Booksnake user aims their phone or tablet at a flat surface (like a
                     table, wall, bed, or floor) and taps the screen to anchor the virtual object to the
                     physical surface. As the user moves, Booksnake uses information from the device’s
                     cameras and sensors to continually adjust the virtual object’s relative position,
                     orientation, and size in the camera view, such that the item appears to remain
                     stationary in physical space as the user and device move. A user thus treats
                     Booksnake like a lens, looking <span class="hi italic">through</span> their device’s screen
                     at a virtual object overlaid on the physical world.</div>
                  
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">The project takes its name from book snakes, the weighted strings used by archival
                     researchers to hold fragile physical materials in place. Similarly, Booksnake enables
                     users to keep virtual materials in place on physical surfaces in the real world. By
                     virtualizing the experience of embodied interaction, Booksnake makes it possible for
                     people who cannot otherwise visit an archive (due to cost, schedule, distance,
                     disability, or other reasons) to physically engage with digitized materials.</div>
                  
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Booksnake represents a proof-of-concept, demonstrating that it is possible to
                     automatically transform existing digital image files of cultural heritage materials
                     into life-size virtual objects on demand. We have designed Booksnake to easily link
                     with existing digitized collections via the International Image Interoperability
                     Framework (or IIIF, pronounced <span class="hi quotes">triple-eye-eff</span>), a
                     widely-supported set of open Web protocols for digitally accessing archival materials
                     and related metadata, developed and maintained by the GLAM community [<a class="ref" href="#cramer_2011">Cramer 2011</a>], [<a class="ref" href="#cramer_2015">Cramer 2015</a>], [<a class="ref" href="#snydman_sanderson_cramer_2015">Snydman, Sanderson, and Cramer 2015</a>]. The first version of Booksnake uses
                     IIIF to access Library of Congress digitized collections, including the Chronicling
                     America collection of historical newspapers. Our goal is for Booksnake to be able
                     to
                     display any image file accessible through IIIF. At this point, the considerable
                     variability in how different institutions record dimensional metadata means that we
                     will have to customize Booksnake's links with each digitized collection, as we
                     discuss further below. </div>
                  
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Our approach represents a novel use of AR for humanistic research, teaching, and
                     learning. While humanists have begun to explore the potential of immersive
                     technologies — primarily virtual reality (VR) and AR — they have largely approached
                     these technologies as publication platforms: ways of hosting custom-built experiences
                     containing visual, textual, spatial, and/or aural content that is manually created
                     or
                     curated by subject-matter experts. Examples include reconstructions of
                     twelfth-century Angkor Wat [<a class="ref" href="#chandler_etal_2017">Chandler et al. 2017</a>], a Renaissance
                     studiolo [<a class="ref" href="#shemek_etal_2018">Shemek 2018</a>], events from the Underground Railroad [<a class="ref" href="#roth_fisher_2019">Roth and Fisher 2019</a>], a medieval Irish castle [<a class="ref" href="#herron_2020">Herron 2020</a>], and an 18th-century Paris theatre [<a class="ref" href="#francois_etal_2021">François et al. 2021</a>]. These are
                     rich, immersive experiences, but they are typically limited to a specific site,
                     narrative, or set of material. The work involved in manually creating these virtual
                     models can be considerable, lengthy, and expensive.</div>
                  
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">In contrast, Booksnake is designed as a general-purpose AR tool for archival
                     exploration. Booksnake's central technical innovation is the automatic transformation
                     of existing digital images of archival cultural heritage materials into
                     dimensionally-accurate virtual objects. We aim to enable users to freely select
                     materials from existing digitized cultural heritage collections for embodied
                     interaction. Unlike custom-built immersive humanities projects, Booksnake makes it
                     possible for a user to create virtual objects easily, quickly, and freely. While
                     Booksnake fundamentally differs from existing Web-based image viewers, it is, like
                     them, an empty frame, waiting for a user to fill it with something interesting. </div>
                  
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">This article presents a conceptual and technical overview of Booksnake. We first
                     critique the accepted method of viewing digitized archival materials in Web-based
                     image viewers on flat screens and discuss the benefits of embodied interaction with
                     archival materials, then describe results from initial user testing and potential
                     use
                     cases. Next, we contextualize Booksnake, as an AR app for mobile devices, within the
                     broader landscape of immersive technologies for cultural heritage. We then detail
                     the
                     technical pipeline by which Booksnake transforms digitized archival materials into
                     virtual objects for interaction in physical space. Ultimately, Booksnake's use of
                     augmented reality demonstrates the potential of spatial interfaces and embodied
                     interaction to improve accessibility to archival materials and activate digitized
                     collections, while its presentation as a mobile app is an argument for the untapped
                     potential of mobile devices to support humanities research, teaching, and
                     learning.</div>
                  
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">Booksnake is designed and built by a multidisciplinary team at the University of
                     Southern California. It represents the combined efforts of humanities scholars,
                     librarians, interactive media designers, and computer scientists — most of them
                     students or early-career scholars — extending over four years.<a class="noteRef" href="#d4e472">[1]</a> The project has been
                     financially supported by the Humanities in a Digital World program (under grants from
                     the Mellon Foundation) in USC's Dornsife College of Letters, Arts, and Sciences; and
                     by the Ahmanson Lab, a scholarly innovation lab in the Sydney Harman Academy for
                     Polymathic Studies in USC Libraries. The research described in this article was
                     supported by a Digital Humanities Advancement Grant (Level II) from the National
                     Endowment for the Humanities (HAA-287859-22). "The project’s next phase will be
                     supported by a second NEH Digital Humanities Advancement Grant (Level II)
                     (HAA-304169-25).</div>
                  </div>
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">FROM FLAT SCREENS TO EMBODIED EXPLORATION</h1>
                  
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">Flat-screen image viewers have long been hiding in plain sight. Each image viewer
                     sits at the end of an institution's digitization pipeline and offer an interface
                     through which users can access, view, and interact with digital image files of
                     collection materials. These image viewers' fundamental characteristic is their
                     transparency: The act of viewing a digital image of an archival item on a flat screen
                     has become so common within contemporary humanities practices that we have
                     unthinkingly naturalized it.</div>
                  
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">But there is nothing <span class="hi quotes">natural</span> about using flat-screen image
                     viewers to apprehend archival materials, because everything about the digitization
                     process is novel. The ability to instantly view a digitized archival item on a
                     computer screen rests on technological developments from the last sixty years: the
                     high-resolution digital cameras that capture archival material; the digital asset
                     management software that organizes it; the cheap and redundant cloud storage that
                     houses it; the high-speed networking infrastructure that delivers it; the Web
                     browsers with which we access it; the high-resolution, full-color screens that
                     display it. Even the concept of a graphical user interface, with its representative
                     icons and mouse-based input, is a modern development, first publicly demonstrated
                     by
                     Douglas Engelbart in 1968 and first commercialized by Apple, with the Lisa and the
                     Macintosh, in the early 1980s [<a class="ref" href="#fisher_2018">Fisher 2019</a>, 17–26, 85–93, 104–117]. Put another way, our now-familiar ways of using a mouse or trackpad to interact
                     with digitized archival materials in a Web-based flat-screen image viewer would be
                     incomprehensibly alien to the people who originally created and used many of these
                     materials — and, in many cases, even to the curators, librarians, and archivists who
                     first acquired and accessioned these items. What does this mean for our relationship
                     with these archival materials? </div>
                  
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13"> Despite this, and although Web-based flat-screen image viewers sustain a robust
                     technical development community, they have been largely overlooked by most digital
                     humanists. A notable exception is manuscript scholars, for whom the relationship
                     between text, object, and digital image is particularly important. Indeed, manuscript
                     scholars have led the charge in identifying flat-screen image viewers as sites of
                     knowledge creation and interpretation — often by expressing their frustration with
                     these viewers' limited affordances or with the contextual information these viewers
                     shear away [<a class="ref" href="#nolan_2013">Nolan 2013</a>], [<a class="ref" href="#szpiech_2014">Szpiech 2014</a>], [<a class="ref" href="#kropf_2017">Kropf 2017</a>], [<a class="ref" href="#almas_etal_2018">Almas et al. 2018</a>], [<a class="ref" href="#porter_2018">Porter 2018</a>], [<a class="ref" href="#van_zundert_2018">van Zundert 2018</a>], [<a class="ref" href="#van_lit_2020">van Lit 2020</a>].</div>
                  
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14"> To see flat-screen image viewers more clearly, it helps to understand them within
                     the context of archival digitization practices. Digitization is usually understood
                     as
                     the straightforward conversion of physical objects into digital files, but this
                     process is never simple and always involves multiple interpretive decisions.<a class="noteRef" href="#d4e511">[2]</a> As Johanna
                     Drucker writes, “the way artifacts are [digitally] encoded
                     depends on the parameters set for scanning and photography. These already embody
                     interpretation, since the resolution of an image, the conditions of lighting under
                     which it is produced, and other factors, will alter the outcome”
                     [<a class="ref" href="#drucker_2013">Drucker 2013</a>, 8]. Some of these encoding decisions are
                     structured by digitization guidelines, such as the U.S. Federal Agency Digitization
                     Guidelines Initiative [<a class="ref" href="#fadgi_2023">Federal Agency Digitization Guidelines Initiative 2023</a>] standards (2023), while other
                     decisions, such as how to light an item or how to color-correct a digital image,
                     depend on the individual training and judgment of digitization professionals.</div>
                  
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15"> A key digitization convention is to render an archival item from an idealized
                     perspective, that of an observer perfectly centered before the item. To achieve this,
                     a photographer typically places the item being digitized perpendicular to the
                     camera's optical axis, centers the item within the camera's view, and orthogonally
                     aligns the item with the viewfinder's edges. During post-processing, a digitization
                     professional can then crop, straighten, and de-skew the resulting image, or stitch
                     together multiple images of a given item into a single cohesive whole. These physical
                     and digital activities produce an observer-independent interpretation of an archival
                     item. Put another way, archival digitization is what Donna Haraway calls a god trick,
                     the act of “seeing everything from nowhere”
                     [<a class="ref" href="#haraway_1988">Haraway 1988</a>, 582].</div>
                  
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16"> Taking these many decisions together, Drucker argues that “digitization is not representation but interpretation”
                     [<a class="ref" href="#drucker_2013">Drucker 2013</a>, 12] (emphasis original). Understanding
                     digitization as a continuous interpretive process, rather than a simple act of
                     representation, helps us see how this process extends past the production of digital
                     image files and into how these files are presented to human users via into
                     traditional flat-screen image viewers. </div>
                  
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17"> Flat-screen image viewers encode a set of decisions about how we can (or should)
                     interact with a digitized item. Just as decisions about resolution, lighting, and
                     file types serve to construct a digitized interpretation of a physical object, so
                     too
                     do decisions about interface affordances for an image viewer serve to construct an
                     interpretive space. Following Drucker, image viewers do not simply <span class="hi italic">represent </span>digital image files to a user, they <span class="hi italic">interpret</span> them. Decisions by designers and developers about an image
                     viewer’s interface affordances (how to zoom, turn pages, create annotations, etc.)
                     structure the conditions of possibility for a user's interactions with a digitized
                     item. </div>
                  
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18"> The implications of these decisions are particularly visible when comparing
                     different image viewers. For example, Mirador, a leading IIIF-based image viewer,
                     enables users to compare two images from different repositories side-by-side [<a class="ref" href="#project_mirador_n.d.">Project Mirador n.d.</a>]. A different IIIF-based image viewer,
                     OpenSeadragon, is instead optimized for viewing individual “high-resolution zoomable images”
                     [<a class="ref" href="#openseadragon_n.d.">OpenSeadragon n.d.</a>]. Mirador encourages juxtaposition, while
                     OpenSeadragon emphasizes attention to detail. These two viewers each represent a
                     particular set of assumptions, goals, decisions, and compromises, which in turn shape
                     how their respective users encounter and read a given item, the interpretations those
                     users form, and the knowledge they create. </div>
                  
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19"> Flat-screen image viewers generally share three attributes that collectively
                     structure a user's interactions with a digitized item. First, and most importantly,
                     flat-screen image viewers directly reproduce the idealized “view
                     from nowhere” delivered by the digitization pipeline. Flat-screen image
                     viewers could present digital images in any number of ways — upside down, canted at
                     an angle away from the viewer, obscured by a digital curtain. But instead,
                     flat-screen image viewers play the god trick. Second, flat-screen image viewers rely
                     on indirect manipulation via a mouse or trackpad. To zoom, pan, rotate, or otherwise
                     navigate a digitized item, a user must repeatedly click buttons or click and drag,
                     positioning and re-positioning the digital image in order to apprehend its content.
                     These interaction methods create friction between the user and the digitized item,
                     impeding discovery [<a class="ref" href="#sundar_etal_2013">Sundar et.al 2013</a>]. Finally, flat-screen image
                     viewers arbitrarily scale digital images to fit a user’s computer screen. “Digitisation doesn't make everything equal, it just makes everything
                     the same size,” writes [<a class="ref" href="#crane_2021">Crane 2021</a>]. In a flat-screen image
                     viewer, a monumental painting and its postcard reproduction appear to be the same
                     size, giving digitized materials a false homogeneity and disregarding the contextual
                     information conveyed by an item's physical dimensions. In sum, flat-screen image
                     viewers are observer-independent interfaces for indirect manipulation of arbitrarily
                     scaled digitized materials. </div>
                  
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20"> In contrast, Booksnake is an observer-dependent interface for embodied interaction
                     with life-size digitized materials. When we encounter physical items, we do so
                     through our bodies, from our individual point of view. Humans are not simply “two eyeballs attached by stalks to a brain computer,” as
                     Catherine D'Ignazio and Lauren Klein write in their discussion of data
                     visceralization [<a class="ref" href="#dignazio_klein_2020">D'Ignazio and Klein 2020</a>, §3]. We strain toward the
                     far corners of maps. We pivot back and forth between the pages of newspapers. We curl
                     ourselves over small objects like daguerreotypes, postcards, or brochures. These
                     embodied, situated, perspectival experiences are inherent to our interactions with
                     physical objects. By using augmented reality to pin life-size digitized items to
                     physical surfaces, Booksnake enables and encourages this kind of embodied
                     exploration. With Booksnake, you can move around an item to see it from all sides,
                     step back to see it in its totality, or get in close to focus on fine details.
                     Integral to Booksnake is Haraway's idea of “the particularity and
                     embodiment of all vision”
                     [<a class="ref" href="#haraway_1988">Haraway 1988</a>, 582]. Put another way, Booksnake lets you break
                     out of the god view and see an object as only you can. As an image viewer with a
                     spatial interface, Booksnake is an argument for a way of seeing that prioritizes
                     embodied interaction with digitized archival materials at real-world size. In this,
                     Booksnake is more than a technical critique of existing flat-screen image viewers.
                     It
                     is also an <span class="hi italic">intellectual</span> critique of how these image viewers
                     foreclose certain types of knowledge creation and interpretation. Booksnake thus
                     offers <span class="hi italic">a new way of looking</span> at digitized materials.</div>
                  
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21"> This is an interpretive choice in our design of Booksnake. Again, image viewers do
                     not simply <span class="hi italic">represent </span>digital image files to a user, they <span class="hi italic">interpret</span> them. Booksnake relies on the same digital image
                     files as do flat-screen image viewers, and these files are just as mediated in
                     Booksnake as they are when viewed in a flat-screen viewer. (“There is no unmediated photograph,” Haraway writes [<a class="ref" href="#haraway_1988">Haraway 1988</a>, 583]. Indeed, Booksnake even displays these image
                     files on the flat screen of a smartphone or tablet — but its use of AR creates the
                     illusion that the object is physically present. Where existing flat-screen image
                     viewers foreground the digital-ness of digitized objects, Booksnake instead recovers
                     and foregrounds their object-ness, their materiality. Drucker writes that “information spaces drawn from a point of view, rather than as if
                     they were observer independent, reinsert the subjective standpoint of their
                     creation”
                     [<a class="ref" href="#drucker_2011">Drucker 2011</a>, ¶20]. Drucker was writing about data
                     visualization, but her point holds for image viewers (which, after all, represent
                     a
                     kind of data visualization). Our design decisions aim to create an interpretive space
                     grounded in individual perspective, with the goal of helping a Booksnake user get
                     closer to the “subjective standpoint” of an archival
                     item's original creators and users. Even as the technology underpinning Booksnake
                     is
                     radically new, it enables methods of embodied looking that are very old, closely
                     resembling in form and substance physical, pre-digitized ways of looking. </div>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  
                  <h1 class="head">INITIAL TESTING RESULTS AND USE CASES</h1>
                  
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22"> Booksnake's emphasis on embodied interaction gives it particular potential as a tool
                     for humanities education. Embodied interaction is a means of accessing situated
                     knowledges [<a class="ref" href="#haraway_1988">Haraway 1988</a>] and is key to apprehending cultural heritage
                     materials in their full complexity. As museum scholars and educators have
                     demonstrated, this is true both for physical objects [<a class="ref" href="#kai-kee_latina_sadoyan_2020">Kai-Kee, Latina, Sadoyan 2020</a>] and for virtual replicas [<a class="ref" href="#kenderdine_yip_2019">Kenderdine and Yip 2019</a>]. Meanwhile, systematic reviews show AR can support
                     student learning gains, motivation, and knowledge transfer [<a class="ref" href="#bacca_etal_2014">Bacca et al. 2014</a>]. By using AR to make embodied interaction possible
                     with digitized items, Booksnake supports student learning through movement,
                     perspective, and scale. For example, a student could use Booksnake to physically
                     follow an explorer's track across a map, watch a painting’s details emerge as she
                     moves closer, or investigate the relationship between a poster’s size and its message
                     — interactions impossible with a flat-screen viewer. Here, we briefly highlight
                     themes that have emerged in user and classroom testing, then discuss potential use
                     cases.</div>
                  
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23"> A common theme in classroom testing was that Booksnake's presentation of life-size
                     virtual objects made students feel like they were closer to digitized sources. A
                     colleague's students used Booksnake as part of an in-class exercise to explore
                     colonial-era Mesoamerican maps and codices, searching for examples of cultural
                     syncretism. In a post-activity survey, students repeatedly described a feeling of
                     presence. Booksnake gave one student “the feeling of actually
                     seeing the real thing up close.” Another student wrote that “I felt that I was actually working with the codex.” A third
                     wrote that “it was cool to see the resource, something I will
                     probably never get to flip through, and get to flip through and examine
                     it.” Students also commented on how Booksnake represented these items'
                     size. One student wrote that Booksnake “gave me the opportunity
                     to get a better idea of the scale of the pieces.” Another wrote that “I liked being able to see the material ‘physically’ and see the
                     scale of the drawings on the page to see what they emphasized and how they took up
                     space.” These comments suggest that Booksnake has the most potential to
                     support embodied learning activities that ask students to engage with an item's
                     physical features (such as an object's size or proportions), or with the relationship
                     between these features and the item's textual and visual content.</div>
                  
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24"> During one-on-one user testing sessions, two history Ph.D. students each separately
                     described feeling closer to digitized sources. One tester described Booksnake as
                     opening “a middle ground” between digital and physical
                     documents by offering the “flexibility” of digital
                     materials, but the “sensorial experience of closeness”
                     with archival documents. The other tester said that Booksnake brought “emotional value” to archival materials. “Being able to stand up and lean over it [the object] brought it to life a little
                     more,” this tester said. “You can’t assign research
                     utility to that, but it was more immersive and interactive, and in a way
                     satisfying.” Their comments suggest that Booksnake can enrich engagement
                     with digitized materials, especially by producing the feeling of physical
                     presence.</div>
                  
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25"> As a virtualization technology, Booksnake makes it possible to present archival
                     materials in new contexts, beyond the physical site of the archive itself. Jeremy
                     Bailenson argues that virtualization technologies are especially effective for
                     scenarios that would otherwise be rare, impractical, destructive, or expensive [<a class="ref" href="#bailenson_2018">Bailenson 2018</a>]. Here, we use these four characteristics to describe
                     potential Booksnake use cases. First, it is rare to have an individual, embodied
                     interaction with one-of-a-kind materials, especially for people who do not work at
                     GLAM institutions. A key theme in student comments from the classroom testing
                     described above, for example, was that Booksnake gave students a feeling of direct
                     engagement with unique Mesoamerican codices. Second, it is often logistically
                     impractical for a class to travel to an archive (especially for larger classes) or
                     to
                     bring archival materials into classrooms outside of library or museum buildings. The
                     class described above, for example, had around eighty students, and Booksnake made
                     it
                     possible for each student to individually interact with these codices, during
                     scheduled class time and in their existing classrooms. Third, the physical fragility
                     and the rarity or uniqueness of many archival materials typically limits who can
                     handle them or where they can be examined. Booksnake makes it possible to engage with
                     virtual archival materials in ways that would cause damage to physical originals.
                     For
                     example, third-grade students could use Booksnake to explore a historic painting by
                     walking atop a virtual copy placed on their classroom floor, or architectural
                     historians could use Booksnake to bring virtual archival blueprints into a physical
                     site. Finally, it is expensive (in both money and time) to physically bring together
                     archival materials that are held by two different institutions. With Booksnake, a
                     researcher could juxtapose digitized items held by one institution with physical
                     items held by a different institution, for purposes of comparison (such as comparing
                     a preparatory sketch to a finished painting) or reconstruction (such as reuniting
                     manuscript pages that had been separated). In each of these scenarios, Booksnake's
                     ability to produce virtual replicas of physical objects lowers barriers to embodied
                     engagement with archival materials and opens new possibilities for research,
                     teaching, and learning. </div>
                  </div>
               
               
               
               <div class="div div0">
                  
                  
                  <h1 class="head">IMMERSIVE TECHNOLOGIES FOR CULTURAL HERITAGE</h1>
                  
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26"> Booksnake is an empty frame. It leverages AR to extend the exploratory freedom that
                     users associate with browsing online collections into physical space. In doing so,
                     Booksnake joins a small collection of digital tools and projects using immersive
                     technologies as the basis for interacting with cultural heritage materials and
                     collections.</div>
                  
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27"> A dedicated and technically adept user could use 3D modeling software (such as
                     Blender, Unity, Maya, or Reality Composer) to manually transform digital images into
                     virtual objects. This can be done with any digital image, but is time-intensive and
                     breaks links between images and metadata. Booksnake automates this process, making
                     it
                     more widely accessible, and preserves metadata, supporting humanistic inquiry.</div>
                  
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28"> The Google Arts &amp; Culture app offers the most comparable use of humanistic AR.
                     Like Booksnake, Arts &amp; Culture is a mobile app for smartphones and tablets that
                     offers AR as an interface for digitized materials. A user can activate the app’s
                     “Art Projector” feature to “display
                     life-size artworks, wherever you are” by placing a digitized artwork in
                     their physical surroundings [<a class="ref" href="#luo_2019">Luo 2019</a>]. But there are three key
                     differences between Google’s app and Booksnake. First, AR is one of many possible
                     interaction methods in Google’s app, which is crowded with stories, exhibits, videos,
                     games, and interactive experiences. In contrast, Booksnake emphasizes AR as its
                     primary interface, foregrounding the embodied experience. Second, Google’s app
                     focuses on visual art (such as paintings and photographs), while Booksnake can
                     display a broader range of archival and cultural heritage materials, making it more
                     useful for humanities scholars. (Booksnake can also display paginated materials, as
                     we discuss further below, while Google's app cannot.) Finally — and most importantly
                     — Google’s app relies on a centralized database model. Google requires participating
                     institutions to upload their collection images and metadata to Google’s own servers,
                     so that Google can format and serve these materials to users [<a class="ref" href="#google_n.d.">Google n.d.</a>]. In contrast, Booksnake’s use of IIIF enables institutions
                     to retain control over their digitized collections and expands the capabilities of
                     an
                     open humanities software ecosystem.</div>
                  
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">Another set of projects approach immersive technologies as tools for designing and
                     delivering exhibition content. Some use immersive technologies to enhance physical
                     exhibits, such as Veholder, a project exploring technologies and methods for
                     juxtaposing 3D virtual and physical objects in museum settings [<a class="ref" href="#haynes_2018">Haynes 2018</a>], [<a class="ref" href="#haynes_2019">Haynes 2010</a>]. Others are tools for using
                     immersive technologies to create entirely virtual spaces. Before its discontinuation,
                     many GLAM institutions and artists adapted Mozilla Hubs, a general-purpose tool for
                     building 3D virtual spaces that could be accessed using a flat-screen Web browser
                     or
                     a VR headset, to build virtual exhibition spaces, although users were required to
                     manually import digitized materials and construct virtual replicas [<a class="ref" href="#cool_2022">Cool 2022</a>]. Another project, Diomira Galleries, is a prototype tool for
                     building VR exhibition spaces with IIIF-compliant resources [<a class="ref" href="#bifrost_2023">Bifrost Consulting Group 2023</a>]. Like Booksnake, Diomira uses IIIF to import digital
                     images of archival materials, but Diomira arbitrarily scales these images onto
                     template canvases that do not correspond to an item’s physical dimensions. As with
                     Booksnake, these projects demonstrate the potential of immersive technologies for
                     new
                     research interactions and collection activation with digitized archival
                     materials.</div>
                  
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">Finally, we are building Booksnake as an AR application for existing consumer mobile
                     devices as a way of lowering barriers to immersive engagement with cultural heritage
                     materials. Most VR projects require expensive special-purpose headsets, which has
                     limited adoption and access [<a class="ref" href="#greengard_2019">Greengard 2019</a>]. In contrast, AR-capable
                     smartphones are ubiquitous, enabling a mobile app to tap the potential of a large
                     existing user base, and positioning such an app to potentially mitigate racial and
                     socioeconomic digital divides in the United States. More Americans own smartphones
                     than own laptop or desktop computers [<a class="ref" href="#pew_2021">Pew Research Center 2021</a>]. And while Black and
                     Hispanic adults in the United States are less likely to own a laptop or desktop
                     computer than white adults, Pew researchers have found “no
                     statistically significant racial and ethnic differences when it comes to
                     smartphone or tablet ownership”
                     [<a class="ref" href="#atske_perrin_2021">Atske and Perrin 2021</a>]. Similarly, Americans with lower household incomes
                     are more likely to rely on smartphones for Internet access [<a class="ref" href="#vogels_2021">Vogels 2021</a>]. Smartphones are thus a key digital platform for engaging and including the
                     largest and most diverse audience. Developing an Android version of Booksnake will
                     enable us to more fully deliver on this potential. </div>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  
                  <h1 class="head">AUTOMATICALLY TRANSFORMING DIGITAL IMAGES INTO VIRTUAL OBJECTS</h1>
                  
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">Booksnake’s central technical innovation is automatically transforming existing
                     digital images of archival cultural heritage materials into dimensionally-accurate
                     virtual objects. To make this possible, Booksnake connects existing software
                     frameworks in a new way. </div>
                  
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">First, Booksnake uses IIIF to download images and metadata.<a class="noteRef" href="#d4e737">[3]</a> IIIF was proposed in 2011 and developed over the
                     early 2010s. Today, the IIIF Consortium is composed of sixty-five global GLAM
                     institutions, from the British Library to Yale University [<a class="ref" href="#iiif_n.d.">International Image Interoperability Framework n.d.</a>],
                     while dozens more institutions offer access to their collections through IIIF because
                     several common digital asset management (DAM) platforms, including CONTENTdm, LUNA,
                     and Orange DAM, support IIIF [<a class="ref" href="#oclc_n.d.">OCLC n.d.</a>]
                     [<a class="ref" href="#l_i_2018">Luna 2018</a>]
                     [<a class="ref" href="#orangelogic_n.d.">orangelogic n.d.</a>]. This widespread use of IIIF means that Booksnake
                     is readily compatible with many existing digitized collections. By using IIIF,
                     Booksnake embraces and extends the capabilities of a robust humanities software
                     ecosystem. By demonstrating a novel method to transform existing IIIF-compliant
                     resources for interaction in augmented reality, we hope that Booksnake will drive
                     wider IIIF adoption and standardization. </div>
                  
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">Next, Booksnake uses a pair of Apple software frameworks, ARKit and RealityKit.
                     ARKit, introduced in 2017, interprets and synthesizes data from an iPhone or iPad's
                     cameras and sensors to understand a user's physical surroundings and to anchor
                     virtual objects to horizontal and vertical surfaces [<a class="ref" href="#apple_n.d._a">Apple n.d._a</a>].
                     RealityKit, introduced in 2019, is a framework for rendering and displaying virtual
                     objects, as well as managing a user's interactions with them (for example, by
                     interpreting a user’s on-screen touch gestures) [<a class="ref" href="#apple_n.d._b">Apple n.d._b</a>]. Both
                     ARKit and RealityKit are built into the device operating system, enabling us to rely
                     on these frameworks to create virtual objects and to initiate and manage AR sessions.
                     </div>
                  
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">Developing Booksnake as a native mobile app, rather than a Web-based tool, makes it
                     possible for Booksnake to take advantage of the powerful camera and sensor
                     technologies in mobile devices. We are developing Booksnake’s first version for
                     iPhone and iPad because Apple’s tight integration of hardware and software supports
                     rapid AR development and ensures consistency in user experience across devices. We
                     plan to extend our work by next developing an Android version of Booksnake, improving
                     accessibility. Another development approach, WebXR, a cross-platform Web-based AR/VR
                     framework currently in development, lacks the features to support our project
                     goals.</div>
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure03.pdf" rel="external"><img src="resources/images/figure03.pdf" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 3. </div>This overview summarizes Booksnake's technical pipeline. Booksnake uses the
                        International Image Interoperability Framework (IIIF) to download item metadata
                        and digital images, then uses Apple's RealityKit to construct a custom virtual
                        object, and finally uses Apple's ARKit to display the custom virtual object in
                        physical space.</div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">Booksnake thus links IIIF with RealityKit and ARKit to produce a novel result: an
                     on-demand pipeline for automatically transforming existing digital images of archival
                     materials into custom virtual objects that replicate the physical original’s
                     real-world proportions, dimensions, and appearance, as well as an AR interface for
                     interacting with these virtual objects in physical space. (See figure 3.) How does
                     Booksnake do this?</div>
                  
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">Booksnake starts with the most humble of Internet components: the URL. A Booksnake
                     user first searches and browses an institution’s online catalog through an in-app
                     Web
                     view. Booksnake offers an “Add” button on catalog pages for individual items.
                     When the user taps this button to add an item to their Booksnake library, Booksnake
                     retrieves the item page’s URL. Because of Apple’s privacy restrictions and
                     application sandboxing, this is the only information that Booksnake can read from
                     a
                     given Web page; it cannot directly access content on the page itself. Instead,
                     Booksnake translates the item page URL into the corresponding IIIF manifest URL.</div>
                  
                  
                  <div class="counter"><a href="#p37">37</a></div>
                  <div class="ptext" id="p37">An IIIF manifest is a JSON file — a highly structured, computer-readable text file
                     —
                     that contains a version of the item’s catalog record, including metadata and URLs
                     for
                     associated images. The exact URL translation process varies depending on how an
                     institution has implemented IIIF, but in many cases it is as simple as appending
                     <span class="monospace">"/manifest.json"</span> to the item URL. For example, the item URL for the
                     Library of Congress’s 1858 “Chart of the submarine Atlantic
                     Telegraph,” is <a href="https://www.loc.gov/item/2013593216/" onclick="window.open('https://www.loc.gov/item/2013593216/'); return false" class="ref">https://www.loc.gov/item/2013593216/</a>, and the item’s IIIF manifest URL is
                     <a href="https://www.loc.gov/item/2013593216/manifest.json" onclick="window.open('https://www.loc.gov/item/2013593216/manifest.json'); return false" class="ref">https://www.loc.gov/item/2013593216/manifest.json</a>. In other cases,
                     Booksnake may extract a unique item identifier from the item URL, then use that
                     unique identifier to construct the appropriate IIIF manifest URL. Booksnake then
                     downloads and parses the item’s IIIF manifest.</div>
                  
                  
                  <div class="counter"><a href="#p38">38</a></div>
                  <div class="ptext" id="p38">First, Booksnake extracts item metadata from the IIIF manifest. Booksnake uses this
                     metadata to construct an item page in the app’s Library tab, enabling a user to view
                     much of the same item-level metadata visible in the host institution’s online
                     catalog. An IIIF manifest presents metadata in key-value pairs, with each pair
                     containing a general label (or key) and a corresponding entry (or value). For
                     example, the IIIF manifest for the 1858 Atlantic telegraph map mentioned above
                     contains the key “Contributors”, representing the catalog-level field listing an
                     item’s authors or creators, and the corresponding item-level value “Barker, Wm. J. (William J.) (Surveyor),” identifying the
                     creator of this specific item. Importantly, while the key-value pair structure is
                     generally consistent across IIIF manifests from different institutions, the key names
                     themselves are not. The "Contributors" key at one institution may be named
                     “Creators” at another institution, and “Authors” at a third. The current
                     version of Booksnake simply displays the key-value metadata as provided in the IIIF
                     manifest. As Booksnake adds support for additional institutions, we plan to identify
                     and link different keys representing the same metadata categories (such as
                     “Contributors,”
                     “Creators,” and “Authors”). This will enable users, for example, to sort
                     items from different institutions by common categories like “Author” or “Date
                     created,” or to search within a common category.</div>
                  
                  
                  <div class="counter"><a href="#p39">39</a></div>
                  <div class="ptext" id="p39">Second, Booksnake uses image URLs contained in the IIIF manifest to download the
                     digital images (typically JPEG or JPEG 2000 files) associated with an item's catalog
                     record. Helpfully, IIIF image URLs are structured so that certain requests — like
                     the
                     image’s size, its rotation, even whether it should be displayed in color or
                     black-and-white — can be encoded in the URL itself. Booksnake leverages this
                     affordance to request images that are sufficiently detailed for virtual object
                     creation, which sometimes means requesting images larger than what the institution
                     serves by default. For example, Library of Congress typically serves images that are
                     25% of the original size, but Booksnake modifies the IIIF image URL to request images
                     at 50% of original size. Our testing indicates that this level of resolution produces
                     sufficiently detailed virtual objects, without visible pixelation, for Library of
                     Congress collections.<a class="noteRef" href="#d4e815">[4]</a> We
                     anticipate customizing this value for other institutions. Having downloaded the
                     item’s metadata and digital images, Booksnake can now create a virtual object
                     replicating the physical original. </div>
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure04.pdf" rel="external"><img src="resources/images/figure04.pdf" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 4. </div>These tables show the relationship between an item's physical dimensions, its
                        digitization resolution, and the pixel dimensions of the resulting digital image.
                        Pixel dimensions are the product of the item’s physical dimensions and its
                        digitization resolution, and can vary considerably depending on the resolution at
                        which it is digitized. As figure 3a shows, a 10 inch by 10 inch item digitized at
                        300 ppi would produce a 3,000 by 3,000 pixel digital image (10 x 300 = 3,000). The
                        same item digitized at 600 ppi—twice the resolution—would produce a digital image
                        measuring 6,000 by 6,000 pixels (10 x 600 = 6,000). The same item digitized at
                        1,200 ppi would produce a digital image measuring 12,000 by 12,000 pixels (10 x
                        1,200 = 12,000). Importantly, there is not a consistent relationship between pixel
                        dimensions and physical dimensions, as figure 3b shows. Digitization is a one-way
                        street: Without knowing an item’s digitization resolution, it is not
                        mathematically possible to convert pixel dimensions back into physical dimensions.
                        A 6000 x 6000 pixel image may represent a 20 x 20 inch item digitized at 300 ppi,
                        or a 10 x 10 inch item digitized at 600 ppi, or a 5 x 5 inch item digitized at
                        1,200 ppi. For this reason, Booksnake requires either an item’s physical
                        dimensions, or both the item’s pixel dimensions and its digitization resolution. </div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p40">40</a></div>
                  <div class="ptext" id="p40">Our initial goal was for Booksnake to display any image resource available through
                     IIIF, but we quickly discovered that differences in how institutions record
                     dimensional metadata meant that we would have to adapt Booksnake to different
                     digitized collections. To produce size-accurate virtual objects, Booksnake requires
                     <span class="hi italic">either</span> an item’s physical dimensions in a computer-readable
                     format, or <span class="hi italic">both </span>the item’s pixel dimensions and its
                     digitization resolution, from which it can calculate the item's physical dimensions
                     (see figures 4a and 4b). Institutions generally take one of three approaches to
                     providing dimensional metadata.</div>
                  
                  
                  <div class="counter"><a href="#p41">41</a></div>
                  <div class="ptext" id="p41">The simplest approach is to list an item’s physical dimensions as metadata in its
                     IIIF manifest. Some archives, such as the David Rumsey Map Collection, provide
                     separate fields for an item’s height and width, each labeled with units of measure.
                     This formatting provides the item’s dimensions in a computer-readable format, making
                     it straightforward to create a virtual object of the appropriate size. Alternatively,
                     an institution may use the IIIF Physical Dimension service, an optional service that
                     provides the scale relationship between an item’s physical and pixel dimensions,
                     along with the physical units of measure [<a class="ref" href="#iiif_2015">International Image Interoperability Framework 2015</a>]. But we are
                     unaware of any institution that has implemented this service for its
                     collections.<a class="noteRef" href="#d4e841">[5]</a>
                     </div>
                  
                  
                  <div class="counter"><a href="#p42">42</a></div>
                  <div class="ptext" id="p42">A more common approach is to provide an item’s physical dimensions in a format that
                     is not immediately computer-readable. The Huntington Digital Library, for example,
                     typically lists an item’s dimensions as part of a text string in the “physical
                     description” field. <a href="https://hdl.huntington.org/digital/collection/p9539coll1/id/12262/rec/2" onclick="window.open('https://hdl.huntington.org/digital/collection/p9539coll1/id/12262/rec/2'); return false" class="ref">This c.1921 steamship poster</a>, for example, is described as: “Print ; image 60.4 x 55 cm (23 3/4 x 21 5/8 in.) ; overall 93.2 x 61
                     cm (36 11/16 x 24 in.)” [spacing <span class="hi italic">sic</span>]. To interpret
                     this text string and convert it into numerical dimensions, a computer program like
                     Booksnake requires additional guidance. Which set of dimensions to use, “image”
                     or “overall”? Which units of measure, centimeters or inches? And what if the
                     string includes additional descriptors, such as “folded” or “unframed”? We
                     are currently collaborating with the Huntington to develop methods to parse
                     dimensional metadata from textual descriptions, with extensibility to other
                     institutions and collections.</div>
                  
                  
                  <div class="counter"><a href="#p43">43</a></div>
                  <div class="ptext" id="p43">Finally, an institution may not provide <span class="hi italic">any</span> dimensional
                     metadata in its IIIF manifests. This is the case with the Library of Congress (LOC),
                     which lists physical dimensions, where available, in an item’s catalog record, but
                     does not provide this information in the item’s IIIF manifest.<a class="noteRef" href="#d4e878">[6]</a> This presented us
                     with a problem: How to create dimensionally-accurate virtual objects without the
                     item’s physical dimensions? After much research and troubleshooting, we hit upon a
                     solution. We initially dismissed pixel dimensions as a source of dimensional metadata
                     because there is no consistent relationship between physical and pixel dimensions.
                     And yet, during early-stage testing, Booksnake consistently created life-size virtual
                     maps from LOC, even as items in other LOC collections resulted in virtual objects
                     with wildly incorrect sizing. This meant there <span class="hi italic">was</span> a
                     relationship, at least for one collection — we just had to find it. </div>
                  
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">LOC digitizes items to FADGI standards, which specify a digitization resolution for
                     different item types. For example, FADGI standards specify a target resolution of
                     600
                     ppi for prints and photographs, and 400 ppi for books.<a class="noteRef" href="#d4e891">[7]</a> We then discovered that
                     LOC scales down item images in some collections. (For example, map images are scaled
                     down by a factor of four.) We then combined digitization resolution and scaling
                     factors for each item type into a pre-coded reference table. When Booksnake imports
                     an LOC item, it consults the item’s IIIF manifest to determine the item type, then
                     consults the reference table to determine the appropriate factor for converting the
                     item’s pixel dimensions to physical dimensions. This solution is similar to a
                     client-side version of the IIIF Physical Dimension service, customized to LOC’s
                     digital collections. </div>
                  
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">As this discussion suggests, determining the physical dimensions of a digitized item
                     is a seemingly simple problem that can quickly become complicated. Developing robust
                     methods for parsing different types of dimensional metadata is a key research area
                     because these methods will allow us to expand the range of institutions and materials
                     with which Booksnake is compatible. While IIIF makes it straightforward to access
                     digitized materials held by different institutions, the differences in how each
                     institution presents dimensional metadata mean that we will currently have to adapt
                     Booksnake to use each institution's metadata schema.<a class="noteRef" href="#d4e897">[8]</a>
                     </div>
                  
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">Booksnake then uses this information to create virtual objects on demand. When a user
                     taps “View in Your Space” to initiate an AR session, Booksnake uses RealityKit
                     to transform the item’s digital image into a custom virtual object suitable for
                     interaction in physical space. First, Booksnake creates a blank two-dimensional
                     virtual plane sized to match the item’s physical dimensions. Next, Booksnake applies
                     the downloaded image to this virtual plane as a texture, scaling the image to match
                     the size of the plane. This results in a custom virtual object that matches the
                     original item’s physical dimensions, proportions, and appearance. This process is
                     invisible to the user — Booksnake “just works.” This process is straightforward
                     for flat digitized items like maps or posters, which are typically digitized from
                     a
                     single perspective, of their recto (front) side.</div>
                  
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">Booksnake’s virtual object creation process is more complex for compound objects,
                     which have multiple images linked to a single item record. Compound objects can
                     include books, issues of periodicals or newspapers, diaries, scrapbooks, photo
                     albums, and postcards. The simplest compound objects, such as postcards, have two
                     images showing the object’s recto (front) and verso (back) sides. Nineteenth-century
                     newspapers may have four or eight pages, while books may run into hundreds of pages,
                     with each page typically captured and stored as a single image. </div>
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure05.jpeg" rel="external"><img src="resources/images/figure05.jpeg" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 5. </div>This and the following figures illustrate how Booksnake constructs virtual
                        compound objects from multiple individual virtual objects. In each figure, the top
                        image shows an annotated screenshot of the live camera view, and the bottom images
                        show what images are stored in device memory. First, Booksnake converts the item's
                        first image into a virtual object (indicated with the red arrow), and creates a
                        matching blank (untextured) virtual object, which acts as an invisible "page zero"
                        (indicated with the yellow dashed line). Booksnake places the invisible "page
                        zero" object to the left of the page one object, and links the two objects, as if
                        along a central spine.</div>
                  </div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure06.jpeg" rel="external"><img src="resources/images/figure06.jpeg" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 6. </div>When a user turns the page of a virtual compound object, Booksnake loads the
                        next two item images and converts them into individual virtual objects. Booksnake
                        aligns the page two object (indicated with the light blue arrow) on the reverse
                        side of the page one object (indicated with the red arrow) and positions the page
                        three object (indicated with the dark blue arrow) directly beneath the page one
                        object. Booksnake then animates a page turn by rotating pages one and two
                        together, over one hundred and eighty degrees. </div>
                  </div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     
                     <div class="ptext"><a href="resources/images/figure07.jpeg" rel="external"><img src="resources/images/figure07.jpeg" style="width: 700px" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 7. </div>When the animation is complete, Booksnake has replaced the original virtual
                        objects representing pages zero and one with the new virtual objects representing
                        pages two and three (indicated with the light blue and dark blue arrows,
                        respectively), and discards the virtual objects representing page zero and page
                        one. This on-demand virtual object creation process optimizes for memory and
                        performance.</div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">Booksnake handles compound objects by creating multiple individual virtual objects,
                     one for each item image, then arranging and animating these objects to support the
                     illusion of a cohesive compound object. Our initial implementation is modeled on a
                     generic paginated codex, with multiple pages around a central spine. As with flat
                     objects, this creation process happens on demand, when a user starts an AR session.
                     Booksnake uses a compound object’s first image as the virtual object’s cover or first
                     page (more on this below). Booksnake creates a virtual object for this first image,
                     then creates a matching invisible virtual object, which acts as an invisible “page
                     zero” (see figure 5). The user sees the first page of a newspaper, for example,
                     sitting and waiting to be opened. When the user swipes from right to left across the
                     object edge to "turn" the virtual page, Booksnake retrieves the next two images,
                     transforms them into virtual objects representing pages two and three, then animates
                     a page turn with the object’s “spine” serving as the rotation axis (see figure
                     6). Once the animation is complete, pages two and three have replaced the invisible
                     “page zero” and page one, and Booksnake discards those virtual objects (see
                     figure 7). This on-demand process means that Booksnake only needs to load a maximum
                     of four images into AR space, optimizing for memory and performance. By using a swipe
                     gesture to turn pages, Booksnake leverages a navigation affordance with which users
                     are already familiar from interactions with physical paginated items, supporting
                     immersion and engagement. The page-turn animation, paired with a page-turn sound
                     effect, further enhances the realism of the virtual experience.</div>
                  
                  
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">A key limitation in our initial approach to creating virtual paginated objects is
                     that our generic codex model is based on one particular type of object, the
                     newspaper. Specifically, we used LOC's Chronicling America collection of historical
                     newspapers as a testbed to develop the pipeline for creating virtual paginated
                     objects, as well as the user interface and methods for interacting with virtual
                     paginated objects in physical space. While the newspaper is broadly representative
                     of
                     the codex form's physical features and affordances, making it readily extensible to
                     other paginated media, there are important differences in how different media are
                     digitized and presented in online collections. For example, while Chronicling America
                     newspapers have one page per image, some digitized books held by LOC have two pages
                     per image. We have adapted Booksnake's object creation pipeline to identify
                     double-page images, split the image in half, and wrap each resulting image onto
                     individual facing pages.<a class="noteRef" href="#d4e953">[9]</a> There are also important
                     cultural differences in codex interaction methods: We plan to further extend the
                     capabilities of this model by building support for IIIF's "right-to-left" reading
                     direction flag, which will enable Booksnake to correctly display paginated materials
                     in languages like Arabic, Chinese, Hebrew, and Japanese.</div>
                  
                  
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">A further limitation of our initial approach is our assumption that all compound
                     objects represent paginated material in codex form, which is not the case. Booksnake
                     cannot yet realistically display items like scrolls or Mesoamerican screenfold books
                     (which are often digitized by page, but differ in physical construction and
                     interaction methods from Western books). In other cases, a compound object comprises
                     a collection of individual items that are stored together but not physically bound
                     to
                     each other. One example is <a href="https://www.loc.gov/item/pldec.110/" onclick="window.open('https://www.loc.gov/item/pldec.110/'); return false" class="ref">“Polish Declarations of Admiration and Friendship for the United
                        States”</a>, held by LOC, which consists of 181 unbound sheets. Or an
                     institution may use a compound object to store multiple different images of a single
                     object. For example, the Yale Center for British Art (YCBA) often provides several
                     images for paintings in its collections, as with<a href="https://collections.britishart.yale.edu/catalog/tms:52309" onclick="window.open('https://collections.britishart.yale.edu/catalog/tms:52309'); return false" class="ref"> the 1769
                        George Stubbs painting “Water Spaniel”.</a> YCBA
                     provides four images: of the framed painting, the unframed painting, the unframed
                     image cropped to show just the canvas, and the framed painting’s verso side. We plan
                     to continue refining Booksnake’s compound object support by using collection- and
                     item-level metadata to differentiate between paginated and non-paginated compound
                     objects, in order to realistically display different types of materials.</div>
                  
                  
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">Having constructed a custom virtual object, Booksnake next opens the live camera view
                     so that the user can interact with the object in physical space. As the user moves
                     their device to scan their surroundings, Booksnake overlays a transparent image of
                     the object, outlined in red, over horizontal and vertical surfaces, giving the user
                     a
                     preview of how the object will fit in their space. Tapping the transparent preview
                     anchors the object, which turns opaque, to a physical surface. As the user and device
                     move, Booksnake uses camera and sensor data to maintain the object's relative
                     location in physical space, opening the object to embodied exploration. The user can
                     crouch down and peer at the object from the edge of the table, pan across the
                     object's surface, or zoom into the key details by moving closer to the object. The
                     user can also enlarge or shrink an object by using the familiar pinch-to-zoom
                     gesture—although in this case, the user is not zooming in or out, but re-scaling the
                     virtual object itself. The object remains in place even when out of view of the
                     device: A user can walk away from an anchored object, then turn back to look at it
                     from across the room. </div>
                  
                  
                  <div class="counter"><a href="#p52">52</a></div>
                  <div class="ptext" id="p52">Again, an object viewed with Booksnake is just as mediated as one viewed in a
                     traditional Web-based viewer. Throughout the development process, we have repeatedly
                     faced interpretive questions at the intersection of technology and the humanities.
                     One early question, for example: Should users be allowed to re-scale objects? The
                     earliest versions of Booksnake lacked this affordance, to emphasize that a given
                     virtual object was presented at life size. But one of our advisory board members,
                     Philip J. Ethington, argued that digital technology is powerful because it can enable
                     interactions that aren't possible in the real world. And so we built a way for users
                     to re-scale objects by pinching them, while also displaying a changing percentage
                     indicator to show users how much larger or smaller the virtual object is when
                     compared to the original. In approaching this and similar questions, our goal is for
                     virtual objects to behave in realistic and familiar ways, to closely mimic the
                     experience of embodied interaction with physical materials.</div>
                  </div>
               
               
               
               <div class="div div0">
                  
                  
                  <h1 class="head">CONCLUSION</h1>
                  
                  
                  <div class="counter"><a href="#p53">53</a></div>
                  <div class="ptext" id="p53">“What kind of interface exists after the screen goes
                     away?” asks Johanna Drucker. “I touch the surface of my
                     desk and it opens to the library of the world? My walls are display points,
                     capable of offering the inventory of masterworks from the world’s museums and
                     collections into view?”
                     [<a class="ref" href="#drucker_2014">Drucker 2014</a>, 195]. These questions point toward spatial
                     interfaces, and this is what Booksnake is building toward. Futurists and
                     science-fiction authors have long positioned virtual reality as a means of
                     transporting users away from their day-to-day humdrum reality into persistent
                     interactive three-dimensional virtual worlds, sometimes called the metaverse [<a class="ref" href="#gibson_1984">Gibson 1984</a>], [<a class="ref" href="#stephenson_1992">Stephenson 1992</a>], [<a class="ref" href="#cline_2011">Cline 2011</a>], [<a class="ref" href="#ball_2022">Ball 2022</a>]. Booksnake takes a different
                     approach, bringing life-size virtual representations of real objects into a user's
                     physical surroundings for embodied interaction. The recent emergence of mixed-reality
                     headsets, such as the Apple Vision Pro, only underscores the potential of spatial
                     computing for humanities work, especially for affordances like hands-on interaction
                     with virtual objects.</div>
                  
                  
                  <div class="counter"><a href="#p54">54</a></div>
                  <div class="ptext" id="p54">The first version of Booksnake represents a proof of concept, both of the pipeline
                     to
                     transform digitized items into virtual objects and of AR as an interface for
                     research, teaching, and learning with digitized archival materials. In addition to
                     researching development of an Android version, our next steps are to refine
                     Booksnake's object creation pipeline and AR interface. One major research focus is
                     improving how Booksnake identifies, ingests, and interprets different types of
                     dimensional metadata, to support as many different metadata schemas and existing
                     online collections as possible. Another research focus is expanding the range of
                     interactions possible with virtual objects — for example, making it possible for
                     users to annotate a virtual object with text or graphics, or to adjust the
                     transparency of a virtual object (to better support comparison between virtual and
                     physical items). In the longer term, once the IIIF Consortium finalizes protocols
                     for
                     3D archival data and metadata [<a class="ref" href="#iiifc_2022">IIIF Consortium 2022</a>], we anticipate that future
                     versions of Booksnake will be able to download and display 3D objects. Ongoing user
                     testing and feedback will further inform Booksnake's continuing development. </div>
                  
                  
                  <div class="counter"><a href="#p55">55</a></div>
                  <div class="ptext" id="p55">Our technical work enables Booksnake users to bring digitized cultural heritage
                     materials out of flat screens and onto their desks and walls, using a tool — the
                     consumer smartphone or tablet — that they already own. Meanwhile, our conceptual work
                     to construct size-accurate virtual objects from existing images and available
                     metadata will help to make a large, culturally significant, and ever-expanding body
                     of digitized materials available for use in other immersive technology projects where
                     the dimensional accuracy of virtual objects is important, such as virtual museum
                     exhibits or manuscript viewers. To put it another way, it’s taken the better part
                     of
                     three decades to <span class="hi italic">digitize</span> millions of cultural heritage
                     materials — and that's barely the tip of the iceberg. How much more time will it take
                     <span class="hi italic">virtualize</span> these cultural heritage materials, that is, to
                     create accurate virtual replicas? Booksnake's automatic transformation method offers
                     a way to repurpose existing resources toward this goal. </div>
                  
                  
                  <div class="counter"><a href="#p56">56</a></div>
                  <div class="ptext" id="p56">We’re building Booksnake to enable digital accessibility and connect more people with
                     primary sources. At its core, Booksnake is very simple. It is a tool for transforming
                     digital images of cultural heritage materials into life-size virtual objects. In
                     doing so, Booksnake leverages augmented reality as a means of interacting with
                     archival materials that is radically new, but that feels deeply familiar. Booksnake
                     is a way to virtualize the reading room or museum experience — and thereby
                     democratize it, making embodied interaction with cultural heritage materials in
                     physical space more accessible to more people in more places.</div>
                  </div>
               
               
               
               
               
               
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e472"><span class="noteRef lang en">[1]  Sean Fraga
                     researched and designed the project's technical architecture and user interface
                     affordances; Fraga also prepared the article manuscript. Christy Ye developed a
                     method for transforming digital images into virtual objects. Samir Ghosh ideated
                     and refined Booksnake's UI/UX fundamentals. Henry Huang built a means of
                     organizing and persistently storing item metadata and digital images using Apple’s
                     Core Data framework. Fraga developed and Huang implemented a method for creating
                     size-accurate virtual objects from Library of Congress metadata. Fraga, Ye, and
                     Huang jointly designed support for compound objects, and Ye and Huang collaborated
                     to implement support for compound objects; this work was supported by an NEH
                     Digital Humanities Advancement Grant (HAA-287859-22). Zack Sai is building support
                     for the new IIIF v3.0 APIs. Michael Hughes is expanding the range of interactions
                     possible with virtual objects in AR. Siyu (April) Yao is building technical links
                     to additional digitized collections. Additionally, Peter Mancall serves as senior
                     advisor to the project. Curtis Fletcher provides strategic guidance, and Mats
                     Borges provides guidance on user testing and feedback.</span></div>
               <div class="endnote" id="d4e511"><span class="noteRef lang en">[2]  We
                     focus here on the decisions involved in digitizing an item itself, rather than the
                     decisions about what items get digitized and why, which also, of course, inform
                     the development and structure of a given digitized collection.</span></div>
               <div class="endnote" id="d4e737"><span class="noteRef lang en">[3]  The first version
                     of Booksnake supports the IIIF v2.1 APIs. We are currently building support for
                     the new IIIF v3.0 APIs.</span></div>
               <div class="endnote" id="d4e815"><span class="noteRef lang en">[4]  To avoid pixelated virtual objects, the host institution
                     must serve through IIIF a sufficiently high-resolution digital image.</span></div>
               <div class="endnote" id="d4e841"><span class="noteRef lang en">[5]  David Newberry's “IIIF for Dolls”
                     web tool makes inventive use of the IIIF Physical Dimension service to rescale
                     digitized materials [<a class="ref" href="#newberry_2023">Newberry 2023</a>].</span></div>
               <div class="endnote" id="d4e878"><span class="noteRef lang en">[6]  Library of
                     Congress uses MARC data to construct IIIF manifests, but this is sometimes a
                     “lossy” process [<a class="ref" href="#woodward_2021">Woodward 2021</a>].</span></div>
               <div class="endnote" id="d4e891"><span class="noteRef lang en">[7]  Both ppi requirements
                     are for FADGI’s highest performance level, 4 Star.</span></div>
               <div class="endnote" id="d4e897"><span class="noteRef lang en">[8]  More information about
                     Booksnake's metadata requirements is available at <a href="https://booksnake.app/glam/" onclick="window.open('https://booksnake.app/glam/'); return false" class="ref">https://booksnake.app/glam/</a>.</span></div>
               <div class="endnote" id="d4e953"><span class="noteRef lang en">[9]  Another example: For some digitized books held by LOC,
                     the first image is of the book spine, not the book cover. Booksnake incorporates
                     logic to identify and ignore book spine images.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="d4e1632"><!-- close --></span></div>
               <div class="bibl"><span class="ref" id="almas_etal_2018"><!-- close -->Almas et al. 2018</span>  Almas, Bridget, et al. (2018)
                  “Manuscript Study in Digital Spaces: The State of the Field
                  and New Ways Forward”, <cite class="title italic">Digital Humanities
                     Quarterly</cite>, 12(2). Available at: <a href="http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html" onclick="window.open('http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html'); return false" class="ref">http://digitalhumanities.org:8081/dhq/vol/12/2/000374/000374.html</a>. </div>
               <div class="bibl"><span class="ref" id="apple_n.d._a"><!-- close -->Apple n.d._a</span>  Apple. (N.d.) <cite class="title italic">ARKit</cite> [Online]. Apple Developer. Available at: <a href="https://developer.apple.com/documentation/arkit/" onclick="window.open('https://developer.apple.com/documentation/arkit/'); return false" class="ref">https://developer.apple.com/documentation/arkit/</a>. </div>
               <div class="bibl"><span class="ref" id="apple_n.d._b"><!-- close -->Apple n.d._b</span>  Apple. (N.d.) <cite class="title italic">RealityKit</cite> [Online]. Apple Developer. Available at: <a href="https://developer.apple.com/documentation/realitykit/" onclick="window.open('https://developer.apple.com/documentation/realitykit/'); return false" class="ref">https://developer.apple.com/documentation/realitykit/</a>. </div>
               <div class="bibl"><span class="ref" id="atske_perrin_2021"><!-- close -->Atske and Perrin 2021</span>  Atske, Sara, and Perrin,
                  Andrew. (2021) “Home broadband adoption, computer ownership vary
                  by race, ethnicity in the U.S.” Pew Research Center. Available at: <a href="https://pewrsr.ch/3Bdn6tW" onclick="window.open('https://pewrsr.ch/3Bdn6tW'); return false" class="ref">https://pewrsr.ch/3Bdn6tW</a>. </div>
               <div class="bibl"><span class="ref" id="azuma_etal_2001"><!-- close -->Azuma et al. 2001</span>  Azuma, Ronald, et al. (2001)
                  “Recent advances in augmented reality”, <cite class="title italic">IEEE Computer Graphics and Applications</cite>, 21(6), 34–47.
                  Available at: <a href="https://doi.org/10.1109/38.963459" onclick="window.open('https://doi.org/10.1109/38.963459'); return false" class="ref">https://doi.org/10.1109/38.963459</a>. </div>
               <div class="bibl"><span class="ref" id="bacca_etal_2014"><!-- close -->Bacca et al. 2014</span>  Bacca, Jorge, et al. (2014)
                  “Augmented Reality Trends in Education: A Systematic Review of
                  Research and Applications”, <cite class="title italic">Educational Technology
                     &amp; Society</cite>, 17(4), pp. 133–149. Available at: <a href="https://www.jstor.org/stable/jeductechsoci.17.4.133" onclick="window.open('https://www.jstor.org/stable/jeductechsoci.17.4.133'); return false" class="ref">https://www.jstor.org/stable/jeductechsoci.17.4.133</a>. </div>
               <div class="bibl"><span class="ref" id="bailenson_2018"><!-- close -->Bailenson 2018</span>  Bailenson, Jeremy. (2018) <cite class="title italic">Experience on Demand: What Virtual Reality is, How it Works, and
                     What it Can Do</cite>. New York: W. W. Norton &amp; Co. </div>
               <div class="bibl"><span class="ref" id="ball_2022"><!-- close -->Ball 2022</span>  Ball, Matthew. (2022) <cite class="title italic">The Metaverse: And How It Will Revolutionize Everything</cite>. New York:
                  Liveright. </div>
               <div class="bibl"><span class="ref" id="bifrost_2023"><!-- close -->Bifrost Consulting Group 2023</span>  Bifrost Consulting
                  Group. (2023) “Let's build cultural spaces in the
                  metaverse”. Available at: <a href="https://diomira.ca/" onclick="window.open('https://diomira.ca/'); return false" class="ref">https://diomira.ca/</a>. </div>
               <div class="bibl"><span class="ref" id="chandler_etal_2017"><!-- close -->Chandler et al. 2017</span>  Chandler, Tom, et al.
                  (2017) “A New Model of Angkor Wat: Simulated Reconstruction as a
                  Methodology for Analysis and Public Engagement”, <cite class="title italic">Australian and New Zealand Journal of Art</cite>, 17(2), pp. 182–194. Available
                  at: <a href="https://doi.org/10.1080/14434318.2017.1450063" onclick="window.open('https://doi.org/10.1080/14434318.2017.1450063'); return false" class="ref">https://doi.org/10.1080/14434318.2017.1450063</a>. </div>
               <div class="bibl"><span class="ref" id="cline_2011"><!-- close -->Cline 2011</span>  Cline, Ernest. (2011) <cite class="title italic">Ready Player One</cite>. New York: Broadway Books. </div>
               <div class="bibl"><span class="ref" id="cool_2022"><!-- close -->Cool 2022</span>  Cool, Matt. (2022) “5
                  Inspiring Galleries Built with Hubs” [Blog]. Mozilla Hubs Creator Labs.
                  Available at: <a href="https://hubs.mozilla.com/labs/5-incredible-art-galleries/" onclick="window.open('https://hubs.mozilla.com/labs/5-incredible-art-galleries/'); return false" class="ref">https://hubs.mozilla.com/labs/5-incredible-art-galleries/</a>. </div>
               <div class="bibl"><span class="ref" id="cramer_2011"><!-- close -->Cramer 2011</span>  Cramer, Tom. (2011) “The International Image Interoperability Framework (IIIF): Laying the Foundation
                  for Common Services, Integrated Resources and a Marketplace of Tools for Scholars
                  Worldwide”, Coalition for Networked Information Fall 2011 Membership
                  Meeting, Arlington, Virginia, December 12–13. Available at: <a href="https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework" onclick="window.open('https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework'); return false" class="ref">https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework</a>. </div>
               <div class="bibl"><span class="ref" id="cramer_2015"><!-- close -->Cramer 2015</span>  Cramer, Tom. (2015) “IIIF Consortium Formed” [Online]. International Image Interoperability
                  Framework. Available at: <a href="https://iiif.io/news/2015/06/17/iiif-consortium/" onclick="window.open('https://iiif.io/news/2015/06/17/iiif-consortium/'); return false" class="ref">https://iiif.io/news/2015/06/17/iiif-consortium/</a>. </div>
               <div class="bibl"><span class="ref" id="crane_2021"><!-- close -->Crane 2021</span>  Crane, Tom. (2021) “On
                  being the right size” [Online]. Canvas Panel. Available at: <a href="https://canvas-panel.digirati.com/developer-stories/rightsize.html" onclick="window.open('https://canvas-panel.digirati.com/developer-stories/rightsize.html'); return false" class="ref">https://canvas-panel.digirati.com/developer-stories/rightsize.html</a>. </div>
               <div class="bibl"><span class="ref" id="dignazio_klein_2020"><!-- close -->D'Ignazio and Klein 2020</span>  D'Ignazio,
                  Catherine, and Klein, Lauren F. (2020) <cite class="title italic">Data Feminism</cite>.
                  Cambridge, Mass.: The MIT Press. Available at: <a href="https://data-feminism.mitpress.mit.edu/" onclick="window.open('https://data-feminism.mitpress.mit.edu/'); return false" class="ref">https://data-feminism.mitpress.mit.edu/</a>. </div>
               <div class="bibl"><span class="ref" id="drucker_2011"><!-- close -->Drucker 2011</span>  Drucker, Johanna. (2011) “Humanities Approaches to Graphical Display”, <cite class="title italic">Digital Humanities Quarterly</cite>, 5(1). Available at: <a href="https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html" onclick="window.open('https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html'); return false" class="ref">https://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</a>. </div>
               <div class="bibl"><span class="ref" id="drucker_2013"><!-- close -->Drucker 2013</span>  Drucker, Johanna. (2013) “Is There a 'Digital' Art History?”, <cite class="title italic">Visual Resources</cite>, 29(1-2). Available at: <a href="https://doi.org/10.1080/01973762.2013.761106" onclick="window.open('https://doi.org/10.1080/01973762.2013.761106'); return false" class="ref">https://doi.org/10.1080/01973762.2013.761106</a>. </div>
               <div class="bibl"><span class="ref" id="drucker_2014"><!-- close -->Drucker 2014</span>  Drucker, Johanna. (2014) <cite class="title italic">Graphesis: Visual Forms of Knowledge Production</cite>. Cambridge,
                  Mass.: Harvard University Press. </div>
               <div class="bibl"><span class="ref" id="falbo_2000"><!-- close -->Falbo 2000</span> Falbo, Bianca. (2000) “Teaching from the Archives”, <cite class="title italic">RBM: A Journal of Rare
                     Books, Manuscripts, and Cultural Heritage</cite>, 1(1), pp. 33–35. Available at:
                  <a href="https://doi.org/10.5860/rbm.1.1.173" onclick="window.open('https://doi.org/10.5860/rbm.1.1.173'); return false" class="ref">https://doi.org/10.5860/rbm.1.1.173</a>.</div>
               <div class="bibl"><span class="ref" id="fadgi_2023"><!-- close -->Federal Agency Digitization Guidelines Initiative 2023</span> Federal Agency Digitization Guidelines Initiative. (2023) <cite class="title italic">Technical Guidelines for Digitizing Cultural Heritage Materials: Third
                     Edition</cite>. [Washington, DC: Federal Agency Digitization Guidelines
                  Initiative.] Available at: <a href="https://www.digitizationguidelines.gov/guidelines/digitize-technical.html" onclick="window.open('https://www.digitizationguidelines.gov/guidelines/digitize-technical.html'); return false" class="ref">https://www.digitizationguidelines.gov/guidelines/digitize-technical.html</a>.</div>
               <div class="bibl"><span class="ref" id="fisher_2018"><!-- close -->Fisher 2019</span> Fisher, Adam. (2018) <cite class="title italic">Valley of Genius: The Uncensored History of Silicon Valley</cite>. New York:
                  Twelve.</div>
               <div class="bibl"><span class="ref" id="francois_etal_2021"><!-- close -->François et al. 2021</span> François, Paul, et al.
                  (2021) “Virtual reality as a versatile tool for research,
                  dissemination and mediation in the humanities,”
                  <cite class="title italic">Virtual Archaeology Review</cite> 12(25), pp. 1–15. Available
                  at: <a href="https://doi.org/10.4995/var.2021.14880" onclick="window.open('https://doi.org/10.4995/var.2021.14880'); return false" class="ref">https://doi.org/10.4995/var.2021.14880</a>.</div>
               <div class="bibl"><span class="ref" id="gibson_1984"><!-- close -->Gibson 1984</span> Gibson, William. (1984) <span class="hi italic">Neuromancer. </span>Reprint, New York: Ace, 2018.</div>
               <div class="bibl"><span class="ref" id="google_n.d."><!-- close -->Google n.d.</span> Google. (N.d.) <cite class="title italic">Add
                     items</cite> [Online]. Google Arts &amp; Culture Platform Help. Available at:
                  <a href="https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA" onclick="window.open('https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA'); return false" class="ref">https://support.google.com/culturalinstitute/partners/answer/4365018?hl=en&amp;ref_topic=6056759&amp;sjid=9709484027047671346-NA</a>
                  </div>
               <div class="bibl"><span class="ref" id="greengard_2019"><!-- close -->Greengard 2019</span> Greengard, Samuel. (2019) <cite class="title italic">Virtual Reality</cite>. Cambridge, Mass.: The MIT Press.</div>
               <div class="bibl"><span class="ref" id="haraway_1988"><!-- close -->Haraway 1988</span> Haraway, Donna. (1988) “Situated Knowledges: The Science Question in Feminism and the
                  Privilege of Partial Perspective,”
                  <cite class="title italic">Feminist Studies </cite>14(3), pp. 575–599. Available at: <a href="https://www.jstor.org/stable/3178066" onclick="window.open('https://www.jstor.org/stable/3178066'); return false" class="ref">https://www.jstor.org/stable/3178066</a>.</div>
               <div class="bibl"><span class="ref" id="haynes_2019"><!-- close -->Haynes 2010</span> Haynes, Ronald.(2019) “To Have and Vehold: Marrying Museum Objects and Virtual Collections
                  via AR” in tom Dieck, M. C., and Jung, T. (eds.) <cite class="title italic">Augmented Reality and Virtual Reality: The Power of AR and VR for
                     Business.</cite> Cham, Switzerland: Springer, pp. 191–202. Available at: <a href="https://doi.org/10.1007/978-3-030-06246-0_14" onclick="window.open('https://doi.org/10.1007/978-3-030-06246-0_14'); return false" class="ref">https://doi.org/10.1007/978-3-030-06246-0_14</a>.</div>
               <div class="bibl"><span class="ref" id="haynes_2018"><!-- close -->Haynes 2018</span> Haynes, Ronald. (2018) “Eye of the Veholder: AR Extending and Blending of Museum Objects and
                  Virtual Collections” in Jung, T., and tom Dieck, M. C. (eds.) <cite class="title italic">Augmented Reality and Virtual Reality: Empowering Human, Place and
                     Business.</cite> Cham, Switzerland: Springer, pp. 79–91. Available at: <a href="https://doi.org/10.1007/978-3-319-64027-3_6" onclick="window.open('https://doi.org/10.1007/978-3-319-64027-3_6'); return false" class="ref">https://doi.org/10.1007/978-3-319-64027-3_6</a>.</div>
               <div class="bibl"><span class="ref" id="herron_2020"><!-- close -->Herron 2020</span> Herron, Thomas. (2020) <cite class="title italic">Recreating Spenser: The Irish Castle of an English Poet</cite>.
                  Greenville, N.C.: Author.</div>
               <div class="bibl"><span class="ref" id="iiifc_2022"><!-- close -->IIIF Consortium 2022</span> IIIF Consortium staff. (2022)
                  <cite class="title italic">New 3D Technical Specification Group</cite> [Online].
                  Available at: <a href="https://iiif.io/news/2022/01/11/new-3d-tsg/" onclick="window.open('https://iiif.io/news/2022/01/11/new-3d-tsg/'); return false" class="ref">https://iiif.io/news/2022/01/11/new-3d-tsg/</a>.</div>
               <div class="bibl"><span class="ref" id="iiif_2015"><!-- close -->International Image Interoperability Framework 2015</span> International Image Interoperability Framework. (2015) “Physical
                  Dimensions” in Albritton, Benjamin, et al. (eds.) <cite class="title italic">Linking to External Services </cite>[Online]. Available at: <a href="https://iiif.io/api/annex/services/%23physical-dimensions" onclick="window.open('https://iiif.io/api/annex/services/%23physical-dimensions'); return false" class="ref">https://iiif.io/api/annex/services/#physical-dimensions</a>.</div>
               <div class="bibl"><span class="ref" id="iiif_n.d."><!-- close -->International Image Interoperability Framework n.d.</span> International Image Interoperability Framework. (N.d.) <cite class="title italic">Consortium</cite>
                  <cite class="title italic">Members</cite> [Online]. Available at: <a href="https://iiif.io/community/consortium/members/" onclick="window.open('https://iiif.io/community/consortium/members/'); return false" class="ref">https://iiif.io/community/consortium/members/</a>.</div>
               <div class="bibl"><span class="ref" id="kai-kee_latina_sadoyan_2020"><!-- close -->Kai-Kee, Latina, Sadoyan 2020</span> Kai-Kee, E., Latina, L., and Sadoyan, L. (2020) <cite class="title italic">Activity-based
                     Teaching in the Art Museum: Movement, Embodiment, Emotion</cite>. Los Angeles:
                  Getty Museum. </div>
               <div class="bibl"><span class="ref" id="kenderdine_yip_2019"><!-- close -->Kenderdine and Yip 2019</span> Kenderdine, Sarah,
                  and Yip, Andrew (2019). “The Proliferation of Aura: Facsimiles,
                  Authenticity and Digital Objects” in Drtoner, K., et al. (eds.) <cite class="title italic">The Routledge Handbook of Museums, Media and Communication</cite>.
                  London and New York: Routledge, pp. 274–289. Available at: <a href="https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip" onclick="window.open('https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip'); return false" class="ref">https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9781315560168-23/proliferation-aura-sarah-kenderdine-andrew-yip</a>
                  </div>
               <div class="bibl"><span class="ref" id="kropf_2017"><!-- close -->Kropf 2017</span> Kropf, Evyn. (2017) “Will that Surrogate Do?: Reflections on Material Manuscript Literacy in the
                  Digital Environment from Islamic Manuscripts at the University of Michigan
                  Library,”
                  <cite class="title italic">Manuscript Studies</cite>, 1(1), pp. 52–70. Available at: <a href="https://doi.org/10.1353/mns.2016.0007" onclick="window.open('https://doi.org/10.1353/mns.2016.0007'); return false" class="ref">https://doi.org/10.1353/mns.2016.0007</a>.</div>
               <div class="bibl"><span class="ref" id="l_i_2018"><!-- close -->Luna 2018</span> Luna Imaging. (2018) <cite class="title italic">LUNA
                     and IIIF</cite> [Online]. Available at: <a href="http://www.lunaimaging.com/iiif" onclick="window.open('http://www.lunaimaging.com/iiif'); return false" class="ref">http://www.lunaimaging.com/iiif</a>.</div>
               <div class="bibl"><span class="ref" id="luo_2019"><!-- close -->Luo 2019</span> Luo, Michelle. (2019) “Explore art and culture through a new lens [Blog]”
                  <cite class="title italic">The Keyword</cite>. Available at: <a href="https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/" onclick="window.open('https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/'); return false" class="ref">https://blog.google/outreach-initiatives/arts-culture/explore-art-and-culture-through-new-lens/</a>
                  </div>
               <div class="bibl"><span class="ref" id="newberry_2023"><!-- close -->Newberry 2023</span> Newberry, David. (2023) <cite class="title italic">IIIF for Dolls</cite> [Online]. Available at: <a href="https://iiif-for-dolls.davidnewbury.com/" onclick="window.open('https://iiif-for-dolls.davidnewbury.com/'); return false" class="ref">https://iiif-for-dolls.davidnewbury.com/</a>.</div>
               <div class="bibl"><span class="ref" id="nolan_2013"><!-- close -->Nolan 2013</span> Nolan, Maura. (2013) “Medieval Habit, Modern Sensation: Reading Manuscripts in the Digital Age,”
                  <cite class="title italic">The Chaucer Review</cite>, 47(4), pp. 465–476. Available at:
                  <a href="https://doi.org/10.5325/chaucerrev.47.4.0465" onclick="window.open('https://doi.org/10.5325/chaucerrev.47.4.0465'); return false" class="ref">https://doi.org/10.5325/chaucerrev.47.4.0465</a>.</div>
               <div class="bibl"><span class="ref" id="oclc_n.d."><!-- close -->OCLC n.d.</span> OCLC. (N.d.) <cite class="title italic">CONTENTdm
                     and the International Image Interoperability Framework (IIIF)</cite> [Online].
                  Available at: <a href="https://www.oclc.org/en/contentdm/iiif.html" onclick="window.open('https://www.oclc.org/en/contentdm/iiif.html'); return false" class="ref">https://www.oclc.org/en/contentdm/iiif.html</a>.</div>
               <div class="bibl"><span class="ref" id="openseadragon_n.d."><!-- close -->OpenSeadragon n.d.</span> OpenSeadragon. (N.d.)
                  <cite class="title italic">OpenSeadragon</cite>. Available at: <a href="http://openseadragon.github.io/" onclick="window.open('http://openseadragon.github.io/'); return false" class="ref">http://openseadragon.github.io/</a>. </div>
               <div class="bibl"><span class="ref" id="pew_2021"><!-- close -->Pew Research Center 2021</span> Pew Research Center. (2021)
                  Mobile Fact Sheet. Available at: <a href="http://pewrsr.ch/2ik6Ux9" onclick="window.open('http://pewrsr.ch/2ik6Ux9'); return false" class="ref">http://pewrsr.ch/2ik6Ux9</a></div>
               <div class="bibl"><span class="ref" id="porter_2018"><!-- close -->Porter 2018</span> Porter, Dot. (2018) “Zombie Manuscripts: Digital Facsimiles in the Uncanny Valley”.
                  Presentation at the International Congress on Medieval Studies, Western Michigan
                  University, Kalamazoo, Michigan, 12 May 2018. Available at: <a href="https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/" onclick="window.open('https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/'); return false" class="ref">https://www.dotporterdigital.org/zombie-manuscripts-digital-facsimiles-in-the-uncanny-valley/</a>
                  </div>
               <div class="bibl"><span class="ref" id="project_mirador_n.d."><!-- close -->Project Mirador n.d.</span> Project Mirador. (N.d.)
                  <cite class="title italic">Mirador</cite>. Available at: <a href="https://projectmirador.org/" onclick="window.open('https://projectmirador.org/'); return false" class="ref">https://projectmirador.org/</a>.</div>
               <div class="bibl"><span class="ref" id="putnam_2016"><!-- close -->Putnam 2016</span> Putnam, Lara. (2016) “The Transnational and the Text-Searchable: Digitized Sources and the Shadows They
                  Cast,”
                  <cite class="title italic">The American Historical Review</cite>, 121(2), pp. 377–402.
                  Available at: <a href="https://doi.org/10.1093/ahr/121.2.377" onclick="window.open('https://doi.org/10.1093/ahr/121.2.377'); return false" class="ref">https://doi.org/10.1093/ahr/121.2.377</a>.</div>
               <div class="bibl"><span class="ref" id="ramsay_2014"><!-- close -->Ramsay 2014</span> Ramsay, Stephen. (2014) “The Hermeneutics of Screwing Around; or What You Do With a Million
                  Books” in Kee, Kevin (ed.) <cite class="title italic">Pastplay: Teaching and
                     Learning History with Technology</cite>. Ann Arbor: University of Michigan Press,
                  pp. 111–120. Available at: <a href="https://doi.org/10.1353/book.29517" onclick="window.open('https://doi.org/10.1353/book.29517'); return false" class="ref">https://doi.org/10.1353/book.29517</a>.</div>
               <div class="bibl"><span class="ref" id="rosenzweig_2003"><!-- close -->Rosenzweig 2003</span> Rosenzweig, Roy. (2003) “Scarcity or Abundance? Preserving the Past in a Digital Era,”
                  <cite class="title italic">The American Historical Review</cite> 108 (3), pp. 735–762.
                  Available at: <a href="https://doi.org/10.1086/ahr/108.3.735" onclick="window.open('https://doi.org/10.1086/ahr/108.3.735'); return false" class="ref">https://doi.org/10.1086/ahr/108.3.735</a>.</div>
               <div class="bibl"><span class="ref" id="roth_fisher_2019"><!-- close -->Roth and Fisher 2019</span> Roth, A., and Fisher, C.
                  (2019) “Building Augmented Reality Freedom Stories: A Critical
                  Reflection,” in Kee, Kevin, and Compeau, Timothy (eds.) <cite class="title italic">Seeing the Past with Computers: Experiments with Augmented Reality
                     and Computer Vision for History.</cite> Ann Arbor: University of Michigan Press,
                  pp. 137–157. Available at: <a href="https://www.jstor.org/stable/j.ctvnjbdr0.11" onclick="window.open('https://www.jstor.org/stable/j.ctvnjbdr0.11'); return false" class="ref">https://www.jstor.org/stable/j.ctvnjbdr0.11</a></div>
               <div class="bibl"><span class="ref" id="schmiesing_hollis_2002"><!-- close -->Schmiesing and Hollis 2002</span> Schmiesing, A.,
                  and Hollis, D. (2002) “The Role of Special Collections
                  Departments in Humanities Undergraduate and Graduate Teaching: A Case
                  Study,”  <cite class="title italic">Libraries and the Academy</cite> 2(3), pp.
                  465–480. Available at: <a href="https://doi.org/10.1353/pla.2002.0065" onclick="window.open('https://doi.org/10.1353/pla.2002.0065'); return false" class="ref">https://doi.org/10.1353/pla.2002.0065</a>.</div>
               <div class="bibl"><span class="ref" id="shemek_etal_2018"><!-- close -->Shemek 2018</span> Shemek, D., et al. (2018) “Renaissance Remix. Isabella d'Este: Virtual Studiolo,”
                  <cite class="title italic">Digital Humanties Quarterly</cite>, 12(4). Available at: <a href="http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html" onclick="window.open('http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html'); return false" class="ref">http://www.digitalhumanities.org/dhq/vol/12/4/000400/000400.html</a>.</div>
               <div class="bibl"><span class="ref" id="snydman_sanderson_cramer_2015"><!-- close -->Snydman, Sanderson, and Cramer 2015</span> Snydman, S., Sanderson, R., &amp; Cramer, T. (2015) “The
                  International Image Interoperability Framework (IIIF): A community &amp;
                  technology approach for web-based images.” In D. Walls (Ed), <cite class="title italic">Archiving 2015: Final Program and Proceedings</cite> (pp. 16–21).
                  Society for Imaging Science and Technology. Available at: <a href="https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf" onclick="window.open('https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf'); return false" class="ref">https://stacks.stanford.edu/file/druid:df650pk4327/2015ARCHIVING_IIIF.pdf</a></div>
               <div class="bibl"><span class="ref" id="solberg_2012"><!-- close -->Solberg 2012</span> Solberg, Janine. (2012) “Googling the Archive: Digital Tools and the Practice of
                  History.”
                  <cite class="title italic">Advances in the History of Rhetoric</cite> 15(1), pp. 53–76.
                  Available at: <a href="https://doi.org/10.1080/15362426.2012.657052" onclick="window.open('https://doi.org/10.1080/15362426.2012.657052'); return false" class="ref">https://doi.org/10.1080/15362426.2012.657052</a>.</div>
               <div class="bibl"><span class="ref" id="stephenson_1992"><!-- close -->Stephenson 1992</span> Stephenson, Neal. (1992) <cite class="title italic">Snow Crash</cite>. Reprint, New York: Bantam, 2000.</div>
               <div class="bibl"><span class="ref" id="sundar_etal_2013"><!-- close -->Sundar et.al 2013</span> Sundar, S., Bellur,
                  Saraswathi, Oh, Jeeyun, Xu, Qian, &amp; Jia, Haiyan. (2013) “User
                  Experience of On-Screen Interaction Techniques: An Experimental Investigation of
                  Clicking, Sliding, Zooming, Hovering, Dragging, and Flipping.”
                  <cite class="title italic">Human–Computer Interaction</cite> 29 (2), pp. 109–152.
                  doi:10.1080/07370024.2013.789347 </div>
               <div class="bibl"><span class="ref" id="szpiech_2014"><!-- close -->Szpiech 2014</span> Szpiech, Ryan. (2014) “Cracking the Code: Reflections on Manuscripts in the Age of Digital
                  Books.”
                  <cite class="title italic">Digital Philology: A Journal of Medieval Cultures</cite> 3(1),
                  pp. 75–100. Available at: <a href="https://doi.org/10.1353/dph.2014.0010" onclick="window.open('https://doi.org/10.1353/dph.2014.0010'); return false" class="ref">https://doi.org/10.1353/dph.2014.0010</a>.</div>
               <div class="bibl"><span class="ref" id="toner_1993"><!-- close -->Toner 1993</span> Toner, Carol. (1993) “Teaching Students to be Historians: Suggestions for an Undergraduate Research
                  Seminar,” <cite class="title italic">The History Teacher</cite> 27(1), pp.
                  37–51. Available at: <a href="https://doi.org/10.2307/494330" onclick="window.open('https://doi.org/10.2307/494330'); return false" class="ref">https://doi.org/10.2307/494330</a>.</div>
               <div class="bibl"><span class="ref" id="vogels_2021"><!-- close -->Vogels 2021</span> Vogels, Emily A. (2021) “Digital divide persists even as Americans with lower incomes make
                  gains in tech adoption”
                  <cite class="title italic">Pew Research Center</cite>. Available at: <a href="https://pewrsr.ch/2TRM7cP" onclick="window.open('https://pewrsr.ch/2TRM7cP'); return false" class="ref">https://pewrsr.ch/2TRM7cP</a></div>
               <div class="bibl"><span class="ref" id="woodward_2021"><!-- close -->Woodward 2021</span> Woodward, Dave. (2021) Email to Sean
                  Fraga, February 2.</div>
               <div class="bibl"><span class="ref" id="orangelogic_n.d."><!-- close -->orangelogic n.d.</span> orangelogic. (N.d.) <cite class="title italic">Connect to your favorite systems: Get DAM integrations that fit your
                     existing workflows</cite> [Online]. Available at: <a href="https://www.orangelogic.com/features/integrations" onclick="window.open('https://www.orangelogic.com/features/integrations'); return false" class="ref">https://www.orangelogic.com/features/integrations</a>.</div>
               <div class="bibl"><span class="ref" id="van_lit_2020"><!-- close -->van Lit 2020</span> van Lit, L. W. C. (2020) <cite class="title italic">Among Digitized Manuscripts: Philology, Codicology, Paleography in a
                     Digital World</cite>. Leiden: Brill.</div>
               <div class="bibl"><span class="ref" id="van_zundert_2018"><!-- close -->van Zundert 2018</span> van Zundert, Joris. (2018)
                  “On Not Writing a Review about Mirador: Mirador, IIIF, and the
                  Epistemological Gains of Distributed Digital Scholarly Resources.”
                  <cite class="title italic">Digital Medievalist</cite> 11(1). Available at: <a href="http://doi.org/10.16995/dm.78" onclick="window.open('http://doi.org/10.16995/dm.78'); return false" class="ref">http://doi.org/10.16995/dm.78</a>.</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>