<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">Capturing Captions: Using AI to Identify and Analyse Image Captions in a Large Dataset of Historical Book 
               Illustrations</title>
            <dhq:authorInfo>
               <dhq:author_name>Julia <dhq:family>Thomas</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation>Cardiff University</dhq:affiliation>
               <email>ThomasJ1@cardiff.ac.uk</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Irene <dhq:family>Testini</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation>Cardiff University</dhq:affiliation>
               <email>Testinii@cardiff.ac.uk</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000740</idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date><!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#visual_art"/>
               <term corresp="#media_history"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item>illustration studies</item>
                  <item>illustrations</item>
                  <item>captions</item>
                  <item>book history</item>
                  <item>big data</item>
                  <item>computer vision</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000740/000740.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <p>This article outlines how AI methods can be used to identify image captions in a large dataset of digitised historical book illustrations. This 
               dataset includes over a million images from 68,000 books published between the eighteenth to early twentieth centuries, covering works of 
               literature, history, geography, and philosophy. The article has two primary objectives. First, it suggests the added value of captions in making 
               digitized illustrations more searchable by picture content in online archives. To further this objective, we describe the methods we have used to 
               identify captions, which can effectively be re-purposed and applied in different contexts. Second, we suggest how this research leads to new 
               understandings of the semantics and significance of the captions of historical book illustrations. The findings discussed here mark a critical 
               intervention in the fields of digital humanities, book history, and illustration studies.
            </p>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>
          <div>
            <head></head>
              <p>The captions that appear alongside pictures in historical books are situated on a threshold, hanging in their own space, somewhere between the image and the text proper. Like the captions that it discusses, the research that we describe here also occupies a threshold: between digital humanities, with its focus on how computational tools and remediation generate research questions, and book history, with its attention to the material narratives of the book as object. It is this liminal space that has come to define the field of Illustration Studies. In his discussion of ‘thresholds’, Gérard Genette famously shied away from discussing illustration because it was an ‘immense continent’ that was too large for him to traverse (Genette, 1987, p. 406). Digital humanities have risen to this challenge both by making illustrated material more accessible than ever before and by engaging with how this accessibility and scale can reveal new ways of analysing historical illustrations (Thomas, 2017).</p>
              <p>From within this critical space, this article sets out to search the ‘immense continent’ of illustration for what is perhaps its most marginalised territory: the caption. Very little work has been undertaken on the significance of captions and how they make their meanings, despite their prevalence in illustrated books. These issues, however, are of some importance for understanding how captions relate to the content of the pictures that they accompany, as well as for the wider dialogue between word and image that characterises illustration as a mode of representation. What we outline below is the first attempt to capture the significance of captions by describing the methods and findings of research that identifies and interrogates the captions of historical book illustrations at scale, research that is only possible in a digital environment where images from different books can be viewed alongside each other.</p>
              <p>The dataset that is central to this study consists of over a million illustrations from 68,000 volumes in the British Library’s collection, which were digitised by Microsoft. These illustrations form the basis of the world’s largest online resource dedicated to book illustration, The Illustration Archive, which we created on a previous AHRC-funded project (https://illustrationarchive.cf.ac.uk/). The illustrations in this dataset span the sixteenth to the twentieth century, with the majority clustering around the late eighteenth and nineteenth centuries, a period that witnessed a global explosion of illustrated material and was the immediate precursor of our own visual culture. The books are written in different languages, with the vast majority in English, and they cover genres categorised as literature, history, philosophy, and geography, a diverse range that provides an insight into the practices and importance of illustration at a time when this mode of representation constituted the ‘mass image’, the dominant visual form of the day. </p>
              <p>The fact that illustrations were so prevalent at this historical moment makes it imperative that we understand their constituents, not least because digitisation has made these illustrations available to view in their hundreds of thousands. Our study suggests that it is the very ambiguity and indeterminacy of captions that opens them up to computational analysis. This proposition might seem counter-intuitive: generally, we think that it is objects that are easier to ‘describe’ and identify computationally that are best suited to analysis. Certainly, some interesting work has been done using historical illustrations in this way. In 2014 an ambitious project was undertaken by Kalev Leetaru to identify the illustrations in books and make them searchable using the captions alongside the paragraphs immediately preceding and following the illustrations (Leetaru, 2014). More recently, Hoyeol Kim has adapted Victorian illustrations as training sets for deep learning models by creating a dataset that has colourised nineteenth-century black and white illustrations (Kim, 2021). </p>
              <p>What we propose in this article is that indeterminacy — what is not clearly known, defined, or fixed — also has an analytical value. Captions can be identified computationally precisely because of their indeterminate space between the text and image: the words of the caption are neither part of the image, nor do they fully belong to the body of the text. It is this liminal space that enables the caption to be identified. This indeterminacy can also be understood and exploited on a semantic level (as we describe in the ‘Analysing Captions’ section below). By using the captions of illustrations as a search tool, we can begin to find those instances when the captions seem to capture the visual content of the image, and those instances when they do not. The drive for searchability and the retrieval of relevant and accurate ‘hits’ generates an analytical space that reveals new insights into the complex interrelationship between the caption, image, and text. Our study, therefore, sets out to identify the captions both as a mechanism for searching the illustrations in a big dataset, and as a way of exploring how the captions signify, an exploration that emerges from the very limits of searchability.</p>
          </div>
          <div>
            <head>What is a Caption?</head>
              <p>The function of the ‘caption’ — a piece of text that lies alongside a visual image — can be traced back to the historical use of the ‘inscription’, ‘motto’, and ‘legend’. The word ‘subscription’ was used from the sixteenth century to describe the words placed below a picture or portrait. ‘Caption’ itself is a more recent term. Deriving from the Latin for ‘taking’, ‘caption’ was originally used in the late eighteenth century in a textual context: to describe    the heading of a chapter, section, or newspaper article. Its specific meaning as the ‘title below an illustration’ emanates from America in the early decades of the twentieth century. </p>
              <p>The Early Modern Graphic Literacies project (University of Turku, Finland), which is working to create an historical taxonomy of visual devices in early English print from the late 1400s to 1800, draws attention to the use of the caption in these forms, noting that the distinction between the captions and other image-related texts (for example, titles and running titles) is not always obvious (Ruokkeinen  <hi rend="italic">et al.</hi>, 2023, p. 9). Historically, our dataset picks up at the point when the Early Modern Graphic Literacies project ends, but even in this later period, the caption is beset with ambiguity and a startling variety of practices. The difficulty of converting some captions into a readable form is a consequence of the fact that the caption is generally printed using the same reproductive method as the illustration, and numerous methods are represented in our dataset: intaglio illustrations (e.g. etching, steel engraving) include an etched/engraved caption as part of the image, lithographed images have lithographed captions, whilst wood-engraved and photomechanically-produced images usually come with letterpress captions that connect the captions visually to the rest of the text. Captions were sometimes reproduced in the front matter of the book in the form of a ‘list of illustrations’, with the caption explicitly adopting the role of image title. The choice of words in the caption could be formulated by the author, the artist (or engraver), or by the publisher.</p>
              <p>The very presence of captions varies considerably across historical book illustrations. There are four main types of illustration represented in our dataset: embellishments that are primarily positioned as the headers or footers of chapters and books (also known as ‘ornaments’ or ‘decorations’); pictorial capital letters; illustrations that are inset anywhere on a page of text; and full-page plates, where the image occupies its own page. Of these types of illustration, the first two (embellishments and pictorial letters) do not include captions; the second two (inset and full-page illustrations) do, although this is not uniform across all inset and full-page illustrations. In our calculations, and with ‘embellishments’ excluded, we have identified 513,914 captioned illustrations from a total of 665,684 illustrations (where overlapping illustrations are counted as a single image). The majority of captions in our dataset are placed below rather than above the image and occupy their own space at a slight distance from the image (see Figures 3, 5, 7, 8 and 9 below). In the case of those illustrations that are set alongside the text on the page, the caption is also positioned at a slight distance from the text proper (see Figures 1, 2 and 4). These physical features allow us to identify the caption computationally in the ways we outline in the following section. </p>
              <p>The positioning of the caption also has semantic implications, suggesting its status on, and between, both sides of the threshold between word and image. Little critical attention has been devoted to the complexities of the caption and how it signifies in relation to the image and to the rest of the text. Roland Barthes is one of the few critics to have engaged with this interaction, suggesting that captions can work to ‘anchor’ the meanings of an image by indicating how the image should be read and directing the viewer’s interpretation. Barthes’s discussion is based on captions in twentieth-century advertising (Barthes, 1977, pp. 32-51), but his ideas are borne out in recent analyses of the captions in historical book illustrations emerging from illustration studies. Sioned Davies gives an account of the possible reasons why captions were added to the illustrations in the second edition of Charlotte Guest’s translation of   <hi rend="italic">Geraint the son of Erbin </hi>(1849), arguing that they add a specific geographical and historical dimension to the settings of the images that are otherwise indistinct (Davies, 2019). Valentina Abbatelli has also drawn attention to the importance of the captions in remediating the racial politics of the early twentieth-century Italian editions of Harriet Beecher Stowe’s novel <hi rend="italic">Uncle Tom’s Cabin </hi>(Abbatelli, 2018). According to Abbatelli, the same image of Uncle Tom learning to write is recast by the captions as a scene where Uncle Tom is ‘failing’ to write and one where he is making an effort. </p>
              <p>In Davies’s and Abbatelli’s strikingly different examples, the illustrations remain the same across editions; it is the captions that are added or changed, and this modification, in turn, changes the meanings of the image. A computational identification of captions gives an opportunity to scale up these analyses and to see patterns that are difficult, if not impossible, to discern in the material form of the book where comparisons between illustrations and captions are necessarily limited. The next section describes the AI methods we used to identify captions in this large dataset of historical illustrations. These methods can effectively be re-purposed across other datasets to add value to the searchability of illustrations. The section following suggests how this identification leads to new understandings of the significance and semantics of the captions of historical book illustrations, which are of relevance not only to scholars working in digital humanities but also to those whose interests lie in illustration studies and its attendant fields.    </p>
          </div>
          <div>
            <head>Using AI to Identify Captions</head>
              <p>The first stage in our identification process was to isolate the captions from the main body of the text and the image, a task that has proven challenging for both machine learning and humanists attempting to formalise what a ‘caption’ is. Zongyi Liu and Hanning Zhou provide a heuristics-based approach to caption finding, suggesting a series of rules to identify a caption, based on size of the font, position relative to the image, and size (Liu and Zhou, 2011). Employing this method led to a high number of false positives and false negatives; our dataset presents wide variation in style of layouts, making it impossible to list a reliable set of heuristics. In order to isolate the caption, we needed to identify the other elements present on the page, that is, we needed to ‘parse’ the layout of the page. For simplicity, we identified four building blocks:  </p>
                 <list type="ordered">
                   <item>text, referring to the main body of text such as paragraphs, introductions, and any text which does not fall into our two other textual categories; </item>
                   <item>caption;</item>
                   <item>heading, referring to titles, chapter titles, and any form of text appearing at the top of the page that might be shared across subsequent pages;</item>
                   <item>figure.</item>
                 </list>
              <p>We then labelled a random subsample of 1,000 images from our dataset using labelme (https://github.com/wkentaro/labelme), a graphical annotation tool, which allows users to draw bounding boxes on an image and assign them a label. The annotations of each page are then saved as a file that contains the coordinates of each bounding box and their respective label. Figure 1 shows a snippet of the labelling tool with one of our images. In the left panel, boxes can be drawn around the different sections of the page, highlighting its layout; on the right, each bounding box is labelled with one of the categories we have identified above. These annotations allow us to re-train a computer vision model to automatically identify the layout of a page and isolate a caption.  </p>
              <figure>
                <head>A snippet of the annotation process using the labelme tool.</head>
                  <graphic url="resources/images/figure01.png"/>
              </figure>
              <p>With this annotated dataset, we turn to fine-tuning. In deep learning, fine-tuning is a transfer learning method where a pre-trained model is trained on new data to fine tune it on a more specific task. In our case, that task is layout parsing on historical illustrated books. Layout parsing is the process of automatically detecting the layout structure of a page using computer vision. A highly effective recent model is Layout Parser (https://layout-parser.github.io/), which provides a range of models pretrained on diverse databases. We used the model trained on the PrImA dataset as it most closely resembled our data. It is a Mask R-CNN model trained on newspapers that identifies text, figures, titles, tables, maths, and separators; newspapers present a similarly busy and interlocked layout as historical illustrated books. We ran training for 10,000 iterations with a learning rate of 0.001, achieving accuracy of 98% on the validation set. The majority of errors occur on pictorial capital letters: at the annotation stage, we decided for simplicity to include pictorial letters within the main body’s bounding box labelled as text, but the model often labels them separately as ‘figure’.       </p>
              <p>We deployed our fine-tuned model on <hi rend="italic">The Illustration Archive</hi> to predict the layout of each page and isolate the captions. The model’s output consists of coordinates for the layout element of each page which can be visualised as bounding boxes. Figures 2–4 demonstrate an example output of the model: on each page, the model has predicted the position and category of a layout element, and it is displayed with a confidence score as a percentage. We can see in Figure 2 that three bounding boxes with the label ‘figure’ have been predicted, as the model is unsure whether the two illustrations should be considered separately. An analogous situation occurs in Figure 4 with the captions.  </p>
              <figure>
                <head></head>
                  <graphic url="resources/images/figure02.jpg"/>
              </figure>
              <figure>
                <head></head>
                  <graphic url="resources/images/figure03.jpg"/>
               </figure>
               <figure>
                 <head/>
                   <graphic url="resources/images/figure04.jpg"/>
               </figure>
               <p>After obtaining labelled layout structures, it becomes easier to perform optical character recognition (OCR) exclusively on the captions. We used pytesseract (https://pypi.org/project/pytesseract/), a Python library which is a wrapper for Google’s Tesseract OCR engine. The main disadvantage of this tool is its speed when attempting to use multiple languages; for this reason, at this stage, we allowed it only to rely on English, meaning that captions in foreign languages were not processed correctly; we are planning to fix this issue in a future iteration.</p>  
               <p>Through these computer vision techniques, we have obtained a dataset of captions that can be used as a search index for the illustrations in  <hi rend="italic">The Illustration Archive</hi>. Integrated as metadata in <hi rend="italic">The Illustration Archive</hi> website, the captions allow users to explore the images, search by captions, and to analyse the relation between the words in the captions and the content of the illustrations. The identification of the captions has also opened up possibilities to use image-to-text and text-to-image models such as CLIP (Contrastive Language-Image Pre-training:  <ref target="https://openai.com/research/clip">https://openai.com/research/clip</ref>) to cluster images with similar captions, identify objects in illustrations using CLIP text encodings as well as our fine-tuned caption encodings, and to discover connections between language representation and visual representation. CLIP is a deep learning model that draws simultaneously from text and image. It is trained on vast amounts of data consisting of pairs of image and descriptive text, allowing it to train a text encoder and an image encoder to produce text and visual embeddings that are close to each other. In this way, a textual prompt can be used to search for images with embeddings resembling the textual prompt. We have used OpenCLIP ViT-L/14 model (Vision Transformer) to build a searchable index of the illustrations in   <hi rend="italic">The Illustration Archive</hi>, following the work of the Visual Geometry Group at the University of Oxford (see their WISE Image Search Engine, which uses AI tools to make image databases searchable by content: <ref target="https://www.robots.ox.ac.uk/~vgg/software/wise/">https://www.robots.ox.ac.uk/~vgg/software/wise/</ref>).</p>
           </div>
           <div>
             <head>Analysing Captions</head>
               <p>Developments in deep learning models mean that we are no longer wholly reliant on textual metadata (tags, bibliographic information, or, indeed, captions) to search the content of images in datasets of historical images. Back in 2004 when we started developing the <hi rend="italic">Database of Mid-Victorian Illustration </hi>(<ref target="https://www.dmvi.org.uk/">https://www.dmvi.org.uk/</ref>), the only way of making the 900 or so images searchable to users was manually to tag their content. Captions, however, can still add value to content-driven visual/iconographic searches. Online archives of historical newspapers often use captions for searchability, although the captions of these illustrations do not necessarily function in the same way as the captions in books. In newspapers, the caption is generally a ‘heading’ that appears at the top of the feature and is the title both for the illustration (if there is one) and the textual article. In   <hi rend="italic">Welsh Newspapers</hi> (https://newspapers.library.wales/), for instance, the title in the form of a heading has been identified as a distinct characteristic of the page. Any alternative captions underneath an image have been OCRed along with the full text and can be searched accordingly. A recent use of OCRed headlines/captions and other text alongside deep learning models is the   <hi rend="italic">Newspaper Navigator</hi>, which searches the visual content of historical newspapers in the Library of Congress (Lee, 2020).</p>
               <p>We conducted a case study to evaluate the effectiveness of using our identified captions to search for specific illustrations in the dataset. In partnership with Lambeth Palace Library, we searched for book illustrations of cathedrals that would enrich the library’s collections. We found 2,087 illustrations captioned with the word ‘cathedral’ and 103 ‘cathedrals’. Data-mining the captions in this way involves the same discrimination as might be adopted on a Google search, in the sense that we needed to anticipate the words that might be used in these captions (e.g., additional terms not covered by the word ‘cathedral’, such as ‘York Minster’, ‘Westminster Abbey’, and ‘chapterhouse’, which might signal a cathedral building).   </p>
               <p>In this study, searching the captions proved a highly effective way of identifying relevant illustrations from the dataset because the words of the captions describe in a broad sense what the illustrations depict: ‘cathedrals’. In some instances, a caption search can be   <hi rend="italic">more</hi> effective than using only a visual search. An image classification (as opposed to a caption) search would retrieve images of cathedrals, but it would also return churches since there are no specific architectural signifiers that mark out a cathedral from a church. Cathedrals are generally larger and more elaborate than churches, but their specific distinction — which would not usually be signalled in an illustration or in visual searches — is that cathedrals are the seat of a bishop. </p>
               <p>Likewise, a caption search is able to retrieve illustrations that show features of cathedrals that are not iconographically specific to cathedrals and would not, therefore, be found in image-based searches that tend to cluster key features of cathedrals (external ‘grand’ perspectives, spires, and high-domed interiors). A search via the captions revealed illustrations of floor plans of cathedrals, different perspectives where the cathedral is in the far distance, so not obviously identified as a ‘cathedral’, as well as many images of interior and interior features (e.g., effigies, gargoyles, windows, staircases; see Figure 5). The captions here retrieve images of ‘cathedrals’ that would not otherwise be found.      </p>
               <p>This example of the effectiveness of captions in searching large datasets raises an important point about the nature of captions, especially in illustrated non-fiction books. As contemporary labels attached to the images, captions can function as ‘expert tags’: authoritative and reliable indicators of what the illustration contains. They can, for instance, identify an otherwise unrecognisable interior setting as depicting ‘Bristol Cathedral’ (Figure 5).    </p>
               <figure>
                 <head>Example of an image retrieved by searching the captions for ‘cathedral’. From [Samuel Griffiths Tovey], Cursory Observations on the Churches of Bristol. By an Occasional Visitor, second edition (Bristol: Mirror Office, 1843), facing p. 6.</head>
                   <graphic url="resources/images/figure05.png"/>
               </figure>
            <p>This aspect of the caption as a reliable descriptor of the image can add to the searchability of large illustration datasets. However, captions are far from neutral or ahistorical descriptors of the image, and it is this problematic relationship that comes to the fore in our research. By identifying hundreds of thousands of captions, we have effectively isolated a corpus of textual metadata that is culturally significant.  Even a term as apparently straightforward as ‘cathedral’ has an historical resonance: its use in a caption speaks to the establishment and embeddedness of (in this case) the Church of England; the rise of tourism in cathedral towns and cities in the UK; and the changing demographics brought about by industrialisation (no new cathedrals were created for nearly 300 years until the Victorian period when the increased population in major industrial cities necessitated new cathedrals). There is a civic pride in the captioning of ‘cathedrals’, especially as illustrations of cathedrals most commonly appear in the context of tourist guidebooks and local histories, like the one in Figure 5. ‘Cathedral’ is just one example of the cultural significance of the words used in captions. Others are more overtly political and unsettling, such as the racist terms that are sometimes used.                </p>
            <p>We can interrogate this corpus of captions in multiple ways, including searching for the frequency distribution of words. Figure 6 shows a random subset of words sampled from different frequency bands.</p>
            <figure>
              <head></head>
                <graphic url="resources/images/figure06.png"/>
            </figure>
            <p>The arbitrary nature of these words reflects the mixed genres that make up our illustration dataset, but, seen in the form of a word cloud, the words in the captions clearly foreground dominant nineteenth-century values. The most frequent keywords are ‘house’ (in 3,043 captions) and church’ (in 2,688 captions); there is also an emphasis on the ‘old’ (2,561 captions), signalled also in terms like ‘castle’ (1,778 captions), ‘ancient’ (592 captions) and ‘abbey’ (556 captions); and the ‘new’ (in 1,760 captions), signalled also in ‘photograph’ (2,009 captions). Aspects of the countryside and landscape are emphasised, alongside towns and cities (‘London’ is one of the top terms, used in 2,219 captions). Gender categories are particularly revealing, with 519 captions containing the word ‘man’ and 307 captions ‘woman’ (the plural suggests a similar margin: 274 ‘men’ and 187 ‘women’); ‘Mr’ appears 1,198 times, ‘Mrs’ 497 times, and ‘miss’ 250 times.        </p>
            <p>Although it is highly likely that the illustrations in the dataset contain more illustrations of men than women, this is not necessarily the full picture. A comparison search for image features can prove illuminating here. An image feature search on a subset of 15,000 illustrations using CLIP found 524 men and 14 women, while, surprisingly, there were no groups of men, 2,487 groups of women and 1,562 groups recognised as people. More work needs to be done comparing different search mechanisms. Our analysis indicates that whilst there is an undoubted bias in the illustrations, there is also a bias in the captions, which might not necessarily be the same as the bias in the images. Identifying and isolating the captions in this way exposes how the caption emphasises and marginalises features of the illustrations.      </p>
            <p>The caption’s privileging and concomitant marginalising of pictorial details is, in fact, one of its major characteristics. The caption of Figure 5, ‘Bristol Cathedral. VIEW FROM THE ELDER LADY’S CHAPEL’, contains no mention of stairs, flagstones, archways, monuments, or any other architectural features. However descriptive and objective it appears, the caption is never fully reflective of the image because it is couched in another form: words; and words cannot fully describe the visual features of an image. What captions emphasise and marginalise, then, is highly significant: they provide a unique insight into how the illustrations were viewed at the time, what were regarded as their most salient features, and what features were overlooked.     </p>
            <p>Using AI to identify the captions at scale allows us to look not only at the words of the caption, but also to trace patterns in how the captions signify in relation to the illustrations and the rest of the text. Our findings point to the fact that the caption makes its meanings in two main ways: it signifies in its conjunction with the image; and it acts as a point of connection, or bridge, between the illustration and the rest of the text. Either, or both, of these relationships can be at play in any captioned illustration, but they seem primarily to be determined by the genre of the book.      </p>
            <p>In texts conventionally classified as non-fiction, including works of history, geography, and science (in the broadest sense), the role of the caption and its relation to the image is characterised by the first of these models, with the illustration and caption forming a complete signifying unit. In theory, this unit could be isolated from the rest of the text and still make sense. Although the caption ‘Bristol Cathedral’ fails to mention certain aspects of the image, it nevertheless signifies in relation to the illustration, with the picture and the words of the caption constituting a signifying partnership.    </p>
            <p>This is the defining model across the range of non-fiction books in the dataset. In simple terms, the captions in these books describe the facts of what is represented in the illustrations; following the Latin etymology of ‘caption’ they attempt to ‘capture’ or ‘seize’ the meanings of the picture. In our dataset, there are tens of thousands of botanical or zoological illustrations captioned with the name of the species, illustrative portraits of people captioned with their names, and illustrations of locations or buildings with captions that identify what locations and buildings they are. In these examples, the signifying conjunction of the illustration and caption creates and enforces a notion of equivalence: the idea that the caption replicates in words what the illustration shows and vice versa (however illusory this notion is).     </p>
            <p>In illustrated fiction and literary texts (short stories, novellas, novels, plays, poetry, etc.), captions are usually citational, taking the form of quotes, or deriving from the words of the text. These captions frequently state the characters’ names and/or what they are doing: ‘Mr. Perry passed by on horseback’ is a typical caption from an illustrated edition of Jane Austen’s     <hi rend="italic">Emma</hi> (1896). (The designations ‘Mr., Mrs., and Miss are more prevalent in the captions of fictional texts where they are used to name a character than they are in non-fiction captions.) In literary texts, the context and meanings of the illustrations and the captions depend on the text; they do not necessarily make sense in isolation, like the non-fiction captioned illustrations described above. The role of these literary captions is to point to the specific episode being illustrated, a role that serves a practical function because book illustrations often appeared on different pages to the text that they depicted (thus many captions also include the relevant page numbers; see Figure 3). Instead of turning in towards the illustration, then, these captions point outside, acting as a bridge between the illustration and the text proper.          </p>
            <p>An example of this bridging technique is the caption ‘Never is a very long word’, which accompanies an illustration for Anthony Trollope’s novel <hi rend="italic">Orley Farm</hi> (Figure 7).  </p>
            <figure>
              <head>John Everett Millais, ‘Never is a very long word’, engr. Dalziel Brothers. Illustration for Anthony Trollope, Orley Farm (London: Chapman and Hall, 1862), facing p. 77.</head>
                <graphic url="resources/images/figure07.jpeg"/>
            </figure>
            <p>‘Never is a very long word’ is a direct quotation from the novel, and this caption is placed securely within quotation marks when it appears as a title on the contents page in the book edition. By citing the text of the novel, the caption works to direct the reader to the episode that is being illustrated, isolating the moment depicted in the illustration from several possibilities (the pose of the two seated ladies could fit any number of episodes). In this way, the caption works to connect the image to the rest of the text. As if to prove that an image cannot translate into words, the illustration does not depict the words ‘Never is a very long word’. How could these words even be illustrated in a picture? This caption, therefore, is a good example of where the use of captions to search for picture content would fail (the content of this image is manually tagged in the           <hi rend="italic">Database of Mid-Victorian Illustration</hi>). In terms of searchability, the use of captions is far more effective when searching across the non-fiction books in the dataset.  </p>
            <p>Our analysis, therefore, indicates that the caption does not signify in the same way across all books. Rather, there are specific conventions at play that are determined primarily by the broad generic classification of the book. An acknowledgement of these conventions allows us to recognise those instances where they are being adapted and manipulated, with implications for the meanings of the text and how it is read. An example of this is Figure 8, the frontispiece illustration for Trollope’s        <hi rend="italic">Orley Farm</hi>. Unlike the caption in Figure 7, which follows the conventions of literary illustrations and quotes directly from the novel, Figure 8 draws instead on the conventions of the ‘factual’ caption.  </p>
            <figure>
              <head>John Everett Millais, ‘ORLEY FARM’, engr. Dalziel Brothers. Illustration for Anthony Trollope, Orley Farm (London: Chapman and Hall, 1862), frontispiece.</head>
                <graphic url="resources/images/figure08.jpeg"/>
            </figure>
            <p>The illustration in Figure 8 is based on a real location, the farmhouse where Trollope lived as a boy. The style of the image itself crosses generic categories: pictures of rural landscapes frequently appeared in both fictional and non-fictional books, including gift books, poetry anthologies and topographical works. However, it is the caption that makes the link to a real geographic location. Capitalised to lend it more authority, ‘ORLEY FARM’ mimics the style of captions in illustrated travel books that label the place illustrated (see, for example, Figure 9).       </p>
            <figure>
              <head>‘BACKBARROW MILLS’, illustration for Rambles in the Lake Country and Other Travel Sketches ed. George Milner (Manchester and London: John Heywood, 1893), p. 68.</head>
                <graphic url="resources/images/figure09.png"/>
            </figure>
            <p>The association of the caption ‘ORLEY FARM’ with the designatory captions of non-fiction books gives a sense of veracity to the novel at its outset (it is placed as the frontispiece illustration), and this has implications for the investment and immersion of readers in the imaginative world of the novel. The caption here functions alongside the realist conventions of the text and adds to the verisimilitude of the fictional world. Significantly, this caption is in marked contrast to the other captions in this novel, which either quote directly from the text (as in Figure 7) and/or identify the characters. The only caption that comes close is ‘Monkton Grange’, which appears underneath a picture of an old manor house where a group of people gather for a hunt. In this example, the image and the caption again function to situate the illustrated episode in a location that would have looked realistic to contemporary readers.     </p>
            <p>Whilst Trollope’s novel is, of course, a work of fiction, there are many eighteenth and nineteenth-century illustrated books that cannot easily be classified as either fiction or non-fiction (e.g., memoirs, certain modes of travel writing etc.). In these books the captions often switch between different modes of address, dictating how the images are read. Charles Knight’s multi-volume  <hi rend="italic">Pictorial Edition of the Works of Shakspere</hi> (1838-1843) is an interesting example of a book in which the literary text competes with historical images. Without their captions, many of these pictures could be viewed as fictional, but, with them, they are fixed in a time and place, such as ‘Room in Cleopatra’s Palace’ or ‘Part of Windsor Castle, built in the time of Elizabeth’. Likewise, some of the illustrations that might otherwise look historically, geographically, or botanically accurate are linked to the imaginative world of Shakespeare’s texts with captions that are direct quotations from the plays. Knight’s edition veers dramatically between the factual and fictional, and this effect is produced as much by the captions as by the images. </p>
        </div>
        <div>
          <head>Conclusion</head>
            <p>Research into the significance and meanings of the captions of historical illustrations has been hampered by the materiality of the book and the difficulty of viewing multiple illustrations and their captions side by side and comparatively. AI tools rectify this by allowing captions to be identified and interrogated alongside each other and across tens of thousands of illustrated books. We suggest that it is the indeterminacy of the caption, its position on the threshold between the image and the text, which opens it up to computational identification and analysis.  </p>
            <p>Using AI to identify the caption offers a mechanism for searching the content of illustrations in large datasets and this can be more effective than using image-only classification searches. However, AI moves beyond its efficacy as a vehicle for searching the content of pictures to reveal the significance of captions as words that describe —  <hi rend="italic">and do not describe</hi> — illustrations. Isolated as a dataset, the captions are historically significant in their own right, and can be interrogated in terms of their linguistic meanings and variations. AI also enables an analysis of the caption in its complex dialogue with the illustration and the rest of the text, allowing a recognition of signifying patterns and conventions across books and genres. For the first time, we can begin to trace at scale the ways in which the caption generates meanings and impacts on the reading and viewing process from its liminal position on the threshold.          </p>
        </div>
        <div>
          <head>Acknowledgements</head>
            <p>We would like to thank Ian Harvey and Unai Lopez-Novoa for their foundational work on <hi rend="italic">The Illustration Archive</hi> and captions. <hi rend="italic">The Illustration Archive</hi> was funded by the AHRC. The current project is funded by the AHRC/NEH as a partnership between Cardiff University and the University of Wyoming.</p>
        </div>
      </body>
      <back>
        <listBibl>
          <bibl xml:id="abbatelli_2018" label="Abbatelli 2018">Abbatelli, V. (2018) <title rend="quotes">Looking at captions to get the full picture: Framing 
             illustrations in Italian editions of <title rend="italic">Uncle Tom's cabin</title></title>, <title rend="italic">Image and Narrative</title>, 
             19(1), pp. 46-61.
          </bibl>
          <bibl xml:id="barthes_1977" label="Barthes 1977">Barthes, R. (1977) <title rend="italic">Rhetoric of the image: Image, music, text</title>, ed. and 
             trans. Stephen Heath. London: Fontana.
          </bibl>
          <bibl xml:id="bradski_2000" label="Bradski 2000">Bradski, G. (2000) <title rend="quotes">The OpenCV library</title>, 
             <title rend="italic">Dr. Dobb's</title>, 1 November. Available at: 
             <ref target="https://www.drdobbs.com/open-source/the-opencv-library/184404319">https://www.drdobbs.com/open-source/the-opencv-library/184404319</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="dmvi_n.d." label="DMVI n.d."><title rend="italic">Database of mid-Victorian illustration</title> (n.d.). Available at: 
             <ref target="https://www.dmvi.org.uk/">https://www.dmvi.org.uk/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="davies_2019" label="Davies 2019">Davies, S. (2019) <title rend="quotes"><quote rend="inline">A most venerable ruin</quote>: Word, image and 
             ideology in Guest's <title rend="italic">Geraint the son of Erbin</title></title>, <title rend="italic">Studia Celtica</title>, 53(1), pp. 53-72.
          </bibl>
          <bibl xml:id="genette_1997" label="Genette 1997">Genette, G. (1997) <title rend="italic">Paratexts: Thresholds of interpretation</title>, trans. 
             Jane E. Lewin. Cambridge: Cambridge University Press.</bibl>
          <bibl xml:id="hoffstaetter_et_al_2022" label="Hoffstaetter et al. 2022">Hoffstaetter, S. et al. (2022) <title rend="italic">pytesseract 03.10</title>. 
             Available at: <ref target="https://pypi.org/project/pytesseract/">https://pypi.org/project/pytesseract/</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="the-illustration-archive_n.d." label="The Illustration Archive n.d."><title rend="italic">The illustration archive</title> (n.d.). 
             Available at: 
             <ref target="https://illustrationarchive.cf.ac.uk/">https://illustrationarchive.cf.ac.uk/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="kim_2021" label="Kim 2021">Kim, H. (2021) <title rend="quotes">Victorian400: Colorizing Victorian illustrations</title>, 
             <title rend="italic">International Journal of Humanities and Arts Computing</title>, 15(1-2), pp. 186-202.
          </bibl>
          <bibl xml:id="lee_n.d." label="Lee n.d.">Lee, B.C.G. (n.d.) <title rend="italic">Newspaper navigator</title>. Available at: 
             <ref target="https://news-navigator.labs.loc.gov/search">https://news-navigator.labs.loc.gov/search</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="leetaru_n.d." label="Leetaru n.d.">Leetaru, K. (n.d.) <title rend="italic">500 years of book images</title>. Available at: 
             <ref target="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/">https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="liu_zhou" label="Liu and Zhou 2011">Liu, Z. and Zhou, H. (2011) <title rend="quotes">A simple and effective figure caption detection 
             system for old-style documents</title>, <title rend="italic">Proceedings of SPIE: The international society for optical engineering</title>. 
             San Jose, CA, 24-29 January 2011. Bellingham, WA: SPIE-IS&amp;T Electrionic Imaging, article #7874-28. Available at: 
             <ref target="https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents">https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="radford_et_al_2021" label="Radford et al. 2021">Radford, A. et al. (2021) <title rend="quotes">Learning transferable visual models 
             from natural language supervision</title>, <title rend="italic">arXiv</title>. 
             <ref target="https://doi.org/10.48550/arXiv.2103.00020">https://doi.org/10.48550/arXiv.2103.00020</ref> (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="radford_et_al_n.d." label="Radford et al. n.d.">Radford, A. et al. (n.d.) <title rend="italic">CLIP</title>. Available at: 
             <ref target="https://github.com/openai/CLIP/?tab=readme-ov-file#readme">https://github.com/openai/CLIP/?tab=readme-ov-file#readme</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="ruokkeinen_et_al_2023" label="Ruokkeinen, S. et al. 2023">Ruokkeinen, S. et al. <title rend="quotes">Developing a classification model 
             for graphic devices in early printed books</title>, <title rend="italic">Studia Neophilologica</title>. 
             <ref target="https://doi.org/10.1080/00393274.2023.2265985">https://doi.org/10.1080/00393274.2023.2265985</ref> (Accessed: 13 December 2023).
          </bibl>
          <bibl xml:id="shen_et_al_2021" label="Shen et al. 2021">Shen, Z. et al. (2021) <title rend="quotes">LayoutParser: A unified toolkit for deep learning 
             based document image analysis</title>, <title rend="italic">arXiv</title>. 
             <ref target="https://doi.org/10.48550/arXiv.2103.15348">https://doi.org/10.48550/arXiv.2103.15348</ref> (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="sridhar_et_al_n.d." label="Sridhar, P. et al. n.d."></bibl>
          <bibl xml:id="thomas_2017" label="Thomas 2017">Thomas, J. (2017) <title rend="italic">Nineteenth-century illustration and the digital: Studies in 
             word and image</title>. New York: Palgrave.
          </bibl>
          <bibl xml:id="wada_n.d." label="Wada n.d.">Wada, K. (n.d.) <title rend="italic">labelme: Image polygonal annotation with Python</title>. Available at: 
             <ref target="https://github.com/labelmeai/labelme">https://github.com/labelmeai/labelme</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="welsh-newspapers" label="Welsh Newspapers n.d."><title rend="italic">Welsh newspapers</title> (n.d.). Available at: 
             <ref target="https://newspapers.library.wales/home">https://newspapers.library.wales/home</ref> (Accessed 11 May 2023).
          </bibl>
        </listBibl>
      </back>
   </text>
</TEI>
