<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">Capturing Captions: Using AI to Identify and Analyse Image Captions in a Large Dataset of Historical Book 
               Illustrations</title>
            <dhq:authorInfo>
               <dhq:author_name>Julia <dhq:family>Thomas</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-1995-5558</idno>
               <dhq:affiliation>School of English Communication and Philosophy, Cardiff University</dhq:affiliation>
               <email>ThomasJ1@cardiff.ac.uk</email>
               <dhq:bio>
                  <p>Julia Thomas is a Professor at <name>Cardiff University</name> where she specialises in Victorian illustration, word and image studies, and 
                     digital humanities. She has published widely in these fields, including <title rend="italic">Pictorial Victorians: The Inscription of Values 
                     in Word and Image</title> (Ohio University Press, 2004) and <title rend="italic">Nineteenth-Century Illustration and the Digital</title> 
                     (Palgrave, 2017). Thomas has been Principal Investigator on a number of AHRC-funded projects focusing on this work and is Director of the 
                     <name>Database of Mid-Victorian Illustration</name> (<ref target="https://www.dmvi.org.uk/">https://www.dmvi.org.uk/</ref>) and 
                     <name>The Illustration Archive</name> (<ref target="https://illustrationarchive.cf.ac.uk/">https://illustrationarchive.cf.ac.uk/</ref>).
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Irene <dhq:family>Testini</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0003-4983-1844</idno>
               <dhq:affiliation>Special Collections and Archives, Cardiff University</dhq:affiliation>
               <email>TestiniI@cardiff.ac.uk</email>
               <dhq:bio>
                  <p>Irene Testini is a computer scientist based in Special Collections and Archives, <name>Cardiff University</name>. She is currently a 
                     researcher on the AHRC/NEH-funded project, <title rend="quotes">Finding a Place</title> 
                     (<ref target="https://findingaplace.org.uk/">https://findingaplace.org.uk/</ref>). Testini specialises in computer vision and its 
                     applications for historical images, and she was previously involved in projects on natural language processing.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000740</idno>
            <idno type="volume">018</idno>
            <idno type="issue">3</idno>
            <date when="2024-07-19">19 July 2024</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#visual_art"/>
               <term corresp="#media_history"/>
               <term corresp="#cultural_heritage"/>
               <term corresp="#images"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item>illustration studies</item>
                  <item>illustrations</item>
                  <item>captions</item>
                  <item>book history</item>
                  <item>big data</item>
                  <item>computer vision</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000740/000740.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <p>This article outlines how AI methods can be used to identify image captions in a large dataset of digitised historical book illustrations. This 
               dataset includes over a million images from 68,000 books published between the eighteenth and early twentieth centuries, covering works of 
               literature, history, geography, and philosophy. The article has two primary objectives. First, it suggests the added value of captions in making 
               digitized illustrations more searchable by picture content in online archives. To further this objective, we describe the methods we have used to 
               identify captions, which can effectively be re-purposed and applied in different contexts. Second, we suggest how this research leads to new 
               understandings of the semantics and significance of the captions of historical book illustrations. The findings discussed here mark a critical 
               intervention in the fields of digital humanities, book history, and illustration studies.
            </p>
         </dhq:abstract>
         <dhq:teaser>
            <p>What is the relation between an image and its caption? This article uses AI tools to analyse this complex relationship in historical book 
               illustrations.</p>
         </dhq:teaser>
      </front>
      <body>
          <div>
            <head></head>
              <p>The captions that appear alongside pictures in historical books are situated on a threshold, hanging in their own space, somewhere between the 
                 image and the text proper. Like the captions that this paper discusses, the research that we describe here also occupies a threshold: between 
                 digital humanities, with its focus on computational tools and remediation to generate research questions, and book history, with its attention 
                 to the material narratives of the book as object. It is this liminal space that has come to define the field of illustration studies. In his 
                 discussion of <quote rend="inline">thresholds</quote>, <name>Gérard Genette</name> famously shied away from discussing illustration because it 
                 was an <quote rend="inline">immense continent</quote> that was too large for him to traverse <ptr target="#genette_1997" loc="406"/>. Digital 
                 humanities has risen to this challenge both by making illustrated material more accessible than ever before and by engaging with how 
                 accessibility and scale can reveal new ways of analysing historical illustrations <ptr target="#thomas_2017"/>.
              </p>
              <p>From within this critical space, this article sets out to search the <q>immense continent</q> of illustration for what is perhaps its most 
                 marginalised territory: the caption. Very little work has been undertaken on the significance of captions and how they make their meanings, 
                 despite their prevalence in illustrated books. These issues, however, are of some importance for understanding how captions relate to the 
                 content of the pictures that they accompany, as well as the wider dialogue between word and image that characterises illustration as a mode of 
                 representation. What we outline below is the first attempt to capture the significance of captions by describing the methods and findings of 
                 research that identifies and interrogates the captions of historical book illustrations at scale, research that is only possible in a digital 
                 environment where images from different books can be viewed alongside each other.
              </p>
              <p>The dataset that is central to this study consists of over a million illustrations from 68,000 volumes in the <name>British Library</name>'s 
                 collection, which were digitised by Microsoft. These illustrations form the basis of the world's largest online resource dedicated to book 
                 illustration, <name>The Illustration Archive</name>, which we created on a previous AHRC-funded project 
                 (<ref target="https://illustrationarchive.cf.ac.uk/">https://illustrationarchive.cf.ac.uk/</ref>). The illustrations in this dataset span the 
                 sixteenth to the twentieth century, with the majority clustering around the late eighteenth and nineteenth centuries, a period that witnessed a 
                 global explosion of illustrated material and was the immediate precursor of our own visual culture. The books are written in different 
                 languages, with the vast majority in English, and they cover genres categorised as literature, history, philosophy, and geography, a 
                 diverse range that provides insight into the practices and importance of illustration at a time when this mode of representation constituted 
                 the <q>mass image</q>, the dominant visual form of the day.
              </p>
              <p>The fact that illustrations were so prevalent at this historical moment makes it imperative that we understand their constituents, not least 
                 because digitisation has made these illustrations available to view in their hundreds of thousands. Our study suggests that it is the very 
                 ambiguity and indeterminacy of captions that opens them up to computational analysis. This proposition might seem counter-intuitive: generally, 
                 we think that it is objects that are easier to <q>describe</q> and identify computationally that are best suited to analysis. Certainly, some 
                 interesting work has been done using historical illustrations in this way. In 2014 an ambitious project was undertaken by 
                 <name>Kalev Leetaru</name> to identify the illustrations in books and make them searchable using the captions alongside the paragraphs 
                 immediately preceding and following the illustrations <ptr target="#leetaru_n.d."/>. More recently, <name>Hoyeol Kim</name> has adapted 
                 Victorian illustrations as training sets for deep learning models by creating a dataset containing colourised black-and-white illustrations from 
                 the nineteenth century <ptr target="#kim_2021"/>.
              </p>
              <p>What we propose in this article is that indeterminacy — what is not clearly known, defined, or fixed — also has an analytical value. Captions 
                 can be identified computationally precisely because of their indeterminate space between the text and image; the words of the caption are 
                 neither part of the image, nor do they fully belong to the body of the text. It is this liminal space that enables the caption to be 
                 identified. This indeterminacy can also be understood and exploited on a semantic level (as we describe in the <q>Analysing Captions</q> section 
                 below). By using the captions of illustrations as a search tool, we can begin to find those instances when the captions seem to capture the 
                 visual content of the image, as well as those instances when they do not. The drive for searchability and the retrieval of relevant and accurate 
                 <q>hits</q> generates an analytical space that reveals new insights into the complex interrelationship between the caption, image, and text. Our 
                 study, therefore, sets out to identify captions both as a mechanism for searching illustrations in a big dataset and as a way of exploring how 
                 captions signify, an exploration that emerges from the very limits of searchability.
              </p>
          </div>
          <div>
            <head>What is a Caption?</head>
              <p>The function of the <term>caption</term> — a piece of text that lies alongside a visual image — can be traced back to the historical use of 
                 the <term>inscription</term>, <term>motto</term>, and <term>legend</term>. The word <term>subscription</term> was used from the sixteenth 
                 century to describe the words placed below a picture or portrait. <term>Caption</term> itself is a more recent term. Deriving from the Latin for 
                 <word>taking</word>, <term>caption</term> was originally used in the late eighteenth century in a textual context: to describe the heading of a 
                 chapter, section, or newspaper article. Its specific meaning as the <q>title below an illustration</q> emanates from America in the early 
                 decades of the twentieth century.
              </p>
              <p>The Early Modern Graphic Literacies project (University of Turku, Finland), which is working to create an historical taxonomy of visual devices 
                 in early English print from the late 1400s to 1800, draws attention to the use of the caption in these forms, noting that the distinction 
                 between captions and other image-related texts (for example, titles and running titles) is not always obvious 
                 <ptr target="#ruokkeinen_et_al_2023" loc="9"/>. Historically, our dataset picks up at the point when the Early Modern Graphic Literacies project 
                 ends, but even in this later period, the caption is beset with ambiguity and a startling variety of practices. The difficulty of converting some 
                 captions into a readable form is a consequence of the fact that the caption is generally printed using the same reproductive method as the 
                 illustration, and numerous printing methods are represented in our dataset: intaglio illustrations (e.g., etchings, steel engravings) include an 
                 etched or engraved caption as part of the image, lithographed images have lithographed captions, and wood-engraved and photomechanically 
                 produced images usually come with letterpress captions that connect the captions visually to the rest of the text. Captions were sometimes 
                 reproduced in the front matter of the book in the form of a <q>list of illustrations</q>, with the caption explicitly adopting the role of image 
                 title. The choice of words in the caption could be formulated by the author, the artist (or engraver), or by the publisher.
              </p>
              <p>The very presence of captions varies considerably across historical book illustrations. There are four main types of illustration represented in 
                 our dataset: embellishments that are primarily positioned as the headers or footers of chapters and books (also known as <term>ornaments</term> 
                 or <term>decorations</term>); pictorial capital letters; illustrations that are inset anywhere on a page of text; and full-page plates, where 
                 the image occupies its own page. Of these types of illustration, the first two (embellishments and pictorial letters) do not include captions. 
                 The second two (inset and full-page illustrations) do, although this is not uniform across all inset and full-page illustrations. In our 
                 calculations, and with <q>embellishments</q> excluded, we have identified 513,914 captioned illustrations from a total of 665,684 illustrations 
                 (where overlapping illustrations are counted as a single image). The majority of captions in our dataset are placed below rather than above the 
                 image and occupy their own space at a slight distance from the image (see Figures 3, 5, 7, 8, and 9 below). In the case of those illustrations 
                 that are set alongside the text on the page, the caption is also positioned at a slight distance from the text proper (see Figures 1, 2, and 4). 
                 These physical features allow us to identify the caption computationally in the ways we outline in the following section.
              </p>
              <p>The positioning of the caption also has semantic implications, suggesting its status on, and between, both sides of the threshold between word 
                 and image. Little critical attention has been devoted to the complexities of the caption and how it signifies in relation to the image and to 
                 the rest of the text. <name>Roland Barthes</name> is one of the few critics to have engaged with this interaction, suggesting that captions can 
                 work to <q>anchor</q> the meanings of an image by indicating how the image should be read and directing the viewer's interpretation. Barthes's 
                 discussion is based on captions in twentieth-century advertising <ptr target="#barthes_1977" loc="32–51"/>, but his ideas are borne out in 
                 recent analyses of the captions of historical illustrations. <name>Thomas Smits</name> has discovered that captions were often added and 
                 amended when illustrations were reused in nineteenth-century European newspapers <ptr target="#smits_2020"/>. <name>Sioned Davies</name> gives an 
                 account of the possible reasons why captions were added to the illustrations in the second edition of <name>Charlotte Guest</name>'s translation 
                 of <title rend="italic">Geraint the Son of Erbin</title> (1849), arguing that they add a specific geographical and historical dimension to the 
                 settings of the images that are otherwise indistinct <ptr target="#davies_2019"/>. <name>Valentina Abbatelli</name> has also drawn attention to 
                 the importance of captions in remediating the racial politics of the early twentieth-century Italian editions of <name>Harriet Beecher 
                 Stowe</name>'s novel <title rend="italic">Uncle Tom's Cabin</title> <ptr target="#abbatelli_2018"/>. According to <name>Abbatelli</name>, the 
                 same image of <name>Uncle Tom</name> learning to write is recast in different ways by the captions, with one caption suggesting that he is 
                 <q>failing</q> to write and another suggesting that he is making an effort.
              </p>
              <p>In <name>Davies</name>'s and <name>Abbatelli</name>'s strikingly different examples, the illustrations remain the same across editions; it is 
                 the captions that are added or changed, and this modification, in turn, changes the meanings of the image. A computational identification of 
                 captions gives an opportunity to scale up these analyses to see patterns that are difficult, if not impossible, to discern in the material form 
                 of the book, where comparisons between illustrations and captions are necessarily limited. The next section describes the AI methods we used to 
                 identify captions in this large dataset of historical illustrations. These methods can effectively be re-purposed across other datasets to add 
                 value to the searchability of illustrations. The section following suggests how this identification leads to new understandings of the 
                 significance and semantics of the captions of historical book illustrations, which are of relevance not only to scholars working in digital 
                 humanities but also to those whose interests lie in illustration studies and its attendant fields.
              </p>
          </div>
          <div>
            <head>Using AI to Identify Captions</head>
              <p>The first stage in our identification process was to isolate the captions from the main body of the text and the image, a task that has proven 
                 challenging for both machine learning and humanists attempting to formalise what a <term>caption</term> is. <name>Zongyi Liu</name> and 
                 <name>Hanning Zhou</name> provide a heuristics-based approach to caption finding, suggesting a series of rules to identify a caption based on 
                 size of the font, position relative to the image, and size of the caption itself <ptr target="#liu_zhou_2011"/>. Employing this method led to a 
                 high number of false positives and false negatives; our dataset presents wide variation in style of layouts, making it impossible to list a 
                 reliable set of heuristics. In order to isolate the caption, we needed to identify the other elements present on the page; that is, we needed to 
                 <q>parse</q> the layout of the page. For simplicity, we identified four building blocks:
                 <list type="ordered">
                   <item>Text, referring to the main body of text, such as paragraphs, introductions, and any text which does not fall into our two other textual 
                      categories;</item>
                   <item>Captions;</item>
                   <item>Headings, referring to titles, chapter titles, and any form of text appearing at the top of the page that might be shared across 
                      subsequent pages; and</item>
                   <item>Figures.</item>
                 </list>
              </p>
              <p>We then labelled a random subsample of 1,000 images from our dataset using labelme 
                 (<ref target="https://github.com/wkentaro/labelme">https://github.com/wkentaro/labelme</ref>), a graphical annotation tool that allows users to 
                 draw bounding boxes on an image and assign them a label. The annotations of each page are then saved as a file that contains the coordinates of 
                 each bounding box and its respective label. Figure 1 shows a snippet of the labelling tool with one of our images. In the left panel, boxes can 
                 be drawn around the different sections of the page, highlighting its layout. On the right, each bounding box is labelled with one of the 
                 categories we have identified above. These annotations allow us to re-train a computer vision model to automatically identify the layout of a 
                 page and isolate a caption.
              </p>
              <figure xml:id="figure01">
                <head>A snippet of the annotation process using the labelme tool.</head>
                 <figDesc>Shows an illustrated page being worked on using the labelme tool. The illustrated page has delineated polygons that are colour coded 
                    for each section identified, namely two green boxes for text, one red box for heading, one brown box for figure (a stone in the woods), and 
                    one blue box for the caption (which reads <quote rend="inline">The Ellicott Stone</quote>. To the right of the illustrated page are four 
                    vertically stacked panels: <q>Flags</q>; <q>Label List (heading, text, figure, caption)</q>, i.e., a list of all labels used across the 
                    selected folder of images; <q>Polygon Labels (heading, text, text, figure, caption)</q>, i.e., a list of the labels present in the image, 
                    <q>File List</q>, showing the list of files in the selected folder, with the working image ticked.</figDesc>
                  <graphic url="resources/images/figure01.png" style="width: 750px"/>
              </figure>
              <p>With this annotated dataset, we turn to fine-tuning. In deep learning, fine-tuning is a transfer learning method where a pre-trained model is 
                 trained on new data to fine-tune it on a more specific task. In our case, that task is layout parsing on historical illustrated books. Layout 
                 parsing is the process of automatically detecting the layout structure of a page using computer vision. A highly effective recent model is 
                 Layout Parser (<ref target="https://layout-parser.github.io/">https://layout-parser.github.io/</ref>), which provides a range of models 
                 pretrained on diverse databases. We used the model trained on the PrImA dataset, which is is a Mask R-CNN model trained on newspapers that 
                 identifies text, figures, titles, tables, maths, and separators. Newspapers present a similarly busy and interlocked layout as historical 
                 illustrated books, meaning that this model most closely resembled our data. We ran training for 10,000 iterations with a learning rate of 
                 0.001, achieving accuracy of 98% on the validation set. The majority of errors occured on pictorial capital letters: at the annotation stage, we 
                 decided for simplicity to include pictorial letters within the main body's bounding box labelled as text, but the model often labels them 
                 separately as figures.</p>
              <p>We deployed our fine-tuned model on <name>The Illustration Archive</name> to predict the layout of each page and isolate the captions. The 
                 model's output consists of coordinates for the layout element of each page which can be visualised as bounding boxes. Figures 2–4 demonstrate an 
                 example output of the model: on each page, the model has predicted the position and category of a layout element, which is displayed with a 
                 confidence score as a percentage. We can see in Figure 2 that three bounding boxes with the label <q>figure</q> have been predicted, as the 
                 model is unsure whether the two illustrations should be considered separately. An analogous situation occurs in Figure 4 with the captions.
              </p>
              <figure xml:id="figure02">
                <head>An example of the output of the parsing tool.</head>
                <figDesc>Shows an illustrated page in which different sections have been correctly identified: a heading, three sections of text, and two 
                    overlapping figures with one corresponding caption reading <quote rend="inline">Kynance Cove</quote>. The figures show a coastline and a 
                    close-up of a rock.</figDesc>
                  <graphic url="resources/images/figure02.png" style="width: 500px"/>
              </figure>
              <figure xml:id="figure03">
                <head>An example of the output of the parsing tool.</head>
                <figDesc>Shows a full-page illustration in which different sections have been correctly identified: one figure and one caption, reading 
                   <quote rend="inline">The herbs caught strongly afire, and the flames beat upon Keola</quote>. The figure shows two men startled by the flames 
                   in front of them; it is a scene from <title rend="quotes">The Isle of Voices</title> by <name>Robert Louis Stevenson</name>.</figDesc>
                  <graphic url="resources/images/figure03.png" style="width: 500px"/>
               </figure>
               <figure xml:id="figure04">
                 <head>An example of the output of the parsing tool.</head>
                 <figDesc>Shows a book page in which different sections have been correctly identified: one heading, two sections of text, one figure, and two 
                    captions reading respectively <quote rend="inline"><name>Harvey</name>'s Patent Husband's Stamps</quote> and <quote rend="inline">Scale: 1 inch 
                    1 foot Fig. 32.</quote>. The figure depicts a technical diagram of a steam stamp.</figDesc>
                   <graphic url="resources/images/figure04.png" style="width: 500px"/>
               </figure>
               <p>After obtaining labelled layout structures, it becomes easier to perform optical character recognition (OCR) exclusively on the captions. We 
                  used pytesseract (<ref target="https://pypi.org/project/pytesseract/">https://pypi.org/project/pytesseract/</ref>), a Python library that is a 
                  wrapper for Google's Tesseract OCR engine. The main disadvantage of this tool is its speed when attempting to use multiple languages. For this 
                  reason, we allowed the tool to rely only on English at this stage, meaning that captions in foreign languages were not processed correctly; we 
                  are planning to fix this issue in a future iteration.
               </p>  
               <p>Through these computer vision techniques, we have obtained a dataset of captions that can be used as a search index for the illustrations in 
                  <name>The Illustration Archive</name>. Integrated as metadata on <name>The Illustration Archive</name> website, the captions allow users to 
                  explore the images, search by captions, and analyse the relation between the words in the captions and the content of the illustrations. The 
                  identification of the captions has also opened up possibilities to use image-to-text and text-to-image models such as CLIP (Contrastive 
                  Language-Image Pre-training: <ref target="https://openai.com/research/clip">https://openai.com/research/clip</ref>) to cluster images with
                  similar captions, identify objects in illustrations using CLIP text encodings as well as our fine-tuned caption encodings, and discover 
                  connections between language representation and visual representation. CLIP is a deep learning model that draws simultaneously from text and 
                  image. It is trained on vast amounts of data consisting of pairs of images and descriptive text, allowing it to train a text encoder and an 
                  image encoder to produce text and visual embeddings that are close to each other. In this way, a textual prompt can be used to search for 
                  images with embeddings resembling the textual prompt. We have used OpenCLIP ViT-L/14 model (Vision Transformer) to build a searchable index of 
                  the illustrations in <name>The Illustration Archive</name>, following the work of the Visual Geometry Group at the <name>University of 
                  Oxford</name> (see their WISE Image Search Engine, which uses AI tools to make image databases searchable by content: 
                  <ref target="https://www.robots.ox.ac.uk/~vgg/software/wise/">https://www.robots.ox.ac.uk/~vgg/software/wise/</ref>).
               </p>
           </div>
           <div>
             <head>Analysing Captions</head>
               <p>Developments in deep learning models mean that we are no longer wholly reliant on textual metadata (tags, bibliographic information, or, 
                  indeed, captions) to search the content of images in datasets of historical images. Back in 2004 when we started developing the 
                  <name>Database of Mid-Victorian Illustration</name> (<ref target="https://www.dmvi.org.uk/">https://www.dmvi.org.uk/</ref>), the only way of 
                  making the 900 or so images searchable to users was manually to tag their content. Captions, however, can still add value to content-driven 
                  visual or iconographic searches. Online archives of historical newspapers often use captions for searchability, although the captions of these 
                  illustrations do not necessarily function in the same way as the captions in books. In newspapers, the caption is generally a <q>heading</q> 
                  that appears at the top of the feature and is the title both for the illustration (if there is one) and the textual article. In the database 
                  <name>Welsh Newspapers</name> (<ref target="https://newspapers.library.wales/">https://newspapers.library.wales/</ref>), for instance, the 
                  title in the form of a heading has been identified as a distinct characteristic of the page. Any alternative captions underneath an image have 
                  been OCRed along with the full text and can be searched accordingly. A recent use of OCRed headlines, captions, and other text alongside deep 
                  learning models is the <name>Newspaper Navigator</name>, which searches the visual content of historical newspapers in the 
                  <name>Library of Congress</name> <ptr target="#lee_n.d."/>.
               </p>
               <p>We conducted a case study to evaluate the effectiveness of using our identified captions to search for specific illustrations in the dataset. 
                  In partnership with <name>Lambeth Palace Library</name>, we searched for book illustrations of cathedrals that would enrich the library's 
                  collections. We found 2,087 illustrations captioned with the word <word>cathedral</word> and 103 with the word <word>cathedrals</word>. 
                  Data-mining the captions in this way involves the same discrimination as might be adopted on a Google search, in the sense that we needed to 
                  anticipate the words that might be used in these captions (e.g., additional terms not covered by the word <word>cathedral</word>, such as 
                  <name>York Minster</name>, <name>Westminster Abbey</name>, and <word>chapterhouse</word>, which might signal a cathedral building).
               </p>
               <p>In this study, searching the captions proved a highly effective way of identifying relevant illustrations from the dataset because the words of 
                  the captions describe in a broad sense what the illustrations depict: <word>cathedrals</word>. In some instances, a caption search can be 
                  <emph>more</emph> effective than using only a visual search. An image classification (as opposed to a caption) search would retrieve images of 
                  cathedrals, but it would also return churches since there are no specific architectural signifiers that mark out a cathedral from a church. 
                  Cathedrals are generally larger and more elaborate than churches, but their specific distinction — which would not usually be signalled in an 
                  illustration or in visual searches — is that cathedrals are the seat of a bishop.
               </p>
               <p>Likewise, a caption search is able to retrieve illustrations that show features of cathedrals that are not iconographically specific to 
                  cathedrals and would not, therefore, be found in image-based searches that tend to cluster key features of cathedrals (external <q>grand</q> 
                  perspectives, spires, and high-domed interiors). A search via the captions revealed illustrations of floor plans of cathedrals and different 
                  perspectives where the cathedral is in the far distance, so not obviously identified as a cathedral, as well as many images of interiors and 
                  interior features (e.g., effigies, gargoyles, windows, staircases; see Figure 5). The captions here retrieve images of cathedrals that would 
                  not otherwise be found.
               </p>
               <p>This example of the effectiveness of searching captions in large datasets raises an important point about the nature of captions, especially in 
                  illustrated non-fiction books. As contemporary labels attached to the images, captions can function as <q>expert tags</q>: authoritative and 
                  reliable indicators of what the illustration contains. They can, for instance, identify an otherwise unrecognisable interior setting as 
                  depicting <quote rend="inline">Bristol Cathedral</quote> (Figure 5).
               </p>
               <figure xml:id="figure05">
                 <head>Example of an image retrieved by searching the captions for <word>cathedral</word>. From [<name>Samuel Griffiths Tovey</name>], 
                    <title rend="italic">Cursory Observations on the Churches of <name>Bristol</name>. By an Occasional Visitor</title>, second edition 
                    (<name>Bristol</name>, <name>England</name>: Mirror Office, 1843), facing p. 6.</head>
                 <figDesc>A black and white illustration that shows an interior view and architectural features of a small space in 
                    <name>Bristol Cathedral</name> that includes a curved stairway, wall carvings, and a decorated curved part of the ceiling. The caption 
                    underneath the image is <quote rend="inline">Bristol Cathedral. View from the Elder Lady's Chapel</quote>.</figDesc>
                   <graphic url="resources/images/figure05.png" style="width: 500px"/>
               </figure>
               <p>This aspect of the caption as a reliable descriptor of the image can add to the searchability of large illustration datasets. However, captions 
                  are far from neutral or ahistorical descriptors of the image, and it is this problematic relationship that comes to the fore in our research. 
                  By identifying hundreds of thousands of captions, we have effectively isolated a corpus of textual metadata that is culturally significant. 
                  Even a term as apparently straightforward as <word>cathedral</word> has a historical resonance: its use in a caption speaks to the 
                  establishment and embeddedness of (in this case) the <name>Church of England</name>, the rise of tourism in cathedral towns and cities in the 
                  <name>UK</name>, and the changing demographics brought about by industrialisation, as no new cathedrals were created for nearly 300 years until 
                  the Victorian period when the increased population in major industrial cities necessitated new cathedrals. There is a civic pride in the 
                  captioning of <word>cathedrals</word>, especially as illustrations of cathedrals most commonly appear in the context of tourist guidebooks and 
                  local histories, like the one in Figure 5. <word>Cathedral</word> is just one example of the cultural significance of the words used in 
                  captions. Others are more overtly political and unsettling, such as racist terms that are sometimes used.
               </p>
               <p>We can interrogate this corpus of captions in multiple ways, including searching for the frequency distribution of words. Figure 6 shows a 
                  random subset of words sampled from different frequency bands.</p>
               <figure xml:id="figure06">
                 <head>A word cloud obtained using Python from a random sample of words occurring in the corpus of captions.</head>
                 <figDesc>A multi-coloured cloud of words of different sizes taken from a random subset of words in our captions dataset. The largest words are: 
                    church, house, old, and street.</figDesc>
                   <graphic url="resources/images/figure06.png" style="width: 750px"/>
               </figure>
               <p>The arbitrary nature of these words reflects the mixed genres that make up our illustration dataset, but, seen in the form of a word cloud, the 
                  words in the captions clearly foreground dominant nineteenth-century values. The most frequent keywords are <word>house</word> (in 3,043 
                  captions) and <word>church</word> (in 2,688 captions); there is also an emphasis on the <word>old</word> (2,561 captions), further signalled 
                  by terms like <word>castle</word> (1,778 captions), <word>ancient</word> (592 captions) and <word>abbey</word> (556 captions); and the 
                  <word>new</word> (in 1,760 captions), signalled by <word>photograph</word> (2,009 captions). Aspects of the countryside and landscape are 
                  emphasised, alongside towns and cities (<word><name>London</name></word> is one of the top terms, used in 2,219 captions). Gender categories 
                  are particularly revealing, with 519 captions containing the word <word>man</word> and 307 containing <word>woman</word> (the plural suggests a 
                  similar margin: 274 <word>men</word> and 187 <word>women</word>); <word>Mr</word> appears 1,198 times, <word>Mrs</word> 497 times, and 
                  <word>Miss</word> 250 times.
               </p>
               <p>Although it is highly likely that the illustrations in the dataset contain more illustrations of men than women, this is not necessarily the 
                  full picture. A comparison search for image features can prove illuminating here. An image feature search on a subset of 15,000 illustrations 
                  using CLIP found 524 men and 14 women. While, surprisingly, there were no groups of men, 2,487 groups of women and 1,562 groups of people, 
                  more generally, were recognised. More work needs to be done comparing different search mechanisms. Our analysis indicates that whilst there is 
                  an undoubted bias in the illustrations, there is also a bias in the captions, which might not necessarily be the same as the bias in the 
                  images. Identifying and isolating captions in this way exposes how captions emphasise and marginalise features of illustrations.
               </p>
               <p>The caption's privileging and concomitant marginalising of pictorial details is, in fact, one of its major characteristics. The caption of 
                  Figure 5, <quote rend="inline">Bristol Cathedral. VIEW FROM THE ELDER LADY'S CHAPEL</quote>, contains no mention of stairs, flagstones, 
                  archways, monuments, or any other architectural features. However descriptive and objective it appears, the caption is never fully reflective 
                  of the image because it is couched in another form: words, which cannot fully describe the visual features of an image. What captions emphasise 
                  and marginalise, then, is highly significant in that they provide a unique insight into how illustrations were viewed at the time, what were 
                  regarded as their most salient features, and what features were overlooked.
               </p>
               <p>Using AI to identify the captions at scale allows us to look not only at the words of the caption, but also to trace patterns in how the 
                  captions signify in relation to the illustrations and the rest of the text. Our findings point to the fact that the caption makes its meanings 
                  in two main ways: it signifies in its conjunction with the image, and it acts as a point of connection, or bridge, between the illustration and 
                  the rest of the text. Either, or both, of these relationships can be at play in any captioned illustration, but they seem primarily to be 
                  determined by the genre of the book.
               </p>
               <p>In texts conventionally classified as non-fiction, including works of history, geography, and science (in the broadest sense), the role of the 
                  caption and its relation to the image is characterised by the first of these models, with the illustration and caption forming a complete 
                  signifying unit. In theory, this unit could be isolated from the rest of the text and still make sense. Although the caption 
                  <quote rend="inline">Bristol Cathedral</quote> fails to mention certain aspects of the image, it nevertheless signifies in relation to the 
                  illustration, with the picture and the words of the caption constituting a signifying partnership.
               </p>
               <p>This is the defining model across the range of non-fiction books in the dataset. In simple terms, the captions in these books describe the 
                  facts of what is represented in the illustrations; following the Latin etymology of <word>caption</word> they attempt to <q>capture</q> or 
                  <q>seize</q> the meanings of the picture. In our dataset, there are tens of thousands of botanical or zoological illustrations captioned with 
                  the names of the species, illustrative portraits of people captioned with their names, and illustrations of locations or buildings 
                  with captions that identify the locations and buildings they represent. In these examples, the signifying conjunction of the illustration and 
                  caption creates and enforces a notion of equivalence: the idea that the caption replicates in words what the illustration shows and vice versa 
                  (however illusory this notion is).
               </p>
               <p>In illustrated fiction and literary texts (short stories, novellas, novels, plays, poetry, etc.), captions are usually citational, taking the 
                  form of quotes or deriving from the words of the text. These captions frequently state the characters' names and/or what they are doing: 
                  <quote rend="inline">Mr. Perry passed by on horseback</quote> is a typical caption from a Victorian illustrated edition of 
                  <name>Jane Austen</name>'s <title rend="italic">Emma</title> (1896). (The designations <word>Mr.</word>, <word>Mrs.</word>, and 
                  <word>Miss</word> are more prevalent in the captions of fictional texts where they are used to name a character than they are in non-fiction 
                  captions.) In literary texts, the contexts and meanings of the illustrations and the captions depend on the text; they do not necessarily make 
                  sense in isolation like the non-fiction captioned illustrations described above. The role of these literary captions is to point to the 
                  specific episode being illustrated, a role that serves a practical function because book illustrations often appeared on different pages than 
                  the text that they depicted (thus many captions also include the relevant page numbers; see Figure 3). Instead of turning in towards the 
                  illustration, then, these captions point outside, acting as a bridge between the illustration and the text proper.
               </p>
               <p>An example of this bridging technique is the caption <quote rend="inline">Never is a very long word</quote>, which accompanies an illustration 
                  for <name>Anthony Trollope</name>'s novel <title rend="italic">Orley Farm</title> (Figure 7).
               </p>
               <figure xml:id="figure07">
                 <head><name>John Everett Millais</name>, <title rend="quotes">Never is a very long word</title>, engr. <name>Dalziel</name> Brothers. 
                    Illustration for <name>Anthony Trollope</name>, <title rend="italic">Orley Farm</title> (<name>London</name>: Chapman and Hall, 1862), facing 
                    p. 77.</head>
                 <figDesc>A black and white, full-page wood-engraved image of a Victorian interior that shows two women in Victorian dress sat in front of a 
                    window. The younger woman stares at the floor while the older woman touches her hand. The caption beneath the image is 
                    <quote rend="inline">Never is a very long word</quote>.</figDesc>
                   <graphic url="resources/images/figure07.png" style="width: 500px"/>
               </figure>
               <p><quote rend="inline">Never is a very long word</quote> is a direct quotation from the novel, and this caption is placed securely within 
                  quotation marks when it appears as a title on the contents page in the book edition. By citing the text of the novel, the caption works to 
                  direct the reader to the episode that is being illustrated, isolating the moment depicted in the illustration from several possibilities (the 
                  pose of the two seated ladies could fit any number of episodes). In this way, the caption works to connect the image to the rest of the text. 
                  As if to prove that an image cannot translate into words, the illustration does not depict the words <quote rend="inline">Never is a very long 
                  word</quote>. How could these words even be illustrated in a picture? This caption, therefore, is a good example of where the use of captions 
                  to search for picture content would fail (the content of this image is manually tagged in the <name>Database of Mid-Victorian 
                  Illustration</name>). In terms of searchability, the use of captions is far more effective when searching across the non-fiction books in the 
                  dataset.
               </p>
               <p>Our analysis, therefore, indicates that the caption does not signify in the same way across all books. Rather, there are specific conventions 
                  at play that are determined primarily by the broad generic classification of the book. An acknowledgement of these conventions allows us to 
                  recognise those instances where they are being adapted and manipulated, with implications for the meanings of the text and how it is read. An 
                  example of this is Figure 8, the frontispiece illustration for <name>Trollope</name>'s <title rend="italic">Orley Farm</title>. Unlike the 
                  caption in Figure 7, which follows the conventions of literary illustrations and quotes directly from the novel, Figure 8 draws instead on the 
                  conventions of the <q>factual</q> caption.
               </p>
               <figure xml:id="figure08">
                 <head><name>John Everett Millais</name>, <title rend="quotes">ORLEY FARM</title>, engr. <name>Dalziel</name> Brothers. Illustration for 
                    <name>Anthony Trollope</name>, <title rend="italic">Orley Farm</title> (<name>London</name>: Chapman and Hall, 1862), frontispiece.</head>
                 <figDesc>A black and white, full-page wood-engraved image of a pretty landscape with trees, which shows a large house set on top of a small 
                    hill and a milkmaid milking a cow in the valley below. Underneath the image is a caption, which reads 
                    <quote rend="inline">ORLEY FARM</quote>.</figDesc>
                   <graphic url="resources/images/figure08.png" style="width: 500px"/>
               </figure>
               <p>The illustration in Figure 8 is based on a real location, the farmhouse where <name>Trollope</name> lived as a boy. The style of the image 
                  itself crosses generic categories: pictures of rural landscapes frequently appeared in both fictional and non-fictional books, including 
                  gift books, poetry anthologies, and topographical works. However, it is the caption that makes the link to a real geographic location. 
                  Capitalised to lend it more authority, <quote rend="inline">ORLEY FARM</quote> mimics the style of captions in illustrated travel books that 
                  label the place illustrated (see, for example, Figure 9).
               </p>
               <figure xml:id="figure09">
                 <head><title rend="quotes">BACKBARROW MILLS</title>, illustration for <name>Edwin Waugh</name>, <title rend="italic">Rambles in the Lake Country 
                    and Other Travel Sketches</title>, ed. <name>George Milner</name> (<name>Manchester</name> and <name>London</name>: John Heywood, 1893), p. 
                    68.</head>
                 <figDesc>A black and white wood-engraved illustration showing a river with trees and buildings around it and a quaint stone water-mill building 
                    on the left-hand side. The caption underneath the image reads <quote rend="inline">BACKBARROW MILLS</quote>.</figDesc>
                   <graphic url="resources/images/figure09.png" style="width: 750px"/>
               </figure>
               <p>The association of the caption <title rend="quotes">ORLEY FARM</title> with the designatory captions of non-fiction books gives a sense of 
                  veracity to the novel at its outset (it is placed as the frontispiece illustration), and this has implications for the investment and immersion 
                  of readers in the imaginative world of the novel. The caption here functions alongside the realist conventions of the text and adds to the 
                  verisimilitude of the fictional world. Significantly, this caption is in marked contrast to the other captions in this novel, which either 
                  quote directly from the text (as in Figure 7) and/or identify the characters. The only caption that comes close is <quote rend="inline">Monkton 
                  Grange</quote>, which appears underneath a picture of an old manor house where a group of people gather for a hunt. In this example, the image 
                  and the caption again function to situate the illustrated episode in a location that would have looked realistic to contemporary readers
               </p>
               <p>Whilst <name>Trollope</name>'s novel is, of course, a work of fiction, there are many eighteenth- and nineteenth-century illustrated books that 
                  cannot easily be classified as either fiction or non-fiction (e.g., memoirs, certain modes of travel writing, etc.). In these books, the 
                  captions often switch between different modes of address, dictating how the images are read. <name>Charles Knight</name>'s multi-volume  
                  <title rend="italic">Pictorial Edition of the Works of <name>Shakspere</name></title> (1838-1843) is an interesting example of a book in which 
                  the literary text competes with historical images. Without their captions, many of these pictures could be viewed as fictional, but, with them, 
                  they are fixed in a time and place, such as <quote rend="inline">Room in <name>Cleopatra</name>'s Palace</quote> or <quote rend="inline">Part 
                  of <name>Windsor Castle</name>, built in the time of <name>Elizabeth</name></quote>. Likewise, some of the illustrations that might otherwise 
                  look historically, geographically, or botanically accurate are linked to the imaginative world of <name>Shakespeare</name>’s texts with 
                  captions that are direct quotations from the plays. <name>Knight</name>'s edition veers dramatically between the factual and fictional, and 
                  this effect is produced as much by the captions as by the images.
               </p>
           </div>
           <div>
             <head>Conclusion</head>
               <p>Research into the significance and meanings of the captions of historical illustrations has been hampered by the materiality of the book and 
                  the difficulty of viewing multiple illustrations and their captions side by side and comparatively. AI tools rectify this by allowing captions 
                  to be identified and interrogated alongside each other and across tens of thousands of illustrated books. We suggest that it is the 
                  indeterminacy of the caption, its position on the threshold between the image and the text, which opens it up to computational identification 
                  and analysis.
               </p>
               <p>Using AI to identify captions offers a mechanism for searching the content of illustrations in large datasets, and this can be more effective 
                  than using image-only classification searches. However, AI moves beyond its efficacy as a vehicle for searching the content of pictures to 
                  reveal the significance of captions as words that describe — <emph>and do not describe</emph> — illustrations. Isolated as a dataset, 
                  captions are historically significant in their own right and can be interrogated in terms of their linguistic meanings and variations. AI also 
                  enables analysis of the caption in its complex dialogue with the illustration and the rest of the text, allowing recognition of signifying 
                  patterns and conventions across books and genres. For the first time, we can begin to trace at scale the ways in which the caption generates 
                  meanings and impacts on the reading and viewing process from its liminal position on the threshold.
               </p>
           </div>
           <div>
             <head>Acknowledgements</head>
               <p>We would like to thank <name>Ian Harvey</name> and <name>Unai Lopez-Novoa</name> for their foundational work on <name>The Illustration 
                  Archive</name> and captions. <name>The Illustration Archive</name> was funded by the <name>AHRC</name>. The current project is funded by the 
                  <name>AHRC</name>/<name>NEH</name> as a partnership between <name>Cardiff University</name> and the <name>University of Wyoming</name>.
               </p>
           </div>
       </body>
      <back>
        <listBibl>
          <bibl xml:id="abbatelli_2018" label="Abbatelli 2018">Abbatelli, V. (2018) <title rend="quotes">Looking at captions to get the full picture: Framing 
             illustrations in Italian editions of <title rend="italic">Uncle Tom's cabin</title></title>, <title rend="italic">Image and Narrative</title>, 
             19(1), pp. 46-61.
          </bibl>
          <bibl xml:id="barthes_1977" label="Barthes 1977">Barthes, R. (1977) <title rend="italic">Rhetoric of the image: Image, music, text</title>, ed. and 
             trans. Stephen Heath. London: Fontana.
          </bibl>
          <bibl xml:id="bradski_2000" label="Bradski 2000">Bradski, G. (2000) <title rend="quotes">The OpenCV library</title>, 
             <title rend="italic">Dr. Dobb's</title>, 1 November. Available at: 
             <ref target="https://www.drdobbs.com/open-source/the-opencv-library/184404319">https://www.drdobbs.com/open-source/the-opencv-library/184404319</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="dmvi_n.d." label="DMVI n.d."><title rend="italic">Database of mid-Victorian illustration</title> (n.d.). Available at: 
             <ref target="https://www.dmvi.org.uk/">https://www.dmvi.org.uk/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="davies_2019" label="Davies 2019">Davies, S. (2019) <title rend="quotes"><quote rend="inline">A most venerable ruin</quote>: Word, image and 
             ideology in Guest's <title rend="italic">Geraint the son of Erbin</title></title>, <title rend="italic">Studia Celtica</title>, 53(1), pp. 53-72.
          </bibl>
          <bibl xml:id="genette_1997" label="Genette 1997">Genette, G. (1997) <title rend="italic">Paratexts: Thresholds of interpretation</title>, trans. 
             Jane E. Lewin. Cambridge: Cambridge University Press.</bibl>
          <bibl xml:id="hoffstaetter_et_al_2022" label="Hoffstaetter et al. 2022">Hoffstaetter, S. et al. (2022) <title rend="italic">pytesseract 03.10</title>. 
             Available at: <ref target="https://pypi.org/project/pytesseract/">https://pypi.org/project/pytesseract/</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="the-illustration-archive_n.d." label="The Illustration Archive n.d."><title rend="italic">The illustration archive</title> (n.d.). 
             Available at: 
             <ref target="https://illustrationarchive.cf.ac.uk/">https://illustrationarchive.cf.ac.uk/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="kim_2021" label="Kim 2021">Kim, H. (2021) <title rend="quotes">Victorian400: Colorizing Victorian illustrations</title>, 
             <title rend="italic">International Journal of Humanities and Arts Computing</title>, 15(1-2), pp. 186-202.
          </bibl>
          <bibl xml:id="lee_n.d." label="Lee n.d.">Lee, B.C.G. (n.d.) <title rend="italic">Newspaper navigator</title>. Available at: 
             <ref target="https://news-navigator.labs.loc.gov/search">https://news-navigator.labs.loc.gov/search</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="leetaru_n.d." label="Leetaru n.d.">Leetaru, K. (n.d.) <title rend="italic">500 years of book images</title>. Available at: 
             <ref target="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/">https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="liu_zhou_2011" label="Liu and Zhou 2011">Liu, Z. and Zhou, H. (2011) <title rend="quotes">A simple and effective figure caption detection 
             system for old-style documents</title>, <title rend="italic">Proceedings of SPIE: The international society for optical engineering</title>. 
             San Jose, CA, 24-29 January 2011. Bellingham, WA: SPIE-IS&amp;T Electrionic Imaging, article #7874-28. Available at: 
             <ref target="https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents">https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents</ref> 
             (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="radford_et_al_2021" label="Radford et al. 2021">Radford, A. et al. (2021) <title rend="quotes">Learning transferable visual models 
             from natural language supervision</title>, <title rend="italic">arXiv</title>. 
             <ref target="https://doi.org/10.48550/arXiv.2103.00020">https://doi.org/10.48550/arXiv.2103.00020</ref> (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="radford_et_al_n.d." label="Radford et al. n.d.">Radford, A. et al. (n.d.) <title rend="italic">CLIP</title>. Available at: 
             <ref target="https://github.com/openai/CLIP/?tab=readme-ov-file#readme">https://github.com/openai/CLIP/?tab=readme-ov-file#readme</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="ruokkeinen_et_al_2023" label="Ruokkeinen, S. et al. 2023">Ruokkeinen, S. et al. <title rend="quotes">Developing a classification model 
             for graphic devices in early printed books</title>, <title rend="italic">Studia Neophilologica</title>. 
             <ref target="https://doi.org/10.1080/00393274.2023.2265985">https://doi.org/10.1080/00393274.2023.2265985</ref> (Accessed: 13 December 2023).
          </bibl>
          <bibl xml:id="shen_et_al_2021" label="Shen et al. 2021">Shen, Z. et al. (2021) <title rend="quotes">LayoutParser: A unified toolkit for deep learning 
             based document image analysis</title>, <title rend="italic">arXiv</title>. 
             <ref target="https://doi.org/10.48550/arXiv.2103.15348">https://doi.org/10.48550/arXiv.2103.15348</ref> (Accessed: 11 May 2023).
          </bibl>
          <bibl xml:id="smits_2020" label="Smits 2020">Smits, T. (2020) <title rend="italic">The European illustrated press and the emergence of a transnational 
             visual culture of the news, 1842-1870</title>. London: Routledge.
          </bibl>
          <bibl xml:id="sridhar_et_al_n.d." label="Sridhar, P. et al. n.d.">Sridhar, P. et al. (n.d.) <title rend="italic">WISE image search engine (WISE)</title>. 
             Available at: 
             <ref target="https://www.robots.ox.ac.uk/~vgg/software/wise/downloads/wikiworkshop2023/sridhar2023wise.pdf">https://www.robots.ox.ac.uk/~vgg/software/wise/downloads/wikiworkshop2023/sridhar2023wise.pdf</ref> 
             (Accessed 20 December 2023).
          </bibl>
          <bibl xml:id="thomas_2017" label="Thomas 2017">Thomas, J. (2017) <title rend="italic">Nineteenth-century illustration and the digital: Studies in 
             word and image</title>. New York: Palgrave.
          </bibl>
          <bibl xml:id="wada_n.d." label="Wada n.d.">Wada, K. (n.d.) <title rend="italic">labelme: Image polygonal annotation with Python</title>. Available at: 
             <ref target="https://github.com/labelmeai/labelme">https://github.com/labelmeai/labelme</ref> 
             (Accessed 11 May 2023).
          </bibl>
          <bibl xml:id="welsh-newspapers" label="Welsh Newspapers n.d."><title rend="italic">Welsh newspapers</title> (n.d.). Available at: 
             <ref target="https://newspapers.library.wales/home">https://newspapers.library.wales/home</ref> (Accessed 11 May 2023).
          </bibl>
        </listBibl>
      </back>
   </text>
</TEI>
