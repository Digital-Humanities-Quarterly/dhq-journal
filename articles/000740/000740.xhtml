<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Capturing Captions: Using AI to Identify and Analyse Image Captions in a Large Dataset
                  of Historical Book 
                  Illustrations</h1>
               
               <div class="author"><span style="color: grey">Julia Thomas
                     </span> &lt;<a href="mailto:ThomasJ1_at_cardiff_dot_ac_dot_uk" onclick="javascript:window.location.href='mailto:'+deobfuscate('ThomasJ1_at_cardiff_dot_ac_dot_uk'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('ThomasJ1_at_cardiff_dot_ac_dot_uk'); return false;">ThomasJ1_at_cardiff_dot_ac_dot_uk</a>&gt;, Cardiff University</div>
               
               <div class="author"><span style="color: grey">Irene Testini
                     </span> &lt;<a href="mailto:Testinii_at_cardiff_dot_ac_dot_uk" onclick="javascript:window.location.href='mailto:'+deobfuscate('Testinii_at_cardiff_dot_ac_dot_uk'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('Testinii_at_cardiff_dot_ac_dot_uk'); return false;">Testinii_at_cardiff_dot_ac_dot_uk</a>&gt;, Cardiff University</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Capturing%20Captions%3A%20Using%20AI%20to%20Identify%20and%20Analyse%20Image%20Captions%20in%20a%20Large%20Dataset%20of%20Historical%20Book%20Illustrations&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Thomas&amp;rft.aufirst=Julia&amp;rft.au=Julia%20Thomas&amp;rft.au=Irene%20Testini"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>This article outlines how AI methods can be used to identify image captions in a large
                     dataset of digitised historical book illustrations. This 
                     dataset includes over a million images from 68,000 books published between the eighteenth
                     to early twentieth centuries, covering works of 
                     literature, history, geography, and philosophy. The article has two primary objectives.
                     First, it suggests the added value of captions in making 
                     digitized illustrations more searchable by picture content in online archives. To
                     further this objective, we describe the methods we have used to 
                     identify captions, which can effectively be re-purposed and applied in different contexts.
                     Second, we suggest how this research leads to new 
                     understandings of the semantics and significance of the captions of historical book
                     illustrations. The findings discussed here mark a critical 
                     intervention in the fields of digital humanities, book history, and illustration studies.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">The captions that appear alongside pictures in historical books are situated on a
                     threshold, hanging in their own space, somewhere between the 
                     image and the text proper. Like the captions that this paper discusses, the research
                     that we describe here also occupies a threshold: between 
                     digital humanities, with its focus on computational tools and remediation to generate
                     research questions, and book history, with its attention 
                     to the material narratives of the book as object. It is this liminal space that has
                     come to define the field of illustration studies. In his 
                     discussion of “thresholds”, Gérard Genette famously shied away from discussing illustration because it 
                     was an “immense continent” that was too large for him to traverse [<a class="ref" href="#genette_1997">Genette 1997</a>, 406]. Digital 
                     humanities has risen to this challenge both by making illustrated material more accessible
                     than ever before and by engaging with how 
                     accessibility and scale can reveal new ways of analysing historical illustrations
                     [<a class="ref" href="#thomas_2017">Thomas 2017</a>].
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">From within this critical space, this article sets out to search the “immense continent” of illustration for what is perhaps its most 
                     marginalised territory: the caption. Very little work has been undertaken on the significance
                     of captions and how they make their meanings, 
                     despite their prevalence in illustrated books. These issues, however, are of some
                     importance for understanding how captions relate to the 
                     content of the pictures that they accompany, as well as the wider dialogue between
                     word and image that characterises illustration as a mode of 
                     representation. What we outline below is the first attempt to capture the significance
                     of captions by describing the methods and findings of 
                     research that identifies and interrogates the captions of historical book illustrations
                     at scale, research that is only possible in a digital 
                     environment where images from different books can be viewed alongside each other.
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The dataset that is central to this study consists of over a million illustrations
                     from 68,000 volumes in the British Library's 
                     collection, which were digitised by Microsoft. These illustrations form the basis
                     of the world's largest online resource dedicated to book 
                     illustration, The Illustration Archive, which we created on a previous AHRC-funded project 
                     (<a href="https://illustrationarchive.cf.ac.uk/" onclick="window.open('https://illustrationarchive.cf.ac.uk/'); return false" class="ref">https://illustrationarchive.cf.ac.uk/</a>). The illustrations in this dataset span the 
                     sixteenth to the twentieth century, with the majority clustering around the late eighteenth
                     and nineteenth centuries, a period that witnessed a 
                     global explosion of illustrated material and was the immediate precursor of our own
                     visual culture. The books are written in different 
                     languages, with the vast majority in English, and they cover genres categorised as
                     literature, history, philosophy, and geography, a 
                     diverse range that provides insight into the practices and importance of illustration
                     at a time when this mode of representation constituted 
                     the “mass image”, the dominant visual form of the day.
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">The fact that illustrations were so prevalent at this historical moment makes it imperative
                     that we understand their constituents, not least 
                     because digitisation has made these illustrations available to view in their hundreds
                     of thousands. Our study suggests that it is the very 
                     ambiguity and indeterminacy of captions that opens them up to computational analysis.
                     This proposition might seem counter-intuitive: generally, 
                     we think that it is objects that are easier to “describe” and identify computationally that are best suited to analysis. Certainly, some 
                     interesting work has been done using historical illustrations in this way. In 2014
                     an ambitious project was undertaken by 
                     Kalev Leetaru to identify the illustrations in books and make them searchable using the captions
                     alongside the paragraphs 
                     immediately preceding and following the illustrations [<a class="ref" href="#leetaru_n.d.">Leetaru n.d.</a>]. More recently, Hoyeol Kim has adapted 
                     Victorian illustrations as training sets for deep learning models by creating a dataset
                     containing colourised black-and-white illustrations from 
                     the nineteenth century [<a class="ref" href="#kim_2021">Kim 2021</a>].
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">What we propose in this article is that indeterminacy — what is not clearly known,
                     defined, or fixed — also has an analytical value. Captions 
                     can be identified computationally precisely because of their indeterminate space between
                     the text and image; the words of the caption are 
                     neither part of the image, nor do they fully belong to the body of the text. It is
                     this liminal space that enables the caption to be 
                     identified. This indeterminacy can also be understood and exploited on a semantic
                     level (as we describe in the “Analysing Captions” section 
                     below). By using the captions of illustrations as a search tool, we can begin to find
                     those instances when the captions seem to capture the 
                     visual content of the image, as well as those instances when they do not. The drive
                     for searchability and the retrieval of relevant and accurate 
                     “hits” generates an analytical space that reveals new insights into the complex interrelationship
                     between the caption, image, and text. Our 
                     study, therefore, sets out to identify captions both as a mechanism for searching
                     illustrations in a big dataset and as a way of exploring how 
                     captions signify, an exploration that emerges from the very limits of searchability.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">What is a Caption?</h1>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">The function of the <em class="term">caption</em> — a piece of text that lies alongside a visual image — can be traced back to the
                     historical use of 
                     the <em class="term">inscription</em>, <em class="term">motto</em>, and <em class="term">legend</em>. The word <em class="term">subscription</em> was used from the sixteenth 
                     century to describe the words placed below a picture or portrait. <em class="term">Caption</em> itself is a more recent term. Deriving from the Latin for 
                     <em class="word">taking</em>, <em class="term">caption</em> was originally used in the late eighteenth century in a textual context: to describe
                     the heading of a 
                     chapter, section, or newspaper article. Its specific meaning as the “title below an illustration” emanates from America in the early 
                     decades of the twentieth century.
                     </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">The Early Modern Graphic Literacies project (University of Turku, Finland), which
                     is working to create an historical taxonomy of visual devices 
                     in early English print from the late 1400s to 1800, draws attention to the use of
                     the caption in these forms, noting that the distinction 
                     between captions and other image-related texts (for example, titles and running titles)
                     is not always obvious 
                     [<a class="ref" href="#ruokkeinen_et_al_2023">Ruokkeinen, S. et al. 2023</a>, 9]. Historically, our dataset picks up at the point when the Early Modern Graphic Literacies
                     project 
                     ends, but even in this later period, the caption is beset with ambiguity and a startling
                     variety of practices. The difficulty of converting some 
                     captions into a readable form is a consequence of the fact that the caption is generally
                     printed using the same reproductive method as the 
                     illustration, and numerous printing methods are represented in our dataset: intaglio
                     illustrations (e.g., etchings, steel engravings) include an 
                     etched or engraved caption as part of the image, lithographed images have lithographed
                     captions, and wood-engraved and photomechanically 
                     produced images usually come with letterpress captions that connect the captions visually
                     to the rest of the text. Captions were sometimes 
                     reproduced in the front matter of the book in the form of a “list of illustrations”, with the caption explicitly adopting the role of image 
                     title. The choice of words in the caption could be formulated by the author, the artist
                     (or engraver), or by the publisher.
                     </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">The very presence of captions varies considerably across historical book illustrations.
                     There are four main types of illustration represented in 
                     our dataset: embellishments that are primarily positioned as the headers or footers
                     of chapters and books (also known as <em class="term">ornaments</em> 
                     or <em class="term">decorations</em>); pictorial capital letters; illustrations that are inset anywhere on a page of text;
                     and full-page plates, where 
                     the image occupies its own page. Of these types of illustration, the first two (embellishments
                     and pictorial letters) do not include captions. 
                     The second two (inset and full-page illustrations) do, although this is not uniform
                     across all inset and full-page illustrations. In our 
                     calculations, and with “embellishments” excluded, we have identified 513,914 captioned illustrations from a total of 665,684
                     illustrations 
                     (where overlapping illustrations are counted as a single image). The majority of captions
                     in our dataset are placed below rather than above the 
                     image and occupy their own space at a slight distance from the image (see Figures
                     3, 5, 7, 8, and 9 below). In the case of those illustrations 
                     that are set alongside the text on the page, the caption is also positioned at a slight
                     distance from the text proper (see Figures 1, 2, and 4). 
                     These physical features allow us to identify the caption computationally in the ways
                     we outline in the following section.
                     </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">The positioning of the caption also has semantic implications, suggesting its status
                     on, and between, both sides of the threshold between word 
                     and image. Little critical attention has been devoted to the complexities of the caption
                     and how it signifies in relation to the image and to 
                     the rest of the text. Roland Barthes is one of the few critics to have engaged with this interaction, suggesting that
                     captions can 
                     work to “anchor” the meanings of an image by indicating how the image should be read and directing
                     the viewer's interpretation. Barthes's 
                     discussion is based on captions in twentieth-century advertising [<a class="ref" href="#barthes_1977">Barthes 1977</a>, 32–51], but his ideas are borne out in 
                     recent analyses of the captions of historical book illustrations emerging from illustration
                     studies. Sioned Davies gives an account 
                     of the possible reasons why captions were added to the illustrations in the second
                     edition of Charlotte Guest's translation of 
                     <cite class="title italic">Geraint the Son of Erbin</cite> (1849), arguing that they add a specific geographical and historical dimension to
                     the 
                     settings of the images that are otherwise indistinct [<a class="ref" href="#davies_2019">Davies 2019</a>]. Valentina Abbatelli has also drawn attention to 
                     the importance of captions in remediating the racial politics of the early twentieth-century
                     Italian editions of Harriet Beecher 
                     Stowe's novel <cite class="title italic">Uncle Tom's Cabin</cite> [<a class="ref" href="#abbatelli_2018">Abbatelli 2018</a>]. According to Abbatelli, the 
                     same image of Uncle Tom learning to write is recast by the captions as a scene where Uncle Tom is “failing” to write and one 
                     where he is making an effort.
                     </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">In Davies's and Abbatelli's strikingly different examples, the illustrations remain the same across editions;
                     it is 
                     the captions that are added or changed, and this modification, in turn, changes the
                     meanings of the image. A computational identification of 
                     captions gives an opportunity to scale up these analyses to see patterns that are
                     difficult, if not impossible, to discern in the material form 
                     of the book, where comparisons between illustrations and captions are necessarily
                     limited. The next section describes the AI methods we used to 
                     identify captions in this large dataset of historical illustrations. These methods
                     can effectively be re-purposed across other datasets to add 
                     value to the searchability of illustrations. The section following suggests how this
                     identification leads to new understandings of the 
                     significance and semantics of the captions of historical book illustrations, which
                     are of relevance not only to scholars working in digital 
                     humanities but also to those whose interests lie in illustration studies and its attendant
                     fields.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Using AI to Identify Captions</h1>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">The first stage in our identification process was to isolate the captions from the
                     main body of the text and the image, a task that has proven 
                     challenging for both machine learning and humanists attempting to formalise what a
                     <em class="term">caption</em> is. Zongyi Liu and 
                     Hanning Zhou provide a heuristics-based approach to caption finding, suggesting a series of rules
                     to identify a caption based on 
                     size of the font, position relative to the image, and size of the caption itself [<a class="ref" href="#liu_zhou_2011">Liu and Zhou 2011</a>]. Employing this method led to a 
                     high number of false positives and false negatives; our dataset presents wide variation
                     in style of layouts, making it impossible to list a 
                     reliable set of heuristics. In order to isolate the caption, we needed to identify
                     the other elements present on the page; that is, we needed to 
                     “parse” the layout of the page. For simplicity, we identified four building blocks:
                     
                     <ol class="list">
                        <li class="item">text, referring to the main body of text, such as paragraphs, introductions, and any
                           text which does not fall into our two other textual 
                           categories;</li>
                        <li class="item">captions;</li>
                        <li class="item">headings, referring to titles, chapter titles, and any form of text appearing at the
                           top of the page that might be shared across 
                           subsequent pages; and</li>
                        <li class="item">figures.</li>
                     </ol>
                     </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">We then labelled a random subsample of 1,000 images from our dataset using labelme
                     
                     (<a href="https://github.com/wkentaro/labelme" onclick="window.open('https://github.com/wkentaro/labelme'); return false" class="ref">https://github.com/wkentaro/labelme</a>), a graphical annotation tool that allows users to 
                     draw bounding boxes on an image and assign them a label. The annotations of each page
                     are then saved as a file that contains the coordinates of 
                     each bounding box and its respective label. Figure 1 shows a snippet of the labelling
                     tool with one of our images. In the left panel, boxes can 
                     be drawn around the different sections of the page, highlighting its layout. On the
                     right, each bounding box is labelled with one of the 
                     categories we have identified above. These annotations allow us to re-train a computer
                     vision model to automatically identify the layout of a 
                     page and isolate a caption.
                     </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 1. </div>A snippet of the annotation process using the labelme tool.</div>
                  </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">With this annotated dataset, we turn to fine-tuning. In deep learning, fine-tuning
                     is a transfer learning method where a pre-trained model is trained on new data to
                     fine tune it on a more specific task. In our case, that task is layout parsing on
                     historical illustrated books. Layout parsing is the process of automatically detecting
                     the layout structure of a page using computer vision. A highly effective recent model
                     is Layout Parser (https://layout-parser.github.io/), which provides a range of models
                     pretrained on diverse databases. We used the model trained on the PrImA dataset as
                     it most closely resembled our data. It is a Mask R-CNN model trained on newspapers
                     that identifies text, figures, titles, tables, maths, and separators; newspapers present
                     a similarly busy and interlocked layout as historical illustrated books. We ran training
                     for 10,000 iterations with a learning rate of 0.001, achieving accuracy of 98% on
                     the validation set. The majority of errors occur on pictorial capital letters: at
                     the annotation stage, we decided for simplicity to include pictorial letters within
                     the main body’s bounding box labelled as text, but the model often labels them separately
                     as ‘figure’.       </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">We deployed our fine-tuned model on <span class="hi italic">The Illustration Archive</span> to predict the layout of each page and isolate the captions. The model’s output consists
                     of coordinates for the layout element of each page which can be visualised as bounding
                     boxes. Figures 2–4 demonstrate an example output of the model: on each page, the model
                     has predicted the position and category of a layout element, and it is displayed with
                     a confidence score as a percentage. We can see in Figure 2 that three bounding boxes
                     with the label ‘figure’ have been predicted, as the model is unsure whether the two
                     illustrations should be considered separately. An analogous situation occurs in Figure
                     4 with the captions.  </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure02.jpg" rel="external"><img src="resources/images/figure02.jpg" style="" alt="" /></a></div>
                     </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure03.jpg" rel="external"><img src="resources/images/figure03.jpg" style="" alt="" /></a></div>
                     </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure04.jpg" rel="external"><img src="resources/images/figure04.jpg" style="" alt="" /></a></div>
                     </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">After obtaining labelled layout structures, it becomes easier to perform optical character
                     recognition (OCR) exclusively on the captions. We used pytesseract (https://pypi.org/project/pytesseract/),
                     a Python library which is a wrapper for Google’s Tesseract OCR engine. The main disadvantage
                     of this tool is its speed when attempting to use multiple languages; for this reason,
                     at this stage, we allowed it only to rely on English, meaning that captions in foreign
                     languages were not processed correctly; we are planning to fix this issue in a future
                     iteration.</div>  
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">Through these computer vision techniques, we have obtained a dataset of captions that
                     can be used as a search index for the illustrations in  <span class="hi italic">The Illustration Archive</span>. Integrated as metadata in <span class="hi italic">The Illustration Archive</span> website, the captions allow users to explore the images, search by captions, and
                     to analyse the relation between the words in the captions and the content of the illustrations.
                     The identification of the captions has also opened up possibilities to use image-to-text
                     and text-to-image models such as CLIP (Contrastive Language-Image Pre-training:  <a href="https://openai.com/research/clip" onclick="window.open('https://openai.com/research/clip'); return false" class="ref">https://openai.com/research/clip</a>) to cluster images with similar captions, identify objects in illustrations using
                     CLIP text encodings as well as our fine-tuned caption encodings, and to discover connections
                     between language representation and visual representation. CLIP is a deep learning
                     model that draws simultaneously from text and image. It is trained on vast amounts
                     of data consisting of pairs of image and descriptive text, allowing it to train a
                     text encoder and an image encoder to produce text and visual embeddings that are close
                     to each other. In this way, a textual prompt can be used to search for images with
                     embeddings resembling the textual prompt. We have used OpenCLIP ViT-L/14 model (Vision
                     Transformer) to build a searchable index of the illustrations in   <span class="hi italic">The Illustration Archive</span>, following the work of the Visual Geometry Group at the University of Oxford (see
                     their WISE Image Search Engine, which uses AI tools to make image databases searchable
                     by content: <a href="https://www.robots.ox.ac.uk/~vgg/software/wise/" onclick="window.open('https://www.robots.ox.ac.uk/~vgg/software/wise/'); return false" class="ref">https://www.robots.ox.ac.uk/~vgg/software/wise/</a>).</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Analysing Captions</h1>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">Developments in deep learning models mean that we are no longer wholly reliant on
                     textual metadata (tags, bibliographic information, or, indeed, captions) to search
                     the content of images in datasets of historical images. Back in 2004 when we started
                     developing the <span class="hi italic">Database of Mid-Victorian Illustration </span>(<a href="https://www.dmvi.org.uk/" onclick="window.open('https://www.dmvi.org.uk/'); return false" class="ref">https://www.dmvi.org.uk/</a>), the only way of making the 900 or so images searchable to users was manually to
                     tag their content. Captions, however, can still add value to content-driven visual/iconographic
                     searches. Online archives of historical newspapers often use captions for searchability,
                     although the captions of these illustrations do not necessarily function in the same
                     way as the captions in books. In newspapers, the caption is generally a ‘heading’
                     that appears at the top of the feature and is the title both for the illustration
                     (if there is one) and the textual article. In   <span class="hi italic">Welsh Newspapers</span> (https://newspapers.library.wales/), for instance, the title in the form of a heading
                     has been identified as a distinct characteristic of the page. Any alternative captions
                     underneath an image have been OCRed along with the full text and can be searched accordingly.
                     A recent use of OCRed headlines/captions and other text alongside deep learning models
                     is the   <span class="hi italic">Newspaper Navigator</span>, which searches the visual content of historical newspapers in the Library of Congress
                     (Lee, 2020).</div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">We conducted a case study to evaluate the effectiveness of using our identified captions
                     to search for specific illustrations in the dataset. In partnership with Lambeth Palace
                     Library, we searched for book illustrations of cathedrals that would enrich the library’s
                     collections. We found 2,087 illustrations captioned with the word ‘cathedral’ and
                     103 ‘cathedrals’. Data-mining the captions in this way involves the same discrimination
                     as might be adopted on a Google search, in the sense that we needed to anticipate
                     the words that might be used in these captions (e.g., additional terms not covered
                     by the word ‘cathedral’, such as ‘York Minster’, ‘Westminster Abbey’, and ‘chapterhouse’,
                     which might signal a cathedral building).   </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">In this study, searching the captions proved a highly effective way of identifying
                     relevant illustrations from the dataset because the words of the captions describe
                     in a broad sense what the illustrations depict: ‘cathedrals’. In some instances, a
                     caption search can be   <span class="hi italic">more</span> effective than using only a visual search. An image classification (as opposed to
                     a caption) search would retrieve images of cathedrals, but it would also return churches
                     since there are no specific architectural signifiers that mark out a cathedral from
                     a church. Cathedrals are generally larger and more elaborate than churches, but their
                     specific distinction — which would not usually be signalled in an illustration or
                     in visual searches — is that cathedrals are the seat of a bishop. </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">Likewise, a caption search is able to retrieve illustrations that show features of
                     cathedrals that are not iconographically specific to cathedrals and would not, therefore,
                     be found in image-based searches that tend to cluster key features of cathedrals (external
                     ‘grand’ perspectives, spires, and high-domed interiors). A search via the captions
                     revealed illustrations of floor plans of cathedrals, different perspectives where
                     the cathedral is in the far distance, so not obviously identified as a ‘cathedral’,
                     as well as many images of interior and interior features (e.g., effigies, gargoyles,
                     windows, staircases; see Figure 5). The captions here retrieve images of ‘cathedrals’
                     that would not otherwise be found.      </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">This example of the effectiveness of captions in searching large datasets raises an
                     important point about the nature of captions, especially in illustrated non-fiction
                     books. As contemporary labels attached to the images, captions can function as ‘expert
                     tags’: authoritative and reliable indicators of what the illustration contains. They
                     can, for instance, identify an otherwise unrecognisable interior setting as depicting
                     ‘Bristol Cathedral’ (Figure 5).    </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 5. </div>Example of an image retrieved by searching the captions for ‘cathedral’. From [Samuel
                        Griffiths Tovey], Cursory Observations on the Churches of Bristol. By an Occasional
                        Visitor, second edition (Bristol: Mirror Office, 1843), facing p. 6.</div>
                  </div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">This aspect of the caption as a reliable descriptor of the image can add to the searchability
                     of large illustration datasets. However, captions are far from neutral or ahistorical
                     descriptors of the image, and it is this problematic relationship that comes to the
                     fore in our research. By identifying hundreds of thousands of captions, we have effectively
                     isolated a corpus of textual metadata that is culturally significant.  Even a term
                     as apparently straightforward as ‘cathedral’ has an historical resonance: its use
                     in a caption speaks to the establishment and embeddedness of (in this case) the Church
                     of England; the rise of tourism in cathedral towns and cities in the UK; and the changing
                     demographics brought about by industrialisation (no new cathedrals were created for
                     nearly 300 years until the Victorian period when the increased population in major
                     industrial cities necessitated new cathedrals). There is a civic pride in the captioning
                     of ‘cathedrals’, especially as illustrations of cathedrals most commonly appear in
                     the context of tourist guidebooks and local histories, like the one in Figure 5. ‘Cathedral’
                     is just one example of the cultural significance of the words used in captions. Others
                     are more overtly political and unsettling, such as the racist terms that are sometimes
                     used.                </div>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">We can interrogate this corpus of captions in multiple ways, including searching for
                     the frequency distribution of words. Figure 6 shows a random subset of words sampled
                     from different frequency bands.</div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a></div>
                     </div>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">The arbitrary nature of these words reflects the mixed genres that make up our illustration
                     dataset, but, seen in the form of a word cloud, the words in the captions clearly
                     foreground dominant nineteenth-century values. The most frequent keywords are ‘house’
                     (in 3,043 captions) and church’ (in 2,688 captions); there is also an emphasis on
                     the ‘old’ (2,561 captions), signalled also in terms like ‘castle’ (1,778 captions),
                     ‘ancient’ (592 captions) and ‘abbey’ (556 captions); and the ‘new’ (in 1,760 captions),
                     signalled also in ‘photograph’ (2,009 captions). Aspects of the countryside and landscape
                     are emphasised, alongside towns and cities (‘London’ is one of the top terms, used
                     in 2,219 captions). Gender categories are particularly revealing, with 519 captions
                     containing the word ‘man’ and 307 captions ‘woman’ (the plural suggests a similar
                     margin: 274 ‘men’ and 187 ‘women’); ‘Mr’ appears 1,198 times, ‘Mrs’ 497 times, and
                     ‘miss’ 250 times.        </div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">Although it is highly likely that the illustrations in the dataset contain more illustrations
                     of men than women, this is not necessarily the full picture. A comparison search for
                     image features can prove illuminating here. An image feature search on a subset of
                     15,000 illustrations using CLIP found 524 men and 14 women, while, surprisingly, there
                     were no groups of men, 2,487 groups of women and 1,562 groups recognised as people.
                     More work needs to be done comparing different search mechanisms. Our analysis indicates
                     that whilst there is an undoubted bias in the illustrations, there is also a bias
                     in the captions, which might not necessarily be the same as the bias in the images.
                     Identifying and isolating the captions in this way exposes how the caption emphasises
                     and marginalises features of the illustrations.      </div>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">The caption’s privileging and concomitant marginalising of pictorial details is, in
                     fact, one of its major characteristics. The caption of Figure 5, ‘Bristol Cathedral.
                     VIEW FROM THE ELDER LADY’S CHAPEL’, contains no mention of stairs, flagstones, archways,
                     monuments, or any other architectural features. However descriptive and objective
                     it appears, the caption is never fully reflective of the image because it is couched
                     in another form: words; and words cannot fully describe the visual features of an
                     image. What captions emphasise and marginalise, then, is highly significant: they
                     provide a unique insight into how the illustrations were viewed at the time, what
                     were regarded as their most salient features, and what features were overlooked. 
                     </div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">Using AI to identify the captions at scale allows us to look not only at the words
                     of the caption, but also to trace patterns in how the captions signify in relation
                     to the illustrations and the rest of the text. Our findings point to the fact that
                     the caption makes its meanings in two main ways: it signifies in its conjunction with
                     the image; and it acts as a point of connection, or bridge, between the illustration
                     and the rest of the text. Either, or both, of these relationships can be at play in
                     any captioned illustration, but they seem primarily to be determined by the genre
                     of the book.      </div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">In texts conventionally classified as non-fiction, including works of history, geography,
                     and science (in the broadest sense), the role of the caption and its relation to the
                     image is characterised by the first of these models, with the illustration and caption
                     forming a complete signifying unit. In theory, this unit could be isolated from the
                     rest of the text and still make sense. Although the caption ‘Bristol Cathedral’ fails
                     to mention certain aspects of the image, it nevertheless signifies in relation to
                     the illustration, with the picture and the words of the caption constituting a signifying
                     partnership.    </div>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">This is the defining model across the range of non-fiction books in the dataset. In
                     simple terms, the captions in these books describe the facts of what is represented
                     in the illustrations; following the Latin etymology of ‘caption’ they attempt to ‘capture’
                     or ‘seize’ the meanings of the picture. In our dataset, there are tens of thousands
                     of botanical or zoological illustrations captioned with the name of the species, illustrative
                     portraits of people captioned with their names, and illustrations of locations or
                     buildings with captions that identify what locations and buildings they are. In these
                     examples, the signifying conjunction of the illustration and caption creates and enforces
                     a notion of equivalence: the idea that the caption replicates in words what the illustration
                     shows and vice versa (however illusory this notion is).     </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">In illustrated fiction and literary texts (short stories, novellas, novels, plays,
                     poetry, etc.), captions are usually citational, taking the form of quotes, or deriving
                     from the words of the text. These captions frequently state the characters’ names
                     and/or what they are doing: ‘Mr. Perry passed by on horseback’ is a typical caption
                     from an illustrated edition of Jane Austen’s     <span class="hi italic">Emma</span> (1896). (The designations ‘Mr., Mrs., and Miss are more prevalent in the captions
                     of fictional texts where they are used to name a character than they are in non-fiction
                     captions.) In literary texts, the context and meanings of the illustrations and the
                     captions depend on the text; they do not necessarily make sense in isolation, like
                     the non-fiction captioned illustrations described above. The role of these literary
                     captions is to point to the specific episode being illustrated, a role that serves
                     a practical function because book illustrations often appeared on different pages
                     to the text that they depicted (thus many captions also include the relevant page
                     numbers; see Figure 3). Instead of turning in towards the illustration, then, these
                     captions point outside, acting as a bridge between the illustration and the text proper.
                     </div>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">An example of this bridging technique is the caption ‘Never is a very long word’,
                     which accompanies an illustration for Anthony Trollope’s novel <span class="hi italic">Orley Farm</span> (Figure 7).  </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure07.jpeg" rel="external"><img src="resources/images/figure07.jpeg" style="" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 7. </div>John Everett Millais, ‘Never is a very long word’, engr. Dalziel Brothers. Illustration
                        for Anthony Trollope, Orley Farm (London: Chapman and Hall, 1862), facing p. 77.</div>
                  </div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">‘Never is a very long word’ is a direct quotation from the novel, and this caption
                     is placed securely within quotation marks when it appears as a title on the contents
                     page in the book edition. By citing the text of the novel, the caption works to direct
                     the reader to the episode that is being illustrated, isolating the moment depicted
                     in the illustration from several possibilities (the pose of the two seated ladies
                     could fit any number of episodes). In this way, the caption works to connect the image
                     to the rest of the text. As if to prove that an image cannot translate into words,
                     the illustration does not depict the words ‘Never is a very long word’. How could
                     these words even be illustrated in a picture? This caption, therefore, is a good example
                     of where the use of captions to search for picture content would fail (the content
                     of this image is manually tagged in the           <span class="hi italic">Database of Mid-Victorian Illustration</span>). In terms of searchability, the use of captions is far more effective when searching
                     across the non-fiction books in the dataset.  </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">Our analysis, therefore, indicates that the caption does not signify in the same way
                     across all books. Rather, there are specific conventions at play that are determined
                     primarily by the broad generic classification of the book. An acknowledgement of these
                     conventions allows us to recognise those instances where they are being adapted and
                     manipulated, with implications for the meanings of the text and how it is read. An
                     example of this is Figure 8, the frontispiece illustration for Trollope’s        <span class="hi italic">Orley Farm</span>. Unlike the caption in Figure 7, which follows the conventions of literary illustrations
                     and quotes directly from the novel, Figure 8 draws instead on the conventions of the
                     ‘factual’ caption.  </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure08.jpeg" rel="external"><img src="resources/images/figure08.jpeg" style="" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 8. </div>John Everett Millais, ‘ORLEY FARM’, engr. Dalziel Brothers. Illustration for Anthony
                        Trollope, Orley Farm (London: Chapman and Hall, 1862), frontispiece.</div>
                  </div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">The illustration in Figure 8 is based on a real location, the farmhouse where Trollope
                     lived as a boy. The style of the image itself crosses generic categories: pictures
                     of rural landscapes frequently appeared in both fictional and non-fictional books,
                     including gift books, poetry anthologies and topographical works. However, it is the
                     caption that makes the link to a real geographic location. Capitalised to lend it
                     more authority, ‘ORLEY FARM’ mimics the style of captions in illustrated travel books
                     that label the place illustrated (see, for example, Figure 9).       </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" style="" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 9. </div>‘BACKBARROW MILLS’, illustration for Rambles in the Lake Country and Other Travel
                        Sketches ed. George Milner (Manchester and London: John Heywood, 1893), p. 68.</div>
                  </div>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">The association of the caption ‘ORLEY FARM’ with the designatory captions of non-fiction
                     books gives a sense of veracity to the novel at its outset (it is placed as the frontispiece
                     illustration), and this has implications for the investment and immersion of readers
                     in the imaginative world of the novel. The caption here functions alongside the realist
                     conventions of the text and adds to the verisimilitude of the fictional world. Significantly,
                     this caption is in marked contrast to the other captions in this novel, which either
                     quote directly from the text (as in Figure 7) and/or identify the characters. The
                     only caption that comes close is ‘Monkton Grange’, which appears underneath a picture
                     of an old manor house where a group of people gather for a hunt. In this example,
                     the image and the caption again function to situate the illustrated episode in a location
                     that would have looked realistic to contemporary readers.     </div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">Whilst Trollope’s novel is, of course, a work of fiction, there are many eighteenth
                     and nineteenth-century illustrated books that cannot easily be classified as either
                     fiction or non-fiction (e.g., memoirs, certain modes of travel writing etc.). In these
                     books the captions often switch between different modes of address, dictating how
                     the images are read. Charles Knight’s multi-volume  <span class="hi italic">Pictorial Edition of the Works of Shakspere</span> (1838-1843) is an interesting example of a book in which the literary text competes
                     with historical images. Without their captions, many of these pictures could be viewed
                     as fictional, but, with them, they are fixed in a time and place, such as ‘Room in
                     Cleopatra’s Palace’ or ‘Part of Windsor Castle, built in the time of Elizabeth’. Likewise,
                     some of the illustrations that might otherwise look historically, geographically,
                     or botanically accurate are linked to the imaginative world of Shakespeare’s texts
                     with captions that are direct quotations from the plays. Knight’s edition veers dramatically
                     between the factual and fictional, and this effect is produced as much by the captions
                     as by the images. </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Conclusion</h1>
                  
                  <div class="counter"><a href="#p37">37</a></div>
                  <div class="ptext" id="p37">Research into the significance and meanings of the captions of historical illustrations
                     has been hampered by the materiality of the book and the difficulty of viewing multiple
                     illustrations and their captions side by side and comparatively. AI tools rectify
                     this by allowing captions to be identified and interrogated alongside each other and
                     across tens of thousands of illustrated books. We suggest that it is the indeterminacy
                     of the caption, its position on the threshold between the image and the text, which
                     opens it up to computational identification and analysis.  </div>
                  
                  <div class="counter"><a href="#p38">38</a></div>
                  <div class="ptext" id="p38">Using AI to identify the caption offers a mechanism for searching the content of illustrations
                     in large datasets and this can be more effective than using image-only classification
                     searches. However, AI moves beyond its efficacy as a vehicle for searching the content
                     of pictures to reveal the significance of captions as words that describe —  <span class="hi italic">and do not describe</span> — illustrations. Isolated as a dataset, the captions are historically significant
                     in their own right, and can be interrogated in terms of their linguistic meanings
                     and variations. AI also enables an analysis of the caption in its complex dialogue
                     with the illustration and the rest of the text, allowing a recognition of signifying
                     patterns and conventions across books and genres. For the first time, we can begin
                     to trace at scale the ways in which the caption generates meanings and impacts on
                     the reading and viewing process from its liminal position on the threshold.      
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgements</h1>
                  
                  <div class="counter"><a href="#p39">39</a></div>
                  <div class="ptext" id="p39">We would like to thank Ian Harvey and Unai Lopez-Novoa for their foundational work
                     on <span class="hi italic">The Illustration Archive</span> and captions. <span class="hi italic">The Illustration Archive</span> was funded by the AHRC. The current project is funded by the AHRC/NEH as a partnership
                     between Cardiff University and the University of Wyoming.</div>
                  </div>
               
               
               
               
               </div>
            
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="abbatelli_2018"><!-- close -->Abbatelli 2018</span> Abbatelli, V. (2018) “Looking at captions to get the full picture: Framing 
                  illustrations in Italian editions of <cite class="title italic">Uncle Tom's cabin</cite>”, <cite class="title italic">Image and Narrative</cite>, 
                  19(1), pp. 46-61.
                  </div>
               <div class="bibl"><span class="ref" id="barthes_1977"><!-- close -->Barthes 1977</span> Barthes, R. (1977) <cite class="title italic">Rhetoric of the image: Image, music, text</cite>, ed. and 
                  trans. Stephen Heath. London: Fontana.
                  </div>
               <div class="bibl"><span class="ref" id="bradski_2000"><!-- close -->Bradski 2000</span> Bradski, G. (2000) “The OpenCV library”, 
                  <cite class="title italic">Dr. Dobb's</cite>, 1 November. Available at: 
                  <a href="https://www.drdobbs.com/open-source/the-opencv-library/184404319" onclick="window.open('https://www.drdobbs.com/open-source/the-opencv-library/184404319'); return false" class="ref">https://www.drdobbs.com/open-source/the-opencv-library/184404319</a> 
                  (Accessed: 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="dmvi_n.d."><!-- close -->DMVI n.d.</span> <cite class="title italic">Database of mid-Victorian illustration</cite> (n.d.). Available at: 
                  <a href="https://www.dmvi.org.uk/" onclick="window.open('https://www.dmvi.org.uk/'); return false" class="ref">https://www.dmvi.org.uk/</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="davies_2019"><!-- close -->Davies 2019</span> Davies, S. (2019) “‘A most venerable ruin’: Word, image and 
                  ideology in Guest's <cite class="title italic">Geraint the son of Erbin</cite>”, <cite class="title italic">Studia Celtica</cite>, 53(1), pp. 53-72.
                  </div>
               <div class="bibl"><span class="ref" id="genette_1997"><!-- close -->Genette 1997</span> Genette, G. (1997) <cite class="title italic">Paratexts: Thresholds of interpretation</cite>, trans. 
                  Jane E. Lewin. Cambridge: Cambridge University Press.</div>
               <div class="bibl"><span class="ref" id="hoffstaetter_et_al_2022"><!-- close -->Hoffstaetter et al. 2022</span> Hoffstaetter, S. et al. (2022) <cite class="title italic">pytesseract 03.10</cite>. 
                  Available at: <a href="https://pypi.org/project/pytesseract/" onclick="window.open('https://pypi.org/project/pytesseract/'); return false" class="ref">https://pypi.org/project/pytesseract/</a> 
                  (Accessed: 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="kim_2021"><!-- close -->Kim 2021</span> Kim, H. (2021) “Victorian400: Colorizing Victorian illustrations”, 
                  <cite class="title italic">International Journal of Humanities and Arts Computing</cite>, 15(1-2), pp. 186-202.
                  </div>
               <div class="bibl"><span class="ref" id="lee_n.d."><!-- close -->Lee n.d.</span> Lee, B.C.G. (n.d.) <cite class="title italic">Newspaper navigator</cite>. Available at: 
                  <a href="https://news-navigator.labs.loc.gov/search" onclick="window.open('https://news-navigator.labs.loc.gov/search'); return false" class="ref">https://news-navigator.labs.loc.gov/search</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="leetaru_n.d."><!-- close -->Leetaru n.d.</span> Leetaru, K. (n.d.) <cite class="title italic">500 years of book images</cite>. Available at: 
                  <a href="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/" onclick="window.open('https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/'); return false" class="ref">https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="liu_zhou_2011"><!-- close -->Liu and Zhou 2011</span> Liu, Z. and Zhou, H. (2011) “A simple and effective figure caption detection 
                  system for old-style documents”, <cite class="title italic">Proceedings of SPIE: The international society for optical engineering</cite>. 
                  San Jose, CA, 24-29 January 2011. Bellingham, WA: SPIE-IS&amp;T Electrionic Imaging, article
                  #7874-28. Available at: 
                  <a href="https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents" onclick="window.open('https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents'); return false" class="ref">https://www.researchgate.net/publication/221253773_A_Simple_and_Effective_Figure_Caption_Detection_System_For_Old-style_Documents</a> 
                  (Accessed: 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="radford_et_al_2021"><!-- close -->Radford et al. 2021</span> Radford, A. et al. (2021) “Learning transferable visual models 
                  from natural language supervision”, <cite class="title italic">arXiv</cite>. 
                  <a href="https://doi.org/10.48550/arXiv.2103.00020" onclick="window.open('https://doi.org/10.48550/arXiv.2103.00020'); return false" class="ref">https://doi.org/10.48550/arXiv.2103.00020</a> (Accessed: 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="radford_et_al_n.d."><!-- close -->Radford et al. n.d.</span> Radford, A. et al. (n.d.) <cite class="title italic">CLIP</cite>. Available at: 
                  <a href="https://github.com/openai/CLIP/?tab=readme-ov-file#readme" onclick="window.open('https://github.com/openai/CLIP/?tab=readme-ov-file#readme'); return false" class="ref">https://github.com/openai/CLIP/?tab=readme-ov-file#readme</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="ruokkeinen_et_al_2023"><!-- close -->Ruokkeinen, S. et al. 2023</span> Ruokkeinen, S. et al. “Developing a classification model 
                  for graphic devices in early printed books”, <cite class="title italic">Studia Neophilologica</cite>. 
                  <a href="https://doi.org/10.1080/00393274.2023.2265985" onclick="window.open('https://doi.org/10.1080/00393274.2023.2265985'); return false" class="ref">https://doi.org/10.1080/00393274.2023.2265985</a> (Accessed: 13 December 2023).
                  </div>
               <div class="bibl"><span class="ref" id="shen_et_al_2021"><!-- close -->Shen et al. 2021</span> Shen, Z. et al. (2021) “LayoutParser: A unified toolkit for deep learning 
                  based document image analysis”, <cite class="title italic">arXiv</cite>. 
                  <a href="https://doi.org/10.48550/arXiv.2103.15348" onclick="window.open('https://doi.org/10.48550/arXiv.2103.15348'); return false" class="ref">https://doi.org/10.48550/arXiv.2103.15348</a> (Accessed: 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="sridhar_et_al_n.d."><!-- close -->Sridhar, P. et al. n.d.</span> </div>
               <div class="bibl"><span class="ref" id="the-illustration-archive_n.d."><!-- close -->The Illustration Archive n.d.</span> <cite class="title italic">The illustration archive</cite> (n.d.). 
                  Available at: 
                  <a href="https://illustrationarchive.cf.ac.uk/" onclick="window.open('https://illustrationarchive.cf.ac.uk/'); return false" class="ref">https://illustrationarchive.cf.ac.uk/</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="thomas_2017"><!-- close -->Thomas 2017</span> Thomas, J. (2017) <cite class="title italic">Nineteenth-century illustration and the digital: Studies in 
                     word and image</cite>. New York: Palgrave.
                  </div>
               <div class="bibl"><span class="ref" id="wada_n.d."><!-- close -->Wada n.d.</span> Wada, K. (n.d.) <cite class="title italic">labelme: Image polygonal annotation with Python</cite>. Available at: 
                  <a href="https://github.com/labelmeai/labelme" onclick="window.open('https://github.com/labelmeai/labelme'); return false" class="ref">https://github.com/labelmeai/labelme</a> 
                  (Accessed 11 May 2023).
                  </div>
               <div class="bibl"><span class="ref" id="welsh-newspapers"><!-- close -->Welsh Newspapers n.d.</span> <cite class="title italic">Welsh newspapers</cite> (n.d.). Available at: 
                  <a href="https://newspapers.library.wales/home" onclick="window.open('https://newspapers.library.wales/home'); return false" class="ref">https://newspapers.library.wales/home</a> (Accessed 11 May 2023).
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>