<?xml version="1.0" encoding="UTF-8"?><?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?><?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article">Six Degrees of Francis Bacon: A Statistical Method for
                    Reconstructing Large Historical Social Networks </title>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Christopher N.
                        <dhq:family>Warren</dhq:family></dhq:author_name>
                    <dhq:affiliation>Carnegie Mellon University</dhq:affiliation>
                    <email>cnwarren@cmu.edu</email>
                    <dhq:bio>
                        <p>Christopher N. Warren is project manager and co-founder (with Daniel
                            Shore) of the <title rend="italic">Six Degrees of Francis Bacon</title>
                            project and author of <title rend="italic">Literature and the Law of
                                Nations, 1580-1680</title> (Oxford University Press, 2015). His
                            articles have appeared in journals including <title rend="italic"
                                >English Literary Renaissance</title>, <title rend="italic">The
                                Seventeenth Century</title>, the <title rend="italic">European
                                Journal of International Law</title>, <title rend="italic"
                                >Humanity</title>, and the <title rend="italic">International
                                Journal for Humanities and Arts Computing</title>. Warren is
                            currently an Associate Professor of English at Carnegie Mellon
                            University.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Daniel <dhq:family>Shore</dhq:family></dhq:author_name>
                    <dhq:affiliation>Georgetown University</dhq:affiliation>
                    <email>ds663@georgetown.edu</email>
                    <dhq:bio>
                        <p>Daniel Shore is Associate Professor in the Department of English at
                            Georgetown University. Shore’s research and teaching are on the
                            literature of the Renaissance, with a special focus on the works of John
                            Milton. His publications include <title rend="italic">Milton and the Art
                                of Rhetoric</title> (2012, Cambridge University Press) as well as
                            numerous articles. He is currently writing his second book, <title
                                rend="italic">Cyberformalism</title>, to be published by Johns
                            Hopkins University Press, which explores how full-text searchable
                            digital archives like Google Books, Early English Books Online, and
                            Eighteenth Century Collections Online allow us to study the history of
                            linguistic forms.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Jessica <dhq:family>Otis</dhq:family></dhq:author_name>
                    <dhq:affiliation>Carnegie Mellon University</dhq:affiliation>
                    <email>jotis@andrew.cmu.edu</email>
                    <dhq:bio>
                        <p>Jessica Otis (<ref target="http://www.twitter.com/jotis13"
                            >@jotis13</ref>) is a CLIR-DLF Postdoctoral Fellow in Early Modern Data
                            Curation at Carnegie Mellon University. She received her MS in
                            Mathematics and PhD in History from the University of Virginia. Her
                            research focuses on the ways people in early modern Britain used numbers
                            and mathematics in their daily lives. For more information, see <ref
                                target="http://www.jessicaotis.com">www.jessicaotis.com</ref>.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Lawrence <dhq:family>Wang</dhq:family></dhq:author_name>
                    <dhq:affiliation>Carnegie Mellon University</dhq:affiliation>
                    <email>lawrencw@andrew.cmu.edu</email>
                    <dhq:bio>
                        <p>Lawrence Wang is pursuing his PhD degree in Statistics at Carnegie Mellon
                            University. His main research area is in statistical methods for
                            inference on network data. He has also done work in extracting
                            relational data from text.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Mike <dhq:family>Finegold</dhq:family></dhq:author_name>
                    <dhq:affiliation>Carnegie Mellon University</dhq:affiliation>
                    <email>mfinegold@gmail.com</email>
                    <dhq:bio>
                        <p>Mike Finegold is Vice President - Analytics at Fulcrum Analytics and a
                            Visiting Research Scientist at the Heinz College at Carnegie Mellon
                            University. He has held faculty positions with the statistics department
                            at Carnegie Mellon University and the school of information systems at
                            Singapore Management University, where his research focused on modeling
                            consumer preferences, inferring latent network structures, and designing
                            marketing experiments for social networks. Prior to academia he worked
                            for several years as a management consultant, business development
                            manager, and educator.</p>
                        <p>He received an AB in mathematics from Princeton University and a PhD in
                            statistics from the University of Chicago.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Cosma <dhq:family>Shalizi</dhq:family></dhq:author_name>
                    <dhq:affiliation>Carnegie Mellon University</dhq:affiliation>
                    <email>cshalizi@stat.cmu.edu</email>
                    <dhq:bio>
                        <p>Cosma Shalizi is an associate professor of statistics at Carnegie Mellon
                            University, and an external faculty member at the Santa Fe Institute. He
                            got his Ph.D. in theoretical physics from the University of
                            Wisconsin-Madison in 2001. Website: <ref
                                target="http://www.stat.cmu.edu/~cshalizi/"
                                >http://www.stat.cmu.edu/~cshalizi/</ref>.</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id">000244</idno>
                <idno type="volume">010</idno>
                <idno type="issue">3</idno>
                <date when="2016-07-12">12 July 2016</date>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item>historical networks</item>
                        <item>network inference</item>
                        <item>graph learning</item>
                        <item>early modern Britain</item>
                        <item>unstructured data</item>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change who="JDF" when="2016-03-03">Created file</change>
        </revisionDesc>
    </teiHeader>

    <text xml:lang="en">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>In this paper we present a statistical method for inferring historical social
                    networks from biographical documents as well as the scholarly aims for doing so.
                    Existing scholarship on historical social networks is scattered across an
                    unmanageable number of disparate books and articles. A researcher interested in
                    how persons were connected to one another in our field of study, early modern
                    Britain (c. 1500-1700), has no global, unified resource to which to turn.
                    Manually building such a network is infeasible, since it would need to represent
                    thousands of nodes and tens of millions of potential edges just to include the
                    relations among the most prominent persons of the period. Our <title
                        rend="italic">Six Degrees of Francis Bacon</title> project takes up recent
                    statistical techniques and digital tools to reconstruct and visualize the early
                    modern social network.</p>
                <p>We describe in this paper the natural language processing tools and statistical
                    graph learning techniques that we used to extract names and infer relations from
                    the <title rend="italic">Oxford Dictionary of National Biography</title>. We
                    then explain the steps taken to test inferred relations against the knowledge of
                    experts in order to improve the accuracy of the learning techniques. Our
                    argument here is twofold: first, that the results of this process, a global
                    visualization of Britain’s early modern social network, will be useful to
                    scholars and students of the period; second, that the pipeline we have developed
                    can, with local modifications, be reused by other scholars to generate networks
                    for other historical or contemporary societies from biographical documents.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>The authors present a statistical method for inferring historical social networks
                    from biographical documents.</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>Introduction</head>
                <p>Historians and critics have long studied the ways that early modern writers and
                    thinkers associated with each other and participated in various kinds of formal
                    and informal groups. Although their findings have been published in countless
                    books and articles, there is currently no way to obtain a unified view of the
                    early modern social network. A scholar must start largely from scratch if she
                    seeks to understand complex relations between multiple people, identify
                    potentially important relationships that have yet to be explored, understand the
                    extent of communities of interaction, or visualize the scholarly consensus
                    regarding networks, whether small or large. The creation of a large scale early
                    modern social network gives scholars a visual way to explore scholarly knowledge
                    of relationships and to see what has – or hasn’t – been studied in the extant
                    historiography.</p>
                <p> The most desirable outcome of our work would of course be a comprehensive map of
                    the way early modern persons were related. Yet practical challenges abound. The
                    population of Britain rose to over 5.5 million people by the end of the
                    seventeenth century, and little documentary evidence survives on much of that
                    population. Attempting to reconstruct the full network would be unrealistic.
                    Even if we limited ourselves to people alive in 1700 and successfully gathered
                    5.5 million names, the number of potential relationships in that set exceeds 15
                    billion. Social relations are exceedingly complex, even in societies
                    considerably smaller than our own. There are thus excellent reasons to proceed
                    more conservatively–focusing only on small, well-documented subsets of the
                    population. Some of the best known digital networks projects, such as Stanford
                    University’s <title rend="italic">Mapping the Republic of Letters</title> and
                    Oxford University's <title rend="italic">Cultures of Knowledge</title>, do just
                    that. Adhering to historians’ venerable practice, they proceed incrementally and
                    only include relationships directly attested by documents such as letters. This
                    approach produces relatively small, highly substantiated networks – on the order
                    of, say, 500 nodes <ptr target="#ahnert2014"/>
                    <ptr target="#basu2015"/> – but it also limits these networks to representing an
                    infinitesimal sliver of the rich and varied kinds of relationships between
                    people.</p>
                <p> Taking a different approach, we identified biographical data as the most
                    productive starting point for our network reconstruction, which we have named
                        <title rend="italic">Six Degrees of Francis Bacon</title> (SDFB), after the
                    early modern figure whose life spanned the sixteenth and seventeenth centuries
                    and whose career spanned the domains of politics, science, and letters. We chose
                    biographies because they are a well-established and highly standardized product
                    of modern historical scholarship. Moreover, a central collection of such data
                    was already available to us digitally through the <title rend="italic">Oxford
                        Dictionary of National Biography</title> (ODNB), which comprises the
                    biographies of people deemed by its editors as significant to British history.
                    Jerome McGann has argued that <cit>
                        <quote rend="inline" source="#mcgann2014">the whole of our cultural
                            inheritance has to be recurated and reedited in digital forms and
                            institutional structures</quote>
                        <ptr target="#mcgann2014" loc="1"/>
                    </cit>. Most often, in his account, this involves <emph>transference</emph> of
                    text <quote rend="inline" source="#mcgann2014">from bibliographical to digital
                        machines.</quote> SDFB tackles a related but more difficult problem: the
                        <emph>transformation</emph> of biographical text, which focuses on a single
                    person but contains rich information about social relations, into a global
                    (non-egocentric) network graph, which requires extracting information about
                    nodes (persons) and edges (relations) while ignoring or discarding other kinds
                    of biographical information.</p>
                <p>From the ODNB biographies of persons who lived between 1500-1700 we created an
                    initial dataset of 13,309 actor nodes. Each actor node could potentially be
                    connected to any of the other nodes, leading to over 88 million potential edges
                    to explore. Even within this initial dataset, already limited for manageability,
                    it was not feasible to verify each potential edge. One approach might have been
                    to curate these relationships in an ad-hoc order, as a scholar became interested
                    in a particular relationship or as relationships were explicitly documented in a
                    scholarly source. We would then have collected as many relationships as the time
                    and labor of scholars allowed, but we would have had little to say about the
                    relative importance of collected relationships and nothing at all to say about
                    those relationships yet to be curated. For instance, would the absence of an
                    edge mean that the two nodes shared no association or that the association has
                    yet to be explored in our network? Rather than rebuilding the network by hand,
                    we chose to employ a computational and statistical approach, unifying the
                    dispersed knowledge already extant in the literature into an inferred graph of
                    the network that can then be made available to scholars for correction and
                    curation.</p>
                <p>In the following sections, we lay out our statistical method for reconstructing
                    the early modern social network in four broad steps, then examine the
                    significance and limitations of our results from the perspective of humanist
                    scholarship. In section one, we discuss the process of identifying a collection
                    of textual documents to use as input, considering both direct and indirect
                    evidence of historical relationships. In section two, we explain how we used
                    Named-Entity Recognition (NER) to process the unstructured text into structured
                    data – specifically a matrix of documents and named entities – that was amenable
                    to statistical analysis. In section three, we give an overview of how we applied
                    statistical graph-learning methods to our structured data, with more detailed
                    technical information included an appendix. In section four, we discuss methods
                    of validating a sample of proposed relationships using the local expertise of
                    humanist scholars. In section five, we step back to examine the broader
                    significance of this process from the perspective of twenty-first-century
                    researchers in the humanities. We also examine the assumptions underlying our
                    statistical methods and potential areas of modification necessary before
                    redeploying these methods with other historical corpora.</p>
                <p>In developing this method, we have demonstrated the feasibility of applying graph
                    learning methods to any large collection of biographical text – early modern or
                    otherwise. This is neither a completely automated process nor a perfect one, but
                    we have also developed a practical mechanism by which expert feedback can
                    improve the network as well as the statistical procedures used to infer it. We
                    have thus created a viable and transferrable approach to inferring large-scale
                    historical social networks, which should be of particular interest to digital
                    humanists, scholars of networks and prosopography, as well as scholars
                    interested in the history of scholarship itself.</p>
            </div>
            <div>
                <head>1. Source Material</head>
                <p>The first step of our process was identifying the extent of available texts and
                    determining which texts were potentially the most useful for network inference.
                    Numerous types of primary and secondary sources can provide evidence of
                    historical relationships. Some of these sources provide direct evidence of a
                    link between two actors – for example, society membership rolls, marriage
                    certificates, or archival letters. Other sources may collectively provide
                    indirect evidence: the same two people mentioned together in numerous accounts
                    or biographies is highly suggestive of the possibility that those two people may
                    have come into contact with one another.</p>
                <p> In an ideal world we would have made use of all the relevant historical sources
                    and scholarship. In this one, we needed to begin with a collection of texts that
                    was well-defined, accessible, machine readable, and relatively uniform. We also
                    wanted to begin with a collection that included a broad range of potentially
                    relevant figures, according to social, geographic, and temporal standards. We
                    therefore decided to focus on the 58,625 biographical entries that make up the
                    ODNB. Running to sixty volumes in its print format, the ODNB is the labor of
                    10,000 scholars who have collectively contributed its 62 million words. </p>
                <p> On a technical level, the ODNB was praised upon its 2004 release for being <cit>
                        <quote rend="inline" source="#collini2005">the first to exploit the
                            potential of electronic publication on so vast and imaginative a
                            scale</quote>
                        <ptr target="#collini2005"/>
                    </cit>. We considered the ODNB an appropriate choice for several other reasons
                    as well. Several of the collaborators on this project share a primary interest
                    in the early modern era (c. 1500-1700) in Britain, a period well covered by the
                    ODNB. Both Carnegie Mellon University and Georgetown University have
                    subscriptions to the ODNB, providing us with legal access to the <cit>
                        <quote rend="inline" source="#collini2005">many possibilities opened up by
                            the online version for accessing and organising the hoard of
                            information</quote>
                        <ptr target="#collini2005"/>
                    </cit>. The ODNB’s dense-in-data documents fit the criteria of machine
                    readability and relative uniformity. Although the biographies vary in length,
                    all have a similar format and the raw text can be extracted in the same manner.
                    Lastly, as biographies, they contain both explicit mentions of relationships –
                    such as <quote rend="inline" source="#peltonen2004">Bacon’s life and career
                        during the 1590s was dominated by his close relationship with Robert
                        Devereux</quote> – as well as numerous implicit indicators of potential
                    relationships. Robert Cecil, for example, is mentioned five times in one section
                    of Bacon’s biography <ptr target="#peltonen2004"/>. The ODNB thus offered
                    opportunities to analyze both direct and indirect relationship data from a
                    single collection.</p>
                <p> As we worked with the ODNB data, a further advantage of this particular
                    collection emerged: its ability to shed light on the current state and history
                    of scholarship. Individually, each document in the ODNB is a roughly
                    chronological account of one person’s life, specifically an individual deemed by
                    nineteenth-, twentieth-, or twenty-first-century editors to have <cit>
                        <quote rend="inline" source="#collini2005">in some way influenced [British]
                            national life</quote>
                        <ptr target="#collini2005"/>
                    </cit>. As a collection, therefore, the ODNB holds significant information about
                    what has and has not risen to the level of scholarly notice since the late
                    Victorian creation of the Dictionary of National Biography, the ODNB’s
                    precursor, in the 1880s. The original DNB primarily emphasized the political,
                    literary, and scientific accomplishments of famous men, dedicating only 5% of
                    its overall entries to women, and only 2% of the entries in the target date
                    range 1500-1700. In the ODNB’s current version, the percentage of women has only
                    increased to 11% overall and 6% in our target date range <ptr
                        target="#matthew2004"/>.</p>
                <p>In our era of text mining and network visualizations, such biases have continued
                    effects. A bias towards men is a known issue in existing historiography; this
                    bias is neither confined to the ODNB nor particularly surprising. However,
                    transforming textual secondary sources into visual representations allows for
                    more purposeful <quote rend="inline" source="#evans2011">critical scrutiny of what is
                        known, how, and by whom</quote> – the branch of knowledge increasingly
                    referred to as <cit>
                        <quote rend="inline" source="#evans2011">metaknowledge</quote>
                        <ptr target="#evans2011"/>
                    </cit>. Our visualizations of the early modern social network demonstrate the
                    need both for more scholarship on women and other marginalized groups and for
                    the integration of this scholarship into broader discussions of earlier modern
                    society and culture; more importantly, it identifies the local areas of the
                    network where the need for such scholarship is most pressing.</p>
            </div>
            <div>
                <head>2. Pre-Processing Source Material</head>
                <p>After having identified our collection of source materials, we then had to
                    process the unstructured text – specifically a collection of HTML-formatted
                    documents acquired through the ODNB website – into a format more amenable for
                    analysis. This was done by extracting only the biographical portions of the text
                    from the initial HTML documents – stripping the HTML formatting, bibliographies,
                    and other extraneous text from the documents.<note>Our R code for both the
                        pre-processing and statistical analysis of our dataset is at <ref
                            target="https://github.com/sdfb/sdfb_network/tree/master/code/ODNB"
                            >https://github.com/sdfb/sdfb_network/tree/master/code/ODNB</ref>.</note>
                    We then ran the plaintext documents through two NER tools: one from Stanford’s
                    Natural Language Processing Group, denoted Stanford <ptr target="#finkel2005"/>,
                    and another from the LingPipe collection of tools <ptr target="#alias2008"
                    />.</p>
                <p> These NER tools use probabilistic methods to identify names and to classify
                    those names according to types such as person, location, or organization. For
                    example, the following sentence – <quote rend="inline" source="#campbell2004">The occasion
                        of ‘Lycidas’ was the death of Edward King, a fellow of Christ's College who
                        had drowned off the coast of Anglesey on 10 August 1637</quote> – might be
                    processed as <cit>
                        <quote rend="inline" source="#campbell2004">The occasion of
                            ‘[PERSON]Lycidas[/PERSON]’ was the death of [PERSON]Edward
                            King[/PERSON], a fellow of [ORGANIZATION]Christ's College[/ORGANIZATION]
                            who had drowned off the coast of [PLACE]Anglesey[/PLACE] on [DATE]10
                            August 1637[/DATE]</quote>
                        <ptr target="#campbell2004"/>
                    </cit>. This example shows that no classifier is perfect – <title rend="quotes"
                        >Lycidas</title> in fact is the title of Milton’s great elegy rather than an
                    historical person – and classifiers face particular challenges with
                    multiple-word entities such as <q>Christ’s College</q> where the the first word
                    separated from its follower could mistakenly if understandably be classed as a
                    person.</p>
                <p>For both Stanford and Lingpipe, we began with the default models trained on news
                    article corpora and ran the tools on ten randomly chosen documents from the
                    ODNB. These documents were then manually tagged to determine the accuracy of the
                    tools’ performance on our target dataset. Two measures of accuracy were used:
                    recall, the fraction of desired results obtained, and precision, or the fraction
                    of obtained results that are correct. For our purposes, high recall was
                    considered necessary, while high precision was desirable but less important.
                    Stanford achieved better recall than LingPipe, at 70.7% and 67.8% respectively,
                    but combining their results led to recall rates of 85.7%. The two tools were
                    combined by taking all of Stanford’s smatches, and then adding in LingPipe’s
                    matches if Stanford did not tag those specific words. In case of overlapping or
                    contradictory tags, we used Stanford’s matches.</p>
                <table>
                    <head>Recall and Precision for Various Subsets of NER Results</head>
                    <row role="label">
                        <cell>Subset</cell>
                        <cell>Recall</cell>
                        <cell>Precision</cell>
                    </row>
                    <row>
                        <cell>Stanford (ST), Person Tags Only <lb/> LingPipe (LP), Person Tags
                            Only</cell>
                        <cell>63.51%<lb/>52.44%</cell>
                        <cell>91.75%<lb/>72.11%</cell>
                    </row>
                    <row>
                        <cell>ST, Person and Organization Tags<lb/>LP, Person and Organization
                            Tags</cell>
                        <cell>70.74%<lb/>67.83%</cell>
                        <cell>74.02%<lb/>46.19%</cell>
                    </row>
                    <row>
                        <cell>ST + LP, Person Tags Only<lb/>ST + LP, Person and Organization
                            Tags</cell>
                        <cell>79.37%<lb/>85.66%</cell>
                        <cell>77.91%<lb/>51.61%</cell>
                    </row>
                </table>

                <p>We then implemented two additional procedures to improve recall and precision.
                    First, to improve recall, we ran the documents through NER twice: once to create
                    the initial tags and a second time using the initial tags as a dictionary, which
                    enabled us to search for missed instances of phrases that were tagged during the
                    first pass through the documents. This latter search was particularly successful
                    at capturing partial name co-references, which occur within documents when
                    historical figures are referred to only by their first or last name. With few
                    exceptions, partial names that are part of a longer name found in that document
                    are not actually different people. <q>Bacon</q> in a document containing
                        <q>Francis Bacon</q> will refer, except in rare cases, to Francis Bacon. If
                    a partial name matched the subject of a biography, it was considered a mention
                    of that subject. Otherwise, partial names were considered mentions of the
                    matching most recent full-name mention.</p>
                <p>Second, to improve precision, we implemented manual rules to reduce the number of
                    non-human names detected. This included removing all phrases that contained
                    words beginning with lower-case letters; exceptions were made for the words
                        <q>of</q> and <q>de</q> which often form part of names during this period,
                    i.e. <q>Katherine of Aragon.</q> We also eliminated phrases with non-alphabetic
                    characters – such as $, *, and numbers – and common non-human proper names
                    supplied by our subject matter experts – such as <q>Commonwealth,</q>
                    <q>Catholic,</q>
                    <q>Greek,</q> and <q>Roman.</q></p>
                <p>This resulted in final recall rates of 96.7% and precision rates of 65.5% on the
                    initial test set. Testing on six new randomly-chosen documents led to a similar
                    95.3% recall rate but a slightly lower 54.0% precision rate. As our priority was
                    a high recall rate, this was deemed acceptable. A later examination of a random
                    200-entity sample indicated the overall dataset’s precision rates were
                    approximately 59% with +/- 7% margin of error.</p>
                <p> From these results, we created a large table of documents and named entities.
                    For each document, we tabulated the named entities and their number of mentions,
                    which led to 494,536 different named entities occurring throughout the
                    collection of 58,625 documents. We then reduced the number of named entities in
                    two ways. First, we ignored named entities that did not occur in an ODNB
                    biography within the period of interest (1500-1700). This made network inference
                    less costly computationally. So too with our second step, in which we omitted
                    names that occurred in fewer than five documents. Since correlations are very
                    difficult to determine with sparse data, inferring relations among low count
                    documents would have increased the number of false positives. While the five
                    mention threshold did unfortunately mean that we had to eliminate many less
                    prominent individuals, or those referred to by different names across the ODNB,
                    the tradeoff was that it helped us achieve better precision at less
                    computational cost. A final stage required further human curation –
                    specifically, searching for names in the ODNB – to disambiguate people who
                    shared the same name and de-duplicate people referenced under multiple names,
                    particularly for names obtained through the NER tools. While recent research in
                    the NLP community has focused on finding a way to automate this final stage,
                    such as the Berkeley Entity Resolution System, we preferred the accuracy of
                    manual curation <ptr target="#durrett2014"/>.</p>
                <p>The resulting table of 58,625 rows and 13,309 columns is known mathematically as
                    a matrix. This <emph>n</emph>×<emph>p</emph> matrix, Y, has <emph>n</emph> rows
                    representing documents and <emph>p</emph> columns representing people, or
                    actors, in the network. The number of times person <emph>j</emph> is mentioned
                    by name in document <emph>i</emph> gives us Y<hi rend="subscript">ij</hi>, a
                    non-negative integer for each document/person pair. We used this document-count
                    matrix to infer the social network.</p>
            </div>
            <div>
                <head>3. Statistical Inference</head>
                <p>We motivated our statistical model for the previously described document-count
                    matrix by assuming that direct connections between historical figures would be
                    reflected by their being mentioned together in documents. Indeed, prior work has
                    shown it possible to infer a rough graph based on co-mentions alone <ptr
                        target="#bosch2011"/>
                    <ptr target="#bosch2012"/>. However, our model requires more than a simple count
                    of co-mentions because co-mentions sometimes result from confounding factors
                    such as mutual acquaintances. </p>

                <figure>
                    <head>Charles I as a Confounding Variable.</head>
                    <graphic url="resources/images/figure01.png"/>

                </figure>

                <p>Consider an example such as the one displayed in Figure 2. George Villiers, Duke
                    of Buckingham (1592-1628), knew King Charles I (1600-1649), and Charles I knew
                    Prince Rupert of the Rhine (1619-1682), but Buckingham and Prince Rupert – whose
                    lives only barely overlapped – never met. Because Prince Rupert and Charles I
                    are connected, they will tend to be mentioned together in source documents. How
                    often Prince Rupert is mentioned can therefore be predicted in part from how
                    often Charles I is mentioned. Likewise if Charles I and Buckingham are
                    connected, mentions of Buckingham predict mentions of Charles I. But in the case
                    of no direct tie between Prince Rupert and Buckingham, as here, their names may
                    still correlate due to mentions of Charles I. Despite such correlation, mentions
                    of Buckingham convey no information about mentions of Prince Rupert not already
                    accounted for by mentions of Charles I. We thus reasoned that co-mentions found
                    in our document-count matrix – and correlations between any two given nodes
                    derived from the matrix – might be the result of one or more confounding
                    factors.</p>
                <p>Under these assumptions, inferring the existence of network connections is the
                    same problem as inferring the conditional independence structure in a particular
                    statistical model – in this case, our document-count matrix <ptr
                        target="#glymour2001"/>. The method we used to infer the conditional
                    independence structure of the document-count matrix is the Poisson Graphical
                    Lasso, a penalized regression method proposed by Allen and Liu. This method is a
                    generalization of Meinshausen and Bühlmann’s computationally faster
                    approximation to the graphical lasso <ptr target="#friedman2008"/>; it defines
                    the relationships between nodes by a conditional Poisson distribution, instead
                    of a multivariate normal distribution, which allows the penalized regression to
                    be modified by an individual node’s count data <ptr target="#allen2012"/>. This
                    method allowed us to create a symmetric <hi rend="italic">p</hi>×<hi
                        rend="italic">p</hi> correlation matrix <hi rend="italic">θ̂</hi>, where two
                    nodes <hi rend="italic">j</hi> and <hi rend="italic">k</hi> are conditionally
                    independent if and only if the coefficient <hi rend="italic">θ̂<hi
                            rend="subscript">jk</hi></hi> = <hi rend="italic">θ̂<hi rend="subscript"
                            >kj</hi></hi> is zero.</p>
                <p>In some applications of graphical models to infer network structure, all non-zero
                    coefficients are of interest. For example, in gene networks, the expression
                    levels of two connected genes may be negatively (conditionally) correlated. In
                    our social network, however, we are primarily concerned with positive
                    coefficients, as a relationship between two people should lead to a positive
                    conditional correlation of their mentions in a document. A small or zero
                    correlation suggests a lack of relationship between two people, while negative
                    correlations might occur for a variety of reasons, including non-overlapping
                    lifespans or two-degree – i.e., friend-of-a-friend – relationships without
                    so-called triadic closure <ptr target="#simmel1950"/>.</p>
                <p> We therefore used our initial correlation matrix to create an adjacency matrix Y
                    – a symmetric <hi rend="italic">p</hi>×<hi rend="italic">p</hi> matrix where
                        Y<hi rend="subscript">ij</hi>=Y<hi rend="subscript">ji</hi>=1 when there is
                    a positive correlation and assumed relationship between person <hi rend="italic"
                        >i</hi> and person <hi rend="italic">j</hi>, and 0 otherwise. Because our
                    data and methods provide more information about some edges than others, however,
                    we wanted to be able to attach a confidence estimate to potential edges instead
                    of simply obtaining a yes or no estimate. </p>
                <p>Confidence estimates were also better suited to the grey areas of humanistic
                    research often requiring interpretation and even guesswork. In order to create
                    this confidence estimate, we fit the Poisson Graphical Lasso on random subsets
                    of our data 100 times and added the resulting adjacency matrices into a final
                    matrix that we called our confidence matrix, C. This calculation gave us a
                        <q>confidence level</q> for the likelihood of a relationship’s existence
                    that ranged between 0 – never inferred – and 100 – always inferred.</p>
                <p>Throughout this process, we experimented with tuning parameters and found that
                    our final estimates did not vary significantly for all reasonable tuning
                    parameters, where reasonable is defined as a low enough penalty such that edges
                    are actually added, but high enough penalty that the algorithm converges
                    rapidly. We also conducted penalty parameter training – using expert knowledge
                    to manually confirm the existence of some relationships – but found this
                    produced only very localized changes and had little impact on the overall
                    network structure. The only significant manual intervention in this basic method
                    thus came from our name disambiguation procedures, as we had nearly one thousand
                    non-unique names in our node set. To deal with the the challenge of multiple
                    individuals sharing the same name, we first disallowed positive adjacency
                    estimates between two people with non-overlapping lifespans (with a one-year
                    margin of error for posthumous children). Second, we used probabilities based on
                    biography length to distribute adjacency estimates among people with overlapping
                    lifespans.</p>
                <p>A fuller explanation of our application of the Poisson Graphical Lasso can be
                    found in our Appendix, along with a link to our code.</p>
            </div>
            <div>
                <head>4. Expert Validation</head>
                <p>Having constructed our confidence matrix of estimated relationships, we then
                    conducted three different types of validation checks: one to ensure that our
                    results showed the homophily that network studies have taught us to expect when
                    semantic context is taken into account; one to confirm that our results were
                    consistent with statistical theory; and one to evaluate the accuracy of our
                    results in comparison with an expert human reading of the ODNB biographies. We
                    first used topic modeling on approximately 90% of our dataset – excluding people
                    with duplicate names whose relationships had to be disambiguated – to evaluate
                    different kinds of actor connectivity in a semantic context. Then, on smaller
                    subsets of our data, we compared our results with alternative statistical
                    methods, and calculated precision and recall rates.</p>
                <p> Our first validation step was motivated by the fact that the Poisson Graphical
                    Lasso counts names but ignores semantic context. As a way to test the validity
                    of this approach, we wanted to compare the connectivity of actors who are
                    mentioned in similar contexts to the connectivity of those mentioned in
                    different contexts, since actors who share contexts are more likely to know one
                    another than those who do not <ptr target="#mcpherson2001"/>. Thus for our first
                    validation step, we decided to create and analyze a latent dirichlet allocation
                    (LDA) topic model – an algorithm for extracting semantic clusters from a set of
                    text <ptr target="#blei2003"/>
                    <ptr target="#weingart2012"/>. We hoped to find our network data showed greater
                    connectivity between actors who share a context than between actors in different
                    contexts, as generated through the topic model. If so, we could conclude that
                    our approach produces results compatible with accepted, semantically-sensitive
                    approaches.</p>
                <p>To generate our topic model, we created a ‘bag of words’ for each person in our
                    dataset, comprised of all words that appear before and after the person’s name
                    in the ODNB. Specifically, for each person in the network, we located all
                    mentions in the ODNB, and used the previous fifteen words and next twenty-five
                    words – excluding named entities – as their <q>bag of words</q>. The choice of
                    these two numbers was motivated by attempting to capture the current sentence
                    and the previous and next sentences. We then removed all named-entity mentions
                    in these biographies and converted the remaining words into lower case. Next we
                    applied the Porter stemmer <ptr target="#porter1980"/>, an algorithm that strips
                    away standard English suffixes in a specific order. For example, the Porter
                    stemmer turns the word ‘publisher’ into ‘publish’, and does same to the word
                    ‘published’. We then dropped words that are in a standard stoplist – which
                    includes words like ‘and’, ‘the’, etc. – provided in the text-mining R package
                    tm <ptr target="#feinerer2008"/>. In addition, we dropped all month words, some
                    numbers, and select relationship terms – a complete list can be seen in our
                    topic modeling R code.<note> Our R code for the topic modeling is at <ref
                            target="https://github.com/sdfb/sdfb_network/tree/master/code/topic_models"
                            >https://github.com/sdfb/sdfb_network/tree/master/code/topic_models</ref></note>
                    The remaining words in the ‘bag of words’ for each person thus approximately
                    reflected their profession, accomplishments, or <q>historical significance</q>
                    as given by the ODNB. </p>
                <p>Using each of these <q>bags of words</q> as an individual text, we fit three
                    topic models to our collection of texts.<note>For several of the known
                        limitations of topic models in digital humanities, see <ptr
                            target="#meeks2012"/>.</note> The three topic models were generated with
                    five, ten, and twenty topics, respectively, to ensure the number of topics did
                    not significantly alter our results. The topics were generated automatically and
                    each text – and the person it represents – was assigned to the topic with the
                    highest probability match. The top representative terms in our ten-topic model
                    can be seen in Figure 3. The clustering of historical subjects and people in our
                    topics is encouraging, as many of the topics clearly represent different kinds
                    of historical activities. For example, Topic 3 in our ten-topic model contains
                    the words <q>bishop,</q>
                    <q>church,</q>
                    <q>minist</q> and <q>preach</q>, as well as a large number of churchmen such as
                    Richard Bancroft and John Whitgift (see Figure 3). Topic 8 includes the words
                        <q>publish</q>, <q>poem</q>, and <q>play,</q> along with poets like Robert
                    Herrick, John Donne, and John Dryden.</p>

                <table>
                    <head>A table of the top representative terms in the ten-topic model</head>
                    <row role="label">
                        <cell>Topic 1</cell>
                        <cell>Topic 2</cell>
                        <cell>Topic 3</cell>
                        <cell>Topic 4</cell>
                        <cell>Topic 5</cell>
                    </row>
                    <row>
                        <cell>london<lb/>famili<lb/>merchant<lb/>coloni<lb/>trade<lb/>work</cell>
                        <cell>earl<lb/>lord<lb/>parliament<lb/>second<lb/>king<lb/>london</cell>
                        <cell>bishop<lb/>colleg<lb/>church<lb/>minist<lb/>preach<lb/>london</cell>
                        <cell>armi<lb/>command<lb/>return<lb/>forc<lb/>captain<lb/>ship</cell>
                        <cell>work<lb/>publish<lb/>letter<lb/>write<lb/>public<lb/>book</cell>
                    </row>
                    <row role="label">
                        <cell>Topic 6</cell>
                        <cell>Topic 7</cell>
                        <cell>Topic 8</cell>
                        <cell>Topic 9</cell>
                        <cell>Topic 10</cell>
                    </row>
                    <row>
                        <cell>king<lb/>polit<lb/>parliament<lb/>appoint<lb/>duke<lb/>lord<lb/></cell>
                        <cell>work<lb/>london<lb/>publish<lb/>book<lb/>print<lb/>physician<lb/></cell>
                        <cell>work<lb/>publish<lb/>poem<lb/>play<lb/>translat<lb/>edit<lb/></cell>
                        <cell>earl<lb/>london<lb/>famili<lb/>marriag<lb/>second<lb/>will<lb/></cell>
                        <cell>king<lb/>queen<lb/>english<lb/>england<lb/>court<lb/>royal<lb/></cell>
                    </row>
                </table>


                <p>We then analyzed the frequency of our estimated relationships between people who
                    do and do not share topics. For all three topic models, estimated relationships
                    between people who shared a topic are more frequent than between-topic estimated
                    relationships; the specific results for the ten-topic model can be seen in
                    Figure 4. This coincides with the expectation for homophily (also known as
                    assortativity) and a qualitative, semantics-based reading of the same data: book
                    authors are more likely to be linked to other notable authors as opposed to
                    notable military personnel. While running a topic model with different
                    parameters (i.e. number of topics) changes the specific results, within-topic
                    relationships remain more frequent than between-topic relationships. We
                    therefore concluded that the Poisson Graphical Lasso produces results compatible
                    with other, semantically-sensitive methods.</p>

                <table>
                    <head>A table of relationship confidence estimates in the ten-topic model</head>
                    <row role="label">
                        <cell>Measure</cell>
                        <cell>Within-Topic</cell>
                        <cell>Between-Topic</cell>
                    </row>
                    <row>
                        <cell>Fraction of Edges with Confidence ≥ 90%</cell>
                        <cell>0.0001444</cell>
                        <cell>0.0000117</cell>
                    </row>
                    <row>
                        <cell>Fraction of Edges with Confidence ≥ 75%</cell>
                        <cell>0.0004523</cell>
                        <cell>0.0000527</cell>
                    </row>
                    <row>
                        <cell>Fraction of Edges with Confidence ≥ 50%</cell>
                        <cell>0.0016402</cell>
                        <cell>0.0003082</cell>
                    </row>
                    <row>
                        <cell>Fraction of Edges with Confidence ≥ 30%</cell>
                        <cell>0.0033207</cell>
                        <cell>0.0007585</cell>
                    </row>
                </table>

                <p>Our second validation step was to compare our results with alternative methods of
                    constructing a confidence matrix. Using Spearman correlations, which measure how
                    well the ordering of two ranked lists align, we evaluated how each method
                    performed against expert-generated ranked relationship lists. We had earlier
                    considered using three possible methods for inferring a correlation matrix from
                    the document-count matrix: 1) ranking by simple correlation (high positive
                    correlations are higher ranked relationships); 2) running the Poisson Graphical
                    Lasso and ranking edges by the value of ϱ in which the edge was added to the
                    model (edges added with more penalization are higher ranked relationships); and
                    3) running the Poisson Graphical Lasso and ranking edges by the value of the
                    regression coefficient (higher positive coefficients are higher ranked
                    relationships). According to statistical theory, both versions of the Poisson
                    Graphical Lasso should perform as well as, if not better than, simple
                    correlation because of their ability to screen off friend-of-a-friend
                    connections, as described in section three above. We hoped to find this
                    reflected in our Spearman correlations, in order to conclude that our approach
                    produces results compatible with statistical theory.</p>
                <p>We chose to test this on James Harrington and John Milton by taking the top
                    thirty relationships according to each of these three methods and combining them
                    to create a master list of thirty and eighty relationships, respectively.
                    Faculty and PhD students with backgrounds in the early modern period were given
                    the combined names in random order and asked, first, to rank the relationships
                    according to a question we used to approximate relationship importance,
                    specifically <q>how unhappy would experts be if this relationship were not
                        included among the main actor’s top relationships?</q> and, second, to mark
                    the relationships as true/false. Despite only being an approximation to
                    relationship importance, the ranking list still proved far more difficult for
                    the humanists to generate than the true/false list.</p>
                <p> We wanted to choose the statistical method that created lists most closely
                    correlated to the humanists’ list, as measured by Spearman correlation. The
                    Spearman correlations of each method were extremely similar in the humanists’
                    ranked lists and – combined with humanists’ concerns over producing the list in
                    the first place – led us to abandon the effort to optimize our algorithm for the
                    order of ranks. Instead, we attempted to determine which method obtained more
                    correct relationships – that is, relationships humanists marked as true – in the
                    top <hi rend="italic">k</hi> estimated connections. For analysis of James
                    Harrington’s thirty connections, all three methods performed similarly; for John
                    Milton’s eighty connections, using simply the correlation coefficient led to
                    worse estimates earlier on, confirming that the Poisson Graphical Lasso can more
                    accurately reproduce sections of the network than correlation alone.</p>
                <p>Lastly, for our third validation step, we wanted to evaluate the accuracy of our
                    final inferred network, in comparison to the relational knowledge conveyed by a
                    humanist reading of the ODNB. We therefore chose twelve people from the network
                    and calculated the precision and recall rates for their relationships. The
                    twelve people were not a random sample. Rather, they were chosen to represent a
                    variety of conditions within our dataset, including gender, number of estimated
                    relationships, deduplicated names, and appearance within individual vs. shared
                    ODNB biographies. Some of these conditions are relatively rare within the
                    dataset on the whole. For each person, we checked their inferred edges from
                    40-100% confidence – qualitatively tagged as our <q>possible</q> to
                        <q>certain</q> confidence interval – against a list of associations manually
                    compiled from the ODNB documents by reading through each person’s biographical
                    entry and other entries in which their name appears.</p>
                <p>Together, these twelve people had twenty-eight relationships in our
                    likely-to-certain (60-100) confidence interval, of which three were incorrect,
                    leading to an 89.29% precision rate (see Figure 5). Expanding our confidence
                    interval to also include possible relationships (40-100) – in other words,
                    sacrificing precision to increase recall – gave us one hundred and seven
                    relationships of which twenty-seven were incorrect, leading to a
                    still-respectable 74.77% precision rate. The majority of these false positives
                    were caused by specific conditions within our data: group biographies, duplicate
                    names, and an abnormally high percentage of co-mentions within related
                    biographies. Removing the four people who satisfied these specific conditions
                    from our sample left us with fifty relationships and a 86.00% precision rate in
                    our 40-100 confidence interval, which suggests that many of the errors in our
                    dataset are associated with people who fulfill these conditions, which impaired
                    our algorithm’s ability to correctly capture their relationships via
                    co-mentions. Because our validation sample had taken care to include some of our
                    most problematic case-types, even though instances of some of those case-types
                    are relatively few, we deemed these measures of precision adequate as a starting
                    point for further curation of the network via crowd-sourcing on our website at
                        <ref target="http://www.sixdegreesoffrancisbacon.com"
                        >www.sixdegreesoffrancisbacon.com</ref>.</p>
                <table>
                    <head>Precision and Recall for a Subset of the Inferred Network</head>
                    <row role="label">
                        <cell>Confidence Interval</cell>
                        <cell>Number of SDFB Inferred Relationships</cell>
                        <cell>Precision <lb/>(# correct / # found)</cell>
                        <cell>Article Recall<lb/>(# found in article / # in article)</cell>
                        <cell>SDFB Recall <lb/>(# found in article also in SDFB / # from
                            article)</cell>
                    </row>
                    <row>
                        <cell>80-100 (certain)</cell>
                        <cell>5</cell>
                        <cell>80.00%</cell>
                        <cell>1.98%</cell>
                        <cell>3.96%</cell>
                    </row>
                    <row>
                        <cell>60-100 (likely)</cell>
                        <cell>28</cell>
                        <cell>89.29%</cell>
                        <cell>8.42%</cell>
                        <cell>16.83%</cell>
                    </row>
                    <row>
                        <cell>40-100 (possible)</cell>
                        <cell>107</cell>
                        <cell>74.77%</cell>
                        <cell>25.74%</cell>
                        <cell>51.49%</cell>
                    </row>
                    <row>
                        <cell>10-100<lb/>(unlikely)</cell>
                        <cell>283</cell>
                        <cell>≥28.27%<note>We judged validating the additional 176 inferred
                                relationships in the 10-39 confidence interval to be too
                                labor-intensive, with little added benefit, to be worth calculating.
                                Given the number of already-validated relationships in the 40-100
                                confidence interval, we calculated the lowest possible precision
                                rate in our 10-100 confidence interval to be 28.27%. It is very
                                likely higher.</note></cell>
                        <cell>33.66%</cell>
                        <cell>67.33%</cell>
                    </row>
                </table>

                <p>Calculating a global recall – the fraction of desired results obtained from the
                    ODNB as a whole – on our dataset would have required us to identify connections
                    across the entire biographical corpus of the ODNB, a prohibitively
                    labor-intensive process when done manually. We therefore calculated two partial
                    measures of recall instead. The first measure is article-level recall – that is,
                    a measure of the ability of our network to capture the same relationships as a
                    human reading a specific biographical article. By this measure, our recall
                    numbers were low, with our 40-100 confidence interval including only 25.74% of
                    the relationships mentioned in the article. Low article recall can be
                    attributed, at least in part, to two factors: first, the decision to impose a
                    five-mentions threshold during the NER stage, which excludes infrequently
                    mentioned names about which the ODNB provides insufficient network data, and,
                    second, the way some names are mentioned in the ODNB, which prevented them from
                    being picked up by NER.</p>
                <p>Next, we calculated the measure we call <q>SDFB</q> recall – that is, the ability
                    of our computer algorithms to infer relationships for the subset of people
                    mentioned in a specific biographical article who were also included in our
                    overall network. This adjustment – excluding people who did not pass the
                    five-mentions threshold or were not captured by NER – leads to a significantly
                    higher recall numbers, at 51.49%, again for the 40-100 confidence interval.
                    Further expansion of the confidence interval to 10-100 increases the SDFB recall
                    rate to 67.33%, showing that within the subset of names captured by NER and
                    included in our node dataset, high recall rates can be achieved at the lowest
                    confidence intervals. Though higher recall rates would of course be desirable in
                    theory, we deemed it preferable to have a relatively accurate but sparse network
                    rather than a full but error-ridden network, and further increases in recall
                    would require corresponding trade-offs in precision. </p>
            </div>
            <div>
                <head>5. Humanities Significance</head>

                <p> Though the map of the early modern social network created by our inference
                    procedures is far from perfect, it provides a sizeable base of persons and
                    relationships that can be gradually corrected and expanded to encompass the
                    interests of a wide range of humanist scholars. This network can also be
                    examined, validated, refined, and expanded by scholars, students, and other
                    end-users through a dynamic wiki front-end with sophisticated network
                    visualization tools. We consider such an approach complementary to several
                    successful approaches that focus on smaller subsets of society <ptr
                        target="#long2013"/>
                    <ptr target="#ahnert2015"/>
                    <ptr target="#basu2015"/>
                    <ptr target="#during2015"/>. An important possible outcome of the project is the
                    integration, or re-integration, of disparate threads of network scholarship. </p>
                <p>The questions humanists care most about often turn on documentary evidence of
                    connections, and immersion in an archive or a published collection of letters
                    yields qualitative knowledge of unparalleled depth and richness. Yet the
                    humanities would need to see massive investments in historical analysis,
                    palaeography, languages, and other humanistic research skills in order to
                    investigate anything close to the number of relationships inferred using our
                    model. Since little in the current funding climate suggests that such
                    investments are immediately forthcoming, the promises of historical network
                    analysis would remain unrealized in the absence of a different approach. Hence
                    our probabilistic network inferences, which create a workable infrastructure for
                    subsequent investigation. Instead of starting the process of mapping the network
                    from scratch, we remediate existing scholarship for further addition, expansion,
                    development, and correction.</p>
                <p>The time, moreover, appears to be right. With open access research gaining
                    momentum, and more and more texts entering the public domain, probabilistic
                    text-mining approaches afford wider lenses and present new opportunities <ptr
                        target="#elson2010"/>
                    <ptr target="#hassan2012"/>
                    <ptr target="#underwood2013"/>
                    <ptr target="#makazhanov2014"/>
                    <ptr target="#riddell2014"/>
                    <ptr target="#smith2014"/>. Even as such approaches will always benefit from the
                    depth and precision afforded by more traditional archival analyses,
                    non-commercial repositories like the HathiTrust Research Center and commercial
                    ones (such as Google Books) represent exciting corpora for large scale
                    reconstruction of historical social networks. Treating the high-quality
                    historical scholarship as a source of unstructured data, moreover, helps us
                    avoid some of the pitfalls recently observed in studies based on less scholarly
                    data sources like Wikipedia and Freebase <ptr target="#gloor2015"/>
                    <ptr target="#schich2014"/>
                    <ptr target="#weingart2015"/>. Such studies based on declared links in
                    non-scholarly corpora haven’t yet achieved the plausibility achieved on the
                    smaller scale by old-fashioned archival work and entering attested links by
                    hand. </p>
                <p>At the same time, partnerships between traditional small-scale projects and
                    larger-scale projects like <title rend="italic">Six Degrees of Francis
                        Bacon</title> offer benefits to both sides. For those studying local
                    networks, large, probabilistic global networks offer chances to compare and
                    contextualize findings from smaller groups. For those working at larger scales
                    and with higher cumulative levels of uncertainty and error, small networks can
                    function as ground truths against which to test inferences and from which
                    partners may improve network models. </p>
                <p>Our approach isn’t just a new method. It yields substantive insights as well.
                    Applying quantitative network measures like network degree has allowed us to
                    identify interesting figures, such as those who have relatively high degrees but
                    who don’t have ODNB entries of their own. An analysis of high-degree nodes
                    without ODNB entries shows an intriguingly high representation of schoolmasters
                    and publishers. Individuals like Thomas Smelt, an ardent royalist who taught at
                    the Northallerton Free School in Yorkshire, and Edward Sylvester, who ran a
                    grammar school in Oxford, were not deemed significant enough to warrant full
                    biographical entries, but they are nevertheless key nodes connecting those who
                    were <ptr target="#otis2014b"/>.</p>
                <p>It is also possible from this work to understand more about non-British people
                    who figure prominently the life of the nation. Scholars can learn much about
                    international dimensions by attending to the frequency of non-native names
                    appearing frequently in the ODNB. Our five-mention threshold also helps us see
                    gender differences in a revealing light. The cultural practice of changing one’s
                    surname at marriage means that women face particular obstacles meeting our
                    artificially-imposed five-mention threshold. In several cases, men appear in the
                    dataset simply because they are mentioned in association with important women –
                    wives, sisters, or mothers who for various reasons may not themselves appear in
                    the dataset. The woman referred to in the ODNB as <title rend="quotes">Audrey,
                        widow of Sir Francis Anderson and eldest daughter of John Boteler, Baron
                        Boteler of Brantfield</title> does not appear in the dataset whereas John
                    Boteler does <ptr target="#seccombe2014"/>. Similarly, men like Thomas Bellenden
                    and Richard Stubbe aren’t known as historically significant, but they appear in
                    the dataset because their names remain consistent whereas their wives and
                    sisters appear by several names. In other cases, it isn’t a personal ODNB entry
                    that ensures a name gets included but a legal case or a much-cited will <ptr
                        target="#otis2014b"/>. Our analysis has also illustrated how inferred
                    networks differ based on ways of talking about people. James VI of Scotland and
                    James I of England name the same person, yet each name is associated with
                    substantially distinct social networks <ptr target="#otis2014a"/>. </p>
                <p>Ultimately, our work with the ODNB has shown that processing an entire corpus of
                    documents and running a statistical procedure is computationally feasible with
                    the resources generally available to university scholars. We have also shown
                    that it is possible to implement a statistical approach that infers a validated
                    social network. While not all highest-confidence edges are among the strongest
                    identified by experts – and some expert-identified relationships are not near
                    the top edges found – there is enough overall validation on many classes of
                    relationships to suggest our method is viable for reconstructing historical
                    social networks, within a reasonable margin of error, from large textual
                    corpora. </p>
                <p> This process admittedly has several shortcomings, especially from the
                    perspective of humanists for whom <q>margin of error</q> is a less than
                    reassuring phrase. Absent further research, there is no surefire way to
                    determine whether a given confidence estimate accurately reflects the current
                    state of scholarship (as represented by the ODNB) or is instead an artifact of
                    the bespoke model we developed. Nor are relationships in the resulting dataset
                        <q>typed</q> – friends and enemies remain functionally identical in our
                    results, though the difference of course matters decisively in real life. Proof
                    or other evidence about a given relationship will initially appear elusive: the
                    process yields few clues about where to start researching a relationship –
                    though our crowd-sourcing website does at least provide users links to ODNB and
                    JSTOR articles that mention both people in a relationship. And humanists must be
                    involved at every stage for validation, interpretation, de-duping, and
                    disambiguation. However, the end result of this process is of demonstrable use
                    to experts in early modern Britain and it is likely extensible to other large
                    corpora. </p>
                <p> We are the first to acknowledge that our network inference procedure comes
                    freighted with assumptions and technical limitations that may pose obstacles to
                    its transferability to other social networks generated from other data sources.
                    Inferring a network from biographical texts requires assuming that the
                    co-occurrence of names in a document is a reasonable predictor of a relationship
                    between the named persons. Although we believe this is a reasonable and
                    productive assumption for ODNB texts, it is not an equally reasonable assumption
                    for all data sources. Network inference will be only as good as the NER on which
                    it depends. Differences in NER availability and accuracy for different languages
                    (Stanford, for example, has separate modules for Spanish, German, and Chinese),
                    as well as differences in naming conventions across cultures, time periods,
                    discourses, and biographical data may decrease its effectiveness, though NER can
                    be tuned for different datasets. Because the ODNB entries have been carefully
                    edited and checked, they are relatively error free, but projects that aim to
                    mine biographical reference works that exist only in uncorrected Optical
                    Character Recognition documents will begin with a significant level of textual
                    error. Those who seek to employ our procedures on other biographical data
                    sources should perform checks to ensure that it is inferring edges between nodes
                    at level of accuracy that they deem acceptable. </p>
            </div>
            <div>
                <head>Conclusion</head>
                <p> While our interest has been in reconstructing the social network of a specific
                    time and place – sixteenth- and seventeenth-century Britain – there are few
                    barriers to re-deploying our method in other historical or contemporary
                    societies. We used short biographical entries, but we could with minor changes
                    have used contemporary book prefaces, modern scholarly articles, blogs, or other
                    kinds of texts. All that is needed is machine-readable text in which the
                    co-occurrence of names is a reasonable indicator of connections between persons.
                    Future work on our specific project may thus involve expanding the collection of
                    documents used in our network. Target documents currently include the publishing
                    data in the English Short Title Catalog and the prefatory material in Early
                    English Books Online. We would also aim to incorporate datasets whose strengths
                    would mitigate the data’s current weaknesses, such as collections of letters
                    written by women or urban apprenticeship rolls.</p>
                <p> We have also begun to expand our network through the data provided by individual
                    scholars via our website interface at <ref
                        target="http://www.sixdegreesoffrancisbacon.com"
                        >www.sixdegreesoffrancisbacon.com</ref>. To encourage mass integration of
                    other datasets, we have incorporated features into our website to allow the
                    tagging of nodes and the visualization of sub-networks by those tags. However,
                    we also have a particular interest in scholars adding citations to confirm our
                    statistically predicted relationships, as well enriching those relationships by
                    providing information about their type and timespan. Our ultimate goal is to
                    create a versatile and extensible network that people interested in all aspects
                    of early modern Britain – including the scholarship on early modern Britain –
                    can use for their research, as well as to pioneer a general technique of
                    creating social networks from texts that other scholars can apply to other
                    periods and societies.</p>
            </div>
            <div>
                <head>APPENDIX: The Poisson Graphical Lasso</head>
                <div>
                    <head>Introduction</head>
                    <p>Our statistical approach follows the model of G.I. Allen and Z. Liu <ptr
                            target="#allen2012"/>. Inference of the network is based on statistical
                        graph learning techniques. Here we have a graph G=(V,E), where V is the set
                        of <hi rend="italic">p</hi> nodes and E is the set of pairwise edges. We
                        relate the graph to a random vector Y=(Y<hi rend="subscript">1</hi>, ... ,
                            Y<hi rend="subscript">p</hi>) by requiring that for each non-edge
                        (j,k)∉E, the variables Y<hi rend="subscript">j</hi> and Y<hi
                            rend="subscript">k</hi> are conditionally independent given all the
                        remaining variables Y<hi rend="subscript">∖{j,k}</hi>, where ∖{j,k} denotes
                        the complement V∖{j,k}. Commonly, Y=(Y<hi rend="subscript">1</hi>, ... ,
                            Y<hi rend="subscript">p</hi>) is assumed to follow a multivariate normal
                        distribution N<hi rend="subscript">p</hi>(µ,∑), in which case pairwise
                        conditional independence holds if and only if ∑<hi rend="subscript"
                            >jk</hi><hi rend="superscript">-1</hi>=0 <ptr target="#lauritzen1996"/>.
                        In this case, inferring the graph corresponds to inferring the non-zero
                        elements of ∑<hi rend="superscript">-1</hi>.</p>
                    <p> If we have <hi rend="italic">n</hi> independent and identically distributed
                        observations of Y, we can employ penalized likelihood methods, where we
                        place a one-norm penalty on elements of the concentration matrix. This
                        penalized likelihood can be maximized efficiently for large <hi
                            rend="italic">p</hi> using a graphical lasso <ptr target="#friedman2008"
                        />. Alternatively, an approximate solution can be obtained through a
                        sequence of penalized regressions of each variable Y<hi rend="subscript"
                            >j</hi> on the remaining variables Y<hi rend="subscript">∖j</hi>=Y<hi
                            rend="subscript">∖{j}</hi>. We estimate σ<hi rend="subscript">jk</hi><hi
                            rend="superscript">-1</hi> = 0 if the estimated regression coefficients
                        of variable <hi rend="italic">j</hi> on <hi rend="italic">k</hi> or <hi
                            rend="italic">k</hi> on <hi rend="italic">j</hi> are estimated to be 0
                            <ptr target="#meinshausen2006"/>.</p>
                </div>
                <div>
                    <head>Poisson Graphical Lasso</head>

                    <p> For count data like ours the normality assumption may be inappropriate and a
                        modification of the above methods was developed by Allen and Liu for Poisson
                        graphical models, in which the relationships between nodes are defined by a
                        conditional Poisson distribution <ptr target="#allen2012"/>. For each node </p>

                    <figure>
                        <graphic url="resources/images/figure05.gif"/>
                    </figure>

                    <p> The Poisson Markov random field implied by this relationship is not amenable
                        to inferring network structures, as it requires θ<hi rend="subscript"
                            >jk</hi> ≤ 0 for all pairs {j,k} <ptr target="#allen2012"/>. We
                        therefore proceed as they did by estimating the local log-linear models</p>

                    <figure>
                        <graphic url="resources/images/figure06.gif"/>
                    </figure>

                    <p>and combine the implied local relationships into a network structure.</p>
                    <p> We can then view θ<hi rend="subscript">ij</hi> as a measure of relationship
                        strength between <hi rend="italic">i</hi> and <hi rend="italic">j</hi>. In
                        Allen and Liu, the model is fit using the Poisson Graphical Lasso – a
                        penalized regression method similar to the graphical lasso <ptr
                            target="#friedman2008"/>, but modified for count data. A penalized
                        Poisson regression is done for each node <hi rend="italic">j</hi>’s counts
                        on the rest. That is, for each node we solve the following: </p>

                    <figure>
                        <graphic url="resources/images/figure07.gif"/>
                    </figure>

                    <p> Here ϱ is a matrix of penalty parameters and ⋆ denotes component-wise
                        multiplication. An edge is determined to exist between nodes <hi
                            rend="italic">j</hi> and <hi rend="italic">k</hi> if <hi rend="italic"
                                >θ̂<hi rend="subscript">jk</hi></hi> &gt; 0 and/or <hi rend="italic"
                                >θ̂<hi rend="subscript">kj</hi></hi> &gt; 0. The tuning parameter ϱ
                        can be the same for all elements and can be chosen, for example, by
                        stability selection <ptr target="#meinshausen2010"/>. Later we will allow
                        elements of the ϱ matrix to differ.</p>
                </div>
                <div>
                    <head>Modifications</head>

                    <p> The motivating data for Allen and Liu are the RNA-sequencing measurements
                        from <hi rend="italic">p</hi> genes in <hi rend="italic">n</hi> experiments;
                        their goal is to determine which genes are <q>connected</q> to each other in
                        a metabolic process <ptr target="#allen2012"/>. Here we have the (noisy)
                        counts of <hi rend="italic">p</hi> names in <hi rend="italic">n</hi>
                        biographies; our goal is to determine which historical figures had
                        "connections" to each other in a variety of social contexts. Two modeling
                        considerations unique to this type of data and practical objective, which
                        lead us to slight modifications in method, are the variance of document
                        lengths and the irrelevance of negative edge estimates.</p>
                    <p> Documents in the ODNB vary greatly in length. People tend to have longer
                        biographies when biographers know more about them or have deemed them
                        historically significant. Allen and Liu note that it is important to
                        normalize the data to be approximately independent and identically
                        distributed Poisson random variables, since their model is sensitive to
                        deviations from this assumption <ptr target="#allen2012"/>. To achieve this,
                        we break the longer documents into 500 word sections and count each section
                        as an observation. This introduces weak dependence among some observations,
                        but the chronological nature of the documents may lessen this effect. That
                        is, the people mentioned in the first section of Bacon’s biography may be
                        very different from those mentioned in the last section. Being mentioned in
                        the same section of a document may also be greater evidence of a connection
                        than simply being mentioned in the same document.</p>
                    <p>As a preliminary test of this method, we calculate the Spearman correlation
                        between lists of relationships provided by humanities scholars and</p>
                    <list type="ordered">
                        <item>relationships produced by simple correlation</item>
                        <item>relationships produced by our model with document sectioning</item>
                        <item>relationships produced by our model without document sectioning</item>
                    </list>
                    <p>For our test set, simple correlation fails first, while those for our model –
                        with and without sectioning – remain similar. Sectioning fails to improve
                        correlation on some historical actors, but it leads to slight improvements
                        in correlation for others.</p>
                    <p> Furthermore, when fitting the model, a large fraction of <hi rend="italic"
                                >θ̂<hi rend="subscript">jk</hi></hi> values are negative. When this
                        coefficient is negative, it does not make sense to estimate a resulting
                        edge, since negative coefficients imply a negative relationship between the
                        counts of name <hi rend="italic">j</hi> and name <hi rend="italic">k</hi>.
                        Because any specific person appears only in a small portion of the
                        documents, and is presumably related to only a small fraction of all the
                        people in the network, fitting this model tends to produce a large amount of
                        negative coefficients compared to positive coefficients. </p>
                </div>
                <div>

                    <head>Confidence Estimate Procedure</head>

                    <p> We want to be able to attach a confidence estimate to all edges (which can
                        be used to rank connections), instead of just obtaining a yes or no estimate
                        for each potential edge. Let the matrix C represent a symmetric confidence
                        matrix (where each entry C<hi rend="subscript">jk</hi> = C<hi
                            rend="subscript">kj</hi> = confidence attached to edge existing between
                        person <hi rend="italic">j</hi> and <hi rend="italic">k</hi>). An informal
                        confidence estimate can be obtained by refitting the model many times on
                        random subsets of the data and computing the fraction of models in which a
                        specific edge is found in the model. </p>
                    <p> The method of estimating the final edge confidences is as follows: </p>
                    <list type="ordered">
                        <item>Sample half of the rows in the data matrix </item>
                        <item>Fit Poisson Graphical Lasso on this data as follows: <list
                                type="unordered">
                                <item>For each <hi rend="italic">j</hi> (column), fit the model in
                                    Equation 3, and obtain the coefficient estimates for ϱ=0.001 </item>
                                <item>Ignore any coefficient that has been estimated as negative
                                </item>
                            </list>
                        </item>
                        <item>Repeat steps (1) and (2) 100 times.</item>
                        <item>Estimate the confidence of an edge between node <title rend="italic"
                                >j</title> and <hi rend="italic">k</hi> as <figure>
                                <graphic url="resources/images/figure08.png"/>
                            </figure></item>
                    </list>
                    <p>Note that <hi rend="italic">θ̂<hi rend="superscript">(t)</hi><hi
                                rend="subscript">jk</hi></hi> is the estimate for the coefficient on
                        the t<hi rend="superscript">th</hi> repetition of the model fitting in Step
                        2. </p>

                    <p> There are a number of methods described in the literature for selecting the
                        tuning parameter ϱ. When the goal is prediction of the response variable,
                        cross-validation is a natural choice. When the goal is network inference –
                        specifically, we want to know whether each edge is <q>in</q> or <q>out</q> –
                        stability selection can be used instead, as is done in Allen and Liu <ptr
                            target="#meinshausen2010"/>
                        <ptr target="#allen2012"/>. Of most use to humanities scholars, however, is
                        an organization of the current knowledge about relationships. Some scholars
                        may wish to explore numerous potential relationships to one actor. Ordering
                        the relationships correctly – in order of likelihood – is therefore more
                        important than determining a cutoff point and excluding all edges that do
                        not make the cut. </p>
                    <p> Our confidence estimates for a specific value of ϱ correspond to a single
                        point on the stability paths mentioned in Meinshausen and Bühlmann <ptr
                            target="#meinshausen2010"/>. They note that the choice of range of ϱ to
                        use when computing the <q>stable variables</q> – or in this case, edges –
                        does not matter significantly. In our experiments with values of ϱ ranging
                        from 0.001 (many edges) to 100 (no edges), we also find the confidence
                        estimates tend to not vary too much for different reasonable values of ϱ,
                        where reasonable is defined as a low enough penalty such that variables are
                        actually added, but high enough penalty so that the algorithm converges
                        rapidly.</p>
                </div>
                <div>
                    <head>Name Disambiguation</head>
                    <p>Different people sometimes have the same names, and disambiguating them is
                        difficult. When name duplication only happens rarely, it may be feasible to
                        disambiguate manually. However, there are no less than twelve John Smith’s
                        and ten Archibald Campbell’s in our node set; overall nearly a thousand
                        names refer to multiple people. Furthermore, many of the people with these
                        names overlap in lifespans, including a large number of parents who gave
                        their own names to their children.</p>
                    <p>To process these duplicate names, we use a twofold method. First, we employ
                        chronological filters on all our potential relationship edges. Two people
                        cannot have a relationship if their lifespans did not overlap. We do,
                        however, allow a one-year margin of error so that posthumous children may
                        still have edges to their biological fathers. For people with unknown birth
                        and death dates, we allow a twenty-year span before and after their known
                        period of activity. For people for whom only a birth or a death date is
                        known, we allow for up to a 110-year lifespan, erring on the side of
                        inclusivity rather than exclusivity.</p>
                    <p>In the cases where there is chronological overlap in the lifespans of people
                        with duplicate names, we fall back on probabilities. If the name was
                        generated by NER, we evenly split the mentions among each of the people with
                        that name – that is, we assign them each an equal probability. However, if
                        our duplicates all have biographical entries, we assign each person a
                        probability based on the length of their biography. This serves as an
                        approximation of the relative frequency we expect each person to appear in
                        the overall ODNB, which we use to weight the mentions accordingly.</p>
                    <p>For example, Francis Walsingham, the principal secretary, has a biography
                        that is 30 times the length of Francis Walsingham, the Jesuit. Therefore we
                        argue a mention of Francis Walsingham in some other ODNB biography is 30
                        times more likely to refer to the former rather than the latter. To follow
                        this logic through, we would assign weights of 97% to the principal
                        secretary and 3% to the Jesuit. Yet we don’t want to obscure the
                        lesser-known Jesuit so thoroughly. Therefore, we cap the percentages at a
                        max/min of 75% and 25% so that someone with an extremely long biography
                        cannot dominate the probabilities completely. Thus in the period of overlap
                        between their two lifespans, 75% of the instances of <q>Francis
                            Walsingham</q> are attached to the principal secretary and 25% are
                        attached to the Jesuit. In practice, this does yield lower confidence
                        estimates and more false positives for <q>split-mention</q> nodes’
                        relationships, but we consider this an acceptable as a starting point for
                        further, manual curation.</p>
                </div>
                <div>
                    <head>Incorporating Humanist Knowledge</head>
                    <p>Prospectively, after enough humanists contribute their expert knowledge to
                        the network via our crowd-sourcing website, it will be possible to use their
                        contributions to refine our inference model by making local changes to the
                        penalty parameter. We could do this by allowing the penalty matrix, ϱ, to
                        vary for different relationships. If our experts confirm a relationship
                        between actors <hi rend="italic">j</hi> and <hi rend="italic">k</hi>, we set
                            ϱ<hi rend="subscript">jk</hi>=ϱ<hi rend="subscript">kj</hi>=0, which
                        usually ensures that <hi rend="italic">θ̂<hi rend="subscript">jk</hi></hi>,
                            <hi rend="italic">θ̂<hi rend="subscript">kj</hi></hi> ≠ 0. Similarly,
                        for a confirmed non-relationship – that two figures are not connected – we
                        would set ϱ<hi rend="subscript">jk</hi>=ϱ<hi rend="subscript">kj</hi>=∞,
                        ensuring <hi rend="italic">θ̂<hi rend="subscript">jk</hi></hi>, <hi
                            rend="italic">θ̂<hi rend="subscript">kj</hi></hi> = 0. As more experts
                        label more potential relationships, we could continue to refine our model
                        iteratively. By helping us to tune the penalty parameter, the contributions
                        of (say) a hundred experts could help us to assess millions of relations
                        more accurately.</p>
                </div>
                <div>
                    <head>Network Code</head>
                    <p>Further information on how we generated our network can be found, along with
                        our R code, at: <ref target="https://github.com/sdfb/sdfb_network"
                            >https://github.com/sdfb/sdfb_network</ref><note>Research for this
                            article was supported by a grant from the Council for Library and
                            Information Resources Award (Con_505), by Google Faculty Awards
                            2012_R1_189 and 2013_R1_26, and by a Falk Fellowship from Carnegie
                            Mellon University’s Dietrich College of Humanities and Social
                            Sciences.</note></p>
                </div>
            </div>




        </body>
        <back>
            <listBibl>

                <bibl label="Ahnert and Ahnert 2014" xml:id="ahnert2014">Ahnert, Ruth, and S.E.
                    Ahnert. <title rend="quotes">A Community Under Attack: Protestant Letter
                        Networks in the Reign of Mary I.</title>
                    <title rend="italic">Leonardo</title> 47, no. 3 (2014): 275–275.
                    doi:10.1162/LEON_a_00778.</bibl>
                <bibl label="Ahnert and Ahnert 2015" xml:id="ahnert2015">Ahnert, Ruth, and S.E.
                    Ahnert. <title rend="quotes">Protestant Letter Networks in the Reign of Mary I:
                        A Quantitative Approach.</title>
                    <title rend="italic">ELH</title> 82, no. 1 (2015): 1-33.</bibl>
                <bibl label="Alias-i 2008" xml:id="alias2008">LingPipe 4.1.0. <ref
                        target="http://alias-i.com/lingpipe"
                    >http://alias-i.com/lingpipe</ref></bibl>
                <bibl label="Allen and Liu 2012" xml:id="allen2012">Allen, G.I. and Z. Liu. <title
                        rend="quotes">A Log-Linear Graphical Model for Inferring Genetic Networks
                        from High-Throughput Sequencing Data.</title>
                    <title rend="italic">ArXiv e-prints</title> (2012).</bibl>
                <bibl label="Basu et al. 2015" xml:id="basu2015">Basu, Anupam, Jonathan Hope, and
                    Michael Witmore. <title rend="quotes">Networks and Communities in the Early
                        Modern Theatre.</title> In Roger Sell and Anthony Johnson, eds., <title
                        rend="italic">Community-making in Early Stuart Theatres: Stage and
                        Audience</title>. Ashgate (forthcoming). <ref
                        target="http://winedarksea.org/wp-content/uploads/2014/08/WH7-Networks-and-Communities.pdf"
                        >http://winedarksea.org/wp-content/uploads/2014/08/WH7-Networks-and-Communities.pdf</ref></bibl>
                <bibl label="Bearman et al. 2002" xml:id="bearman2002">Bearman, Peter, James Moody,
                    and Robert Faris. <title rend="quotes">Networks and History.</title>
                    <title rend="italic">Complexity</title> 8, no. 1 (2002): 61–71.
                    doi:10.1002/cplx.10054.</bibl>
                <bibl label="Blei et al. 2003" xml:id="blei2003">Blei, David M., Andrew Y. Ng, and
                    Michael I. Jordan, <title rend="quotes">Latent Dirichlet Allocation.</title>
                    <title rend="italic">Journal of Machine Learning Research</title> 3 (2003):
                    993-1022.</bibl>
                <bibl label="Bosch and Camp 2011" xml:id="bosch2011">Bosch, A. and M. Camp. <title
                        rend="quotes">A Link to the Past: Constructing Historical Social
                        Networks.</title> In <title rend="italic">The Proceedings of the Association
                        for Computational Linguistics Workshop on Computational Approaches to
                        Subjectivity and Sentiment Analysis</title> (2011): 61–69.</bibl>
                <bibl label="Bosch and Camp 2012" xml:id="bosch2012">Bosch, Antal van den and Matje
                    van de Camp. <title rend="quotes">The socialist network.</title>
                    <title rend="italic">Decision Support Systems</title> 53, no. 4 (2012): 761-69.
                        <ref target="http://dx.doi.org/10.1016/j.dss.2012.05.031"
                        >doi:10.1016/j.dss.2012.05.031</ref>.</bibl>
                <bibl label="Campbell 2004" xml:id="campbell2004">Campbell, Gordon. <title
                        rend="quotes">Milton, John (1608-1674), poet and polemicist.</title> In
                    Matthew, H.C.G. and Brian Harrison (eds). <title rend="italic">Oxford Dictionary
                        of National Biography</title>. Oxford University Press, Oxford (2004);
                    online edn, (2007).</bibl>

                <bibl label="Collini 2005" xml:id="collini2005">Collini, Stefan. <title
                        rend="quotes">Our Island Story,</title><title rend="italic"> London Review
                        of Books,</title> 27.2 (2005): 3-8.</bibl>
                <bibl label="Durrett and Klein 2014" xml:id="durrett2014">Durret, Greg and Dan
                    Klein. <title rend="quotes">A Joint Model for Entity Analysis: Coreference,
                        Typing, and Linking</title> (2014). <ref
                        target="http://www.eecs.berkeley.edu/~gdurrett/papers/durrett-klein-tacl2014.pdf"
                        >http://www.eecs.berkeley.edu/~gdurrett/papers/durrett-klein-tacl2014.pdf</ref></bibl>
                <bibl label="During 2015" xml:id="during2015"> Düring, Marten, <title rend="italic"
                        >Historical Network Research</title>. <ref
                        target="http://historicalnetworkresearch.org/"
                        >http://historicalnetworkresearch.org</ref>.</bibl>
                <bibl label="Elson et al. 2010" xml:id="elson2010">Elson, David K., Nicholas Dames,
                    and Kathleen R. McKeown. <title rend="quotes">Extracting Social Networks from
                        Literary Fiction,</title>
                    <title rend="italic">Proceedings of the 48th Annual Meeting of the Association
                        for Computational Linguistics</title> (2010): 138-147. <ref
                        target="https://www.aclweb.org/anthology/P/P10/P10-1015.pdf"
                        >https://www.aclweb.org/anthology/P/P10/P10-1015.pdf</ref></bibl>
                <bibl label="Evans 2011" xml:id="evans2011">Evans, James A. and Jacob G. Goster.
                        <title rend="quotes">Metaknowledge,</title>
                    <title rend="italic">Science</title>, 331.6018 (2011): 721-725.</bibl>
                <bibl label="Feinerer et al. 2008" xml:id="feinerer2008">Feinerer, I., K. Hornik,
                    and D. Meyer. <title rend="quotes">Text Mining Infrastructure in R,</title>
                    <title rend="italic">Journal of Statistical Software</title> 25 (2008): 1-54.
                        <ref target="http://www.jstatsoft.org/v25/i05/paper"
                        >http://www.jstatsoft.org/v25/i05/paper</ref></bibl>
                <bibl label="Finkel et al. 2005" xml:id="finkel2005">Finkel, J.R., T. Grenager, and
                    C. Manning. <title rend="quotes">Incorporating Non-local Information into
                        Information Extraction Systems by Gibbs Sampling.</title> In <title
                        rend="italic">Proceedings of the 43rd Annual Meeting of the Association for
                        Computational Linguistics</title> (2005): 363-370. <ref
                        target="http://nlp.standford.edu/manning/papers/gibbscrf3.pdf"
                        >http://nlp.standford.edu/manning/papers/gibbscrf3.pdf</ref></bibl>
                <bibl label="Friedman et al. 2008" xml:id="friedman2008">Friedman, J., T. Hastie,
                    and R. Tibshirani. <title rend="quotes">Sparse inverse covariance estimation
                        with the graphical lasso.</title>
                    <title rend="italic">Biostatistics</title> 9 (2008): 432-441.</bibl>
                <bibl label="Gloor et al. 2015" xml:id="gloor2015">Gloor, Peter, Patrick De Boer,
                    Wei Lo, Stefan Wagner, Keiichi Nemoto, and Hauke Fuehres. <title rend="quotes"
                        >Cultural Anthropology Through the Lens of Wikipedia - A Comparison of
                        Historical Leadership Networks in the English, Chinese, Japanese and German
                        Wikipedia.</title> arXiv:1502.05256 [cs], February 18, 2015. <ref
                        target="http://arxiv.org/abs/1502.05256"
                        >http://arxiv.org/abs/1502.05256</ref>.</bibl>
                <bibl label="Glymour et al. 2001" xml:id="glymour2001">Glymour, Clark, Richard
                    Scheines, and Peter Spirtes. <title rend="italic">Causation, Prediction, and
                        Search, 2nd Ed.</title> MIT Press, Cambridge Mass. (2001).</bibl>
                <bibl label="Goldfarb et al. 2013" xml:id="goldfarb2013">Goldfarb, Doron, Max
                    Arends, Josef Froschauer, and Dieter Merkl. <title rend="quotes">Comparing Art
                        Historical Networks.</title>
                    <title rend="italic">Leonardo</title> 46, no. 3 (2013): 279–279.
                    doi:10.1162/LEON_a_00575.</bibl>
                <bibl label="Hassan et al. 2012" xml:id="hassan2012">Hassan, Ahmed, Amjad Abu-Jbara,
                    and Dragomir Radev, <title rend="quotes">Extracting Signed Social Networks From
                        Text.</title>
                    <title rend="italic">Proceedings of the TextGraphs-7 Workshop</title> (2012):
                    6-14. <ref target="http://www.aclweb.org/anthology/W12-4102"
                        >http://www.aclweb.org/anthology/W12-4102</ref></bibl>
                <bibl label="Lauritzen 1996" xml:id="lauritzen1996">Lauritzen, S.L. <title
                        rend="italic"> Graphical Models.</title> Oxford Statistical Science Series
                    17. The Clarendon Press Oxford University Press, New York (1996).</bibl>
                <bibl label="Long and So 2013" xml:id="long2013">Long, Hoyt, and Richard So. <title
                        rend="quotes">Network Science and Literary History.</title>
                    <title rend="italic">Leonardo</title> 46, no. 3 (2013): 274–274.
                    doi:10.1162/LEON_a_00570.</bibl>
                <bibl label="Makazhanov et al. 2014" xml:id="makazhanov2014">Makazhanov, Aibek,
                    Denilson Barbosa, and Grzegorz Kondrak. <title rend="quotes">Extracting Family
                        Relationship Networks from Novels.</title> arXiv:1405.0603 [cs.CL].</bibl>
                <bibl label="Matthew and Harrison 2004" xml:id="matthew2004">Matthew, H.C.G. and
                    Brian Harrison (eds). <title rend="italic">Oxford Dictionary of National
                        Biography</title>. Oxford University Press, Oxford (2004); online edn,
                    (2007).</bibl>
                <bibl label="McGann 2014" xml:id="mcgann2014">McGann, Jerome. <title rend="italic">A
                        New Republic of Letters: Memory and Scholarship in the Age of Digital
                        Reproduction</title>. Cambridge, Massachusetts: Harvard University Press,
                    2014.</bibl>
                <bibl label="McPherson, Smith-Love and Cook 2001" xml:id="mcpherson2001">McPherson,
                    Miller, Lynn Smith-Lovin and James M. Cook. <title rend="quotes">Birds of a
                        Feather: Homophily in Social Networks.</title>
                    <title rend="italic">Annual Review of Sociology</title> 27 (2001):
                    415-444.</bibl>
                <bibl label="Meeks and Weingart 2012" xml:id="meeks2012">Elijah Meeks and Scott
                    Weingart, eds., <title rend="italic">Journal of Digital Humanities</title> 2.1
                    (2012), <title rend="quotes">Topic Modeling</title> special issue. </bibl>
                <bibl label="Meinshausen and Bühlmann 2006" xml:id="meinshausen2006">Meinshausen, N.
                    and P. Bühlmann. <title rend="quotes">High-dimensional graphs and variable
                        selection with the lasso.</title>
                    <title rend="italic">Annals of Statistics</title> 34 (2006): 1436-1462.</bibl>
                <bibl label="Meinshausen and Bühlmann 2010" xml:id="meinshausen2010">Meinshausen, N.
                    and P. Bühlmann. <title rend="quotes">Stability selection.</title>
                    <title rend="italic">Journal of the Royal Statistical Society: Series B
                        (Statistical Methodology)</title> 72 (2010): 417-473.</bibl>
                <bibl label="Moretti 2011" xml:id="moretti2011">Moretti, Franco. <title
                        rend="quotes">Network Theory, Plot Analysis.</title>
                    <title rend="italic">New Left Review</title> 68 (2011): 80–102.</bibl>
                <bibl label="Otis 2014a" xml:id="otis2014a">Otis, Jessica. <title rend="quotes"
                        >What’s in a Name? The Many Nodes of King James VI and I.</title>
                    <title rend="italic">Six Degrees of Francis Bacon: Reassembling the Early Modern
                        Social Network</title>, Sept. 16, 2014. <ref
                        target="http://6dfb.tumblr.com/post/97645842306/"
                        >http://6dfb.tumblr.com/post/97645842306/</ref></bibl>
                <bibl label="Otis 2014b" xml:id="otis2014b">Otis, Jessica. <title rend="quotes"
                        >Tales from the Raw NER Data.</title>
                    <title rend="italic">Six Degrees of Francis Bacon: Reassembling the Early Modern
                        Social Network</title>, Oct/Nov 2014. <ref
                        target="http://6dfb.tumblr.com/tagged/tales-from-the-raw-ner-data"
                        >http://6dfb.tumblr.com/tagged/tales-from-the-raw-ner-data</ref></bibl>
                <bibl label="Peltonen 2004" xml:id="peltonen2004">Peltonen, Markku. <title
                        rend="quotes">Bacon, Francis, Viscount St Alban (1561-1626), lord
                        chancellor, politician, and philosopher</title>. In Matthew, H.C.G. and
                    Brian Harrison (eds). <title rend="italic">Oxford Dictionary of National
                        Biography</title>. Oxford University Press, Oxford (2004); online edn,
                    (2007).</bibl>
                <bibl label="Porter 1980" xml:id="porter1980">Porter, M.F. <title rend="quotes">An
                        algorithm for suffix stripping.</title>
                    <title rend="italic">Program</title>, 14(3) (1980): 130−137. </bibl>
                <bibl label="Riddell 2014" xml:id="riddell2014">Riddell, A. <title rend="quotes">How
                        to Read 22,198 Journal Articles: Studying the History of German Studies with
                        Topic Models.</title> In <title rend="italic">Distant Readings: Topologies
                        of German Culture in the Long Nineteenth Century</title>, edited by Matt
                    Erlin and Lynne Tatlock, 91--114. Rochester, New York: Camden House,
                    2014.</bibl>
                <bibl label="Schich et al. 2014" xml:id="schich2014">Schich, Maximilian, Chaoming
                    Song, Yong-Yeol Ahn, Alexander Mirsky, Mauro Martino, Albert-László Barabási,
                    and Dirk Helbing. <title rend="quotes">A Network Framework of Cultural
                        History.</title>
                    <title rend="italic">Science</title> 345, no. 6196 (August 1, 2014): 558–62.
                    doi:10.1126/science.1240064.</bibl>
                <bibl label="Seccombe and Kelsey 2014" xml:id="seccombe2014">Seccombe, Thomas and
                    Sean Kelsey. <title rend="quotes">Leigh, Francis, first earl of Chichester (d.
                        1653), politician and courtier</title>. In Matthew, H.C.G. and Brian
                    Harrison (eds). <title rend="italic">Oxford Dictionary of National
                        Biography</title>. Oxford University Press, Oxford (2004); online edn,
                    (2007).</bibl>
                <bibl label="Simmel 1950" xml:id="simmel1950">Simmel, Georg. <title rend="italic"
                        >The Sociology of Georg Simmel</title>. Translated by Kurt H. Wolff. Simon
                    and Schuster, 1950.</bibl>
                <bibl label="Smith et al. 2014" xml:id="smith2014">Smith, D.A., R. Cordell, E.M.
                    Dillon, N. Stramp, and J. Wilkerson. <title rend="quotes">Detecting and Modeling
                        Local Text Reuse.</title> In <title rend="italic">2014 IEEE/ACM Joint
                        Conference on Digital Libraries (JCDL)</title>, 183–92, 2014.
                    doi:10.1109/JCDL.2014.6970166.</bibl>
                <bibl label="Underwood et al. 2013" xml:id="underwood2013">Underwood, Ted, Michael
                    L. Black, Loretta Auvil, and Boris Capitanu. <title rend="quotes">Mapping
                        Mutable Genres in Structurally Complex Volumes.</title>
                    <title rend="italic">arXiv:1309.3323 [cs]</title>, September 12, 2013. <ref
                        target="http://arxiv.org/abs/1309.3323"
                    >http://arxiv.org/abs/1309.3323</ref>.</bibl>
                <bibl label="Weingart 2012" xml:id="weingart2012">Weingart, Scott. <title
                        rend="quotes">Topic Modeling for Humanists: A Guided Tour.</title>
                    <title rend="italic">The Scottbot Irregular</title>, July 25, 2012. <ref
                        target="http://www.scottbot.net/HIAL/?p=19113"
                        >http://www.scottbot.net/HIAL/?p=19113</ref></bibl>
                <bibl label="Weingart 2015" xml:id="weingart2015">Weingart, Scott. <title
                        rend="quotes">Culturomics 2: The Search for More Money.</title>
                    <title rend="italic">The Scottbot Irregular</title>, March 5, 2015. <ref
                        target="http://www.scottbot.net/HIAL/?p=41200"
                        >http://www.scottbot.net/HIAL/?p=41200</ref>.</bibl>



            </listBibl>

        </back>
    </text>
</TEI>
