<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            	
            <div class="DHQheader">
               		
               			
               				
               				
               <h1 class="articleTitle lang en">Project Quintessence: Examining Textual
                  					Dimensionality with a Dynamic Corpus
                  					Explorer</h1>
               				
               				
               <div class="author"><span style="color: grey">Samuel Pizelo</span> &lt;<a href="mailto:spizelo_at_ucdavis_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('spizelo_at_ucdavis_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('spizelo_at_ucdavis_dot_edu'); return false;">spizelo_at_ucdavis_dot_edu</a>&gt;, UC Davis</div>
               				
               <div class="author"><span style="color: grey">Arthur Koehl</span> &lt;<a href="mailto:avkoehl_at_ucdavis_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('avkoehl_at_ucdavis_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('avkoehl_at_ucdavis_dot_edu'); return false;">avkoehl_at_ucdavis_dot_edu</a>&gt;, UC Davis</div>
               				
               <div class="author"><span style="color: grey">Chandni Nagda</span> &lt;<a href="mailto:cjnagda2_at_illinois_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('cjnagda2_at_illinois_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('cjnagda2_at_illinois_dot_edu'); return false;">cjnagda2_at_illinois_dot_edu</a>&gt;, University of Illinois at Urbana-Champaign</div>
               				
               <div class="author"><span style="color: grey">Carl Stahmer</span> &lt;<a href="mailto:cstahmer_at_ucdavis_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('cstahmer_at_ucdavis_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('cstahmer_at_ucdavis_dot_edu'); return false;">cstahmer_at_ucdavis_dot_edu</a>&gt;, UC Davis</div>
               			
               			
               			
               		
               		
               		
               		
               	<span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Project%20Quintessence%3A%20Examining%20Textual%20Dimensionality%20with%20a%20Dynamic%20Corpus%20Explorer&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Pizelo&amp;rft.aufirst=Samuel&amp;rft.au=Samuel%20Pizelo&amp;rft.au=Arthur%20Koehl&amp;rft.au=Chandni%20Nagda&amp;rft.au=Carl%20Stahmer"> </span></div>
            	
            <div id="DHQtext">
               		
               			
               <div id="abstract">
                  <h2>Abstract</h2>
                  				
                  				
                  <p>In this paper, we present a free and open-access web tool for exploring the
                     					EEBO-TCP early modern English corpus. Our tool combines several unsupervised
                     					computational techniques into a coherent exploratory framework that allows for
                     					textual analysis at a variety of scales. Through this tool, we hope to integrate
                     					close-reading and corpus-wide analysis with the wider scope that computational
                     					analysis affords. This integration, we argue, allows for an augmentation of both
                     					methods: contextualizing close reading practices within historically- and
                     					regionally-specific word usage and semantics, on the one hand, and concretizing
                     					thematic and statistical trends by locating them at the textual level, on the
                     					other. We articulate a design principle of <em class="term">textual dimensionality</em>
                     					or approximating through visualization the abstract relationships between words
                     					in any text. We argue that <cite class="title italic">Project Quintessence</cite>
                     					represents a method for researchers to navigate archives at a variety of scales
                     					by helping to visualize the many latent dimensions present in texts.</p>
                  			</div>
               			
               		
               		
               			
               <div class="div div0">
                  				
                  <h1 class="head">INTRODUCTION<a class="noteRef" href="#d3e291">[1]</a> <a class="noteRef" href="#d3e293">[2]</a></h1>
                  				
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">While the use of computational methods in corpus-level analysis has become
                     					ubiquitous in recent years, the optimal integration of these tools into digital
                     					archiving and corpus presentation remains an open question. Massive archives
                     					such as JSTOR, Gale, and Google Books have experimented with computational tools
                     					for traversing and exploring underlying documents in their collections.<a class="noteRef" href="#d3e304">[3]</a> Meanwhile, progress in machine
                     					learning (ML) and natural language processing (NLP) continues to advance the
                     					state-of-the-art for the underlying computational tools and methods these
                     					archives employ. However, there is a critical disjuncture between the datasets
                     					these latter tools and methods are trained upon and the real-world needs and
                     					expectations of digital archives [<a class="ref" href="#jo_etal2020">Jo and Gebru 2020</a>] [<a class="ref" href="#birhane_etal2022">Birhane et al 2022</a>]. The application of computational methods to
                     					archival research thus risks both misrepresenting the findings of models and
                     					techniques, on the one hand, and misconstruing the exigencies of the underlying
                     					archive on the other. These concerns have resulted in a trend toward
                     					out-of-the-box computational tools that are not well integrated with the
                     					underlying archive and often obscure rather than explain interesting phenomena.
                     					Computational tools in archival analysis are thus frequently designed,
                     					implemented, and presented as abstractions of the archive. As such, these tools
                     					rarely serve as more than a heuristic method of inquiry, failing to meet the
                     					higher expectations of either computational humanities scholarship or humanistic
                     					textual inquiry.</div>
                  				
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">We designed <cite class="title italic">Project Quintessence</cite> with these concerns
                     					in mind. By bringing together a team of scholars intimately acquainted both with
                     					computational analysis in NLP and with the texts and historical context of our
                     					archive, this project has attempted to build a corpus exploration tool from the
                     					ground up that fully integrates computational tools and methods with the
                     					specific archive in question. For our corpus, we chose the Early English Books
                     					Online Text Creation Partnership (EEBO-TCP) archive, a collection of 60,331
                     					texts transcribed and tagged with TEI markup by the TCP team. Each model we
                     					trained and technique we incorporated was designed to take advantage of this
                     					highly idiosyncratic archive. Rather than decoupling textual analysis from the
                     					texts themselves, we incorporated these tools into the very architecture of the
                     					site. The result is a tool that allows one to dynamically interact with the
                     					archive in a way that is not possible with any one method independently, and
                     					which we believe presents a new model for archival research and analysis. By
                     					presenting multiple, complementary tools side-by-side for use by domain experts,
                     					we allow for the tools to be checked against each other and also compared with
                     					the underlying archive. We believe that this framework provides a stable site
                     					for the continual improvement of knowledge about the archive and domain
                     					alike.</div>
                  				
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">In this paper, we first contextualize <cite class="title italic">Project
                        					Quintessence</cite> within a longer trajectory of digital archival research
                     					tools, and then describe how the various elements of the project cohere by
                     					employing our concept of <em class="term">dimensionality</em>. We continue this
                     					discussion by outlining the project pipeline, including model training
                     					parameters and other considerations for each of the models and tools included.
                     					Finally, we explore some potential future directions for our project and for
                     					database visualization frameworks in general. We argue that an attention to
                     					<em class="term">textual dimensionality</em> allows for an interdependent database
                     					framework that augments research at any scale of analysis and has the capacity
                     					to both to confirm the doxa of pre-existing research while also providing
                     					surprising results that enrich the body of knowledge in the field. We believe
                     					that our web tool accomplishes all of these charges.</div>
                  				
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4"><cite class="title italic">Project Quintessence</cite> is built using
                     					well-established, highly nimble modeling techniques; primarily Latent Dirichlet
                     					Analysis (LDA) Topic Modeling and Word2Vec Word Embeddings. By leveraging
                     					dynamic visualization libraries such as Plotly, we were able to extract a high
                     					level of archival information from each of our models. Each dynamic
                     					visualization on the site can accommodate any number of research questions or
                     					approaches. In addition, by synthesizing approaches from multiple techniques
                     on
                     					the same archive, we were able to harness the affordances of each method rather
                     					than attempting to answer all questions with a single approach. We describe
                     					below how these methods differ from more contemporary techniques, such as
                     					Structural Topic Models (STM), Dynamic Topic Models (DTM), Document Embeddings,
                     					Contextual Embeddings (CoVe), Transformer Embeddings (such as BERT), and Stacked
                     					Embeddings (such as Flair); most of which we used at different stages of our
                     					design process to supplement our findings and enrich our analysis. Overall, the
                     					trend toward large language models and massive amounts of masked perimeters has
                     					introduced elements of bias and structuring assumptions which do not hold true
                     					for all datasets, disguising crucial inferences the model makes based on its
                     					training data rather than the object being interpreted [<a class="ref" href="#bender_etal2021">Bender et al 2021</a>] [<a class="ref" href="#chun2008">Chun 2008</a>]. By incorporating
                     					historical lexicographic tools like MorphAdorner into our text cleaning pipeline
                     					and generating many smaller language models on specific cross-sections of the
                     					archive, we were able to preserve germane distinctions within the archive while
                     					still providing a mode of ingress into its analysis.</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">RELATED WORK</h1>
                  				
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">The use of visualizations in representing digital archives has become more
                     					nuanced with the recent increase in critical attention to these methods. Most
                     of
                     					the early literature on data visualization in the Digital Humanities has
                     					remarked on the late adoption of visualization techniques by researchers in the
                     					humanities, a trend that appears to be justified when examining naive approaches
                     					to what visualizations can and do tell us of the underlying data, and the extent
                     					to which that lens is mediated and controlled by its designers. However, as
                     					media studies and digital humanities scholarship have produced compelling
                     					critiques of the unmediated view of visualization, alongside compelling design
                     					objectives for researchers and engineers, dynamic visualizations are
                     					increasingly seen as a site for emancipatory politics and critical inquiry.</div>
                  				
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">To that end, we have framed our project’s intervention as one of harnessing new
                     					technological advances in ways that opens rather than closes archives to
                     					countervailing narratives and critique of canon. In our design practices, we
                     					asked how digital methods might appropriately interact with digital textual
                     					archives. Michael Witmore has aptly described this relationship in terms of
                     					addressability, and describes archives of texts as digital objects that are,
                     					“massively addressable at different levels of
                     					scale” [<a class="ref" href="#whitmore2010">Whitmore 2010</a>]. We suggest that the
                     					oscillations of scale afforded by dynamic exploration tools could be best
                     					considered by thinking in terms of dimensionality. As described in David J.
                     					Staley’s influential account, the capacity for visualization techniques to
                     					augment the affordances of text can be understood of as an increase in
                     					dimensionality [<a class="ref" href="#staley2015">Staley 2015</a>]. Staley frames visualization as a way
                     					of breaking out of the implied linear one-dimensionality of linguistic syntax.
                     					However, we can expand upon this account. As Peter Gärdenfors has suggested,
                     					treating semantic meaning as a conceptual space with abstract, dimensional
                     					relations allows for complex semantic representation and analysis [<a class="ref" href="#gardenfors2014">Gärdenfors 2014</a>]. Word embeddings, for instance, are commonly employed
                     					to locate clusters of semantically similar words by assigning spatial
                     					coordinates to those words based on hundreds of dimensions of training data.
                     					Following Gärdenfors, we do not treat dimensionality rigidly as a geometrical
                     					principle, contrasted with hierarchical or ordinal relationships, but instead
                     as
                     					a higher-level abstract relationality of terms. By allowing ourselves to depart
                     					from three-dimensional thinking, dynamic visualizations are freed from their
                     					metaphorical relationship with abstract, Cartesian space, and instead function
                     					as tools for making tangible the many relationships between words with startling
                     					complexity.</div>
                  				
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Over the course of our project, we have increasingly become informed by this
                     					dimensional thinking. Describing syntactic, semantic, and thematic relationships
                     					in texts as dimensional relationships insists on the reality of those
                     					relationships. By virtue of the relational character of meaning-making,
                     					individual uses of words are necessarily and inevitably put into relationship
                     					with other words. While any computational attempt at measuring or visualizing
                     					those relationships is necessarily an approximation of varying validity, these
                     					computational tools are nevertheless measuring real relationships. Thus, we
                     					argue that maximizing researcher access to <em class="term">textual dimensionality</em>
                     					should be a primary objective for database design and visualization. This
                     					emphasis on dimensionality has been influenced by many earlier digital projects
                     					that we will discuss in three groupings; large scale digital archives,
                     					small-scale digital projects, and pre-existing work on the EEBO-TCP corpus. Our
                     					objective was to design <cite class="title italic">Project Quintessence</cite> as a
                     					coherent visualization tool that incorporated lessons from each of these
                     					categories.</div>
                  				
                  <div class="div div1">
                     					
                     <h2 class="head">Large-Scale Digital Archives </h2>
                     					
                     <div class="counter"><a href="#p8">8</a></div>
                     <div class="ptext" id="p8">The first strain of projects that influenced our approach are digital
                        						archives that are sufficiently large that they cannot be grouped
                        						thematically and are not housed within a single physical location. With the
                        						release of increasingly massive language models in recent years, such as
                        						OpenAI’s GPT-3 or Google’s PaLM, the trend in data analysis has undeniably
                        						moved toward the analysis of larger and larger quantities of data with less
                        						concern for text processing, archive curation, or corpus-dependent
                        						techniques [<a class="ref" href="#gebru_etal2021">Gebru et al 2021</a>]. This trend is most evident in
                        						tools provided by Google for the analysis of the Google Books archive,
                        						comprised of five million books published between 1800-2000, or over half a
                        						trillion tokens. Yet perversely, the astounding breadth of materials both
                        						risks misrepresenting the actual published content during that period, while
                        						also serving as a barrier to more nuanced NLP techniques [<a class="ref" href="#pechenick_etal2015">Pechenick et al 2015</a>] [<a class="ref" href="#schmidt_etal2021">Schmidt et al 2021</a>]. This trend
                        						is also evident to a lesser degree with recent archival exploration tools,
                        						such as the Digital Scholars Lab produced by Gale and JSTOR’s Text Analyzer
                        						beta tool. While both of these methods effectively incorporate computational
                        						methods in ways that enhance search functionality and research inquiry, in
                        						both cases modeling techniques are utilized with minimal control over
                        						parameters and do not change to fit the limitations and possibilities of the
                        						underlying archive. Thus, neither tool is intended for or achieves the rigor
                        						of computational humanities scholarship.</div>
                     				</div>
                  				
                  <div class="div div1">
                     					
                     <h2 class="head">Small-Scale Digital Projects </h2>
                     					
                     <div class="counter"><a href="#p9">9</a></div>
                     <div class="ptext" id="p9">While <cite class="title italic">Project Quintessence</cite> houses an archive of
                        						over sixty-thousand texts — over two billion tokens — it shares more in
                        						common thematically with smaller digital archives that aim for a higher
                        						level of research efficacy than the large-scale archives. These projects —
                        						often supported by research, institutions, and scholars of the digital and
                        						computational humanities — tend to emphasize curatorial control,
                        						transcription accuracy, representative sampling or strong thematic coherence
                        						of archives, and descriptive visualization techniques. Some examples of this
                        						style that were instructive to our work were Andrew Goldstone’s
                        						“dfr-browser,” the JeSeMe Semantic Explorer project, and the English
                        						Broadside Ballad Archive (EBBA). Each of these projects emphasize different
                        						aspects of the dynamic exploratory archive model that we believe are all
                        						synthesized in <cite class="title italic">Quintessence</cite>.</div>
                     				</div>
                  				
                  <div class="div div1">
                     					
                     <h2 class="head">Pre-Existing Work on EEBO-TCP</h2>
                     					
                     <div class="counter"><a href="#p10">10</a></div>
                     <div class="ptext" id="p10">The third strain of influence upon our work has been projects specific to the
                        						Early English Books Online (EEBO) and EEBO Text Creation Partnership
                        						(EEBO-TCP) archives. As a digital archive, EEBO grew from the microfilm
                        						series of Early English Books that began in the 1930s, but has since grown
                        						to encompass over 146,000 texts [<a class="ref" href="#mccollough_etal2019">McCollough and Lesser 2019</a>]. Of
                        						these 146,000 texts, 60,331 have been transcribed using TEI markup through
                        						the EEBO-TCP project. EEBO is currently housed within the ProQuest digital
                        						archives, which has maintained the general emphasis on facsimile
                        						reproductions of physical items in the archive, and a limited boolean and
                        						keyword search functionality.</div>
                     					
                     <div class="counter"><a href="#p11">11</a></div>
                     <div class="ptext" id="p11">The greatest improvement on this current presentation of the archives has
                        						been undertaken in a joint project by Northwestern University and Washington
                        						University in St. Louis called Early Print. This archival project boasts
                        						both improvements to the EEBO-TCP transcriptions through inbuilt
                        						crowdsourcing functionality, along with a number of fascinating
                        						visualizations and experimentation with computational modeling to aid search
                        						and retrieval. Early Print represents an exciting development in EEBO-TCP
                        						digital projects, and has the most in common with <cite class="title italic">Project Quintessence</cite> of the projects discussed in this section.
                        						Nonetheless, we feel that there remains a need for a generalizable framework
                        						that more fully integrates texts with their many contexts through
                        						computational tools. We also aim to better synthesize many pre-existing
                        						tools and visualizations into a single extensible research apparatus. As
                        						such, our emphasis is on providing a tool of sufficient rigor that its
                        						results can be relied upon for digital humanities research in the EEBO-TCP
                        						archive.</div>
                     				</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">DIMENSIONALITY AT SCALE</h1>
                  				
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Recent work in the digital humanities has rightly critiqued the over-reliance on
                     					spatial metaphors of analysis through terms like “close-reading” or
                     					“distant-reading” [<a class="ref" href="#da2019">Da 2019</a>] [<a class="ref" href="#bode2018">Bode 2018</a>].
                     					Project <cite class="title italic">Quintessence</cite> fully incorporates these
                     					critiques into its design. As such, conventional notions of scale collapse
                     					entirely when using our web tool for textual analysis: at the same time as a
                     					researcher is engaging in a <em class="term">close reading</em> of a text, they can
                     					inspect the semantic associations of any word in the document with other
                     					documents written by that author, in that decade, or printed at that print
                     					location. At the same time as one collects a personal archive of documents using
                     					our subsetting tool, one can also inspect the thematic topic distribution of
                     					that archive, and compare it to the distribution of topics in the corpus as a
                     					whole. These authorial, temporal, spatial, and thematic contexts allow for a
                     					renegotiation of how scale operates in textual analysis. Rather than having to
                     					choose between the closeness of a “human scale” or the distance of a
                     					“computational scale,” <cite class="title italic">Quintessence</cite> instead
                     					encourages a multiscalar view of the corpus by emphasizing different units of
                     					analysis — word, document, or topic — each providing a unique approach to
                     					exploring the entire archive.</div>
                  				
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">In summary, <cite class="title italic">Project Quintessence</cite> de-emphasizes
                     					discussion of <em class="emph">scale</em> that predominate in digital archival contexts,
                     					favoring instead a focus on <em class="emph">dimensionality</em> <span class="hi italic">.</span>
                     					Prompted by the computational tools incorporated into our project, we describe
                     					token-, document-, and topic-level relationships in the archive as real and
                     					meaningful discursive relationships that can be explored through quantitative
                     					and inferential analysis despite not being reducible to the latter. The goal
                     of
                     					computational modeling and data visualization can and should be the expansion
                     of
                     					textual dimensionality in productive and meaningful ways. Moreover, we believe
                     					these visualizations should prioritize utility and interpretability for actual
                     					domain experts. That is why we chose tools and designed the site based on our
                     					own research priorities and those of our advisory team. In conversations with
                     					early users of <cite class="title italic">Project Quintessence</cite>, we have
                     					observed that the site becomes more useful the better one understands the EEBO
                     					archive; the inverse trend of many out-of-the-box methods.</div>
                  				
                  <div class="div div1">
                     					
                     <h2 class="head">Elements of <cite class="title italic">Project Quintessence</cite></h2>
                     					
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">While a full catalog of all features of <cite class="title italic">Project
                           						Quintessence</cite> exceeds the bounds of this paper, we outline here the
                        						two general modes of ingress into the EEBO-TCP archive that our site
                        						affords: a “Topic Exploration” view and a “Word Exploration” view,
                        						with both views connecting back to the individual documents themselves.
                        						These two views incorporate multiple interactive and dynamic visualizations
                        						using statistical models trained on the documents of the EEBO-TCP archive.
                        						These modes are meant to be used in conjunction when the site is used for
                        						research purposes. By synthesizing multiple visualizations users can better
                        						uncover patterns, identify what each model has learned from the archive, and
                        						better understand the features of the archive itself.</div>
                     					
                     <div class="div div2">
                        						
                        <h3 class="head">Topic Exploration</h3>
                        						
                        <div class="counter"><a href="#p15">15</a></div>
                        <div class="ptext" id="p15">Since the emergence of topic modeling in the toolkits of Digital
                           							Humanities researchers a decade ago, it has been considered an
                           							archetypal <em class="term">distant reading</em> method [<a class="ref" href="#meeks_etal2012">Meeks and Weingart 2012</a>]. The presentation of topic exploration in
                           							<cite class="title italic">Project Quintessence</cite> recasts this
                           							association entirely. By integrating several different dynamic
                           							visualizations with database search, subsetting, and indexing functions,
                           							the “Topics” page allows a seamless toggling between the topics
                           							themselves, search keywords, author, print date and location metadata,
                           							and hand-tagged text keywords from the EEBO-TCP team. Rather than a
                           							static abstraction of the archive, the topic model on our site depicts
                           							abstract relationships between each of these categories as tangible
                           							visual dimensions. Connecting the topics learned with our topic model
                           							with document level metadata opens opportunities for users of the site
                           							to explore the EEBO-TCP corpus from the top down and the bottom up. In
                           							addition, the interactive and dynamic visualizations allow for greater
                           							ability to judge and understand the models’ output. Lastly, the
                           							visualizations connect with the other models and visualizations of the
                           							site, to allow users to leverage the strengths of each model, and
                           							critically evaluate each model’s limitation. While corpus visualizations
                           							often invite an impression of cohesion and totality, the organization of
                           							our site works against this tendency. The limitations of each of our
                           							models can be directly explored within the site’s framework.</div>
                        						
                        <div id="figure01" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="" /></a></div>
                           						
                           <div class="caption">
                              <div class="label">Figure 1. </div>Text Topics view. Each bubble represents one of seventy-five
                              								topics and is numbered accordingly.</div>
                        </div>
                        						
                        <div class="counter"><a href="#p16">16</a></div>
                        <div class="ptext" id="p16">We trained a topic model with seventy-five topics on the EEBO-TCP corpus:
                           							the topics are displayed in <a href="#figure01">Figure 1</a> above
                           							as nodes in a bubble plot. Each node in the bubble plot represents a
                           							single topic scaled according to the proportion of the archive that was
                           							assigned to that topic. Proportions were estimated using the method
                           							described in the LDAvis paper [<a class="ref" href="#sievert_etal2014">Sievert and Shirley 2014</a>].<a class="noteRef" href="#d3e513">[4]</a>
                           							The nodes are placed based on the coordinates of each topic mapped to
                           							two-dimensional space according to the calculated Jensen-Shannon (JS)
                           							divergence of topic term probabilities.<a class="noteRef" href="#d3e519">[5]</a> As a result, node proximity
                           							translates to similarity in topic composition. This means that nodes are
                           							roughly grouped based on topic similarity. This feature alone allows for
                           							novel observations of the archive: religious topics are close to
                           							philosophical and poetical ones, and scientific texts are close to
                           							nautical guides and recipe books.</div>
                        						
                        <div class="counter"><a href="#p17">17</a></div>
                        <div class="ptext" id="p17">To aid visual exploration of the topic model, we have included hover-text
                           							for each topic, which can be modified by the user in the top-right
                           							dropdown to display the associated metadata that best fits their
                           							research question: “Top Keywords,” “Top Authors,” or “Top
                           							Locations” of text publication.</div>
                        						
                        <div id="figure02" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="" /></a></div>
                           
                           						
                           <div class="caption">
                              <div class="label">Figure 2. </div>Text Topics view. Topic bubbles are scaled according to importance
                              								of search term for topic.</div>
                        </div>
                        						
                        <div class="counter"><a href="#p18">18</a></div>
                        <div class="ptext" id="p18">In addition to displaying the topic proportions for the archive, the
                           							scatter plot can be rescaled to reflect the topic proportions for each
                           							word (in this case the number of times the word has been assigned to
                           							each topic – see <a href="#figure02">Figure 02</a>). The display
                           							above reflects how the scale of each node is dynamically shifted to
                           							reflect the importance of each topic for the searched word.</div>
                        						
                        <div id="figure03" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" style="" alt="" /></a></div>
                           						
                           <div class="caption">
                              <div class="label">Figure 3. </div>Topic terms histogram. Words are ranked by importance to topic.
                              								Adjusting the relevance metric reweights the scores for each term,
                              								the lower the value for the metric the more words that are unique to
                              								that topic are boosted according to the formula found in Sievert and
                              								Shirley 2014.</div>
                        </div>
                        
                        
                        						
                        <div class="counter"><a href="#p19">19</a></div>
                        <div class="ptext" id="p19">To the right of the topic visualization proper is another companion
                           							visualization (<a href="#figure03">Figure 03</a>), which depicts
                           							the thirty most important words for that topic. The red bar in this
                           							stacked bar plot represents the prevalence of the word within the
                           							selected topic, while the blue bar represents the total occurrences of
                           							the word throughout the entire archive. To better understand the
                           							particularities of a specific topic, we have implemented a “Relevance
                           							Metric,” also modeled on the LDAvis package, which allows the user to
                           							preference words that are uniquely important to the selected topic
                           							(e.g., “promise”) rather than words that tend to be overrepresented
                           							across the archive and are thus important to many topics (e.g.,
                           							“god”) [<a class="ref" href="#sievert_etal2014">Sievert and Shirley 2014</a>].</div>
                        						
                        <div id="figure04" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" style="" alt="" /></a></div>
                           
                           						
                           <div class="caption">
                              <div class="label">Figure 4. </div>Topic info tab. Histogram displays topic proportion of entire
                              								corpus per decade in percentages.</div>
                        </div>
                        						
                        <div class="counter"><a href="#p20">20</a></div>
                        <div class="ptext" id="p20">But again, the design of the topics page insists on a contextual
                           							understanding of each topic. The “Info” tab for each topic will
                           							display the proportion of the archive captured by that topic in each
                           							decade and the top authors, text keywords, and print locations
                           							associated with that topic (<a href="#figure04">Figure 04</a>). In
                           							addition, the latter metadata categories can be explored for each decade
                           							of the archive. This feature might allow a researcher to answer
                           							questions such as; “which print locations were printing herbals in
                           							the 1620s?”, or “which authors most heavily employed New
                           							Testament biblical language around 1700?” These increases in the
                           							textual dimensionality of the EEBO-TCP archive help researchers forge
                           							new relationships with the texts themselves.</div>
                        						
                        <div class="div div3">
                           							
                           <h4 class="head">Topics Over Time</h4>
                           							
                           <div class="counter"><a href="#p21">21</a></div>
                           <div class="ptext" id="p21">The EEBO-TCP archive contains texts from 1473-1700. We provide users
                              								two ways to explore the evolution of the topics covered in the
                              								archive over this time range. A well-known limitation of LDA Gibbs
                              								topic modeling is that the model does not account for the date of
                              								publication when analyzing topic composition [<a class="ref" href="#blei_etal2007">Blei and Lafferty 2007</a>] [<a class="ref" href="#roberts_etal2019">Roberts, Stewart, and Tingley 2019</a>]. This
                              								imprecision sometimes results in topics that group together words
                              								despite linguistic shifts in the meaning of words and changes in
                              								local context of use at the sentence- or paragraph-level of a text.
                              								This effect is made worse by the uneven distribution of documents
                              								across time within EEBO-TCP, which contains significantly more
                              								documents starting in 1640 than the previous years. As a result, an
                              								abstracted static display of the topic proportions of our archive
                              								would collapse much of the potential information about the themes
                              								present in the underlying texts, especially from documents published
                              								before 1640. However, by maintaining a relationship between multiple
                              								dynamic visualizations and the texts themselves, and by presenting
                              								topic modeling as one of several modes of obtaining archival
                              								information, <cite class="title italic">Quintessence</cite> strikes a
                              								balance between using language models as research tools while
                              								foregrounding the limitations of such models.</div>
                           							
                           <div class="counter"><a href="#p22">22</a></div>
                           <div class="ptext" id="p22">In the same ‘Info’ tab described in <a href="#figure04">Figure
                                 								4</a>, we provide a visualization of the selected topic’s
                              								proportion within the archive over the full range, shown above.
                              								Thus, for each decade, from 1470-1700, we compute each topic’s
                              								proportion of the subset of EEBO-TCP that was published within that
                              								decade.</div>
                           							
                           <div class="counter"><a href="#p23">23</a></div>
                           <div class="ptext" id="p23">To further aid in the exploration of these trends, we link the topic
                              								proportions back with the original bubble plot that visualizes both
                              								topic similarity and topic proportions. On the “topics over
                              								time” page, we provide an animation of the computed topic
                              								proportions for each decade. By playing the animation a user can see
                              								the bright spots on the quadrants of the bubble plot grow and shrink
                              								as the associated topics change in proportion. This animated plot
                              								can also be paused and resumed, and any decade can be manually
                              								selected.</div>
                           							
                           <div id="figure05" class="figure">
                              								
                              								
                              <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="" /></a></div>
                              
                              							
                              <div class="caption">
                                 <div class="label">Figure 5. </div>Animated view of topic composition proportion by decade. Each
                                 									bubble represents one topic as labeled.</div>
                           </div>
                           							
                           <div class="counter"><a href="#p24">24</a></div>
                           <div class="ptext" id="p24">The animated view of the topic model depicted in <a href="#figure05">Figure 5</a> helps to illustrate in a tangible
                              								way changing representations in the archive over the time frame
                              								analyzed. While a static topic model has great utility for exploring
                              								unexpected textual relationships across the archive, this
                              								presentation of the texts risks abstracting somewhat-arbitrary topic
                              								assignments from historical changes and contingencies in the texts
                              								themselves. We believe this time series animation helps remind the
                              								user of these elements of the archive.</div>
                           						</div>
                        						
                        <div class="div div3">
                           							
                           <h4 class="head">Topic Document Exploration</h4>
                           							
                           <div class="counter"><a href="#p25">25</a></div>
                           <div class="ptext" id="p25">Due to the dynamic design of the topic model visualization, we were
                              								able to provide researchers an additional capability: our subset
                              								function allows one to create an individualized research archive out
                              								of any combination of texts; filtered by date range, keyword,
                              								author, or location.</div>
                           							
                           <div id="figure06" class="figure">
                              								
                              								
                              <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a></div>
                              							
                              <div class="caption">
                                 <div class="label">Figure 6. </div>Topic subset settings. Date range slider is in decades.
                                 									Keywords, authors, and locations taken from EEBO-TCP metadata
                                 									tags.</div>
                           </div>
                           
                           
                           							
                           <div class="counter"><a href="#p26">26</a></div>
                           <div class="ptext" id="p26">Once the filter for this research archive has been applied to the
                              								model, the topic visualization will display the topic proportions
                              								for whatever collection of texts are included in the selected subset
                              								(<a href="#figure06">Figure 06</a>). This tool allows
                              								researchers to examine the prevalence of religious language in the
                              								texts of Robert Boyle, for instance, or assess at a glance the topic
                              								similarities between William Shakespeare, Christopher Marlowe and
                              								Ben Jonson.</div>
                           							
                           <div id="figure07" class="figure">
                              								
                              								
                              <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" style="" alt="" /></a></div>
                              
                              							
                              <div class="caption">
                                 <div class="label">Figure 7. </div>Document subset view (continued). All filtered documents
                                 									appear in a list with associated metadata.</div>
                           </div>
                           							
                           <div class="counter"><a href="#p27">27</a></div>
                           <div class="ptext" id="p27">But due to the persistent nature of the subsetting function, one can
                              								also scroll past the model itself to see each text included in the
                              								selected research archive (<a href="#figure07">Figure 07</a>).
                              								This list displays all available metadata, a sample fragment of the
                              								document, as well as a link to the full document in our database.
                              								The simultaneous access to dimensional representations of discursive
                              								relationships in the archive as well as the underlying documents
                              								from which those relationships were derived allows for <cite class="title italic">Project Quintessence</cite> to reorient common
                              								assumptions about depth and proximity in Digital Humanities
                              								scholarship. As we will return to at the end of this article, future
                              								directions for this project will emphasize making accessible these
                              								discursive relationships in the document reader itself, thus
                              								emphasizing dimensionality in close-reading practices in addition to
                              								the visualizations themselves.</div>
                           						</div>
                        					</div>
                     					
                     <div class="div div2">
                        						
                        <h3 class="head">Word Exploration</h3>
                        						
                        <div class="counter"><a href="#p28">28</a></div>
                        <div class="ptext" id="p28">Topic modeling provides an efficient way of summarizing an unreadable
                           							amount of text. However, on its own, a topic model does not enable
                           							nuanced exploration of language evolution in the EEBO-TCP. This is due
                           							in part to the fact that many of the characteristics of the EEBO-TCP
                           							archive come in direct conflict with some common constraints of the
                           							model’s statistical assumptions – namely, variance of document length,
                           							variance in time of publication, a reliance on a “bag-of-words”
                           							representation of documents (that is, removing metadata and token
                           							sequence when examining textual relationships), and an
                           							oversimplification of terms as static entities (ignoring polysemy and
                           							semantic shift). To ameliorate this shortcoming in an otherwise useful
                           							modeling method, we incorporate word embedding models, which are trained
                           							using a sequential, <em class="term">skip-gram</em> learning process, where the
                           							local context of the word is the primary information ingested by the
                           							model.</div>
                        						
                        <div class="counter"><a href="#p29">29</a></div>
                        <div class="ptext" id="p29">Another limitation of topic modeling that word embeddings avoids is the
                           							relative arbitrariness of topic assignments. While we relied on a series
                           							of well-known metrics to determine an optimal number of topics for our
                           							model, the design of an LDA topic model necessitates prior determination
                           							of certain hyperparameters that are imposed upon the text from outside,
                           							rather than inferred from the texts themselves. We used the
                           							<em class="term">ldatuning</em> package in R by Nikita Murzintcev, which combines
                           							metrics designed to prioritize low topic entropy and high topic
                           							diversity.<a class="noteRef" href="#d3e683">[6]</a> Ultimately, the metrics we
                           							prioritize will result in several candidate topic counts that will
                           							present similarly useful topic distributions. Rather than choosing an
                           							exceedingly large topic count that renders visual model analysis
                           							exceedingly difficult, we chose 75 topics, which struck a balance
                           							between metric optimization and readability. We help to correct for many
                           							of these limitations with our dynamic visualizations. However, we also
                           							supplement topic models with word level exploration. This includes
                           							visualizations of word frequency, views of words in context, and
                           							longitudinal word2vec models of the archive. Through the use of these
                           							visualizations, researchers can explore the semantic evolution of terms,
                           							distinctions in language usage across documents and metadata, and the
                           							broader thematic contexts of topics learned from LDA Gibbs.</div>
                        						
                        <div id="figure08" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" style="" alt="" /></a></div>
                           						
                           <div class="caption">
                              <div class="label">Figure 8. </div>Word Meanings view. Search word appears at center of network plot
                              								with nine most similar words. Bubble size scaled for word frequency
                              								in selected model.</div>
                        </div>
                        
                        
                        						
                        <div class="counter"><a href="#p30">30</a></div>
                        <div class="ptext" id="p30">By training hundreds of word embeddings models on subsets of our corpus
                           							grouped by author, print location, and decade of publication, we have
                           							obtained data on contextual distinctions in usage patterns for
                           							approximately four hundred thousand unique tokens. Gathering contextual
                           							word use data on this high number of words would be intractable with
                           							topic modeling for most computational humanities labs due to the
                           							enormous amount of computational resources necessary to dedicate to this
                           							process. Because word embeddings output dimensional coordinates for each
                           							word in the model, they also allow for a measurement of the similarity
                           							between words in the learned embedded space. The network plot of the
                           							search term “quintessence” depicted in <a href="#figure08">Figure 8</a> allows for a comparison of the term usage across every
                           							text in the archive with the usage of the top ten most similar words
                           							[<a class="ref" href="#katricheva_etal2020">Katricheva et al 2020</a>]. The size of each node reflects
                           							total occurrences of the word in question, and the width of each edge
                           							reflects the similarity between the two connected nodes.</div>
                        						
                        <div id="figure09" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" style="" alt="" /></a></div>
                           
                           						
                           <div class="caption">
                              <div class="label">Figure 9. </div>Word Meanings view. Word clustering suggests polysemy of search
                              								term.</div>
                        </div>
                        						
                        <div class="counter"><a href="#p31">31</a></div>
                        <div class="ptext" id="p31">Due to their contextual specificity, word embeddings also emphasize
                           							another linguistic feature missing in most topic modeling
                           							representations: the allowance for polysemy. The network graph of the
                           							word “actor” in <a href="#figure09">Figure 9</a> illustrates
                           							this feature. By clustering nearest words to the word being examined,
                           							this network representation allows one to see when a word has obvious
                           							distinctions in use. “Actor” usually denotes a dramaturge in our
                           							archive, but also has a conspiratorial valence (e.g., “instigator,”
                           							“plotter”). Finally, the presence of the word “spectator”
                           							demonstrates the high level of interchangeability between antonymous
                           							words.</div>
                        						
                        <div id="figure10" class="figure">
                           							
                           							
                           <div class="ptext"><a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" style="" alt="" /></a></div>
                           
                           						
                           <div class="caption">
                              <div class="label">Figure 10. </div>Word Meanings view. Network graph of term “hamlet” in the
                              								William Shakespeare model.</div>
                        </div>
                        						
                        <div class="counter"><a href="#p32">32</a></div>
                        <div class="ptext" id="p32">Another feature of Quintessence, due to the high number of smaller
                           							embeddings models we have trained, is the ability to examine the oeuvres
                           							of specific authors to examine the semantic relationships of their
                           							idiosyncratic word usage. In <a href="#figure10">Figure 10</a>, we
                           							can see that William Shakespeare uses the word “Hamlet” with
                           							respect to certain other proper names and character titles, indicating
                           							the term takes on a unique meaning in the Shakespeare model.</div>
                        						
                        <div class="div div3">
                           							
                           <h4 class="head">Word in Context</h4>
                           							
                           <div id="figure11" class="figure">
                              								
                              								
                              <div class="ptext"><a href="resources/images/figure11.png" rel="external"><img src="resources/images/figure11.png" style="" alt="" /></a></div>
                              
                              							
                              <div class="caption">
                                 <div class="label">Figure 11. </div>Word Meanings view. Keyword in context for search term.
                                 									Documents labeled by document number.</div>
                           </div>
                           							
                           <div class="counter"><a href="#p33">33</a></div>
                           <div class="ptext" id="p33">Exploration of individual words in the corpus is also facilitated by
                              								the Word in Context tab, which gives contextual word usage data for
                              								every instance of the chosen word in the corpus (<a href="#figure11">Figure 11</a>). These results are filtered by
                              								<em class="term">tf-idf</em> scores (term frequency-inverse document
                              								frequency) to ensure the top results are highly relevant to an
                              								exploration of the word’s usage. By displaying this textual context
                              								alongside our word embeddings model outputs, our site maintains a
                              								relationship between machine learning abstractions and close reading
                              								techniques.</div>
                           						</div>
                        						
                        <div class="div div3">
                           							
                           <h4 class="head">Word Semantic Shift</h4>
                           							
                           <div id="figure12" class="figure">
                              								
                              								
                              <div class="ptext"><a href="resources/images/figure12.png" rel="external"><img src="resources/images/figure12.png" style="" alt="" /></a></div>
                              							
                              <div class="caption">
                                 <div class="label">Figure 12. </div>Word Meanings view. Search term represented for each decade
                                 									according to percentage similarity with term usage in 1700
                                 									model. Precise similarity depicted as “Similarity Score” on
                                 									mouseover, along with most similar terms.</div>
                           </div>
                           
                           
                           							
                           <div class="counter"><a href="#p34">34</a></div>
                           <div class="ptext" id="p34">Another function word embeddings allow for is the alignment of
                              								different models to gauge the relative position of words in their
                              								respective subset. Such a method estimates the comparative usage and
                              								semantics of the same word in two different subsets of an archive.
                              								This has classically been implemented only for time-slices in
                              								historical archives, and is thus termed “diachronic embedding” [<a class="ref" href="#hamilton_etal2016">Hamilton et al 2016</a>]. <cite class="title italic">Project Quintessence</cite> has
                              								implemented an experimental beta visualization that allows one to
                              								explore an aligned embedding space comprising the twenty-four
                              								decades in the EEBO-TCP archive. The visualization depicted in <a href="#figure12">Figure 12</a> can be uniquely generated for
                              								over sixty thousand words in our archive, and represents an
                              								alignment between each decade and 1700 — the last decade captured in
                              								the archive. The higher the “Similarity Score,” the more
                              								similar the word is to its usage in 1700. Sudden peaks or valleys in
                              								the plot usually signal a semantic shift in that word’s usage during
                              								that time period. The node for each decade can be hovered over to
                              								display the Similarity Score along with the top ten most similar
                              								words in that decade.</div>
                           							
                           <div class="counter"><a href="#p35">35</a></div>
                           <div class="ptext" id="p35">While recent empirical analyses of such embedding alignment methods
                              								have cast some doubt upon their reliability [<a class="ref" href="#hellrich_etal2018">Hellrich et al 2018</a>] [<a class="ref" href="#hellrich2019">Hellrich 2019</a>]), and
                              								thus a final version will need to overcome certain pitfalls, we have
                              								observed that this tool, when combined with document retrieval and
                              								keyword in context functionality, provides a unique representation
                              								of the evolution of language across historical time. In addition,
                              								the textual metadata of the EEBO-TCP will allow us to use similar
                              								alignment methods to compare word usage similarity between authors,
                              								in different regions, and more. We are not aware of any other
                              								attempts to implement such longitudinal linguistic analysis.</div>
                           							
                           <div class="counter"><a href="#p36">36</a></div>
                           <div class="ptext" id="p36">In summary, the three primary modes of analysis afforded by Project
                              								<cite class="title italic">Quintessence</cite> are word-specific
                              								analysis, topic-level patterns, and document-based inquiry. Rather
                              								than understanding these as operating at three distinct scales of
                              								analysis, the dynamic design of our site allows an interpolation of
                              								these methods into an integrated dimensional analysis. Word usage
                              								and semantics are available during a close reading; individual
                              								documents can be examined while investigating archive-wide thematic
                              								trends; and the gradual semantic and contextual shifts of a word sit
                              								alongside the actual uses of that word in the texts underlying our
                              								computational analyses. We believe that this rich conjunction of
                              								visualizations and tools provides a new model for digital archive
                              								presentation and scholarly analysis in the digital humanities.</div>
                           						</div>
                        					</div>
                     				</div>
                  				
                  <div class="div div1">
                     					
                     <h2 class="head">Project Pipeline</h2>
                     					
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">While idiosyncrasies found in the EEBO-TCP corpus are the norm for large
                        						textual archives, they present unique challenges and opportunities for
                        						digital humanities research. This is in no small part due to the reliance on
                        						uniform, carefully manicured datasets for the development of nearly every
                        						method of computational analysis. Increasing attention has been paid to the
                        						reproducibility of language models tuned to a few industry-standard datasets
                        						that bear little resemblance to those encountered in archival research [<a class="ref" href="#jo_etal2020">Jo and Gebru 2020</a>]. While models improve on benchmark performance
                        						year-by-year, it becomes increasingly difficult to describe the nature of
                        						those improvements or to predict the performance of competing models in
                        						real-world contexts.</div>
                     					
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">However, these discussions on archive construction rarely broach the question
                        						of data asymmetry, enormous vocabulary sizes, or multilingual texts. All of
                        						these aspects are present in the EEBO-TCP corpus. The largest document in
                        						the corpus contains nearly 3.1 million words, whereas the smallest document
                        						is only 5 words long. There are over 2.5 million unique words in the corpus.
                        						After aggressive spelling variant reduction, spell-correction, and
                        						lemmatization, that number still only shrinks to 1.5 million. Even after
                        						filtering out texts tagged in primary languages other than English–of which
                        						fourteen are represented–the corpus still contains frequent block quotes,
                        						references, and terms in Latin, Greek, French, and occasionally other
                        						languages. For these reasons, EEBO-TCP becomes an exemplary corpus for
                        						computational research, not for its uniformity, but for precisely the
                        						opposite reason.</div>
                     					
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Development of a coherent toolset for <span class="hi italic">Project
                           						Quintessence</span> thus necessitated an iterative, empirical process. Each
                        						word stemmer and lemmatizer had to be compared statistically and
                        						hand-checked for overall accuracy. The same held true for clustering
                        						algorithms, word embedding models, topic models, and network analysis. This
                        						arena of labor, so often ignored by out-of-the-box toolkits and invisible in
                        						massive search interfaces, necessitated the care and expertise of scholars
                        						educated in early modern literature and culture. Thus, our project pipeline
                        						incorporated at every stage a negotiation between conventional literary
                        						knowledges and the production of computational tools.</div>
                     					
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">The first stage of our pipeline involved text cleaning. Due to the
                        						crowdsourced nature of the EEBO-TCP project, we encountered inconsistencies
                        						in character encodings as well as TEI tagging schemata. Thus, we had to
                        						encode all texts into UTF-8 and include several automated character
                        						alterations to ensure each encoding was handled properly. As we wanted to
                        						maintain reproducibility of research findings for the archive itself, we
                        						adopted a minimalist approach to textual and tag alterations. We intend to
                        						make available an open-source version of our text cleaning and modeling
                        						source code to maximize transparency of results. Tags that were either too
                        						widely used or too seldomly used were winnowed to preserve functionality. In
                        						order to model linguistic change over time and obtain decade-specific
                        						language models, we also had to normalize dates by converting Roman numerals
                        						and reducing date ranges to a single four-digit year.<a class="noteRef" href="#d3e841">[7]</a> Word spelling corrections and
                        						standardizations, along with word stems, lemmata, and part-of-speech
                        						information, were obtained by using a locally-run build of MorphAdorner–a
                        						bespoke NLP software tool trained on an early modern archive.</div>
                     					
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">The second stage of the pipeline was dedicated to database architecture and
                        						API development. While we relied on relational database configurations in
                        						early versions of the project, the performance gains and nimbleness of
                        						MongoDB ultimately outweighed the intuitive structure of SQL and similar
                        						configurations. Because each visualization and search function on our
                        						website accesses different elements of the text database and models, we
                        						found database calls impossible to construct efficiently enough to maintain
                        						speed and performance. This performance was aided after our implementation
                        						of a RESTful API written in PHP to serve as intermediary for database calls.
                        						In addition to these obvious end-user functionality gains, our shift to
                        						MongoDB (a popular document-oriented database program) resulted in a drastic
                        						reduction of compute-time, as new versions of the database could be iterated
                        						without a complete reindexing. This flexibility gives us the opportunity to
                        						grow our archive over time without the need to start from scratch and debug
                        						queries with each alteration.</div>
                     					
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">The third stage of the pipeline was dedicated to modeling and model
                        						optimization. While we experimented with many different topic models and
                        						word embedding techniques in preliminary stages, we decided to use Latent
                        						Dirichlet Allocation (LDA) topic modeling with posterior Gibbs sampling for
                        						our base topic models. Our topic models were computed using the Gensim
                        						wrapper to MALLET, a Java-based machine learning toolkit dedicated to NLP
                        						modeling. We optimized the total topics in the model according to a balance
                        						of metrics provided by the LDATuning package in R. We have found that what
                        						is sacrificed in performance and possible insight is made up for in
                        						reproducibility, field familiarity, efficiency and flexibility. As our
                        						corpus grew to over two billion words, the compute-time for LDA modeling
                        						quickly grew unfeasibly large with several modeling techniques.<a class="noteRef" href="#d3e850">[8]</a></div>
                     					
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">A different issue was faced with our choice of word embedding model. While
                        						BERT, ELMo, CoVE, and other “state-of-the-art” techniques boast much
                        						higher performance on carefully-tagged and uniform models in
                        						“resource-rich” contemporary languages (e.g. English, French, German),
                        						it is uncertain how this performance translates to different historical and
                        						multilingual contexts. We currently lack a sufficiently curated early modern
                        						database to support training a similar transformer or tag-dependent model
                        						from scratch. Thus, the potential for historical bias to influence model
                        						outcomes is great, and our ability to detect such distortions is highly
                        						limited. Our choice to use Word2Vec for our embeddings models was also
                        						influenced by the highly nimble nature of the technique, which allows for
                        						reliable word vectors in corpora as low as two million words. This allowed
                        						us to train smaller embedding models for subsets of our corpus based on
                        						EEBO-TCP metadata tags, including; Author, Location, and Publication
                        						Decade.</div>
                     					
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">To preprocess the corpus for our Word2Vec models, we tokenized documents into
                        						sentences using the NLTK Sentence Tokenizer. We trained embeddings models
                        						using 250 dimensions, with a skipgram window of 15. For all author,
                        						location, and decade models, we trained models on any subset greater than
                        						two million words in size. For our diachronic embeddings visualization, we
                        						used procrustes alignment on the word vector for each decade model it
                        						appeared in, beginning with the last decade in our archive (1700) and
                        						working backward to leverage the richer spatial orientation of a
                        						high-vocabulary model.<a class="noteRef" href="#d3e864">[9]</a> Because
                        						larger language models are generally intended for word prediction and
                        						translation, they are built by design to leverage pre-existing model data to
                        						predict a single, correct meaning for each word usage. While it is perhaps
                        						possible to counteract this tendency with a nuanced implementation, we
                        						determined that the loss in accuracy of idiosyncratic word usage data was
                        						far greater than any potential gains in a naive implementation of such a
                        						method.</div>
                     					
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">With the website construction, emphasis was placed on creating virtual
                        						environments to most effectively toggle between scales of the database. The
                        						website itself was built with PHP and JavaScript, with a sparse UI and
                        						accessible color palettes. We worked on UI/UX in consultation with Kimmy
                        						Hescock at the UC Davis Library to present users with a learnable visual
                        						grammar that combined the many different visualizations into a single
                        						text-exploration environment. With this in mind, we found the open-source
                        						dynamic visualization library, plotly.js, particularly useful.<a class="noteRef" href="#d3e872">[10]</a> In
                        						choosing specific visualizations from the many possibilities, we wanted to
                        						minimize redundancy or contradictions in the data presented, maximizing
                        						instead the recombinant affordances of using the many tools in conjunction
                        						to glean research insights from the database. Thus, we also worked
                        						periodically with test users researching early modern literature to better
                        						understand what sorts of research questions for which a scholar might
                        						approach this archive, and to imagine ways of accommodating the exploration
                        						of those questions.</div>
                     					
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46">Due to our prioritization of accuracy, reproducibility, and flexibility, we
                        						have built a framework for interactive language modeling that persists
                        						beyond any given implementation. While our methodology insists on a close
                        						relationship between area experts and data scientists, we nevertheless have
                        						produced a framework that allows for continual additions to and alterations
                        						of our archive. This allows both internal and external users and researchers
                        						to participate in the construction of more representative and inclusive
                        						historical archives, and safeguards our project from ossifying a dated
                        						snapshot of textual history. Looking forward, we also anticipate that this
                        						framework can be used similarly on other textual archives to similar effect.
                        						While archives should not be treated as interchangeable by data scientists
                        						and engineers, the agility of our framework nevertheless facilitates its
                        						tuning on archives beyond the EEBO-TCP.</div>
                     				</div>
                  			</div>
               			
               <div class="div div0">
                  				
                  <h1 class="head">CONCLUSION AND NEXT STEPS</h1>
                  				
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">We hope in this paper and through the <cite class="title italic">Quintessence</cite>
                     					site to have contributed to the design of future archival research frameworks
                     					through an emphasis on dimensionality and dynamic exploration that interfaces
                     					with texts. As with any project at this scale, limitations on resources and time
                     					have necessarily influenced the scope of the site itself. Nevertheless,
                     					continued advancements in textual analysis tools present many exciting
                     					opportunities for future enhancements.</div>
                  				
                  <div id="figure13" class="figure">
                     					
                     					
                     <div class="ptext"><a href="resources/images/figure13.png" rel="external"><img src="resources/images/figure13.png" style="" alt="" /></a></div>
                     
                     				
                     <div class="caption">
                        <div class="label">Figure 13. </div>Prototype of Document Viewer. Clickable tags are generated for word topic
                        						assignment, part-of-speech, or named entity assignment.</div>
                  </div>
                  				
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">Our “Document Viewer” prototype (<a href="#figure13">Figure 13</a>)
                     					perhaps best represents the future direction of our project. We are designing
                     					this tool to be fully integrated into other models and visualizations on our
                     					site as a dynamic overlay to any text that is accessed from our database.
                     					Through an automatically-generated element tagging method, topic,
                     					part-of-speech, and named entity information will be available as one is
                     					examining the text itself. The Document Viewer figures this relational data for
                     					tokens in the corpus as latent dimensions within the texts themselves that can
                     					be studied and visualized. As with every other tool on our site, one can
                     					seamlessly navigate between this textual view and the models discussed above,
                     					which we believe mirrors the recursive nature of the archival research
                     					process.</div>
                  				
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">As mentioned above, a project to produce a ground-up bidirectional encoding or
                     					contextual embedding model for the EEBO-TCP would enable a trove of new tools
                     					and methods. Such a project might help to integrate top-down topic modeling
                     					approaches with bottom-up embeddings, like in the Neural Topic Modeling
                     					approaches of Wang et al. and Gupta et al., or phrase- and document-focused
                     					comparison tools like phrase vectorization, <cite class="title italic">Gensim’s
                        					Doc2Vec</cite>, or Kusner et al.’s <cite class="title italic">Word Mover’s
                        					Distance</cite> [<a class="ref" href="#wang_etal2018">Wang et al 2018</a>] [<a class="ref" href="#gupta_etal2021">Gupta, Chaudhary, and Schütze 2021</a>]
                     					[<a class="ref" href="#kusner_etal2015">Kusner 2015</a>]. These approaches could radically alter
                     					researcher interactions with texts if vector data were built into document
                     					indexing through a program like Apache Lucene. A researcher could plausibly
                     					search or access dynamic maps of phrases, documents, topics and more based on
                     					semantic similarity. A vector-based approach has also been fruitfully paired
                     					with image and sound tagging, and our analysis of the latter could be enhanced
                     					by reinserting these media into their textual contexts.</div>
                  				
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">The normalized tagging necessary to produce such an embedding model could be used
                     					for network analysis of named entities, enabling search and categorization by
                     					metadata categories such as author, publisher, and location, and even diegetic
                     					textual categories like character, historical figure, location, or chronology.
                     A
                     					synthesis between diachronic embeddings and diachronic topic modeling, such as
                     					Blei et al.’s Dynamic Topic Model (DTM) or Roberts et al.’s Structural Topic
                     					Model (STM) could help enrich our understanding of the complexity of discursive
                     					change beyond a simple word-based or topic-based approach. We have also
                     					experimented with the use of Gerrish et al.’s Document Influence Model (DIM)
                     to
                     					map the persistence and change of topics over time linked to the publication
                     of
                     					specific texts. This modeling technique would only be enhanced by integrating
                     					embeddings with topic modeling, and could assist with recovery projects and
                     					questions of canonicity and biases in scholarly attention.</div>
                  				
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">While the above by no means exhausts the potential future directions of the
                     					<cite class="title italic">Quintessence</cite> framework, we hope it paints a picture
                     					of the rich potential for future archival research that harnesses textual
                     					dimensionality. As we extend the <cite class="title italic">Quintessence</cite>
                     					framework beyond the EEBO-TCP archive to inform database design on other
                     					archives and in other domains, we hope to reverse the trend toward massive and
                     					opaque automated textual modeling and encourage critical and scholarly
                     					apprehension of an ever-growing archive of digitized historical material.</div>
                  			</div>
               		
               		
               			
               		
               	</div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d3e291"><span class="noteRef lang en">[1] The first two authors contributed equally to this research.
                     					The authors would like to thank the UC Davis DataLab and UC Davis Library for
                     					supporting this project. In particular, we would like to acknowledge the
                     					invaluable feedback and assistance of Tyler Shoemaker, Kimmy Hescock, Hugo
                     					Mailhot, Jane Carlen, Wesley Brooks, Anupam Basu, and Duncan Temple Lang.
                     					Finally, we would like to thank our anonymous reviewers for their useful
                     					recommendations and the DHQ editorial team for their work.</span></div>
               <div class="endnote" id="d3e293"><span class="noteRef lang en">[2] Our code
                     					repositories can be found at the following links: For the web app, <a href="https://github.com/datalab-dev/quintessence_web_app" onclick="window.open('https://github.com/datalab-dev/quintessence_web_app'); return false" class="ref">https://github.com/datalab-dev/quintessence_web_app</a>; and for all NLP
                     					analysis, <a href="https://github.com/datalab-dev/quintessence_analysis" onclick="window.open('https://github.com/datalab-dev/quintessence_analysis'); return false" class="ref">https://github.com/datalab-dev/quintessence_analysis</a>.</span></div>
               <div class="endnote" id="d3e304"><span class="noteRef lang en">[3] 
                     					Three examples of this style of tool are: JSTOR Labs Text Analyzer beta <a href="https://www.jstor.org/analyze/" onclick="window.open('https://www.jstor.org/analyze/'); return false" class="ref">https://www.jstor.org/analyze/</a>,
                     					Gale’s Digital Scholars Lab <a href="https://gale.com/intl/primary-sources/digital-scholar-lab" onclick="window.open('https://gale.com/intl/primary-sources/digital-scholar-lab'); return false" class="ref">https://gale.com/intl/primary-sources/digital-scholar-lab</a>, and Google
                     					Books N-Gram Viewer, <a href="https://books.google.com/ngrams/" onclick="window.open('https://books.google.com/ngrams/'); return false" class="ref">https://books.google.com/ngrams/</a>.</span></div>
               <div class="endnote" id="d3e513"><span class="noteRef lang en">[4] Our proportion formula is:$$\frac{(\text{document length})\times(\text{document topic
                     							distribution})}{\text{sum of all document lengths}}$$This
                     							formula for estimating topic proportion is used across the topic model
                     							visualizations, including when subsetting the archive and for the topic
                     							proportions over time plots, both of which are described below.</span></div>
               <div class="endnote" id="d3e519"><span class="noteRef lang en">[5] Due to the collapse of the
                     							JS divergence into two dimensions, a certain level of imprecision is
                     							expected in the visual representation of the distance between topics.
                     							See [<a class="ref" href="#lin1991">Lin 1991</a>].</span></div>
               <div class="endnote" id="d3e683"><span class="noteRef lang en">[6] For the metrics used, see [<a class="ref" href="#arun_etal2010">Arun et al 2010</a>]
                     							[<a class="ref" href="#cao_etal2009">Cao et al 2009</a>] [<a class="ref" href="#deveaud_etal2014">Deveaud, San Juan, and Bellot 2014</a>] [<a class="ref" href="#griffiths_etal2004">Griffiths and Steyvers 2004</a>].</span></div>
               <div class="endnote" id="d3e841"><span class="noteRef lang en">[7] When dates were
                     						included as ranges, the earlier year was used. If a date was not included,
                     						or the uncertainty threshold was too great (e.g. the century was known but
                     						the year itself was not), the text was excluded from any temporal analysis,
                     						but remained in other models.</span></div>
               <div class="endnote" id="d3e850"><span class="noteRef lang en">[8] 
                     						Because LDA is a widely-used technique, it is also the most likely source of
                     						operability and efficiency gains in the topic modeling space. As an example,
                     						see the recent GPU implementation of LDA called CuLDA_CGS [<a class="ref" href="#xie_etal2019">Xie et al 2019</a>].</span></div>
               <div class="endnote" id="d3e864"><span class="noteRef lang en">[9] This methodology follows that of Hamilton,
                     						Leskovec and Juravsky [<a class="ref" href="#hamilton_etal2016">Hamilton et al 2016</a>].</span></div>
               <div class="endnote" id="d3e872"><span class="noteRef lang en">[10] In
                     						addition to plotly.js, we also used Vec2Graph in the design of the word
                     						embeddings visualizations. [<a class="ref" href="#katricheva_etal2020">Katricheva et al 2020</a>].</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="arun_etal2010"><!-- close -->Arun et al 2010</span> Arun, Rajkumar,
                  					Venkatasubramaniyan Suresh, C. E. Veni Madhavan, and Narasimha Murthy. (2010)
                  					“On finding the natural number of topics with latent
                  					dirichlet allocation: Some observations”, in <cite class="title italic">Pacific-Asia conference on knowledge discovery and data mining</cite>. Berlin,
                  					Heidelberg: Springer, pp. 391–402.</div>
               <div class="bibl"><span class="ref" id="bender_etal2021"><!-- close -->Bender et al 2021</span> Bender, Emily M., Timnit
                  					Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. (2021) “On the Dangers of Stochastic Parrots: Can Language Models Be Too
                  					Big?🦜”, in <cite class="title italic"> Proceedings of the 2021 ACM Conference
                     					on Fairness, Accountability, and Transparency</cite>, pp. 610–623.</div>
               <div class="bibl"><span class="ref" id="birhane_etal2022"><!-- close -->Birhane et al 2022</span> Birhane, Abeba, Pratyusha
                  					Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. (2022)
                  					“The values encoded in machine learning research”, in
                  					<cite class="title italic">2022 ACM Conference on Fairness, Accountability, and
                     					Transparency</cite>, pp. 173–184.</div>
               <div class="bibl"><span class="ref" id="blei_etal2007"><!-- close -->Blei and Lafferty 2007</span> Blei, David M., and John
                  					D. Lafferty. (2007) “A Correlated Topic Model of
                  					Science”, <cite class="title italic">The Annals of Applied Statistics</cite>,
                  					1(1), pp. 17–35. Available at: <a href="https://doi.org/10.1214/07-AOAS114" onclick="window.open('https://doi.org/10.1214/07-AOAS114'); return false" class="ref">https://doi.org/10.1214/07-AOAS114</a>.</div>
               <div class="bibl"><span class="ref" id="blei_etal2003"><!-- close -->Blei, Ng, and Jordan 2003</span> Blei, David M.,
                  					Andrew Y. Ng, and Michael I. Jordan. (2003) “Latent
                  					dirichlet allocation”, <cite class="title italic">Journal of machine Learning
                     					research</cite>, 3(Jan), pp. 993–1022.</div>
               <div class="bibl"><span class="ref" id="blei_etal2012"><!-- close -->Blei, Ng, and Jordan 2012</span> Blei, David M.,
                  					Andrew Y. Ng, and Michael I. Jordan. (2012) “Topic Modeling
                  					and Digital Humanities”, <cite class="title italic">Journal of Digital
                     					Humanities</cite>, 2(1).</div>
               <div class="bibl"><span class="ref" id="bode2018"><!-- close -->Bode 2018</span> Bode, Katherine. (2018) <cite class="title italic">A World of Fiction: Digital Collections and the Future of Literary
                     					History</cite>. Ann Arbor: University of Michigan Press.</div>
               <div class="bibl"><span class="ref" id="burns2013"><!-- close -->Burns 2013</span> Burns, Philip R. (2013) <cite class="title italic">Morphadorner v2: A Java Library for the Morphological Adornment of
                     					English Language Texts</cite>. Evanston, IL: Northwestern University.</div>
               <div class="bibl"><span class="ref" id="cao_etal2009"><!-- close -->Cao et al 2009</span> Cao, Juan, Tian Xia, Jintao Li,
                  					Yongdong Zhang, and Sheng Tang. (2009) “A density-based
                  					method for adaptive LDA model selection”, <cite class="title italic">Neurocomputing</cite>, 72(7–9), pp. 1775–1781.</div>
               <div class="bibl"><span class="ref" id="chun2008"><!-- close -->Chun 2008</span> Chun, Wendy Hui Kyong. (2021)
                  					Discriminating data: Correlation, neighborhoods, and the new politics of
                  					recognition. MIT Press.</div>
               <div class="bibl"><span class="ref" id="crother_etal2008"><!-- close -->Crowther et al 2008</span> Crowther, Stefania, et
                  					al. (2008) “New Scholarship, New Pedagogies: Views from the
                  					‘EEBO Generation’”, <cite class="title italic"> Early Modern Literary
                     					Studies</cite>, 14(2), pp. 3–1.</div>
               <div class="bibl"><span class="ref" id="da2019"><!-- close -->Da 2019</span> Da, Nan Z. (2019) “The
                  					computational case against computational literary studies”, <cite class="title italic">Critical inquiry</cite>, 45(3), pp. 601–639.</div>
               <div class="bibl"><span class="ref" id="deveaud_etal2014"><!-- close -->Deveaud, San Juan, and Bellot 2014</span> Deveaud,
                  					Romain, Eric San Juan, and Patrice Bellot. (2014) “Accurate
                  					and effective latent concept modeling for ad hoc information retrieval”,
                  					Document numérique, 17(1), pp. 61–84.</div>
               <div class="bibl"><span class="ref" id="froehlich_etal2012"><!-- close -->Froehlich, Whitt, and Hope 2012</span> Froehlich,
                  					Heather, Richard J. Whitt, and Jonathan Hope. (2012) <cite class="title italic">EEBO-TCP as a Tool for Integrating Teaching and Research</cite>. Available at:
                  					<a href="https://ora.ox.ac.uk/objects/uuid:3a7c3b8f-cdac-4bdd-8b6c-33caa6f04179" onclick="window.open('https://ora.ox.ac.uk/objects/uuid:3a7c3b8f-cdac-4bdd-8b6c-33caa6f04179'); return false" class="ref">https://ora.ox.ac.uk/objects/uuid:3a7c3b8f-cdac-4bdd-8b6c-33caa6f04179</a>.</div>
               <div class="bibl"><span class="ref" id="gardenfors2014"><!-- close -->Gärdenfors 2014</span> Gärdenfors, Peter. (2014) <cite class="title italic">The Geometry of Meaning:
                     					Semantics Based on Conceptual Spaces</cite>. Massachusetts: MIT press.</div>
               <div class="bibl"><span class="ref" id="garg2018"><!-- close -->Garg et al 2018</span> Garg, Nikhil, et al. (2018) “Word Embeddings Quantify 100 Years of Gender and Ethnic
                  					Stereotypes”, <cite class="title italic">Proceedings of the National Academy of
                     					Sciences</cite>, 115(16), pp. 3635–44. Available at: <a href="https://doi.org/10.1073/pnas.1720347115" onclick="window.open('https://doi.org/10.1073/pnas.1720347115'); return false" class="ref">https://doi.org/10.1073/pnas.1720347115</a>.</div>
               <div class="bibl"><span class="ref" id="gebru_etal2021"><!-- close -->Gebru et al 2021</span> Gebru, Timnit, Jamie
                  					Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal
                  					Daumé Iii, and Kate Crawford. (2021) “Datasheets for
                  					datasets”, <cite class="title italic">Communications of the ACM</cite>,
                  					64(12), pp. 86–92.</div>
               <div class="bibl"><span class="ref" id="gerow_etal2018"><!-- close -->Gerow et al 2018</span> Gerow, Aaron, et al. (2018)
                  					“Measuring Discursive Influence across Scholarship”,
                  					<cite class="title italic"> Proceedings of the National Academy of Sciences</cite>,
                  					115(13), pp. 3308–13. Available at: <a href="https://doi.org/DOI.org" onclick="window.open('https://doi.org/DOI.org'); return false" class="ref">https://doi.org/DOI.org</a>.</div>
               <div class="bibl"><span class="ref" id="gerrish_etal2009"><!-- close -->Gerrish and Blei 2009</span> Gerrish, Sean, and
                  					David Blei. (2009) <cite class="title italic">Modeling Influence in Text
                     					Corpora</cite>. Available at: <a href="https://cdn.tc-library.org/Rhizr/Files/XtymaxJvoXXM89ffG/files/document_influence_model_1.pdf" onclick="window.open('https://cdn.tc-library.org/Rhizr/Files/XtymaxJvoXXM89ffG/files/document_influence_model_1.pdf'); return false" class="ref">https://cdn.tc-library.org/Rhizr/Files/XtymaxJvoXXM89ffG/files/document_influence_model_1.pdf</a>.</div>
               <div class="bibl"><span class="ref" id="griffiths_etal2004"><!-- close -->Griffiths and Steyvers 2004</span> Griffiths,
                  					Thomas L., and Mark Steyvers. (2004) “Finding scientific
                  					topics”, <cite class="title italic">Proceedings of the National academy of
                     					Sciences</cite>, 101(suppl_1), pp. 5228–5235.</div>
               <div class="bibl"><span class="ref" id="gupta_etal2021"><!-- close -->Gupta, Chaudhary, and Schütze 2021</span> Gupta,
                  					Pankaj, Yatin Chaudhary, and Hinrich Schütze. (2021) “Multi-source Neural Topic Modeling in Multi-view Embedding
                  					Spaces”.</div>
               <div class="bibl"><span class="ref" id="hamilton_etal2016"><!-- close -->Hamilton et al 2016</span> Hamilton, William L.,
                  					et al. (2016) “Diachronic Word Embeddings Reveal Statistical
                  					Laws of Semantic Change”, in Proceedings of the 54th Annual Meeting of
                  					the Association for Computational Linguistics (Volume 1: Long Papers.
                  					Association for Computational Linguistics, pp. 1489–501. Available at: <a href="https://doi.org/10.18653/v1/P16-1141" onclick="window.open('https://doi.org/10.18653/v1/P16-1141'); return false" class="ref">https://doi.org/10.18653/v1/P16-1141</a>.</div>
               <div class="bibl"><span class="ref" id="hellrich2019"><!-- close -->Hellrich 2019</span> Hellrich, Johannes. (2019) <cite class="title italic">Word embeddings: reliability &amp; semantic change</cite>. IOS
                  					Press.</div>
               <div class="bibl"><span class="ref" id="hellrich_etal2018"><!-- close -->Hellrich et al 2018</span> Hellrich, Johannes, et
                  					al. (2018) “JeSemE: A Website for Exploring Diachronic
                  					Changes in Word Meaning and Emotion”. Available at: <a href="http://arxiv.org/abs/1807.04148" onclick="window.open('http://arxiv.org/abs/1807.04148'); return false" class="ref">http://arxiv.org/abs/1807.04148</a>.</div>
               <div class="bibl"><span class="ref" id="jo_etal2020"><!-- close -->Jo and Gebru 2020</span> Jo, Eun Seo, and Timnit Gebru.
                  					(2020) “Lessons from archives: Strategies for collecting
                  					sociocultural data in machine learning”, in <cite class="title italic">Proceedings of the 2020 conference on fairness, accountability, and
                     					transparency</cite>, pp. 306–316.</div>
               <div class="bibl"><span class="ref" id="katricheva_etal2020"><!-- close -->Katricheva et al 2020</span> Katricheva, N.,
                  					Yaskevich, A., Lisitsina, A., Zhordaniya, T., Kutuzov, A., Kuzmenko, E. (2020)
                  					“Vec2graph: A Python Library for Visualizing Word Embeddings
                  					as Graphs”, <cite class="title italic">In</cite>, 1086. Available at: <a href="https://doi.org/10.1007/978-3-030-39575-9_20" onclick="window.open('https://doi.org/10.1007/978-3-030-39575-9_20'); return false" class="ref">https://doi.org/10.1007/978-3-030-39575-9_20</a>.</div>
               <div class="bibl"><span class="ref" id="kim_etal2014"><!-- close -->Kim et al 2014</span> Kim, Yoon, et al. (2014) “Temporal Analysis of Language through Neural Language
                  					Models”, in Proceedings of the ACL 2014 Workshop on Language Technologies
                  					and Computational Social Science. Association for Computational Linguistics,
                  pp.
                  					61–65. Available at: <a href="https://doi.org/10.3115/v1/W14-2517" onclick="window.open('https://doi.org/10.3115/v1/W14-2517'); return false" class="ref">https://doi.org/10.3115/v1/W14-2517</a>.</div>
               <div class="bibl"><span class="ref" id="kulkarni_etal2015"><!-- close -->Kulkarni et al 2015</span> Kulkarni, Vivek, et al.
                  					(2015) “Statistically Significant Detection of Linguistic
                  					Change”, in <cite class="title italic"> Proceedings of the 24th International
                     					Conference on World Wide Web, International World Wide Web Conferences Steering
                     					Committee</cite>, pp. 625–35. Available at: <a href="https://doi.org/10.1145/2736277.2741627" onclick="window.open('https://doi.org/10.1145/2736277.2741627'); return false" class="ref">https://doi.org/10.1145/2736277.2741627</a>.</div>
               <div class="bibl"><span class="ref" id="kusner_etal2015"><!-- close -->Kusner 2015</span> Kusner, M. J., Sun, Y., Kolkin,
                  					N. I., and Weinberger, K. Q. (2015) “From Word Embeddings to
                  					Document Distances”, in <cite class="title italic">Proceedings of the 32nd
                     					International Conference on Machine Learning, ICML</cite>, pp. 957-966.</div>
               <div class="bibl"><span class="ref" id="lesser2019"><!-- close -->Lesser 2019</span> Lesser, Zachary. (2019) “Xeroxing the Renaissance: The Material Text of Early Modern
                  					Studies”, <cite class="title italic">Shakespeare Quarterly</cite>, 70(1), pp.
                  					3–31. Available at: <a href="https://doi.org/10.1093/sq/quz001" onclick="window.open('https://doi.org/10.1093/sq/quz001'); return false" class="ref">https://doi.org/10.1093/sq/quz001</a>.</div>
               <div class="bibl"><span class="ref" id="lin1991"><!-- close -->Lin 1991</span> Lin, Jianhua. (1991) “Divergence Measures Based on the Shannon Entropy”, <cite class="title italic">IEEE Transactions on Information Theory</cite>, 37(1), pp. 145–151.</div>
               <div class="bibl"><span class="ref" id="mccollough_etal2019"><!-- close -->McCollough and Lesser 2019</span> McCollough,
                  					Aaron, and Zachary Lesser. (2019) <cite class="title italic">Xeroxing the Renaissance:
                     					The Material Text of Early Modern Studies</cite>. Folger Shakespeare
                  					Library.</div>
               <div class="bibl"><span class="ref" id="meeks_etal2012"><!-- close -->Meeks and Weingart 2012</span> Meeks, Elijah, and
                  					Scott B. Weingart. (2012) “The digital humanities
                  					contribution to topic modeling”, <cite class="title italic">Journal of Digital
                     					Humanities</cite>, 2(1), pp. 1–6.</div>
               <div class="bibl"><span class="ref" id="mikolov_etal2013"><!-- close -->Mikolov et al 2013</span> Mikolov, Tomas, et al.
                  					(2013) “Efficient Estimation of Word Representations in
                  					Vector Space”. Available at: <a href="http://arxiv.org/abs/1301.3781" onclick="window.open('http://arxiv.org/abs/1301.3781'); return false" class="ref">http://arxiv.org/abs/1301.3781</a>.</div>
               <div class="bibl"><span class="ref" id="mueller2012"><!-- close -->Mueller 2012</span> Mueller, Martin. (2012) <cite class="title italic">Towards a Book of English: A Linguistically Annotated Corpus of
                     					the EEBO-TCP Texts</cite>.</div>
               <div class="bibl"><span class="ref" id="nikita2016"><!-- close -->Nikita 2016</span> Nikita, Murzintcev. (2016) “Ldatuning: Tuning of the Latent Dirichlet Allocation Models
                  					Parameters”. R Package Version 0.2-0, URL https://CRAN, R-project.
                  					org/package= ldatuning.</div>
               <div class="bibl"><span class="ref" id="pechenick_etal2015"><!-- close -->Pechenick et al 2015</span> Pechenick, Eitan
                  					Adam, Christopher M. Danforth, and Peter Sheridan Dodds. (2015) “Characterizing the Google Books corpus: Strong limits to
                  					inferences of socio-cultural and linguistic evolution”, <cite class="title italic">PloS one</cite>, 10(10).</div>
               <div class="bibl"><span class="ref" id="roberts_etal2019"><!-- close -->Roberts, Stewart, and Tingley 2019</span> Roberts,
                  					Margaret E., Brandon M. Stewart, and Dustin Tingley. (2019) “Stm: An R package for structural topic models”, <cite class="title italic">Journal of Statistical Software</cite>, 91, pp. 1–40.</div>
               <div class="bibl"><span class="ref" id="schmidt_etal2021"><!-- close -->Schmidt et al 2021</span> Schmidt et al. (2021)
                  					“Uncontrolled Corpus Composition Drives an Apparent Surge in
                  					Cognitive Distortions”, <cite class="title italic">PNAS</cite>, 118(45), pp.
                  					1–2.</div>
               <div class="bibl"><span class="ref" id="shoemaker2020"><!-- close -->Shoemaker 2020</span> Shoemaker, Tyler. (2020) <cite class="title italic">Literalism: Reading Machines Reading</cite>. UC Santa Barbara.
                  					(Dissertation).</div>
               <div class="bibl"><span class="ref" id="sievert_etal2014"><!-- close -->Sievert and Shirley 2014</span> Sievert, Carson,
                  					and Kenneth Shirley. (2014) “LDAvis: A Method for
                  					Visualizing and Interpreting Topics”, in Proceedings of the Workshop on
                  					Interactive Language Learning, Visualization, and Interfaces. Association for
                  					Computational Linguistics, pp. 63–70. Available at: <a href="https://doi.org/10.3115/v1/W14-3110" onclick="window.open('https://doi.org/10.3115/v1/W14-3110'); return false" class="ref">https://doi.org/10.3115/v1/W14-3110</a>.</div>
               <div class="bibl"><span class="ref" id="staley2015"><!-- close -->Staley 2015</span> Staley, David J. (2015) <cite class="title italic">Computers, visualization, and history: How new technology will
                     					transform our understanding of the past</cite>. Routledge.</div>
               <div class="bibl"><span class="ref" id="wang_etal2018"><!-- close -->Wang et al 2018</span> Wang, Wenlin, et al. (2018)
                  					“Topic Compositional Neural Language Model”, in
                  					<cite class="title italic">International Conference on Artificial Intelligence and
                     					Statistics</cite>. PMLR.</div>
               <div class="bibl"><span class="ref" id="welzenbach2012"><!-- close -->Welzenbach 2012</span> Welzenbach, Rebecca. (2012)
                  					“Transcribed by Hand, Owned by Libraries, Made for Everyone:
                  					EEBO-TCP in 2012”.</div>
               <div class="bibl"><span class="ref" id="whitmore2010"><!-- close -->Whitmore 2010</span> Whitmore, Michael. (2010) “Text: A Massively Addressable Object”. Available at: <a href="http://winedarksea.org/?p=926" onclick="window.open('http://winedarksea.org/?p=926'); return false" class="ref">http://winedarksea.org/?p=926</a>.</div>
               <div class="bibl"><span class="ref" id="xie_etal2019"><!-- close -->Xie et al 2019</span> Xie, Xiaolong, Yun Liang, Xiuhong
                  					Li, and Wei Tan. (2019) “CuLDA: solving large-scale LDA
                  					Problems on GPUs”, in <cite class="title italic"> Proceedings of the 28th
                     					International Symposium on High-Performance Parallel and Distributed
                     					Computing</cite>, pp. 195–205.</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>