!<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!-- article title in English --></title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>first name(s) <dhq:family>family name</dhq:family></dhq:author_name>
                    <idno type="ORCID"><!-- if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000 --></idno>
                    <dhq:affiliation></dhq:affiliation>
                    <email></email>
                    <dhq:bio><p></p></dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt><publisher>Alliance of Digital Humanities Organizations</publisher>
<publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id"><!-- including leading zeroes: e.g. 000110 --></idno>
                <idno type="volume"><!-- volume number, with leading zeroes as needed to make 3 digits: e.g. 006 --></idno>
                <idno type="issue"><!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date></date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
<!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->                    
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            	<taxonomy xml:id="project_keywords">
            		<bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
            	</taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->
                	
                	<!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp=""/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item></item>
                    </list>
                </keywords>
            	<keywords scheme="#project_keywords">
            		<list type="simple">
            			<item></item>
            		</list>
            	</keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
        	<!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
        	<change>The version history for this file can be found on <ref target=
        		"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/NNNNNN/NNNNNN.xml">GitHub
        	</ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
   In this paper, we present a case study on segmentation, granularity and interpretation depth as quality criteria for the robustness of categories in pragmalinguistic tagset development. Our use case is a classification task applied to linguistic routines of discourse referencing in the plenary minutes of the German Bundestag. By discourse referencing we mean explicit and implicit intertextual references to previous utterances. We further distinguish discourse references in which the source is named from those in which it is not. With machine learnability of categories in mind, our focus is on principles and conditions of category development in collaborative annotation. These prerequisites provide the basis for experiments with different<hi rend="color(212121)" xml:space="preserve"> machine learning frameworks to automatically predict labels from our tag set. We apply BERT (Devlin et al., 2019), a pretrained neural transformer language models which we finetune and constrain for our labeling and classification tasks, and compare it against Naïve Bayes as a probabilistic knowledge-agnostic baseline model.</hi> The goal of our experiments and tests on pilot corpora is to investigate to which extent statistical measures indicate whether interpretative classifications are machine-reproducible and reliable. For this purpose, we compare gold-standard datasets annotated with different segment sizes (phrases, sentences) and categories with different granularity, respectively. The results from these experiments contribute to the development and reflection of our category systems.
                <p></p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p></p>
            </dhq:teaser>
        </front>
         <body>
            <head></head>
                <div>
                    <head></head>
                    <p>In this paper, we present a case study on the development of annotation categories in linguistic discourse research. Our use case is discourse referencing practices in parliamentary debates. By discourse referencing we mean sentences with which a speaker refers to utterances preceding in the discourse. Thus, we study intertextual references to oral utterances and written texts. While this topic is important in its own right for understanding the mechanism of parliamentary discourse, here we focus on the methodological aspect of category development with respect to automatic detection of such references in large datasets. For this purpose, we conduct a collaborative annotation study and run experiments with probabilistic classifiers such as Naive Bayes (Jurafsky and Martin, 2022) and transformer language models such as BERT (Devlin et al., 2019). We obtain the dataset for our case study from the linguistically preprocessed corpus of the plenary minutes of the German Bundestag. </p>

                    <p>In linguistic heuristics, discourse referencing belongs to pragmatics because it involves linguistic practices whose function can only be inferred based on contextual knowledge. Pragmalinguistic phenomena in general confront computational processing and machine learning with a particular challenge: The capture of implicit and inferred aspects as well as the inclusion of contextual knowledge is of central importance. This also poses a particular difficulty in annotation studies with an automation perspective (Archer et al., 2008, p. 615). Discourse referencing, however, may neither appear to require deep interpretation nor be characterized by implicitness at first glance. In fact, some forms are easily detectable on the linguistic surface. Consider, for example, explicitly marked quotations or communication verbs. However, discourse referencing can also be indicated implicitly. Formulations, such as “we offered opportunities” (1) in the following example require interpretation based on contextual knowledge in order to be identified as practices of discourse referencing:</p>

                    <list rend="numbered"><item><hi rend="italic" xml:space="preserve">With your behaviour [...] you have placed yourself in an improper proximity to your neighbours here further to the right. </hi></item></list><p rend="List Paragraph" style="text-align: justify;">[Sie haben sich mit Ihrem Verhalten […] in eine ungute Nähe zu Ihren Nachbarinnen und Nachbarn hier weiter rechts begeben.]</p>


   <p>Such contextual and interpretative phenomena cannot be easily captured by corpus linguistic or algorithmic access to the linguistic surface, making them difficult to analyze in an automated way. One approach to solving this problem is to combine interpretive-categorizing annotation and machine learning. 
In summary, the central challenge in deductive-inductive category development is this: to capture the phenomena under investigation as precisely as possible and at the same time maintain a certain balance of granularity and variance in the category contents. 
</p>


   <p>In the following, we first provide an overview of preliminary work on category design in pragmalinguistics and linguistic discourse research. We focus on already known success factors in the formal and contextual tailoring of categories.  Then, we introduce the pragmatic phenomenon of discourse referencing and describe the properties that are relevant in our heuristic model building. Subsequently, we describe and discuss our dataset and the collaborative annotation of discourse referencing practices in terms of assumptions, process, and results. The annotation process consists of two phases: 1. We test the aspect of granularity of categorization by applying a binary classification task (discourse referencing present or not). 2. In a second step, we tag our data in a more fine-grained way, focusing on actors of referenced utterances. Additionally, we extract phrases that have been identified to indicate discourse referencing. Furthermore, we ran linguistic experiments using probabilistic and neural classifiers to detect discourse referencing. In these experiments, we test the influence of different input data in terms of taxonomies (category number) and segment sizes (phrase input vs. sentence input). Eventually, we discuss our results with respect to the question of category design and conclude with a summary.</p>


   <p><hi rend="normalweight">Capturing discourse referencing by annotation</hi>Criteria for the development of machine-learnable categories in a pragmalinguistic annotation approach<p style="text-align: justify;"><hi style="font-size:12pt">In pragmalinguistic annotation, various issues, aspects, and criteria for the development of category systems have been discussed as important for adequately capturing relevant phenomena. Archer, Culpeper and Davies (2008, p. 615) refer to the following levels of pragmatic information relevant to category development: the formal, the illocutionary, the implied/inferred, the interactional and the contextual level. The consistent consideration of these level differences is seen as an important criterion for the design of annotation schemes. In particular, Archer, Culpeper and Davies (2008, p. 633) highlight segmentation: “Segmentation requires us not only to state what unit we will be analysing, but also to define it in a way that will enable us to measure one unit against another, and, by so doing, ensure a level of consistency.”  This aspect has been described as an important quality criterion in other works in the field as well, e.g., in the annotation of speech acts (c.f. Leech and Weisser, 2003). Teufel also addresses the segmentation problem – from a more computational linguistic point of view –she reflects on the difficulty of assigning abstract categories to linguistic units. She also addresses the problem that categories can overlap, but is critical of multiple annotation with regard to the evaluability (cf. Teufel, 1999, p. 108). Instead, opts for selective annotation with exclusive categories and consistent segmentation  (Teufel, 1999, p. 111; cf. Weisser, 2018, pp. 213–277).</p>


   <p>These aspects – consistent segmentation and a distinctive category system – have likewise proven crucial in our previous studies on pragmalinguistic annotation, also with respect to the combination of pragmatic annotation and machine learning. In addition to these two aspects, we have worked out the factors of granularity and context sensitivity / depth of interpretation
in prior studies (Becker, Bender, and Müller, 2020; Bender 2022). We developed a complex annotation scheme with pragmalinguistic categories at different levels of granularity. The objects were academic text routines. We used this scheme to manually annotate sentences in a corpus of texts from different academic disciplines in order to train a recurrent neural network for classifying text routines. The experiments showed that the annotation categories are robust enough to be recognized by the model, which learns similarities between sentence surfaces represented as vectors. Nevertheless, the accuracy of the model depended strongly on the granularity of the category level (ibid., pp. 450–455).
</p>


   <p>In general, pragmalinguistic questions raise the challenge of operationalizing and segmenting phenomena that are context-dependent rather than bound to a formal segment. In a great number of cases, discourse referencing acts can be delimited to certain phrases. However, there are cases – e.g., certain anaphoric references – where the indicators of discourse referencing can only be fully captured in the extended cotext. Thus, in addition to the aspect of segmentation consistency, the granularity of segmentation and the size of the cotext window is also important. 
Granularity, in addition to distinctiveness, is relevant not only for the segmentation but also for the robustness of the category system. It plays a role for the semantic and pragmatic content of the categories in annotation schemes. The granularity of the tag set influences the accuracy of the algorithm (cf. ibid., p. 455). This does not mean that schemes with few categories or tags are always better. Rather, it is important to capture a certain phenomenon as good as possible through the operationalization in the scheme and to make it analyzable at first. Secondly, insufficiently differentiated tag sets lead to overly heterogenous categories, which in turn limits machine learnability.
</p>

   <p>The annotation guidelines need to consider such indicators. For instance, they need to specify exactly how much communicative and contextual knowledge may be included and how deeply it may be interpreted in order to determine whether an utterance is a reference to a communicative act – even if this is not made explicit through appropriate lexis (see example in the introduction).</p>


   <p>To achieve agreement in the annotation process, one must reach explicit common ground on the depth of interpretation when assigning segments to categories. The more context available to annotators, the more they will interpretively work out what was "actually" meant by a sentence, and the higher the risk that annotators will disagree. Therefore, it may be useful to deliberately limit the co-textual information. Categories designed to be distinctive (allowing no overlap of categories) and exhaustive (covering the whole variety of phenomena in the data), have proven to optimize machine learning (ibid., p. 430). This robustness can be evaluated by calculating the Inter-annotator agreement (Artstein and Poesio, 2008, pp. 555–596). The above-mentioned factors also represent quality criteria for the explicitness and intersubjective comprehensibility of interpretative categorizations in annotation studies, i.e., they determine whether categorizations are compatible with machine learning, for one, and comprehensible for human addressees, such as other annotators or recipients of the respective study, for another. Besides this, the accuracy values of the different algorithmic models we will test represent verification results. </p>


   <p>In summary, our category development considers the factors of segmentation, granularity, distinctiveness and context sensitivity / depth of interpretation on different levels as well as in their mutual interaction with regard to the machine learnability of the category system in experiments. In this study, we draw on these findings and test the effects of changes in these factors as well as their impact in various experiments (on the Inter-annotator agreement and the learning success of different algorithmic models). Furthermore, we test whether the trained algorithmic models cope better with sentence segmentation or with phrase-level segmentation.</p>

   <head>Linguistic routines of discourse referencing </head>
   <p>By discourse referencing, we mean making reference to a communicative act preceding in discourse (Müller, 2007, p. 261; Feilke, 2012, p. 19). We are thus dealing with particular cases of intertextuality (Allen, 2000). These are characterized by concrete and explicit references to other communicates, which can be called ‘texts’ in a broad sense. They include not only pre-texts such as laws, templates, drafts, and policy papers, but also oral utterances. For all cases, the referenced act is in the past from the speaker’s point of view. The reference can be uttered as a complete proposition, as a verbal phrase (VP), as a noun phrase (NP), with the subject of the utterance fully named, metonymically named, or without naming the subject of the utterance. Discourse referents in this sense are constitutive of many genres, e.g., academic or legal discourse.</p>

   <p>Our case study examines practices of discourse referencing in parliamentary debates.  Communicative practices in parliaments are fundamentally relevant for understanding the mechanisms of Western parliamentary democracies. But discourse references in parliamentary discourse also have functions that are interesting in terms of linguistic systematics: First, they serve to orient and co-orient political statements in different discourses (citation 2; e.g., the parliamentary-procedural, the economic, the academic); second, they are used to index institutional and situational coalitions or oppositions (3); and third, they are used to invoke the legal basis of parliamentary action (4; laws, directives, regulations). In the sense of this last point, discourse references serve to recall the distinguished function of the parliamentary arena as a laboratory in which the legal framework of our social life is forged.</p>


<list rend="numbered"><item><hi rend="italic">Those who say this are subject to an essential misjudgement, because they do not know or misjudge what great added value it means in terms of acceptance and industrial peace when important decisions are discussed beforehand in the works council and then implemented together in the company</hi>. </item></list><p rend="List Paragraph" style="text-align: justify;">[Die, die das äußern, unterliegen einer wesentlichen Fehleinschätzung; denn sie wissen nicht oder schätzen falsch ein, welchen großen Mehrwert es im Hinblick auf Akzeptanz und Betriebsfrieden bedeutet, wenn wichtige Entscheidungen zuvor im Betriebsrat besprochen und dann gemeinsam im Betrieb umgesetzt werden.]</p><list rend="numbered"><item><hi rend="italic">The suitable and also still possible minimal invasive solution in the remaining weeks is an opening of the contribution guarantee, which leads also according to the opinion of the science to more net yield and more security.</hi> </item></list><p rend="List Paragraph" style="text-align: justify;">[Die passende und auch noch mögliche minimalinvasive Lösung in den verbleibenden Wochen ist eine Öffnung der Beitragsgarantie, die auch nach Meinung der Wissenschaft zu mehr Rendite und mehr Sicherheit führt.]</p><list rend="numbered"><item><hi rend="italic">Please read the act first, before you argue in a populist way here</hi><hi style="font-size:12pt">.</hi></item></list><p rend="List Paragraph" style="text-align: justify;">[Lesen Sie doch bitte erst das Gesetz, bevor Sie hier populistisch argumentieren.]</p>

   <p>One can see from these first examples that the focus and concreteness of the intertextual reference varies considerably. (2) contains a reference to a concrete and theoretically precisely determinable group of speakers antecedent in the discourse, but introduced into the discourse only unspecifically (those who say this). In (3), there is a similarly unspecific reference that is introduced with a metonymic shift (according to the opinion of the science instead of ‘according to the opinion of some academic scholars who are concerned with this issue’). In (4), a legal statute is referred to as the manifest result of a communicative act, without addressing the actors involved in the writing of the statute at all. Such a reference to texts as instances independent of the author, as it were autonomously effective, is a common rhetorical procedure in parliamentary debates.</p>

   <p>In other cases, of course, utterances refer to concrete empirical persons. These can be groups (see Example 5), or even individuals (6). Besides this, there are (albeit rare) cases in which reference is made to a text preceding in the discourse, such that the text itself takes the place of the actor in a communicative action (7). These metonymic shifts are interesting because they give a different hue to the action structure of the discourse that is being produced by means of discourse referencing: the cognitive focus, the claim of validity, and also the authority are shifted from the author to the text in such cases. Methodologically, what is interesting here is the extent to which such metonymic constructions can be found automatically, especially since they are rare.</p>

<list rend="numbered"><item><hi rend="italic">After all, the concern of the democratic opposition groups is a correct one</hi>. </item></list><p rend="List Paragraph" style="text-align: justify;">[Denn das Anliegen der demokratischen Oppositionsfraktionen ist ja ein richtiges.]</p><list rend="numbered"><item><hi rend="italic">Ladies and gentlemen, Kohl, a historian by training, once said: “Those who do not know the past cannot understand the present and cannot shape the future.”</hi></item></list><p rend="List Paragraph" style="text-align: justify;">[Meine Damen und Herren, der gelernte Historiker Kohl hat einmal gesagt: „Wer die Vergangenheit nicht kennt, kann die Gegenwart nicht verstehen und die Zukunft nicht gestalten.”]</p><list rend="numbered"><item><hi rend="italic">The report confirms: Inner cities are losing their individuality and thus their attractiveness</hi></item></list><p rend="List Paragraph" style="text-align: justify;">[Der Bericht bestätigt: Die Innenstädte verlieren ihre Individualität und damit Attraktivität.]</p>

      <p>We exemplify our methodological considerations and experiments on category design with the following research questions: 1. Which types of discourse referents occur in our data set and in which distribution? 2. What role do actors play in discourse referencing? That is, when are the speakers and writers of utterances explicitly named, and when, instead, in a metonymic thrust, does the text itself move into the position of actor (as in evidence 7)?</p>

     <head><hi rend="normalweight">Dataset and annotation workflow</hi></head><div><head>Dataset</head>To investigate discourse referencing in parliamentary discourse, we draw on the plenary minutes of the German Bundestag. Discourse Lab (Müller, 2022a) hosts a linguistically processed and metadata-enriched corpus of the plenary minutes that currently covers the period from 1949 to May 2021, i.e., all completed election periods from 1 to 19. The corpus contains about 810,000 texts (debate contributions) and about 260 million tokens. It is expanded at regular intervals with current data (Müller, 2022b) which is provided by the German Bundestag (https://www.bundestag.de/services/opendata). Pre-processing includes tokenization, sentence segmentation, lemmatization, part-of-speech tagging, marking of speakers’ party affiliation, and separate marking of heckling. This way, speeches with and without heckling or even heckling separately can be searched. The basic unit (&lt;text&gt;) of the corpus is the parliamentary speech. It is subclassified by speakers’ texts &lt;sp&gt; and heckling &lt;z&gt;. Text attributes are: speaker’s fraction, year, month, speaker, session, legislative period, text ID and day of the week. The corpus is managed via the IMS Corpus Workbench (Evert and Hardie, 2011). For our categorization experiment, we draw a random sample of 6000 sentences from the May 5-7, 2021 plenary transcripts. We exclude hecklings in the process. The sample is homogeneous across time and actors: Since our study is about methodological experiments on category formation, the variation of parameters should be controlled. With the sample design, we exclude diachronic variation and variation caused by changing groups of actors. We include various types of discourse referencing in that our dataset covers functional, thematic, and interpersonal variation.</hi></p></div><div><head>Collaborative annotation</head>

     <p>The first part of our experimental annotation study on discourse referencing deals with collaborative manual annotation. We consider collaborative annotation to mean not only that several annotators assign categories, but also that categories and guidelines are developed in a team (cf. Bender and Müller, 2020; Bender, 2020). The understanding of discourse referencing described in chapter 3 requires - at least in less explicit cases - linguistic expertise. Thus, we cannot simply assume everyday linguistic intuition to be sufficient, but must develop criteria and guidelines and make them available to annotators, or at least train them to some extent in the application of the guidelines. Of course, it is best to involve all annotators in the development of the categories as well, if possible. We have been able to do this, at least in part, in the study described here. For this purpose, we discussed the theoretical concept of categorization in the team and, on this basis, first established criteria for assigning categories to segments.
          </p>
<p>The basic unit of annotation was set to be sentences. The reason for this is that linguistic actions are typically represented in sentences. Co-textual information was intentionally narrowed down in this study by extracting individual sentences and making them available to annotators in random order. No overlap of categories was allowed in annotation. The next smaller unit in the linguistic system is phrases, which were used in this case for the extraction of classification-relevant indicators. Evident indicators of discourse referencing are phrases with communication verbs and noun phrases that introduce sources of referenced utterances (i.e. authors, speakers). Other - context-sensitive - indicators were identified in the collaborative data analysis in the course of pilot annotations. For example, discourse references in parliamentary discourse are also made with action verbs in conjunction with nominal mentions of texts or utterances (e.g., “with the draft we initiated the debate”).</p>
<p>After determining relevant categories deductively, trial annotations were carried out. The category system was revised inductively in a data-driven manner and team members discussed cases of doubt. The abductive differentiation or reconfiguration of the schema is necessary when in the course of annotation the assignment of text segments (Peirce 1903, CP 7.677 calls it "percept") to categories ("percipuum", ibid.) by qualitative induction fails. In our annotation process, however, we understand this new construction or configuration not as a result of purely individual insights, but as a collaborative-discursive process of negotiating categories that are plausible for all annotators.</p>
<p>An additional goal that made this collaborative discursive negotiation process of category system development even more complex was to combine a linguistic analysis perspective with computational linguistic expertise in order to better anticipate what different machine learning algorithms can capture. For example, we decided against annotating verbatim quotations and indirect speech because we wanted to train the algorithmic models primarily on indicators which show that referencing is taking place, instead of focusing on what is being referenced. After all, the formation of linguistic routines occurs at the level of referencing, while what is referenced can vary indefinitely. Since we aim to discuss the question of category design at the intersection of disciplinary heuristics and machine-learning, we have developed an annotation workflow that allows us to conduct machine-learning experiments on categories of varying complexity in terms of form and content.
We decided on different levels of annotation complexity for which we developed the appropriate categories:</p>


<table rend="rules"><row><cell style="text-align: justify;">Annotation step</cell><cell style="text-align: justify;">Complexity level</cell><cell style="text-align: justify;">Category</cell><cell style="text-align: justify;">Segment</cell><cell style="text-align: justify;">Classification decision</cell><cell style="text-align: justify;">Possible numbers of segments per instance</cell></row><row><cell style="text-align: justify;">1</cell><cell style="text-align: justify;">1</cell><cell style="text-align: justify;">discourse referencing</cell><cell style="text-align: justify;">sentence</cell><cell style="text-align: justify;">yes/no</cell><cell style="text-align: justify;">1</cell></row><row><cell style="text-align: justify;">2a</cell><cell style="text-align: justify;">2</cell><cell style="text-align: justify;">mention of the source (author/speaker) of the referenced utterance</cell><cell style="text-align: justify;">sentence</cell><cell style="text-align: justify;">explicit/metonymic/none</cell><cell style="text-align: justify;">1</cell></row><row><cell style="text-align: justify;">2b</cell><cell style="text-align: justify;">3</cell><cell style="text-align: justify;">discourse referencing</cell><cell style="text-align: justify;">phrase</cell><cell style="text-align: justify;">yes/no</cell><cell style="text-align: justify;"><hi style="font-size:12pt">n</hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt">Table 1: Manual annotation - workflow</hi></p>

<p>Table 1 presents the different annotation steps, which are designed according to increasing complexity: Step 1 is a binary classification task with two labels. In step 2, we ran 2 annotation tasks at the same time. First, different types of thematization of authors/speakers of the text and oral utterances were classified - at the sentence level: explicit/metonymic/non. Second, within the sentences that were already classified as discourse referencing, those phrases that were relevant to the classification decision were identified (see Table 2). This step requires accurate annotation of phrases representing relevant actors, actions and products. Even though step 2b is a binary classification task, the decisions required for classification are even more complex because any number of segments can be annotated for each instance and the three-item classification from step 2a is presupposed.</p>

<p>The first annotation phase consisted of a binary classification task that required distinguishing between sentences with and without discourse referencing. According to this criterion, all 6000 sentences of the corpus sample were double annotated (sentences as segments). Teams of two performed the annotation of 3000 sentences each in Excel spreadsheets independently. The sentences were arranged in random order to avoid possible context effects. After double annotation, the inter-annotator agreement was calculated based on Cohen's kappa (Cohen 1960). Agreement scores varied among groups in the first run. In group 1, 2566 of 2919 sentences were annotated in agreement (88%, Cohen’s kappa: 72.87), in group 2, 2408 of 2883 (83.5%, Cohen’s kappa: 57.44). The difference in kappa score between the groups is linked to the fact that in group 2 the rarer label ('+ discourse referencing') was assigned less frequently in agreement (in 487 cases), due to a misunderstanding that became apparent late in the annotation process. This had a disproportionately large impact on the calculation of agreement statistics using Cohen’s Kappa. That is because more infrequent labels are calculated to have a lower probability of overruling annotations by random chance than high-frequency ones. Cohen’s kappa is designed to compute the randomly corrected matches of annotations from different annotators. This way, it expresses a ratio between the randomly expected agreement and the actually observed agreement, assuming that annotators can also assign the same label to an instance by random chance with a certain probability (cf. Greve and Wentura, 1997, p. 111; Zinsmeister et al., 2008, pp. 765 f.).</p>

<p>The averaged agreement score was nevertheless acceptable (Cohen’s kappa: 65.02). Kappa scores are evaluated differently in the literature. Greve and Wentura (1997) categorize kappa scores above 0.75 as excellent, scores between 0.61 and 0.75 as good. In more recent NLP work, even lower values are accepted as good (e.g., Ravenscroft et al., 2016; cf. Becker, Bender, and Müller, 2020, p. 442). Based on this assessment of the kappa value and the high degree of any other agreement between the annotations, the results of phase one were accepted as the basis for the second phase. That is, all cases in which different categories were assigned were filtered out. These cases were then decided by an independent annotator according to the criteria of the Guidelines. 1935 of 6000 sentences (32.25%) were identified as discourse referencing, which indicates the importance of such practices in parliamentary discourse.</p>

<p>In the second annotation phase, these 1935 sentences were annotated according to a more fine-grained scheme: The classification task was to distinguish discourse references in which the actor (author/speaker) of the referenced utterance is explicitly named from those in which the text becomes the actor in a metonymic shift and those in which no actors are named. (see Table 2).</p>

<table rend="rules"><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Tag</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Description</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Example</hi></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">1</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">Actor explicitly mentioned.</hi> </cell><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">Twelve years ago, the Chancellor, together with the prime ministers of the time, proclaimed the "7 percent" goal.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[Die Kanzlerin hat gemeinsam mit den damaligen Ministerpräsidentinnen und Ministerpräsidenten vor zwölf Jahren das Ziel „7 Prozent“ ausgerufen.]</hi></p></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">2</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">Metonymic mention of the actor.</hi></cell><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">Our Basic Constitutional Law obligates us to create equal living conditions in Germany.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[Unser Grundgesetz verpflichtet uns zur Schaffung gleichwertiger Lebensbedingungen in Deutschland.]</hi></p></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">3</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">No actor mentioned.</hi> </cell><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">The recommended resolution is adopted.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[Die Beschlussempfehlung ist angenommen.]</hi></p></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt">Table 2: Tagset of the second annotation round</hi> </p> 

<p>As a result, we measured a very good agreement (Cohen’s kappa:  84.35). After curating the annotations and producing the gold standard, 721 sentences (37.26%) were assigned to category 3, 1155 (59.69%) to category 1, and 59 (3.05%) to category 2.
In the same step, we extracted thephrases thathad been rated by the annotators as crucial for the categorization as discourse referencing. These included, for example, noun phrases (NP) representing communicative acts or texts or discourse actors (without heads of embedding phrases such as prepositions in a prepositional phrase) or relevant verb phrases (VP) (including verbs that express communicative action, as shown in the examples) without complements and adverbials. Table 3 gives an example of the phrase extraction. 
</p>

<table rend="rules"><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Categorized sentence</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Extracted phrases critical to categorization</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase type</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Referenced</hi></cell></row><row><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">For me, there are three good reasons to reject this proposal of the AfD today : The first is the sheer thin scope already mentioned by colleague Movassat; I do not need to say much more about it.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[Für mich gibt es drei gute Gründe, diesen Antrag der AfD heute abzulehnen: Der erste ist der schon vom Kollegen Movassat erwähnte schiere dünne Umfang; dazu brauche ich nicht mehr viel zu sagen.]</hi></p></cell><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">this proposal of the AfD</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[diesen Antrag der AfD]</hi> </p></cell><cell style="text-align: justify;"><hi style="font-size:12pt">NP</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">Text</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">colleague Movassat</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[Kollege Movassat]</hi></p></cell><cell style="text-align: justify;"><hi style="font-size:12pt">NP</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">actor</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><p style="text-align: justify;"><hi rend="italic" style="font-size:12pt">Mentioned</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">[erwähnte]</hi></p></cell><cell style="text-align: justify;"><hi style="font-size:12pt">VP</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">utterance</hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt">Table 3: Manual phrase extraction from sentences categorized as ‘discourse referencing’</hi></p>

 <p>The phrases „to reject” [abzulehnen] und „I do not need to say” [brauche ich nicht … zu sagen] were not extracted because they are expressions representing possible future utterance acts, not preceding ones. 
This extraction was intended to work out what annotators are looking at when they detect discourse referencing. In machine learning, an "attention mechanism" is used to try to mimic human cognitive attention. The extraction of relevant phrases will be used to test whether this principle can be supported in this way. Which effects can be observed will be reflected in the next chapter.</p>

<head><hi rend="normalweight">Automatic classification / machine learning </hi></head>
<p>In this section we describe how we build and apply different machine learning algorithms in order to detect and classify discourse references in political debates. The goal of this research is to assess the ability of computational models such as traditional classification algorithms as well als Deep learning techniques to detect discourse references in texts and to classify them. </p>

<div><head>Task Description</head>
<p>As mentioned before, we developed our category scheme with regard to the machine learnability of the different labels, and paid particular attention to aspects such as segmentation, granularity, distinctiveness and context sensitivity. In line with the two phases of annotations as described above, we designed two tasks for probing the ability of computational models to learn our category system:</hi></p>

<p style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Task 1: Detecting discourse references.</hi><hi style="font-size:12pt" xml:space="preserve"> In the first annotation phase, our annotators had to distinguish sentences with discourse referencing from sentences without discourse referencing. For computational modeling, this can be framed as a binary classification task; the task of detecting discourse references in texts on the sentence level:  Given a sentence, the task is to predict if this sentence contains a discourse reference or not. We use each of the given 6000 sentences as input, and let the model predict for each of them one of the two labels </hi><hi rend="italic" style="font-size:12pt">discourse referencing</hi><hi style="font-size:12pt" xml:space="preserve"> (1) and </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">no discourse referencing </hi><hi style="font-size:12pt">(0).</hi></p>

<p style="text-align: justify;"><hi rend="bold" style="font-size:12pt" xml:space="preserve">Task 2: Classifying types of discourse references. </hi><hi style="font-size:12pt" xml:space="preserve">The second task is to classify the discourse references into the three categories </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned, Metonymic mention of the actor </hi><hi style="font-size:12pt" xml:space="preserve">and </hi><hi rend="italic" style="font-size:12pt">No actor mentioned</hi><hi style="font-size:12pt" xml:space="preserve"> (see Table 4). We use all instances that have been annotated as discourse references in the gold version of our annotations for training and testing our models (n=1935). We experiment with </hi><hi rend="bold" style="font-size:12pt">three different input formats</hi><hi style="font-size:12pt">: providing the model (a) with the full sentence as input, (b) only with the phrase marked as relevant for discourse referencing as input, and (c) with both, the full sentence and the marked phrase, by concatenating the sentence and the phrase, separated by a separator token. </hi></p><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">We then train and evaluate the models in </hi><hi rend="bold" style="font-size:12pt">three settings</hi><hi style="font-size:12pt" xml:space="preserve">. In the first setting </hi><hi rend="bold" style="font-size:12pt">A</hi><hi style="font-size:12pt" xml:space="preserve">, all three categories are taken in account; in the second setting </hi><hi rend="bold" style="font-size:12pt">B</hi><hi style="font-size:12pt" xml:space="preserve">, the least frequently assigned category metonymy is excluded, and in the third setting </hi><hi rend="bold" style="font-size:12pt">C</hi><hi style="font-size:12pt">, the categories 1 and 2 are combined as actor-naming categories and contrasted with category 3, in which no actors are mentioned.</hi></p></div><div>

<head>Description of Models</head><p style="text-align: justify;"><hi style="font-size:12pt">In order to investigate if our category system as described above can be learned by machine learning techniques, we test the ability of two different supervised machine learning approaches: (I) Naive Bayes, a traditional classification algorithm, serves as our baseline model and is compared to (II) BERT, a State of the Art Transformer Language Model that has shown great success in various NLP tasks. Both models are applied to detect (Task 1) and classify (Task 2) discourse references in texts. </hi></p><p style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Baseline Model – Naive Bayes</hi><hi style="font-size:12pt">. Naive Bayes is a probabilistic classifier that makes assumptions about the interaction of features (Jurafsky and Martin, 2019, p. 59). The text is treated “as if it were a bag-of-words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document.” (ibid.) This means that first, the occurrence of words in a category is counted (‘bag-of-words’). Then, for each word, the probability that it occurs in each category can be calculated. For each new observation, a probability value is calculated based on each category. That means, it is assumed at first, that the sentence belongs to category 1. The overall probability of category 1 to be classified is then added to the probabilities of each word to occur in category 1. In the next step, the same calculation is performed, assuming the new observation belongs to category 2. After calculating these values for each category, the values are compared with each other. The category with the highest value is the prediction of the classifier.</hi></p>


<p style="text-align: justify;"><hi style="font-size:12pt">For our approach, we use the Multinomial Naive Bayes model as implemented in the python package scikit-learn (</hi>hi rend="color(212529)" style="font-size:12pt" xml:space="preserve">Pedregosa et al., 2011). </hi><hi style="font-size:12pt">We use 90% of the data for training and keep 10% for testing.</hi></p><p style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Transformer Language Model – BERT</hi><hi style="font-size:12pt">. The application of pretrained language models, such as BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) or XLNet (Yang et al., 2019), has recently shown great success and led to improvements for various downstream NLP tasks. Through pretraining on large textual corpora, these models store vast amounts of latent linguistic knowledge (Peters et al., 2018; Orbach and Goldberg, 2020). After pretraining, the models can be fine-tuned on specific tasks with a small labeled dataset and a minimal set of new parameters to learn. </hi></p><p style="text-align: justify;"><hi style="font-size:12pt">Language models have been successfully applied to various language classification tasks, such as emotion classification (Schmidt et al., 2021), sentiment analysis (Yin et al., 2020), or relation classifications (Becker et al., 2021). Inspired by these insights, we make use of the latent knowledge embodied in large scale pretrained language models and explore how we can finetune them for our two classification tasks - the detection of sentences  with discourse referencing and the classification of different types of discourse references.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">Initial experiments with different models had shown that the transformer language model BERT (</hi><ref target="https://arxiv.org/abs/1810.04805"><hi style="font-size:12pt">Devlin, et al., 2019</hi></ref><hi style="font-size:12pt">), which is pretrained on the Google Books Corpus and Wikipedia (in sum 3.3 billion words), yields best performances for our two tasks. For efficient computing and robustness, we use the distilled version of BERT, DistilBERT (Sanh et al., 2019), for our experiments. DistilBERT uses the so-called knowledge distillation technique which compresses a large model, called the teacher (here: BERT), into a smaller model, called the student (here: DistilBERT). The student is trained to reproduce the behavior of the teacher</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">by matching the output distribution. As a result, DistilBERT is 60% faster than the original BERT and requires less computing capacities, while retaining almost its full performance.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">DistilBERT</hi> <hi rend="bold" style="font-size:12pt">–</hi><hi style="font-size:12pt" xml:space="preserve"> as well as its teacher BERT </hi><hi rend="bold" style="font-size:12pt">–</hi><hi style="font-size:12pt" xml:space="preserve"> makes use of Transformer, a multihead attention mechanism that learns relations between words in a text. In contrast to other language models that process a text sequence from left to right, DistilBERT applies a bidirectional training, which means that during training, it reads the entire sequence of words at once. More specifically, during training the model is provided with sentences where some words are missing. The task for the model is then to predict the missing (masked) words based on their given context. By learning to predict missing words, the model learns about the structure and semantics of a language during the training phase, which leads to a deeper sense of language context.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">For our experiments, we use the pretrained DistilBERT model from HuggingFace Transformers (Wolf et al., 2020) and finetune the training modules on our labelled training data. We use 70% of the data for training and keep 15% for validation and testing, respectively. We optimize the model parameters and configurations on the validation set and report results for the test set. The optimal hyperparameters for our two classification tasks are displayed in Table x. As our output layer, we use softmax. This function enables us to interpret the output vectors of the last layer from the model as probabilities, by mapping them to values between 0 and 1 that all add up to 1.</hi></p><table rend="rules"><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi style="font-size:12pt">Task 1</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">Task 2</hi></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">Number of training epochs</hi>   </cell><cell style="text-align: justify;"><hi style="font-size:12pt">4</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">4</hi></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">Batch size</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">16</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">4</hi></cell></row><row><cell style="text-align: justify;"><hi style="font-size:12pt">Learning rate</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">5e-5</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">5e-5</hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt">Table 4: Hyperparamter setting for DistilBERT.</hi></p></div><div><head>Results</head><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">For both tasks, when </hi><hi rend="bold" style="font-size:12pt">evaluating</hi><hi style="font-size:12pt" xml:space="preserve"> the two models, respectively,  we compare the predicted labels to the gold version of our annotations. We report results on the test sets and use the evaluation metrics Precision, Recall and F1 (we report all scores as micro scores, that means they are weighted according to the label distribution).</hi></p><table rend="rules"><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Input</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Prec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Rec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">F1</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Naive Bayes</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.98 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">79.84</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.30 </hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">DistilBERT</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">93.17 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">93.15 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">93.16 </hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt">Table 5: Results for Task 1: Binary classification between discourse referencing and no discourse referencing.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">Table 5 displays the results for our first task - which was, given a sentence, predict if this sentence contains a discourse reference or not. We find that both models - Naive Bayes and DistilBERT - outperform the majority baseline (64.48% for the Label 0, No Discourse referencing) significantly. DistilBERT outperforms our baseline model Naive Bayes by 13 percentage points (F1 score), which matches our expectations that the latent linguistic knowledge that DistilBERT stores through its pretraining on large corpora can successfully be utilized for the task of detecting discourse references in political debates.</hi></p><table rend="rules"><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Input</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Prec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Rec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">F1</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Naive Bayes</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.55</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.86</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">78.81</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">82.04</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">82.34</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.34</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sent + phrase </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">83.06</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">83.03</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">81.45</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">DistilBERT</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.44 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.44 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.41 </hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">97.08 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">96.79 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">96.90 </hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sent + phrase </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">96.13 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">95.88 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">95.98 </hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">Table 6: Results for Task 2, Setting A: Classifying types of discourse references, three classes: </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned </hi><hi style="font-size:12pt">vs</hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">. Metonymic mention of the actor </hi><hi style="font-size:12pt">vs</hi><hi rend="italic" style="font-size:12pt">. No actor mentioned.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">Table 6-8 display the results for our second task - which was to classify the discourse references into different categories. The results for Setting A in which we distinguish the three categories </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned, Metonymic mention of the actor </hi><hi style="font-size:12pt" xml:space="preserve">and </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">No actor mentioned </hi><hi style="font-size:12pt" xml:space="preserve">are shown in Table 3. Both models outperform the majority baseline (59.69% for the label </hi><hi rend="italic" style="font-size:12pt">Actor explicitly mentioned</hi><hi style="font-size:12pt">) significantly. For both models, we find that providing the model with relevant phrases instead of or in addition to complete sentences improves the model’s performance. The best results for the Naive Bayes model are obtained by combining the sentence with the relevant phrase as input to the model, while DistilBERT learns best when provided only with the relevant phrase. This indicates that the models are not always fully able to detect which parts of the sentences are relevant for classifying types of discourse references and can benefit from that information when provided with it as input.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">When comparing the scores of the best input formats for each model, again we find that DistilBERT outperforms Naive Bayes significantly (15.5 percentage points F1 score), again demonstrating the superiority of pretrained language models opposed to knowledge-agnostic classification models.</hi></p>

<table rend="rules"><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Input</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Prec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Rec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">F1</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Naive Bayes</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">79.13</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">79.30</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">79.20</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">87.30</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">87.19</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">87.23</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sent + phrase </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">84.63</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">83.95</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">84.14</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">DistilBERT</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.80 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.82 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.79 </hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">98.48 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">98.47 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">98.47 </hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sent + phrase </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">98.01 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">98.00 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">97.99 </hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">Table 7: Results for Task 2, Setting B: Classifying types of discourse references, two classes: </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned </hi><hi style="font-size:12pt">vs</hi><hi rend="italic" style="font-size:12pt">. No actor mentioned.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">Table 7 displays the results for Task 2, Setting B in which we exclude the least frequently assigned category</hi><hi rend="italic" style="font-size:12pt" xml:space="preserve"> metonymy</hi><hi style="font-size:12pt" xml:space="preserve"> and only distinguish between the instances of the two classes </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned </hi><hi style="font-size:12pt" xml:space="preserve">and </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">No actor mentioned. </hi><hi style="font-size:12pt" xml:space="preserve">We find that the Naive Bayes model significantly improves when excluding the small class </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">metonymy </hi><hi style="font-size:12pt">(6 percentage points F1 score when provided with the marked phrase), while DistilBERT improves only by 1.5 percentage points compared to setting A (F1 score when provided with the marked phrase). Again we find that providing both models with relevant phrases instead of complete sentences improves the model’s performance - which especially applies for Naive Bayes.</hi></p><table rend="rules"><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Input</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Prec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Rec</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">F1</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Naive Bayes</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.73</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.73</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.73</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">80.93</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">81.71</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">81.14</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">sent + phrase </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">82.84</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">81.88</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">82.25</hi></cell></row><row><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">DistilBERT</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Sentence</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.56 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.55</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">92.50</hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Phrase</hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">97.83 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">97.82 </hi></cell><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">97.82 </hi></cell></row><row><cell style="text-align: justify;"/><cell style="text-align: justify;"><hi rend="bold" style="font-size:12pt">sent + phrase </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">97.37 </hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">97.37</hi></cell><cell style="text-align: justify;"><hi style="font-size:12pt">97.36 </hi></cell></row></table><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">Table 8: Results for Task 2, Setting C: Classifying types of discourse references, two classes: </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned+Metonymic mention of the actor  </hi><hi style="font-size:12pt">vs</hi><hi rend="italic" style="font-size:12pt">. No actor mentioned.</hi></p>

<p>In Table 8 we finally display the results for Task 2, Setting C, in which we subsume the category Actor explicitly mentioned with the category Metonymic mention of the actor under the main category actor-naming references and binarily distinguish between the categories actor-naming references and No actor mentioned. While the results for DistilBERT stay almost the same as in Setting B, we find that the performance of Naives Bayes drops drastically (-5 percentage points, F1 Score when provided with a phrase as input). This indicates that the model struggles with the category Metonymic mention of the actor - even when this category is subsumed under one label with another category.</p>

<p>To summarize, our results show that both models are able to learn to detect and classify discourse references in political debates. The pretrained knowledge-rich model DistilBERT outperforms the knowledge-agnostic model Naïve Bayes significantly on all tasks and settings. We furthermore find that providing the models with relevant phrases instead of or in addition to complete sentences improves the model’s performance, which indicates that the models can benefit from being explicitly hinted at the parts of the sentences that are relevant for classifying different types of discourse references. It furthermore shows that the parts of the sentences which are not relevant for distinguishing between different types of discourse markers are not only useless for the classification, but even lower the model’s performance.</p>

<head><hi rend="normalweight">Analysis of results</hi></head>
<p>In this section, we conduct a deeper analysis of the predictions, performance, and errors of our best performing model DistilBERT.</p>
<p>Figure 1 displays the error matrix for Task 1 where DistilBERT achieves a performance of 93.16 F1 score (cf. Table 5). We find that the cases where the models predicts a discourse reference but according to the gold data the respective instance contains no discourse referencing (false positives, n=37) and vice versa (false negatives, n=29) are almost balanced. </p><p style="text-align: justify;"><hi style="font-size:12pt">While the manual analysis of the 29 false negatives did not lead to any observation of linguistic patterns which might lead the model to wrong predictions, the analysis of the 37 false positives showed that in many cases, DistilBERT predicts a discourse reference for those instances that mention an actor, but not in a discourse referencing function such as in example 9 and 10:</hi></p><list rend="numbered"><item><hi rend="italic" xml:space="preserve">When it comes to religious constitutional law and legal history at this late hour, I can understand that Mr. von Notz is not the only one who cannot wait to enter this debate. </hi>[Wenn es zu vorgerückter Stunde um Religionsverfassungsrecht und Rechtsgeschichte geht kann ich verstehen dass Herr von Notz nicht der Einzige ist der es gar nicht abwarten kann in diese Debatte einzutreten.]</item><item><hi rend="italic" xml:space="preserve">The Highway GmbH of the federal state examines the facts of the case. - </hi>[Die Autobahn GmbH des Bundes prüft den Sachverhalt<hi style="font-size:12pt">.]</hi></item></list><p style="text-align: justify;"><hi style="font-size:12pt">In both examples, actors are named (</hi><hi rend="italic" style="font-size:12pt">Herr von Notz</hi><hi style="font-size:12pt" xml:space="preserve">; </hi><hi rend="italic" style="font-size:12pt">Die Autobahn GmbH des Bundes</hi><hi style="font-size:12pt">), which leads to the assumption that the model interprets explicit mentions of actors as indicators for discourse referencing.</hi></p><figure><graphic n="1001" width="9.136944444444444cm" height="7.297786111111111cm" url="media/image1.png" rend="inline"/></figure><p style="text-align: justify;"><hi style="font-size:12pt">Fig. 1: Confusion matrix for DistilBERT on Task 1.</hi></p><p style="text-align: justify;"><hi style="font-size:12pt">Figure 2-4 display the error matrices for the different settings of Task 2. Since the performance of DistilBERT on Task 2 is very high in all three settings, we find only very few errors. A systematic manual analysis of the misclassified instances revealed three main sources of errors:</hi></p><figure><graphic n="1002" width="16.352408333333333cm" height="4.691944444444444cm" url="media/image2.png" rend="inline"/></figure>

<p style="text-align: justify;">Sentence            Phrase              Sentence + Phrase</p><p style="text-align: justify;">Fig. 2: Confusion matrices for DistilBERT on Task 2, Setting A.</p><figure><graphic n="1003" width="16.28056388888889cm" height="4.639027777777778cm" url="media/image3.png" rend="inline"/></figure><p style="text-align: justify;">Sentence          Phrase              Sentence + Phrase</p><p style="text-align: justify;">Fig. 3: Confusion matrices for DistilBERT on Task 2, Setting B.</p><figure><graphic n="1004" width="16.316005555555556cm" height="4.674305555555556cm" url="media/image4.png" rend="inline"/></figure><p style="text-align: justify;">Sentence         Phrase              Sentence + Phrase</p><p style="text-align: justify;">Fig. 2: Confusion matrices for DistilBERT on Task 2, Setting C</p><p style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Error type 1: The model confuses the labels actor and metonymy (Setting A)</hi></p><p style="text-align: justify;"><hi style="font-size:12pt" xml:space="preserve">One common error in setting A is that the category </hi><hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned </hi><hi style="font-size:12pt" xml:space="preserve">and the category </hi><hi rend="italic" style="font-size:12pt">Metonymic mention of the actor</hi> <hi style="font-size:12pt">are being confused by the model. Examples 10 displays an example that mentions an actor only m</hi> <hi rend="italic" style="font-size:12pt">etonymically</hi> <hi style="font-size:12pt" xml:space="preserve">according to the annotation guidelines, but is misclassified by DistilBERT (with all three input options) as an instance that </hi>  <hi rend="italic" style="font-size:12pt">explicitly mentions the actor.</hi></p><list rend="numbered"><item><hi rend="italic">Our Basic Law also protects freedom of occupation in Article 12.</hi> </item></list><p rend="List Paragraph" style="text-align: justify;">[Unser Grundgesetz schützt in Artikel 12 auch die Berufsfreiheit.] </p>

<p>A reason for this type of errors may be the small size of the class</hi>  <hi rend="italic" style="font-size:12pt">Metonymic mention of the actor</hi><hi style="font-size:12pt" xml:space="preserve">, as it accounts only 3.05% of the annotations in the gold standard. In the following discussion we will also reflect the distinctiveness of these two classes. This error type confirms our choice of Setting B and C, where metonomy is either excluded (B) or subsumed together with the frequent category </hi> <hi rend="italic" style="font-size:12pt" xml:space="preserve">Actor explicitly mentioned </hi><hi style="font-size:12pt" xml:space="preserve">under the main category </hi><hi rend="italic" style="font-size:12pt">actor-naming references</hi> <hi style="font-size:12pt">(C).</hi></p><p style="text-align: justify;"><hi rend="bold" style="font-size:12pt">Error type 2: An actor was predicted when there was none </hi></p>

<p>Similar to the first type of error, the model misclassifies instances as belonging to the class of actors being explicitly mentioned, whereas according to the gold standard, no actor is mentioned. An explanation may be the mentioning of actors that are not part of the discourse reference made (e.g. “The Bundestag” and “US President  Donald Trump” in figure 11, or the use of pronouns (</hi> <hi rend="italic" style="font-size:12pt" xml:space="preserve">we </hi><hi style="font-size:12pt">in (14)). This assumption is reinforced by the fact that this error mostly occurs when sentences build the input. When providing the model with a phrase (underlined in the examples), which usually does not contain a named entity/pronoun, the model makes correct predictions.</hi></p>












                                    <p></p>
                   </div>
        </body>
        <back>
            <listBibl>
                <bibl></bibl>
            </listBibl>
           
        </back>
    </text>
</TEI>
