<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="../../common/schema/DHQauthor-TEI.rng"    type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0" ?>
<?xml-model href="../../common/schema/DHQauthor-TEI.isosch" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-model href="../../common/schema/dhqTEI-ready.sch"     type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<TEI xmlns=      "http://www.tei-c.org/ns/1.0"
     xmlns:cc=   "http://web.resource.org/cc/"
     xmlns:dhq=  "http://www.digitalhumanities.org/ns/dhq"
     xmlns:html= "http://www.w3.org/1999/xhtml"
     xmlns:mml=  "http://www.w3.org/1998/Math/MathML"
     xmlns:rdf=  "http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Slow, Painful and Expensive: Current Challenges in
               Text-Mining Corpus Construction for the Digital
               Humanities<!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Matt <dhq:family>Warner</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0003-4469-481X<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Stanford University</dhq:affiliation>
               <email>mattgw@stanford.edu</email>
               <dhq:bio>
                  <p>Matt Warner is the Associate Director of the Stanford Literary Lab. Matt's
                     research focuses on the theoretical foundations and implications of
                     quantitative and computational literary studies.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Nichole <dhq:family>Nomura</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-5624-9011<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Wyoming</dhq:affiliation>
               <email>nnomura@uwyo.edu</email>
               <dhq:bio>
                  <p>Nichole Misako Nomura is an Assistant Professor of Public Humanities and
                     English at the University of Wyoming. Her research centers on how literature
                     teaches/is taught, using methods from the social sciences, literary study,
                     education, and the digital humanities. </p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Carmen <dhq:family>Thong</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-4576-1415<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Stanford University</dhq:affiliation>
               <email>tcarmen@stanford.edu</email>
               <dhq:bio>
                  <p>Carmen Thong is a PhD candidate in English at Stanford University. She works in
                     postcolonial studies, digital humanities, and contemporary book studies. Her
                     work is published with MLQ, Medical Humanities, World Literature in Motion
                     (Columbia UP: 2020) and UNESCO.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Alix <dhq:family>Keener</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-5606-9176<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Stanford University</dhq:affiliation>
               <email>alixkee@stanford.edu</email>
               <dhq:bio>
                  <p>Alix Keener is an academic librarian with a Master’s in Information Science
                     from the University of Michigan School of Information. She has held digital
                     scholarship and digital humanities librarian positions at Stanford University
                     Libraries and the University of Michigan Library.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Alexander <dhq:family>Sherman</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-6811-1685<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Texas, Austin</dhq:affiliation>
               <email>alexander.sherman@austin.utexas.edu</email>
               <dhq:bio>
                  <p>Alexander Sherman is an Assistant Professor of English at the University of
                     Texas at Austin. He studies literature and science across the long eighteenth
                     century, the digital humanities, the geography of colonialism, and the oceanic
                     humanities. His work has appeared in PMLA, Space and Literary Studies
                     (Cambridge UP: 2025), Eighteenth-Century Studies, and Cultural Analytics and
                     Post45.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Gabi <dhq:family>Birch</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0009-0001-4417-4024<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Stanford University</dhq:affiliation>
               <email>gab03@stanford.edu</email>
               <dhq:bio>
                  <p>Gabi Birch is a graduate student in English and digital humanities at Stanford
                     University. She studies literature and technology in the late nineteenth and
                     early twentieth centuries.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Maciej <dhq:family>Kurzynski</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0009-0006-5466-2423<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Lingnan University</dhq:affiliation>
               <email>maciejkurzynski@ln.edu.hk</email>
               <dhq:bio>
                  <p>Maciej Kurzynski is an Assistant Professor of Chinese and Digital Humanities at
                     Lingnan University, Hong Kong.</p>
               </dhq:bio>
            </dhq:authorInfo>

            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Mark <dhq:family>Algee-Hewitt</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-2628-588X<!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Stanford University</dhq:affiliation>
               <email>malgeehe@stanford.edu</email>
               <dhq:bio>
                  <p>Mark Algee-Hewitt is an Associate Professor English and Digital Humanities at
                     Stanford University where he directs the Literary Lab and the Center for
                     Spatial and Textual Analysis and leads the program in Data Science for Artistic
                     and Critical Analysis. His research combines critical analysis of text with
                     computational and quantitative methods, primarily focused on the writing of
                     England and Germany over the past three centuries. He is particularly
                     interested in the history of aesthetic theory and has recently led projects on
                     the identification of domestic space in nineteenth-century novels, the
                     evolution of the concept of prestige, and the transformation of disciplinary
                     discourse in the eighteenth-century.</p>
               </dhq:bio>
            </dhq:authorInfo>


         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000790<!--including leading zeroes: e.g. 000110--></idno>
            <idno type="volume">019</idno>
            <idno type="issue">3</idno>
            <date when="2025-09-12">12 September 2025</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp="#access"/>
               <term corresp="#digital_libraries"/>
               <term corresp="#digitization"/>
               <term corresp="#corpora"/>
               <term corresp="#nlp"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item>text mining</item>
                  <item>corpus creation</item>
                  <item>DMCA</item>
                  <item>cultural analytics</item>
                  <item>digitization</item>
                  <item>digital library resources</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000790/000790.xml">GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>The process of assembling corpora for text-mining-based Digital Humanities projects
               is a crucial and yet frequently overlooked aspect of the research process. Often
               complicated by text availability and cost, as well as legal restrictions on
               in-copyright text, DH scholars frequently resort to “found” corpora marketed to
               libraries by publishing companies or questionably sourced corpora that inhabit legal
               grey areas. While such corpora have led to methodological developments in the field,
               there is a general sense that the biases of these corpora and the inability to share
               their raw data have made them imperfect vehicles for large-scale critical claims in
               the humanities. Recent developments, however, suggest that this situation may be
               changing. In the United States, the 2021 text and data-mining exemption to the
               Digital Millennium Copyright Act (DMCA) has promised to improve the viability of
               bespoke corpora for text-mining research. In this paper, we put these improvements to
               the test, reporting on our efforts to source a relatively small corpus of literary
               theory monographs. Focusing primarily on born-digital works and operating under all
               of the practical and legal constraints dictated by the exemption to the DMCA, we
               sought to assemble a corpus of 402 pre-selected theoretical works. We found that,
               despite the recent legal changes, and even with extensive support from a
               well-resourced library, it remains overly difficult to assemble a pre-selected corpus
               of scholarly works, even under ideal financial and institutional conditions. While
               scholars outside of the United Stats will face somewhat different legal restrictions
               on the collection of electronic texts than we did, we found that many of the
               obstacles we faced were practical, rather than regulatory, and in many cases, we
               found that scanning books was the easiest and most efficient route to digital
               versions of the texts we sought.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>Recent legal changes in the USA don't change the fact that ebooks are expensive and
               generally unsuitable for digital corpus construction.</p>
         </dhq:teaser>
      </front>

      <body>
         <div>
            <head/>
            <div>
               <head>Introduction</head>

               <p>Over the past fifteen years, subfields in the Digital Humanities (DH) that focus
                  on computational text analysis have grown rapidly. But at the same time, an
                  imbalance has emerged between the development of new computational and statistical
                  methods for text analysis and the availability of material on which this analysis
                  can be performed. While dataset publication and citation has increasingly become a
                  standard in the sciences,<note> Borgman provides numerous examples of this in the
                     sciences and social sciences, but is only able to point to the example of
                     Archeology within the Humanities <ptr target="#borgman2018"/>.</note> scholars
                  in the digital humanities working on text, especially in-copyright text, face
                  significant barriers to dataset publication and, subsequently, data sharing.<note>
                     Attempts to address the problems of data publication for professional metrics
                     (tenure, funding and similar) have seen the recent development of hybrid models
                     of dataset-article, such as <hi rend="italic">Cultural Analytics</hi>’s <quote rend="inline">Data Set</quote> articles, Post45’s “data essays” or the <hi rend="italic">Journal of Open Humanities Data</hi>’s <quote rend="inline">Data Papers</quote>.</note> Whether under the rubric of Cultural Analytics,
                  Computational Literary Studies, Stylometry, Distant Reading or Text Mining, the
                  gains in these subfields have largely been methodological <ptr target="#janicke_etal2015"/>, <ptr target="#underwood2019"/>, <ptr target="#gius_jacke2022"/>. The large-scale resources that have enabled work in
                  these fields, particularly for Anglophone texts, are the same that existed over a
                  decade ago <ptr target="#gale_cengage2014"/>, <ptr target="#project_gutenberg_nd"/>.<note> Large bibliographical efforts at the national scale have created
                     important new resources for several, particularly European, languages <ptr target="#bnf_nd"/>, <ptr target="#spk_nd"/>. Yet, despite these efforts,
                     given the under resourced nature of non-English archives, the challenges we
                     describe in this paper are just as pressing, if slightly altered, for other
                     languages. </note> These pre-assembled corpora, often sold and distributed by
                  large publishers despite being mostly out of copyright, come with a host of
                  challenges that complicate the early promises of statistical rigor and
                  representative sampling made by DH practitioners <ptr target="#jockers2013"/>
                  <ptr target="#ramsay2011"/>. The challenges of assembling relevant corpora for
                  particular projects have been relegated to the bibliographical, archival, and, for
                  in-copyright works, legal domains, which have long been subordinated to the
                  interpretive and methodological concerns of the field <ptr target="#bode2020"/>.
                  And while there exists a tacit assumption among practitioners that these
                  challenges are slowly (if unevenly) being addressed, making large corpora of texts
                  progressively more available, this availability continues to lag other
                  developments in the field, and the gap between developments in methods versus in
                  corpora may be widening.</p>

               <p> This article focuses specifically on these issues, particularly in the legal
                  domain, as they play out in the American context, where the 2021 introduction of
                  the Text and Data Mining (TDM) exemption to the Digital Millennium Copyright Act
                  (DMCA) represents a crucial step towards overcoming the legal dimension of these
                     challenges.<note> Exemption to Prohibition on Circumvention of Copyright
                     Protection Systems for Access Control Technologies. 37 CFR Part 201. For a
                     research-oriented summary, see that of <ptr target="#authors_alliance2021"/>.
                     The bulk of the research described here was conducted during the fall of 2022
                     and through 2023, under the first version of the TDM exemption, which has since
                     been renewed, subject to minor modifications, particularly relating to
                     collaboration, which we discuss in the conclusion below.</note> While the
                  implications of copyright for digital humanists are widely understood and
                  discussed — if certainly not resolved — the work we present here focuses on the
                  interplay between the licensing of ebooks and their protection via DRM. Unlike
                  printed matter, whose use is restricted only by copyright, ebooks are subject to
                  license agreements which restrict the ways in which they may be used, and often
                  specifically disallow text-extraction. Even when such use is not prohibited,
                  however, ebooks are generally protected from text-extraction by a variety of
                  technical measures, generally referred to as <quote rend="inline">digital rights
                     management</quote> (DRM). The circumvention of DRM, in turn, is typically
                  legally proscribed — in our case by the Digital Millenium Copyright Act in the
                  United States, and in other nations by relevant local law (for example, in the
                  European Union, the Directive on Copyright and Related Rights in the Digital
                  Single Market requires EU member states to forbid the circumvention of DRM,
                  stating in its preamble that <quote rend="inline">The protection of technological
                     measures … remains essential to ensure the protection and the effective
                     exercise of the rights granted to authors and to other rightholders under Union
                     law</quote>.<note> Directive (EU) 2019/790 of the European Parliament and of
                     the Council of 17 April 2019 on copyright and related rights in the Digital
                     Single Market and amending Directives 96/9/EC and 2001/29/EC (Text with EEA
                     relevance.) Preamble, 7.</note>) </p>

               <p>Unfortunately, compared to even the patchwork standardization of copyright
                  (attempts at the harmonization of which date back to the 1886 Bern Convention),
                  frameworks for licensing and DRM are significantly more varied, and many of the
                  legal details of what we present here are specific to the US context. While the
                  particular legal issues facing scholars will vary from jurisdiction to
                  jurisdiction, however, we found significant practical and technical barriers to
                  our corpus-compilation efforts that are, we believe, relevant and significant to
                  scholars working under even considerably different legal frameworks. In our
                  American case, the new TDM provision of the DMCA decriminalizes the act of
                  extracting text protected by DRM software in the US for text and data mining
                  research, and its explicit goal is to allow scholars to build responsible corpora
                  for research.<note> The pivot towards so-called <q>A.I.</q> technologies rests
                     largely upon companies’ willingness to train large language models with
                     questionably sourced in-copyright works. Given the for-profit nature of
                     technologies like OpenAI’s GPT4 or Google’s Bard, the fair-use doctrine that
                     underlies permissions for extracting in-copyright text for research purposes
                     does not generally apply; however, the existence and use of such corpora has
                     already been adopted by opponents of all text-mining work, research and
                     industry alike.</note> (Similar exemptions exist in other jurisdictions; for
                  example article 3 of the Directive on Copyright in the Digital Single Market
                  specifically protects “reproductions and extractions made by research
                  organisations and cultural heritage institutions in order to carry out, for the
                  purposes of scientific research, text and data mining of works or other subject
                  matter to which they have lawful access.” In addition to letting scholars assemble
                  and share their own digital in-copyright corpora, the TDM exemption, and other
                  similar frameworks, also create an opening for the discussion of the collection,
                  use, and distribution of in-copyright texts.<note> In addition to increasing the
                     availability of recently published texts, the push to obtain born-digital texts
                     lies in the field’s need to overcome the systematic corpus errors introduced by
                     OCRed text, which is endemic within the field.</note> As such, DH scholars
                  working in the US have looked with excitement to these new provisions as a
                  potential watershed in making possible the creation of more deliberate,
                  representative, and accessible corpora of the kind previously only possible using
                  out of copyright material <ptr target="#bode2018"/>. </p>

               <p>Yet the particulars of this exemption — in its current form — leave many of the
                  same legal hurdles in place, particularly for scholars working independently or at
                  smaller institutions without access to significant research budgets and
                  well-resourced IT departments and legal teams. In order to work within the
                  boundaries set by the exemption, scholars face new restraints around sourcing,
                  processing, storing, and above all, sharing, these materials, many of which
                  contradict the norms and aims of the field. Similarly, the easing of legal
                  barriers to assembling corpora of in-copyright texts does not address the other
                  kinds of challenges practitioners in DH face when sourcing texts. While the gains
                  made possible by the passing of this exemption are a necessary step towards
                  helping scholars research copyrighted materials, the exemption itself is anything
                  but a sufficient solution. </p>

               <p>Our goal in this paper is to document our attempt to take advantage of this new
                  legal framework to build a bespoke corpus of largely in-copyright, born-digital
                  texts for the purpose of text mining. Readers can access the <ref target="resources/data/corpus_data.csv">corpus manifest</ref>. Starting not
                  from a found corpus of pre-assembled works, but instead from a list of desired
                  texts around a single subject, we sought to collect a complete corpus of 402 works
                  of twentieth century literary theory, following all the rules set by the TDM
                  exemption. In the end, we have discovered that even when supported by both a
                  large, wealthy university library and a grant specifically funding the assembly of
                  such a corpus, building a born-digital corpus remains difficult, if not
                  impossible. Our struggles to do so implicate not just the legal mechanisms of
                  copyright, licensing and DRM, but also the publishing and book selling industries,
                  the collection and purchasing protocols of university libraries, and the general
                  availability of texts themselves. We are particularly dispirited to report, then,
                  that even armed with a considerable quantity of grant money, a supportive library,
                  and a team of researchers working together on this project, we have found the
                  acquisition of texts under these constraints to be slow, painful, and expensive.
                  If efficiency had been our goal, it would have been unquestionably faster for us
                  to cull our corpus of outrageously expensive titles, and then purchase,
                  disassemble, and scan every book from a physical copy — not, alas, a ringing
                  endorsement for the new legal regime. It is clear that the challenges of corpus
                  construction extend far beyond the legal domain and into all facets of book
                  publishing, selling, and archiving. </p>
            </div>


            <div>

               <head>Project Overview</head>

               <p>The corpus whose assembly we document here is composed of literary theory,
                  collected in support of a research project examining the movement of ideas,
                  concepts, and discourses between literary theory and literary criticism. Alongside
                  the hand-curated corpus of literary-theoretical texts detailed here, this project
                  will consider a collection of literary monographs based on recently recovered
                  scans of books created by the Google Books initiative (see <quote rend="inline">Google Books Return</quote>, below). To digital humanists working with textual
                  corpora, this corpus will be a familiar example of the <quote rend="inline">found-corpus,</quote> a pre-existing set of texts only partially aligned with
                  our research goals, and marred by significant bibliographic and representational
                  limitations. The principle of selection behind this set of critical texts is
                  pragmatic in the extreme: it is simply the books that Stanford held which Google
                  had not already digitized from another library’s collection. The scale of the
                  collection is considerable, including 24,981 English-language titles under the
                  Library of Congress <q>P</q> classification (<quote rend="inline">Language and
                     Literature</quote>) and its subclasses on literature and literary criticism.
                  Our library’s records, moreover, distinguish between primary source and other
                  texts, allowing us to identify, request, and access some 4,395 works that we were
                  relatively confident represented chiefly works of literary criticism and analysis
                  written in English. We cannot, and will not, claim that these texts are in any way
                  representative of the field. In fact, based on our exploration of the overlap the
                  between the texts we selected for our theoretical corpus and the works digitized
                  in this Google Books data, we are nearly certain that this collection of texts is
                  deeply idiosyncratic, missing exceptionally important works, and overdetermined by
                  the particular strengths and weaknesses of the Stanford library’s collections. </p>

               <p> We introduce this second corpus of literary criticism as a counterexample to the
                  purpose-built in-copyright corpus that is our primary object of discussion. We
                  will therefore not explore these works of criticism in this article, nor analyze
                  the texts of literary theory that we have collected in any way. Instead, our focus
                  here is methodological, foregrounding issues of corpus construction — how we
                  found, obtained and tracked the modest number of texts we required, and the many
                  obstacles we encountered in doing so. Many of these obstacles lie in the blurry
                  interface between researchers and librarians, and we hope that by presenting them
                  here, we can draw attention to the particular needs and expectations of each of
                  these two groups and offer some insight into how best to negotiate this kind of
                  collaboration. Of course, some of the issues we encountered were purely of our own
                  devising, and many were, we would like to think, beyond our control, if sometimes
                  retrospectively predictable. </p>
            </div>

            <div>

               <head>Methods</head>
               <div>

                  <head>Overview</head>

                  <div>

                     <head>Corpus Selection</head>

                     <p>For the research project underlying this corpus, we selected eight
                        theoretical fields to focus on — feminism, Black studies, Marxism,
                        psychoanalysis, post-structuralism, postcolonial studies, narratology and
                        Russian formalism. These fields, we felt, had a variety of different
                        relationships to literary criticism that would facilitate comparative
                        analyses in our later research. Some felt distinctly dated, others seemed to
                        us to have more lingering influence; some were older, dating to before the
                        institutionalization of literary studies, others were more recent. This
                        heterogeneity was an asset, too, insofar as we expected that the
                        availability of texts from each of these schools would vary according to
                        their dates, languages of composition, and other similar factors. While the
                        specific selections we made are not the focus of the present work, we wish
                        to highlight the necessarily incomplete nature of our case study.
                        Comprehensiveness is not a reasonable standard for something as dynamic and
                        contentious as twentieth-century theory, and so we do not want to even
                        inadvertently give the impression that we believe that these fields and our
                        resulting corpora represent <q>literary theory</q> writ large. </p>

                     <p>Within each of these fields, we carefully selected a few dozen works, mostly
                        monographs, which we felt were important to that field and which we hoped
                        would shed light on the questions we cared about.<note> Because we made our
                           selections for each field independently, we have several works that are
                           associated with multiple fields (e.g., Sadiya Hartman’s <hi rend="italic">Scenes of Subjugation</hi>, which we included in both feminism and
                           Black studies).</note> We consulted tables of contents from anthologies,
                        introductory texts, and readers, and relied in large measure on memory,
                        expertise, and debates among ourselves and with outside scholars of these
                        fields. We sought a corpus that, even if incomplete, represented a broad
                        swath of texts that we felt were important to each field and which we hoped
                        would facilitate meaningful, complex findings in our later research (see
                        Table 1). In some — if not all — of our categories, we also had to make
                        decisions as to whether a text was theory or literary criticism. How much of
                        the monograph could be concerned with interpreting primary material before
                        it would be relegated to criticism rather than theory? What if an especially
                        influential concept was introduced, but nestled within a carefully applied
                        close reading? Importantly, however, we did not make our selections with any
                        eye to the availability of digital (or other) versions of these texts: we
                        wanted very specifically to explore the accessibility of literary-academic
                        titles for text mining research, and we have been able to locate a text —
                        albeit far from always a digital one — for nearly every text we initially
                        listed. </p>

                     <table>
                        <head>Table : number of texts per theoretical field. Some texts are included
                           in multiple fields</head>
                        <row role="label">
                           <cell>Field</cell>
                           <cell>Number of Texts</cell>
                        </row>
                        <row role="data">
                           <cell>Black Studies</cell>
                           <cell>76</cell>
                        </row>
                        <row role="data">
                           <cell>Feminism</cell>
                           <cell>50</cell>
                        </row>
                        <row role="data">
                           <cell>Marxism</cell>
                           <cell>51</cell>
                        </row>
                        <row role="data">
                           <cell>Narratology</cell>
                           <cell>66</cell>
                        </row>
                        <row role="data">
                           <cell>Post-Structuralism</cell>
                           <cell>36</cell>
                        </row>
                        <row role="data">
                           <cell>Postcolonial</cell>
                           <cell>75</cell>
                        </row>
                        <row role="data">
                           <cell>Psychoanalysis</cell>
                           <cell>41</cell>
                        </row>
                        <row role="data">
                           <cell>Russian Formalism</cell>
                           <cell>24</cell>
                        </row>
                     </table>


                     <p>A significant preliminary obstacle to our work was simply identifying books
                        that corresponded to the texts that we had selected. Here, we ran into
                        bibliographical problems that will be familiar to anyone who has assembled a
                        corpus from a list of titles, as well as several new issues. In many cases,
                        these problems stem from the particular features of the academic books in
                        our corpus, which overrepresent translated works and often feature lengthy
                        titles prone to abbreviation, sometimes with slightly amorphous punctuation
                        dividing them (e.g., we initially included <hi rend="italic">Anti-Oedipus</hi>, <hi rend="italic">A Thousand Plateaus</hi>, and <hi rend="italic">Capitalism and Schizophrenia</hi> in our corpus before
                        resolving these into <hi rend="italic">Anti-Oedipus: Capitalism and
                           Schizophrenia</hi> and <hi rend="italic">A Thousand Plateaus: Capitalism
                           and Schizophrenia</hi>). As a preliminary step towards the compilation of
                        our corpus, we sought to link each of our texts to a library catalog entry
                        to find out if our library provides access to the text in question in an
                        electronic form (or, in some cases, if our library has the text in <hi rend="italic">any</hi> form). </p>

                     <p>Once we had checked for library catalog entries for each work, we consulted
                        the corresponding MARC (Machine Readable Catalog) record, a standard for the
                        representation and communication of bibliographic and related information in
                        machine-readable form <ptr target="#loc_nd"/> for each text, which our
                        library makes available in its online catalog through what it terms <quote rend="inline">librarian view</quote>. From this record, we then recorded
                        information about whether a text had been included in Google’s digitization
                        initiative, and, for texts where a library ebook was available, the details
                        of that availability (this information was not available in the more public
                        facing catalogue entry). Finally, we used this information to develop a
                        preliminary plan for obtaining each of our texts. </p>

                  </div>

                  <div>

                     <head>Acquisition Process</head>

                     <p>We selected our acquisition methods with a careful eye to copyright law, the
                        DMCA, and the license agreements that govern ebooks. While we are aware that
                        a case can be made that fair use of copyright-protected texts produced via
                        another party’s independent violation of the DMCA may not violate either
                        copyright or the DMCA, this method represents the kind of “grey area” that
                        we are avoiding.<note> The recent and increasing reliance on this argument
                           being made by corporations engaged in the training of large language
                           models (LLMs), moreover, makes us uncertain as to its future, and
                           uncomfortable with its present-day ramifications. A growing pile of
                           lawsuits have been filed on behalf of authors against companies using
                           copyrighted texts as training data for their LLMs <ptr target="#saveri_butterick2023"/>, <ptr target="#reisner2023"/>. The
                           legality of using copyrighted texts, or even copyrighted art images, as
                           training data is in the process of debate <ptr target="#knibbs2023"/>,
                              <ptr target="#helms_krieser2023"/>, <ptr target="#sekhon_etal2023"/>,
                              <ptr target="#pike2022"/>, <ptr target="#jung2020"/>. Most academic
                           work on this issue is still in its nascent stages, with promising new
                           work across fields beginning to come up as preprints (such as <ptr target="#chu_etal2023"/>).</note> The most direct route to such a
                        legally unburdened corpus would have been to digitize each of the works
                        ourselves, but we prioritized obtaining born-digital versions of our texts
                        wherever it was possible to do so, both because — in principle — this offers
                        a considerable saving of time and labor, and because we wanted to acquire
                        the cleanest possible texts with the fewest errors introduced via either
                        scanning or processing the texts (with optical character recognition [OCR]).
                        For our research needs, our ebook processing and OCR-based pipelines were
                        designed to produce plaintext UTF-8 encoded files, though other project
                        might prefer to work with annotated data (e.g. XML or TEI), or even raw page
                        images, and we anticipate that many of the issues we discuss here would
                        apply in these cases as well. </p>

                     <p> Initially, we conceptualized our acquisition of texts quite simply (see
                        Figure 1). We proposed to turn first to library-owned ebooks; then purchase
                        ebooks as needed to fill in any remaining holes in the corpus; leaving, we
                        naively assumed, a small residuum of texts to be scanned by the library or
                        project members (e.g., long-out-of-print texts unavailable as ebooks). </p>

                     <figure>
                        <head>Initial process for acquisition of texts</head>
                        <figDesc>Flowchart showing three step text acquisition priorities: first
                           library owned ebooks, then newly purchased ebooks, then scans of printed
                           works.</figDesc>
                        <graphic url="resources/images/figure01.jpeg"/>
                     </figure>

                     <p>Our workflow quickly grew vastly more complicated. We found that the library
                           <hi rend="italic">owned</hi> (as opposed to licensed) electronic copies
                        of fewer texts than we expected, lacking ebook copies of widely read and
                        cited theoretical texts. When purchasing ebooks, we were confronted by
                        spotty availability, and we discovered later in the process that the
                        availability of ebooks to the library changes relatively frequently. When we
                        turned to digitization, we discovered that not all the texts which Stanford
                        had provided to Google as part of the Google Books initiative had actually
                        been scanned. Even with printed texts, there were unforeseen complexities,
                        and as our scanning queue grew, we ultimately divided it into two different
                        workflows. The result, as shown in Figure 2, was a far more tangled,
                        burdensome, and slow process than we had anticipated, involving at least
                        five and as many as eleven steps to go from identified text to file in the
                        corpus.</p>

                     <figure>
                        <head>Final workflow for text acquisition</head>
                        <figDesc>Flowchart showing complex multistage pipeline for acquiring digital
                           copies of texts, divided into born digital and print sources, with as
                           many as 11 steps for some works.</figDesc>
                        <graphic url="resources/images/figure02.jpeg"/>
                     </figure>

                  </div>
               </div>

               <div>

                  <head>Types of acquisition</head>

                  <div>

                     <head>Electronic Books</head>

                     <p>Whenever possible, we prioritized the acquisition of texts via library
                        ebooks. To remain in compliance with the TDM exemption to the DMCA, the text
                        must be owned by Stanford University (or perpetually licensed), rather than
                        available via subscription, as the exemption to the DMCA excludes ebooks
                        acquired via subscription services. Of our 402 titles, 116 — slightly more
                        than one quarter — were available as non-subscription ebooks via the
                        library. Of these, 51 were available as full-text download (almost always as
                        a PDF); while the remainder were protected by DRM or available only as
                        chapter-by-chapter individual PDF downloads. Of these 65 DRM-protected or
                        fragmented texts, 47 items were published by either University of Minnesota
                        or Duke University Presses. In these cases, we took advantage of a
                        previously unused term of our libraries’ license agreements with the presses
                        and/or distributors of the ebooks (entities such as ProQuest), and asked our
                        library to submit requests for DRM-free complete copies of each of the texts
                        for text and data mining purposes.<note> We selected these presses as they
                           were the only ones from whom we needed a substantial number of texts. We
                           later learned that when the ebooks are distributed by a third party (such
                           as Project Muse or ProQuest), these requests are handled by the
                           distributor, not the publisher, which would have made a broader request
                           more practical (though it is not clear if every distributor still checks
                           each request with every publisher).</note> One publisher mentioned they
                        had not received a similar request before, but after some back and forth to
                        clarify what we were requesting, we eventually received delivery of the
                        full-text PDFs. </p>

                     <p>In cases where the library did not already own or perpetually license an
                        ebook, we requested that the library purchase the ebook if possible. The
                        acquisition of ebooks by libraries is somewhat different from the processes
                        by which individuals purchase the rights to such works. In Stanford’s case,
                        library ebook acquisitions are made via GOBI, a vendor system owned by
                        EBSCO, the library services corporation likely more familiar to scholars for
                        its databases. Ebooks can be purchased through GOBI (or similar competing
                        systems) by libraries according to a variety of schemes determined by their
                        publishers and/or platform (such as Project Muse) — for example, cheaper
                        single-user vs more expensive unlimited-user licenses, DRM-free (for a
                        higher price), etc. Regardless of the attendant terms, ebooks purchased in
                        this way are considerably more expensive than those purchased by individual
                        users, and, for libraries, also more expensive than print <ptr target="#bailey_etal2017"/>. While we cannot share exact costs for this
                        project, the library purchased approximately 130 ebooks, with ebook prices
                        for academic libraries ranging from USD $55-$200, averaging approximately
                        $150 per book, for an approximate total cost of around $19,500. Originally,
                        we had intended to finance these purchases via grant funding, but we were
                        fortunate to discover that the relative importance of many of the texts we
                        had selected meant the library was able to use our project to help justify a
                        long-term shift in its collection. For many of the texts we had selected,
                        the library has long had multiple print copies (especially for titles
                        originally published before the digital era) but in many cases, in the ebook
                        era, when it can afford to do so, our library would prefer to hold a single
                        — perhaps less frequently consulted — print copy alongside a perpetually
                        licensed ebook. </p>

                  </div>

                  <div>

                     <head>Google Books Return</head>

                     <p>When we could not obtain born-digital copies of the texts we needed, we
                        turned to digitization, initially examining the set of texts that Stanford
                        received from Google in exchange for participation in their Google Books
                        digitization project. While these texts are older scans, and the provided
                        OCR does not match modern standards, we deemed this an acceptable trade-off
                        for the labor savings. (Of course, these digital versions were produced
                        using extensive human labour, as works like Andrew Norman Wilson’s <hi rend="italic">ScanOps</hi> (2012) and the research of <ptr target="#chalmers_edwards2017"/> make clear.) Unfortunately, the coverage
                        of the Google Books collection is far from comprehensive, and even in the
                        case of texts that Stanford provided to Google, there was no guarantee that
                        they were ultimately digitized from Stanford’s copy (the terms of the
                        agreement between Stanford and Google give us access only to those works
                        actually digitized from Stanford’s own collection, not to all works provided
                        for digitization).<note> In addition to whether Stanford specifically could
                           access the digitized text because of its place in Google’s scanning
                           order, the coverage of our recovered corpus is also fundamentally
                           influenced by Google’s digitization pipeline and material workflow.
                           Google’s process excluded fragile, large, small, tightly-bound, brittle,
                           and uncatalogued books from its scanning queue, taking an <quote rend="inline">opportunistic rather than systematic approach to
                              digitization [that] may amplify existing selection biases in physical
                              print collections</quote>
                           <ptr target="#chalmers_edwards2017" loc="13"/>. For the purposes of our
                           project, Google’s priorities — far removed from those of DH and literary
                           researchers and of the libraries Google pulled books from — resulted in a
                           corpus that is not only biased towards the canonical and available, but
                           also excludes some of our highest priority texts.</note> For scholars
                        fortunate enough to be working at institutions whose collections Google
                        digitized more extensively, this may be a more fruitful source of digitized
                        texts (if their libraries are equipped to provide access, that is). In the
                        end, we were able to obtain 35 titles in this way, from a total of 53
                        provided to Google for potential digitization. </p>
                  </div>

                  <div>

                     <head>Scanned Books</head>

                     <p>A significant number of texts in our final corpus were, ultimately, derived
                        from physical copies we purchased specifically for this project. In the end,
                        we turned to self-digitization with greater frequency than we had initially
                        anticipated, simply because not all texts we wished to include in our corpus
                        were available in digital formats. In addition to relatively obscure items,
                        including items out of print since the widespread adoption of ebook
                        publication, we found that born-digital copies of some prominent works of
                        theory simply did not exist (e.g., <hi rend="italic">S/Z </hi>by Roland
                        Barthes). Additionally, sometimes digital versions of texts <hi rend="italic">were </hi>available, but not from a publication that
                        offered ebook license terms allowing us to extract the text: the DMCA
                        exemption does not in any way affect the contractual terms of ebook license
                        agreements (notably, this is the case for many direct-to-consumer ebook
                        distributors such as Amazon, although we were able to purchase a small
                        number of non-institutionally licensed ebooks directly from some university
                        presses). In these cases, we were also forced to fall back on scanning the
                        texts.</p>

                     <p> We divided our scanning into several batches, according primarily to the
                        expense and rarity of the physical book in question (see Figure 3). In this,
                        we were once again significantly aided by the library, whose Digital
                        Production Group agreed to scan a significant number of items (86) using
                        their large-scale scanning system. In cases where a library copy of the item
                        was available, this semi-automated scanning was our preferred option;
                        however, the relatively slow pace of this process (which needed to contend
                        with other, higher-priority digitization needs) meant that we sometimes
                        opted to scan items ourselves. For items in print and available in
                        relatively affordable editions, we opted for destructive scanning, in which
                        the books are disassembled and scanned page-by-page using a feed scanner.
                        For the majority (33 of 35 items) of our destructive scanning, we used a
                        commercial scanning service. These companies offer a variety of services,
                        typically priced per-item (e.g., the company we selected charges more for
                        higher quality scans, OCR, or the option of receiving items directly from
                        booksellers), and we found that the cost compared favorably to the cost of
                        the books themselves. Additionally, however, we destructively scanned two
                        items ourselves (both texts we had originally anticipated obtaining
                        digitally). For these books, one of our project members used a bandsaw to
                        remove the books’ spines and we scanned the pages using a sheet-feed
                        scanner. Since it requires some care and experience to produce cleanly cut
                        pages that scan without issue in this way — not to mention access to power
                        tools (or, ideally, a more specialized guillotine cutter such as would be
                        used for bookbinding) — we think that for most projects, small lots of books
                        are most practically scanned non-destructively, with larger lots scanned
                        destructively by professional services. Finally, we scanned two out-of-print
                        books manually using an overhead scanner. </p>

                     <p>Whatever the manner of scanning used, we converted our digital images to
                        plain text using ABBYY FineReader, which, for our relatively modern texts in
                        English, produced the best results of the software we tried.<note> Note that
                           the MacOS and Windows versions of FineReader are not equivalent; our
                           testing showed the full-featured Windows version to noticeably outperform
                           the Mac version.</note> Since FineReader is commercial software (and
                        moderately expensive at that), we first tried a variety of other
                        alternatives, including the OCR engine embedded in Adobe Acrobat (also
                        commercial, and more expensive than FineReader, but something that some
                        scholars may already pay for), as well as the open-source Tesseract, both of
                        which performed noticeably worse than FineReader (we were particularly
                        disappointed by the results from Tesseract). Depending on the nature of
                        texts being digitized, as well as the downstream use cases (e.g., some users
                        will prefer raw xml OCR output to plaintext), we anticipate that the
                        relative tradeoffs of convenience, cost and performance will vary from
                        project to project and we encourage anyone considering these questions to
                        test a variety of options. </p>

                     <figure>
                        <head>Sources of digitized books. Readers can download the <ref target="resources/data/code_plot.R">code plots.</ref>
                        </head>
                        <figDesc>Treemap showing relative numbers of each source of digitized books:
                           86 from library scanning, 35 each from Google Books and destructive
                           scanning, and 2 for overhead scanning. </figDesc>
                        <graphic url="resources/images/figure03.png"/>
                     </figure>
                  </div>

                  <div>

                     <head>Articles</head>

                     <p>While our corpus of theory was assembled with a deliberate focus on the
                        scholarly monograph, we included a number of articles and book chapters as
                        well, in addition to a small number of miscellaneous shorter works (e.g.
                        pamphlets, transcripts of speeches). Russian Formalism, for example,
                        features significantly fewer monographs than other schools of literary
                        theory: important works by Boris Eichenbaum, Yury Tynyanov, and Viktor
                        Shklovsky translated into English are essays, not full-length books. While
                        in a small number of cases we scanned older works from journals or collected
                        editions, in most cases, we were able to obtain these texts in their
                        entirety via standard scholarly databases,<note> We are fortunate that we
                           had access to these databases and recognize that this access is subject
                           to our individual institution’s financial decisions. Scholarly database
                           access is notoriously costly to access at the level of the institution,
                           as well as for independent researchers <ptr target="#bjork2021"/>, <ptr target="#bosch_etal2019"/>.</note> confirming one of our project’s
                        premises, that computational metacritical and scientometric work on articles
                        is considerably easier than similar projects focused on the monograph.</p>

                  </div>


                  <div>

                     <head>Open Access and Works in the Public Domain</head>

                     <p>A small but important part of our corpus consists of texts not protected by
                        copyright, a group that includes both texts whose circumstances of
                        publication mean that they are no longer protected in the United States (a
                        category that will have a different parameters for scholars working in
                        countries with different copyright terms), as well as texts that have been
                        proactively released into the public domain or made available under other
                        open-access (OA) terms.<note> Note that OA is not the same as open-source
                           (OS) — which, while predominantly used as a framework for understanding
                           software, has been applied to other domains (see, for example, the essays
                           in <ptr target="#hawkins2021"/>).</note> Unfortunately, OA materials for
                        human reading are not the same as OA materials for text-mining and may not
                        be easy to obtain in a format compatible with computational study. Some OA
                        texts were not available for complete download and required
                        chapter-by-chapter navigation. (We encountered this same phenomenon with
                        non-OA texts, where it seemed to be a text-protection mechanism, but many OA
                        platforms appeared to do so simply to break longer texts into human-friendly
                        units). While some OA platforms were user-friendly for both text-mining and
                        reading in a range of formats, this was far more often the case for
                        public-domain works, which were usually already available for direct
                        download in raw text form, albeit with rather uneven quality. </p>

                     <p>While OA and public domain texts are legally (and often practically) easier
                        to work with than those protected by copyright, we found that identifying
                        these texts as such was not always a straightforward process. OA monographs,
                        in particular, are not always cataloged by libraries, meaning individual
                        scholars must locate an OA copy and, more importantly, know that an OA copy
                        might exist, as publishers have a financial interest in making OA versions
                        of their texts as obscure as possible. We were surprised to find, moreover,
                        that even the category of public domain texts could occasionally pose
                        problems. In particular, while it is in principle relatively straightforward
                        to identify texts published before the date of copyright as belonging to the
                        public domain, translations (protected by copyright of their own) are not
                        always as easy to date or identify. For texts like Vladimir Lenin’s <hi rend="italic">Imperialism, the Highest Stage of Capitalism</hi>,
                        underlying copyright law regarding some translations produced in the earlier
                        years of the USSR may have allowed us to use copies freely available online.
                        But the complexities of international copyright law sometimes led us to
                        purchase such texts anyway when we found the legal guidelines
                        undecipherable, especially since these texts were cheap and frequently still
                        in print.</p>
                  </div>
               </div>

               <div>

                  <head>Data management</head>

                  <div>

                     <head>Data Security Considerations</head>
                     <p>The DMCA exemption contains the stipulation that data obtained via the
                        circumvention of DRM for the purposes of text and data mining be secured
                        using <quote rend="inline">effective security measures</quote>, which are
                        defined as those agreed to by the university and the rights-holder, or,
                        absent such an agreement, those measures <quote rend="inline">that the
                           institution uses to keep its own highly confidential information
                           secure.</quote><note> Specifically: The term <quote rend="inline">effective security measures</quote> is defined as: <quote rend="inline">(1) Security measures that have been agreed to by all
                              interested copyright owners of literary works and institutions of
                              higher education; or (2) Security measures that the institution uses
                              to keep its own highly confidential information secure.</quote>
                           (b)(5)(ii)(B)</note> We anticipated from the outset of this project that
                        this provision would prove significantly burdensome, and we found that this
                        indeed was the case. Much of the burden has been interpretative: the text of
                        the exemption does not clearly specify what kinds of measures should be
                        taken by the institution to secure the data in question. Universities
                        typically hold a wide variety of confidential information — employee
                        records, medical records, banking details, and more — and it is not clear
                        what standards should apply in the case of text mining as opposed to what
                           <ptr target="#borgman2018"/> terms <quote rend="inline">grey
                        data</quote>. Moreover, in developing workflows that complied with the
                        exemption, we found not only that existing university guidance on data
                        security and risk did not anticipate our particular situation, but also that
                        the very tools set up by the university to help scholars assess their data
                        risks were fundamentally unsuited for our particular situation. Our approach
                        was ultimately determined by a series of in-person meetings with the
                        university’s general counsel and information security office, an additional
                        expenditure of the project team and university’s labor to comply with the
                        exemption. These meetings led to the determination that data obtained via
                        the DMCA exemption would be treated as <quote rend="inline">high
                           risk</quote>, a data category at our institution that includes, among
                        other things, personally identifiable health information, USA Social
                        Security numbers, and credit card numbers.<note> We use the term <quote rend="inline">high-risk</quote> below in reference not just to our own
                           institution’s data risk classification scheme, but also, more broadly, to
                           gesture to data deemed particularly sensitive by other analogous
                           institutional schemes for classifying and handling data that poses
                           privacy, financial, ethical, legal or other risks.</note>
                     </p>

                     <p> As digital humanists accustomed to working on our own devices and with
                        varied technical skill sets, we have found the requirements for processing
                        and collecting high-risk data challenging. While our university’s data
                        management policies allow personal computers to be configured for high-risk
                        data and for such data to be stored locally, doing so requires a degree of
                        oversight and monitoring that we felt was broadly unsuitable for
                        personally-owned devices that are also used for non-university purposes.
                        While it is our intention to perform much of the downstream analysis of our
                        texts using our university’s secure high-performance computing (HPC)
                        environment, this was not a suitable solution for collection of our
                           texts.<note> As our university also maintains a separate HPC environment
                           for non-high-risk data which allows non-paying users to access the
                           computational resources of paid users when they would otherwise be idle,
                           switching to the per-hourly secure HPC environment will also entail
                           additional financial costs. This cost-differential reflects in
                           significant part the primary use case for high-risk compliant
                           computational resources, biomedical research funded on an entirely
                           different scale from all other forms of academic research: in the US, for
                           example, the total federal appropriations for the National Institute of
                           Health is $49B compared to $9.45B for the National Science Foundation,
                           and $207M for the National Endowment for the Humanities <ptr target="#nih2023"/>, <ptr target="#nsf2023"/>, <ptr target="#neh2023"/>.</note> First, such an environment imposes a significant technical
                        burden on a task that, otherwise, could be done without coding or
                        command-line experience, familiarity with Amazon Web Services (used by our
                        secure HPC environment), and similar skills. Second, and more importantly,
                        the workflow we adopted to remove the DRM from protected ebooks was one
                        that, to the best of our knowledge, was simply not compatible with such a
                        computing environment. Instead — again attesting to the indirect expenses
                        imposed by the TDM exemption — one of our project members furnished us with
                        a spare computer which we configured for high-risk data.</p>
                  </div>

                  <div>
                     <head>Processing Ebooks and Removing DRM</head>
                     <p>Users who, pursuant to any of the previously granted exemptions of section
                        201 of the DMCA, have attempted to circumvent DRM for any of the allowed
                        purposes have long reported technical difficulties.<note> This is true for
                           both deliberate DRM/TPM <ptr target="#fisher2020"/> and the lack of
                           interoperability found in older, proprietary software <ptr target="#mcdonough_etal2010"/>. </note> In many cases, the lack of
                        legitimate tools for such a task was a significant obstacle. The tools
                        developed for removing the DRM from ebooks are, generally speaking, intended
                        for piracy, not fair use, let alone use protected by an exemption to the
                        DMCA. Because the development of such tools is intended to facilitate the
                        violation of copyright, and is in violation of the DMCA (i.e., unlawful in
                        the USA), their availability is subject to sudden takedown and cannot be
                        guaranteed in the long term. Additionally, such tools are engaged in a kind
                        of arms-race with the developers of DRM technology, and there is no
                        guarantee that, at any given time, these tools will remain operational. </p>

                     <p> All the ebook DRM that we encountered in this project took the form of
                        works distributed as Adobe Digital Editions. To extract the text from such
                        works, we first downloaded them and used Adobe Digital Editions to extract a
                        protected PDF file for each text. We were then able to remove the protection
                        from this PDF using a plugin for the open-source ebook manager Calibre — it
                        is this plugin specifically whose development is unlawful. Subsequently,
                        Calibre’s built-in ebook conversion tools allow the PDF to be converted to a
                        text file. Technically, this process is relatively straightforward, but its
                        reliance on Adobe Digital Editions poses two significant problems. First,
                        there is no command-line interface for this piece of proprietary software,
                        making it more challenging to use as part of a larger workflow. (Calibre
                        does provide a command line interface.) Second, Adobe Digital Editions is
                        limited to Windows and MacOS, precluding its use on most Linux-based HPC or
                        remote computing environments. For our project, the limited availability of
                        ebooks to process via the exemption meant that these limitations were
                        principally of issue because of the aforementioned data management
                        restrictions; for a larger project, however, they would necessitate a
                        significant amount of manual labor. </p>
                  </div>

                  <div>
                     <head>Metadata and Database Management</head>
                     <p>In the early stages of our project, we considered whether or not to store
                        the corpus metadata and the texts themselves in a database (as many DH
                        projects are wont to do), and we made the wrong decision about databases (as
                        many DH projects are wont to do). Initially, our thinking had been that with
                        only about 400 texts, the total amount of data that we would need to track
                        was relatively manageable, and its highly regular, tabular nature meant that
                        a spreadsheet would be an adequate way to store this information.<note> For
                           convenience and to combat the proliferation of columns, we ultimately
                           split this spreadsheet into two unique-identifier linked sheets, one
                           containing data associated primarily with the acquisition of the texts
                           (library records, price information, availability for purchase, etc.) and
                           one with more traditionally bibliographic metadata.</note> Our approach
                        was first vexed by book-historical research questions that led us to collect
                        more thorough metadata of more kinds than we originally anticipated. We were
                        interested in equipping ourselves to answer questions — or at least raise
                        them — about, for example, the role of particular publishing houses in the
                        development of literary theory, and we wanted to be able to attend closely
                        to questions of translation, necessitating tracking translation dates as
                        well as dates of first publication (in the original language as well as
                        English), and so on. As columns and spreadsheets proliferated, recording
                        consistent and complete metadata became onerous.</p>

                     <p>These choices resulted in endless manual reconciliation of our data against
                        our library’s records, often complicated by the fundamental differences in
                        data models preferred by literary scholars and libraries. To us as critics,
                        the basic unit of data is a <hi rend="italic">literary </hi>(or critical)
                           <hi rend="italic">work</hi>, for example Franz Fanon’s <hi rend="italic">The Wretched of the Earth</hi>, with associated metadata concerning,
                        e.g., its first publication, its eventual translation into English, which
                        version of the text is included in our corpus, the presence of an
                        introduction by Jean Paul Sartre, and so on.<note> Of course, we are not
                           ignorant of the multiplicity of such texts and the differences between
                           editions — but we are accustomed to viewing these items as,
                           fundamentally, editions<hi rend="italic"> of a text</hi>, a unifying
                           structure that in most cases connects them. Even so, earlier projects
                           from the Literary Lab have confronted the process of selecting between
                           multiple editions of, for example, <hi rend="italic">Robinson
                           Crusoe</hi>, whose text changes drastically, on a sentence-by-sentence
                           level, between editions from the early and the late eighteenth-century.
                        </note> Our library, by contrast, tracks five individual versions of this
                        text in English alone, in addition to two versions in French, one in
                        Persian, one in Pashto, and a French edition of Fanon’s <hi rend="italic">Œuvres.</hi> For the library’s purposes, texts come in <hi rend="italic">editions</hi>, and while these texts are usefully cross-referenced by
                        the inclusion in their catalog data of the original French title, they are
                        fundamentally different books that exist in different numbers of copies —
                        for example, two copies of the 1963-65 Grove Press edition translated by
                        Constance Farrington, and one copy of Grove’s 60th anniversary edition
                        published in 2021 (<quote rend="inline">translated from the French by
                           Richard Philcox; with commentary by Jean-Paul Sartre, Homi K. Bhabha, and
                           Cornel West</quote>; <ptr target="#fanon2021"/>. While in principle our
                        work-oriented model could link each item to a particular instantiation
                        available via the library (or an instantiation of that text purchased for
                        this project), this proved exceptionally difficult to do, in large part
                        because of the need to track library-collection data at the level of the
                        work, not the edition. For example, we found ourselves in situations where
                        we wanted, for one single work, to track that one edition had been sent to
                        Google for potential inclusion in Google Books; to note that another ebook
                        version was available only via subscription; and to record a reference to a
                        new, purchased ebook of the same work. We found, moreover, that as we
                        navigated the library ecosystem, we were continually returning to the same
                        items’ catalog entries to record data that we had not initially collected. A
                        database that linked our 400-odd works directly to the (potentially
                        multiple) MARC records associated with all the editions of each of the texts
                        would have, ultimately, proven simpler.</p>
                  </div>
               </div>
            </div>

            <div>

               <head>Discussion</head>

               <div>
                  <head>Analysis of sources</head>
                  <p>After sixteen months of work, we obtained, or, in many cases, created plain
                     text files for 383 of 402 texts, as well as DRM-protected ebooks corresponding
                     to the remaining 19 texts (which we are waiting to process so as not to create
                     high risk data before we need to use it). Of these 402 texts, 206 — somewhat
                     more than half — were born-digital files derived from ebooks, nearly
                     exclusively “circulating” texts owned or perpetually licensed to Stanford
                     University Libraries (see Figure 4). A small number of additional ebooks were
                     purchased on non-institutional licenses. While born-digital sources, taken
                     collectively, provide cleaner texts than digitized ones, they are encumbered by
                     awkward workflows to remove DRM and extract text. Moreover, the need to
                     robustly track license information for each text requires access to data that
                     may not always be available to library users, and which is in any event likely
                     unfamiliar to most scholars working outside of libraries. We found, then, that
                     although ebooks presented a certain measure of (physical) labor savings and
                     offer the highest text quality, they were correspondingly more complicated to
                     work with than digitized sources, to the extent that, for a project of this
                     size, digitization would have been the simpler and ultimately faster way to
                     obtain our texts. Because many of these logistical costs are relatively fixed,
                     larger projects may find greater benefit from working with library ebooks, and
                     projects that already have the infrastructure to track licensing details,
                     secure their data, and similar, may likewise find this balance tilted in favor
                     of ebook sources.</p>

                  <figure>
                     <head>Overall corpus source types</head>
                     <figDesc>Treemap of source types included in the final corpus: 206 ebooks, 124
                        printed works, 37 articles, and 35 works recovered from Google
                        Books. </figDesc>
                     <graphic url="resources/images/figure04.png"/>
                  </figure>

                  <p>One particular aspect of the library-scholar interface that we feel it is
                     important to draw attention to is the complex interrelation of public facing
                     library catalogues, internal library records, and scholarly metadata. For
                     projects drawing upon library resources, and, <hi rend="italic">a
                     fortiori</hi>, when those projects are more bibliographically rigorous than
                     ours, we think that aligning these three systems of bibliographic information
                     will be invaluable. For scholars, this means acquiescing to the printed reality
                     that texts come in multiple versions, which can be handled either by a
                     many-to-one database relationship, or by simply committing to one particular
                     version of a text, while for libraries, this means exposing data that, while
                     not of general interest to most users, is necessary for scholars who might wish
                     to cross-reference between, for example, call numbers, barcodes and book
                     titles. Making this information available to scholars exposes them to a mode of
                     thinking about books that is importantly different from the perspective adopted
                     by most researchers. This allows researchers to expedite and clarify their
                     communication with librarians, for example by providing barcode numbers as
                     disambiguation in library requests, rather than more cumbersome edition
                     information.</p>

                  <p> The single most important uses of MARC record information for this project has
                     been to verify ebook ownership: like most academic library catalogues,
                     Stanford’s online system makes ebooks available to end-users in a way that does
                     not expose the ebook’s source, whereas the MARC records for each digital item
                     include field 856, which contains <quote rend="inline">Electronic Location and
                        Access</quote> information, which our library uses to track whether a
                     particular ebook was <quote rend="inline">subscribed</quote> or <quote rend="inline">purchased</quote>. To facilitate the use of library purchases
                     for digital research and text mining, we suggest that information such as this
                     be easily accessed by researchers, or even searchable as a filter option
                     through the online catalogue. While the direct use of this information by
                     researchers may be relatively esoteric, we think that a wider number of users
                     concerned with access to (and the affordability of) library resources should be
                     presented with information on which holdings their library owns, and which are
                     merely licensed. </p>

                  <p>In particular, the impediments posed by commercial, individual (i.e.,
                     non-institutional) ebook licenses make the most straightforward approach to
                     ebook acquisition significantly more burdensome. As the DMCA exemption provides
                     no relief from restrictive license agreements, many texts are simply
                     unavailable commercially under terms that permit the extraction of text for
                     text-mining purposes. Even when texts are available without such restrictions,
                     the texts in question often need to be sourced from smaller publishers and
                     distributors (rather than large online resellers), increasing the logistical
                     complexity of identifying, ordering, and tracking such works. </p>

                  <p>Texts scanned and digitized from originally printed sources (as opposed to born
                     digital texts) offer the considerable advantage of simplicity, and with the
                     increasing power and speed of modern scanning technology and OCR, provide the
                     possibility of relatively clean texts, albeit less so when texts include tables
                     or visual media.<note> OCR has improved dramatically over the last decade
                        (notably, since our Google Books files were digitized), although
                        benchmarking and measuring that improvement for individual corpora remains
                        difficult <ptr target="#hegghammer2021"/>, <ptr target="#neudecker_etal2021"/>. As the quality of OCR has known downstream effects in text-mining that
                        vary by method (e.g. topic modeling or natural language processing), the
                        benefits of born-digital texts over digitized ones will vary by project <ptr target="#hegghammer2021"/>.</note> Digitization, moreover, allows for the
                     creation of output in a wide variety of formats, including markup-based OCR
                     formats that preserve information about page layout and similar textual
                     features, which can be difficult to extract from ebooks. The principal
                     disadvantage of digitization is time: with overhead scanners, digitization is
                     slow, especially if image quality is a priority, and mechanized solutions are
                     imperfect and tremendously expensive. Feed-scanning is faster and produces
                     acceptable results relatively quickly, but it precludes non-destructive
                     scanning of books, necessitating higher costs for the end-users.<note> Google
                        Books scanned their books using manual labor at a cost of US$10 each, a
                        non-destructive approach dependent on <quote rend="inline">an army of human
                           page-turners,</quote> a sizable amount of computational power, and an
                        agreement with libraries that did not require them to purchase the books in
                        question <ptr target="#leetaru2008"/>.</note> And, while disassembled texts can be stored
                     for future re-scanning, most digital humanities groups are probably not
                     equipped to store tens or hundreds of thousands of pages of unbound literature
                     in anticipation of future developments in scanning technology. While the
                     digital storage of scanned material is less of an issue than the physical, it
                     is nevertheless worth calling attention to, since high-resolution images of
                     thousands of pages of text lead rapidly to voluminous storage requirements; and
                     though such images can be discarded, this essentially <q>freezes</q> the OCR
                     output and prevents future reprocessing. </p>

                  <figure>
                     <head>Distribution (by age and source type) of each theoretical field</head>
                     <figDesc>Plot showing the distribution by source type (ebook, print, article or
                        google books) and date for each of the 8 theoretical fields in the
                        corpus.</figDesc>
                     <graphic url="resources/images/figure05.png"/>
                  </figure>

                  <p>The works included in our corpus span the range of the 20th and 21st centuries,
                     with a small tail into the 19th (three works by Karl Marx and one by Sigmund
                     Freud), and we had expected that the age of the texts in question would have
                     some correlation with the medium in which it was possible to locate those
                     texts. In practice, this effect was relatively minor: the median age of the
                     works in each of our source type categories ranges from 1978 (works recovered
                     from Google Books) to 1989 (ebooks) (see Figure 6). Of potentially greater
                     interest, nearly all — 71 of 85 non-article works first published after 2000,
                     including 29 of 32 from after 2010 — of the most recent works were available in
                     born-digital format (see Figure 7). </p>

                  <figure>
                     <head>Distribution of texts’ age by each source type category. Medians
                        indicated by horizontal rule.</head>
                     <figDesc>Box and whisker plot showing the age of each of the works by their
                        source type (ebook, print, article or google books).</figDesc>
                     <graphic url="resources/images/figure06.png"/>
                  </figure>


                  <figure>
                     <head>Proportion of corpus texts available in born-digital format.</head>
                     <figDesc>Density plot showing the relative proportion of works available in
                        born digital formats by date of first publication. The majority of works
                        published in the 2000s are available in born-digital formats.</figDesc>
                     <graphic url="resources/images/figure07.png"/>
                  </figure>

                  <p>Since a significant number of older texts are also available as ebooks, we
                     hypothesize that ebook availability, while obviously correlated with recency,
                     has much to do with the specifics of publication history, scholarly use, and
                     other similar factors that we did not track (recent new editions, frequent
                     course assignments, etc.). We hypothesize, for example, that for theoretical
                     fields whose popularity peaked prior to 1990, such as Poststructuralism,
                     libraries would own multiple print copies of books related to this field and
                     thus be disinclined to repurchase ebooks. Theoretical fields with more recent
                     popular engagement, such as Black studies and Postcolonial studies, had a
                     higher proportion of ebooks even for texts written prior to 1990, as demand
                     within libraries resulted in ebook purchases (see Figure 5). Since one of the
                     main limitations of working with the DMCA exemption for this project has been
                     the limited number of accessible ebooks, further research on the forces shaping
                     ebook availability would be a great asset to digital humanists, since it would
                     better equip researchers to understand whether the exemption might be valuable
                     to them in assembling corpora. Since ebook availability varies from country to
                     country, internationally collaborative work in this area would be especially
                     valuable, since it would help to illuminate different patterns in ebook
                     availability across different national and linguistic contexts. Similarly, we
                     suspect that the trends in academic publishing that we observe may be quite
                     different from those that obtain in the case of primary sources such as novels
                     and other texts with significant commercial markets. </p>
               </div>
            </div>

            <div>
               <head>Articles and Monographs</head>

               <p>Previous DH work on academic publication, especially that modelled after
                  scientometric work conducted in the sciences and social sciences, has tended to
                  focus on the scholarly article, at this point nearly the sole academic currency in
                  most scientific fields <ptr target="#goldstone_underwood2012"/>, <ptr target="#riddell2014"/>, <ptr target="#feeney2017"/>, <ptr target="#ambrosino_etal2018"/>, <ptr target="#piper2020"/>. Work in this area,
                  moreover, has been greatly streamlined by requirements for open-access
                  publication, as well as government-funded databases such as PubMed, which make the
                  text of academic articles relatively accessible to scholarly research <ptr target="#gusenbauer_haddaway2020"/>, <ptr target="#falagas_etal2008"/>. In the
                  humanities, by contrast, the state of open-access publication is significantly
                  less advanced, and the monograph, typically produced as an expensive printed book
                  by an academic publisher, remains an important element in the scholarly economy,
                  albeit one increasingly supplemented by simultaneous publication as an expensive
                  academic ebook. Monographs are well-established as central to humanistic citation
                     <ptr target="#thompson2002"/> and tenure <ptr target="#estabrook_warner2003"/>.
                  In neither electronic nor codex form, however, are academic books particularly
                  amenable to computational exploration, which we posit has been the reason for
                  their exclusion even from studies of scholarship in the humanities, where their
                  exclusion significantly reshapes the scholarly field. For this reason, the present
                  project was deliberately designed to center the academic book and further work on
                  monographs across a wide variety of humanistic fields is clearly needed. </p>
            </div>

            <div>

               <head>Conclusion</head>

               <p>Embarking on this project, we had hoped that, over the past decade, the increase
                  in ebook publications and their subsequent heightened representation in library
                  collections, together with the movement towards open access scholarship and,
                  recently, the changes in the US legal regime surrounding in-copyright electronic
                  media would make the task of assembling a corpus of bespoke texts easier or, at
                  the very least, attainable. What we have found instead is that humanities
                  scholars, facing the time and funding constraints endemic to our field, still face
                  significant challenges when trying to source a bibliography of in-copyright texts
                  for the purposes of text and data mining. Given the complexities (and expense)
                  involved in assembling a corpus of e-books and legally extracting their texts, it
                  is still the case that it may be preferrable to assemble a print corpus to scan,
                  OCR, and hand correct. While intensive in terms of labor, it may still save a
                  significant amount of time. </p>

               <p>From a bibliographic standpoint, we found the necessity of consulting, comparing,
                  and tracking the availability of texts across multiple digital venues and
                  platforms (open-access, library ebooks, digitized texts from already extant
                  collections, etc.) to bring with it significant administrative overhead. Such
                  overhead requires the expertise and access permissions of a librarian, which may
                  not be available to all research teams and is beyond the reach of individual
                  researchers not affiliated with a library. While the recent changes to the DMCA
                  have improved the legal situation for researchers wishing to work with copyrighted
                  texts, the restrictions imposed by the DMCA exemption have made working under its
                  terms particularly challenging. Projects considering this route would be
                  well-advised to consider whether sufficient numbers of institutionally-owned,
                  DRM-protected ebooks exist to justify the additional labor required to work with
                  this category of text. We are more optimistic about the use of DRM-free library
                  texts, which poses fewer legal obstacles and which might fruitfully be a
                  supplemental source of texts for many projects. The expense involved in acquiring
                  library ebooks, however, compares relatively unfavorably to digitization, and,
                  particularly as the quality of OCR continues to improve, we are skeptical that
                  this will play a significant role in the future acquisition of digital
                  corpora.</p>

               <p>Despite the difficulties that we have encountered, there are two particular
                  domains of academic publishing and book acquisition in which relatively minor
                  changes could offer substantial improvements to the process of corpus building and
                  which we offer as recommendations for future work. First, the movement towards
                  open-access publication for academic texts could be transformative for scholars
                  looking to do cultural analytics work on contemporary scholarly sources. Although
                  researchers have urged scholarly publication venues to embrace open-access
                  publishing for much of the past decade, even those that do take up the significant
                  financial and publishing challenges of doing so still undertake this work through
                  a non-standardized and even scattershot approach <ptr target="#eve2014"/>. The
                  variety of formats that open-access texts take, from PDF files to proprietary data
                  formats on publisher websites, means that consistent use of these resources as
                  textual data remains much more difficult than it should be. We believe that
                  publishers who are willing to consider open-access publication should be
                  encouraged to adopt data standards for open-access formats and to make text files
                  and other machine-readable file formats of the open access materials available to
                     researchers.<note> Given that the concerns around open access availability lie
                     in the possibility of consumptive use on the part of researchers (that is, that
                     they could read the open access version instead of buying the book), making the
                     text available in a format amendable to TDM (for example a text file) should
                     not increase this risk, as most readers do not want to consume text in its raw
                     form. </note>
               </p>

               <p>Second, the importance of the library in our research pipeline is difficult to
                  overstate, in terms of both resources and relationships: in particular, the
                  willingness of library staff to work with team members to acquire these texts, and
                  their financial ability to do so, has made our project possible. In particular,
                  the inclusion of a hybrid library staff member among our project team members
                  indicates the importance of the library/researcher relationship in successfully
                  assembling a corpus of this nature, and we are fortunate to work at an institution
                  whose infrastructure and resources allow the privileging of this relationship.
                  Nevertheless, barriers remain. Even for forward-thinking libraries that have
                  full-time staff dedicated to digital licensing and who include language in their
                  licenses about obtaining full-text files of ebooks they have purchased, there are
                  still significant barriers to actually obtaining these files. Delays by publishers
                  and vendors as they work through their legal teams (despite license language);
                  liaising between researchers and library staff to identify the correct vendor and
                  contact information; clarifying that what is needed are the full text resources
                  rather than access to a proprietary TDM platform: all combine to add to the
                  difficulty of this work. A concerted effort by university libraries, particularly
                  those with the resources of Stanford, to normalize the availability of collection
                  material for TDM research from the publishers could help to create a sustainable
                  set of practices for researchers at other, often less well-resourced institutions.
                  Further, given their work with publishers, libraries themselves might be able to
                  prioritize building corpora out of their collection materials that they could make
                  available to scholars at their institutions. Many libraries are already engaged in
                  building digital repositories of data and, to the extent that this could be
                  extended to curating data that is available for TDM research, libraries seem
                  excellently positioned to play a significant role in developing and disseminating
                  these resources. Finally, in the US, the importance of institutional text
                  ownership to the DMCA exemption provides an impetus for libraries to focus on
                  purchasing texts rather than renting them through third-party content
                  distributors.</p>

               <p>In a global context, the exemption to the DMCA is not likely to be directly
                  relevant to scholars not actively engaged in collaboration with US-based
                  researchers, and the particular legal structures that we have had to navigate will
                  be different from those encountered by researchers working in other countries. A
                  large number of the issues we have encountered in assembling our corpora have <hi rend="italic">not </hi>been legal, however, and we imagine that many of the
                  practical ramifications of navigating ebook licenses, tracking subscription data,
                  and similar will apply to such projects, even if, for example, the onerous data
                  security provisions of the DMCA exemption prove to be singular. More
                  fundamentally, we think our work offers a strong case for the benefits of
                  digitization’s simplicity. Of course, large scale digitization projects already
                  require extensive infrastructure; and for projects larger than ours, the benefits
                  of adapting to a regime such as the DMCA, and the potential for increased savings
                  of time and money through the use of ebooks will require careful consideration of
                  the possibility of working with ebooks, whether obtained via licensing agreements
                  allowing for text and data mining, or DRM-circumvention exceptions designed to
                  facilitate research.</p>

               <p>In the fall of 2024, the TDM exemption to the DMCA was renewed, subject to minor
                  changes intended to better facilitate collaboration between institutions. The most
                  substantive of these changes allows for the sharing of in-copyright digital
                  corpora collected in line with the exemption between researchers at different
                  institutions, even if they do not both own a copy of the text. (Previously,
                  sharing corpora between institutions was only possible between direct
                  collaborators working together on the same research project.) While this will
                  prove particularly valuable for researchers working at less resourced institutions
                  (since it will enable the construction of mutually beneficial and partially open
                  corpora) the added administrative overhead of such collaborations will only add to
                  the bookkeeping burdens faced by researchers that we describe above. Moreover, the
                  work of building and maintaining the networks required for creating such shared
                  resources is likely to fall disproportionately to less privileged scholars, and
                  may not appeal to scholars who can more easily rely on their own institution’s
                  resources. We suspect, too, that developing corpora that can be shared across
                  international borders will result in further complications due to the need to
                  comply with two (or more) potentially contradictory regulatory frameworks. </p>

               <p>Overall, then, while the bibliographic and material conditions have improved such
                  that it is now possible to obtain a bespoke, hand-assembled corpus of selected
                  Anglophone literary works in digitized format, <hi rend="italic">possible</hi>
                  does not indicate <hi rend="italic">easy</hi>. Without the support of a
                  well-resourced and cooperative library, we do not believe this project could have
                  been completed at all. As it currently stands, the DMCA exemption does not
                  significantly expand scholarly access to fair use text mining, and its data
                  sharing restrictions may reify existing funding barriers for those not working
                  within research-intensive institutions. Moreover, without significant shifts in
                  the availability and cost of ebooks, and legislation to directly address the
                  licensing thereof, frameworks such as the DMCA exemption seem unlikely to prove
                  useful for scholars working at small to medium scales. While it is tempting to
                  look at developments like the DMCA exemption, the recent push by universities to
                  create open-access archives of scholarship, and the ever-increasing size of
                  repositories such as HathiTrust, and speculate that we are on the cusp of a
                  revolution in sourcing an exponentially larger number of texts for our work, we
                  are all too aware that such claims echo the utopian predictions of a decade ago
                  (e.g., <ptr target="#poole2013"/>). As much as things have changed (including the
                  appetite of large language models for ever more terabytes of text), our research
                  still takes place against a system that is structurally hostile to even the least
                  responsible creation of bibliographically-based corpora. </p>
               <p/>

            </div>

         </div>


         <div type="appendix">
            <head>Acknowledgements</head>
            <p>This project is supported by a Public Knowledge grant from the Andrew W. Mellon
               Foundation. We are grateful to Julia Gershon and Sarah Sophie Schwarzhappel,
               undergraduate research assistants to the Stanford Literary Lab, for their aid in the
               collection and verification of some metadata, and to Casey Patterson, who consulted
               on the selection of Black studies texts. We are also tremendously grateful for the
               support of the Stanford University Library system and the help of all of its staff,
               without which it simply would not have been possible to do this project. </p>

            <p>Author Contributions: Matt Warner managed the project, led the writing of the paper
               and produced the visualizations; Nichole Nomura and Carmen Thong (equal
               contributions) contributed to the selection of texts and the collection of data, and
               to the writing of the paper; Alix Keener managed the corpus and worked with the
               library on the acquisition and digitization of texts; Maciej Kurzynski and Alexander
               Sherman (equal contributions with Gabi Birch) contributed to the selection of texts;
               Gabi Birch contributed to the writing of the paper; Mark Algee-Hewitt (supervising
               author) contributed to the writing of the paper, managed the data and legal risk and
               wrote the original grant application. All authors contributed to the theorization and
               intellectual direction of the project.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="ambrosino_etal2018" label="Ambrosino et al. 2018">Ambrosino, A. et al.
               (2018) <title rend="quotes">What topic modeling could reveal about the evolution of
                  economics</title>, <title rend="italic">Journal of Economic Methodology</title>,
               24(4), pp. 329–348.</bibl>

            <bibl xml:id="authors_alliance2021" label="Authors Alliance 2021">Authors Alliance.
               (2021) <title rend="quotes">Update: Librarian of Congress Grants 1201 Exemption to
                  Enable Text Data Mining Research</title>, <title rend="italic">Authors
                  Alliance</title>, 27 October. Available at: <ref target="https://www.authorsalliance.org/2021/10/27/update-librarian-of-congress-grants-1201-exemption-to-enable-text-data-mining-research/">https://www.authorsalliance.org/2021/10/27/update-librarian-of-congress-grants-1201-exemption-to-enable-text-data-mining-research/</ref>
               (Accessed: 19 October 2023).</bibl>

            <bibl xml:id="bailey_etal2017" label="Bailey et al. 2017">Bailey, T.P., Scott, A.L. and
               Best, R.D. (2017) <title rend="quotes">Cost Differentials between E-Books and Print
                  in Academic Libraries</title>. Available at: <ref target="https://doi.org/10.5860/crl.76.1.6">https://doi.org/10.5860/crl.76.1.6</ref>.</bibl>

            <bibl xml:id="bnf_nd" label="BNF n.d.">Bibliothèque nationale de France. (n.d.) <title rend="quotes">Gallica: The BNF Digital Library</title>. Available at: <ref target="https://www.bnf.fr/en/gallica-bnf-digital-library">https://www.bnf.fr/en/gallica-bnf-digital-library</ref> (Accessed: 20 October
               2023).</bibl>

            <bibl xml:id="bjork2021" label="Björk 2021">Björk, B.-C. (2021) <title rend="quotes">Why
                  Is Access to the Scholarly Journal Literature So Expensive?</title>, <title rend="italic">portal: Libraries and the Academy</title>, 21(2), pp. 177–192.
               Available at: <ref target="https://doi.org/10.1353/pla.2021.0010">https://doi.org/10.1353/pla.2021.0010</ref>.</bibl>

            <bibl xml:id="bode2018" label="Bode 2018">Bode, K. (2018) <title rend="italic">A World
                  of Fiction: Digital Collections and the Future of Literary History</title>.
               University of Michigan Press. Available at: <ref target="https://doi.org/10.3998/mpub.8784777">https://doi.org/10.3998/mpub.8784777</ref>.</bibl>

            <bibl xml:id="bode2020" label="Bode 2020">Bode, K. (2020) <title rend="quotes">Why You
                  Can't Model Away Bias</title>, <title rend="italic">Modern Language
                  Quarterly</title>, 81(1), pp. 95–124.</bibl>

            <bibl xml:id="borgman2018" label="Borgman 2018">Borgman, C.L. (2018) <title rend="quotes">Open Data, Grey Data, and Stewardship: Universities at the Privacy
                  Frontier</title>, <title rend="italic">Berkeley Technology Law Journal</title>,
               33(2), pp. 365–412. Available at: <ref target="https://doi.org/10.15779/Z38B56D489">https://doi.org/10.15779/Z38B56D489</ref>.</bibl>

            <bibl xml:id="bosch_etal2019" label="Bosch et al. 2019">Bosch, S., Albee, B. and
               Romaine, S. (2019) <title rend="italic">Deal or No Deal | Periodicals Price Survey
                  2019</title>, <title rend="italic">Library Journal</title>. Available at: <ref target="https://www.libraryjournal.com/story/Deal-or-No-Deal-Periodicals-Price-Survey-2019">https://www.libraryjournal.com/story/Deal-or-No-Deal-Periodicals-Price-Survey-2019</ref>
               (Accessed: 19 October 2023).</bibl>

            <bibl xml:id="chalmers_edwards2017" label="Chalmers and Edwards 2017">Chalmers, M.K. and
               Edwards, P.N. (2017) <title rend="quotes">Producing "one vast index": Google Book
                  Search as an algorithmic system</title>, <title rend="italic">Big Data &amp;
                  Society</title>, 4(2), p. 205395171771695. Available at: <ref target="https://doi.org/10.1177/2053951717716950">https://doi.org/10.1177/2053951717716950</ref>.</bibl>

            <bibl xml:id="chu_etal2023" label="Chu et al. 2023">Chu, T., Song, Z. and Yang, C.
               (2023) <title rend="quotes">How to Protect Copyright Data in Optimization of Large
                  Language Models?</title> Available at: <ref target="https://doi.org/10.48550/ARXIV.2308.12247">https://doi.org/10.48550/ARXIV.2308.12247</ref>.</bibl>

            <bibl xml:id="estabrook_warner2003" label="Estabrook and Warner 2003">Estabrook, L. and
               Warner, B. (2003) <title rend="italic">The Book as the Gold Standard for Tenure and
                  Promotion in the Humanistic Disciplines</title>. Urbana-Champaign: University of
               Illinois.</bibl>

            <bibl xml:id="eve2014" label="Eve 2014">Eve, M.P. (2014) <title rend="italic">Open
                  Access and the Humanities: Contexts, Controversies and the Future</title>.
               Cambridge: Cambridge UP.</bibl>

            <bibl xml:id="falagas_etal2008" label="Falagas et al. 2008">Falagas, M.E. et al. (2008)
                  <title rend="quotes">Comparison of PubMed, Scopus, Web of Science, and Google
                  Scholar: strengths and weaknesses</title>, <title rend="italic">The FASEB
                  Journal</title>, 22(2), pp. 338–342. Available at: <ref target="https://doi.org/10.1096/fj.07-9492LSF">https://doi.org/10.1096/fj.07-9492LSF</ref>.</bibl>

            <bibl xml:id="fanon2021" label="Fanon 2021">Fanon, F. (2021) <title rend="italic">The
                  Wretched of the Earth</title>. 60th anniversary edition. Translated by R. Philcox.
               New York: Grove Press.</bibl>

            <bibl xml:id="feeney2017" label="Feeney 2017">Feeney, M. (2017) <title rend="quotes">What Can Text Mining Reveal about the Use of Newspapers in Research?</title>, in
                  <title rend="italic">Collecting, Preserving, and Transforming the News - for
                  Research and the Public</title>. <title rend="italic">IFLA International News
                  Media Conference</title>, Reykjavik, Iceland.</bibl>

            <bibl xml:id="fisher2020" label="Fisher 2020">Fisher, K. (2020) <title rend="quotes">Copyright and Preservation of Born-digital Materials: Persistent Challenges and
                  Selected Strategies</title>, <title rend="italic">The American Archivist</title>,
               83(2), pp. 238–267. Available at: <ref target="https://doi.org/10.17723/0360-9081-83.2.238">https://doi.org/10.17723/0360-9081-83.2.238</ref>.</bibl>

            <bibl xml:id="gale_cengage2014" label="Gale Cengage 2014">Gale Cengage. (2014) <title rend="quotes">Eighteenth Century Collections Online</title>.</bibl>

            <bibl xml:id="gius_jacke2022" label="Gius and Jacke 2022">Gius, E. and Jacke, J. (2022)
                  <title rend="quotes">Are Computational Literary Studies Structuralist?</title>,
                  <title rend="italic">Journal of Cultural Analytics</title>, 7(4). Available at:
                  <ref target="https://doi.org/10.22148/001c.46662">https://doi.org/10.22148/001c.46662</ref>.</bibl>

            <bibl xml:id="goldstone_underwood2012" label="Goldstone and Underwood 2012">Goldstone,
               A. and Underwood, T. (2012) <title rend="quotes">What can Topic Models of PMLA Teach
                  Us About the History of Literary Scholarship?</title>, <title rend="italic">Journal of Digital Humanities</title>, 2(1).</bibl>

            <bibl xml:id="gusenbauer_haddaway2020" label="Gusenbauer and Haddaway 2020">Gusenbauer,
               M. and Haddaway, N.R. (2020) <title rend="quotes">Which academic search systems are
                  suitable for systematic reviews or meta‐analyses? Evaluating retrieval qualities
                  of Google Scholar, PubMed, and 26 other resources</title>, <title rend="italic">Research Synthesis Methods</title>, 11(2), pp. 181–217. Available at: <ref target="https://doi.org/10.1002/jrsm.1378">https://doi.org/10.1002/jrsm.1378</ref>.</bibl>

            <bibl xml:id="hawkins2021" label="Hawkins 2021">Hawkins, S. (ed.) (2021) <title rend="italic">Access and control in digital humanities</title>. Abingdon, Oxon ;
               New York, NY: Routledge.</bibl>

            <bibl xml:id="hegghammer2021" label="Hegghammer 2021">Hegghammer, T. (2021) <title rend="quotes">OCR with Tesseract, Amazon Textract, and Google Document AI: a
                  benchmarking experiment</title>, <title rend="italic">Journal of Computational
                  Social Science</title>, 5(1), pp. 861–882. Available at: <ref target="https://doi.org/10.1007/s42001-021-00149-1">https://doi.org/10.1007/s42001-021-00149-1</ref>.</bibl>

            <bibl xml:id="helms_krieser2023" label="Helms and Krieser 2023">Helms, S. and Krieser,
               J. (2023) <title rend="quotes">Copyrights, Professional Perspective - Copyright
                  Chaos: Legal Implications of Generative AI</title>, <title rend="italic">Bloomberg
                  Law</title>. Available at: <ref target="https://www.bloomberglaw.com/external/document/XDDQ1PNK000000/copyrights-professional-perspective-copyright-chaos-legal-implic">https://www.bloomberglaw.com/external/document/XDDQ1PNK000000/copyrights-professional-perspective-copyright-chaos-legal-implic</ref>
               (Accessed: 15 October 2023).</bibl>

            <bibl xml:id="janicke_etal2015" label="Jänicke et al. 2015">Jänicke, S. et al. (2015)
                  <title rend="quotes">On Close and Distant Reading in Digital Humanities: A Survey
                  and Future Challenges</title>, in <title rend="italic">STAR-State of The Art
                  Report</title>. Eurographics Conference on Visualization (EuroVis), Ed. R. Borgo,
               F. Ganovelli, and I. Viola.</bibl>

            <bibl xml:id="jockers2013" label="Jockers 2013">Jockers, M. (2013) <title rend="italic">Macroanalysis: Digital Methods and Literary History</title>. Urbana-Champaign:
               University of Illinois Press.</bibl>

            <bibl xml:id="jung2020" label="Jung 2020">Jung, G. (2020) <title rend="quotes">Do
                  Androids Dream of Copyright?: Examining AI Copyright Ownership</title>, <title rend="italic">Berkeley Technology Law Journal</title>, 35(4), pp.
               1151–1178.</bibl>

            <bibl xml:id="knibbs2023" label="Knibbs 2023">Knibbs, K. (2023) <title rend="quotes">The
                  Battle Over Books3 Could Change AI Forever</title>, <title rend="italic">Wired</title>, 4 September. Available at: <ref target="https://www.wired.com/story/battle-over-books3/">https://www.wired.com/story/battle-over-books3/</ref> (Accessed: 15 October
               2023).</bibl>

            <bibl xml:id="loc_nd" label="Library of Congress n.d.">Library of Congress (no date)
                  <title rend="quotes">What is a MARC Record and Why is it Important?</title>
               Available at: <ref target="https://www.loc.gov/marc/umb/um01to06.html">https://www.loc.gov/marc/umb/um01to06.html</ref> (Accessed: 20 October
               2023).</bibl>

            <bibl xml:id="leetaru2008" label="Leetaru 2008">Leetaru, K. (2008). <title rend="quotes">Mass Book Digitization: The Deeper Story of Google Books and the Open Content
                  Alliance.</title>
               <title rend="italic">First Monday</title>, 13. Available at: <ref target="https://doi.org/10.5210/fm.v13i10.2101">https://doi.org/10.5210/fm.v13i10.2101</ref>.</bibl>

            <bibl xml:id="mcdonough_etal2010" label="McDonough et al. 2010">McDonough, J. et al.
               (2010) <title rend="quotes">Preserving Virtual Worlds Final Report</title>.</bibl>

            <bibl xml:id="neh2023" label="NEH 2023">National Endowment for the Humanities. (n.d)
                  <title rend="quotes">Appropriations of the National Endowment for the
                  Humanities</title>. Available at: <ref target="https://www.neh.gov/neh-appropriations-history">https://www.neh.gov/neh-appropriations-history</ref>.</bibl>

            <bibl xml:id="nih2023" label="NIH 2023">National Institute of Health. (n.d) <title rend="quotes">Appropriations of the NIH</title>, <title rend="italic">Appropriations of the NIH</title>. Available at: <ref target="https://www.nih.gov/about-nih/what-we-do/nih-almanac/appropriations-section-1">https://www.nih.gov/about-nih/what-we-do/nih-almanac/appropriations-section-1</ref>.</bibl>

            <bibl xml:id="nsf2023" label="NSF 2023">National Science Foundation (n.d) <title rend="quotes">Appropriations of the NSF</title>, <title rend="italic">Appropriations of the NSF</title>. Available at: <ref target="https://new.nsf.gov/about/budget/fy2023/appropriations#:~:text=The%20%22Consolidated%20Appropriations%20Act%20of,above%20the%20FY%202022%20appropriation">https://new.nsf.gov/about/budget/fy2023/appropriations#:~:text=The%20%22Consolidated%20Appropriations%20Act%20of,above%20the%20FY%202022%20appropriation</ref>.</bibl>

            <bibl xml:id="neudecker_etal2021" label="Neudecker et al. 2021">Neudecker, C. et al.
               (2021) <title rend="quotes">A survey of OCR evaluation tools and metrics</title>, in
                  <title rend="italic">The 6th International Workshop on Historical Document Imaging
                  and Processing</title>. HIP '21: The 6th International Workshop on Historical
               Document Imaging and Processing, Lausanne Switzerland: ACM, pp. 13–18. Available at:
                  <ref target="https://doi.org/10.1145/3476887.3476888">https://doi.org/10.1145/3476887.3476888</ref>.</bibl>

            <bibl xml:id="pike2022" label="Pike 2022">Pike, G. (2022) <title rend="quotes">Copyright
                  and AI</title>, <title rend="italic">Information Today</title>, 39(9), pp.
               26–27.</bibl>

            <bibl xml:id="piper2020" label="Piper 2020">Piper, A. (2020) <title rend="italic">Can We
                  Be Wrong: The Problem of Textual Evidence in a Time of Data</title>. Cambridge:
               Cambridge University Press (Elements in Digital Literary Studies).</bibl>

            <bibl xml:id="poole2013" label="Poole 2013">Poole, A.H. (2013) <title rend="quotes">Now
                  is the Future Now? The Urgency of Digital Curation in the Digital
                  Humanities</title>, <title rend="italic">Digital Humanities Quarterly</title>,
               7(2).</bibl>

            <bibl xml:id="project_gutenberg_nd" label="Project Gutenberg n.d."><title rend="quotes">Project Gutenberg</title> (n.d.). Urbana, Illinois.</bibl>

            <bibl xml:id="ramsay2011" label="Ramsay 2011">Ramsay, S. (2011) <title rend="italic">Reading machines: Toward an Algorithmic Criticism</title>. Urbana-Champaign:
               University of Illinois Press (Topics in the Digital Humanities).</bibl>

            <bibl xml:id="reisner2023" label="Reisner 2023">Reisner, A. (2023) <title rend="quotes">Revealed: The Authors Whose Pirated Books Are Powering Generative AI</title>,
                  <title rend="italic">The Atlantic</title>, 19 August. Available at: <ref target="https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/">https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/</ref>
               (Accessed: 13 October 2023).</bibl>

            <bibl xml:id="riddell2014" label="Riddell 2014">Riddell, A.B. (2014) <title rend="quotes">How to Read 22,198 Journal Articles: Studying the History of German
                  Studies with Topic Models</title>, in <title rend="italic">Distant Readings:
                  Topologies of German Culture in the Long Nineteenth Century</title>. Rochester:
               Boydell &amp; Brewer, pp. 91–114.</bibl>

            <bibl xml:id="saveri_butterick2023" label="Saveri and Butterick 2023">Saveri, J. and
               Butterick, M. (2023) <title rend="quotes">LLM litigation</title> · Joseph Saveri Law
               Firm &amp; Matthew Butterick, <title rend="italic">LLM Litigation</title>. Available
               at: <ref target="https://llmlitigation.com/">https://llmlitigation.com/</ref>
               (Accessed: 13 October 2023).</bibl>

            <bibl xml:id="sekhon_etal2023" label="Sekhon et al. 2023">Sekhon, J., Ozcan, O. and
               Ozcan, S. (2023) <title rend="quotes">ChatGPT: what the law says about who owns the
                  copyright of AI-generated content</title>, <title rend="italic">The
                  Conversation</title>. Available at: <ref target="http://theconversation.com/chatgpt-what-the-law-says-about-who-owns-the-copyright-of-ai-generated-content-200597">http://theconversation.com/chatgpt-what-the-law-says-about-who-owns-the-copyright-of-ai-generated-content-200597</ref>
               (Accessed: 15 October 2023).</bibl>

            <bibl xml:id="spk_nd" label="SPK n.d.">Stiftung Preußischer Kulturbesitz (n.d.) <title rend="quotes">Deutsche Digitale Bibliothek (German Digital Library)</title>.
               Available at: <ref target="https://www.preussischer-kulturbesitz.de/en/about-us/tasks-of-national-interest/deutsche-digitale-bibliothek.html">https://www.preussischer-kulturbesitz.de/en/about-us/tasks-of-national-interest/deutsche-digitale-bibliothek.html</ref>
               (Accessed: 20 October 2023).</bibl>

            <bibl xml:id="thompson2002" label="Thompson 2002">Thompson, J.W. (2002) <title rend="quotes">The Death of the Scholarly Monograph in the Humanities? Citation
                  Patterns in Literary Scholarship</title>, <title rend="italic">Libri</title>,
               52(3). Available at: <ref target="https://doi.org/10.1515/LIBR.2002.121">https://doi.org/10.1515/LIBR.2002.121</ref>.</bibl>

            <bibl xml:id="underwood2019" label="Underwood 2019">Underwood, T. (2019) <title rend="italic">Distant Horizons: Digital Evidence and Literary Change</title>.
               Chicago: Chicago UP.</bibl>

         </listBibl>
      </back>
   </text>
</TEI>