<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en">Assemblies of Points: Strategies to
                    Art-historical Human Pose Estimation and Retrieval</title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Stefanie <dhq:family>Schneider</dhq:family></dhq:author_name>
                    <idno type="ORCID">hhttps://orcid.org/0000-0003-4915-6949</idno>
                    <dhq:affiliation>AFFILIATION PLACEHOLDER</dhq:affiliation>
                    <email>stefanie.schneider@itg.uni-muenchen.de</email>
                    <dhq:bio>
                        <p>BIO PLACEHOLDER</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>

                <!-- This information should be added when the file is created -->
                <idno type="DHQarticle-id">000813</idno>


                <!-- This information will be completed at publication -->
                <idno type="volume">019</idno>
                <idno type="issue">4</idno>
                <date when="2025-12-01">1 December 2025</date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
                    <!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
                <taxonomy xml:id="project_keywords">
                    <bibl>DHQ project registry; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/projects.xml"
                            >http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->

                    <!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp="#visual_art"/>
                    <term corresp="#data_visualization"/>
                    <term corresp="#corpora"/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item>digital art history</item>
                        <item>object retrieval</item>
                        <item>recommendation systems</item>
                    </list>
                </keywords>
                <keywords scheme="#project_keywords">
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
            <change>The version history for this file can be found on <ref
                    target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000813/000813.xml"
                    >GitHub </ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>This paper attempts to construct a virtual space of possibilities for the
                    historical embedding of the human figure, and its posture, in the visual arts by
                    proposing a view-invariant approach to <term>Human Pose Retrieval</term> (HPR) that resolves
                    the ambiguity of projecting three-dimensional postures onto their
                    two-dimensional counterparts. In addition, we present a refined approach for
                    classifying human postures using a support set of 110 art-historical reference
                    postures. The method’s effectiveness on art-historical images was validated
                    through a two-stage approach of broad-scale filtering preceded by a detailed
                    examination of individual postures: an aggregate-level analysis of
                    metadata-induced hotspots, and an individual-level analysis of topic-centered
                    query postures. As a case study, we examined depictions of the crucified, which
                    often adhere to a canonical form with little variation over time — making it an
                    ideal subject for testing the validity of <term>Deep Learning</term> (DL)-based methods.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>PLACEHOLDER</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head> Introduction</head>
                <p>As early as 1884, the French physiologist Étienne-Jules Marey (1830–1904)
                    invented <term>chronophotography</term> to derive human motion only from points
                    and lines. He dressed his subjects in dark suits, with metal buttons at the
                    joints connected by metal stripes and photographed them with a camera that
                    captured multiple exposures on a single, immobile plate. In so doing, he
                    introduced the human skeleton as an objectified distillation of <quote rend="inline">pure movement</quote>
                    <ptr target="#mccarren_2003" loc="p.29"/>, emphasizing only trajectory snapshots of points and
                    lines, eliminating inherently human features, such as skin and muscle; it is
                    <quote rend="inline">visually unmoored from bodies and space,</quote> as Noam M. Elcott <ptr target="#elcott_2016" loc="p. 24"/> put it. By abstracting the human form, as Marey demonstrated, we are
                    prompted to inquire about the intrinsic qualities that remain when the body is
                    stripped of its psychological and most physical identifiers. This theme of
                    abstraction has been a recurring element in art history, a discipline inherently
                    linked to the visual, where the human form not only is a subject of
                    representation but a <term>carrier of meaning </term>— along with the body
                    movements and postures that emanate from it <ptr target="#egidi_2000"/>.
                    However, art history has emphasized gesture, which pertains to localized
                    movements of body parts — especially the head and hands — rather than posture,
                    which, as in Marey’s chronophotography, describes the entire body’s stance,
                    extending beyond the upper extremities to also include the lower body (see e.g.,
                        <ptr target="#gombrich_1966"/>
                    <ptr target="#baxandall_1972"/>). Acknowledging this historical perspective on
                    abstraction informs our methodology in this paper, which employs
                    state-of-the-art <term>Human Pose Estimation</term> (HPE) and <term>Human Pose
                        Retrieval</term> (HPR) techniques to capture — and analyze — posture in its
                    full complexity.<note> The attempt to forge a unified terminology for
                        body-related behavior has, at least in the visual arts, only been partially
                        realized. In this paper, we primarily employ the term <term>posture</term>.
                        The term <term>pose</term> is only employed when a semantic figure is
                        discernible, or in conjunction with the application of computational methods
                        in HPE and HPR, where the term is essential for the terminology of the
                        method.</note> By leveraging computational methods that echo Marey’s
                    trajectory snapshots, we intend to better understand both the universal and
                    culturally specific aspects of the human form.</p>
                <p>Specifically, this paper harnesses the ‘artificial eye’ of computational
                    methodologies to construct a <term>virtual space of possibilities </term>— of
                    associations, references, and similarities — for the historical embedding of the
                    human figure, and its posture, in the visual arts. By explicitly applying
                    state-of-the-art HPE and HPR methods, we bridge the gap between traditional
                    historical analysis and modern computational approaches. In this context, we
                    argue for the integration of both <term>close</term> and <term>distant
                        viewing</term>, where the global analysis of <term>distant viewing</term>
                    logically precedes and enhances the localized, detailed analysis of <term>close
                        viewing</term> — i.e., the qualitative analysis of individual artworks
                    within their spatio-temporal contexts.<note> See <ptr target="#arnold_2019"/>
                        for a discussion on the tension between <term>close</term> and <term>distant
                            viewing</term>.</note> From a computational point of view, the approach
                    first <term>estimates</term> joint positions of human figures,
                        <term>keypoints</term> (Figure 1), that resemble Marey’s metal buttons, to
                    create a vector representation, <term>embedding</term>, of the human skeleton,
                    which is then utilized to <term>retrieve</term> figures of potential relevance —
                    with two postures considered similar if they represent variations of the same
                    action or movement. To this end, we utilize a view-invariant embedding,
                    following the methodology proposed by T. Liu et al. (2022), that projects
                    three-dimensional postures onto their two-dimensional counterparts, recognizing
                    that postures can be visually identical but mathematically distinct, depending
                    on the observer’s perspective <ptr target="#liu_2022"/>.</p>
                <figure>
                    <head>In Andrea del Sarto’s Pietà with Saints (1523–1524), the joints of human
                        figures are indicated by green keypoints in the detail view.</head>
                    <graphic url="resources/images/figure01.png"/>
                </figure>

                <p>Although embeddings derived from neural networks — or more generally: <term>Deep
                        Learning</term> (DL) methods — have proven beneficial for various
                    art-historical retrieval tasks (e.g., <ptr target="#ufer_2021"/>
                    <ptr target="#karjus_2023"/>
                    <ptr target="#offert_2023"/>) by capturing semantically pertinent information
                    within dense vector spaces, their practical inspection remains underexplored in
                    scholarly discourse. While previous research has provided the groundwork for
                    DL-based retrieval, our study narrows the focus by specifically investigating
                    HPR applications. This leads us to our main research question:</p>
                <quote rend="block">How can the perceptual space of embeddings, obtained by DL models, be explored
                    and interpreted? What spatial patterns emerge within these spaces, and to what
                    extent can they encode posture?</quote>
                <p>Put simply, we determine how DL techniques can be leveraged to systematically
                    explore the representation of posture. We outline that, in their current state,
                    such methods can serve as <term>recommender systems</term> in art-historical
                    posture analysis, but also in related fields — such as theatre and dance studies
                    — that critically examine human posture and movement. To this end, we introduce
                    a two-stage approach of broad-scale filtering preceded by a detailed examination
                    of individual postures: an aggregate-level analysis of metadata-induced
                    hotspots, and an individual-level analysis of topic-centered query postures.
                    Both pipelines are intended, at least in this paper, to reaffirm the embedding
                    space’s usefulness in art-historical research, rather than to discover novel
                    art-historical knowledge. Our study, therefore, is designed to synthesize DL
                    methods into a unified workflow, easily adaptable to other disciplines, enabling
                    scholars to potentially trace the evolution of posture in art-historical
                    objects, allowing a thorough exploration of their <quote rend="inline">underlying psychic
                    structures</quote> <ptr target="#butterfield-rosen_2021" loc="p. 21"/> (Butterfield-Rosen, 2021, p. 21). As a concrete example, our case
                    study focuses on depictions of the crucified. These depictions, while globally
                    diverse, often adhere to a <q>canonical</q> form that exhibits little variation
                    over time — characterized by specific positions of Christ’s head and the arch of
                    his torso — making them an ideal subject for testing the validity and
                    transferability of DL-based posture analysis that is accessible even to
                    non-experts in the field.</p>
                <p>Methodologically, the paper thus contributes to the field of <term>Digital Art
                        History</term> (DAH), which has grown considerably since Johanna Drucker in
                    2013 criticized the delayed advent of computational methods for art-historical
                    inquiry, despite the increasing availability of suitable data in online
                    repositories <ptr target="#drucker_2013"/>. Section 2 first discusses related
                    work. In Section 3, we introduce our proposed methodology that processes images
                    of human figures into a machine-readable format. This involves first abstracting
                    the figures’ postures into skeletal representations and then converting these
                    into robust, view-invariant posture embeddings suitable for retrieval and
                    classification tasks. Section 4 elaborates on the data set used for interaction
                    with and exploration of the embedding space during the case study’s inference
                    phase. The case study itself is detailed in Section 5 and extensively discussed
                    in Section 6. The paper concludes with Section 7, which summarizes the findings
                    and discusses future research directions.</p>
            </div>
            <div>
                <head>Related Work</head>
                <p>Art-historical research has emphasized the analysis of gesture, a semantically
                    charged sub-category of bodily movement, over posture — influenced by the
                    Renaissance’s re-invigoration of scholarly rhetoric <ptr target="#zimmermann_2011" loc="p. 179"/>.
                    For instance, Ernst Gombrich in 1966 suggested that gestures in the visual arts
                    originate from natural human expressions that have been ritualized, thus
                    acquiring unique forms and meanings <ptr target="#gombrich_1966"/>. In
                    particular, Warburg’s concept of <foreign xml:lang="de">Pathosformeln</foreign>
                    contributed significantly to the discourse: by identifying formally stable
                    gestures from antiquity that were in the Renaissance repeatedly employed to
                    express primal emotions <ptr target="#warburg_1998"/>. Nevertheless, Emmelyn
                    Butterfield-Rosen advocates for posture as a <quote rend="inline">more useful category</quote> for
                    exploring <quote rend="inline">permanent, underlying psychic structures</quote> <ptr target="#butterfield-rosen_2021" loc="p. 21"/>. Already in the early 20th century, the Finnish art historian Johan Jakob
                    Tikkanen proposed a typology of leg postures as <quote rend="inline">cultural motifs</quote> in the visual
                    arts <ptr target="#tikkanen_1912"/>; these motifs, according to Tikkanen, have
                    evolving functions that may be traced through European art from antiquity to
                    modernity. Despite, or perhaps because of, Tikkanen’s own admission of the
                    typology’s limitations and his insistence on a multidisciplinary approach to
                    better understand the evolution of motifs, his work has remained largely
                    unrecognized in academic discourse, often relegated to the footnotes (see, e.g.,
                    <ptr target="#steinberg_2018" loc="p. 200"/> <ptr target="#butterfield-rosen_2021" loc="p. 279"/>).</p>
                <p>In response, recent efforts have employed digital methods to the large-scale
                    investigation of art-historical posture. Yet, the adoption of HPE methods
                    remains limited: on the one hand, due to significant variations in human
                    morphology between artistic depictions and real-world photography, and, on the
                    other hand, due to the absence of domain-specific, annotated data required to
                    train neural networks. Predominantly, existing approaches <ptr target="#impett_2016"/> <ptr target="#jenicek_2019"/> <ptr target="#madhu_2020"/> <ptr target="#zhao_2022"/> therefore utilize models trained on real-world photographs without
                    integrating art-historical material. Only recently, <ptr target="#springstein_2022"/>
                    and <ptr target="#madhu_2023"/> have begun to refine model accuracy by fine-tuning with
                    domain-specific images — an approach we also leverage in this paper. To then
                    assess how similar these machine-abstracted postures are to every other posture
                    in a data set, <term>low-level approaches</term> obtain scores directly from
                    keypoint positions (or angles) using numerical similarity metrics <ptr target="#kovar_2002"/> <ptr target="#pehlivan_2011"/>. However, as
                    morphological or perceptual features cause variations in keypoint positions <ptr
                        target="#harada_2004"/>
                    <ptr target="#so_2005"/>, which may affect the reliability of similarity metrics
                    even between identical postures, <term>high-level approaches</term> have begun
                    to exploit the latent layers of neural networks <ptr target="#rhodin_2018"/>
                    <ptr target="#ren_2020"/>. Given the task-specific nature of these latent
                    representations, <ptr target="#liu_2021"/> suggest normalized posture features
                    that are invariant to morphological structure and viewpoint; however, the method
                    relies on fully estimated postures without missing keypoints. Following the
                    strategy of <ptr target="#liu_2022"/>, called <term>Probabilistic View-invariant
                        Pose Embedding</term> (Pr-VIPE), we instead propose a solution that is not
                    only robust to occlusion,<note> See <ptr target="#liu_2022"/> for additional
                        information.</note> but also easily adaptable to various domains.</p>
            </div>
            <div>
                <head>Methodology</head>
                <p>The proposed methodology transforms images of human figures into a
                    machine-readable format by first simplifying their postures into skeletal
                    representations. These skeletal models eliminate non-essential visual
                    information — such as background scenery and clothing — so that only the body’s
                    posture is retained. Each posture is then numerically encoded as an array of
                    real numbers, referred to as an <term>embedding</term>, which compresses
                    postural specifics and enables similar postures to be retrieved across a large
                    collection of artworks. Consequently, the embedding represents a distilled
                    version of the original figure, while the embedding space constitutes a
                        <term>virtual space of possibilities</term>, i.e., of all conceivable
                    postures. The overall architecture of this pipeline is shown in Figure 2.</p>
                <figure>
                    <head>Our methodology first localizes human figures within an image by bounding
                        boxes, which are then examined to identify keypoints. Based on the human
                        figure’s estimated keypoints, we construct three posture configurations,
                        compress them into posture embeddings, and then classify each
                        configuration.</head>
                    <graphic url="resources/images/figure02.png"/>
                </figure>
                <div>
                    <head>View-invariant Human Posture Embedding</head>
                    <p>To make posture recognition more accurate, we follow an approach inspired by
                            <ptr target="#springstein_2022"/> that integrates <term>Semi-supervised
                            Learning</term> (SSL). This means that our system learns from both
                        labeled and unlabeled images, allowing it to improve over time by generating
                        its own training data. We implement the regression-based <term>Pose
                            Recognition Transformer</term> (PRTR), as outlined by <ptr
                            target="#li_2021"/>, which employs a cascaded Transformer architecture
                        with separate components for bounding box and keypoint detection <ptr
                            target="#vaswani_2017"/>
                        <ptr target="#carion_2020"/>.<note> For further discussion and insights into
                            the methodology, we refer to <ptr target="#springstein_2022"/>.</note> The
                        proposed methodology for HPE thus leverages a <term>top-down
                        strategy</term>: In the first stage, human figures are localized within an
                        image by rectangular bounding boxes, which are then examined in the second
                        stage to identify keypoints, i.e., points relevant to the abstraction of the
                        figures’ posture.</p>
                    <p>The estimated whole-body posture is segregated into the upper and lower body,
                        shown in green and red, respectively, in Figure 3; these components, along
                        with the whole body, then comprise the search query employed for retrieval
                        purposes. The lower body is composed of six keypoints (ankles, knees, hip)
                        and the upper body of eight keypoints (hip, wrists, elbows, shoulders). This
                        division increases the reliability of the system: Even if the whole-body
                        posture cannot be detected (e.g., due to missing or obscured joints), the
                        upper and lower body may still provide enough information to accurately
                        classify the figure. In the next step, configurations exceeding 50% of the
                        maximum possible keypoints are filtered to eliminate configurations with
                        high uncertainty. Valid configurations are then projected into
                        320-dimensional embeddings using the Pr-VIPE proposed by <ptr target="#liu_2022"/>.<note> This implies that only keypoints associated with the
                            respective configuration are utilized to create the embedding.</note>
                        Unlike methods that depend solely on keypoint positions, Pr-VIPE encodes the
                        appearance of a posture — as perceived by humans — so that it remains robust
                        to changes in perspective, allowing similar postures to be compared across
                        different viewpoints. It does so by mapping each two-dimensional posture
                        into a probabilistic embedding space where the representation is not a fixed
                        point, but a probability distribution. During training, the system is
                        exposed to pairs of two-dimensional postures obtained from different camera
                        views, often augmented by synthesizing multi-view projections from
                        three-dimensional keypoints. The learning objective encourages the
                        embeddings of postures that are visually similar (i.e., represent the same
                        underlying three-dimensional posture) to be close to each other in the
                        probabilistic space — even if there are perspective-induced variations in
                        the two-dimensional keypoint positions.<note> Already <ptr
                                target="#ullman_1979" loc="p. 6"/>, with reference to <ptr
                                target="#eriksson_1973"/>, stated that <quote rend="inline">there is no unique structure
                            and motion consistent with a given two-dimensional (2-D) transformation
                            [...],</quote> but an <quote rend="inline">infinite number of motions of the elements that will
                            produce the same 2-D projection.</quote></note>
                    </p>
                </div>
                <div>
                    <head>One-shot Human Posture Classification</head>
                    <p>To classify postures, the pipeline is as follows: We first manually identify
                        110 art-historical images with reference postures of human figures and label
                        them with keypoints. We then re-use the Pr-VIPE and compute the cosine
                        distances between the embeddings, associating each query to the closest
                        matching reference posture(s). This allows a fine-grained indexing of
                        postures, even when the body parts of each configuration could not be
                        adequately estimated. At the same time, there is no fixed, semantically
                        dubious categorization into groups, as is the case with agglomerative
                        clustering methods <ptr target="#impett_2016"/>. However, as shown in
                        Figure 3, where example images of the reference postures are shown, a single
                        human figure can only model a fraction of the high postural variability
                        within the subnotations.</p>
                    <figure>
                        <head>Close-ups of sample images for each direct subnotation of the relevant
                            Iconclass notations.</head>
                        <graphic url="resources/images/figure03.jpeg"/>
                    </figure>
                    <p>The selected postures, derived from the Iconclass taxonomy <ptr target="#van-de-waal_1973"/>, range from elementary configurations, such as <q>arm raised upward,</q> to
                        complex full-body arrangements, such as <q>lying on one side, stretched out,</q>
                        reducing the limitations of culturally or temporally dependent labels.
                        Iconclass, while explicitly designed for the iconography of Western fine
                        art, also includes universal definitions ranging from natural phenomena to
                        anatomical specifics, the latter of which are pertinent not only to our
                        research but also potentially beneficial for scholarly investigations into
                        human anatomy across various disciplines, such as dance studies. Each
                        definition within the Iconclass taxonomy is represented by a unique
                        combination of alphanumeric characters, referred to as the ‘notation,’ and a
                        description, the ‘textual correlate,’ accompanied by a set of keywords. A
                        notation consists of at least one digit symbolizing the first level of the
                        hierarchy, ‘division.’ This may be followed by another digit at the
                        secondary level, and one or two (identical) capital letters at the tertiary
                        level. The structure, referred to as the ‘basic notation,’ can be further
                        supplemented with auxiliary components <ptr target="#van-straten_1994"/>. We
                        consider four groups of Iconclass notations: notation 31A23 (textual
                        correlate <q>postures of the human figure</q>), 31A25 (<q>postures and gestures of
                        the arms and hands</q>), 31A26 (<q>postures of the legs</q>), and 31A27 (<q>movements
                        of the human body</q>). Notations 31A23 and 31A27 represent the whole body,
                        31A25 the upper body, and 31A26 the lower body. Each group is almost equally
                        represented: upper-body notations with 31 instances, lower-body notations
                        with 25, and whole-body notations with 27 each.</p>
                </div>
            </div>
            <div>
                <head>Data Set</head>
                <p>Using the Wikidata SPARQL endpoint,<note>
                        <ref target="https://query.wikidata.org/">https://query.wikidata.org/</ref>,
                        last accessed on August 24, 2024.</note> we extract 644,155 art-historical
                    objects classified as either <title rend="quotes">visual artwork</title> (Wikidata item Q4502142) or
                    <title rend="quotes">artwork series</title> (Q15709879) that have a two-dimensional image.<note> In this
                        section, we focus solely on the data set used for practical interaction with
                        and exploration of the constructed embedding space during the inference
                        phase of the case study. For information on the data sets used to train the
                        SSL model, we again refer to <ptr target="#springstein_2022"/>.</note> For
                    these objects, our model identifies 9,694,248 human figures, which are — after
                    the filtering stage — reduced to 2,355,592 figures.<note> The filtering stage
                        aimed to increase the accuracy of human figure identification by emphasizing
                        scale-specific variations through a binary <term>eXtreme Gradient
                            Boosting</term> (XGBoost) decision tree <ptr target="#chen_2016"
                        />.</note> Wikidata was chosen primarily for practical reasons: It surpasses
                    other art-related databases that are limited by institutional and regional
                    focus, like that of the Metropolitan Museum of Art. Moreover, unlike distributed
                    image archives such as Europeana,<note>
                        <ref target="https://www.europeana.eu/">https://www.europeana.eu/</ref>,
                        last accessed on August 24, 2024.</note> where considerable effort is
                    required to filter out reproductions of the same original, Wikidata is
                    continuously updated, reducing the number of near-duplicates and thus increasing
                    the reliability of the data.</p>
                <figure>
                    <head>Distribution of creation dates in art-historical Wikidata objects.</head>
                    <graphic url="resources/images/figure04.png"/>
                </figure>
                <p>We determine the objects’ creation dates based on the earliest probable date and
                    ending with the latest. As shown in Figure 4a, using only the starting point of
                    the interval causes many data points to converge around the turn of the
                    centuries, leading to an inaccurate peak in the number of objects. To overcome
                    this, we employ bootstrapping: For the 71.9% of objects with defined time
                    intervals, we randomly select a point within the time interval over
                    50 iterations and calculate the average number of objects per time point. This
                    method is valuable for all types of objects that are dated by time intervals, as
                    is common in historical research. In addition, it enables the creation of
                    so-called <term>confidence intervals</term>, which reflect the natural
                    uncertainty in the dating of historical objects. The result, as shown in
                    Figure 4b, provides a better understanding of the potential density of objects
                    in different time periods: There is a continuous increase in the number of
                    objects after 1400, with isolated peaks around 1500 and 1650. More dominant
                    peaks appear only around 1900, with a pronounced one between 1935 and 1940,
                    largely due to the collecting activities of the National Gallery of Art that is
                    featured prominently in Wikidata.</p>
                <p>Filtering the Wikidata data set for the term <q>crucifix</q> and its French and German
                    translations yields a subset of 1,516 objects for the case study (Figure 7 in
                    Appendix A). Most of these objects are labeled as paintings (65.1%), prints
                    (6.3%), or sculptures (4.4%), with crucifixes themselves accounting for 4.2%.
                    Featured are works from artists such as El Greco (1541–1614; 33 objects),
                    Anthony van Dyck (1599–1641; 19 objects), and Lucas Cranach the Elder
                    (1472–1553; 17 objects); 32.7% of the objects are without attribution. Of
                    course, not all human figures depicted in these metadata-selected objects are
                    crucified, as we filter solely for terms related to crucifixion scenes, not for
                    figures depicted as crucified.</p>
            </div>
            <div>
                <head>Case Study</head>
                <p>In the following, we outline two analytical pipelines for exploring the
                    constructed embedding space as a <term>virtual space of possibilities</term>: a
                        <term>distant viewing</term> focused on an aggregate-level analysis of
                    metadata-induced hotspots, and a <term>close viewing</term> focused on an
                    individual-level analysis of topic-centered query postures.</p>
                <div>
                    <head>Analytical Process</head>
                    <p>The first pipeline entails filtering objects based on their metadata, in
                        particular the Wikidata <q>depicts</q> property (P180) and labels in English,
                        French, and German. Objects containing any of the query terms are selected
                        and then analyzed for their spatial position within the embedding space to
                        identify density structures — the metadata-induced density peaks are thus
                        utilized for the <term>distant viewing</term> of objects whose postures
                        closely resemble those pre-selected by the metadata. Each point in the
                        embedding space denotes a unique posture, with density referring to how
                        these points are grouped — either clustering in high-density areas or
                        scattering in low-density regions. The second pipeline is predicated on the
                        density peaks identified at the aggregate level; it focuses on the
                        recognition and exploitation of individual postures associated with specific
                        iconographies, thus enabling a <term>posture-to-posture</term> search. This
                        involves a <term>close viewing</term> of objects that are visually related
                        to the identified query postures; it also explores the developmental
                        trajectories of different postures within the embedding space to trace the
                        ‘evolution’ of iconographic elements.</p>
                    <p>Our case study examines the variability in the representation of the
                        crucified. It focuses specifically on the depiction of the crucified Christ,
                        excluding accompanying figures traditionally portrayed at the cross’s base,
                        such as Mary and Christ’s disciples — especially the apostle John. Although
                        depictions of the crucified Christ vary widely throughout the world,
                        influenced by local and ethnic traditions, they are generally unified by a
                            <q>canonical</q> form: <quote rend="inline">dead upon the Cross, Jesus’s head is slung to
                        one side, typically our left; his torso is naked and upright, sometimes
                        slightly arched</quote> <ptr target="#merback_2001" loc="p. 69"/>.</p>
                    <p>Variations in the positioning of Christ’s limbs are subtly evident in
                        different artistic traditions: Gothic crucifixes, for instance, show Christ
                        in a dynamic, bent position, with legs thrust forward and knees spread <ptr
                            target="#brandmair_2015" loc="p. 100"/>, while in Italian iconography,
                        Christ is often depicted with upward-angled arms, his head slumped on his
                        chest, with varying leg positions to represent the deceased body’s movement
                            <ptr target="#haussherr_1971" loc="p. 50"/>.</p>
                    <p>To conduct a qualitative analysis, the embedding spaces of the embeddings are
                        reduced from 320 to two dimensions. Traditional dimension reduction
                        techniques such as <term>t-distributed Stochastic Neighbor Embedding</term>
                        (t-SNE) <ptr target="#van-der-maaten_2008"/>, <term>Uniform Manifold
                            Approximation and Projection</term> (UMAP) <ptr
                            target="#mcinnes_2018"/>, and their variants (e.g., <ptr
                            target="#im_2018"/>
                        <ptr target="#linderman_2019"/> are known to preserve either local or
                        global spatial relationships, which can result in the formation of
                        artificial clusters that are not present in the original data.<note> See
                            <ptr target="#wang_2021"/> for an empirical comparison of the algorithms and
                            their weaknesses.</note> To overcome this limitation, we employ
                            <term>Pairwise Controlled Manifold Approximation</term> (PaCMAP), which
                        first identifies global structures and then refines them locally <ptr target="#wang_2021"/>. We re-use the Pr-VIPE model checkpoints from <ptr target="#liu_2022"/>.<note>
                            <ref target="https://sites.google.com/view/pr-vipe/model-checkpoints"
                                >https://sites.google.com/view/pr-vipe/model-checkpoints</ref>, last
                            accessed on August 24, 2024.</note> To analyze the density structure of
                        the embedding spaces, and thus the <term>virtual space of
                            possibilities</term>, we employ three visualization techniques: scatter
                        plots, two-dimensional histograms, and contour plots (Figure 5). Both
                        histograms and contour plots employ a color gradient ranging from blue
                        (denoting lower concentrations) to red (denoting higher concentrations),
                        with white signifying the midpoint. Contour plots are often favorable
                        because of their superior ability to discern isolated high-density areas
                        clearly; however, smaller, less dense regions are more reliably identified
                        in two-dimensional histograms. Further details of the techniques are given
                        in Appendix B.</p>
                    <figure>
                        <head>Scatter plots, two-dimensional histograms, and contour plots of the
                            two-dimensional whole-body, upper-body, and lower-body posture embedding
                            spaces of all images, left, and of images labeled <q>crucifix,</q> right,
                            respectively. In each case, the same PaCMAP projection is
                            employed.</head>
                        <graphic url="resources/images/figure05.png"/>
                    </figure>
                </div>
                <div>
                    <head>Aggregate Level</head>
                    <p>The embedding space is structured so that similar postures are grouped
                        closely together. This proximity implies that finding a posture in a
                        particular region of the embedding space generally means that another
                        similar posture can be found nearby — hence, semantic groups of postures
                        typically cluster, providing a structured approach to efficient retrieval
                        based on these spatial concentrations, as will be discussed in the
                        following. In each case, our analysis begins with an examination of the
                        embedding spaces for all images to establish a baseline understanding. We
                        then narrow our focus to compare these embedding spaces with those
                        containing only crucifixion scenes, in order to determine how particular
                        thematic elements influence the spatial organization within the
                        embedding.</p>
                    <div>
                        <head>5.2.1. Whole-body posture embedding</head>
                        <p>The distribution within the two-dimensional whole-body posture embedding
                            space of all images is characterized by a significant concentration in
                            the upper-left quadrant, with density gradually decreasing towards the
                            southern regions (Figures 5a, b, and c). The upper-right quadrant, while
                            also densely populated, features a slightly downward-shifted center of
                            density, aligning it more centrally within the embedding space; it
                            mostly features postures in which the upper arms or thighs are
                            perpendicular to the upper body. In contrast, postures with nearly
                            extended arms or legs dominate the middle-lower quadrant, exemplified by
                            notation 31A2363 (<q>Lying on one side, stretched out</q>).</p>
                        <p>In addition, manual inspection of the whole-body embedding space reveals
                            two primary cluster-like formations that correspond mainly to specific
                            configurations of the upper and lower body — which is noteworthy given
                            that we are not focusing here on the upper-body and lower-body
                            embeddings. In the lower segment, we identify figures classified with
                            subnotations under 31AA2511 (<q>Arm raised upward – AA – both arms or
                            hands</q>), including <hi rend="italic">T</hi>- or <hi rend="italic"
                            >Y</hi>-shaped crucified ones in the lower-central area (31A237,
                            <q>Hanging figure</q>). Those stretching sideways or backwards are centered
                            with a downward tilt, mapped to the subnotations 31A2513 (<q>Arm stretched
                            sidewards</q>) and 31A2514 (<q>Arm held backwards</q>). The upper-left segment
                            is dominated by configurations of the lower body. For instance, the
                            standing figure with straight legs is prominent in the upper left,
                            identified as 31A26111 (<q>Standing or leaning with both legs straight,
                            side by side, feet flat on the ground</q>); she is usually depicted with
                            her arms held down. A third segment in the upper-right quadrant features
                            postures with bent limbs that is particularly difficult to capture
                            semantically; it includes the subnotations 31A234 (<q>Squatting, crouching
                            figure</q>) and 31A235 (<q>Sitting figure</q>). The center’s less dense regions
                            frequently show errors in HPE that cannot be meaningfully
                            classified.</p>
                        <p>In the crucifixion’s embeddings, a compact hotspot is centrally situated
                            at the bottom, indicating specific posture configurations that differ
                            substantially from other configurations in the data set, with lesser
                            populated zones in the remaining embedding space, as evident in
                            Figures 5d, e, and f. The sparsity of data in other areas is primarily
                            related to objects associated with terms relevant to crucifixion scenes,
                            as opposed to crucified figures themselves; naturally, not all human
                            figures depicted in the metadata-selected objects are crucified, given
                            the large number of figures involved in these scenarios. The utility of
                            hotspot post-filtering becomes apparent when comparing the postures in
                            images labeled with <q>crucifix</q> to those labeled and within the hotspot
                            located at the central bottom. Notable are Iconclass notations with a
                            clear emphasis on upper-body positions, such as 31A2364 (<q>Lying on one
                            side, with uplifted upper part of the body and leaning on the arm</q>) and
                            31A23711 (<q>Hanging by one arm</q>).</p>
                    </div>
                    <div>
                        <head>5.2.2. Upper-body posture embedding</head>
                        <p>In the two-dimensional upper-body posture embedding space, we observe a
                            singular high-density region towards the lower center (Figures 5g, h,
                            and i). This area, with its circular shape, implies a central group of
                            similar upper-body postures in the data set. Elsewhere, the embedding
                            space exhibits a gradient of posture densities with no other significant
                            hotspots, suggesting a more dispersed representation of upper-body
                            postures; this pattern is likely due to the limited number of eight
                            keypoints used in the embedding, which reduces the variation between
                            postures. The lower center of the embedding space contains mostly
                            postures with one arm bent in front of or behind the body.<note> Whether
                                the arm is bent in front of or behind the body is not made explicit
                                by the estimated keypoints.</note> In the lower-left quadrant,
                            postures often feature an arm bent upwards, as indicated by notation
                            31A2513 (<q>Arm stretched sidewards</q>).</p>
                        <p>The distinct posture of the crucified — with arms outstretched sidewards
                            — is validated in the two-dimensional upper-body posture embedding space
                            by a small, elongated high-density area in the lower central region
                            (Figures 5j, k, and l). This area corresponds with the high-density
                            region of the notation 31AA2513 (<q>Arm stretched sidewards – AA – both
                            arms or hands</q>). When examining the distribution of the upper-body
                            similarities, there are only slight variations between images labeled
                            <q>crucifix</q> and the data set’s remainder, due to the large spread of
                            points in the embedding space. Yet, differences are evident when
                            considering dominant Iconclass notations compared to the general
                            population: Notations such as 31A2531 (<q>Hand(s) bent towards the head</q>)
                            and 31AA2514 (<q>Arm held backwards – AA – both arms or hands</q>) have
                            notably higher similarity values in the metadata- and hotspot-filtered
                            subset.</p>
                    </div>
                    <div>
                        <head>5.2.3. Lower-body posture embedding</head>
                        <p>The configuration of hotspots in the lower-body posture embedding space
                            of all images is mostly elongated and narrow, suggesting a linear
                            progression of posture similarities (Figures 5m, n, and o). The contour
                            lines in the lower-body posture embedding follow a sinusoidal pattern,
                            with pronounced peaks and valleys — maybe reflecting an inherent data
                            structure where certain lower-body positions are more common, and others
                            less so. The lower-right quadrant of the embedding space is dominated by
                            postures with straight or slightly bent legs. In contrast, the
                            upper-left quadrant mostly features postures with more significantly
                            bent legs, encompassing squatting and various sitting positions, such as
                            notation 31A26123 (<q>Squatting with legs side by side</q>).</p>
                        <p>Subtle but discernible shifts compared to the density structure of the
                            overall data set are observed in the crucifixion’s lower-body embedding
                            only at the sinusoidal pattern’s edges, particularly in the central left
                            and right quadrants, with a slight downward extension in the right
                            quadrant (Figures 5p, q, and r). The infrequency of leg positions that
                            uniquely denote crucified figures, as evidenced by the rarity of
                            hotspots, confirms the lack of specific leg configurations in these
                            depictions, which often resemble normal standing positions with slightly
                            bent legs. This absence of specific configurations in the iconography
                            compared to the broader data set is further accentuated when assessing
                            lower-body posture similarities: There are extensive overlaps between
                            the Iconclass notations, like those of the upper-body similarity
                            distributions.</p>
                        <figure>
                            <head>Whole-body HPR results for the thief to Christ’s right in James
                                Tissot’s <title rend="italic">The Strike of the Lance</title>
                                (1886–1894; a) with the estimated keypoints in green.</head>
                            <graphic url="resources/images/figure06.jpeg"/>
                        </figure>
                    </div>
                </div>
                <div>
                    <head>Individual Level</head>
                    <p>In the second pipeline, our approach departs from the broad analysis of
                        metadata-induced hotspots and instead focuses on the in-depth study of
                        individual artworks and their comparative analysis based on the generated
                        embeddings. To this end, we implement an approximate <hi rend="italic"
                            >k</hi>-nearest neighbor graph, <term>Hierarchical Navigable Small
                            World</term> (HNSW) <ptr target="#malkov_2020"/>, which
                        contains the 320-dimensional embeddings of the whole body, upper body, and
                        lower body. To illustrate its practical application, we examine James
                        Tissot’s <title rend="italic">The Strike of the Lance</title> (1886–1894)
                        and more specifically, the thief crucified to Christ’s right (Figure 6a). As
                        shown in Figure 6, the thief’s bent arm is echoed in the Pietà, for instance
                        in a rendition after Marcello Venusti (c. 1515–1579; Figure 6g), where Mary
                        is portrayed mourning her son. Interestingly, the horse’s upright stance in
                        Jacques Louis David’s <hi rend="italic">Napoleon on the Great St
                            Bernard</hi> (1801; Figure 6p) is mistakenly recognized as a human
                        figure — with outstretched arms and bent legs — echoing the thief’s posture;
                        this error results from imprecise bounding box detection, which frequently
                        misclassifies animals with human-like physiognomies as human figures.
                        Misalignments of the lower body are evident also in works such as Perino del
                        Vaga’s <hi rend="italic">A Fragment: The Good Thief (Saint Dismas)</hi>
                        (c. 1520–1525; Figure 6h), Hendrick ter Brugghen’s <hi rend="italic">The
                            Crucifixion with the Virgin and St John</hi> (1625; Figure 6k), and Jan
                        de Hoey’s <hi rend="italic">Ignatius of Loyola</hi> (c. 1601–1700;
                        Figure 6r), primarily with inaccurately shortened ankles, which also trace
                        back to limitations in bounding box detection. Despite these issues, minor
                        inaccuracies, such as the misestimation of the left wrist in Figure 6l, seem
                        to not affect the retrieval performance. The degree of keypoint
                        misestimation is proportional to its effect on the generated feature vector,
                        which is essential for accurate retrieval. Even when keypoints are grossly
                        misestimated, as shown in Figure 6l, the model only marginally penalizes
                        these errors in the resulting embedding, provided that the majority of
                        keypoints are estimated with sufficient accuracy.</p>
                    <p>To understand the evolution of iconographies in art history — here: the
                        postural configurations in crucifixion scenes — I t is essential to analyze
                        both the posture and its temporal distribution. This involves, in
                        crucifixion scenes, distinguishing between <hi rend="italic">T</hi>- and <hi
                            rend="italic">Y</hi>-shaped upper-body postures that depend on the arms’
                        position on the cross. To assess the Pr-VIPE’s ability to differentiate
                        between such configurations, we first select three crucifixion depictions
                        from Wikidata: Marcello Venusti’s <title rend="italic">Christ on the
                            Cross</title> (1500–1625), in <hi rend="italic">T</hi>-shape, with arms
                        outstretched horizontally;<note>
                            <ref target="https://www.wikidata.org/wiki/Q29650159"
                                >https://www.wikidata.org/wiki/Q29650159</ref>, last accessed on
                            August 24, 2024.</note> Diego Velazquez’s <title rend="italic">Christ
                            Crucified</title> (1632), in weak <hi rend="italic">Y</hi>-shape, with
                        arms slightly upraised;<note>
                            <ref target="https://www.wikidata.org/wiki/Q2528741"
                                >https://www.wikidata.org/wiki/Q2528741</ref>, last accessed on
                            August 24, 2024.</note> and Peter Paul Rubens’ <title rend="italic"
                            >Christ Expiring on the Cross</title> (1619), in strong <hi
                            rend="italic">Y</hi>-shape, with arms strongly raised.<note>
                            <ref target="https://www.wikidata.org/wiki/Q47413264"
                                >https://www.wikidata.org/wiki/Q47413264</ref>, last accessed on
                            August 24, 2024.</note> That is, each image served as a query to
                        identify the top 100 similar Wikidata object entities based on their
                        upper-body similarity. Analysis of the bootstrapped distribution of creation
                        dates then shows that <hi rend="italic">T</hi>-shapes start increasing
                        around 1200, peak sharply around 1400, and then gradually decrease, with
                        another smaller peak around 1800, while <hi rend="italic">Y</hi>-shaped
                        figures have a more widespread, uniform distribution between 1400 and 1900 —
                        in case of weak <hi rend="italic">Y</hi>-shapes, without notable peaks
                        (Figure 8 in Appendix A). The evolution of the shapes can be also observed
                        clearly in the contour plot of the two-dimensional upper-body posture
                        embedding. Naturally, <hi rend="italic">T</hi>- or slightly <hi
                            rend="italic">Y</hi>-shaped figures, with their symmetrically
                        outstretched arms, are most often associated with crucifixions, whereas the
                        upward curvature of stronger <hi rend="italic">Y</hi>-shapes appears in a
                        wider range of contexts, including depictions of ascension and resurrection
                        that imply spiritual elevation, such as the putti in Rembrandt’s <title
                            rend="italic">The Ascension</title> (1636). This variation in upper-body
                        posture is also reflected in the artworks’ temporal distribution: Strong <hi
                            rend="italic">Y</hi>-shapes occur mostly around 1600, with another
                        notable peak around 1900, and are rarely found before 1400. The Pr-VIPE’s
                        ability to distinguish these postures suggests its effectiveness in
                        identifying both the physical configurations and, on closer historical
                        inspection, the underlying symbolic meanings. This evidence suggests
                        potential verifiable patterns in the representation of different upper-body
                        forms across historical periods, although conclusive historical evidence
                        remains to be established.</p>
                </div>
            </div>
            <div>
                <head>Discussions</head>
                <p>In contrast to previous approaches that compute how similar two postures are
                    based solely on the positions or angles of two-dimensional keypoints <ptr target="#kovar_2002"/> <ptr target="#pehlivan_2011"/>, our three-step HPR
                    pipeline integrates the relational context between joint positions by encoding
                    them in 320-dimensional view-invariant embeddings. Qualitative experiments on a
                    data set of 644,155 Wikidata object entities confirm the method’s validity: They
                    reveal dense clusters in the two-dimensional embedding spaces corresponding to
                    the 110 manually selected reference postures derived from Iconclass. These
                    findings demonstrate the system’s ability to detect clusters of similar postures
                    in the context to their artistic expression. Although some inaccuracies — mainly
                    due to bounding box detection limitations — are observed, these errors in
                    individual joint estimates do not significantly affect retrieval performance. As
                    shown, at the aggregate level, certain posture configurations are identified as
                    compact hotspots that differ from other, more common postures in the Wikidata
                    within the embedding space. However, while it is possible to interpret the
                    embedding space from a historical perspective, the methodology at the aggregate
                    level, as opposed to the posture-to-posture search at the individual level,
                    differs markedly from traditional historical research, with the reliance on
                    metadata-induced density peaks being intuitive to statisticians, but highly
                    unconventional to art historians.</p>
                <p>It is therefore of utmost importance to emphasize that HPR is designed to support
                    knowledge generation, rather than to directly ‘produce’ knowledge — as evidenced
                    by the <hi rend="italic">T</hi>- and <hi rend="italic">Y</hi>-shaped upper-body
                    configurations of the crucifixion scenes, which may spotlight historically
                    significant conjunctions. DL models can identify areas that may contain evidence
                    or knowledge, but these need to be validated, at least by random sampling;
                    without integrated close viewing, computational analysis cannot be taken as
                    conclusive evidence or knowledge unless DL models independently provide a
                    historically plausible interpretation of their results — a capability yet to be
                    achieved. They, at least currently, can serve only as <term>recommender
                        systems</term>, provided they achieve high accuracy, as our research has
                    shown. Still, the utility of computational methods in humanities research should
                    not be underestimated: They allow researchers to quickly navigate and reduce
                    large data sets to tractable sizes, making possible studies that would otherwise
                    be logistically unfeasible due to the sheer volume of the data involved.<note>
                        For a similar discussion, see also <ptr target="#impett_2022"/>.</note> To fully
                    realize the potential of these methods, we consider a hybrid approach that
                    combines traditional scholarly expertise with computational techniques to be
                    essential. Such integration could enrich the interpretive possibilities of
                    data-driven inquiries into the past and facilitate a more nuanced understanding
                    of cultural heritage, revealing previously overlooked or misinterpreted
                    connections between objects. Thus, while computational analysis is neither
                    infallible nor standalone, it does represent a significant advance in the
                    toolkit of humanities researchers, providing them with unprecedented analytical
                    capabilities — if they know how to utilize them.</p>
                <p>However, whether a research question can be effectively pursued using DL methods
                    also depends heavily on the availability of the study’s objects, similar to
                    traditional humanities research where the identification and collection of
                    relevant sources, including archival material, is a crucial preliminary step. As
                    shown in Figure 5, Wikidata primarily features post-1800 artworks, with medieval
                    art notably underrepresented — a pattern that is common across databases. Since
                    the true distribution of artworks is not known definitively, reliance on
                    digitally available repositories might introduce research bias by either under-
                    or over-representing certain historical periods or artistic styles. At present,
                    and likely in the future, it is not possible to assemble a data set that
                    represents the full range of art history — or other historically oriented areas
                    of visual culture — as many objects have been lost or have not yet been
                    digitized. Nevertheless, for narrowly defined research questions, it may still
                    be possible to rely on a manually compiled and thoroughly curated data set, such
                    as one developed within a specific project. Despite the potential for
                    large-scale data sets like Wikidata to include works outside the traditional
                    canon, they may also inadvertently reinforce the canon within the digital
                    environment. It is therefore essential to critically evaluate the data sets
                    employed in digital humanities research, especially considering their potential
                    impact on the results: The skewed focus on more contemporary objects may cause
                    researchers to overlook or inadequately cover older or lesser-known historical
                    periods and styles, thereby perpetuating a biased view of art history.</p>
                <p>Moreover, the performance of HPE, and by extension HPR, depends fundamentally on
                    the accuracy of the bounding box detection in the pipeline’s initial stage.
                    Incorrectly estimated bounding boxes will almost certainly lead to incorrectly
                    estimated — or even missing — keypoints, which in turn will affect, to a lesser
                    extent, the embeddings generated from these keypoints. Improving the accuracy of
                    bounding box detection could mitigate cascading errors in the pipeline, thereby
                    increasing the reliability of the entire process. As shown by <ptr target="#springstein_2022"/>, the solution may not lie in the adoption of progressively newer
                    models that yield little practical benefit. Instead, optimizing the training
                    data used to create these models might prove more effective — for instance, by
                    merging data sets like PoPArt <ptr target="#schneider_2023"/> and
                    SniffyArt <ptr target="#zinnen_2023"/> to provide a broader range of
                    posture scenarios and thus enrich the model’s learning environment. However,
                    increasing model performance depends more on the quality of the data rather than
                    just its quantity. Investing in well-curated training data sets that feature a
                    diverse array of human figures, especially those previously underrepresented,
                    could significantly boost model training (cf., <ptr target="#zhao_2024"
                    />). <ptr target="#springstein_2022"/> confirm that data sets with thorough,
                    high-quality domain coverage outperform larger but less diverse alternatives.
                    Consequently, for studies requiring high visual specificity not represented in
                    available data sets, a small, manually annotated training data set is still
                    imperative for achieving high model performance.</p>
                <p>It could be argued that relying on a single embedding space provides only a
                    partial understanding of the underlying structures in high-dimensional data. By
                    reducing the dimensionality of an originally practically inscrutable latent
                    space, one inevitably imposes certain modeling assumptions that may obscure or
                    distort inherent relationships. In addition, the specific parameters employed in
                    any given dimensionality reduction technique yield divergent visualizations of
                    the same data, i.e., not all apparent clusters in a two- or three-dimensional
                    projection necessarily exist in the original high-dimensional space, and points
                    that appear far apart in the embedded space may actually be close neighbors in
                    the latent space. However, <ptr target="#wang_2021"/> observed empirically that certain
                    approaches can maintain more robust structures under parameter variation. Their
                    sensitivity analysis across multiple datasets — with both known local and global
                    structures — demonstrates that PaCMAP consistently preserves qualitative
                    relationships regardless of parameter choice. In the context of our case study,
                    this finding implies that the semantic positioning of similar postures remains
                    reproducible and is not merely a product of random chance or overly sensitive
                    parameters. This level of robustness contrasts with methods such as UMAP <ptr
                        target="#mcinnes_2018"/>, which are more sensitive to parameter
                    tuning. Ultimately, the decision to employ a particular dimensionality reduction
                    technique should be made in the context of the research scenario — the data’s
                    size and the nature of the structures one seeks to preserve — to ensure that any
                    conclusions drawn from a visual analysis are based on solid empirical ground
                    rather than the artifacts of a particular algorithm.</p>
            </div>
            <div>
                <head>Conclusion</head>
                <p>The analysis of posture has traditionally been marginalized in art-historical
                    scholarship, prompting the adoption of digital methodologies to re-examine it on
                    a large scale. In Section 3, we proposed a three-step methodology for this
                    purpose that leverages whole-body keypoints to differentiate the human body into
                    upper- and lower-body segments. These segments were first encoded in
                    320-dimensional <term>Probabilistic View-invariant Pose Embeddings</term>
                    (Pr-VIPEs). Using these Pr-VIPEs, we then assembled 110 reference postures for
                    classification purposes. This methodology’s relevance to art-historical research
                    was validated through qualitative experiments with a data set of
                    644,155 art-historical Wikidata objects, primarily addressing the following
                    questions:</p>
                <quote rend="block">How can the perceptual space of embeddings, obtained by <term>Deep
                        Learning</term> (DL) models, be explored and interpreted? What spatial
                    patterns emerge within these spaces, and to what extent can they encode
                    posture?</quote>
                <p>In this context, Section 4 outlined a two-stage approach of broad-scale filtering
                    preceded by a detailed examination of individual postures: a <term>distant
                        viewing</term> focused on an aggregate-level analysis of density-based
                    hotspots, and a <term>close viewing</term> focused on an individual-level
                    analysis of query postures. At the aggregate level, certain posture
                    configurations were identified as compact hotspots that differed from other,
                    more common postures within the embedding space. Yet, for postures that have
                    undergone considerable historical evolution, the effectiveness of
                    metadata-driven pre-filtering diminishes. Here, the individual-level
                        <term>posture-to-posture</term> search is advantageous because it provides
                    resilience to minor inaccuracies without significantly affecting the retrieval’s
                    performance. Moreover, the case study revealed numerous dense clusters in the
                    two-dimensional embedding spaces, largely corresponding to the reference
                    postures. It proved the system’s ability to identify hotspots and decipher
                    connections between object entities, thus expediting cluster identification in
                    the embedding space — DL models can thus serve effectively as <term>recommender
                        systems</term>. However, their application must be critically evaluated and
                    not undertaken without scholarly oversight, as misestimations could raise
                    concerns among traditional art historians about the trustworthiness of
                    computational methods. Nevertheless, we argue that even with manual
                    re-evaluation, the likelihood of encountering works outside the canon increases,
                    although the reproduction of the canon is, of course, also perpetuated in the
                    digital space.</p>
                <p>In the future, we plan to refine our research pipeline, focusing in particular on
                    improving the classification process, which currently — due to the limited
                    number of reference postures — makes it difficult to achieve the granularity
                    required for complex art-historical inquiries. Although it was shown that the
                    integration of a reference set can largely align with existing taxonomies of
                    body-related behavior, it also highlighted the challenges of discriminating
                    among a large number of postures with high variability. We intend to adopt a
                    cross-modal retrieval framework, as introduced by <ptr target="#delmas_2022"/>, which
                    integrates two-dimensional keypoints with textual descriptions into a joint
                    embedding space. By synthesizing these methods into a unified pipeline, we are
                    for the first time able to trace the evolution of posture on a large scale,
                    allowing a thorough exploration of their psychic structures. This approach is
                    not limited to art-historical objects but also holds promise for cultural
                    heritage objects in a broader sense. For instance, in theatre studies, the
                    analysis of body postures and movements could facilitate comparative
                    examinations across different artistic traditions, periods, and conventions.</p>
                <p/>
            </div>
            <div>
                <head>Appendix A</head>
                <figure>
                    <head>Distribution of creation dates in art-historical Wikidata objects filtered
                        by the term <q>crucifix</q> and its French and German translations.</head>
                    <graphic url="resources/images/figure07.png"/>
                </figure>
                <figure>
                    <head>Bootstrapped distribution of creation dates in the 100 Wikidata object
                        entities most similar to the query postures of the crucified Christ in
                        Marcello Venusti’s <title rend="italic">Christ on the Cross</title> (1500 —
                        1625; <hi rend="italic">T-shape</hi>), Diego Velazquez’s <title
                            rend="italic">Christ Crucified</title> (1632; <hi rend="italic">Y-shape
                            (weak)</hi>), and Peter Paul Rubens’ <title rend="italic">Christ
                            Expiring on the Cross</title> (1619; <hi rend="italic">Y-shape
                            (strong)</hi>).</head>
                    <graphic url="resources/images/figure08.png"/>
                </figure>

            </div>
            <div>
                <head>Appendix B</head>
                <p>Scatter plots represent human postures as individual points, which are placed
                    according to their <hi rend="italic">x</hi> and <hi rend="italic">y</hi>
                    coordinates from the reduced two-dimensional embedding. In contrast to scatter
                    plots, two-dimensional histograms visualize the density of the underlying data
                    rather than individual points. They partition the embedding space into a fixed
                    number of bins, here 250, with each bin colored according to the number of
                    figure postures it contains. Similarly, contour plots visualize the density of
                    postures on a two-dimensional plane. Much like topographic maps, they connect
                    points of equal density with contour lines and label each contour with its
                    corresponding density level. Thus, while contour plots offer an abstract,
                    continuous representation of density gradients, two-dimensional histograms
                    provide a segmented, quantitative analysis. Areas of high density may indicate
                    frequently observed or ‘standard’ postures, whereas regions of low density may
                    be related to less common postures.</p>
                <p/>
            </div>

        </body>
        <back>
            <listBibl>
                <bibl xml:id="arnold_2019" label="Arnold and Tilton 2019">Arnold, T. and Tilton, L.
                    (2019) <title rend="quotes">Distant viewing: Analyzing large visual
                        corpora</title>, <title rend="italic">Digital Scholarship in the
                        Humanities</title>, (34), pp. i3–i16. Available at: <ref
                        target="https://doi.org/10.1093/llc/fqz013"
                        >https://doi.org/10.1093/llc/fqz013</ref>. </bibl>
                <bibl xml:id="baxandall_1972" label="Baxandall 1972">Baxandall, M. (1972) <title
                        rend="italic">Painting and Experience in 15th Century Italy: A Primer in the
                        Social History of Pictorial Style</title>. Oxford: Oxford University
                    Press.</bibl>
                <bibl xml:id="brandmair_2015" label="Brandmair 2015">Brandmair, K. (2015) <title
                        rend="italic" xml:lang="de">Kruzifixe und Kreuzigungsgruppen aus dem Bereich der
                        ,,Donauschule''</title>. Petersberg: Imhof.</bibl>
                <bibl xml:id="butterfield-rosen_2021" label="Butterfield-Rosen_2021"
                    >Butterfield-Rosen, E. (2021) <title rend="italic">Modern Art and the Remaking
                        of Human Disposition</title>. Chicago, IL: University of Chicago
                    Press.</bibl>
                <bibl xml:id="carion_2020" label="Carion et al. 2020">Carion, N., Massa, F.,
                    Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko S. (2020) <title
                        rend="quotes">End-to-end object detection with transformers</title>, in
                        <title rend="italic">Computer Vision – ECCV 2020</title>. Cham: Springer
                    (Lecture Notes in Computer Science), pp. 213–229. Available at: <ref
                        target="https://doi.org/10.1007/978-3-030-58452-8_13"
                        >https://doi.org/10.1007/978-3-030-58452-8_13</ref>. </bibl>
                <bibl xml:id="chen_2016" label="Chen and Guestrin 2016">Chen, T. and Guestrin, C.
                    (2016) <title rend="quotes">XGBoost. A scalable tree boosting system</title>, in
                    B. Krishnapuram et al. (eds.) <title rend="italic">Proceedings of the 22nd ACM
                        SIGKDD International Conference on Knowledge Discovery and Data
                        Mining</title>. New York: ACM, pp. 785–794. Available at: <ref
                        target="https://doi.org/10.1145/2939672.2939785"
                        >https://doi.org/10.1145/2939672.2939785</ref>. </bibl>
                <bibl xml:id="delmas_2022" label="Delmas et al. 2022">Delmas, G., Weinzaepfel, P.,
                    Lucas, T., Moreno-Noguer, F., and Rogez, G. (2022) <title rend="quotes"
                        >PoseScript. 3D human poses from natural language</title>, in S. Avidan et
                    al. (eds.) <title rend="italic">Computer Vision – ECCV 2022 – 17th European
                        Conference</title>. Cham: Springer (Lecture Notes in Computer Science), pp.
                    346–362. Available at: <ref
                        target="https://doi.org/10.1007/978-3-031-20068-7_20"
                        >https://doi.org/10.1007/978-3-031-20068-7_20</ref>. </bibl>
                <bibl xml:id="drucker_2013" label="Drucker 2013">Drucker, J. (2013) <title
                        rend="quotes">Is there a <q>digital</q> art history?</title>, <title
                        rend="italic">Visual Resources</title>, 29, pp. 5–13. Available at: <ref
                        target="https://doi.org/10.1080/01973762.2013.761106"
                        >https://doi.org/10.1080/01973762.2013.761106</ref>. </bibl>
                <bibl xml:id="egidi_2000" label="Egidi et al. 2000">Egidi, M., Schneider, O.,
                    Schöning, M., Schütze, I., and Torra-Mattenklott, C. (2000) <title rend="quotes" xml:lang="de"
                        >Riskante Gesten. Einführung</title>, in M. Egidi et al. (eds.) <title
                            rend="italic" xml:lang="de">Gestik. Figuren des Körpers in Text und Bild</title>, pp.
                    11–41. <foreign xml:lang="de">Tübingen: Narr (Literatur und Anthropologie</foreign>, 8).</bibl>
                <bibl xml:id="elcott_2016" label="Elcott 2016">Elcott, N.M. (2016) <title
                        rend="italic">Artificial Darkness. An Obscure History of Modern Art and
                        Media</title>. Chicago: University of Chicago Press.</bibl>
                <bibl xml:id="eriksson_1973" label="Eriksson 1973">Eriksson, E.S. (1973) <title
                        rend="quotes">Distance perception and the ambiguity of visual stimulation. A
                        theoretical note</title>, <title rend="italic">Perception &amp;
                        Psychophysics</title>, 13(3), pp. 379–381. Available at: <ref
                        target="https://doi.org/10.3758/BF03205789"
                        >https://doi.org/10.3758/BF03205789</ref>. </bibl>
                <bibl xml:id="gombrich_1966" label="Gombrich 1966">Gombrich, E.H. (1966) <title
                        rend="quotes">Ritualized gesture and expression in art</title>, in <title
                        rend="italic">Philosophical Transactions of the Royal Society of
                        London</title>. (B, Biological Sciences), pp. 393–401. Available at: <ref
                        target="https://doi.org/10.1098/rstb.1966.0025"
                        >https://doi.org/10.1098/rstb.1966.0025</ref>. </bibl>
                <bibl xml:id="harada_2004" label="Harada et al. 2004">Harada, T., Taoka, S., Mori,
                    T., and Sato, T. (2004) <title rend="quotes">Quantitative evaluation method for
                        pose and motion similarity based on human perception</title>, in <title
                        rend="italic">4th IEEE/RAS International Conference on Humanoid Robots,
                        Humanoids 2004</title>. New York: IEEE, pp. 494–512. Available at: <ref
                        target="https://doi.org/10.1109/ICHR.2004.1442140"
                        >https://doi.org/10.1109/ICHR.2004.1442140</ref>. </bibl>
                <bibl xml:id="haussherr_1971" label="Haussherr 1971">Haussherr, R. (1971) <title
                    rend="italic" xml:lang="de">Michelangelos Kruzifixus für Vittoria Colonna: Bemerkungen zu
                    Ikonographie und theologischer Deutung</title>. <foreign xml:lang="de">Opladen: Westdeutscher
                    Verlag</foreign>.</bibl>
                <bibl xml:id="im_2018" label="Im, Verma, and Branson 2018">Im, D.J., Verma, N. and
                    Branson, K. (2018) <title rend="quotes">Stochastic neighbor embedding under
                        f-divergences</title>, [Preprint] Available at: <ref
                        target="https://doi.org/10.48550/arXiv.1811.01247"
                        >https://doi.org/10.48550/arXiv.1811.01247</ref>. </bibl>
                <bibl xml:id="impett_2022" label="Impett and Offert 2022">Impett, L. and Offert, F.
                    (2022) <title rend="quotes">There is a digital art history,</title>
                    <title rend="italic">Visual Resources</title>, 38(2), pp. 186–209. Available at:
                        <ref target="https://doi.org/10.1080/01973762.2024.2362466"
                        >https://doi.org/10.1080/01973762.2024.2362466</ref>. </bibl>
                <bibl xml:id="impett_2016" label="Impett and Süsstrunk 2016">Impett, L. and
                    Süsstrunk, S. (2016) <title rend="quotes">Pose and pathosformel in Aby Warburg’s
                        Bilderatlas</title>, in G. Hua and H. Jégou (eds.) <title rend="italic"
                        >Computer Vision – ECCV 2016 Workshops</title>. Cham: Springer (Lecture
                    Notes in Computer Science), pp. 888–902. Available at: <ref
                        target="https://doi.org/10.1007/978-3-319-46604-0_61"
                        >https://doi.org/10.1007/978-3-319-46604-0_61</ref>. </bibl>
                <bibl xml:id="jenicek_2019" label="enícek and Chum 2019" sortKey="Jenicek">Jenícek,
                    T. and Chum, O. (2019) <title rend="quotes">Linking art through human
                        poses</title>, in <title rend="italic">International Conference on Document
                        Analysis and Recognition, ICDAR 2019</title>. New York: IEEE, pp. 1338–1345.
                    Available at: <ref target="https://doi.org/10.1109/ICDAR.2019.00216"
                        >https://doi.org/10.1109/ICDAR.2019.00216</ref>. </bibl>
                <bibl xml:id="karjus_2023" label="Karjus et al. 2023">Karjus, A., Solà, M.C., Ohm,
                    T., Ahnert, S.E., and Schich, M. (2023) <title rend="quotes">Compression
                        ensembles quantify aesthetic complexity and the evolution of visual
                        art</title>, <title rend="italic">EPJ Data Science</title>, 12(1). Available
                    at: <ref target="https://doi.org/10.1140/EPJDS/S13688-023-00397-3"
                        >https://doi.org/10.1140/EPJDS/S13688-023-00397-3</ref>. </bibl>
                <bibl xml:id="kovar_2002" label="Kovar, Gleicher, and Pighin 2002">Kovar, L.,
                    Gleicher, M. and Pighin, F.H. (2002) <title rend="quotes">Motion graphs</title>,
                        <title rend="italic">ACM Transactions on Graphics</title>, 21(3), pp.
                    473–482. Available at: <ref target="https://doi.org/10.1145/566654.566605"
                        >https://doi.org/10.1145/566654.566605</ref>. </bibl>
                <bibl xml:id="li_2021" label="Li et al. 2021">Li, K., Wang, S., Zhang, X., Xu, W.,
                    and Tu, Z. (2021) <title rend="quotes">Pose recognition with cascade
                        transformers</title>, in <title rend="italic">IEEE Conference on Computer
                        Vision and Pattern Recognition, CVPR 2021</title>. New York: IEEE, pp.
                    1944–1953. Available at: <ref target="https://arxiv.org/abs/2104.06976"
                        >https://arxiv.org/abs/2104.06976</ref>. </bibl>
                <bibl xml:id="linderman_2019" label="Linderman et al. 2019">Linderman, G.C., Rachh,
                    M., Hoskins, J.G., Steinerberger, S., and Kluger, Y. (2019) <title rend="quotes"
                        >Fast interpolation-based t-SNE for improved visualization of single-cell
                        RNA-seq data</title>, <title rend="italic">Nature Methods</title>, 16(3).
                    Available at: <ref target="https://doi.org/10.1038/s41592-018-0308-4"
                        >https://doi.org/10.1038/s41592-018-0308-4</ref>. </bibl>
                <bibl xml:id="liu_2021" label="Liu, J. et al. 2021">Liu, J., Shi, M., Chen, Q., Fu,
                    H., and Tai, C-L. (2021) <title rend="quotes">Normalized human pose features for
                        human action video alignment</title>, in <title rend="italic">IEEE/CVF
                        International Conference on Computer Vision, ICCV 2021</title>. IEEE, pp.
                    11501–11511. Available at: <ref
                        target="https://doi.org/10.1109/ICCV48922.2021.01132"
                        >https://doi.org/10.1109/ICCV48922.2021.01132</ref>. </bibl>
                <bibl xml:id="liu_2022" label="Liu, T. et al. 2022">Liu, T., Sun, J.J., Zhao, L.,
                    Zhao, J., Yuan, L., Wang, Y., Chen, C.-L., Schroff, F, and Adam, H. (2022)
                        <title rend="quotes">View-invariant, occlusion-robust probabilistic
                        embedding for human pose</title>, <title rend="italic">International Journal
                        of Computer Vision</title>, 130(1), pp. 111–135. Available at: <ref
                        target="https://doi.org/10.1007/s11263-021-01529-w"
                        >https://doi.org/10.1007/s11263-021-01529-w</ref>. </bibl>
                <bibl xml:id="van-der-maaten_2008" label="van der Maaten and Hinton 2008"
                    sortKey="Vandermaaten">van der Maaten, L. and Hinton, G. (2008) <title
                        rend="quotes">Visualizing data using t-SNE</title>, <title rend="italic"
                        >Journal of Machine Learning Research</title>, 9, pp. 2579–2605. Available
                    at: <ref target="https://www.jmlr.org/papers/v9/vandermaaten08a.html"
                        >https://www.jmlr.org/papers/v9/vandermaaten08a.html</ref>. </bibl>
                <bibl xml:id="madhu_2020" label="Madhu_2020">Madhu, P., Marquart, T., Kosti, R.,
                    Bell, P., Maier, A., and Christlein, V. (2020) <title rend="quotes"
                        >Understanding compositional structures in art historical images using pose
                        and gaze priors. Towards scene understanding in digital art history</title>,
                    in A. Bartoli and A. Fusiello (eds.) <title rend="italic">Computer Vision – ECCV
                        2020 Workshops</title>. Cham: Springer (Lecture Notes in Computer Science),
                    pp. 109–125. Available at: <ref
                        target="https://doi.org/10.1007/978-3-030-66096-3_9"
                        >https://doi.org/10.1007/978-3-030-66096-3_9</ref>. </bibl>
                <bibl xml:id="madhu_2023" label="Madhu et al. 2023">Madhu, P., Villar-Corrales, A.,
                    Kosti, R., Bendschus, T., Reinhardt, C., Bell, P., Maier, A., and Christlein, V.
                    (2023) <title rend="quotes">Enhancing human pose estimation in ancient vase
                        paintings via perceptually-grounded style transfer learning</title>, <title
                        rend="italic">ACM Journal on Computing and Cultural Heritage</title>, 16(1),
                    pp. 1–17. Available at: <ref target="https://doi.org/10.1145/3569089"
                        >https://doi.org/10.1145/3569089</ref>. </bibl>
                <bibl xml:id="malkov_2020" label="Malkov_2020">Malkov, Y.A. and Yashunin, D.A.
                    (2020) <title rend="quotes">Efficient and robust approximate nearest neighbor
                        search using hierarchical navigable small world graphs,</title>
                    <title rend="italic">IEEE Transactions on Pattern Analysis and Machine
                        Intelligence</title>, 42(4), pp. 824–836. Available at: <ref
                        target="https://doi.org/10.1109/TPAMI.2018.2889473"
                        >https://doi.org/10.1109/TPAMI.2018.2889473</ref>. </bibl>
                <bibl xml:id="mccarren_2003" label="McCarren 2003" sortKey="Mccarren">McCarren, F.M.
                    (2003) <title rend="italic">Dancing Machines. Choreographies of the Age of
                        Mechanical Reproduction</title>. Stanford: Stanford University Press.</bibl>
                <bibl xml:id="mcinnes_2018" label="McInnes 2018" sortKey="Mcinnes">McInnes, L.,
                    Healy, J., Saul, N., and Großberger, L. (2018) <title rend="quotes">UMAP.
                        Uniform Manifold Approximation and Projection</title>, <title rend="italic"
                        >Journal of Open Source Software</title>, 3(29). Available at: <ref
                        target="https://doi.org/10.21105/joss.00861"
                        >https://doi.org/10.21105/joss.00861</ref>. </bibl>
                <bibl xml:id="merback_2001" label="Merback 2001">Merback, M.B. (2001) <title
                        rend="italic">The Thief, the Cross and the Wheel: Pain and the Spectacle of
                        Punishment in Medieval and Renaissance Europe</title>. London: Reaktion
                    Books.</bibl>
                <bibl xml:id="offert_2023" label="Offert and Bell 2023">Offert, F. and Bell, P.
                    (2023) <title rend="quotes">imgs.ai. A deep visual search engine for digital art
                        history</title>, in A. Baillot et al. (eds.) <title rend="italic"
                        >International Conference of the Alliance of Digital Humanities
                        Organizations, DH 2022</title>. Available at: <ref
                        target="https://doi.org/10.5281/zenodo.8107778"
                        >https://doi.org/10.5281/zenodo.8107778</ref>. </bibl>
                <bibl xml:id="pehlivan_2011" label="Pehlivan and Duygulu 2011">Pehlivan, S. and
                    Duygulu, P. (2011) <title rend="quotes">A new pose-based representation for
                        recognizing actions from multiple cameras</title>, <title rend="italic"
                        >Computer Vision and Image Understanding</title>, 115(2), pp. 140–151.
                    Available at: <ref target="https://doi.org/10.1016/j.cviu.2010.11.004"
                        >https://doi.org/10.1016/j.cviu.2010.11.004</ref>. </bibl>
                <bibl xml:id="ren_2020" label="Ren et al. 2020">Ren, X., Li, H., Huang, Z., and
                    Chen, Q. (2020) <title rend="quotes">Self-supervised dance video synthesis
                        conditioned on music</title>, in C.W. Chen et al. (eds.) <title
                        rend="italic">MM ’20: The 28th ACM International Conference on
                        Multimedia</title>. ACM, pp. 46–54. Available at: <ref
                        target="https://doi.org/10.1145/3394171.3413932"
                        >https://doi.org/10.1145/3394171.3413932</ref>. </bibl>
                <bibl xml:id="rhodin_2018" label="Rhodin, Salzmann, and Fua 2018">Rhodin, H.,
                    Salzmann, M. and Fua, P. (2018) <title rend="quotes">Unsupervised geometry-aware
                        representation for 3D human pose estimation</title>, in V. Ferrari et al.
                    (eds.) <title rend="italic">Computer Vision – ECCV 2018 – 15th European
                        Conference</title>. Springer (Lecture Notes in Computer Science), pp.
                    765–782. Available at: <ref
                        target="https://doi.org/10.1007/978-3-030-01249-6_46"
                        >https://doi.org/10.1007/978-3-030-01249-6_46</ref>. </bibl>
                <bibl xml:id="schneider_2023" label="Schneider and Vollmer 2023">Schneider, S. and
                    Vollmer, R. (2023) <title rend="quotes">Poses of people in art. A data set for
                        human pose estimation in digital art history</title>. Available at: <ref
                        target="https://doi.org/10.48550/arXiv.2301.05124"
                        >https://doi.org/10.48550/arXiv.2301.05124</ref>. </bibl>
                <bibl xml:id="so_2005" label="So and Baciu 2005">So, C.K.-F. and Baciu, G. (2005)
                        <title rend="quotes">Entropy-based motion extraction for motion capture
                        animation</title>, <title rend="italic">Computer Animation and Virtual
                        Worlds</title>, 16(3–4), pp. 225–235. Available at: <ref
                        target="https://doi.org/10.1002/cav.107"
                        >https://doi.org/10.1002/cav.107</ref>. </bibl>
                <bibl xml:id="springstein_2022" label="Springstein et al. 2022">Springstein, M.,
                    Schneider, S., Alhaus, C., and Ewerth, R. (2022) <title rend="quotes"
                        >Semi-supervised human pose estimation in art-historical images</title>, in
                    J. Magalhães et al. (eds.) <title rend="italic">MM ’22: The 30th ACM
                        International Conference on Multimedia</title>. ACM, pp. 1107–1116.
                    Available at: <ref target="https://doi.org/10.1145/3503161.3548371"
                        >https://doi.org/10.1145/3503161.3548371</ref>. </bibl>
                <bibl xml:id="steinberg_2018" label="Steinberg 2018">Steinberg, L. (2018) <title
                        rend="italic">Michelangelo’s Sculpture: Selected Essays</title>. Edited by
                    S. Schwartz. Chicago: University of Chicago Press.</bibl>
                <bibl xml:id="van-straten_1994" label="van Straten 1994" sortKey="Vanstraten">van
                    Straten, R. (1994) <title rend="italic">Iconography, Indexing, Iconclass. A
                        Handbook</title>. Leiden: Foleor.</bibl>
                <bibl xml:id="tikkanen_1912" label="Tikkanen 1912">Tikkanen, J.J. (1912) <title
                    rend="italic" xml:lang="de">Die Beinstellungen in der Kunstgeschichte: Ein Beitrag zur
                    Geschichte der künstlerischen Motive</title>. <foreign xml:lang="de">Helsingfors: Druckerei der
                    finnischen Litteraturgesellschaft</foreign>.</bibl>
                <bibl xml:id="ufer_2021" label="Ufer et al. 2021">Ufer, N., Simon, M., Lang, S., and
                    Ommer, B. (2021) <title rend="quotes">Large-scale interactive retrieval in art
                        collections using multi-style feature aggregation</title>, <title
                        rend="italic">PLOS ONE</title>, 16(11), pp. 1–38. Available at: <ref
                        target="https://doi.org/10.1371/journal.pone.0259718"
                        >https://doi.org/10.1371/journal.pone.0259718</ref>. </bibl>
                <bibl xml:id="ullman_1979" label="Ullman 1979">Ullman, S. (1979) <title
                        rend="quotes">The interpretation of structure from motion</title>, in <title
                        rend="italic">Proceedings of the Royal Society of London</title>. (B,
                    Biological Sciences), pp. 405–426. Available at: <ref
                        target="https://doi.org/10.1098/rspb.1979.0006"
                        >https://doi.org/10.1098/rspb.1979.0006</ref>. </bibl>
                <bibl xml:id="vaswani_2017" label="Vaswani et al. 2017">Vaswani, A., Shazeer, N.,
                    Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., and Polosukhin,
                    I. (2017) <title rend="quotes">Attention is all you need</title>, in I. Guyon et
                    al. (eds.) <title rend="italic">Advances in Neural Information Processing
                        Systems 30, Annual Conference on Neural Information Processing Systems
                        2017</title>, pp. 5998–6008. Available at: <ref
                        target="https://doi.org/10.48550/arXiv.1706.03762"
                        >https://doi.org/10.48550/arXiv.1706.03762</ref>. </bibl>
                <bibl xml:id="van-de-waal_1973" label="van de Waal 1973" sortKey="Vandewaal">van de
                    Waal, H. (1973) <title rend="italic">Iconclass. An Iconographic Classification
                        System. Completed and Edited by L. D. Couprie with R. H. Fuchs</title>.
                    Amsterdam: North-Holland Publishing Company.</bibl>
                <bibl xml:id="wang_2021" label="Wang et al. 2021">Wang, Y., Huang, H., Rudin, C.,
                    and Shaposhnik, Y. (2021) <title rend="quotes">Understanding how dimension
                        reduction tools work. An empirical approach to deciphering t-SNE, UMAP,
                        TriMap, and PaCMAP for data visualization</title>, <title rend="italic"
                        >Journal of Machine Learning Research</title>, 22, p. 1–73. Available at:
                        <ref target="https://doi.org/10.48550/arXiv.2012.04456"
                        >https://doi.org/10.48550/arXiv.2012.04456</ref>. </bibl>
                <bibl xml:id="warburg_1998" label="Warburg 1998">Warburg, A. (1998) <title
                    rend="quotes" xml:lang="de">Dürer und die italienische Antike</title>, in H. Bredekamp and
                    M. Diers (eds.) <title rend="italic" xml:lang="de">Die Erneuerung der heidnischen Antike.
                        Kulturwissenschaftliche Beiträge zur Geschichte der europäischen
                        Renaissance. Gesammelte Schriften</title>. Berlin: Akademie Verlag, pp.
                    443–449.</bibl>
                <bibl xml:id="zhao_2024" label="Zhao et al. 2024">Zhao, D., Andres, J.T.A.,
                    Papakyriakopoulos, O, and Xiang, A. (2024) <title rend="quotes">Position.
                        Measure dataset diversity, don’t just claim it.</title> Available at: <ref
                        target="https://doi.org/10.48550/arXiv.2407.08188"
                        >https://doi.org/10.48550/arXiv.2407.08188</ref>. </bibl>
                <bibl xml:id="zhao_2022" label="Zhao, Salah, and Salah 2022">Zhao, S., Salah, A.A.
                    and Salah, A.A. (2022) <title rend="quotes">Automatic analysis of human body
                        representations in western art</title>, in L. Karlinsky, T. Michaeli, and K.
                    Nishino (eds.) <title rend="italic">Computer Vision – ECCV 2022
                        Workshops</title>. Cham: Springer (Lecture Notes in Computer Science), pp.
                    282–297. Available at: <ref
                        target="https://doi.org/10.1007/978-3-031-25056-9_19"
                        >https://doi.org/10.1007/978-3-031-25056-9_19</ref>. </bibl>
                <bibl xml:id="zimmermann_2011" label="Zimmerman 2011">Zimmermann, M.F. (2011) <title
                    rend="quotes" xml:lang="de">Die Sprache der Gesten und der Ursprung der menschlichen
                        Kommunikation. Bildwissenschaftliche Überlegungen im Ausgang von
                        Leonardo</title>, in H. Böttger, G. Gien, and T. Pittrof (eds.) <title
                            rend="italic" xml:lang="de">Aufbrüche. Für Andreas Lob-Hüdepohl</title>. Eichstätt:
                    Academic Press, pp. 178–197.</bibl>
                <bibl xml:id="zinnen_2023" label="Zinnen et al. 2023">Zinnen, M., Hussian, A., Tran,
                    H., Madhu, P., Maier, A., and Christlein, V. (2023) <title rend="quotes"
                        >SniffyArt. The dataset of smelling persons</title>, in V. Gouet-Brunet, R.
                    Kosti, and L. Weng (eds.) <title rend="italic">Proceedings of the 5th Workshop
                        on analySis, Understanding and proMotion of heritAge Contents, SUMAC
                        2023</title>. New York: ACM, pp. 49–58. Available at: <ref
                        target="https://doi.org/10.1145/3607542.3617357"
                        >https://doi.org/10.1145/3607542.3617357</ref>. </bibl>
            </listBibl>

        </back>
    </text>
</TEI>
