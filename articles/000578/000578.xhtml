<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Compounded Mediation:
                  A Data Archaeology of the Newspaper Navigator Dataset</h1>
               
               
               <div class="author"><span style="color: grey">Benjamin Lee
                     </span> &lt;<a href="mailto:bcgl_at_cs_dot_washington_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('bcgl_at_cs_dot_washington_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('bcgl_at_cs_dot_washington_dot_edu'); return false;">bcgl_at_cs_dot_washington_dot_edu</a>&gt;, The Library of Congress &amp; The University of Washington</div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Compounded%20Mediation%3A%20A%20Data%20Archaeology%20of%20the%20Newspaper%20Navigator%20Dataset&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Lee&amp;rft.aufirst=Benjamin&amp;rft.au=Benjamin%20Lee"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>The increasing roles of machine learning and artificial intelligence in the construction
                     of cultural heritage and humanities datasets necessitate critical examination of the
                     myriad biases introduced by machines, algorithms, and the humans who build and deploy
                     them. From image classification to optical character recognition, the effects of
                     decisions ostensibly made by machines compound through the digitization pipeline and
                     redouble in each step, mediating our interactions with digitally-rendered artifacts
                     through the search and discovery process. As a result, scholars within the digital
                     humanities community have begun advocating for the proper contextualization of cultural
                     heritage datasets within the socio-technical systems in which they are created and
                     utilized. One such approach to this contextualization is the <em class="term">data
                        archaeology</em>, a form of humanistic excavation of a dataset that Paul Fyfe defines
                     as “recover[ing] and reconstitut[ing] media objects within their changing ecologies”
                     [<a class="ref" href="#fyfe2016">Fyfe 2016</a>]. Within critical data studies, this excavation of a dataset - including its
                     construction and mediation via machine learning - has proven to be a capacious approach.
                     However, the data archaeology has yet to be adopted as standard practice among cultural
                     heritage practitioners who produce such datasets with machine learning. 
                     </p>
                  
                  <p>In this article, I present a data archaeology of the Library of Congress’s <cite class="title italic">Newspaper Navigator </cite>dataset, which I created as part of the
                     Library of Congress’s Innovator in Residence program [<a class="ref" href="#lee2020b">Lee et al. 2020</a>]. The dataset
                     consists of visual content extracted from 16 million historic newspaper pages in the
                     <cite class="title italic">Chronicling America</cite> database using machine learning techniques. In
                     this case study, I examine the manifold ways in which a <cite class="title italic">Chronicling
                        America</cite> newspaper page is transmuted and decontextualized during its journey
                     from a physical artifact to a series of probabilistic photographs, illustrations,
                     maps,
                     comics, cartoons, headlines, and advertisements in the<cite class="title italic"> Newspaper
                        Navigator</cite> dataset [<a class="ref" href="#fyfe2016">Fyfe 2016</a>]. Accordingly, I draw from fields of scholarship
                     including media archaeology, critical data studies, science and technology studies,
                     and
                     the autoethnography throughout. <a class="noteRef" href="#d4e175">[1]</a>
                     </p>
                  
                  <p>To excavate the <cite class="title italic">Newspaper Navigator</cite> dataset, I consider
                     the digitization journeys of four different pages in Black newspapers included in
                     <cite class="title italic">Chronicling America</cite>, all of which reproduce the same photograph of
                     W.E.B. Du Bois in an article announcing the launch of <cite class="title italic">The Crisis</cite>,
                     the official magazine of the NAACP. In tracing the newspaper pages’ journeys, I unpack
                     how each step in the <cite class="title italic">Chronicling America</cite> and <cite class="title italic">Newspaper Navigator</cite> pipelines, such as the imaging process and the construction
                     of training data, not only imprints bias on the resulting <cite class="title italic">Newspaper
                        Navigator</cite> dataset but also propagates the bias through the pipeline via the
                     machine learning algorithms employed. Along the way, I investigate the limitations
                     of
                     the <cite class="title italic">Newspaper Navigator</cite> dataset and machine learning techniques
                     more generally as they relate to cultural heritage, with a particular focus on
                     marginalization and erasure via algorithmic bias, which implicitly rewrites the archive
                     itself. </p>
                  
                  <p>In presenting this case study, I argue for the value of the data archaeology as a
                     mechanism for contextualizing and critically examining cultural heritage datasets
                     within
                     the communities that create, release, and utilize them. I offer this autoethnographic
                     investigation of the <cite class="title italic">Newspaper Navigator </cite>dataset in the hope that
                     it will be considered not only by users of this dataset in particular but also by
                     digital humanities practitioners and end users of cultural heritage datasets writ
                     large. 
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">I. An Introduction to the Newspaper Navigator Dataset</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">In partnership with LC Labs, the National Digital Newspaper Program, and IT Design
                     &amp;
                     Development at the Library of Congress, as well as Professor Daniel Weld at the
                     University of Washington, I constructed the <cite class="title italic">Newspaper Navigator</cite>
                     dataset as the first phase of my Library of Congress Innovator in Residence project,<cite class="title italic"> Newspaper Navigator</cite>. The project has its origins in <cite class="title italic">Chronicling America</cite>, a database of digitized historic American newspapers
                     created and maintained by the National Digital Newspaper Program, itself a partnership
                     between the Library of Congress and the National Endowment for the Humanities. Content
                     in <cite class="title italic">Chronicling America</cite>is contributed by state partners of the National Digital
                     Newspaper Program who have applied for and received awards from the Division of
                     Preservation and Access at the National Endowment for the Humanities [<a class="ref" href="#mears2014">Mears 2014</a>]. At
                     the time of the construction of the <cite class="title italic">Newspaper Navigator</cite> dataset in March, 2020,
                     <cite class="title italic">Chronicling America</cite> contained approximately 16.3 million digitized historic newspaper
                     pages published between 1789 and 1963, covering 47 states as well as Washington, D.C.
                     and Puerto Rico. The technical specifications of the National Digital Newspaper Program
                     require that each digitized page in <cite class="title italic">Chronicling America</cite> comprises the following digital
                     artifacts [<a class="ref" href="#national2020">National Digital Newspaper Program 2020</a>]: </div>
                  
                  <div class="ptext">
                     <ol class="list">
                        <li class="item">A page image in two raster formats:
                           <div class="ptext">
                              <ol class="list">
                                 <li class="item">Grayscale, scanned for maximum resolution possible between 300-400 DPI,
                                    relative to the original material, uncompressed TIFF 6.0 </li>
                                 <li class="item">Same image, compressed as JPEG2000</li>
                              </ol>
                           </div>
                           </li>
                        <li class="item">Optical character recognition (OCR) text and associated bounding boxes for words
                           (one file per page image)</li>
                        <li class="item">PDF Image with Hidden Text, i.e., with text and image correlated</li>
                        <li class="item">Structural metadata (a) to relate pages to title, date, and edition; (b) to
                           sequence pages within issue or section; and (c) to identify associated image and OCR
                           files</li>
                        <li class="item">Technical metadata to support the functions of a trusted repository</li>
                     </ol>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Additional artifacts and metadata are contributed for each digitized newspaper issue
                     and
                     microfilm reel. All digitized pages are in the public domain and are available online
                     via a public search user interface,<a class="noteRef" href="#d4e283">[1]</a> making <cite class="title italic">Chronicling America</cite> an immensely rich resource for the American
                     public.</div> 
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The central goal of <cite class="title italic">Newspaper Navigator </cite>is to re-imagine how the
                     American public explores <cite class="title italic">Chronicling America</cite> by utilizing
                     emerging machine learning techniques to extract, categorize, and search over the visual
                     content and headlines in <cite class="title italic">Chronicling America</cite>’s 16.3 million pages
                     of digitized historic newspapers. <cite class="title italic">Newspaper Navigator</cite> was both
                     inspired and directly enabled by the Library of Congress’s <cite class="title italic">Beyond
                        Words</cite> crowdsourcing initiative [<a class="ref" href="#ferriter2017">Ferriter 2017</a>]. Launched by LC Labs in 2017, <cite class="title italic">Beyond Words</cite> engages the American public by asking volunteers to
                     identify and draw boxes around photographs, illustrations, maps, comics, and editorial
                     cartoons on World War I-era pages in <cite class="title italic">Chronicling America</cite>, note
                     the visual content categories, and transcribe the relevant textual information such
                     as
                     titles and captions.<a class="noteRef" href="#d4e319">[2]</a> The thousands of annotations
                     created by <cite class="title italic">Beyond Words </cite>volunteers are in the public domain and
                     available for download online. <cite class="title italic">Newspaper Navigator </cite>directly
                     builds on <cite class="title italic">Beyond Words</cite> by utilizing these annotations, as well as
                     additional annotations of headlines and advertisements, to train a machine learning
                     model to detect visual content in historic newspapers.<a class="noteRef" href="#d4e339">[3]</a> Because <cite class="title italic">Beyond Words</cite> volunteers were asked
                     to draw bounding boxes to include any relevant textual content, such as a photograph’s
                     title, this machine learning model learns during training to include relevant textual
                     content when predicting bounding boxes.<a class="noteRef" href="#d4e344">[4]</a> Furthermore, in the <cite class="title italic">Transcribe </cite>step of <cite class="title italic">Beyond Words</cite>, the system provided the
                     OCR with each bounding box as an initial transcription for the volunteer to correct;
                     inspired by this, the <cite class="title italic">Newspaper Navigator</cite> pipeline automatedly
                     extracts the OCR falling within each predicted bounding box in order to provide noisy
                     textual metadata for each image. In the case of headlines, this method enables the
                     headline text to be directly extracted from the bounding box predictions. Lastly,
                     the
                     pipeline generates image embeddings for the extracted visual content using an image
                     classification model trained on ImageNet.<a class="noteRef" href="#d4e356">[5]</a> A diagram
                     of the full <cite class="title italic">Newspaper Navigator</cite> pipeline can be found in Figure
                     1. </div> 
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image1.png" rel="external"><img src="resources/images/image1.png" style="" alt="A diagram of newspaper screenshots" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 1. </div>A diagram showing the <cite class="title italic">Newspaper Navigator</cite> pipeline,
                        which processed over 16.3 million historic newspaper pages in <cite class="title italic">Chronicling America</cite>, resulting in the <cite class="title italic">Newspaper
                           Navigator</cite> dataset.</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">Over the course of 19 days from late March to early April of 2020, the <cite class="title italic">Newspaper Navigator</cite> pipeline processed 16.3 million pages in <cite class="title italic">Chronicling America</cite>; the
                     resulting <cite class="title italic">Newspaper Navigator </cite>dataset was publicly released in
                     May, 2020. The full dataset, as well as all code written for this project, are available
                     online and have been placed in the public domain for unrestricted re-use.<a class="noteRef" href="#d4e393">[6]</a> Currently,
                     the <cite class="title italic">Newspaper Navigator</cite> dataset can be queried using HTTPS and
                     Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made
                     available for download, along with associated metadata. These pre-packaged datasets
                     consist of different types of visual content for each year, from 1850 to 1963, allowing
                     users to download, for example, all of the maps from 1863 or all of the photographs
                     from
                     1910. For more information on the technical aspects of the pipeline and the construction
                     of the <cite class="title italic">Newspaper Navigator</cite> dataset, I refer the reader to [<a class="ref" href="#lee2020b">Lee et al. 2020</a>]</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">II. Why a Data Archaeology?</h1>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">As machine learning and artificial intelligence play increasing roles in digitization
                     and digital content stewardship, the Libraries, Archives, and Museums (“LAM”) community
                     has repeatedly emphasized the importance of ensuring that these emerging methodologies
                     are incorporated ethically and responsibly. Indeed, a major theme that emerged from
                     the
                     “Machine Learning + Libraries Summit” hosted by LC Labs in September, 2019, was that
                     “there is much more ‘human’ in machine learning than the name conveys” and that
                     transparency and communication are first steps toward addressing the “human
                     subjectivities, biases, and distortions” embedded within machine learning systems [<a class="ref" href="#lclabs2020">LC Labs and Digital Strategy Directorate 2020</a>]. 
                     This data archaeology has been written in
                     support of this call for transparency and responsible stewardship, which is echoed
                     in
                     the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan
                     Cordell’s report to the Library of Congress <cite class="title quote">ML + Libraries: A Report on the State of
                        the Field,</cite> Thomas Padilla’s OCLC position paper <cite class="title quote">Responsible Operations: Science,
                        Machine Learning, and AI in Libraries</cite>, and the University of Nebraska-Lincoln’s report
                     on machine learning to the Library of Congress [<a class="ref" href="#congress2019">Library of Congress 2019</a>]; [<a class="ref" href="#cordell2020">Cordell 2020</a>];
                     [<a class="ref" href="#padilla2019">Padilla 2019</a>]; [<a class="ref" href="#lorang2020">Lorang et al 2020</a>]. I write this data archaeology from my
                     perspective of having created the dataset, and although I am not without my own biases,
                     I have attempted to represent my work as honestly as possible. Accordingly, I seek
                     not
                     only to document the construction of the <cite class="title italic">Newspaper Navigator
                        </cite>dataset through the lens of data stewardship but also to critically examine the
                     dataset’s limitations. In doing so, I advocate for the importance of autoethnographic
                     approaches to documenting a cultural heritage dataset’s construction from a humanistic
                     perspective.
                     </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">This article draws inspiration from recent works in media and data archaeology,
                     including Paul Fyfe’s “An Archaeology of Victorian Newspapers”; Bonnie Mak’s
                     “Archaeology of a Digitization”; Kate Crawford and Trevor Paglen’s “Excavating AI: The
                     Politics of Images in Machine Learning Training Sets”; and, most directly, Ryan
                     Cordell’s “Qi-jtb the Raven: Taking Dirty OCR Seriously,” in which Cordell traces the
                     digitization of a single issue of the <cite class="title italic">Lewisburg Chronicle</cite> from
                     its selection by the Pennsylvania Digital Newspaper Project to its ingestion into
                     the
                     <cite class="title italic">Chronicling America </cite>online database, with a focus on the
                     distortive effects of OCR [<a class="ref" href="#fyfe2016">Fyfe 2016</a>]; [<a class="ref" href="#mak2017">Mak 2017</a>]; [<a class="ref" href="#crawford2019">Crawford and Paglen 2019</a>]; [<a class="ref" href="#cordell2017">Cordell 2017</a>].
                     As argued by Trevor Owens and Thomas Padilla, it is essential to “document how
                     digitization practices and how the affordances of particular sources … produce
                     unevenness in the discoverability and usability of collections” [<a class="ref" href="#owens2020">Owens and Padilla 2020</a>].
                     Recent works within the machine learning literature have analogously emphasized
                     the importance of documenting the collection and curation efforts underpinning community
                     datasets and machine learning models. Reporting mechanisms include “Datasheets for
                     Datasets,” “Dataset Nutrition Labels,” “Data Statements for NLP,” “Model Cards for Model
                     Reporting,” and “Algorithmic Impact Assessments” [<a class="ref" href="#gebru2020">Gebru et al. 2020</a>]; 
                     [<a class="ref" href="#holland2018">Holland et al. 2018</a>]; [<a class="ref" href="#bender2018">Bender and Friedman 2018</a>]; [<a class="ref" href="#mitchell2019">Mitchell et al. 2019</a>]; [<a class="ref" href="#reisman2018">Reisman et al. 2018</a>]. This
                     case study adopts a similar framing in stressing the importance of reporting mechanisms,
                     with a particular focus on the data archaeology in the context of cultural heritage
                     datasets.
                     </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7"> In the following sections, I trace the digitization process and data flow for <cite class="title italic">Newspaper Navigator</cite>, beginning with the physical artifact of the
                     newspaper itself and ending with the machine learning predictions that constitute
                     the
                     <cite class="title italic">Newspaper Navigator </cite>dataset, reflecting on each step through
                     the lens of discoverability and erasure. In particular, I study four different <cite class="title italic">Chronicling America </cite>Black newspaper pages published in 1910, each
                     depicting the same photograph of W.E.B. Du Bois, as the pages move through the <cite class="title italic">Chronicling America</cite> and <cite class="title italic">Newspaper Navigator</cite>
                     pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter
                     from <cite class="title italic">The Baltimore Afro-American </cite> [<a class="ref" href="#farrar1998">Farrar 1998</a>]; the headline is
                     as follows: </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">NEW MOVEMENT </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">BEGINS WORK</div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">Plan and Scope of the Asso-</div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">ciation Briefly Told. </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Will Publish the Crisis. </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">Review of Causes Which Led to the </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">Organization of the Association in </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">New York and What Its Policy Will </div>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">Be-Career and Work of Professor </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">W.E.B. Du Bois</div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">The article describes the creation of the National Association for the Advancement
                     of
                     Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch
                     of
                     <cite class="title italic">The Crisis</cite>, the official magazine of the NAACP, with Du Bois
                     as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910,
                     issue of the <cite class="title italic">Iowa State Bystander </cite> [<a class="ref" href="#iowa1910">Iowa State Bystander 1910</a>];
                     the 16th page of the October 15th, 1910, issue of <cite class="title italic">Franklin’s Paper the
                        Statesman </cite> [<a class="ref" href="#franklin1910">Franklin’s Paper the Statesman 1910</a>]; and the 2nd and 3rd pages of
                     the October 15th, 1910, and November 26th, 1910, issues of <cite class="title italic">The Broad
                        Ax</cite>, respectively [<a class="ref" href="#ax1910a">The Broad Ax 1910a</a>]; [<a class="ref" href="#ax1910b">The Broad Ax 1910b</a>]. All four digitized
                     pages are reproduced in the Appendix. </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">III. <cite class="title italic">Chronicling America</cite>: A Genealogy of Collecting, Microfilming, and Digitizing</h1>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">Any examination of <cite class="title italic">Newspaper Navigator </cite>must begin with the
                     genealogy of collecting, microfilming, and digitizing that dictates which newspapers
                     have been ingested into the <cite class="title italic">Chronicling America </cite>database. The
                     question of what to digitize is, in practice, answered and realized incrementally
                     over
                     decades, beginning at its most fundamental level with the question of which newspapers
                     have survived and which have been reduced to lacunae in the historical record [<a class="ref" href="#hardy2019">Hardy and DiCuirci 2019</a>].
                     <a class="noteRef" href="#d4e614">[7]</a> Historic
                     newspapers present challenges for digitization in part due to the ephemerality of
                     the
                     physical printed newspaper itself: many newspapers were microfilmed and immediately
                     discarded due to a fear that the physical pages would deteriorate.<a class="noteRef" href="#d4e619">[8]</a> Indeed, almost all of the
                     pages included in <cite class="title italic">Chronicling America</cite> have been digitized
                     directly from microfilm. In the next section, I will examine the microfilm imaging
                     process in more detail; however, in most cases, librarians selected newspapers for
                     collecting and microfilming decades before the National Digital Newspaper Program
                     was
                     launched in 2004. These selections were informed by a range of factors including
                     historical significance - itself a subjective, nebulous, and ever-evolving notion
                     that
                     has historically served as the basis for perpetuating oppression within the historical
                     record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black
                     newspapers in <cite class="title italic">Chronicling America</cite>, in particular in relation to
                     pre-Civil War era newspapers [Fagan 2016]. It is imperative to remember that this
                     paucity can directly be traced back decades to the collecting and preserving
                     stages.<a class="noteRef" href="#d4e631">[9]</a>
                     </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">In regard to collecting, the newspaper page is both an informational object (i.e.,
                     the
                     newspaper page as defined by its content) and a material object (i.e., the specific
                     printed copy of the newspaper page) [<a class="ref" href="#owens2018">Owens 2018</a>]. At some point in time, librarians
                     accessioned a specific copy of each printed page and microfilmed it or contracted
                     out
                     the microfilming. The materiality of that specific printed page is a confluence of
                     unique ink smudges, rips, creases, and page alignment, much of which is captured in
                     the
                     microfilm imaging process. Though we may not make much of a crease or a smudge on
                     a
                     digitized page when we find it in the <cite class="title italic">Chronicling America</cite>
                     database, it can very well take on a life of its own with a machine learning algorithm
                     in <cite class="title italic">Newspaper Navigator</cite>. The machine learning algorithm might deem
                     two newspaper photographs as similar simply due to the presence of creases or smudges,
                     even if the photographs are easily discernible to the naked eye, or the smudges are
                     of
                     entirely different origin (i.e., a printing imperfection versus a smudge from a dirty
                     hand). </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">It is only by foregrounding these subtleties of the collection, preservation, and
                     microfilming processes that we can understand the selection process for <cite class="title italic">Chronicling America</cite> in its proper context. The grant-seeking
                     process dictates selection criteria for <cite class="title italic">Chronicling America </cite>by
                     which state-level institutions including state libraries, historical societies, and
                     universities apply for two years of grant funding from the National Digital Newspaper
                     Program via the Division of Preservation and Access at the National Endowment for
                     the
                     Humanities. With the awarding of a grant, a state-level awardee then digitizes
                     approximately 100,000 newspaper pages published in their state for inclusion in
                     <cite class="title italic">Chronicling America</cite> [<a class="ref" href="#national2020">National Digital Newspaper Program 2020</a>]; [<a class="ref" href="#neh2020">NEH Division of Preservation and Access 2020</a>].
                     The grant-seeking and awarding process is nuanced, but
                     salient points include that state-level applicants must assemble an advisory board
                     including scholars, teachers, librarians, and archivists to aid in the selection of
                     newspapers, and grants are reviewed by National Endowment for the Humanities staff,
                     as
                     well as peer reviewers.<a class="noteRef" href="#d4e670">[10]</a>
                     </div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">Regarding selection criteria for newspaper titles, the National Digital Newspaper
                     Program defines the following factors for state-level awardees to consider for content
                     selection after a newspaper is determined to be in the public domain [<a class="ref" href="#national">National Digital Newspaper Program no date</a>]: </div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">image quality in the selection of microfilm</li>
                        <li class="item">research value</li>
                        <li class="item">geographic representation</li>
                        <li class="item">temporal coverage</li>
                        <li class="item">bibliographic completeness of microfilm copy</li>
                        <li class="item">diversity (i.e., “newspaper titles that document a significant minority community
                           at the state or regional level”)</li>
                        <li class="item">whether the title is orphaned (i.e., whether the newspaper has “ceased publication
                           and lack[s] active ownership” [Chronicling America no date])</li>
                        <li class="item">whether the title has already been digitized.</li>
                     </ul>
                  </div>
                  
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">Though factors such as research value are considered by each state awardee’s advisory
                     board, as well as by the National Endowment for the Humanities and peer review experts,
                     the titles included in <cite class="title italic">Chronicling America</cite> are largely dictated
                     by which exist on microfilm and are of sufficient image quality within a state-level
                     grantee’s collection. Thus, the significance of the collection and microfilming
                     practices of decades prior cannot be understated.</div>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">I also highlight that assessing microfilmed titles based on image quality is a complex
                     procedure in its own right. The National Digital Newspaper Program has made publicly
                     available a number of resources devoted specifically to this task, including documents
                     and video tutorials [<a class="ref" href="#barrall2005">Barrall and Guenther 2005</a>]; [<a class="ref" href="#meta">Meta | Morphosis no date</a>]. They
                     articulate factors such as the microfilm generation (archive master, print master,
                     or
                     review copy), the material (polyester or acetate), the reduction ratio, and the physical
                     condition. The detailed resources made available by the National Digital Newspaper
                     Program, the Library of Congress, and the National Endowment for the Humanities for
                     navigating this process are testaments to the multidimensional complexity of the
                     selection process for <cite class="title italic">Chronicling America </cite> [<a class="ref" href="#national2019">National Digital Newspaper Program 2019</a>];
                     [<a class="ref" href="#national">National Digital Newspaper Program no date</a>]; [<a class="ref" href="#neh2020">NEH Division of Preservation and Access 2020</a>].</div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">We have not yet investigated the topic of digitization, and we have already encountered
                     a profusion of factors from collection to digitization that mediate which artifacts
                     appear in <cite class="title italic">Chronicling America</cite> and thus <cite class="title italic">Newspaper Navigator</cite>. Let us now examine the microfilm itself.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">IV. The Microfilm</h1>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">In “What Computational Archival Science Can Learn from Art History and Material Culture
                     Studies,” Lyneise Williams shares a powerful anecdote of coming across a physical copy
                     of a 1927 issue of the French sports newspaper <cite class="title italic">Match L’Intran</cite>
                     that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front
                     cover [<a class="ref" href="#williams2019">Williams 2019</a>]. Williams describes Brown as “glowing. He looked like a 1920s film
                     star rather than a boxer” [<a class="ref" href="#williams2019">Williams 2019</a>]. Curious to learn more about the printing
                     process, Williams discovered that the issue of <cite class="title italic">Match L’Intran</cite> was
                     produced using rotogravure, a specific printing process that could “capture details in
                     dark tones” [<a class="ref" href="#williams2019">Williams 2019</a>]. However, when Williams found a version of the same
                     newspaper cover that had been digitized from microfilm, it was apparent that the
                     microfilming process had washed out the detail of the rotogravure, reducing Brown
                     to a
                     “flat black, cartoonish form” [<a class="ref" href="#williams2019">Williams 2019</a>]. Williams relays the anecdote to
                     articulate that the microfilming process itself is thus a form of erasure for
                     communities of color [<a class="ref" href="#williams2019">Williams 2019</a>].</div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">The grayscale saturation of photographs induced by microfilming is widely documented
                     and
                     recognizable to most researchers who have ever worked with the medium [<a class="ref" href="#baker2001">Baker 2001</a>];
                     however, Lyneise Williams’s article affords us a lens into what precisely is lost
                     amongst the distortive effects of the microfilming process. This erasure via
                     microfilming can be seen in <cite class="title italic">Chronicling America </cite>directly. In
                     Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different
                     <cite class="title italic">Chronicling America</cite> newspaper pages published during October and
                     November of 1910 and digitized from microfilm [<a class="ref" href="#iowa1910">Iowa State Bystander 1910</a>]; [<a class="ref" href="#franklin1910">Franklin’s Paper the Statesman 1910</a>]; 
                     [<a class="ref" href="#ax1910a">The Broad Ax 1910a</a>]; [<a class="ref" href="#ax1910b">The Broad Ax 1910b</a>]. The phenomenon
                     described by Williams is immediately recognizable in these four images: Du Bois’s
                     facial
                     features are distorted by the grayscale saturation. In the case of the <cite class="title italic">Iowa State Bystander</cite>, Du Bois has been rendered into a silhouette.</div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">Moreover, each digitized reproduction reveals unique visual qualities, varying in
                     contrast, sharpness, and noise - a testament to the confluence of mediating conditions
                     from printing through digitization that have rendered each newspaper photograph in
                     digital form. Even in the case of the two images reproduced in the <cite class="title italic">The
                        Broad Ax</cite>, which were digitized from the very same microfilm reel (reel
                     #00280761059) by the University of Illinois at Urbana-Champaign Library, variations
                     are
                     still apparent. To understand how these subtle differences between images are amplified
                     through digitization, we now turn to optical character recognition.</div>
                  
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image2.jpg" rel="external"><img src="resources/images/image2.jpg" style="" alt="Images of W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 2. </div>The same image of W.E.B. Du Bois reproduced in 4 different digitized Black
                        newspapers in <cite class="title italic">Chronicling America</cite> from 1910. Note that the
                        combined effects of printing, microfilming, and digitizing have led to different visual
                        effects in each image, ranging from contrast to sharpness.</div>
                  </div>
                  
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">V. OCR</h1>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">Optical character recognition, commonly called OCR, refers to machine learning
                     algorithms that are trained to read images of typewritten text and output
                     machine-readable text, thereby providing the bridge between an image of typewritten
                     text
                     and the transcribed text itself. Because OCR algorithms are “trained and evaluated using
                     labeled data: examples with ground-truth classification labels that have been assigned
                     by another means,” the algorithms are considered a form of <em class="term">supervised learning</em> in the
                     machine learning literature [<a class="ref" href="#lee2019">Lee 2019</a>]. OCR engines are remarkably powerful in their
                     ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata
                     for <cite class="title italic">Chronicling America</cite>, enabling keyword search in the search
                     portal and making possible scholarship with the newspaper text at large scales.<a class="noteRef" href="#d4e833">[11]</a> However,
                     OCR is not perfect. Although humans are able to discern an “E” from an “R” on a
                     digitized page even if the type has been smudged, an OCR engine is not always able
                     to do
                     so: its performance is dependent on factors ranging from the sharpness of text in
                     an
                     image to printing imperfections to the specific typography on the page. </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">In Figure 3, I show the same four images shown in Figure 2, along with OCR
                     transcriptions of the captions provided by <cite class="title italic">Chronicling America</cite>.
                     All four transcriptions fail to reproduce the true caption with 100% accuracy, differing
                     from one another by at least one character. Consequently, a keyword search of “W. E. B.
                     Du Bois” over the raw text would not register the caption for any of the four
                     photographs (the <cite class="title italic">Chronicling America</cite> search portal utilizes a
                     form of relevance search to alleviate this problem). These examples reveal how sensitive
                     OCR engines are to slight perturbations, or “noise,” in the digitized images, from ink
                     smudges to text sharpness to page contrast. Though the NDNP awardees who contributed
                     these pages may have utilized different OCR engines or chosen different OCR settings,
                     the OCR for the two image captions from <cite class="title italic">The Broad Ax </cite>that have
                     been digitized from the very same microfilm reel was in all likelihood generated using
                     the same
                     OCR engine and settings. Put succinctly, OCR engines amplify the noise from both the
                     material page and the digitization pipeline.<a class="noteRef" href="#d4e865">[12]</a>
                     </div>
                  
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image3.jpg" rel="external"><img src="resources/images/image3.jpg" style="" alt="four images of W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 3. </div>The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in
                        the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in
                        <cite class="title italic">Chronicling America</cite>. These OCR transcriptions are provided by <cite class="title italic">Chronicling America</cite>.</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">Though OCR engines have become standard components of digitization pipelines, it is
                     important to remember that OCR engines are themselves machine learning models that
                     have
                     been trained on sets of transcribed typewritten pages. Like any machine learning model,
                     OCR predictions are thus subject to biases encoded not only in the OCR engine’s
                     architecture but also in the training data itself. Though it is often called 
                     <em class="term">algorithmic bias</em>, this bias is undeniably human, in that the
                     construction of training data machine learning models are imprinted with countless
                     human
                     decisions and judgment calls. For example, if an OCR engine is trained on transcriptions
                     that consistently misspell a word, the OCR engine will amplify this misspelling across
                     all transcriptions of processed pages.<a class="noteRef" href="#d4e899">[13]</a> A recurring theme of algorithmic bias is that it is a force for
                     marginalization, especially in the context of how we navigate information digitally.
                     In
                     <cite class="title italic">Algorithms of Oppression</cite>, Safiya Noble describes how Google’s
                     search engine consistently marginalizes women and people of color by displaying search
                     results that reinforce racism [<a class="ref" href="#noble2018">Noble 2018</a>]. This bias is not restricted to Google: in
                     <cite class="title italic">Masked by Trust: Bias in Library Discovery</cite>, Matthew Reidsma
                     articulates how library search engines suffer from similar biases [<a class="ref" href="#reidsma2019">Reidsma 2019</a>].
                     Despite the fact that knowledge of algorithmic bias in relation to search engines
                     and
                     image recognition tools is becoming increasingly widespread among the cultural heritage
                     community, the errors introduced by OCR engines are often accepted as inevitable without
                     critical inquiry from this perspective. However, algorithmic bias is a useful framework
                     for examining OCR engines [<a class="ref" href="#alpertabrams2016">Alpert-Adams 2016</a>]. </div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">Perhaps the most significant challenge to studying OCR engines is that the
                     best-performing and most widely-used OCR engines are proprietary. Though ABBYY
                     FineReader and Google Cloud Vision API offer high performance, the systems fundamentally
                     are black boxes: we have no access to the underlying algorithms or the training data.
                     The ability to audit a system is crucial to developing an understanding of how it
                     works
                     and the biases it encodes. The fact that many OCR engines are opaque prevents us from
                     disentangling whether poor performance on a particular page is due to algorithmic
                     limitations or due to a lack of relevant training data. The distinction is significant:
                     the former may reflect an algorithmic upper bound, whereas the latter reflects decisions
                     made by humans.</div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">Indeed, algorithmic bias distorts and occludes the historical record, as it is made
                     discoverable through OCR. Discrepancies in OCR performance for different languages
                     and
                     scripts is a consequence of human prioritization, from the collection of training
                     data
                     and lexicons to the development of the algorithms themselves. As articulated by Hannah
                     Alpert-Abrams in “Machine Reading the <cite class="title italic">Primeros Libros</cite>,” 
                     “the machine-recognition of printed characters is a historically charged event, in
                     which the
                     system and its data conspire to embed cultural biases in the output, or to affix them
                     as
                     supplementary information hidden behind the screen” [<a class="ref" href="#alpertabrams2016">Alpert-Adams 2016</a>].
                     Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded
                     in colonial scripts perpetuate colonialism. For other languages such as Ladino,
                     typically typeset in Rashi script, the lack of high-performing OCR has presented
                     consistent challenges for digitization and scholarship.</div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">In the case of <cite class="title italic">Chronicling America</cite>, the National Digital
                     Newspaper Program is exemplary in its efforts to support OCR for non-English languages.
                     In the Notice of Funding Opportunity for the National Digital Newspaper Program produced
                     by the Division of Preservation of Access at the National Endowment for the Humanities,
                     OCR performance in different languages is explicitly addressed: “Applicants proposing to
                     digitize titles in languages other than English must include staff with the relevant
                     language expertise to review the quality of the converted content and related metadata”
                     [<a class="ref" href="#neh2020">NEH Division of Preservation and Access 2020</a>]. I have included this discussion of OCR
                     and algorithmic bias to offer a broader provocation regarding machine learning and
                     digitization: how much text in digitized sources has been transmuted by this effect
                     and
                     thus effectively erased due to inaccessibility when using search and discovery
                     platforms?</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">VI. The Visual Content Recognition Model</h1>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">I will now turn to the <cite class="title italic">Newspaper Navigator</cite> pipeline itself, in
                     particular the visual content recognition model. Trained on annotations from the <cite class="title italic">Beyond Words </cite>crowdsourcing initiative, as well as additional
                     annotations of headlines and advertisements, the visual content recognition model
                     detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and
                     advertisements on historic newspaper pages. </div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">As described in the previous section, examining training data is an essential component
                     of auditing any machine learning model, from understanding how the dataset was
                     constructed to uncovering any biases in the composition of the dataset itself. For
                     the
                     visual content recognition model, this examination begins with <cite class="title italic">Beyond
                        Words</cite>. Launched in 2017 by LC Labs, <cite class="title italic">Beyond Words </cite>has
                     collected to-date over 10,000 verified annotations of visual content in World War
                     1-era
                     newspaper pages from <cite class="title italic">Chronicling America</cite>. The <cite class="title italic">Beyond Words </cite>workflow consists of the three steps listed below:</div>
                  
                  <div class="ptext">
                     <ol class="list">
                        <li class="item">
                           <div class="ptext">A “Mark” step, in which volunteers are asked to draw bounding boxes around visual
                              content on the page [<a class="ref" href="#lclabs2017a">LC Labs 2017a</a>]. The instructions read as follows:</div>
                           
                           
                           <div class="ptext">
                              <blockquote>
                                 <p>In the Mark step, your task is to identify and select pictures in newspaper pages.
                                    For
                                    our project, “pictures” means illustrations, photographs, comics, and cartoons. You'll
                                    use the marking tool to draw a box around the picture using your mouse. After you
                                    have
                                    marked all pictures on the newspaper page, click the ‘DONE’ button. Skip the page
                                    altogether by clicking the “Skip this page” button. If no illustrations, photographs, or
                                    cartoons appear on the page, click the “DONE” button. Not sure if a picture should be
                                    marked? Select the “Done for now, more left to mark” button so another volunteer can
                                    help finish that page. Please do not select pictures within advertisements.</p>
                              </blockquote>
                           </div>
                        </li>
                        <li class="item">
                           <div class="ptext">A “Transcribe” step, in which volunteers are asked to transcribe the caption of
                              the highlighted visual content, as well as note the artist and visual content
                              category (“Photograph,” “Illustration,” “Map,” “Comics/Cartoon,” “Editorial Cartoon”)
                              [<a class="ref" href="#lclabs2017b">LC Labs 2017b</a>]. The transcription is pre-populated with the OCR falling within the
                              bounding box in question. The instructions for this step state:</div>
                           
                           
                           <div class="ptext">
                              <blockquote>
                                 <p>Most pictures have captions or descriptions. Enter the text exactly as you see it.
                                    Include capitalization and punctuation, but remove hyphenation that breaks words at
                                    the
                                    end of the line. Use new lines to separate different parts of captions and descriptions.
                                    You can zoom in for better looks at the page. You can also select “View the original
                                    page” in the upper right corner of the screen to view the original high resolution image
                                    of the newspaper.</p>
                              </blockquote>
                           </div>
                           
                           
                           <div class="ptext">An example of this step can be seen in Figure 4.</div>
                           </li>
                        <li class="item">
                           <div class="ptext">A “Verify” step, in which volunteers are asked to select the best caption for an
                              identified region of visual content from at least two examples; alternatively, a
                              volunteer can add another caption [<a class="ref" href="#lclabs2017c">LC Labs 2017c</a>]. The instructions state:</div>
                           
                           
                           <div class="ptext"> 
                              <blockquote>
                                 <p>Choose the transcription that most accurately captures the text as written. If
                                    multiple transcriptions appear valid, choose the first one. If the selected region
                                    isn't
                                    appropriate for the prompt, click “Bad region”.</p>
                              </blockquote>
                           </div>
                           </li>
                     </ol>
                  </div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image4.png" rel="external"><img src="resources/images/image4.png" style="" alt="screenshot of a newspaper article and a text box where the article has been transcribed" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 4. </div>A screenshot showing an example of the “Transcribe” step of the <cite class="title italic">Beyond Words </cite>workflow. Note that the photograph caption is
                        pre-populated using the OCR falling within the bounding box [<a class="ref" href="#lclabs2017b">LC Labs 2017b</a>].</div>
                  </div>
                  
                  
                  
                  
                  <div class="counter"><a href="#p44">44</a></div>
                  <div class="ptext" id="p44">For the purposes of <cite class="title italic">Newspaper Navigator</cite>, only the bounding boxes
                     from the “Mark” step and the category labels from the “Transcribe” step were utilized as
                     training data; however, understanding the full workflow is essential because annotations
                     are considered “verified” only if they have passed through the full workflow.</div>
                  
                  <div class="counter"><a href="#p45">45</a></div>
                  <div class="ptext" id="p45">A number of factors contribute to which <cite class="title italic">Chronicling America </cite>pages
                     were processed by volunteers in <cite class="title italic">Beyond Words</cite>. First, the temporal
                     restriction to World War 1-era pages affects the ability of the visual content
                     recognition model to generalize: after all, if the model is trained on World War 1-era
                     pages, how well should we expect it to perform on 19th century pages? I will return
                     to
                     this question later in the section. Moreover, <cite class="title italic">Beyond Words
                        </cite>volunteers could select either an entirely random page or a random page from a
                     specific state, an important affordance from an engagement perspective, as volunteers
                     could explore the local histories of states in which they are interested. But this
                     affordance is also imprinted on the training data, as certain states - and thus, certain
                     newspapers - appear at a higher frequency than if the World War-1 era <cite class="title italic">Chronicling America</cite> pages had been drawn randomly from this temporal range in
                     <cite class="title italic">Chronicling America</cite>.</div>
                  
                  <div class="counter"><a href="#p46">46</a></div>
                  <div class="ptext" id="p46">Furthermore, it should be noted that the “Mark” and “Transcribe” steps - specifically,
                     drawing bounding boxes and labeling the visual content category - are complex tasks.
                     Because newspaper pages are remarkably heterogenous, ambiguities and edge-cases abound.
                     Should a photo collage be marked as one unit or segmented into constituent parts?
                     What
                     precisely is the distinction between an editorial cartoon and an illustration? How
                     much
                     relevant textual content should be included in a bounding box? Naturally, volunteers
                     did
                     not always agree on these choices. In this regard, the notion of a ground-truth, a
                     set
                     of perfect annotations against which we can assess performance, is itself called into
                     question. Moreover, with thousands of annotations, mistakes in the form of missed
                     visual
                     content, as well as misclassifications, are inevitable.<a class="noteRef" href="#d4e1099">[14]</a> These ambiguities
                     and errors are natural components of <em class="emph">any</em> training dataset and
                     must be taken into account when analyzing a machine learning model’s predictions.</div>
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">A breakdown of <cite class="title italic">Beyond Words</cite> annotations included in the training
                     data can be found in the second column of Table 1. I downloaded these 6,732
                     publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals
                     an
                     imbalance between the number of examples for each category; in the language of machine
                     learning, this is called <em class="emph">class imbalance</em>. While the discrepancy
                     between maps and photographs is to be expected, the fact that so few maps were included
                     was concerning from a machine learning standpoint: a machine learning algorithm’s
                     ability to generalize to new data is dependent on having many diverse training examples.
                     To address this concern, I searched <cite class="title italic">Chronicling America </cite>and
                     identified 134 pages published between January 1st, 1914, and December 31st, 1918,
                     that
                     contain maps. I then annotated these pages myself.</div>
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">In addition, during the development of the <cite class="title italic">Newspaper Navigator</cite>
                     pipeline, I realized the value in training the visual content recognition model to
                     identify headlines and advertisements. Consequently, I added annotations of headlines
                     and advertisements for all 3,559 pages included in the training data. The statistics
                     for
                     this augmented set of annotations can be found in the third column of Table 1. Though
                     I
                     attempted to use a consistent approach to annotating the headlines and advertisements,
                     my interpretation of what constitutes a headline is certainly not unimpeachable: I
                     am
                     not a trained scholar of periodicals or of print culture; even if I were, the task
                     itself is inevitably subjective. Furthermore, I made decisions to annotate large grids
                     of classified ads as a single ad to expedite the annotation process. Whether this
                     was a
                     correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines
                     and advertisements required a significant amount of time, and there are inevitably
                     mistakes and inconsistencies embedded within the annotations. My own decisions in
                     terms
                     of how to annotate, as well as my mistakes and inconsistencies, are embedded within
                     the
                     visual content recognition model through training. For those interested in examining
                     the
                     training data directly, the data can be found in the GitHub repository for this project
                     [<a class="ref" href="#lee2020a">Lee 2020</a>].</div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Category</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Beyond Words Annotations</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Total Annotations</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Photograph</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">4,193</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">4,254</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Illustration</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1,028</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1,048</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Map</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">79</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">215</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Comic/Cartoon</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1,139</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1,150</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Editorial Cartoon</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">293</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">293</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Headline</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">27,868</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Advertisement</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">13,581</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              <cite class="title italic">Total</cite>
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">6,732</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">48,409</td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 1. </div>A breakdown of <cite class="title italic">Beyond Words </cite>annotations included in the
                        training data for the visual content recognition model, as well as all annotations
                        constituting the training data.</div>
                  </div> 
                  
                  
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">Beyond the construction of the training data, I made manifold decisions regarding
                     the
                     selection of the correct model architecture and the training of the model. Because
                     this
                     discussion surrounding these choices is quite technical, I refer the reader to [<a class="ref" href="#lee2020b">Lee et al. 2020</a>]
                     for an in-depth examination. However, I will state
                     that the choice of model, the number of iterations for which the model was trained,
                     and
                     the choice of model parameters are all of significant import for the resulting trained
                     model and consequently, the <cite class="title italic">Newspaper Navigator </cite>dataset.</div>
                  
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">I will now turn to the visual content recognition model’s outputs in relation to the
                     <cite class="title italic">Newspaper Navigator </cite>pipeline. The model itself consumes a
                     lower-resolution version of a <cite class="title italic">Chronicling America </cite>page as input
                     and then outputs a JSON file containing predictions, each of which consists of bounding
                     box coordinates,<a class="noteRef" href="#d4e1265">[15]</a> the predicted
                     class (i.e., “photograph”, “map”, etc.), and a confidence score generated by the machine
                     learning model.<a class="noteRef" href="#d4e1272">[16]</a> Cropping out and saving the visual content required extra code to be
                     written. Because the high-resolution images of the <cite class="title italic">Chronicling
                        America</cite> pages, in addition to the METS/ALTO OCR, amount to many tens of
                     terabytes of data, questions of data storage became major considerations in the
                     pipeline. I chose to save the extracted visual content as lower-resolution JPEG images
                     in order to reduce the upload time and lessen the storage burden. Though the <cite class="title italic">Newspaper Navigator</cite> dataset retains identifiers to all
                     high-resolution pages in <cite class="title italic">Chronicling America, </cite>the images in the
                     <cite class="title italic">Newspaper Navigator </cite>dataset are altered by the downsampling
                     procedure. This downsampling procedure should be free of any significant biasing
                     effects. </div>
                  
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">For visual content recognition, “Newspaper Navigator” utilized an object detection model,
                     which is a type of widely-used computer vision technique for identifying objects in
                     images. The performance for computer vision techniques is regularly measured using
                     metrics such as average precision. For “Newspaper Navigator”, the model’s performance on a
                     specific page, as measured by average precision, is dependent on a confluence of
                     factors. These factors include the page’s layout, artifacts and distortions introduced
                     in the microfilming and digitization process, and - most importantly - the composition
                     of the training data. Thus, each image is “seen” differently by the visual content
                     recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified
                     by the visual content recognition model and saved in the <cite class="title italic">Newspaper
                        Navigator </cite>dataset. Each image is cropped slightly differently. In the case of
                     the image from the <cite class="title italic">Iowa State Bystander</cite>, extra text is included,
                     while in the case of the images from <cite class="title italic">The Broad Ax</cite>, the captions
                     are partially cut off. The loss in image quality is due to the aforementioned
                     downsampling performed by the pipeline. This downsampling leads to artifacts such
                     as the
                     dots appearing on Du Bois’s face in the image from the <cite class="title italic">Iowa State
                        Bystander</cite>, as well as the streaks in the image from <cite class="title italic">Franklin’s
                        Paper the Statesman</cite>, that are not present in Figure 2.</div>
                  
                  <div class="counter"><a href="#p52">52</a></div>
                  <div class="ptext" id="p52">Returning to the question of the visual content recognition model’s performance on
                     pages
                     published outside of the temporal range of the training data (1914-1918), it is possible
                     to provide a quantitative answer by measuring average precision on test sets of
                     annotated pages from different periods of time. In [<a class="ref" href="#lee2020b">Lee et al. 2020</a>], I describe this
                     analysis in detail and demonstrate that the performance declines for pages published
                     between 1875 and 1900 and further declines for pages published between 1850 and 1875.
                     This confirms that the composition of the training data directly manifests in the
                     model’s performance. While it is certainly the case that the <cite class="title italic">Newspaper
                        Navigator</cite> dataset can still be used for scholarship related to 19th century
                     newspapers in <cite class="title italic">Chronicling America</cite>, any scholarship with the 19th
                     century visual content in the <cite class="title italic">Newspaper Navigator </cite>dataset must
                     consider how the dataset may skew what visual content is represented.</div>
                  
                  
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image5.jpg" rel="external"><img src="resources/images/image5.jpg" style="" alt="four images of W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 5. </div>The four images of W.E.B. Du Bois, as identified by the visual content
                        recognition model and included in the <cite class="title italic">Newspaper Navigator </cite>dataset
                        [<a class="ref" href="#navigator1910a">Newspaper Navigator 1910a</a>]; [<a class="ref" href="#navigator1910c">Newspaper Navigator 1910c</a>]; [<a class="ref" href="#navigator1910e">Newspaper Navigator 1910e</a>];
                        [<a class="ref" href="#navigator1910g">Newspaper Navigator 1910g</a>].</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p53">53</a></div>
                  <div class="ptext" id="p53">Let me conclude this section with a discussion of the act of visual content extraction
                     itself in relation to digitization. While this extraction enables a wide range of
                     affordances for searching <cite class="title italic">Chronicling America</cite>, it is also an act
                     of decontextualization: visual content no longer appears in relation to the <cite class="title italic">mise-en-page</cite>. In the Appendix, the full pages containing the
                     photographs of W.E.B. Du Bois are reproduced, showing each photograph in context.
                     Only
                     by examining the full pages does it become clear that the article featuring W.E.B.
                     Du
                     Bois was printed with a second article in the <cite class="title italic">Iowa State
                        Bystander</cite> and <cite class="title italic">The Broad Ax</cite>, the headline of which reads:
                     “ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active
                     Campaign Against Injustice.” Furthermore, upon examination, the <cite class="title italic">Iowa
                        State Bystander </cite>front page features the article on <cite class="title italic">The
                        Crisis</cite> and W.E.B. Du Bois as the most prominent article of the issue. Though
                     links between the extracted visual content and the original <cite class="title italic">Chronicling America</cite> pages are always retained, this decontextualization
                     inevitably transmutes <em class="emph">how</em> we perceive and interact with the
                     visual content in <cite class="title italic">Chronicling America</cite>. Indeed, all uses of
                     machine learning for metadata enhancement are a form of decontextualization, centering
                     the user’s discovery and analysis of content around the metadata itself. 
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  
                  <h1 class="head">VII. Prediction Uncertainty</h1>
                  
                  <div class="counter"><a href="#p54">54</a></div>
                  <div class="ptext" id="p54">Perhaps the most fundamental question to ask of the <cite class="title italic">Newspaper
                        Navigator </cite>dataset is: “How many photographs does the dataset contain?” Because
                     the dataset has been constructed using a machine learning model, predictions are
                     ultimately probabilistic in nature, quantified by the confidence score returned by
                     the
                     model. This begs the question of what counts as an identified unit of visual content:
                     a
                     user is much more inclined to tally a prediction of a map if it has an associated
                     confidence score of 99% rather than 1%. However, choosing this cut is fundamentally
                     a
                     subjective decision, informed by the user’s end goals with the dataset. In the language
                     of machine learning, picking a stringent confidence cut (i.e., only counting predictions
                     with high confidence scores) emphasizes <em class="emph">precision</em>: a prediction
                     of a photograph likely corresponds to a true photograph, but the predictions will
                     suffer
                     from false negatives. Conversely, picking a loose confidence cut (i.e., counting
                     predictions with low confidence scores) emphasizes <em class="emph">recall</em>: most
                     true photographs are identified as such, but the predictions will suffer from many
                     false
                     positives. In this regard, the total number of images in the <cite class="title italic">Newspaper
                        Navigator </cite>dataset is dependent on one’s desired tradeoff between precision and
                     recall. In Table 2, I show the dynamic range of the dataset size, as induced by three
                     different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects
                     of
                     different cuts on confidence score for the page featuring W.E.B. Du Bois in the November
                     26,1910, issue of <cite class="title italic">The Broad Ax</cite>.</div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Category</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">≥ 90%</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">≥ 70%</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">≥ 50%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Photograph</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.59 x 10<span class="hi superscript">6</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2.63 x 10<span class="hi superscript">6</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">3.29 x 10<span class="hi superscript">6</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Illustration</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">8.15 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2.52 x 10<span class="hi superscript">6</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">4.36 x 10<span class="hi superscript">6</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Map</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2.07 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">4.59 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">7.54 x 10<span class="hi superscript">5</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Comic/Cartoon</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">5.35 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.23 x 10<span class="hi superscript">6</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2.06 x 10<span class="hi superscript">6</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Editorial Cartoon</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2.09 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">6.67 x 10<span class="hi superscript">5</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.27 x 10<span class="hi superscript">6</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Headline</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">3.44 x 10<span class="hi superscript">7</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">5.37 x 10<span class="hi superscript">7</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">6.95 x 10<span class="hi superscript">7</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Advertisement</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">6.42 x 10<span class="hi superscript">7</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">9.48 x 10<span class="hi superscript">7</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.17 x 10<span class="hi superscript">8</span></td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              <em class="emph">Total</em>
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.02 x 10<span class="hi superscript">8</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.56 x 10<span class="hi superscript">8</span></td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.98 x 10<span class="hi superscript">8</span></td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 2. </div>The number of occurrences of each category of visual content in the <cite class="title italic">Newspaper Navigator</cite> dataset with confidence scores above the
                        listed thresholds (0.9, 0.7, 0.5).</div>
                  </div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image6.jpg" rel="external"><img src="resources/images/image6.jpg" style="" alt="screenshot of four newspaper pages" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 6. </div>The same page of <cite class="title italic">The Broad Ax </cite>from November 26, 1910,
                        along with predictions from the visual content recognition model, thresholded on
                        confidence score at 5%, 50%, 70%, and 90% [<a class="ref" href="#navigator1910g">Newspaper Navigator 1910g</a>]; [<a class="ref" href="#navigator1910h">Newspaper Navigator 1910h</a>].
                        Note that red corresponds to a prediction of “photograph”, cyan
                        corresponds to a prediction of “headline”, and blue corresponds to a prediction of
                        “advertisement”.</div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p55">55</a></div>
                  <div class="ptext" id="p55">Rather than pre-selecting a confidence score threshold, the <cite class="title italic">Newspaper
                        Navigator </cite>dataset contains all predictions with confidence scores greater than
                     5%,<a class="noteRef" href="#d4e1630">[17]</a>
                     allowing the user to define their own confidence cut when querying the dataset. However,
                     the website for the <cite class="title italic">Newspaper Navigator</cite> dataset also includes
                     hundreds of pre-packaged datasets in order to make it easier for users to work with
                     the
                     dataset. In particular, users can download zip files containing all of the visual
                     content of a specific type with confidence scores greater than or equal to 90%, for
                     any
                     year from 1850 to 1963. I made this choice of 90% as the threshold cut for these
                     pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged
                     datasets by eye. However, as articulated above, based on different use cases, this
                     cut
                     of 90% may be too restrictive or permissive: relevant visual content may be absent
                     from
                     the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the
                     visual content recognition model’s confidence scores for the four images of W.E.B.
                     Du
                     Bois described throughout this data archaeology. The effect of a cut on confidence
                     score
                     can be seen here: selecting a cut of 95% would exclude the image from Franklin’s Paper the Statesman. I raise this point to emphasize that even this
                     seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery
                     process and thus can have an impact on scholarship. </div>
                  
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image8.jpg" rel="external"><img src="resources/images/image8.jpg" style="" alt="Four images of W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 7. </div>The visual content recognition model’s confidence score for each of the four
                        images of W.E.B. Du Bois. Note how the model assigns a different confidence score
                        to
                        each identified image [<a class="ref" href="#navigator1910b">Newspaper Navigator 1910b</a>]; [<a class="ref" href="#navigator1910d">Newspaper Navigator 1910d</a>];
                        [<a class="ref" href="#navigator1910f">Newspaper Navigator 1910f</a>]; [<a class="ref" href="#navigator1910h">Newspaper Navigator 1910h</a>].</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p56">56</a></div>
                  <div class="ptext" id="p56">Just as the bounding box predictions themselves are affected by the training data,
                     as
                     well as newspaper page layout, date of publication, and noise from the digitization
                     pipeline, so too are the confidence scores. In particular, the visual content
                     recognition model suffers from high-confidence misclassifications, for example,
                     crossword puzzles that are identified as maps with confidence scores greater than
                     90%.
                     High-confidence misclassifications pose challenges for machine learning writ large,
                     and
                     the field of explainable artificial intelligence is largely devoted to developing
                     tools
                     for understanding this type of misclassification [<a class="ref" href="#weld2019">Weld and Bansal 2019</a>]. However, these
                     high-confidence misclassifications can often be traced back to the composition of
                     the
                     training set. For example, the fact that the visual content recognition model sometimes
                     identifies crossword puzzles as maps with high confidence is likely due to the fact
                     that
                     the training data did not contain enough labeled examples of maps and crossword puzzles
                     for the visual content recognition model to differentiate them with high accuracy.
                     </div>
                  
                  <div class="counter"><a href="#p57">57</a></div>
                  <div class="ptext" id="p57">The questions surrounding confidence scores and probabilistic descriptions of items
                     is
                     by no means restricted to the <cite class="title italic">Newspaper Navigator </cite>dataset. I echo
                     Thomas Padilla’s assertion that “attempts to use algorithmic methods to describe
                     collections must embrace the reality that, like human descriptions of collections,
                     machine descriptions come with varying measure of certainty” [<a class="ref" href="#padilla2019">Padilla 2019</a>].
                     Machine-generated metadata such as OCR are also fundamentally probabilistic in nature;
                     this fact is not immediately apparent to end users of cultural heritage collections
                     because cuts on confidence score are typically chosen before surfacing the metadata.
                     Effectively communicating confidence scores, probabilistic descriptions, and the
                     decisions surrounding them to end users remains a challenge for content stewards.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">VIII. OCR Extraction</h1>
                  
                  <div class="counter"><a href="#p58">58</a></div>
                  <div class="ptext" id="p58">In the <cite class="title italic">Newspaper Navigator </cite>pipeline, a textual description of
                     each prediction is obtained by extracting the OCR within each predicted bounding box.
                     The resulting textual description is thus dependent on not only the OCR provided by
                     <cite class="title italic">Chronicling America </cite>but also the exact coordinates of the bounding
                     box: if the coordinates of a word in the localized OCR extend beyond the bounds of
                     the
                     box, the word is excluded. I experimented with utilizing tolerance limits to allow
                     words
                     that extend just beyond the bounds of the boxes to be included, but doing so ultimately
                     introduces false positives as well, as words from neighboring articles or visual content
                     were inevitably included some fraction of the time. Once again, the tradeoff between
                     false positives and false negatives is manifest.</div>
                  
                  <div class="counter"><a href="#p59">59</a></div>
                  <div class="ptext" id="p59">In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois,
                     as
                     identified by the <cite class="title italic">Newspaper Navigator</cite> pipeline. Significantly, in
                     the <cite class="title italic">Newspaper Navigator </cite>dataset, the OCR is stored as a list of
                     words, with line breaks removed; these lists are what appear in Figure 8. These four
                     examples provide intuition as to how the captions are altered. While the examples
                     from
                     the <cite class="title italic">Iowa State Bystander </cite>and <cite class="title italic">Franklin’s Paper
                        the Statesman</cite> both have very similar captions as shown in Figure 3, the captions for
                     both of the examples from <cite class="title italic">The Broad Ax</cite> are unrecognizable.
                     Because the bounding boxes have clipped the caption, none of the characters from the
                     proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR
                     noise due to the OCR engine attempting to read text from the photographs. Consequently,
                     the mentions of W.E.B. Du Bois are erased from the textual descriptions in the <cite class="title italic">Newspaper Navigator </cite>dataset. The visual content in the <cite class="title italic">Newspaper Navigator </cite>dataset is thus decontextualized not only in
                     the sense that the visual content is extracted from the newspaper pages but also in
                     the
                     sense that the OCR extraction method further alters the textual descriptions. While
                     the
                     images from the <cite class="title italic">Iowa State Bystander</cite> and <cite class="title italic">Franklin’s Paper the Statesman</cite> are still recoverable with fuzzy keyword search,
                     the two images from <cite class="title italic">The Broad Ax </cite>are impossible to retrieve with
                     <cite class="title italic">any </cite>form of keyword search, revealing another instance in
                     which employing automated techniques for collections processing affects
                     discoverability.</div>
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image9.jpg" rel="external"><img src="resources/images/image9.jpg" style="" alt="four images of W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 8. </div>The textual descriptions of each image, as extracted from the OCR and saved in
                        the <cite class="title italic">Newspaper Navigator </cite>dataset [<a class="ref" href="#navigator1910b">Newspaper Navigator 1910b</a>];
                        [<a class="ref" href="#navigator1910d">Newspaper Navigator 1910d</a>]; [<a class="ref" href="#navigator1910f">Newspaper Navigator 1910f</a>]; [<a class="ref" href="#navigator1910h">Newspaper Navigator 1910h</a>].</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p60">60</a></div>
                  <div class="ptext" id="p60">Fortunately, visual content can still be recovered using similarity search over the
                     images themselves; these methods are discussed in detail in the next section. However,
                     in the case of headlines, the errors introduced by OCR engines and the subsequent
                     OCR
                     extraction have no recourse, as similarity search for images of headlines would only
                     capture similar typography and text layout.<a class="noteRef" href="#d4e1752">[18]</a>
                     </div>
                  
                  <div class="counter"><a href="#p61">61</a></div>
                  <div class="ptext" id="p61">To illustrate the effects of this OCR extraction on headlines, I reproduce in Table
                     3
                     the extracted OCR as it appears in the <cite class="title italic">Newspaper Navigator
                        </cite>dataset for Franklin F. Johnson’s headline: </div>
                  
                  <div class="counter"><a href="#p62">62</a></div>
                  <div class="ptext" id="p62">NEW MOVEMENT </div>
                  
                  <div class="counter"><a href="#p63">63</a></div>
                  <div class="ptext" id="p63">BEGINS WORK</div>
                  
                  <div class="counter"><a href="#p64">64</a></div>
                  <div class="ptext" id="p64">Plan and Scope of the Asso-</div>
                  
                  <div class="counter"><a href="#p65">65</a></div>
                  <div class="ptext" id="p65">ciation Briefly Told. </div>
                  
                  <div class="counter"><a href="#p66">66</a></div>
                  <div class="ptext" id="p66">Will Publish the Crisis. </div>
                  
                  <div class="counter"><a href="#p67">67</a></div>
                  <div class="ptext" id="p67">Review of Causes Which Led to the </div>
                  
                  <div class="counter"><a href="#p68">68</a></div>
                  <div class="ptext" id="p68">Organization of the Association in </div>
                  
                  <div class="counter"><a href="#p69">69</a></div>
                  <div class="ptext" id="p69">New York and What Its Policy Will </div>
                  
                  <div class="counter"><a href="#p70">70</a></div>
                  <div class="ptext" id="p70">Be-Career and Work of Professor </div>
                  
                  <div class="counter"><a href="#p71">71</a></div>
                  <div class="ptext" id="p71">W.E.B. Du Bois</div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">
                              
                              <cite class="title italic">Iowa State Bystander </cite>
                              
                              (14 Oct. 1910)
                              </td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">
                              
                              <cite class="title italic">Franklin’s Paper the Statesman</cite>
                              
                              (15 Oct. 1910)
                              </td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">
                              
                              <cite class="title italic">The Broad Ax </cite>
                              
                              (15 Oct. 1910)
                              </td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">
                              
                              <cite class="title italic">The Broad Ax </cite>
                              
                              (26 Nov. 1910)
                              </td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">98.72%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">99.57%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">99.76%</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">99.70%</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              [“NEW ”, “MOVEMENT ”, “BEGINS ”, “WORK ”, “and ”, “Plan ”, “Scope ”, “of ”, “the ”,
                              “Asso\u00ad ”, “ciation ”, “Briefly ”, “Told. ”, “WILL ”, “PUBLISH ”, “THE ”,
                              “CRISIS. ”, “Review ”, “of ”, “Causae ”, “Which ”, “Lad ”, “to ”, “the ”,
                              “Organisation ”, “of ”, “the ”, “Auooiation ”, “In ”, “Naw ”, “York ”, “and ”, “JWhat ”,
                              “It* ”, “Polioy ”, “Will ”, “Ba\u2014Career ”, “and ”, “Wark ”, “of ”,
                              “Profeasor”]
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              [“NEW ”, “MOVEMENT ”, “BEGINS ”, “WORK ”, “Plan ”, “and ”, “Scope ”, “of ”, “the ”,
                              “Asso ”, “ciation ”, “Briefly ”, “Told. ”, “WILL ”, “PUBLISH ”, “THE ”, “CRISIS.”]
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              [“NEW ”, “MOVEMENT ”, “BEGINS ”, “WORK ”, “Plan ”, “and ”, “Sep ”, “if ”, “the ”,
                              “Asso ”, “ciation ”, “Briefly ”, “Told. ”, “WILL ”, “PUBLISH ”, “THE ”, “CRISIS, ”,
                              “Be ”, “Career ”, “nnd ”, “Work ”, “of ”, “Professor ”, “W. ”, “E. ”, “B. ”, “Du ”,
                              “Bois. ”, “Review ”, “of ”, “Causes ”, “Which ”, “Led ”, “to ”, “the ”, “Oraanteallon ”,
                              “of ”, “th. ”, “A.Me!.!?n ”, “i ”, “i ”, “New ”, “York ”, “and ”, “What ”, “IU ”,
                              “Policy ”, “Will”]
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              [“NEW ”, “MOVEMENT ”, “BEGINS ”, “WORK ”, “Plan ”, “and ”, “Scope ”, “of ”, “the ”,
                              “Asso ”, “ciation ”, “Briefly ”, “Told. ”, “WILL ”, “PUBLISH ”, “THE ”, “CRISIS. ”,
                              “Review ”, “of ”, “Causes ”, “Which ”, “Lad ”, “to ”, “tha ”, “Organization ”, “of ”,
                              “the\" ”, “Association ”, “In ”, “New ”, “York ”, “and ”, “What ”, “Its ”, “Policy ”,
                              “Will”]
                              </td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 3. </div>Table 3. The extracted OCR associated with each of the four photographs of W.E.B.
                        Du
                        Bois [<a class="ref" href="#navigator1910b">Newspaper Navigator 1910b</a>]; [<a class="ref" href="#navigator1910d">Newspaper Navigator 1910d</a>]; [<a class="ref" href="#navigator1910f">Newspaper Navigator 1910f</a>]; [<a class="ref" href="#navigator1910h">Newspaper Navigator 1910h</a>].</div>
                  </div>
                  
                  
                  
                  
                  <div class="counter"><a href="#p72">72</a></div>
                  <div class="ptext" id="p72">The full pages are reproduced in the appendix for reference. Notably, all four extracted
                     headlines contain OCR errors, as well as missing words due to the OCR extraction.
                     The
                     visual content recognition model consistently fails to include the last line of the
                     headline, “W.E.B. Du Bois,” revealing another case in which Du Bois’s name is rendered
                     inaccessible by keyword search in the <cite class="title italic">Newspaper Navigator
                        </cite>dataset. </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">IX. Image Embeddings</h1>
                  
                  <div class="counter"><a href="#p73">73</a></div>
                  <div class="ptext" id="p73">An <em class="term">image embedding</em> canonically refers to a low-dimensional
                     representation of an image, often a list of a few hundred or a few thousand numbers,
                     that captures much of the image’s semantic content. Image embeddings are typically
                     generated by feeding an image into a pre-trained neural image classification model
                     (i.e., a model that takes in an image and outputs a label of “dog” or “cat”) and
                     extracting a representation of the image from one of the model’s hidden layers, often
                     the penultimate layer.<a class="noteRef" href="#d4e2197">[19]</a> Image embeddings are valuable for three reasons:</div>
                  
                  <div class="ptext">
                     <ol class="list">
                        <li class="item">Image embeddings are remarkably adept at capturing semantic similarity between
                           images. For example, images of dogs tend to be clustered together in embedding space,
                           with images of bicycles in another cluster and images of buildings in yet another.
                           These clusters can be fine-grained: sometimes, the red bicycles are grouped closer
                           together than the blue bicycles.</li>
                        <li class="item">Image embeddings can be constructed by feeding images into an image classification
                           model already trained on another dataset (such as ImageNet), meaning that generating
                           image embeddings is a useful method for comparing images without having to construct
                           training data by labeling images. </li>
                        <li class="item">Image embeddings are low-dimensional and thus much smaller in size than the images
                           themselves (i.e., on the order of kilobytes instead of megabytes). As a result, image
                           embeddings are much less computationally expensive to compare to one another when
                           conducting similarity search, clustering, or related tasks. In short, image
                           embeddings speed up image comparison.</li>
                     </ol>
                  </div>
                  
                  <div class="counter"><a href="#p74">74</a></div>
                  <div class="ptext" id="p74">Utilizing image embeddings to visualize and explore large collections of images has
                     become an increasingly common approach among cultural heritage practitioners. Projects
                     and institutions that have utilized image embeddings for visualizing cultural heritage
                     collections include the Yale Digital Humanities Lab’s PixPlot interface [<a class="ref" href="#yale2017">Yale Digital Humanities Lab 2017</a>],
                     the National Neighbors project [<a class="ref" href="#lincoln2019">Lincoln et al. 2019</a>], Google Arts
                     and Culture [<a class="ref" href="#googlearts2018">Google Arts and Culture 2018</a>], The Norwegian National Museum’s Principal
                     Components project [<a class="ref" href="#nasjonalmuseet2017">Nasjonalmuseet 2017</a>], the State Library of New South Wales’s Aero
                     Project [<a class="ref" href="#geraldo2020">Geraldo 2020</a>], the Royal Photographic Society [<a class="ref" href="#vane2018">Vane 2018</a>], The American Museum
                     of Natural History [<a class="ref" href="#foo2019">Foo 2019</a>], and The National Library of the Netherlands [<a class="ref" href="#lonij2017">Lonij and Weavers 2017</a>];
                     [<a class="ref" href="#weavers2020">Weavers and Smits 2020</a>]. These visualizations provide insights into
                     broader themes in the collections, thereby allowing curators, researchers, and the
                     public to explore collections at a scale previously only possible by organizing images
                     by color or other low-level features.<a class="noteRef" href="#d4e2230">[20]</a> In this regard, image
                     embeddings provide new affordances for searching over images that complement canonical
                     faceted and keyword search.</div>
                  
                  <div class="counter"><a href="#p75">75</a></div>
                  <div class="ptext" id="p75">Because these image embeddings enable these visualization approaches and open the
                     door
                     to similarity search and recommendation, I opted to include image embeddings as part
                     of
                     the <cite class="title italic">Newspaper Navigator</cite> pipeline. Indeed, these image embeddings
                     power the similarity search functionality in the <cite class="title italic">Newspaper
                        Navigator</cite> user interface and, in this regard, are crucial to the broader vision
                     of the project [<a class="ref" href="#lee2020c">Lee and Weld 2020</a>].<a class="noteRef" href="#d4e2246">[21]</a> To generate the
                     embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep
                     learning architecture for image classification, both of which had already been
                     pre-trained on ImageNet [<a class="ref" href="#he2016">He et al. 2016</a>]. </div>
                  
                  <div class="counter"><a href="#p76">76</a></div>
                  <div class="ptext" id="p76">ImageNet is perhaps the most well-known image dataset in the history of machine
                     learning. Constructed by scraping publicly available images from the internet and
                     recruiting Amazon Mechanical Turk workers to annotate the images, ImageNet contains
                     approximately 14 million images across 20,000 categories [<a class="ref" href="#deng2009">Deng et al. 2009</a>]; [<a class="ref" href="#imagenet2020a">ImageNet 2020</a>].
                     Kate Crawford and Trevor Paglen’s essay “Excavating AI: The Politics of Images in
                     Machine Learning Training Sets” offers a history and incisive critique of the
                     classification schema of ImageNet; here, I will summarize the most salient critiques.
                     First, many of the categories in the taxonomy utilized are themselves marginalizing
                     [<a class="ref" href="#crawford2019">Crawford and Paglen 2019</a>]. Though many of the classes relating to people were removed
                     in 2019, ImageNet had previously bifurcated the “Natural Object <span class="monospace">&gt;</span> Body <span class="monospace">&gt;</span> Adult
                     Body” category into “Male Body” and “Female Body” subcategories. Second, ethnic classes
                     were included, implying that 1) classification into rigid categories of ethnicity
                     is
                     possible and appropriate and 2) a machine learning system could learn how to classify
                     ethnicity from these images. Diving deeper, the classifications become horrifying
                     in
                     their supposed granularity: until 2019, an image of a woman in a bikini was accompanied
                     with the tags “slattern, slut, slovenly woman, trollop” [<a class="ref" href="#crawford2019">Crawford and Paglen 2019</a>].
                     Though many embedding models are pre-trained on subsets of ImageNet categories included
                     in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly
                     troubling classifications, these classifications nonetheless necessitate a reckoning
                     with our use of ImageNet writ large, especially in regard to how the semantics of
                     ImageNet are projected onto any image embedding generated with such a model [<a class="ref" href="#russakovsky2015">Russakovsky et al. 2015</a>].
                     <a class="noteRef" href="#d4e2286">[22]</a>
                     </div>
                  
                  <div class="counter"><a href="#p77">77</a></div>
                  <div class="ptext" id="p77">However, questions probing the data in ImageNet fail to critique the ethically
                     questionable practices on which ImageNet is built. Though the researchers responsible
                     for the dataset scraped all 14 million images from public URLs, ImageNet does not
                     provide any guarantees on image copyright, as only the URLs are provided in the
                     database: “The images in their original resolutions may be subject to copyright, so we
                     do not make them publicly available on our server” [<a class="ref" href="#imagenet2020b">ImageNet: What about the Images? 2020</a>].
                     It is highly unlikely that a photographer with an image in the dataset could have
                     known that a photograph could be used this way, much less actively consent to the
                     image’s inclusion, as is the case with subjects in the photographs. Furthermore, the
                     labels themselves were collected using Amazon’s Mechanical Turk platform, which has
                     been
                     repeatedly criticized for its exploitative labor practices: as of 2017, workers earned
                     a
                     median wage of approximately $2 an hour on the platform [<a class="ref" href="#haro2018">Haro et al. 2018</a>]. Scholars
                     including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced
                     marginalized labor underpins digitization efforts, and the reliance on Mechanical
                     Turk
                     for the production of ImageNet further entrenches the digitization and discovery process
                     within a system of labor exploitation [<a class="ref" href="#cecire2011">Cecire 2011</a>]; [<a class="ref" href="#mak2017">Mak 2017</a>]; [<a class="ref" href="#fyfe2016">Fyfe 2016</a>]. As
                     cultural heritage practitioners and humanities researchers, we must acknowledge these
                     exploitative practices, and we must reckon with how we perpetuate them through the
                     use
                     of ImageNet as a training source for image search and discovery. </div>
                  
                  <div class="counter"><a href="#p78">78</a></div>
                  <div class="ptext" id="p78">In offering these critiques, my intention is not to dismiss ImageNet in a wholesale
                     manner. Certainly, the benefits of utilizing ImageNet are manifold, as evidenced by
                     widespread community adoption, as well as new affordances for searching cultural
                     heritage collections enabled by the dataset that are shaping the contours of digital
                     scholarship. In the case of my own scholarship with Newspaper Navigator, I have elected
                     to utilize machine learning models pre-trained on ImageNet precisely for these reasons.
                     I offer these provocations instead to question how we can do better as a community,
                     not
                     only in imagining alternatives but in bringing them to fruition. Classification is
                     an
                     act of interpretive reduction, whether by human or machine, and thus manifests all
                     too
                     often as an act of oppression.<a class="noteRef" href="#d4e2312">[23]</a> And yet, the structure imposed by classification constitutes the
                     very basis for search and discovery systems. The salient question is thus not how
                     we
                     dispense of these systems but rather how we progressively realize a more inclusive
                     vision of these systems, from the labor practices behind their construction to the
                     very
                     classification taxonomies themselves.</div>
                  
                  <div class="counter"><a href="#p79">79</a></div>
                  <div class="ptext" id="p79">How, then, do image embeddings derived from ImageNet mediate our interactions with
                     the
                     photographs in <cite class="title italic">Newspaper Navigator</cite>? Figure 9 shows a
                     visualization of 1,000 photographs from the <cite class="title italic">Newspaper Navigator
                        </cite>dataset published during the year 1910. This visualization was created using the
                     ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known
                     as
                     T-SNE [<a class="ref" href="#vandermaaten2009">Van der Maaten and Hinton 2009</a>]. With T-SNE, a cluster of photographs indicates
                     that the photographs are likely semantically similar, but the size of the cluster
                     and
                     distances from other clusters bear no meaning [<a class="ref" href="#wattenberg2016">Wattenberg, Viégas, and Johnson 2016</a>].
                     With this in mind, we can examine the clusters. Despite the fact that the high-contrast,
                     grayscale photographs in <cite class="title italic">Newspaper Navigator</cite> are markedly
                     different, or “out-of-sample,” in comparison to the clear, color images in ImageNet, the
                     clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering
                     of photographs depicting crowds of people, as well as photographs depicting ships
                     and
                     the sea. This visualization technique with the image embeddings is thus powerful in
                     helping to navigate large collections of photographs by their semantic content.</div>
                  
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image10.jpg" rel="external"><img src="resources/images/image10.jpg" style="" alt="image of a network graph" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 9. </div>A visualization of 1,000 photographs from the year 1910 in the <cite class="title italic">Newspaper Navigator </cite>dataset, generated using the <cite class="title italic">Newspaper Navigator</cite> ResNet-50 image embeddings.</div>
                  </div>
                  
                  
                  
                  <div class="counter"><a href="#p80">80</a></div>
                  <div class="ptext" id="p80">What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters
                     containing these four photographs. This visualization affords us a lens into the
                     limitations of image embeddings. First, it is evident that image embeddings are directly
                     impacted by the distortions of the digitization process: while the three photographs
                     from <cite class="title italic">Franklin’s Paper the Statesman </cite>and <cite class="title italic">The
                        Broad Ax</cite> are clustered together with other portraits, the photograph from the
                     <cite class="title italic">Iowa State Bystander</cite> is located in an entirely different
                     cluster - a consequence of the fact that the <cite class="title italic">Iowa State Bystander</cite>
                     photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably,
                     neighboring photographs suffer from similar distortions). A search engine powered
                     with
                     these image embeddings would in all likelihood return the three photographs from <cite class="title italic">Franklin’s Paper the Statesman</cite> and <cite class="title italic">The Broad
                        Ax</cite> together, but the fourth photograph would effectively be lost. This
                     algorithmic mediation is particularly troubling because, as described in Section IV,
                     the
                     microfilming digitization process causes newspaper photographs of darker-skinned people
                     to lose contrast. While this loss in image quality is marginalizing in its own right,
                     image embeddings perpetuate this marginalization: digitized newspaper portraits of
                     darker-skinned individuals are more likely to suffer from saturated facial features,
                     in
                     turn resulting in these photographs being lost during the discovery and retrieval
                     process, as is the case with the saturated <cite class="title italic">Iowa State Bystander
                        </cite>photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image
                     embeddings are particularly relevant in the case of <cite class="title italic">Newspaper
                        Navigator</cite>, as these image embeddings power the visual similarity search
                     affordance within the publicly-deployed <cite class="title italic">Newspaper Navigator
                        </cite>search application [<a class="ref" href="#lee2020c">Lee and Weld 2020</a>]. Though machine learning methods are often
                     offered as panaceas for automation, this algorithmic erasure reminds us that traditional
                     methods of scholarship and historiography, such as detailed analyses and close readings
                     of Black newspapers in <cite class="title italic">Chronicling America</cite>, are more important
                     than ever to counter algorithmic bias.</div>
                  
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image11.jpg" rel="external"><img src="resources/images/image11.jpg" style="" alt="image of a network graph focused on W.E.B. Du Bois" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 10. </div>The same visualization as in Figure 9, this time showing the locations of the
                        four photographs of W.E.B. Du Bois.</div>
                  </div>
                  
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">X. Environmental Impact</h1>
                  
                  <div class="counter"><a href="#p81">81</a></div>
                  <div class="ptext" id="p81">Any examination of a dataset whose construction required large-scale computing would
                     be
                     remiss in not investigating the environmental impact of the computation itself. The
                     carbon emissions generated from training a state-of-the-art machine learning model
                     such
                     as BERT is comparable to a single flight across the United States; however, factoring
                     in
                     experimentation and tuning, the carbon emissions can quickly amount to the carbon
                     emissions of a car over its entire lifetime, including fuel [<a class="ref" href="#strubell2019">Strubell et al. 2019</a>].
                     OpenAI’s GPT-3 model required several thousand petaflop/s-days to train; without
                     specific numbers, the carbon emissions are not possible to calculate exactly, but
                     they
                     are nonetheless substantial [<a class="ref" href="#brown2020">Brown et al. 2020</a>]. In response, machine learning
                     researchers have recommended ideas such as <em class="term">Green AI,</em> with the goal of encouraging the
                     community to value computational efficiency and not just accuracy [<a class="ref" href="#schwartz2019">Schwartz et al. 2019</a>].</div>
                  
                  <div class="counter"><a href="#p82">82</a></div>
                  <div class="ptext" id="p82">In the case of <cite class="title italic">Newspaper Navigator</cite>, most of the compute time was
                     devoted to processing all 16.3 million <cite class="title italic">Chronicling America</cite> pages
                     with the visual content recognition model, as opposed to training the model itself.
                     In
                     Tables 4 and 5, I report details on training the model and running the pipeline, as
                     well
                     as the carbon emissions generated by each step, computed using the Machine Learning
                     Impact Calculator [<a class="ref" href="#lacoste2019">Lacoste et al. 2019</a>]. In total, approximately 380 kg CO2 were emitted
                     during the construction of the <cite class="title italic">Newspaper Navigator </cite>dataset,
                     including development, experimentation, training, pipeline processing, and
                     post-processing. It should be noted that this number is an estimate, as the statistics
                     for experimentation and post-processing are difficult to quantify exactly. Nonetheless,
                     this is approximately equivalent to the carbon emissions incurred by a single person
                     flying from Washington, D.C. to Boston [<a class="ref" href="#carbon">Carbon Footprint Calculator no date</a>]. I include
                     these numbers in the hope that cultural heritage practitioners will consider the
                     environmental impact of utilizing machine learning and artificial intelligence for
                     digital content stewardship. Doing so is essential to the data archaeology: given
                     that
                     climate change will disproportionately affect cultural heritage institutions in regions
                     unable to develop proper infrastructure to withstand rapid temperature fluctuations
                     and
                     unprecedented flooding, even the environmental impacts of utilizing machine learning
                     within digital content stewardship has the capacity to contribute to erasure and
                     marginalization. 
                     </div>
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Activity</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1"># of NVIDIA T4 GPUs</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">GPU Hours (each)</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Carbon Emissions</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Training</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">19</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">0.96 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Pipeline Processing</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">8</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">456</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">144.56 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Experimentation for Training and Pipeline Processing (estimate)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">8</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">24</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">7.66 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              <em class="emph">Total</em>
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">153.18 kg CO2</td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 4. </div>Carbon emissions from the GPU usage for <cite class="title italic">Newspaper
                           Navigator</cite>, broken down by project component. Note that all computation was done
                        on Amazon AWS g4dn instances in the zone “us-east-2”. The carbon emissions were
                        calculated using the Machine Learning Impact Calculator [<a class="ref" href="#lacoste2019">Lacoste et al. 2019</a>].</div>
                  </div>
                  
                  
                  
                  <div class="table">
                     <table class="table">
                        <tr class="row label">
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Activity</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">CPU Processor (#)</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1"># Processor CPU Cores</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">CPU Hours (each)</td>
                           
                           <td valign="top" class="cell label" colspan="1" rowspan="1">Carbon Emissions</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Training</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">4 CPUs</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">19</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1.13 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Pipeline Processing</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">48 CPUs</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">456</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">181.9 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Experimentation for Training and Pipeline Processing (<em class="emph">estimate</em>)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">2</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">48 CPUs</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">24</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">9.57 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">Extra Computation (dataset post-processing, etc., <em class="emph">estimate</em>)</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">1</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">48 CPUs</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">168</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">33.52 kg CO2</td>
                           </tr>
                        <tr class="row">
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">
                              <em class="emph">Total</em>
                              </td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">-</td>
                           
                           <td valign="top" class="cell" colspan="1" rowspan="1">226.12 kg CO2</td>
                           </tr>
                     </table>
                     <div class="caption">
                        <div class="label">Table 5. </div>Carbon emissions from the CPU usage for <cite class="title italic">Newspaper
                           Navigator</cite>, broken down by project component. Note that all computation was done
                        on Amazon AWS g4dn instances in the zone “us-east-2”. The CPU processors are all 2nd
                        generation Intel Xeon Scalable Processors (Cascade Lake) [<a class="ref" href="#amazon2020">Amazon Web Services, Inc. 2020</a>].
                        The 48-core processor outputs approximately 350 W; the 4-core processor outputs
                        approximately 104 W [<a class="ref" href="#intel2020a">Intel 2020a</a>]; [<a class="ref" href="#intel2020b">Intel 2020b</a>]. The carbon emissions were calculated
                        using the Machine Learning Impact Calculator [<a class="ref" href="#lacoste2019">Lacoste et al. 2019</a>]. Note that the energy
                        consumption by RAM is not factored in, but it is insignificant in comparison to the
                        CPU
                        and GPU energy consumption.</div>
                  </div>
                  </div>
               
               <div class="div div0">
                  
                  
                  <h1 class="head"> XI. Conclusion</h1>
                  
                  <div class="counter"><a href="#p83">83</a></div>
                  <div class="ptext" id="p83">In this data archaeology, I have traced four <cite class="title italic">Chronicling America</cite>
                     pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through
                     the <cite class="title italic">Chronicling America</cite> and <cite class="title italic">Newspaper
                        Navigator</cite> pipelines. The excavated genealogy of digital artifacts has revealed
                     the imprintings of the complex interactions between humans and machines. Indeed, the
                     journey of each newspaper page through the <cite class="title italic">Chronicling America
                        </cite>and <cite class="title italic">Newspaper Navigator</cite> pipelines is one of refraction,
                     mediation, and decontextualization that is compounded upon with each step. Decisions
                     made decades ago when microfilming a newspaper page inevitably affect how the machine
                     learning models employed for OCR, visual content extraction, and image embedding
                     generation ultimately process the pages, render them as digital artifacts in the <cite class="title italic">Newspaper Navigator </cite>dataset, and mediate their
                     discoverability.</div>
                  
                  <div class="counter"><a href="#p84">84</a></div>
                  <div class="ptext" id="p84">As articulated by Trevor Owens in <cite class="title italic">The Theory and Craft of Digital
                        Preservation</cite>, machine learning and artificial intelligence are the “underlying
                     sciences for digital preservation” [<a class="ref" href="#owens2018">Owens 2018</a>]. Though machine learning techniques
                     provide us with new affordances for searching and studying cultural heritage materials,
                     they have the power to perpetuate and amplify the marginalization and erasure of entire
                     communities within the archive. This erasure, coupled with the labor practices involved
                     in creating training data as well as the environmental impact of training and deploying
                     machine learning models in large-scale digitization pipelines, necessitates that we
                     continue to examine the broader socio-technical ecosystems in which we participate.
                     In
                     doing so, we can work toward a more inclusive vision of the digital collection and
                     the
                     ways in which we render its contents discoverable.</div>
                  
                  <div class="counter"><a href="#p85">85</a></div>
                  <div class="ptext" id="p85">How, then, is <cite class="title italic">Newspaper Navigator</cite> situated within this vision? In
                     reimagining how we search over the visual content in <cite class="title italic">Chronicling
                        America</cite>, one explicit goal of the project is to engage the public with the rich
                     history preserved within historic American periodicals and thus build on <cite class="title italic">Chronicling America</cite> as a free-to-use, public domain resource for
                     scholars, educators, students, journalists, genealogists, and beyond [<a class="ref" href="#lee2021a">Lee, Berson, and Berson 2021</a>]. [<a class="ref" href="#lee2021b">Lee et al. 2021</a>].
                     With <cite class="title italic">Newspaper Navigator</cite>, it is
                     my belief that the new modes of interacting with <cite class="title italic">Chronicling
                        America</cite> have the capacity to not only enable a breadth of new scholarship but
                     also foster engagement in and reckoning with America’s multilayered history of
                     oppression. In documenting the different components of the project with this data
                     archaeology and corresponding technical paper [<a class="ref" href="#lee2020b">Lee et al. 2020</a>], as well as releasing
                     the full dataset and all code into the public domain, I have intended to be as
                     transparent as possible with the tools and methodologies employed. <cite class="title italic">Newspaper Navigator </cite>is not without its shortcomings, but my hope is that the
                     project contributes to this vision of the digital collection through transparency
                     and
                     inclusivity, as well as the scholarship and pedagogy that it has enabled.</div>
                  
                  <div class="counter"><a href="#p86">86</a></div>
                  <div class="ptext" id="p86">I offer this case study not only to contextualize the <cite class="title italic">Newspaper
                        Navigator </cite>dataset but also to advocate for the autoethnographic data archaeology
                     as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic
                     perspective. Though the digital humanities community has yet to adopt the data
                     archaeology as standard practice when creating and releasing cultural heritage datasets,
                     doing so has the capacity to improve accountability and context surrounding applications
                     of machine learning for both practitioners and end users. Given the manifold ways
                     in
                     which machine learning mediates access to the archive and perpetuates erasure,
                     reflecting critically on these systems is not only urgent but essential for transparency
                     and inclusivity. 
                     
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head"> Sources of Funding</h1>
                  
                  <div class="counter"><a href="#p87">87</a></div>
                  <div class="ptext" id="p87">This material is based upon work supported by the National Science Foundation Graduate
                     Research Fellowship under Grant DGE-1762114, as well as the Library of Congress
                     Innovator in Residence Position.</div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgments</h1>
                  
                  <div class="counter"><a href="#p88">88</a></div>
                  <div class="ptext" id="p88">I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter,
                     Robin
                     Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy
                     and
                     Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful
                     and
                     enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor,
                     Daniel Weld, at the University of Washington, for his support, guidance, and invaluable
                     advice with <cite class="title italic">Newspaper Navigator</cite>. In addition, I would like to
                     thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity
                     to formulate and write early sections of this data archaeology as part of this Spring’s
                     CSE 599: “Computing for Social Good” course. </div>
                  
                  <div class="counter"><a href="#p89">89</a></div>
                  <div class="ptext" id="p89">Lastly, I would like to thank the following people who have shaped <cite class="title italic">Newspaper Navigator</cite>: Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris
                     Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire
                     National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen
                     Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael
                     Haley
                     Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States
                     Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter
                     at Texas A&amp;M University-Corpus Christi; and Gabriel Pizzorno at Harvard University.
                     </div>
                  
                  </div>
               
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix:</h1>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image12.png" rel="external"><img src="resources/images/image12.png" style="" alt="screenshot of a newspaper page" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 11. </div><cite class="title italic"> Iowa state bystander</cite>. [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling America:
                        Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a></div>
                  </div>
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image13.png" rel="external"><img src="resources/images/image13.png" style="" alt="screenshot of a newspaper page" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 12. </div><cite class="title italic">Franklin's paper the statesman</cite>. (Denver, Colo.), 15 Oct. 1910. Chronicling America:
                        Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a></div>
                  </div>
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image14.png" rel="external"><img src="resources/images/image14.png" style="" alt="screenshot of a newspaper page" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 13. </div><cite class="title italic">The broad ax.</cite> [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling America:
                        Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a></div>
                  </div>
                  
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/image15.png" rel="external"><img src="resources/images/image15.png" style="" alt="screenshot of a newspaper page" /></a></div>
                     
                     
                     <div class="caption">
                        <div class="label">Figure 14. </div><cite class="title italic">The broad ax.</cite> [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling America:
                        Historic American Newspapers. Lib. of Congress. <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a></div>
                  </div>
                  
                  </div>
               
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e175"><span class="noteRef lang en">[1]  For a representative example of an
                     autoethnography, see [<a class="ref" href="#bailey2015">Bailey 2015</a>].</span></div>
               <div class="endnote" id="d4e283"><span class="noteRef lang en">[1]  The public search interface is available at:
                     <a href="https://chroniclingamerica.loc.gov/" onclick="window.open('https://chroniclingamerica.loc.gov/'); return false" class="ref">https://chroniclingamerica.loc.gov/</a>
                     </span></div>
               <div class="endnote" id="d4e319"><span class="noteRef lang en">[2]  For more information on the <cite class="title italic">Beyond Words</cite> workflow, see 
                     [<a class="ref" href="#lclabs">LC Labs no date</a>], as well as [<a class="ref" href="#lee2020b">Lee et al. 2020</a>].</span></div>
               <div class="endnote" id="d4e339"><span class="noteRef lang en">[3]  In particular, the
                     annotations were used to finetune an object detection model that had been pre-trained
                     on Common Objects in Context, a common dataset for benchmarking object detection
                     algorithms.</span></div>
               <div class="endnote" id="d4e344"><span class="noteRef lang en">[4]  A screenshot of the workflow can be found
                     later in this article in Figure 4. </span></div>
               <div class="endnote" id="d4e356"><span class="noteRef lang en">[5]  For those who are not familiar with
                     image embeddings, a detailed description is provided in Section IX.</span></div>
               <div class="endnote" id="d4e393"><span class="noteRef lang en">[6]  For the
                     dataset, see: <a href="https://news-navigator.labs.loc.gov" onclick="window.open('https://news-navigator.labs.loc.gov'); return false" class="ref">https://news-navigator.labs.loc.gov</a>; for the code, see <a href="https://github.com/LibraryOfCongress/newspaper-navigator" onclick="window.open('https://github.com/LibraryOfCongress/newspaper-navigator'); return false" class="ref">https://github.com/LibraryOfCongress/newspaper-navigator</a>.</span></div>
               <div class="endnote" id="d4e614"><span class="noteRef lang en">[7]  Indeed, compiling bibliographies of serials published after 1820
                     remains an immensely difficult task [<a class="ref" href="#hardy2019">Hardy and DiCuirci 2019</a>].</span></div>
               <div class="endnote" id="d4e619"><span class="noteRef lang en">[8]  The extent to
                     which newspaper microfilming was driven by credible fear of deterioration versus
                     other factors, such as microfilm marketing, is an important question that is rightly
                     debated. For more on this topic, see [<a class="ref" href="#baker2001">Baker 2001</a>]. </span></div>
               <div class="endnote" id="d4e631"><span class="noteRef lang en">[9]  For example, a 2017 article describing the West Virginia University
                     Libraries’ West Virginia &amp; Regional History Center and its participation in the
                     National Digital Newspaper Program states:“ By August 2017, all known issues of West
                     Virginia’s African-American newspapers from the 19th and early 20th centuries will
                     have been digitized ” [<a class="ref" href="#maxwell2017">Maxwell 2017</a>]. The article describes Curator Stewart Plein’s
                     efforts to locate surviving copies of three Black West Virginia newspapers in order
                     to digitize and include them in <cite class="title italic">Chronicling America</cite>.</span></div>
               <div class="endnote" id="d4e670"><span class="noteRef lang en">[10]  For a thorough case study of this process, I direct the
                     reader to “Qi-jtb the Raven,” in which Ryan Cordell walks through an example with the
                     Pennsylvania Digital Newspaper Program [<a class="ref" href="#cordell2017">Cordell 2017</a>]. </span></div>
               <div class="endnote" id="d4e833"><span class="noteRef lang en">[11] 
                     For exemplary research collaborations that utilize the <cite class="title italic">Chronicling
                        America </cite>bulk OCR, see the Viral Text Project and the Oceanic Exchanges
                     Project [<a class="ref" href="#cordell2017">Cordell 2017</a>]; [<a class="ref" href="#oceanic2017">Oceanic Exchanges Project 2017</a>]. </span></div>
               <div class="endnote" id="d4e865"><span class="noteRef lang en">[12]  For other examinations of how OCR
                     mediates our interactions with digital archives, see [<a class="ref" href="#hitchcock2013">Hitchcock 2013</a>]; [<a class="ref" href="#milligan2013">Milligan 2013</a>];
                     [<a class="ref" href="#strange2014">Strange et al. 2014</a>]; [<a class="ref" href="#traub2015">Traub, van Ossenbruggen, and Hardman 2015</a>]; [<a class="ref" href="#wright2019">Wright 2019</a>].</span></div>
               <div class="endnote" id="d4e899"><span class="noteRef lang en">[13]  For a concrete example of a similar
                     phenomenon in the image domain, see [<a class="ref" href="#lee2019">Lee 2019</a>], in which a machine learning algorithm
                     was trained to classify digitized images but consistently misclassified images that
                     had been misoriented 180 degrees in the scanning bed - a consequence of the
                     classifier not having seen enough instances of these misoriented scans during
                     training.</span></div>
               <div class="endnote" id="d4e1099"><span class="noteRef lang en">[14]  It should be noted that
                     <cite class="title italic">Beyond Words </cite>was introduced by LC Labs as an experiment,
                     with no interventions in workflow or community management.</span></div>
               <div class="endnote" id="d4e1265"><span class="noteRef lang en">[15]  Bounding box coordinates refer to the positions of the corners of
                     the predicted bounding box, relative to the image coordinates.</span></div>
               <div class="endnote" id="d4e1272"><span class="noteRef lang en">[16]  The confidence score is examined in more detail in the next
                     section.</span></div>
               <div class="endnote" id="d4e1630"><span class="noteRef lang en">[17]  This modest cut is provided to remove the large number of predictions with
                     confidence scores between 0% and 5%, which have high false-positive rates, and thus
                     reduce the size of the <cite class="title italic">Newspaper Navigator </cite>dataset.</span></div>
               <div class="endnote" id="d4e1752"><span class="noteRef lang en">[18]  The<cite class="title italic"> Newspaper
                        Navigator </cite>dataset does not retain the cropped images of headlines, as the
                     textual content is more salient than visual snippets in the case of headlines.</span></div>
               <div class="endnote" id="d4e2197"><span class="noteRef lang en">[19]  If these words are unfamiliar, the three takeaways listed
                     are more important.</span></div>
               <div class="endnote" id="d4e2230"><span class="noteRef lang en">[20]  For an introduction to some of these methods
                     with lower-level features, see [<a class="ref" href="#manovich2012">Manovich 2012</a>].</span></div>
               <div class="endnote" id="d4e2246"><span class="noteRef lang en">[21]  The search application can be found at: <a href="https://news-navigator.labs.loc.gov/search" onclick="window.open('https://news-navigator.labs.loc.gov/search'); return false" class="ref">https://news-navigator.labs.loc.gov/search</a>.</span></div>
               <div class="endnote" id="d4e2286"><span class="noteRef lang en">[22]  The specific categories used in the challenge can be found at: <a href="http://image-net.org/challenges/LSVRC/2010/browse-synsets" onclick="window.open('http://image-net.org/challenges/LSVRC/2010/browse-synsets'); return false" class="ref">http://image-net.org/challenges/LSVRC/2010/browse-synsets</a>.</span></div>
               <div class="endnote" id="d4e2312"><span class="noteRef lang en">[23]  For more reading on this topic, see [<a class="ref" href="#bowker2000">Bowker and Star 2000</a>].
                     </span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="alpertabrams2016"><!-- close -->Alpert-Adams 2016</span>  Alpert-Abrams, H.“Machine Reading the Primeros Libros,”<cite class="title italic">Digital Humanities Quarterly</cite> 10:4 (2016).</div>
               <div class="bibl"><span class="ref" id="amazon2020"><!-- close -->Amazon Web Services, Inc. 2020</span>  “Amazon EC2 Instance Types - Amazon Web Services,”
                  (2020) Amazon Web Services, Inc.<a href="https://aws.amazon.com/ec2/instance-types/" onclick="window.open('https://aws.amazon.com/ec2/instance-types/'); return false" class="ref"> Available at: </a>
                  <a href="https://aws.amazon.com/ec2/instance-types/" onclick="window.open('https://aws.amazon.com/ec2/instance-types/'); return false" class="ref">https://aws.amazon.com/ec2/instance-types/</a>. (Accessed: 5 June 2020).</div>
               <div class="bibl"><span class="ref" id="bailey2015"><!-- close -->Bailey 2015</span>  Bailey, M. “#transform(Ing)DH Writing and Research: An Autoethnography of
                  Digital Humanities and Feminist Ethics,” <cite class="title italic">Digital Humanities
                     Quarterly</cite> 9:2 (2015).</div>
               <div class="bibl"><span class="ref" id="baker2001"><!-- close -->Baker 2001</span>  Baker, N. <cite class="title italic">Double Fold: Libraries and the Assault on
                     Paper</cite>. Random House (2001).</div>
               <div class="bibl"><span class="ref" id="barrall2005"><!-- close -->Barrall and Guenther 2005</span>  Barrall, K. and Guenther, C. “Microfilm Selection for
                  Digitization,” (2005). Available at: <a href="https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf" onclick="window.open('https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf'); return false" class="ref">https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</a>.</div>
               <div class="bibl"><span class="ref" id="bender2018"><!-- close -->Bender and Friedman 2018</span>  Bender, E., and Friedman, B. “Data Statements for Natural
                  Language Processing: Toward Mitigating System Bias and Enabling Better Science.” <cite class="title italic">Transactions of the Association for Computational Linguistics</cite> 6
                  (2018): 587–604.
                  <a href="https://doi.org/10.1162/tacl_a_00041" onclick="window.open('https://doi.org/10.1162/tacl_a_00041'); return false" class="ref">https://doi.org/10.1162/tacl_a_00041</a> (Accessed 29 July 2021).</div>
               <div class="bibl"><span class="ref" id="bowker2000"><!-- close -->Bowker and Star 2000</span>  Bowker, G., and Star, S. <cite class="title italic">Sorting Things Out:
                     Classification and Its Consequences</cite>. MIT Press, Cambridge (2000).</div>
               <div class="bibl"><span class="ref" id="brown2020"><!-- close -->Brown et al. 2020</span> Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
                  A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
                  Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen,
                  M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
                  S., Radford, A., Sutskever, I., and Amodei D. “Language Models Are Few-Shot
                  Learners,”<cite class="title italic">ArXiv:2005.14165 [Cs]</cite> (2020),<a href="http://arxiv.org/abs/2005.14165" onclick="window.open('http://arxiv.org/abs/2005.14165'); return false" class="ref"> Available at: </a>
                  <a href="http://arxiv.org/abs/2005.14165" onclick="window.open('http://arxiv.org/abs/2005.14165'); return false" class="ref">http://arxiv.org/abs/2005.14165</a>
                  (Accessed: 6 June 2020).</div>
               <div class="bibl"><span class="ref" id="carbon"><!-- close -->Carbon Footprint Calculator no date</span>  “Carbon Footprint Calculator,” <a href="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3" onclick="window.open('https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3'); return false" class="ref">Available at: </a>
                  <a href="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3" onclick="window.open('https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3'); return false" class="ref">https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3</a>.
                  (Accessed: 6 June 2020).</div>
               <div class="bibl"><span class="ref" id="cecire2011"><!-- close -->Cecire 2011</span>  Cecire, N.“Works Cited: The Visible Hand,”<cite class="title italic"> Works
                     Cited</cite> (blog) (2011). Available at: <a href="http://nataliacecire.blogspot.com/2011/05/visible-hand.html" onclick="window.open('http://nataliacecire.blogspot.com/2011/05/visible-hand.html'); return false" class="ref">http://nataliacecire.blogspot.com/2011/05/visible-hand.html</a>.</div>
               <div class="bibl"><span class="ref" id="chronicling"><!-- close -->Chronicling America no date</span>  “Chronicling America | Library of Congress,” Available at:
                  <a href="https://chroniclingamerica.loc.gov/about/" onclick="window.open('https://chroniclingamerica.loc.gov/about/'); return false" class="ref">https://chroniclingamerica.loc.gov/about/</a> (Accessed 3 July 2020).</div>
               <div class="bibl"><span class="ref" id="cordell2017"><!-- close -->Cordell 2017</span>  Cordell, R. “‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,” <cite class="title italic">Book History</cite> 20:1, pp. 188–225 (2017). Available at:
                  <a href="https://doi.org/10.1353/bh.2017.0006" onclick="window.open('https://doi.org/10.1353/bh.2017.0006'); return false" class="ref">https://doi.org/10.1353/bh.2017.0006</a>.</div>
               <div class="bibl"><span class="ref" id="cordell2020"><!-- close -->Cordell 2020</span>  Cordell, R. “Machine Learning + Libraries: A Report on the State of the
                  Field” (2020). Available at: <a href="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig" onclick="window.open('https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig'); return false" class="ref">https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</a>.</div>
               <div class="bibl"><span class="ref" id="cordellsmith2017"><!-- close -->Cordell and Smith 2017</span>  Cordell, R., and Smith, D. <cite class="title italic">Viral Texts:
                     Mapping Networks of Reprinting in 19th-Century Newspapers and Magazines</cite> (2017),
                  Available at: <a href="http://viraltexts.org/" onclick="window.open('http://viraltexts.org/'); return false" class="ref">http://viraltexts.org</a>.</div>
               <div class="bibl"><span class="ref" id="crawford2019"><!-- close -->Crawford and Paglen 2019</span>  Crawford, K., and Paglen, T. “Excavating AI: The Politics of
                  Training Sets for Machine Learning” (2019). Available at: <a href="https://excavating.ai" onclick="window.open('https://excavating.ai'); return false" class="ref">https://excavating.ai</a> (Accessed: 19 September
                  2019).</div>
               <div class="bibl"><span class="ref" id="deng2009"><!-- close -->Deng et al. 2009</span>  Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L.
                  “ImageNet: A Large-Scale Hierarchical Image Database,”in <cite class="title italic">2009 IEEE
                     Conference on Computer Vision and Pattern Recognition</cite> (2009), pp. 248–55,<a href="https://doi.org/10.1109/CVPR.2009.5206848" onclick="window.open('https://doi.org/10.1109/CVPR.2009.5206848'); return false" class="ref"> Available at: </a>
                  <a href="https://doi.org/10.1109/CVPR.2009.5206848" onclick="window.open('https://doi.org/10.1109/CVPR.2009.5206848'); return false" class="ref">https://doi.org/10.1109/CVPR.2009.5206848</a>.</div>
               <div class="bibl"><span class="ref" id="fagan2016"><!-- close -->Fagan 2016</span>  Fagan, B. “Chronicling White America.”American Periodicals: A Journal of
                  History &amp; Criticism 26:1, pp. 10-13 (2016). Available at: <a href="https://muse.jhu.edu/article/613375" onclick="window.open('https://muse.jhu.edu/article/613375'); return false" class="ref">https://www.muse.jhu.edu/article/613375</a>.</div>
               <div class="bibl"><span class="ref" id="farrar1998"><!-- close -->Farrar 1998</span>  Farrar, H. <cite class="title italic">The Baltimore Afro-American, 1892-1950</cite>.
                  Greenwood Publishing Group (1998). </div>
               <div class="bibl"><span class="ref" id="ferriter2017"><!-- close -->Ferriter 2017</span>  Ferriter, M. “Introducing Beyond Words | The Signal,” (2017). Available
                  at: <a href="https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/" onclick="window.open('https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/'); return false" class="ref">//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</a>. (Accessed: 13
                  July 2020).</div>
               <div class="bibl"><span class="ref" id="foo2019"><!-- close -->Foo 2019</span>  Foo, B. “AMNH Photographic Collection,” (2020). Available at: <a href="https://amnh-sciviz.github.io/image-collection/about.html" onclick="window.open('https://amnh-sciviz.github.io/image-collection/about.html'); return false" class="ref">https://amnh-sciviz.github.io/image-collection/about.html</a> (Accessed: 11 June
                  2020).</div>
               <div class="bibl"><span class="ref" id="franklin1910"><!-- close -->Franklin’s Paper the Statesman 1910</span>  Franklin's paper the statesman. (Denver, Colo.),
                  15 Oct. 1910. <cite class="title italic">Chronicling America: Historic American Newspapers</cite>. Library of Congress.
                  Available at: <a href="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</a>
                  </div>
               <div class="bibl"><span class="ref" id="fyfe2016"><!-- close -->Fyfe 2016</span>  Fyfe, P. “An Archaeology of Victorian Newspapers,” <cite class="title italic">Victorian Periodicals
                     Review</cite> 49:4, pp. 546–77 (2016). Available at: <a href="https://doi.org/10.1353/vpr.2016.0039" onclick="window.open('https://doi.org/10.1353/vpr.2016.0039'); return false" class="ref">https://doi.org/10.1353/vpr.2016.0039</a>.</div>
               <div class="bibl"><span class="ref" id="gebru2020"><!-- close -->Gebru et al. 2020</span>  Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H.,
                  and Crawford, K. “Datasheets for Datasets.”<cite class="title italic">ArXiv:1803.09010 [Cs]</cite>, March 19, 2020.
                  <a href="http://arxiv.org/abs/1803.09010" onclick="window.open('http://arxiv.org/abs/1803.09010'); return false" class="ref">http://arxiv.org/abs/1803.09010</a>
                  (Accessed: July 29 2021).</div>
               <div class="bibl"><span class="ref" id="geraldo2020"><!-- close -->Geraldo 2020</span>  Giraldo, M. “Building Aereo,” DX Lab | State Library of NSW (2020).
                  Available at: <a href="https://dxlab.sl.nsw.gov.au/blog/building-aereo" onclick="window.open('https://dxlab.sl.nsw.gov.au/blog/building-aereo'); return false" class="ref">https://dxlab.sl.nsw.gov.au/blog/building-aereo</a> (Accessed: 2 July 2020).</div>
               <div class="bibl"><span class="ref" id="googlearts2018"><!-- close -->Google Arts and Culture 2018</span>  “Google Arts &amp; Culture Experiments - t-SNE Map
                  Experiment” (2018). Available at:
                  <a href="https://artsexperiments.withgoogle.com/tsnemap/" onclick="window.open('https://artsexperiments.withgoogle.com/tsnemap/'); return false" class="ref">https://artsexperiments.withgoogle.com/tsnemap/</a> (Accessed: 11 June 2020).</div>
               <div class="bibl"><span class="ref" id="hardy2019"><!-- close -->Hardy and DiCuirci 2019</span>  Hardy, M., and DiCuirci, L. “Critical Cataloging and the
                  Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’”
                  Archive Journal, Available at: <a href="http://www.archivejournal.net/?p=8073" onclick="window.open('http://www.archivejournal.net/?p=8073'); return false" class="ref">http://www.archivejournal.net/?p=8073</a>.</div>
               <div class="bibl"><span class="ref" id="haro2018"><!-- close -->Haro et al. 2018</span>  Hara, K. et al.“A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,”in
                  <cite class="title italic">Proceedings of the 2018 CHI Conference on Human Factors in
                     Computing Systems</cite>, CHI ’18 (Montreal QC, Canada: Association for Computing
                  Machinery, 2018), pp. 1–14. Available at: <a href="https://doi.org/10.1145/3173574.3174023" onclick="window.open('https://doi.org/10.1145/3173574.3174023'); return false" class="ref">https://doi.org/10.1145/3173574.3174023</a>.</div>
               <div class="bibl"><span class="ref" id="he2016"><!-- close -->He et al. 2016</span>  He, K., Zhang, X., Ren, S., and Sun, J.“Deep Residual Learning for
                  Image Recognition,” in <cite class="title italic">2016 IEEE Conference on Computer Vision and
                     Pattern Recognition (CVPR)</cite>, 2016, pp. 770–78,<a href="https://doi.org/10.1109/CVPR.2016.90" onclick="window.open('https://doi.org/10.1109/CVPR.2016.90'); return false" class="ref"> Available at: </a>
                  <a href="https://doi.org/10.1109/CVPR.2016.90" onclick="window.open('https://doi.org/10.1109/CVPR.2016.90'); return false" class="ref">https://doi.org/10.1109/CVPR.2016.90</a>.</div>
               <div class="bibl"><span class="ref" id="hitchcock2013"><!-- close -->Hitchcock 2013</span>  Hitchcock, T. “Confronting the Digital,” <cite class="title italic">Cultural and
                     Social History</cite> 10:1. pp. 9–23 (2013). Available at:
                  <a href="https://doi.org/10.2752/147800413X13515292098070" onclick="window.open('https://doi.org/10.2752/147800413X13515292098070'); return false" class="ref">https://doi.org/10.2752/147800413X13515292098070</a>.</div>
               <div class="bibl"><span class="ref" id="holland2018"><!-- close -->Holland et al. 2018</span>  Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski,
                  K.“The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.”
                  <cite class="title italic">ArXiv:1805.03677 [Cs]</cite>, May 9, 2018.
                  <a href="http://arxiv.org/abs/1805.03677" onclick="window.open('http://arxiv.org/abs/1805.03677'); return false" class="ref">http://arxiv.org/abs/1805.03677</a>
                  (Accessed 29 July 2021).</div>
               <div class="bibl"><span class="ref" id="imagenet2020a"><!-- close -->ImageNet 2020</span>  “ImageNet, ”Available at:
                  <a href="http://image-net.org/index" onclick="window.open('http://image-net.org/index'); return false" class="ref">http://image-net.org/index</a> (Accessed: 8
                  June 2020).</div>
               <div class="bibl"><span class="ref" id="imagenet2020b"><!-- close -->ImageNet: What about the Images? 2020</span>  “What about the images?”Available at: <a href="http://image-net.org/download-faq" onclick="window.open('http://image-net.org/download-faq'); return false" class="ref">http://image-net.org/download-faq</a>
                  (Accessed: 8 June 2020).</div>
               <div class="bibl"><span class="ref" id="intel2020a"><!-- close -->Intel 2020a</span>  “Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product
                  Specifications,” Available at: <a href="https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html" onclick="window.open('https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html'); return false" class="ref">https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html</a>
                  (Accessed: 5 June 2020).</div>
               <div class="bibl"><span class="ref" id="intel2020b"><!-- close -->Intel 2020b</span>  “Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product
                  Specifications,” Available at: <a href="https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html" onclick="window.open('https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html'); return false" class="ref">https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html</a>
                  (Accessed: June 5, 2020).</div>
               <div class="bibl"><span class="ref" id="iowa1910"><!-- close -->Iowa State Bystander 1910</span>  Iowa state bystander. [volume] (Des Moines, Iowa), 14 Oct.
                  1910. <cite class="title italic">Chronicling America: Historic American Newspapers</cite>. Library of Congress. Available
                  at: <a href="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</a>
                  </div>
               <div class="bibl"><span class="ref" id="lclabs2017a"><!-- close -->LC Labs 2017a</span>  LC Labs, “Beyond Words: Mark”Available at: <a href="http://beyondwords.labs.loc.gov/#/mark" onclick="window.open('http://beyondwords.labs.loc.gov/#/mark'); return false" class="ref">http://beyondwords.labs.loc.gov/#/mark</a> (Accessed 5 June, 2020).</div>
               <div class="bibl"><span class="ref" id="lclabs2017b"><!-- close -->LC Labs 2017b</span>  LC Labs, “Beyond Words: Transcribe,” Available at: <a href="http://beyondwords.labs.loc.gov/#/transcribe" onclick="window.open('http://beyondwords.labs.loc.gov/#/transcribe'); return false" class="ref">http://beyondwords.labs.loc.gov/#/transcribe</a> (Accessed 5 June, 2020).</div>
               <div class="bibl"><span class="ref" id="lclabs2017c"><!-- close -->LC Labs 2017c</span>  LC Labs, “Beyond Words: Veriffy,” Available at: <a href="http://beyondwords.labs.loc.gov/#/verify" onclick="window.open('http://beyondwords.labs.loc.gov/#/verify'); return false" class="ref">http://beyondwords.labs.loc.gov/#/verify</a> (Accessed 5 June, 2020).</div>
               <div class="bibl"><span class="ref" id="lclabs2020"><!-- close -->LC Labs and Digital Strategy Directorate 2020</span>  LC Labs and Digital Strategy
                  Directorate, “Machine Learning + Libraries Summit Event Summary”(2020). Available at:
                  <a href="https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf" onclick="window.open('https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf'); return false" class="ref">https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</a>. </div>
               <div class="bibl"><span class="ref" id="lclabs"><!-- close -->LC Labs no date</span>  LC Labs, Beyond Words | Experiments. Available at: <a href="https://labs.loc.gov/work/experiments/beyond-words/" onclick="window.open('https://labs.loc.gov/work/experiments/beyond-words/'); return false" class="ref">https://labs.loc.gov/work/experiments/beyond-words/</a> (Accessed 5 June,
                  2020).</div>
               <div class="bibl"><span class="ref" id="lacoste2019"><!-- close -->Lacoste et al. 2019</span>  Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T.
                  “Quantifying the Carbon Emissions of Machine Learning,” <cite class="title italic">ArXiv:1910.09700 [Cs]</cite> (2019). Available at: <a href="http://arxiv.org/abs/1910.09700" onclick="window.open('http://arxiv.org/abs/1910.09700'); return false" class="ref">http://arxiv.org/abs/1910.09700</a>. </div>
               <div class="bibl"><span class="ref" id="lee2019"><!-- close -->Lee 2019</span>  Lee, B. “Machine Learning, Template Matching, and the International Tracing
                  Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards
                  from 40 Million Document Scans,” <cite class="title italic">Digital Scholarship in the
                     Humanities</cite> 34:3, pp. 513-535 (2019). <a href="https://doi.org/10.1093/llc/fqy063" onclick="window.open('https://doi.org/10.1093/llc/fqy063'); return false" class="ref">Available at: </a>
                  <a href="https://doi.org/10.1093/llc/fqy063" onclick="window.open('https://doi.org/10.1093/llc/fqy063'); return false" class="ref">https://doi.org/10.1093/llc/fqy063</a>.</div>
               <div class="bibl"><span class="ref" id="lee2020a"><!-- close -->Lee 2020</span>  Lee, B. <cite class="title italic">LibraryOfCongress/Newspaper-Navigator</cite>, GitHub
                  Repository ( Library of Congress, 2020).<a href="https://github.com/LibraryOfCongress/newspaper-navigator" onclick="window.open('https://github.com/LibraryOfCongress/newspaper-navigator'); return false" class="ref"> Available at: </a>
                  <a href="https://github.com/LibraryOfCongress/newspaper-navigator" onclick="window.open('https://github.com/LibraryOfCongress/newspaper-navigator'); return false" class="ref">https://github.com/LibraryOfCongress/newspaper-navigator</a>.</div>
               <div class="bibl"><span class="ref" id="lee2020c"><!-- close -->Lee and Weld 2020</span>  Lee, B., and Weld, D. “Newspaper Navigator: Open Faceted Search for
                  1.5 Million Images,” <a href="https://dl.acm.org/doi/proceedings/10.1145/3379350" onclick="window.open('https://dl.acm.org/doi/proceedings/10.1145/3379350'); return false" class="ref">UIST '20 Adjunct: Adjunct Publication of the 33rd Annual ACM Symposium on User
                     Interface Software and Technology</a>, pp. 120-122 (2020). Available at: <a href="https://doi.org/10.1145/3379350.3416143" onclick="window.open('https://doi.org/10.1145/3379350.3416143'); return false" class="ref">https://doi.org/10.1145/3379350.3416143</a>.</div>
               <div class="bibl"><span class="ref" id="lee2020b"><!-- close -->Lee et al. 2020</span>  Lee, B., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage,
                  N., Thomas, D., Zwaard, K., and Weld, D. “The Newspaper Navigator Dataset: Extracting
                  And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling
                  America,” <a href="https://dl.acm.org/doi/proceedings/10.1145/3340531" onclick="window.open('https://dl.acm.org/doi/proceedings/10.1145/3340531'); return false" class="ref">CIKM '20:
                     Proceedings of the 29th ACM International Conference on Information &amp; Knowledge
                     Management</a>
                  <cite class="title italic">, </cite>pp. 3055–3062 (2020). Available at:
                  <a href="https://doi.org/10.1145/3340531.3412767" onclick="window.open('https://doi.org/10.1145/3340531.3412767'); return false" class="ref">https://doi.org/10.1145/3340531.3412767</a>.</div>
               <div class="bibl"><span class="ref" id="lee2021b"><!-- close -->Lee et al. 2021</span>  Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A.
                  “Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,”<cite class="title italic">EuropeanaTech Insight</cite> 16 (2021). Available at: <a href="https://pro.europeana.eu/page/issue-16-newspapers" onclick="window.open('https://pro.europeana.eu/page/issue-16-newspapers'); return false" class="ref">https://pro.europeana.eu/page/issue-16-newspapers</a>.</div>
               <div class="bibl"><span class="ref" id="lee2021a"><!-- close -->Lee, Berson, and Berson 2021</span>  Lee, B., Berson, I., and Berson, M. “Machine Learning and
                  the Social Studies,” <cite class="title italic">Social Education</cite> 85:2, pp. 88-92 (2021).
                  Available at: <a href="https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies" onclick="window.open('https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies'); return false" class="ref">https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</a>.</div>
               <div class="bibl"><span class="ref" id="congress2019"><!-- close -->Library of Congress 2019</span>  “Digital Strategy | Library of Congress,” Library of Congress
                  (2019). Available at: <a href="https://www.loc.gov/digital-strategy/" onclick="window.open('https://www.loc.gov/digital-strategy/'); return false" class="ref">https://www.loc.gov/digital-strategy/</a> (Accessed: 30 May 2020).</div>
               <div class="bibl"><span class="ref" id="lincoln2019"><!-- close -->Lincoln et al. 2019</span>  Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019) “National
                  Neighbors: Distant Viewing the National Gallery of Art's Collection of Collections”
                  (2019) Available at: <a href="https://nga-neighbors.library.cmu.edu/" onclick="window.open('https://nga-neighbors.library.cmu.edu/'); return false" class="ref">https://nga-neighbors.library.cmu.edu</a>. (Accessed: 30 May 2020).</div>
               <div class="bibl"><span class="ref" id="lonij2017"><!-- close -->Lonij and Weavers 2017</span>  Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague
                  (2017). Available at: <a href="http://lab.kb.nl/tool/siamese" onclick="window.open('http://lab.kb.nl/tool/siamese'); return false" class="ref">http://lab.kb.nl/tool/siamese</a>.</div>
               <div class="bibl"><span class="ref" id="lorang2020"><!-- close -->Lorang et al 2020</span>  Lorang, E., Soh, L., Liu, Y., and Pack, C. “Digital Libraries,
                  Intelligent Data Analytics, and Augmented Description: A Demonstration Project” (2020).
                  Available at: <a href="https://digitalcommons.unl.edu/libraryscience/396/" onclick="window.open('https://digitalcommons.unl.edu/libraryscience/396/'); return false" class="ref">https://digitalcommons.unl.edu/libraryscience/396/</a>.</div>
               <div class="bibl"><span class="ref" id="mak2017"><!-- close -->Mak 2017</span>  Mak, B. “Archaeology of a Digitization,” <cite class="title italic">Journal of the
                     Association for Information Science and Technology</cite> 65:8, pp. 1515–26 (2014).
                  Available at: <a href="https://doi.org/10.1002/asi.23061" onclick="window.open('https://doi.org/10.1002/asi.23061'); return false" class="ref">https://doi.org/10.1002/asi.23061</a>.</div>
               <div class="bibl"><span class="ref" id="manovich2012"><!-- close -->Manovich 2012</span>  Manovich, L. “How to Compare One Million Images?,” in <cite class="title italic">Understanding Digital Humanities</cite>, ed. David M. Berry (London: Palgrave
                  Macmillan UK, 2012), pp. 249–78. Available at:
                  <a href="https://doi.org/10.1057/9780230371934_14" onclick="window.open('https://doi.org/10.1057/9780230371934_14'); return false" class="ref">https://doi.org/10.1057/9780230371934_14</a>.</div>
               <div class="bibl"><span class="ref" id="maxwell2017"><!-- close -->Maxwell 2017</span>  Maxwell, M. “WVU Today | WVRHC Seeking Copies of Rare African-American
                  Newspapers” (2017). Available at: <a href="https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers" onclick="window.open('https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers'); return false" class="ref">https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</a>.
                  (Accessed 11 July 2020).</div>
               <div class="bibl"><span class="ref" id="mears2014"><!-- close -->Mears 2014</span>  Mears, J. <cite class="title italic">National Digital Newspaper Program Impact Study
                     2004-2014</cite>, National Endowment for the Humanities (2014). Available at: <a href="https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study" onclick="window.open('https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study'); return false" class="ref">https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</a>.
                  (Accessed 29 May 2020).</div>
               <div class="bibl"><span class="ref" id="meta"><!-- close -->Meta | Morphosis no date</span>  “Meta | Morphosis: Tutorials,” National Digital Newspaper
                  Program and the University of Kentucky Libraries. Available at: <a href="https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html" onclick="window.open('https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html'); return false" class="ref">https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html</a> (Accessed 3
                  July 2020).</div>
               <div class="bibl"><span class="ref" id="milligan2013"><!-- close -->Milligan 2013</span>  Milligan, I. “Illusionary Order: Online Databases, Optical Character
                  Recognition, and Canadian History, 1997–2010,” <cite class="title italic">Canadian Historical
                     Review</cite> 94:4, pp. 540–69 (2013). Available at: <a href="https://doi.org/10.3138/chr.694" onclick="window.open('https://doi.org/10.3138/chr.694'); return false" class="ref">https://doi.org/10.3138/chr.694</a>.</div>
               <div class="bibl"><span class="ref" id="mitchell2019"><!-- close -->Mitchell et al. 2019</span>  Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L.,
                  Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T. “Model Cards for Model Reporting.”
                  <cite class="title italic">Proceedings of the Conference on Fairness, Accountability, and
                     Transparency</cite>, January 29, 2019, 220–29.
                  <a href="https://doi.org/10.1145/3287560.3287596" onclick="window.open('https://doi.org/10.1145/3287560.3287596'); return false" class="ref">https://doi.org/10.1145/3287560.3287596</a>.</div>
               <div class="bibl"><span class="ref" id="neh2020"><!-- close -->NEH Division of Preservation and Access 2020</span>  Division of Preservation and Access
                  (NEH), “Notice of Funding Opportunity, National Digital Newspaper Program” (2020).
                  Available at: <a href="https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf" onclick="window.open('https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf'); return false" class="ref">https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</a>.
                  (Accessed 28 June 2020).</div>
               <div class="bibl"><span class="ref" id="nasjonalmuseet2017"><!-- close -->Nasjonalmuseet 2017</span>  “Project: «Principal Components»,” Nasjonalmuseet (2018).
                  Available at: <a href="https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/" onclick="window.open('https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/'); return false" class="ref">https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management —
                     -behind-the-scenes/digital-collection-management/project-principal-components/</a>
                  (Accessed 11 June 2020).</div>
               <div class="bibl"><span class="ref" id="national2019"><!-- close -->National Digital Newspaper Program 2019</span>  “About the Program - National Digital
                  Newspaper Program (Library of Congress),” (2019). Available at: <a href="https://www.loc.gov/ndnp/about.html" onclick="window.open('https://www.loc.gov/ndnp/about.html'); return false" class="ref">https://www.loc.gov/ndnp/about.html</a> (Accessed 3 July
                  2020).
                  </div>
               <div class="bibl"><span class="ref" id="national2020"><!-- close -->National Digital Newspaper Program 2020</span>  The National Digital Newspaper Program (NDNP)
                  Technical Guidelines for Applicants 2020-22 Awards (2020). Available at: <a href="https://www.loc.gov/ndnp/guidelines/" onclick="window.open('https://www.loc.gov/ndnp/guidelines/'); return false" class="ref">https://www.loc.gov/ndnp/guidelines/</a> (Accessed 28 June 2020).</div>
               <div class="bibl"><span class="ref" id="national"><!-- close -->National Digital Newspaper Program no date</span>  “Content Selection - National Digital
                  Newspaper Program (Library of Congress)”(2020). Available at: <a href="https://www.loc.gov/ndnp/guidelines/selection.html" onclick="window.open('https://www.loc.gov/ndnp/guidelines/selection.html'); return false" class="ref">https://www.loc.gov/ndnp/guidelines/selection.html</a> (Accessed 3 July
                  2020).
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910a"><!-- close -->Newspaper Navigator 1910a</span>  Image of W.E.B. Du Bois from the <cite class="title italic">Iowa
                     State Bystander</cite> (14 October 1910). From the Library of Congress, Newspaper
                  Navigator dataset: Extracted Visual Content from Chronicling America. Available at:
                  
                  <a href="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg" onclick="window.open('https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg</a>.</div>
               <div class="bibl"><span class="ref" id="navigator1910b"><!-- close -->Newspaper Navigator 1910b</span> [Newspaper Navigator 1910b] <cite class="title italic">Newspaper Navigator </cite> metadata for the
                  <cite class="title italic">Iowa State Bystander</cite> (14 October 1910). From the Library of
                  Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling
                  America. Available at: 
                  <a href="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json" onclick="window.open('https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</a>.</div>
               <div class="bibl"><span class="ref" id="navigator1910c"><!-- close -->Newspaper Navigator 1910c</span>  Image of W.E.B. Du Bois from<cite class="title italic"> Franklin’s
                     Paper the Statesman</cite> (15 October 1910). From the Library of Congress, Newspaper
                  Navigator dataset: Extracted Visual Content from Chronicling America. Available at:
                  <a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg" onclick="window.open('https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg'); return false" class="ref">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg</a>
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910d"><!-- close -->Newspaper Navigator 1910d</span>  <cite class="title italic">Newspaper Navigator </cite> metadata for <cite class="title italic">Franklin’s Paper the Statesman</cite> (15 October 1910). From the Library
                  of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling
                  America. Available at: <a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json" onclick="window.open('https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json'); return false" class="ref">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272</a>
                  <a href="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json" onclick="window.open('https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json'); return false" class="ref">.json</a>
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910e"><!-- close -->Newspaper Navigator 1910e</span>  Image of W.E.B. Du Bois from <cite class="title italic">The Broad Ax
                     </cite> (15 October 1910). From the Library of Congress, Newspaper Navigator dataset:
                  Extracted Visual Content from Chronicling America. Available at: <a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg" onclick="window.open('https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg</a>
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910f"><!-- close -->Newspaper Navigator 1910f</span>  <cite class="title italic">Newspaper Navigator </cite>metadata for <cite class="title italic">The Broad Ax </cite>(15 October 1910). From the Library of Congress,
                  Newspaper Navigator dataset: Extracted Visual Content from Chronicling America.
                  Available at: <a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json" onclick="window.open('https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json</a>
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910g"><!-- close -->Newspaper Navigator 1910g</span>  Image of W.E.B. Du Bois from The Broad Ax (26 November
                  1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual
                  Content from Chronicling America. Available at: <a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg" onclick="window.open('https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg</a>
                  </div>
               <div class="bibl"><span class="ref" id="navigator1910h"><!-- close -->Newspaper Navigator 1910h</span>  <cite class="title italic">Newspaper Navigator </cite>metadata for The
                  Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset:
                  Extracted Visual Content from Chronicling America. Available at: <a href="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json" onclick="window.open('https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json'); return false" class="ref">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json</a>
                  </div>
               <div class="bibl"><span class="ref" id="noble2018"><!-- close -->Noble 2018</span>  Noble, S. <cite class="title italic">Algorithms of Oppression: How Search Engines
                     Reinforce Racism</cite>. NYU Press, New York (2018).</div>
               <div class="bibl"><span class="ref" id="oceanic2017"><!-- close -->Oceanic Exchanges Project 2017</span>  Oceanic Exchanges Project Team. Oceanic Exchanges:
                  Tracing Global Information Networks In Historical Newspaper Repositories, 1840-1914
                  (2017). Available at: 10.17605/OSF.IO/WA94S.</div>
               <div class="bibl"><span class="ref" id="owens2018"><!-- close -->Owens 2018</span>  Owens, T. <cite class="title italic">The Theory and Craft of Digital Preservation</cite>. Johns Hopkins
                  University Press, Baltimore (2018).</div>
               <div class="bibl"><span class="ref" id="owens2020"><!-- close -->Owens and Padilla 2020</span>  Owens, T., and Padilla, T. “Digital Sources and Digital
                  Archives: Historical Evidence in the Digital Age,” <cite class="title italic">International
                     Journal of Digital Humanities </cite>(2020). Available at:<a href="https://doi.org/10.1007/s42803-020-00028-7" onclick="window.open('https://doi.org/10.1007/s42803-020-00028-7'); return false" class="ref">https://doi.org/10.1007/s42803-020-00028-7</a>
                  <a href="https://doi.org/10.1007/s42803-020-00028-7" onclick="window.open('https://doi.org/10.1007/s42803-020-00028-7'); return false" class="ref">https://doi.org/10.1007/s42803-020-00028-7</a>. </div>
               <div class="bibl"><span class="ref" id="padilla2019"><!-- close -->Padilla 2019</span>  Padilla, T. <cite class="title italic">Responsible Operations: Data Science,
                     Machine Learning, and AI in Libraries</cite> (2019). Available at: <a href="https://doi.org/10.25333/xk7z-9g97" onclick="window.open('https://doi.org/10.25333/xk7z-9g97'); return false" class="ref">https://doi.org/10.25333/xk7z-9g97</a>.</div>
               <div class="bibl"><span class="ref" id="reidsma2019"><!-- close -->Reidsma 2019</span>  Reidsma, M. <cite class="title italic">Masked by Trust: Bias in Library
                     Discovery.</cite> Litwin Books, Sacramento (2019).</div>
               <div class="bibl"><span class="ref" id="reisman2018"><!-- close -->Reisman et al. 2018</span>  Reisman, D., Schultz, J., Crawford, K., Whittaker, M. <cite class="title italic">Algorithmic Impact Assessments: A Practical Framework for Public Agency
                     Accountability</cite> (2018). Available at: <a href="https://ainowinstitute.org/aiareport2018.pdf" onclick="window.open('https://ainowinstitute.org/aiareport2018.pdf'); return false" class="ref">https://ainowinstitute.org/aiareport2018.pdf</a>.</div>
               <div class="bibl"><span class="ref" id="russakovsky2015"><!-- close -->Russakovsky et al. 2015</span>  Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
                  Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei,
                  L. “ImageNet Large Scale Visual Recognition Challenge,”<cite class="title italic">International
                     Journal of Computer Vision</cite> 115:3, pp. 211-252 (2015). Available at:<a href="https://doi.org/10.1007/s11263-015-0816-y" onclick="window.open('https://doi.org/10.1007/s11263-015-0816-y'); return false" class="ref">https://doi.org/10.1007/s11263-015-0816-y</a>
                  <a href="https://doi.org/10.1007/s11263-015-0816-y" onclick="window.open('https://doi.org/10.1007/s11263-015-0816-y'); return false" class="ref">https://doi.org/10.1007/s11263-015-0816-y</a>. </div>
               <div class="bibl"><span class="ref" id="schwartz2019"><!-- close -->Schwartz et al. 2019</span>  Schwartz, R., Dodge, J., Smith, N., and Etzioni, O. “Green AI,”
                  ArXiv:1907.10597 [Cs, Stat], (2019). Available at:<a href="http://arxiv.org/abs/1907.10597" onclick="window.open('http://arxiv.org/abs/1907.10597'); return false" class="ref">http://arxiv.org/abs/1907.10597</a>
                  <a href="http://arxiv.org/abs/1907.10597" onclick="window.open('http://arxiv.org/abs/1907.10597'); return false" class="ref">http://arxiv.org/abs/1907.10597</a>.</div>
               <div class="bibl"><span class="ref" id="strange2014"><!-- close -->Strange et al. 2014</span>  Strange, C., McNamara, D., Wodak, J., and Wood, I. “Mining for the
                  Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical
                  Newspapers,” <cite class="title italic">Digital Humanities Quarterly</cite> 8:1 (2014). Available
                  at: <a href="http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html" onclick="window.open('http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html'); return false" class="ref">http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</a>.</div>
               <div class="bibl"><span class="ref" id="strubell2019"><!-- close -->Strubell et al. 2019</span>  Strubell, E., Ganesh, A., and McCallum, A. “Energy and Policy
                  Considerations for Deep Learning in NLP,” ArXiv:1906.02243 [Cs] (2019). Available at:
                  <a href="http://arxiv.org/abs/1906.02243" onclick="window.open('http://arxiv.org/abs/1906.02243'); return false" class="ref">http://arxiv.org/abs/1906.02243</a>.</div>
               <div class="bibl"><span class="ref" id="ax1910a"><!-- close -->The Broad Ax 1910a</span>  The broad ax. [volume] (Salt Lake City, Utah), 15 Oct. 1910.
                  <cite class="title italic">Chronicling America: Historic American Newspapers</cite>. Library of Congress. Available at:
                  <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</a>
                  </div>
               <div class="bibl"><span class="ref" id="ax1910b"><!-- close -->The Broad Ax 1910b</span>  The broad ax. [volume] (Salt Lake City, Utah), 26 Nov. 1910.
                  <cite class="title italic">Chronicling America: Historic American Newspapers</cite>. Library of Congress. Available at:
                  <a href="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/" onclick="window.open('https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/'); return false" class="ref">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</a>
                  </div>
               <div class="bibl"><span class="ref" id="traub2015"><!-- close -->Traub, van Ossenbruggen, and Hardman 2015</span>  Traub, M., van Ossenbruggen, J., and
                  Hardman, L. “Impact Analysis of OCR Quality on Research Tasks in Digital Archives,” in
                  <cite class="title italic">Research and Advanced Technology for Digital Libraries</cite>, ed.
                  Sarantos Kapidakis, Cezary Mazurek, and Marcin Werla (Cham: Springer International
                  Publishing, 2015), 252–263.</div>
               <div class="bibl"><span class="ref" id="vandermaaten2009"><!-- close -->Van der Maaten and Hinton 2009</span>  van der Maaten, L., and Hinton, G. “Visualizing Data
                  Using T-SNE,” <cite class="title italic">Journal of Machine Learning Research</cite> 9, pp.
                  2579-2605 (2008). Available at: <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html" onclick="window.open('http://www.jmlr.org/papers/v9/vandermaaten08a.html'); return false" class="ref">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>.</div>
               <div class="bibl"><span class="ref" id="vane2018"><!-- close -->Vane 2018</span>  Vane, O. “Visualising the Royal Photographic Society Collection: Part 2 •
                  V&amp;A Blog,” <cite class="title italic">V&amp;A Blog</cite> (2018). Available at: <a href="https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2" onclick="window.open('https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2'); return false" class="ref">https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</a>.</div>
               <div class="bibl"><span class="ref" id="wattenberg2016"><!-- close -->Wattenberg, Viégas, and Johnson 2016</span>  Wattenberg, M., Viégas, F., and Johnson, I. “How
                  to Use T-SNE Effectively,” <cite class="title italic">Distill</cite> 1:10 (2016). Available at:
                  <a href="https://doi.org/10.23915/distill.00002" onclick="window.open('https://doi.org/10.23915/distill.00002'); return false" class="ref">https://doi.org/10.23915/distill.00002</a>.</div>
               <div class="bibl"><span class="ref" id="weavers2020"><!-- close -->Weavers and Smits 2020</span>  Wevers, M., and Smits, T. “The Visual Digital Turn: Using
                  Neural Networks to Study Historical Images,” <cite class="title italic">Digital Scholarship in
                     the Humanities</cite> 35:1, pp. 194-207 (2020). Available at:
                  <a href="https://doi.org/10.1093/llc/fqy085" onclick="window.open('https://doi.org/10.1093/llc/fqy085'); return false" class="ref">https://doi.org/10.1093/llc/fqy085</a>.</div>
               <div class="bibl"><span class="ref" id="weld2019"><!-- close -->Weld and Bansal 2019</span>  Weld, D., and Bansal, G. 2019. The challenge of crafting
                  intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at: <a href="https://doi.org/10.1145/3282486" onclick="window.open('https://doi.org/10.1145/3282486'); return false" class="ref">https://doi.org/10.1145/3282486</a>.</div>
               <div class="bibl"><span class="ref" id="williams2019"><!-- close -->Williams 2019</span>  Williams, L. “What Computational Archival Science Can Learn from Art
                  History and Material Culture Studies,” in <cite class="title italic">2019 IEEE International
                     Conference on Big Data (Big Data)</cite>, 2019, pp. 3153–55. Available at:
                  <a href="https://doi.org/10.1109/BigData47090.2019.9006527" onclick="window.open('https://doi.org/10.1109/BigData47090.2019.9006527'); return false" class="ref">https://doi.org/10.1109/BigData47090.2019.9006527</a>. </div>
               <div class="bibl"><span class="ref" id="wright2019"><!-- close -->Wright 2019</span>  Wright, R. “Typewriting Mass Observation Online: Media Imprints on the
                  Digital Archive,” <cite class="title italic">History Workshop Journal</cite> 87, pp. 118–38 (2019).
                  Available at:
                  <a href="https://doi.org/10.1093/hwj/dbz005" onclick="window.open('https://doi.org/10.1093/hwj/dbz005'); return false" class="ref">https://doi.org/10.1093/hwj/dbz005</a>.</div>
               <div class="bibl"><span class="ref" id="yale2017"><!-- close -->Yale Digital Humanities Lab 2017</span>  “Yale Digital Humanities Lab - PixPlot” (2020).
                  Available at: <a href="https://dhlab.yale.edu/projects/pixplot/" onclick="window.open('https://dhlab.yale.edu/projects/pixplot/'); return false" class="ref">https://dhlab.yale.edu/projects/pixplot/</a> (Accessed 11 June 2020).</div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>