<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt><!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English--></title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>first name(s) <dhq:family>family
                            name</dhq:family>
               </dhq:author_name>
               <dhq:affiliation/>
               <email/>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000578</idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date/>
            <dhq:articleType>article</dhq:articleType>
            <availability>
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords"><!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords"><!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc><!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change/>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract><!--Include a brief abstract of the article-->
            <p/>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>
         <p>Compounded Mediation: A Data Archaeology of the Newspaper
                Navigator Dataset</p>
         <p>Benjamin Charles Germain
                    Lee</p>
         <p>2020 Innovator in Residence,
                    Library of Congress</p>
         <p>Ph.D. Student, Computer
                    Science &amp; Engineering, University of Washington</p>
         <p>Abstract</p>
         <p>The increasing roles of machine learning and artificial intelligence in the construction of cultural heritage and humanities datasets necessitate critical examination of the myriad biases introduced by machines, algorithms, and the humans who build and deploy them. From image classification to optical character recognition, the effects of decisions ostensibly made by machines compound through the digitization pipeline and redouble in each step, mediating our interactions with digitally-rendered artifacts through the search and discovery process. As a result, scholars within the digital humanities community have begun advocating for the proper contextualization of cultural heritage datasets within the socio-technical systems in which they are created and utilized. One such approach to this contextualization is the <hi rend="italic">data
                    archaeology</hi>, a form of humanistic excavation of a dataset that Paul Fyfe defines as “recover[ing] and reconstitut[ing] media objects within their changing ecologies” [Fyfe 2016]. Within critical data studies, this excavation of a dataset - including its construction and mediation via machine learning - has proven to be a capacious approach. However, the data archaeology has yet to be adopted as standard practice among cultural heritage practitioners who produce such datasets with machine learning. <note>
               <date when="2021-07-30T04:57:00Z"/>I added these sentences to introduce the
                    data archaeology and motivate this case study.</note>
         </p>
         <p>In this article, I present a data archaeology of the Library of Congress’s <hi rend="italic">Newspaper Navigator </hi>dataset, which I created as part of the Library of Congress’s Innovator in Residence program [Lee et al 2020]. The dataset consists of visual content extracted from 16 million historic newspaper pages in the <hi rend="italic">Chronicling
                    America</hi> database using machine learning techniques. In this case study, I examine the manifold ways in which a <hi rend="italic">Chronicling
                    America</hi> newspaper page is transmuted and decontextualized during its journey from a physical artifact to a series of probabilistic photographs, illustrations, maps, comics, cartoons, headlines, and advertisements in the<hi rend="italic"> Newspaper Navigator</hi> dataset [Fyfe 2016]. Accordingly, I draw from fields of scholarship including media archaeology, critical data studies, science and technology studies, and the autoethnography throughout.<note> For a representative example of an autoethnography, see [Bailey 2015].</note>
         </p>
         <p>To excavate the <hi rend="italic">Newspaper
                    Navigator</hi> dataset,<note>
               <date when="2021-07-30T04:57:00Z"/>(minor change in wording
                    here)</note> I consider the digitization journeys of four different pages in Black newspapers included in <hi rend="italic">Chronicling
                    America</hi>, all of which reproduce the same photograph of W.E.B. Du Bois in an article announcing the launch of <hi rend="italic">The
                    Crisis</hi>, the official magazine of the NAACP. In tracing the newspaper pages’ journeys, I unpack how each step in the <hi rend="italic">Chronicling
                    America</hi> and <hi rend="italic">Newspaper
                    Navigator</hi> pipelines, such as the imaging process and the construction of training data, not only imprints bias on the resulting <hi rend="italic">Newspaper
                    Navigator</hi> dataset but also propagates the bias through the pipeline via the machine learning algorithms employed. Along the way, I investigate the limitations of the <hi rend="italic">Newspaper
                Navigator</hi> dataset and machine learning techniques more generally as they relate to cultural heritage, with a particular focus on marginalization and erasure via algorithmic bias, which implicitly rewrites the archive itself. </p>
         <p>In presenting this case study, I argue for the value of the data archaeology as a mechanism for contextualizing and critically examining cultural heritage datasets within the communities that create, release, and utilize them. I offer this autoethnographic investigation of the <hi rend="italic">Newspaper Navigator </hi>dataset in the hope that it will be considered not only by users of this dataset in particular but also by digital humanities practitioners and end users of cultural heritage datasets writ large. <note>
               <date when="2021-07-30T04:58:00Z"/>I added this paragraph to signpost the
                    central argument of this article.</note>
         </p>
         <p>I. An Introduction
                    to
                    the Newspaper Navigator Dataset</p>
         <p>In partnership with LC Labs, the National Digital Newspaper Program, and IT Design &amp; Development at the Library of Congress, as well as Professor Daniel Weld at the University of Washington, I constructed the <hi rend="italic">Newspaper
                    Navigator</hi> dataset as the first phase of my Library of Congress Innovator in Residence project,<hi rend="italic"> Newspaper Navigator</hi>.<note> More on the organizational considerations surrounding <hi rend="italic">Newspaper Navigator </hi>can be found in [Lee et al.
                    2021].</note>
                The project has its origins in <hi rend="italic">Chronicling America</hi>, a database of digitized historic American newspapers
                    created and maintained
                    by the National Digital Newspaper Program, itself a partnership between the Library of Congress and the National Endowment for the Humanities. Content in Chronicling
                    America is contributed by state partners of the National Digital Newspaper Program who have applied for and received awards from the Division of Preservation and Access at the National Endowment for the Humanities [Mears 2014]. At the time of the construction of the Newspaper
                    Navigator dataset in March, 2020, Chronicling
                    America contained approximately 16.3 million digitized historic newspaper pages published between 1789 and 1963, covering 47 states as well as Washington, D.C. and Puerto Rico. The technical specifications of the National Digital Newspaper Program require that each digitized page in Chronicling
                America comprises the following digital artifacts [National Digital Newspaper Program 2020]: </p>
         <list type="ordered">
            <item>A page image in two raster
                        formats:<list type="ordered">
                  <item>Grayscale, scanned for maximum resolution possible between 300-400 DPI, relative to the original material, uncompressed TIFF 6.0 </item>
                  <item>Same image,
                                compressed as JPEG2000</item>
               </list>
            </item>
            <item>Optical character
                        recognition (OCR) text and associated bounding boxes for words (one file per
                        page image)</item>
            <item>PDF Image with Hidden
                        Text, i.e., with text and image correlated</item>
            <item>Structural metadata (a) to
                        relate pages to title, date, and edition; (b) to sequence pages within issue
                        or section; and (c) to identify associated image and OCR files</item>
            <item>Technical metadata to
                        support the functions of a trusted repository</item>
         </list>
         <p>Additional artifacts and metadata
                    are contributed for each digitized newspaper issue and microfilm reel. All
                    digitized pages are in the public domain and are available online via a public
                    search user interface,<note> The public search interface is available at: <ref target="https://chroniclingamerica.loc.gov/">https://chroniclingamerica.loc.gov/</ref>
            </note>
                making Chronicling America an immensely rich resource for
                    the American public.</p>
         <p>The central goal of <hi rend="italic">Newspaper Navigator </hi>is to re-imagine how the American public explores <hi rend="italic">Chronicling
                    America</hi> by utilizing emerging machine learning techniques to extract, categorize, and search over the visual content and headlines in <hi rend="italic">Chronicling
                    America</hi>’s 16.3 million pages of digitized historic newspapers. <hi rend="italic">Newspaper
                    Navigator</hi> was both inspired and directly enabled by the Library of Congress’s <hi rend="italic">Beyond
                    Words</hi> crowdsourcing initiative [Ferriter 2017]. Launched by LC Labs in 2017, <hi rend="italic">Beyond Words</hi> engages the American public by asking volunteers to identify and draw boxes around photographs, illustrations, maps, comics, and editorial cartoons on World War I-era pages in
                <hi rend="italic">Chronicling America</hi>, note the visual content categories, and transcribe the
                    relevant textual information such as titles and captions.<note> For more information on the Beyond Words workflow, see [LC Labs no date], as well as [Lee et al. 2020].</note>
                The thousands of annotations created by <hi rend="italic">Beyond Words </hi>volunteers are in the public domain and available for download online. <hi rend="italic">Newspaper Navigator </hi>directly builds on <hi rend="italic">Beyond
                    Words</hi> by utilizing these annotations, as well as additional annotations of headlines and advertisements, to train a machine learning model to detect visual content in historic newspapers.<note> In particular, the annotations were used to finetune an object detection model that had been pre-trained on Common Objects in Context, a common dataset for benchmarking object detection algorithms.</note>
                Because <hi rend="italic">Beyond
                    Words</hi> volunteers were asked to draw bounding boxes to include any relevant textual content, such as a photograph’s title, this machine learning model learns during training to include relevant textual content when predicting bounding boxes.<note> A screenshot of the workflow can be found later in this article in Figure 4. </note>
                Furthermore, in the <hi rend="italic">Transcribe </hi>step of <hi rend="italic">Beyond
                    Words</hi>, the system provided the OCR with each bounding box as an initial transcription for the volunteer to correct; inspired by this, the <hi rend="italic">Newspaper
                    Navigator</hi> pipeline automatedly extracts the OCR falling within each predicted bounding box in order to provide noisy textual metadata for each image. In the case of headlines, this method enables the headline text to be directly extracted from the bounding box predictions. Lastly, the pipeline generates image embeddings for the extracted visual content using an image classification model trained on ImageNet.<note> For those who are not familiar with image embeddings, a detailed description is provided in Section IX.</note>
                A diagram of the full <hi rend="italic">Newspaper
                Navigator</hi> pipeline can be found in Figure 1. </p>
         <figure>
            </figure>
         <p>Figure
                    1. A diagram showing the <hi rend="italic">Newspaper
                    Navigator</hi> pipeline, which processed over 16.3 million historic newspaper pages in <hi rend="italic">Chronicling
                    America</hi>, resulting in the <hi rend="italic">Newspaper
                Navigator</hi> dataset.</p>
         <p>Over the course of 19 days from late March to early April of 2020, the <hi rend="italic">Newspaper Navigator</hi>
         </p>
         <p>pipeline processed 16.3 million pages in <hi rend="italic">Chronicling
                    America</hi>; the resulting <hi rend="italic">Newspaper Navigator </hi>dataset was publicly released in May, 2020. The full
                    dataset, as well as all code written for this project, are available online and
                    have been placed in the public domain for unrestricted re-use.<note> For the dataset, see: <ref target="https://news-navigator.labs.loc.gov">https://news-navigator.labs.loc.gov</ref>; for the code, see <ref target="https://github.com/LibraryOfCongress/newspaper-navigator">https://github.com/LibraryOfCongress/newspaper-navigator</ref>.</note>
                Currently, the <hi rend="italic">Newspaper
                    Navigator</hi> dataset can be queried using HTTPS and Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made available for download, along with associated metadata. These pre-packaged datasets consist of different types of visual content for each year, from 1850 to 1963, allowing users to download, for example, all of the maps from 1863 or all of the photographs from 1910. For more information on the technical aspects of the pipeline and the construction of the <hi rend="italic">Newspaper
                    Navigator</hi> dataset, I refer the reader to [Lee et al<hi rend="italic">.</hi> 2020].</p>
         <p>II. Why a Data
                    Archaeology?</p>
         <p>As machine learning and artificial intelligence play increasing roles in digitization and digital content stewardship, the Libraries, Archives, and Museums (“LAM”) community has repeatedly emphasized the importance of ensuring that these emerging methodologies are incorporated ethically and responsibly. Indeed, a major theme that emerged from the “Machine Learning + Libraries Summit” hosted by LC Labs in September, 2019, was that “there is much more ‘human’ in machine learning than the name conveys” and that transparency and communication are first steps toward addressing the “human subjectivities, biases, and distortions” embedded within machine learning systems [LC Labs and Digital Strategy Directorate 2020]. This data archaeology has been written in support of this call for transparency and responsible stewardship, which is echoed in the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan Cordell’s report to the Library of Congress “ML + Libraries: A Report on the State of the Field,” Thomas Padilla’s OCLC position paper “Responsible Operations: Science, Machine Learning, and AI in Libraries” and the University of Nebraska-Lincoln’s report on machine learning to the Library of Congress [Library of Congress 2019]; [Cordell 2020]; [Padilla 2019]; [Lorang et al. 2020]. I write this data archaeology from my perspective of having created the dataset, and although I am not without my own biases, I have attempted to represent my work as honestly as possible. Accordingly, I seek not only to document the construction of the <hi rend="italic">Newspaper Navigator </hi>dataset through the lens of data stewardship but also to
                    critically examine the dataset’s limitations. In doing so, I advocate for the importance of autoethnographic approaches to documenting a cultural heritage dataset’s construction from a humanistic perspective.<note>
               <date when="2021-07-30T05:04:00Z"/>I added this sentence to refer back to
                    the central argument around the data archaeology as a case study.</note>
         </p>
         <p>This article draws inspiration from recent works in media and data archaeology, including Paul Fyfe’s “An Archaeology of Victorian Newspapers”; Bonnie Mak’s “Archaeology of a Digitization”; Kate Crawford and Trevor Paglen’s “Excavating AI: The Politics of Images in Machine
                    Learning Training
                    Sets”; and, most directly, Ryan Cordell’s “Qi-jtb the Raven: Taking Dirty OCR Seriously,” in which Cordell traces the digitization of a single issue of the <hi rend="italic">Lewisburg
                    Chronicle</hi> from its selection by the Pennsylvania Digital Newspaper Project to its ingestion into the <hi rend="italic">Chronicling America </hi>online database, with a focus on the distortive effects of OCR [Fyfe 2018]; [Mak 2015]; [Crawford and Paglen 2019]; [Cordell 2017]. As argued by Trevor Owens and Thomas Padilla, it is essential to “document how digitization practices and how the affordances of particular sources … produce unevenness in the discoverability and usability of collections” [Owens and Padilla 2020]. Recent works within
                    the machine learning literature have analogously emphasized the importance of
                    documenting the collection and curation efforts underpinning community datasets
                    and machine learning models. Reporting mechanisms include “Datasheets for
                    Datasets,” “Dataset Nutrition Labels,” “Data Statements for NLP,” “Model Cards
                    for Model Reporting,” and “Algorithmic Impact Assessments” [Gebru et al. 2020];
                    [Holland et al. 2018]; [Bender and Friedman 2018]; [Mitchell et al. 2019].;
                    [Reisman et al. 2018]. This case study adopts a similar framing in stressing the
                    importance of reporting mechanisms, with a particular focus on the data
                    archaeology in the context of cultural heritage datasets.<note>
               <date when="2021-07-29T23:10:00Z"/>I added this section to capture related
                    works from machine learning and also to connect back to my
                    argument surrounding the data archaeology as a valuable reporting
                    mechanism.</note>
         </p>
         <p> In the following sections, I trace the digitization process and data flow for <hi rend="italic">Newspaper
                    Navigator</hi>, beginning with the physical artifact of the newspaper itself and ending with the machine learning predictions that constitute the <hi rend="italic">Newspaper Navigator </hi>dataset, reflecting on each step through the lens of discoverability and erasure. In particular, I study four different <hi rend="italic">Chronicling America </hi>Black newspaper pages published in 1910, each depicting the same photograph of W.E.B. Du Bois, as the pages move through the <hi rend="italic">Chronicling
                    America</hi> and <hi rend="italic">Newspaper
                Navigator</hi> pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter from <hi rend="italic">The Baltimore Afro-American </hi>[Farrar 1998]; the headline is as follows:  </p>
         <p>NEW MOVEMENT </p>
         <p>BEGINS WORK</p>
         <p>Plan and Scope of the
                    Asso-</p>
         <p>ciation Briefly Told. </p>
         <p>Will Publish the Crisis. </p>
         <p>Review of Causes Which Led to the </p>
         <p>Organization of the Association in </p>
         <p>New York and What Its Policy Will </p>
         <p>Be-Career and Work of Professor </p>
         <p>W.E.B. Du Bois</p>
         <p>The article describes the creation of the National Association for the Advancement of Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch of <hi rend="italic">The
                    Crisis</hi>, the official magazine of the NAACP, with Du Bois as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910, issue of the <hi rend="italic">Iowa State Bystander </hi>[Iowa State Bystander 1910]; the 16th page of the October 15th, 1910, issue of <hi rend="italic">Franklin’s Paper the Statesman </hi>[Franklin’s Paper the Statesman 1910]; and the 2nd and 3rd pages of the October 15th, 1910, and November 26th, 1910, issues of <hi rend="italic">The Broad Ax</hi>, respectively [The Broad Ax 1910a]; [The Broad Ax 1910b]. All four digitized
                    pages are reproduced in the Appendix.</p>
         <p>III. Chronicling America: A Genealogy of Collecting, Microfilming,
                    and Digitizing</p>
         <p>Any examination of <hi rend="italic">Newspaper Navigator </hi>must begin with the genealogy of collecting, microfilming, and digitizing that dictates which newspapers have been ingested into the <hi rend="italic">Chronicling America </hi>database. The question of what to digitize is, in
                    practice, answered and realized incrementally over decades, beginning at its
                    most fundamental level with the question of which newspapers have survived and
                    which have been reduced to lacunae in the historical record [Hardy and DiCuirci
                    2019].<note> Indeed, compiling bibliographies of serials published after 1820 remains an immensely difficult task [Hardy and DiCuirci 2019].</note>
                Historic newspapers present challenges for digitization
                    in part due to the ephemerality of the physical printed newspaper itself: many
                    newspapers were microfilmed and immediately discarded due to a fear that the
                    physical pages would deteriorate.<note> The extent to which newspaper microfilming was driven by credible fear of deterioration versus other factors, such as microfilm marketing, is an important question that is rightly debated. For more on this topic, see [Baker 2001]. </note>
                Indeed, almost all of the pages included in <hi rend="italic">Chronicling
                    America</hi> have been digitized directly from microfilm. In the next section, I will examine the microfilm imaging process in more detail; however, in most cases, librarians selected newspapers for collecting and microfilming decades before the National Digital Newspaper Program was launched in 2004. These selections were informed by a range of factors including historical significance - itself a subjective, nebulous, and ever-evolving notion that has historically served as the basis for perpetuating oppression within the historical record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black newspapers in <hi rend="italic">Chronicling America</hi>, in particular in relation to pre-Civil War era
                    newspapers [Fagan 2016]. It is imperative to remember that this paucity can
                    directly be traced back decades to the collecting and preserving
                    stages.<note> For example, a 2017 article describing the West Virginia
                            University Libraries’ West Virginia &amp; Regional History Center and its participation in the National Digital Newspaper Program states: “By August 2017, all known issues of West Virginia’s African-American newspapers from the 19th and early 20th centuries will have been digitized” [Maxwell 2017]. The article describes Curator Stewart Plein’s efforts to locate surviving copies of three Black West Virginia newspapers in order to digitize and include them in Chronicling America.</note>
         </p>
         <p>In regard to collecting, the newspaper page is both an informational object (i.e., the newspaper page as defined by its content) and a material object (i.e., the specific printed copy of the newspaper page) [Owens 2018]. At some point in time, librarians accessioned a specific copy of each printed page and microfilmed it or contracted out the microfilming. The materiality of that specific printed page is a confluence of unique ink smudges, rips, creases, and page alignment, much of which is captured in the microfilm imaging process. Though we may not make much of a crease or a smudge on a digitized page when we find it in the <hi rend="italic">Chronicling
                    America</hi> database, it can very well take on a life of its own with a machine learning algorithm in <hi rend="italic">Newspaper
                Navigator</hi>. The machine learning algorithm might deem two newspaper photographs as similar simply due to the presence of creases or smudges, even if the photographs are easily discernible to the naked eye, or the smudges are of entirely different origin (i.e., a printing imperfection versus a smudge from a dirty hand). </p>
         <p>It is only by foregrounding these subtleties of the collection, preservation, and microfilming processes that we can understand the selection process for <hi rend="italic">Chronicling
                    America</hi> in its proper context. The grant-seeking process dictates selection criteria for <hi rend="italic">Chronicling America </hi>by which state-level institutions including state libraries, historical societies, and universities apply for two years of grant funding from the National Digital Newspaper Program via the Division of Preservation and Access at the National Endowment for the Humanities. With the awarding of a grant, a state-level awardee then digitizes approximately 100,000 newspaper pages published in their state for inclusion in Chronicling
                    America [National Digital Newspaper Program 2020]; [NEH Division of Preservation and Access 2020]. The grant-seeking and awarding
                    process is nuanced, but salient points include that state-level applicants must
                    assemble an advisory board including scholars, teachers, librarians, and
                    archivists to aid in the selection of newspapers, and grants are reviewed by
                    National Endowment for the Humanities staff, as well as peer
                    reviewers.<note> For a thorough case study of this process, I direct the reader to “Qi-jtb the Raven,” in which Ryan Cordell walks through an example with the Pennsylvania Digital Newspaper Program [Cordell 2017]. </note>
         </p>
         <p>Regarding selection criteria for newspaper titles, the National Digital Newspaper Program defines the following factors for state-level awardees to consider for content selection after a newspaper is determined to be in the public domain [National Digital Newspaper Program no date]: </p>
         <list type="unordered">
            <item>image quality in the selection of
                    microfilm</item>
            <item>research value</item>
            <item>geographic representation</item>
            <item>temporal coverage</item>
            <item>bibliographic completeness of microfilm
                    copy</item>
            <item>diversity (i.e., “newspaper
                        titles that document a significant minority community at the state or
                        regional level”)</item>
            <item>whether the title is orphaned (i.e., whether the newspaper
                        has “ceased publication and lack[s] active ownership” [Chronicling America
                        no date])</item>
            <item>whether the title has already been
                    digitized.</item>
         </list>
         <p>Though factors such as research value are considered by each state awardee’s advisory board, as well as by the National Endowment for the Humanities and peer review experts, the titles included in <hi rend="italic">Chronicling
                America</hi> are largely dictated by which exist on microfilm and are of sufficient image quality within a state-level grantee’s collection. Thus, the significance of the collection and microfilming practices of decades prior cannot be understated.</p>
         <p>I also highlight that assessing microfilmed titles based on image quality is a complex procedure in its own right. The National Digital Newspaper Program has made publicly available a number of resources devoted specifically to this task, including documents and video tutorials [Barrall and Guenther 2005]; [Meta | Morphosis no date]. They articulate factors such as the microfilm generation (archive master, print master, or review copy), the material (polyester or acetate), the reduction ratio, and the physical condition. The detailed resources made available by the National Digital Newspaper Program, the Library of Congress, and the National Endowment for the Humanities for navigating this process are testaments to the multidimensional complexity of the selection process for <hi rend="italic">Chronicling America </hi>[National Digital Newspaper Program 2019]; [National
                    Digital Newspaper Program no date]; [NEH Division of Preservation and Access
                    2020].</p>
         <p>We have not yet investigated the topic of digitization, and we have already encountered a profusion of factors from collection to digitization that mediate which artifacts appear in <hi rend="italic">Chronicling
                    America</hi> and thus <hi rend="italic">Newspaper Navigator</hi>. Let us now examine the microfilm itself.</p>
         <p>IV. The
                    Microfilm</p>
         <p>In “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” Lyneise Williams shares a powerful anecdote of coming across a physical copy of a 1927 issue of the French sports newspaper <hi rend="italic">Match
                    L’Intran</hi> that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front cover [Williams 2019]. Williams describes Brown as “glowing. He looked like a 1920s film star rather than a boxer” [Williams 2017]. Curious to learn more about the printing process, Williams discovered that the issue of <hi rend="italic">Match
                L’Intran</hi> was produced using rotogravure, a specific printing process that could “capture details in dark tones” [Williams 2019]. However, when Williams found a version of the same newspaper cover that had been digitized from microfilm, it was apparent that the microfilming process had washed out the detail of the rotogravure, reducing Brown to a “flat black, cartoonish form” [Williams 2019]. Williams relays the anecdote to articulate that the microfilming process itself is thus a form of erasure for communities of color [Williams 2019].</p>
         <p>The grayscale saturation of photographs induced by microfilming is widely documented and recognizable to most researchers who have ever worked with the medium [Baker 2001]; however, Lyneise Williams’s article affords us a lens into what precisely is lost amongst the distortive effects of the microfilming process. This erasure via microfilming can be seen in <hi rend="italic">Chronicling America </hi>directly. In Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different <hi rend="italic">Chronicling America</hi> newspaper pages published during October and November of 1910 and digitized from microfilm [Iowa State Bystander 1910]; [Franklin’s Paper the Statesman 1910]; [The Broad Ax 1910a]; [The Broad Ax 1910b]. The phenomenon described by Williams is immediately recognizable in these four images: Du Bois’s facial features are distorted by the grayscale saturation. In the case of the 
                <hi rend="italic">Iowa State Bystander</hi>, Du Bois has been rendered into a silhouette.</p>
         <p>Moreover, each digitized reproduction reveals unique visual qualities, varying in contrast, sharpness, and noise - a testament to the confluence of mediating conditions from printing through digitization that have rendered each newspaper photograph in digital form. Even in the case of the two images reproduced in the <hi rend="italic">The Broad Ax</hi>, which were digitized from the very same microfilm reel (reel #00280761059) by
                    the University of Illinois at Urbana-Champaign Library, variations are still
                    apparent. To understand how these subtle differences between images are
                    amplified through digitization, we now turn to optical character
                    recognition.</p>
         <figure>
            </figure>
         <p>Figure
                    2. The same image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <hi rend="italic">Chronicling
                America</hi> from 1910. Note that the combined effects of printing, microfilming, and digitizing have led to different visual effects in each image, ranging from contrast to sharpness. </p>
         <p>V. OCR</p>
         <p>Optical character recognition, commonly called OCR, refers
                    to machine learning algorithms that are trained to read images of typewritten
                    text and output machine-readable text, thereby providing the bridge between an
                    image of typewritten text and the transcribed text itself. Because OCR
                    algorithms are
                    “trained and evaluated using labeled data: examples with ground-truth classification labels that have been assigned by another means,” the algorithms are considered a form of supervised
                    learning in the machine learning literature [Lee 2019]. OCR engines are remarkably powerful in their ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata for <hi rend="italic">Chronicling America</hi>, enabling keyword search in the search portal and making
                    possible scholarship with the newspaper text at large scales.<note> For exemplary research collaborations that utilize the <hi rend="italic">Chronicling America </hi>bulk OCR, see the Viral Text Project and the Oceanic Exchanges Project [Cordell and Smith 2017]; [Oceanic Exchanges Project 2017]. </note>
                However, OCR is not perfect. Although humans are able to discern an “E” from an “R” on a digitized page even if the type has been smudged, an OCR engine is not always able to do so: its performance is dependent on factors ranging from the sharpness of text in an image to printing imperfections to the specific typography on the page. </p>
         <p>In Figure 3, I show the same four images shown in Figure 2, along with OCR transcriptions of the captions provided by <hi rend="italic">Chronicling
                    America</hi>. All four transcriptions fail to reproduce the true caption with 100% accuracy, differing from one another by at least one character. Consequently, a keyword search of “W. E. B. Du Bois” over the raw text would not register the caption for any of the four photographs (the <hi rend="italic">Chronicling
                    America</hi> search portal utilizes a form of relevance search to alleviate this problem). These examples reveal how sensitive OCR engines are to slight perturbations, or “noise,” in the digitized images, from ink smudges to text sharpness to page contrast. Though the NDNP awardees who contributed these pages may have utilized different OCR engines or chosen different OCR settings, the OCR for the two image captions from <hi rend="italic">The Broad Ax </hi>that have been from the very same microfilm reel was in
                    all likelihood generated using the same OCR engine and settings. Put succinctly,
                    OCR engines amplify the noise from both the material page and the digitization
                    pipeline.<note> For other examinations of how OCR mediates our interactions with digital archives, see [Hitchcock 2013]; [Milligan 2013]; [Strange et al. 2014]; [Traub, van Ossenbruggen, and Hardman 2015]; [Wright 2019].</note>
         </p>
         <figure>
            </figure>
         <p>Figure 3. The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <hi rend="italic">Chronicling
                    America</hi>. These OCR transcriptions are provided by <hi rend="italic">Chronicling America</hi>.</p>
         <p>Though OCR engines have become standard components of digitization pipelines, it is important to remember that OCR engines are themselves machine learning models that have been trained on sets of transcribed typewritten pages. Like any machine learning model, OCR predictions are thus subject to biases encoded not only in the OCR engine’s architecture but also in the training data itself. Though it is often called <hi rend="italic">algorithmic bias</hi>, this bias is undeniably human, in that the construction
                    of training data machine learning models are imprinted with countless human
                    decisions and judgment calls. For example, if an OCR engine is trained on
                    transcriptions that consistently misspell a word, the OCR engine will amplify
                    this misspelling across all transcriptions of processed pages.<note> For a concrete example of a similar phenomenon in the image domain, see [Lee 2019], in which a machine learning algorithm was trained to classify digitized images but consistently misclassified images that had been misoriented 180 degrees in the scanning bed - a consequence of the classifier not having seen enough instances of these misoriented scans during training.</note>
                A recurring theme of algorithmic bias is that it is a force for marginalization, especially in the context of how we navigate information digitally. In <hi rend="italic">Algorithms of
                    Oppression</hi>, Safiya Noble describes how Google’s search engine consistently marginalizes women and people of color by displaying search results that reinforce racism [Noble 2018]. This bias is not restricted to Google: in <hi rend="italic">Masked by Trust: Bias in Library
                    Discovery</hi>, Matthew Reidsma articulates how library search engines suffer from similar biases [Reidsma 2019]. Despite the fact that knowledge of algorithmic bias in relation to search engines and image recognition tools is becoming increasingly widespread among the cultural heritage community, the errors introduced by OCR engines are often accepted as inevitable without critical inquiry from this perspective. However, algorithmic bias is a useful framework for examining OCR engines [Alpert-Abrams 2016]. </p>
         <p>Perhaps the most significant challenge to studying OCR
                    engines is that the best-performing and most widely-used OCR engines are
                    proprietary. Though ABBYY FineReader and Google Cloud Vision API offer high
                    performance, the systems fundamentally are black boxes: we have no access to the
                    underlying algorithms or the training data. The ability to audit a system is
                    crucial to developing an understanding of how it works and the biases it
                    encodes. The fact that many OCR engines are opaque prevents us from
                    disentangling whether poor performance on a particular page is due to
                    algorithmic limitations or due to a lack of relevant training data. The
                    distinction is significant: the former may reflect an algorithmic upper bound,
                    whereas the latter reflects decisions made by humans.</p>
         <p>Indeed, algorithmic bias distorts and occludes the historical record, as it is made discoverable through OCR. Discrepancies in OCR performance for different languages and scripts is a consequence of human prioritization, from the collection of training data and lexicons to the development of the algorithms themselves. As articulated by Hannah Alpert-Abrams in “Machine Reading the <hi rend="italic">Primeros Libros</hi>,”
                    “the machine-recognition of printed characters is a historically charged event, in which the system and its data conspire to embed cultural biases in the output, or to affix them as supplementary information hidden behind the screen” [Alpert-Abrams 2016]. Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded in colonial scripts perpetuate colonialism. For other languages such as Ladino, typically typeset in Rashi script, the lack of high-performing OCR has presented consistent challenges for digitization and
                    scholarship.</p>
         <p>In the case of <hi rend="italic">Chronicling America</hi>, the
                    National Digital Newspaper Program is exemplary in its efforts to support OCR
                    for non-English languages. In the Notice of Funding Opportunity for the National
                    Digital Newspaper Program produced by the Division of Preservation of Access at
                    the National Endowment for the Humanities, OCR performance in different
                    languages is explicitly addressed: “Applicants proposing to digitize titles in languages
                    other than English must include staff with the relevant language expertise to
                    review the quality of the converted content and related metadata” [NEH Division
                    of Preservation and Access 2020]. I have
                    included this discussion of OCR and algorithmic bias to offer a broader
                    provocation regarding machine learning and digitization: how much text in
                    digitized sources has been transmuted by this effect and thus effectively erased
                    due to inaccessibility when using search and discovery platforms?</p>
         <p>VI. The Visual
                    Content Recognition Model</p>
         <p>I will now turn to the <hi rend="italic">Newspaper
                Navigator</hi> pipeline itself, in particular the visual content recognition model. Trained on annotations from the <hi rend="italic">Beyond Words </hi>crowdsourcing initiative, as well as additional annotations of headlines and advertisements, the visual content recognition model detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and advertisements on historic newspaper pages. </p>
         <p>As described in the previous section, examining training data is an essential component of auditing any machine learning model, from understanding how the dataset was constructed to uncovering any biases in the composition of the dataset itself. For the visual content recognition model, this examination begins with <hi rend="italic">Beyond
                    Words</hi>. Launched in 2017 by LC Labs, <hi rend="italic">Beyond Words </hi>has collected to-date over 10,000 verified annotations of visual content in World War 1-era newspaper pages from <hi rend="italic">Chronicling
                    America</hi>. The <hi rend="italic">Beyond Words </hi>workflow consists of the three steps listed
                below:</p>
         <list type="ordered">
            <item>A “Mark” step, in which volunteers are asked to
                        draw bounding boxes around visual content on the page [LC Labs 2017a]. The
                        instructions read as follows:</item>
         </list>
         <p>“In the Mark step, your task is to identify and select
                    pictures in newspaper pages. For our project, ‘pictures’ means illustrations,
                    photographs, comics, and cartoons. You'll use the marking tool to draw a box
                    around the picture using your mouse. After you have marked all pictures on the
                    newspaper page, click the ‘DONE’ button. Skip the page altogether by clicking
                    the ‘Skip this page’ button. If no illustrations, photographs, or cartoons
                    appear on the page, click the ‘DONE’ button. Not sure if a picture should be
                    marked? Select the ‘Done for now, more left to mark’ button so another volunteer
                    can help finish that page. Please do not select pictures within
                    advertisements.”</p>
         <list type="ordered">
            <item>A “Transcribe” step, in which volunteers are asked
                        to transcribe the caption of the highlighted visual content, as well as note
                        the artist and visual content category (“Photograph”, “Illustration”, “Map”,
                        “Comics/Cartoon”, “Editorial Cartoon”) [LC Labs 2017b]. The transcription is
                        pre-populated with the OCR falling within the bounding box in question. The
                        instructions for this step state:</item>
         </list>
         <p> b “Most pictures have captions or descriptions. Enter the text exactly as you see it. Include capitalization and punctuation, but remove hyphenation that breaks words at the end of the line. Use new lines to separate different parts of captions and descriptions. You can zoom in for better looks at the page. You can also select ‘View the original page’ in the upper right corner of the screen to view the original high resolution image of the newspaper.”</p>
         <p>An example of this step can be seen in Figure 4.</p>
         <list type="ordered">
            <item>A “Verify” step, in which volunteers are asked to
                        select the best caption for an identified region of visual content from at
                        least two examples; alternatively, a volunteer can add another caption [LC
                        Labs 2017c]. The instructions state:</item>
         </list>
         <p> “Choose the transcription that most accurately captures the text as written. If multiple transcriptions appear valid, choose the first one. If the selected region isn't appropriate for the prompt, click “Bad region.”</p>
         <figure>
            </figure>
         <p>Figure 4. A screenshot showing an example of the “Transcribe” step of the <hi rend="italic">Beyond Words </hi>workflow. Note that the photograph caption is
                    pre-populated using the OCR falling within the bounding box [LC Labs
                    2017b].</p>
         <p>For the purposes of <hi rend="italic">Newspaper Navigator</hi>, only the bounding boxes from the “Mark” step and the
                    category labels from the “Transcribe” step were utilized as training data;
                    however, understanding the full workflow is essential because annotations are
                    considered “verified” only if they have passed through the full
                workflow.</p>
         <p>A number of factors contribute to which <hi rend="italic">Chronicling America </hi>pages were processed by volunteers in <hi rend="italic">Beyond
                    Words</hi>. First, the temporal restriction to World War 1-era pages affects the ability of the visual content recognition model to generalize: after all, if the model is trained on World War 1-era pages, how well should we expect it to perform on 19th century pages? I will return to this question later in the section. Moreover, <hi rend="italic">Beyond Words </hi>volunteers could select either an entirely random page or a random page from a specific state, an important affordance from an engagement perspective, as volunteers could explore the local histories of states in which they are interested. But this affordance is also imprinted on the training data, as certain states - and thus, certain newspapers - appear at a higher frequency than if the World War-1 era <hi rend="italic">Chronicling
                    America</hi> pages had been drawn randomly from this temporal range in <hi rend="italic">Chronicling America</hi>.</p>
         <p>Furthermore, it should be noted that the “Mark” and
                    “Transcribe” steps - specifically, drawing bounding boxes and labeling the
                    visual content category - are complex tasks. Because newspaper pages are
                    remarkably heterogenous, ambiguities and edge-cases abound. Should a photo
                    collage be marked as one unit or segmented into constituent parts? What
                    precisely is the distinction between an editorial cartoon and an illustration?
                    How much relevant textual content should be included in a bounding box?
                    Naturally, volunteers did not always agree on these choices. In this regard, the
                    notion of a ground-truth, a set of perfect annotations against which we can
                    assess performance, is itself called into question. Moreover, with thousands of
                    annotations, mistakes in the form of missed visual content, as well as
                    misclassifications, are inevitable.<note> It should be noted that <hi rend="italic">Beyond Words </hi>was introduced by LC Labs as an experiment, with
                            no interventions in workflow or community
                    management.</note> These ambiguities and errors are natural components of <hi rend="italic">any </hi>training dataset and must be taken into account when
                    analyzing a machine learning model’s predictions.</p>
         <p>A breakdown of <hi rend="italic">Beyond
                    Words</hi> annotations included in the training data can be found in the second column of Table 1. I downloaded these 6,732 publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals an imbalance between the number of examples for each category; in the language of machine learning, this is called <hi rend="italic">class
                    imbalance</hi>. While the discrepancy between maps and photographs is to be expected, the fact that so few maps were included was concerning from a machine learning standpoint: a machine learning algorithm’s ability to generalize to new data is dependent on having many diverse training examples. To address this concern, I searched <hi rend="italic">Chronicling America </hi>and identified 134 pages published between January 1st,
                    1914, and December 31st, 1918, that contain maps. I then annotated these pages
                    myself.</p>
         <p>In addition, during the development of the <hi rend="italic">Newspaper
                Navigator</hi> pipeline, I realized the value in training the visual content recognition model to identify headlines and advertisements. Consequently, I added annotations of headlines and advertisements for all 3,559 pages included in the training data. The statistics for this augmented set of annotations can be found in the third column of Table 1. Though I attempted to use a consistent approach to annotating the headlines and advertisements, my interpretation of what constitutes a headline is certainly not unimpeachable: I am not a trained scholar of periodicals or of print culture; even if I were, the task itself is inevitably subjective. Furthermore, I made decisions to annotate large grids of classified ads as a single ad to expedite the annotation process. Whether this was a correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines and advertisements required a significant amount of time, and there are inevitably mistakes and inconsistencies embedded within the annotations. My own decisions in terms of how to annotate, as well as my mistakes and inconsistencies, are embedded within the visual content recognition model through training. For those interested in examining the training data directly, the data can be found in the GitHub repository for this project [Lee 2020].</p>
         <table>
            <row role="data">
               <cell>Category</cell>
               <cell>Beyond Words Annotations</cell>
               <cell>Total
                            Annotations</cell>
            </row>
            <row role="data">
               <cell>Photograph</cell>
               <cell>4,193</cell>
               <cell>4,254</cell>
            </row>
            <row role="data">
               <cell>Illustration</cell>
               <cell>1,028</cell>
               <cell>1,048</cell>
            </row>
            <row role="data">
               <cell>Map</cell>
               <cell>79</cell>
               <cell>215</cell>
            </row>
            <row role="data">
               <cell>Comic/Cartoon</cell>
               <cell>1,139</cell>
               <cell>1,150</cell>
            </row>
            <row role="data">
               <cell>Editorial
                            Cartoon</cell>
               <cell>293</cell>
               <cell>293</cell>
            </row>
            <row role="data">
               <cell>Headline</cell>
               <cell>-</cell>
               <cell>27,868</cell>
            </row>
            <row role="data">
               <cell>Advertisement</cell>
               <cell>-</cell>
               <cell>13,581</cell>
            </row>
            <row role="data">
               <cell>
                  <hi rend="italic">Total</hi>
               </cell>
               <cell>6,732</cell>
               <cell>48,409</cell>
            </row>
         </table>
         <p>Table
                    1. A breakdown of <hi rend="italic">Beyond Words </hi>annotations included in the training data for the visual
                    content recognition model, as well as all annotations constituting the training
                    data.</p>
         <p>Beyond the construction of the training data, I made
                    manifold decisions regarding the selection of the correct model architecture and
                    the training of the model. Because this discussion surrounding these choices is
                    quite technical, I refer the reader to Lee et al<hi rend="italic">.</hi> 2020 for an in-depth examination. However, I will state that the choice of model, the number of iterations for which the model was trained, and the choice of model parameters are all of significant import for the resulting trained model and consequently, the <hi rend="italic">Newspaper Navigator </hi>dataset.</p>
         <p>I will now turn to the visual content recognition model’s outputs in relation to the <hi rend="italic">Newspaper Navigator </hi>pipeline. The model itself consumes a lower-resolution version of a <hi rend="italic">Chronicling America </hi>page as input and then outputs a JSON file containing
                    predictions, each of which consists of bounding box coordinates,<note> Bounding box coordinates refer to the positions of the corners of the predicted bounding box, relative to the image coordinates.</note>
                the predicted class (i.e., “photograph”, “map”, etc.),
                    and a confidence score generated by the machine learning model.<note> The confidence score is examined in more detail in the next section.</note>
                Cropping out and saving the visual content required extra code to be written. Because the high-resolution images of the <hi rend="italic">Chronicling
                    America</hi> pages, in addition to the METS/ALTO OCR, amount to many tens of terabytes of data, questions of data storage became major considerations in the pipeline. I chose to save the extracted visual content as lower-resolution JPEG images in order to reduce the upload time and lessen the storage burden. Though the <hi rend="italic">Newspaper
                Navigator</hi> dataset retains identifiers to all high-resolution pages in <hi rend="italic">Chronicling America, </hi>the images in the <hi rend="italic">Newspaper Navigator </hi>dataset are altered by the downsampling procedure. This downsampling procedure should be free of any significant biasing effects. </p>
         <p>For visual content recognition, Newspaper Navigator utilized an object detection model, which is a type of widely-used computer vision technique for identifying objects in images. The performance for computer vision techniques is regularly measured using metrics such as average
                    precision. For Newspaper
                    Navigator, the model’s
                    performance on a specific page, as measured by average precision, is
                    dependent on a confluence of factors. These factors include the page’s layout, artifacts and distortions introduced in the microfilming and digitization process, and - most importantly - the composition of the training data. Thus, each image is “seen” differently by the visual content recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified by the visual content recognition model and saved in the <hi rend="italic">Newspaper Navigator </hi>dataset. Each image is cropped slightly differently. In the case of the image from the <hi rend="italic">Iowa State
                    Bystander</hi>, extra text is included, while in the case of the images from <hi rend="italic">The Broad
                    Ax</hi>, the captions are partially cut off. The loss in image quality is due to the aforementioned downsampling performed by the pipeline. This downsampling leads to artifacts such as the dots appearing on Du Bois’s face in the image from the <hi rend="italic">Iowa State
                    Bystander</hi>, as well as the streaks in the image from <hi rend="italic">Franklin’s Paper the Statesman</hi>, that are not present in Figure 2.</p>
         <p>Returning to the question of the visual content recognition model’s performance on pages published outside of the temporal range of the training data (1914-1918), it is possible to provide a quantitative answer by measuring average precision on test sets of annotated pages from different periods of time. In [Lee et al. 2020], I describe this analysis in detail and demonstrate that the performance declines for pages published between 1875 and 1900 and further declines for pages published between 1850 and 1875. This confirms that the composition of the training data directly manifests in the model’s performance. While it is certainly the case that the <hi rend="italic">Newspaper
                    Navigator</hi> dataset can still be used for scholarship related to 19th century newspapers in <hi rend="italic">Chronicling
                    America</hi>, any scholarship with the 19th century visual content in the <hi rend="italic">Newspaper Navigator </hi>dataset must consider how the dataset may skew what
                    visual content is represented.</p>
         <figure>
            </figure>
         <p>Figure 5. The four images of W.E.B. Du Bois, as identified by the visual content recognition model and included in the <hi rend="italic">Newspaper Navigator </hi>dataset [Newspaper Navigator 1910a]; [Newspaper Navigator
                    1910c]; [Newspaper Navigator 1910e]; [Newspaper Navigator 1910g].</p>
         <p>Let me conclude this section with a discussion of the act of visual content extraction itself in relation to digitization. While this extraction enables a wide range of affordances for searching <hi rend="italic">Chronicling
                    America</hi>, it is also an act of decontextualization: visual content no longer appears in relation to the <hi rend="italic">mise-en-page</hi>. In the Appendix, the full pages containing the photographs of W.E.B. Du Bois are reproduced, showing each photograph in context. Only by examining the full pages does it become clear that the article featuring W.E.B. Du Bois was printed with a second article in the <hi rend="italic">Iowa State
                    Bystander</hi> and <hi rend="italic">The Broad
                    Ax</hi>, the headline of which reads: “ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active Campaign Against Injustice.” Furthermore, upon examination, the <hi rend="italic">Iowa State Bystander </hi>front page features the article on <hi rend="italic">The
                    Crisis</hi> and W.E.B. Du Bois as the most prominent article of the issue. Though links between the extracted visual content and the original <hi rend="italic">Chronicling
                    America</hi> pages are always retained, this decontextualization inevitably transmutes <hi rend="italic">how</hi> we perceive and interact with the visual content in <hi rend="italic">Chronicling
                    America</hi>. Indeed, all uses of machine learning for metadata enhancement are a form of decontextualization, centering the user’s discovery and analysis of content around the metadata itself. <note>
               <date when="2021-07-30T05:35:00Z"/>I added this sentence as a connective to
                    tie back to the central argument.</note>
         </p>
         <p>VII. Prediction
                    Uncertainty</p>
         <p>Perhaps the most fundamental question to ask of the <hi rend="italic">Newspaper Navigator </hi>dataset is: “How many photographs does the dataset contain?” Because the dataset has been constructed using a machine learning model, predictions are ultimately probabilistic in nature, quantified by the confidence score returned by the model. This begs the question of what counts as an identified unit of visual content: a user is much more inclined to tally a prediction of a map if it has an associated confidence score of 99% rather than 1%. However, choosing this cut is fundamentally a subjective decision, informed by the user’s end goals with the dataset. In the language of machine learning, picking a stringent confidence cut (i.e., only counting predictions with high confidence scores) emphasizes <hi rend="italic">precision</hi>: a prediction of a photograph likely corresponds to a true photograph, but the predictions will suffer from false negatives. Conversely, picking a loose confidence cut (i.e., counting predictions with low confidence scores) emphasizes <hi rend="italic">recall</hi>: most true photographs are identified as such, but the predictions will suffer from many false positives. In this regard, the total number of images in the <hi rend="italic">Newspaper Navigator </hi>dataset is dependent on one’s desired tradeoff between precision and recall. In Table 2, I show the dynamic range of the dataset size, as induced by three different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects of different cuts on confidence score for the page featuring W.E.B. Du Bois in the November 26,1910, issue of <hi rend="italic">The Broad Ax</hi>.</p>
         <table>
            <row role="data">
               <cell>Category</cell>
               <cell>≥ 90%</cell>
               <cell>≥ 70%</cell>
               <cell>≥ 50%</cell>
            </row>
            <row role="data">
               <cell>Photograph</cell>
               <cell>1.59 x 106</cell>
               <cell>2.63 x 106</cell>
               <cell>3.29 x 106</cell>
            </row>
            <row role="data">
               <cell>Illustration</cell>
               <cell>8.15 x 105</cell>
               <cell>2.52 x 106</cell>
               <cell>4.36 x 106</cell>
            </row>
            <row role="data">
               <cell>Map</cell>
               <cell>2.07 x 105</cell>
               <cell>4.59 x 105</cell>
               <cell>7.54 x 105</cell>
            </row>
            <row role="data">
               <cell>Comic/Cartoon</cell>
               <cell>5.35 x 105</cell>
               <cell>1.23 x 106</cell>
               <cell>2.06 x 106</cell>
            </row>
            <row role="data">
               <cell>Editorial
                            Cartoon</cell>
               <cell>2.09 x 105</cell>
               <cell>6.67 x 105</cell>
               <cell>1.27 x 106</cell>
            </row>
            <row role="data">
               <cell>Headline</cell>
               <cell>3.44 x 107</cell>
               <cell>5.37 x 107</cell>
               <cell>6.95 x 107</cell>
            </row>
            <row role="data">
               <cell>Advertisement</cell>
               <cell>6.42 x 107</cell>
               <cell>9.48 x 107</cell>
               <cell>1.17 x 108</cell>
            </row>
            <row role="data">
               <cell>
                  <hi rend="italic">Total</hi>
               </cell>
               <cell>1.02 x 108</cell>
               <cell>1.56 x 108</cell>
               <cell>1.98 x 108</cell>
            </row>
         </table>
         <p>Table
                    2. The number of occurrences of each category of visual content in the <hi rend="italic">Newspaper
                Navigator</hi> dataset with confidence scores above the listed thresholds (0.9, 0.7, 0.5).</p>
         <figure>
            <figure>
                </figure>
            <figure>
                </figure>
         </figure>
         <p>Figure
                    6. The same page of <hi rend="italic">The Broad Ax </hi>from November 26, 1910, along with predictions from the
                    visual content recognition model, thresholded on confidence score at 5%, 50%,
                    70%, and 90% [Newspaper Navigator 1910g]; [Newspaper Navigator 1910h]. Note that
                    red corresponds to a prediction of “photograph”, cyan corresponds to a
                    prediction of “headline”, and blue corresponds to a prediction of
                    “advertisement”.</p>
         <p>Rather than pre-selecting a confidence score threshold, the <hi rend="italic">Newspaper Navigator </hi>dataset contains all predictions with confidence scores
                    greater than 5%,<note> This modest cut is provided to remove the large number of predictions with confidence scores between 0% and 5%, which have high false-positive rates, and thus reduce the size of the <hi rend="italic">Newspaper Navigator </hi>dataset.</note>
                allowing the user to define their own confidence cut when querying the dataset. However, the website for the <hi rend="italic">Newspaper
                    Navigator</hi> dataset also includes hundreds of pre-packaged datasets in order to make it easier for users to work with the dataset. In particular, users can download zip files containing all of the visual content of a specific type with confidence scores greater than or equal to 90%, for any year from 1850 to 1963. I made this choice of 90% as the threshold cut for these pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged datasets by eye. However, as articulated above, based on different use cases, this cut of 90% may be too restrictive or permissive: relevant visual content may be absent from the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the visual content recognition model’s confidence scores for the four images of W.E.B. Du Bois described throughout this data archaeology. The effect of a cut on confidence score can be seen here: selecting a cut of 95% would exclude the image from <hi rend="italic">Franklin’s Paper the
                Statesman</hi>. I raise this point to emphasize that even this seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery process and thus can have an impact on scholarship. </p>
         <figure>
            </figure>
         <p>Figure
                7. The visual content recognition model’s confidence score for each of the four images of W.E.B. Du Bois. Note how the model assigns a different confidence score to each identified image [Newspaper Navigator 1910b]; [Newspaper Navigator 1910d]; [Newspaper Navigator 1910f]; [Newspaper Navigator 1910h].</p>
         <p>Just as the bounding box predictions themselves are affected by the training data, as well as newspaper page layout, date of publication, and noise from the digitization pipeline, so too are the confidence scores. In particular, the visual content recognition model suffers from high-confidence misclassifications, for example, crossword puzzles that are identified as maps with confidence scores greater than 90%. High-confidence misclassifications pose challenges for machine learning writ large, and the field of explainable artificial intelligence is largely devoted to developing tools for understanding this type of misclassification [Weld and Bansal 2019]. However, these high-confidence misclassifications can often be traced back to the composition of the training set. For example, the fact that the visual content recognition model sometimes identifies crossword puzzles as maps with high confidence is likely due to the fact that the training data did not contain enough labeled examples of maps and crossword puzzles for the visual content recognition model to differentiate them with high accuracy. </p>
         <p>The questions surrounding confidence scores and probabilistic descriptions of items is by no means restricted to the 
                <hi rend="italic">Newspaper Navigator </hi>dataset. I echo Thomas Padilla’s assertion that “attempts
                    to use algorithmic methods to describe collections must embrace the reality
                    that, like human descriptions of collections, machine descriptions come with
                    varying measure of certainty” [Padilla 2019]. Machine-generated metadata such as
                    OCR are also fundamentally probabilistic in nature; this fact is not immediately
                    apparent to end users of cultural heritage collections because cuts on
                    confidence score are typically chosen before surfacing the metadata. Effectively
                    communicating confidence scores, probabilistic descriptions, and the decisions
                    surrounding them to end users remains a challenge for content stewards.</p>
         <p>VIII. OCR
                    Extraction</p>
         <p>In the <hi rend="italic">Newspaper Navigator </hi>pipeline, a textual description of each prediction is obtained by extracting the OCR within each predicted bounding box. The resulting textual description is thus dependent on not only the OCR provided by <hi rend="italic">Chronicling America </hi>but also the exact coordinates of the bounding box: if
                    the coordinates of a word in the localized OCR extend beyond the bounds of the
                    box, the word is excluded. I experimented with utilizing tolerance limits to
                    allow words that extend just beyond the bounds of the boxes to be included, but
                    doing so ultimately introduces false positives as well, as words from
                    neighboring articles or visual content were inevitably included some fraction of
                    the time. Once again, the tradeoff between false positives and false negatives
                    is manifest.</p>
         <p>In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois, as identified by the <hi rend="italic">Newspaper
                    Navigator</hi> pipeline. Significantly, in the <hi rend="italic">Newspaper Navigator </hi>dataset, the OCR is stored as a list of words, with line breaks removed; these lists are what appear in Figure 8. These four examples provide intuition as to how the captions are altered. While the examples from the <hi rend="italic">Iowa State Bystander </hi>and <hi rend="italic">Franklin’s Paper the
                    Statesman</hi> both have the same captions as shown in Figure 3, the captions for both of the examples from <hi rend="italic">The Broad
                    Ax</hi> are unrecognizable. Because the bounding boxes have clipped the caption, none of the characters from the proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR noise due to the OCR engine attempting to read text from the photographs. Consequently, the mentions of W.E.B. Du Bois are erased from the textual descriptions in the <hi rend="italic">Newspaper Navigator </hi>dataset. The visual content in the <hi rend="italic">Newspaper Navigator </hi>dataset is thus decontextualized not only in the sense that the visual content is extracted from the newspaper pages but also in the sense that the OCR extraction method further alters the textual descriptions. While the images from the <hi rend="italic">Iowa State
                    Bystander</hi> and <hi rend="italic">Franklin’s Paper the
                    Statesman</hi> are still recoverable with fuzzy keyword search, the two images from <hi rend="italic">The Broad Ax </hi>are impossible to retrieve with <hi rend="italic">any </hi>form of keyword search, revealing another instance in
                    which employing automated techniques for collections processing affects
                    discoverability.</p>
         <figure>
            </figure>
         <p>Figure 8. The textual descriptions of each image, as extracted from the OCR and saved in the <hi rend="italic">Newspaper Navigator </hi>dataset [Newspaper Navigator 1910b]; [Newspaper Navigator
                    1910d]; [Newspaper Navigator 1910f]; [Newspaper Navigator 1910h].</p>
         <p>Fortunately, visual content can still be recovered using
                    similarity search over the images themselves; these methods are discussed in
                    detail in the next section. However, in the case of headlines, the errors
                    introduced by OCR engines and the subsequent OCR extraction have no recourse, as
                    similarity search for images of headlines would only capture similar typography
                    and text layout.<note> The<hi rend="italic"> Newspaper Navigator </hi>dataset does not retain the cropped images of
                            headlines, as the textual content is more salient than visual snippets
                            in the case of headlines.</note>
         </p>
         <p>To illustrate the effects of this OCR extraction on headlines, I reproduce in Table 3 the extracted OCR as it appears in the <hi rend="italic">Newspaper Navigator </hi>dataset for Franklin F. Johnson’s headline: </p>
         <p>NEW MOVEMENT </p>
         <p>BEGINS WORK</p>
         <p>Plan and Scope of the
                    Asso-</p>
         <p>ciation Briefly Told. </p>
         <p>Will Publish the Crisis. </p>
         <p>Review of Causes Which Led to the </p>
         <p>Organization of the Association in </p>
         <p>New York and What Its Policy Will </p>
         <p>Be-Career and Work of Professor </p>
         <p>W.E.B. Du Bois</p>
         <table>
            <row role="data">
               <cell>
                  <p>
                     <hi rend="italic">Iowa State Bystander </hi>
                  </p>
                  <p>(14 Oct. 1910)</p>
               </cell>
               <cell>
                  <p>
                     <hi rend="italic">Franklin’s Paper the Statesman</hi>
                  </p>
                  <p>(15 Oct. 1910)</p>
               </cell>
               <cell>
                  <p>
                     <hi rend="italic">The Broad Ax </hi>
                  </p>
                  <p>(15 Oct. 1910)</p>
               </cell>
               <cell>
                  <p>
                     <hi rend="italic">The Broad Ax </hi>
                  </p>
                  <p>(26 Nov. 1910)</p>
               </cell>
            </row>
            <row role="data">
               <cell>98.72%</cell>
               <cell>99.57%</cell>
               <cell>99.76%</cell>
               <cell>99.70%</cell>
            </row>
            <row role="data">
               <cell>
                  <p>["NEW", "MOVEMENT", "BEGINS", "WORK", "and",
                            "Plan", "Scope", "of", "the", "Asso\u00ad", "ciation", "Briefly",
                            "Told.", "WILL", "PUBLISH", "THE", "CRISIS.", "Review", "of", "Causae",
                            "Which", "Lad", "to", "the", "Organisation", "of", "the", "Auooiation",
                            "In", "Naw", "York", "and", "JWhat", "It*", "Polioy", "Will",
                            "Ba\u2014Career", "and", "Wark", "of", "Profeasor"]</p>
               </cell>
               <cell>
                  <p>["NEW", "MOVEMENT", "BEGINS", "WORK", "Plan",
                            "and", "Scope", "of", "the", "Asso", "ciation", "Briefly", "Told.",
                            "WILL", "PUBLISH", "THE", "CRISIS."]</p>
               </cell>
               <cell>
                  <p>["NEW", "MOVEMENT", "BEGINS", "WORK", "Plan",
                            "and", "Sep", "if", "the", "Asso", "ciation", "Briefly", "Told.",
                            "WILL", "PUBLISH", "THE", "CRISIS,", "Be", "Career", "nnd", "Work",
                            "of", "Professor", "W.", "E.", "B.", "Du", "Bois.", "Review", "of",
                            "Causes", "Which", "Led", "to", "the", "Oraanteallon", "of", "th.",
                            "A.Me!.!?n", "i", "i", "New", "York", "and", "What", "IU", "Policy",
                            "Will"]</p>
               </cell>
               <cell>
                  <p>["NEW", "MOVEMENT", "BEGINS", "WORK", "Plan",
                            "and", "Scope", "of", "the", "Asso", "ciation", "Briefly", "Told.",
                            "WILL", "PUBLISH", "THE", "CRISIS.", "Review", "of", "Causes", "Which",
                            "Lad", "to", "tha", "Organization", "of", "the\"", "Association", "In",
                            "New", "York", "and", "What", "Its", "Policy", "Will"]</p>
               </cell>
            </row>
         </table>
         <p>Table 3. The extracted OCR associated with each of the four
                    photographs of W.E.B. Du Bois [Newspaper Navigator 1910b]; [Newspaper Navigator
                    1910d]; [Newspaper Navigator 1910f]; [Newspaper Navigator 1910h].</p>
         <p>The full pages are reproduced in the appendix for reference. Notably, all four extracted headlines contain OCR errors, as well as missing words due to the OCR extraction. The visual content recognition model consistently fails to include the last line of the headline, “W.E.B. Du Bois,” revealing another case in which Du Bois’s name is rendered inaccessible by keyword search in the <hi rend="italic">Newspaper Navigator </hi>dataset. </p>
         <p>IX. Image
                    Embeddings</p>
         <p>An <hi rend="italic">image
                    embedding</hi> canonically refers to a low-dimensional representation of an image, often a list of a few hundred or a few thousand numbers, that captures much of the image’s semantic content. Image embeddings are typically generated by feeding an image into a pre-trained neural image classification model (i.e., a model that takes in an image and outputs a label of “dog” or “cat”) and extracting a representation of the image from one of the model’s hidden layers, often the penultimate layer.<note> If these words are unfamiliar, the three takeaways listed are more important.</note>
                Image embeddings are valuable for three reasons:</p>
         <list type="ordered">
            <item>Image embeddings are remarkably adept at capturing
                        semantic similarity between images. For example, images of dogs tend to be
                        clustered together in embedding space, with images of bicycles in another
                        cluster and images of buildings in yet another. These clusters can be
                        fine-grained: sometimes, the red bicycles are grouped closer together than
                        the blue bicycles.</item>
            <item>Image embeddings can be constructed by feeding images into an image classification model already trained on another dataset (such as ImageNet), meaning that generating image embeddings is a useful method for comparing images without having to construct training data by labeling images. </item>
            <item>Image embeddings are low-dimensional and thus much
                        smaller in size than the images themselves (i.e., on the order of kilobytes
                        instead of megabytes). As a result, image embeddings are much less
                        computationally expensive to compare to one another when conducting
                        similarity search, clustering, or related tasks. In short, image embeddings
                        speed up image comparison.</item>
         </list>
         <p>Utilizing image embeddings to visualize and explore large
                    collections of images has become an increasingly common approach among cultural
                    heritage practitioners. Projects and institutions that have utilized image
                    embeddings for visualizing cultural heritage collections include the Yale
                    Digital Humanities Lab’s PixPlot interface [Yale Digital Humanities Lab 2017],
                    the National Neighbors project [Lincoln et al. 2019], Google Arts and Culture
                    [Google Arts and Culture 2018], The Norwegian National Museum’s Principal
                    Components project [
                Nasjonalmuseet
                    2017], the State Library of New South Wales’s
                    Aero Project [Geraldo 2020], the Royal Photographic Society [Vane 2018], The
                    American Museum of Natural History [Foo 2019], and The National Library of the
                    Netherlands [Lonij and Weavers 2017]; [Weavers and Smits 2020]. These
                    visualizations provide insights into broader themes in the collections, thereby
                    allowing curators, researchers, and the public to explore collections at a scale
                    previously only possible by organizing images by color or other low-level
                    features.<note> For an introduction to some of these methods with lower-level features, see [Manovich 2012].</note>
                In this regard, image embeddings provide new affordances
                    for searching over images that complement canonical faceted and keyword
                    search.</p>
         <p>Because these image embeddings enable these visualization approaches and open the door to similarity search and recommendation, I opted to include image embeddings as part of the <hi rend="italic">Newspaper
                    Navigator</hi> pipeline. Indeed, these image embeddings power the similarity search functionality in the <hi rend="italic">Newspaper
                    Navigator</hi> user interface and, in this regard, are crucial to the broader vision of the project [Lee and Weld 2020].<note> The search application can be found at: <ref target="https://news-navigator.labs.loc.gov/search">https://news-navigator.labs.loc.gov/search</ref>.</note>
                To generate the embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep learning architecture for image classification, both of which had already been pre-trained on ImageNet [He et al. 2016]. </p>
         <p>ImageNet is perhaps the most well-known image dataset in the history of machine learning. Constructed by scraping publicly
                    available images from the internet and recruiting Amazon Mechanical Turk workers
                    to annotate the images, ImageNet contains approximately 14 million images across
                    20,000 categories [Deng et al. 2009]; [ImageNet 2020]. Kate Crawford and Trevor
                    Paglen’s essay “Excavating AI: The Politics of Images in Machine Learning
                    Training Sets” offers a history and incisive critique of the classification
                    schema of ImageNet; here, I will summarize the most salient critiques. First,
                    many of the categories in the taxonomy utilized are themselves marginalizing
                    [Crawford and Paglen 2019]. Though many of the classes relating to people were
                    removed in 2019, ImageNet had previously bifurcated the “Natural Object &gt;
                    Body &gt; Adult Body” category into “Male Body” and “Female Body” subcategories.
                    Second, ethnic classes were included, implying that 1) classification into rigid
                    categories of ethnicity is possible and appropriate and 2) a machine learning
                    system could learn how to classify ethnicity from these images. Diving deeper,
                    the classifications become horrifying in their supposed granularity: until 2019,
                    an image of a woman in a bikini was accompanied with the tags “slattern, slut,
                    slovenly woman, trollop” [Crawford and Paglen
                    2019]. Though many embedding models are pre-trained on subsets of ImageNet categories included in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly troubling classifications, these classifications nonetheless necessitate a reckoning with our use of ImageNet writ large, especially in regard to how the semantics of ImageNet is projected onto any image embedding generated with such a model
                    [Russakovsky et al. 2015].<note> The specific categories used in the challenge can be found at: <ref target="http://image-net.org/challenges/LSVRC/2010/browse-synsets">http://image-net.org/challenges/LSVRC/2010/browse-synsets</ref>.</note>
         </p>
         <p>However, questions probing the data in ImageNet fail to critique the ethically questionable practices on which ImageNet is built. Though the researchers responsible for the dataset scraped all 14 million images from public URLs, ImageNet does not provide any guarantees on image copyright, as only the URLs are provided in the database:
                “The images in their original resolutions may be subject to copyright, so we do not make them publicly available on our server” [ImageNet: What about the Images? 2020]. It is highly unlikely that a photographer with an image in the dataset could have known that a photograph could be used this way, much less actively consent to the image’s inclusion, as is the case with subjects in the photographs. Furthermore, the labels themselves were collected using Amazon’s Mechanical Turk platform, which has been repeatedly criticized for its exploitative labor practices: as of 2017, workers earned a median wage of approximately $2 an hour on the platform [Haro et al. 2018]. Scholars including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced marginalized labor underpins digitization efforts, and the reliance on Mechanical Turk for the production of ImageNet further entrenches the digitization and discovery process within a system of labor exploitation [Cecire 2011]; [Mak 2014]; [Fyfe 2016]. As cultural heritage practitioners and humanities researchers, we must acknowledge these exploitative practices, and we must reckon with how we perpetuate them through the use of ImageNet as a training source for image search and discovery.  </p>
         <p>In offering these critiques, my
                    intention is not to dismiss ImageNet in a wholesale manner. Certainly, the
                    benefits of utilizing ImageNet are manifold, as evidenced by widespread
                    community adoption, as well as new affordances for searching cultural heritage
                    collections enabled by the dataset that are shaping the contours of digital
                    scholarship. In the case of my own scholarship with Newspaper Navigator, I have
                    elected to utilize machine learning models pre-trained on ImageNet precisely for
                    these reasons. I offer these provocations instead to question how we can do
                    better as a community, not only in imagining alternatives but in bringing them
                    to fruition. Classification is an act of interpretive reduction, whether by
                    human or machine, and thus manifests all too often as an act of
                    oppression.<note> For more reading on this topic, see [Bowker and Star 2000].</note>
                And yet, the structure imposed by
                    classification constitutes the very basis for search and discovery systems. The
                    salient question is thus not how we dispense of these systems but rather how we
                    progressively realize a more inclusive vision of these systems, from the labor
                    practices behind their construction to the very classification taxonomies
                    themselves.</p>
         <p>How, then, do image embeddings derived from ImageNet mediate our interactions with the photographs in <hi rend="italic">Newspaper
                    Navigator</hi>?  Figure 9 shows a visualization of 1,000 photographs from the <hi rend="italic">Newspaper Navigator </hi>dataset published during the year 1910. This visualization was created using the ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known as T-SNE [van der Maaten and Hinton 2009]. With T-SNE, a cluster of photographs indicates that the photographs are likely semantically similar, but the size of the cluster and distances from other clusters bear no meaning [Wattenberg, Viégas, and Johnson 2016] . With this in mind, we can examine the clusters. Despite the fact that the high-contrast, grayscale photographs in <hi rend="italic">Newspaper
                Navigator</hi> are markedly different, or “out-of-sample,” in comparison to the clear, color images in ImageNet, the clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering of photographs depicting crowds of people, as well as photographs depicting ships and the sea. This visualization technique with the image embeddings is thus powerful in helping to navigate large collections of photographs by their semantic content.</p>
         <figure>
            <lb/>
         </figure>
         <p>Figure
                    9. A visualization of 1,000 photographs from the year 1910 in the <hi rend="italic">Newspaper Navigator </hi>dataset, generated using the <hi rend="italic">Newspaper
                Navigator</hi> ResNet-50 image embeddings.</p>
         <p>What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters containing these four photographs. This visualization affords us a lens into the limitations of image embeddings. First, it is evident that image embeddings are directly impacted by the distortions of the digitization process: while the three photographs from <hi rend="italic">Franklin’s Paper the Statesman </hi>and <hi rend="italic">The Broad
                    Ax</hi> are clustered together with other portraits, the photograph from the <hi rend="italic">Iowa State
                    Bystander</hi> is located in an entirely different cluster - a consequence of the fact that the <hi rend="italic">Iowa State
                    Bystander</hi> photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably, neighboring photographs suffer from similar distortions). A search engine powered with these image embeddings would in all likelihood return the three photographs from <hi rend="italic">Franklin’s Paper the
                    Statesman</hi> and <hi rend="italic">The Broad
                    Ax</hi> together, but the fourth photograph would effectively be lost. This algorithmic mediation is particularly troubling because, as described in Section IV, the microfilming digitization process causes newspaper photographs of darker-skinned people to lose contrast. While this loss in image quality is marginalizing in its own right, image embeddings perpetuate this marginalization: digitized newspaper portraits of darker-skinned individuals are more likely to suffer from saturated facial features, in turn resulting in these photographs being lost during the discovery and retrieval process, as is the case with the saturated <hi rend="italic">Iowa State Bystander </hi>photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image embeddings are particularly relevant in the case of <hi rend="italic">Newspaper
                    Navigator</hi>, as these image embeddings power the visual similarity search affordance within the publicly-deployed <hi rend="italic">Newspaper Navigator </hi>search application [Lee and Weld 2020]. Though machine learning methods are often offered as panaceas for automation, this algorithmic erasure reminds us that traditional methods of scholarship and historiography, such as detailed analyses and close readings of Black newspapers in <hi rend="italic">Chronicling America</hi>, are more important than ever to counter algorithmic
                    bias.</p>
         <figure>
            </figure>
         <p>Figure
                10. The same visualization as in Figure 9, this time showing the locations of the four photographs of W.E.B. Du Bois.</p>
         <p>X. Environmental
                    Impact</p>
         <p>Any examination of a dataset whose construction required
                    large-scale computing would be remiss in not investigating the environmental
                    impact of the computation itself. The carbon emissions generated from training a
                    state-of-the-art machine learning model such as BERT is comparable to a single
                    flight across the United States; however, factoring in experimentation and
                    tuning, the carbon emissions can quickly amount to the carbon emissions of a car
                    over its entire lifetime, including fuel [Strubell et al. 2019]. OpenAI’s GPT-3
                    model required several thousand petaflop/s-days to train; without specific
                    numbers, the carbon emissions are not possible to calculate exactly, but they
                    are nonetheless substantial [Brown et al. 2020]. In response, machine learning
                    researchers have recommended ideas such as “Green AI,” with the goal of
                    encouraging the community to value computational efficiency and not just
                    accuracy [Schwartz et al. 2019].</p>
         <p>In the case of <hi rend="italic">Newspaper
                    Navigator</hi>, most of the compute time was devoted to processing all 16.3 million <hi rend="italic">Chronicling
                    America</hi> pages with the visual content recognition model, as opposed to training the model itself. In Tables 4 and 5, I report details on training the model and running the pipeline, as well as the carbon emissions generated by each step, computed using the Machine Learning Impact Calculator [Lacoste et al. 2019]. In total, approximately 380 kg CO2 were emitted during the construction of the <hi rend="italic">Newspaper Navigator </hi>dataset, including development, experimentation, training, pipeline processing, and post-processing. It should be noted that this number is an estimate, as the statistics for experimentation and post-processing are difficult to quantify exactly. Nonetheless, this is approximately equivalent to the carbon emissions incurred by a single person flying from Washington, D.C., to Boston [Carbon Footprint Calculator no date]. I include these numbers in the hope that cultural heritage practitioners will consider the environmental impact of utilizing machine learning and artificial intelligence for digital content stewardship. Doing so is essential to the data archaeology: given that climate change will disproportionately affect cultural heritage institutions in regions unable to develop proper infrastructure to withstand rapid temperature fluctuations and unprecedented flooding, even the environmental impacts of utilizing machine learning within digital content stewardship has the capacity to contribute to erasure and marginalization. <note>
               <date when="2021-07-30T05:40:00Z"/>I added this to tie this section back to
                    the central theme of erasure.</note>
         </p>
         <table>
            <row role="data">
               <cell>Activity</cell>
               <cell># of NVIDIA T4
                            GPUs</cell>
               <cell>GPU Hours
                            (each)</cell>
               <cell>Carbon
                            Emissions</cell>
            </row>
            <row role="data">
               <cell>Training</cell>
               <cell>1</cell>
               <cell>19</cell>
               <cell>0.96 kg CO2</cell>
            </row>
            <row role="data">
               <cell>Pipeline
                            Processing</cell>
               <cell>8</cell>
               <cell>456</cell>
               <cell>144.56 kg
                            CO2</cell>
            </row>
            <row role="data">
               <cell>Experimentation for
                            Training and Pipeline Processing (estimate)</cell>
               <cell>8</cell>
               <cell>24</cell>
               <cell>7.66 kg CO2</cell>
            </row>
            <row role="data">
               <cell>
                  <hi rend="italic">Total</hi>
               </cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>153.18 kg
                            CO2</cell>
            </row>
         </table>
         <p>Table
                    4. Carbon emissions from the GPU usage for <hi rend="italic">Newspaper Navigator</hi>, broken down by project component. Note that all
                    computation was done on Amazon AWS g4dn instances in the zone “us-east-2”. The
                    carbon emissions were calculated using the Machine Learning Impact Calculator
                    [Lacoste et al. 2019].</p>
         <table>
            <row role="data">
               <cell>Activity</cell>
               <cell>CPU Processor
                            (#)</cell>
               <cell># Processor CPU
                            Cores</cell>
               <cell>CPU Hours
                            (each)</cell>
               <cell>Carbon
                            Emissions</cell>
            </row>
            <row role="data">
               <cell>Training</cell>
               <cell>1</cell>
               <cell>4 CPUs</cell>
               <cell>19</cell>
               <cell>1.13 kg CO2</cell>
            </row>
            <row role="data">
               <cell>Pipeline
                            Processing</cell>
               <cell>2</cell>
               <cell>48 CPUs</cell>
               <cell>456</cell>
               <cell>181.9 kg CO2</cell>
            </row>
            <row role="data">
               <cell>Experimentation for
                            Training and Pipeline Processing (<hi rend="italic">estimate</hi>)</cell>
               <cell>2</cell>
               <cell>48 CPUs</cell>
               <cell>24</cell>
               <cell>9.57 kg CO2</cell>
            </row>
            <row role="data">
               <cell>Extra Computation (dataset post-processing, etc., <hi rend="italic">estimate</hi>)</cell>
               <cell>1</cell>
               <cell>48 CPUs</cell>
               <cell>168</cell>
               <cell>33.52 kg CO2</cell>
            </row>
            <row role="data">
               <cell>
                  <hi rend="italic">Total</hi>
               </cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>226.12 kg
                            CO2</cell>
            </row>
         </table>
         <p>Table
                    5. Carbon emissions from the CPU usage for <hi rend="italic">Newspaper Navigator</hi>, broken down by project component. Note that all
                    computation was done on Amazon AWS g4dn instances in the zone “us-east-2”. The
                    CPU processors are all 2nd generation Intel Xeon Scalable Processors (Cascade
                    Lake) [Amazon Web Services, Inc. 2020]. The 48-core processor outputs
                    approximately 350 W; the 4-core processor outputs approximately 104 W [Intel
                    2020a]; [Intel 2020b]. The carbon emissions were calculated using the Machine
                    Learning Impact Calculator [Lacoste et al. 2019]. Note that the energy
                    consumption by RAM is not factored in, but it is insignificant in comparison to
                    the CPU and GPU energy consumption.</p>
         <p> XI. Conclusion</p>
         <p>In this data archaeology, I have traced four <hi rend="italic">Chronicling
                    America</hi> pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through the <hi rend="italic">Chronicling
                    America</hi> and <hi rend="italic">Newspaper
                    Navigator</hi> pipelines. The excavated genealogy of digital artifacts has revealed the imprintings of the complex interactions between humans and machines. Indeed, the journey of each newspaper page through the <hi rend="italic">Chronicling America </hi>and <hi rend="italic">Newspaper
                    Navigator</hi> pipelines is one of refraction, mediation, and decontextualization that is compounded upon with each step. Decisions made decades ago when microfilming a newspaper page inevitably affect how the machine learning models employed for OCR, visual content extraction, and image embedding generation ultimately process the pages, render them as digital artifacts in the <hi rend="italic">Newspaper Navigator </hi>dataset, and mediate their discoverability.</p>
         <p>As articulated by Trevor Owens in <hi rend="italic">The Theory and Craft of Digital
                    Preservation</hi>, machine learning and artificial
                    intelligence are the “underlying sciences for digital preservation” [Owens
                    2018]. Though machine learning techniques provide us with new affordances for
                    searching and studying cultural heritage materials, they have the power to
                    perpetuate and amplify the marginalization and erasure of entire communities
                    within the archive. This erasure, coupled with the labor practices involved in
                    creating training data as well as the environmental impact of training and
                    deploying machine learning models in large-scale digitization pipelines,
                    necessitates that we continue to examine the broader socio-technical ecosystems
                    in which we participate. In doing so, we can work toward a more inclusive vision
                    of the digital collection and the ways in which we render its contents
                    discoverable.</p>
         <p>How, then, is <hi rend="italic">Newspaper
                    Navigator</hi> situated within this vision? In reimagining how we search over the visual content in <hi rend="italic">Chronicling
                    America</hi>, one explicit goal of the project is to engage the public with the rich history preserved within historic American periodicals and thus build on <hi rend="italic">Chronicling America</hi> as a free-to-use, public domain resource for scholars, educators, students, journalists, genealogists, and beyond [Lee, Berson, and Berson 2021]. [Lee et al. 2021]. With 
                <hi rend="italic">Newspaper
                    Navigator</hi>, it is my belief that the new modes of interacting with <hi rend="italic">Chronicling
                    America</hi> have the capacity to not only enable a breadth of new scholarship but also foster engagement in and reckoning with America’s multilayered history of oppression. In documenting the different components of the project with this data archaeology and corresponding technical paper [Lee et al. 2020], as well as releasing the full dataset and all code into the public domain, I have intended to be as transparent as possible with the tools and methodologies employed. <hi rend="italic">Newspaper Navigator </hi>is not without its shortcomings, but my hope is that the
                    project contributes to this vision of the digital collection through
                    transparency and inclusivity, as well as the scholarship and pedagogy that it
                    has enabled.</p>
         <p>I offer this case study not only to contextualize the <hi rend="italic">Newspaper Navigator </hi>dataset but also to advocate for the autoethnographic data archaeology as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic perspective. Though the digital humanities community has yet to adopt the data archaeology as standard practice when creating and releasing cultural heritage datasets, doing so has the capacity to improve accountability and context surrounding applications of machine learning for both practitioners and end users. Given the manifold ways in which machine learning mediates access to the archive and perpetuates erasure, reflecting critically on these systems is not only urgent but essential for transparency and inclusivity. <note>
               <date when="2021-07-30T04:57:00Z"/>I added this paragraph to conclude the
                    article by returning to the central argument surrounding the data
                    archaeology.</note>
         </p>
         <p>
                Sources of Funding</p>
         <p>This material is based upon work supported by the National
                    Science Foundation Graduate Research Fellowship under Grant DGE-1762114, as well
                    as the Library of Congress Innovator in Residence Position.</p>
         <p>Acknowledgments</p>
         <p>I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter, Robin Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy and Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful and enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor, Daniel Weld, at the University of Washington, for his support, guidance, and invaluable advice with <hi rend="italic">Newspaper
                Navigator</hi>. In addition, I would like to thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity to formulate and write early sections of this data archaeology as part of this Spring’s CSE 599: “Computing for Social Good” course.  </p>
         <p>Lastly, I would like to thank the following people who have shaped <hi rend="italic">Newspaper
                    Navigator</hi>: Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael Haley Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter at Texas A&amp;M University-Corpus
                    Christi; and Gabriel Pizzorno at Harvard University. </p>
         <p>References</p>
         <p>[Alpert-Abrams 2016]. Alpert-Abrams, H. “Machine Reading the Primeros Libros,” <hi rend="italic">Digital Humanities
                Quarterly</hi> 10:4 (2016).</p>
         <p>[Amazon Web Services, Inc. 2020] “Amazon EC2 Instance
                    Types - Amazon Web Services,” (2020) Amazon Web Services, Inc.<ref target="https://aws.amazon.com/ec2/instance-types/"> Available at: </ref>
            <ref target="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</ref>. (Accessed: 5 June 2020).</p>
         <p>[Bailey 2015] Bailey, M. “#transform(Ing)DH Writing and Research: An Autoethnography of Digital Humanities and Feminist Ethics,” <hi rend="italic">Digital Humanities
                Quarterly</hi> 9:2 (2015).</p>
         <p>[Baker 2001] Baker, N. <hi rend="italic">Double Fold: Libraries and the Assault on
                    Paper</hi>. Random House (2001).</p>
         <p>[Barrall and Guenther 2005] Barrall, K. and Guenther, C. “Microfilm Selection for Digitization,” (2005). Available at: <ref target="https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf">https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</ref>.</p>
         <p>[Bender and Friedman 2018] Bender, E., and Friedman, B. “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.” <hi rend="italic">Transactions of the Association for
                    Computational
                    Linguistics</hi> 6 (2018): 587–604.<ref target="https://doi.org/10.1162/tacl_a_00041" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1162/tacl_a_00041">https://doi.org/10.1162/tacl_a_00041</ref> (Accessed 29 July 2021).</p>
         <p>[Bowker and Star 2000] Bowker, G., and Star, S. <hi rend="italic">Sorting Things Out: Classification and Its
                    Consequences</hi>. MIT Press, Cambridge
                (2000).</p>
         <p>[Brown et al. 2020] Brown, T., Mann, B., Ryder, N.,
                    Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
                    Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,
                    Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
                    Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford,
                    A., Sutskever, I., and
                    Amodei D. “Language Models Are Few-Shot Learners,” <hi rend="italic">ArXiv:2005.14165
                    [Cs]</hi> (2020),<ref target="http://arxiv.org/abs/2005.14165">  Available at: </ref>
            <ref target="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</ref> (Accessed: 6 June 2020).</p>
         <p>[Carbon Footprint Calculator no date]  “Carbon Footprint Calculator,” <ref target="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3">Available at: </ref>
            <ref target="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3">https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3</ref>. (Accessed: 6 June 2020).</p>
         <p>[Cecire 2011] Cecire, N. “Works Cited: The Visible Hand,” <hi rend="italic">Works
                    Cited</hi> (blog) (2011). Available at: <ref target="http://nataliacecire.blogspot.com/2011/05/visible-hand.html">http://nataliacecire.blogspot.com/2011/05/visible-hand.html</ref>.</p>
         <p>[Chronicling America no date] “Chronicling America | Library of Congress,” Available at: <ref target="https://chroniclingamerica.loc.gov/about/">https://chroniclingamerica.loc.gov/about/</ref> (Accessed 3 July 2020).</p>
         <p>[Cordell 2017] Cordell, R. “‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,” <hi rend="italic">Book
                    History</hi> 20:1, pp. 188–225 (2017). Available at:<ref target="https://doi.org/10.1353/bh.2017.0006" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1353/bh.2017.0006">https://doi.org/10.1353/bh.2017.0006</ref>.</p>
         <p>[Cordell 2020] Cordell, R. “Machine Learning + Libraries: A Report on the State of the Field” (2020). Available at: <ref target="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig">https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</ref>.</p>
         <p>[Cordell and Smith 2017] Cordell, R., and Smith, D. <hi rend="italic">Viral Texts: Mapping Networks of Reprinting
                    in 19th-Century Newspapers and
                    Magazines</hi> (2017), Available at: <ref target="http://viraltexts.org/">http://viraltexts.org</ref>.</p>
         <p>[Crawford and Paglen 2019] Crawford, K., and Paglen, T.
                    “Excavating AI: The Politics of Training Sets for Machine Learning” (2019).
                    Available at:
                <ref target="https://excavating.ai">https://excavating.ai</ref>
                (Accessed: 19 September 2019).</p>
         <p>[Deng et al. 2009]  Deng, J., Dong, W., Socher, R.,
                    Li, L., Li, K., and
                    Fei-Fei, L. “ImageNet: A Large-Scale Hierarchical Image Database,” in <hi rend="italic">2009 IEEE Conference on Computer Vision and
                    Pattern
                    Recognition</hi> (2009), pp. 248–55,<ref target="https://doi.org/10.1109/CVPR.2009.5206848"> Available at: </ref>
            <ref target="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</ref>.</p>
         <p>[Fagan 2016] Fagan, B. “Chronicling White America.” American Periodicals: A
                    Journal of History &amp; Criticism 26:1, pp. 10-13 (2016). Available at:
                <ref target="https://muse.jhu.edu/article/613375">https://www.muse.jhu.edu/article/613375</ref>.</p>
         <p>[Farrar 1998] Farrar, H. <hi rend="italic">The Baltimore Afro-American,
                1892-1950</hi>. Greenwood Publishing Group (1998). </p>
         <p>[Ferriter 2017] Ferriter, M. “Introducing Beyond Words | The Signal,” (2017). Available at:  <ref target="https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/">//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</ref>. (Accessed: 13 July 2020).</p>
         <p>[Foo 2019] Foo, B. “AMNH Photographic Collection,” (2020). Available at: <ref target="https://amnh-sciviz.github.io/image-collection/about.html">https://amnh-sciviz.github.io/image-collection/about.html</ref> (Accessed: 11 June 2020).</p>
         <p>[Franklin’s Paper the Statesman 1910] Franklin's paper the
                    statesman. (Denver, Colo.), 15 Oct. 1910. Chronicling America:
                    Historic American
                    Newspapers. Library of Congress. Available at: <ref target="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</ref>
         </p>
         <p>[Fyfe 2016] Fyfe, P. “An Archaeology of Victorian Newspapers,” Victorian Periodicals Review 49:4, pp. 546–77 (2016). Available at: 
                <ref target="https://doi.org/10.1353/vpr.2016.0039">https://doi.org/10.1353/vpr.2016.0039</ref>.</p>
         <p>[Gebru et al. 2020] Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H., and Crawford, K. “Datasheets for Datasets.” <hi rend="italic">ArXiv:1803.09010 [Cs]</hi>, March 19,
                    2020.<ref target="http://arxiv.org/abs/1803.09010" xml:space="preserve"> </ref>
            <ref target="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</ref> (Accessed: July 29 2021).</p>
         <p>[Geraldo 2020] Giraldo, M. “Building Aereo,” DX Lab | State Library of NSW (2020). Available at: <ref target="https://dxlab.sl.nsw.gov.au/blog/building-aereo">https://dxlab.sl.nsw.gov.au/blog/building-aereo</ref> (Accessed: 2 July 2020).</p>
         <p>[Google Arts and Culture 2018] “Google Arts &amp; Culture
                    Experiments - t-SNE Map Experiment” (2018). Available
                    at:<ref target="https://artsexperiments.withgoogle.com/tsnemap/" xml:space="preserve"> </ref>
            <ref target="https://artsexperiments.withgoogle.com/tsnemap/">https://artsexperiments.withgoogle.com/tsnemap/</ref> (Accessed: 11 June 2020).</p>
         <p>[Hardy and DiCuirci 2019] Hardy, M., and DiCuirci, L. “Critical Cataloging and the Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’” Archive Journal, Available at: <ref target="http://www.archivejournal.net/?p=8073">http://www.archivejournal.net/?p=8073</ref>.</p>
         <p>[Haro et al. 2018] Hara, K., Adams, A., Milland, K., Savage, S., Callison-Burch, C., and Bigham, J. “A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,” in <hi rend="italic">Proceedings of the 2018 CHI Conference on
                    Human Factors in Computing
                    Systems</hi>, CHI ’18 (Montreal QC, Canada: Association for Computing Machinery, 2018), pp. 1–14. Available at: <ref target="https://doi.org/10.1145/3173574.3174023">https://doi.org/10.1145/3173574.3174023</ref>.</p>
         <p>[He et al. 2016]  He, K., Zhang, X., Ren, S., and Sun, J. “Deep Residual Learning for Image Recognition,” in <hi rend="italic">2016 IEEE Conference on Computer Vision and
                    Pattern Recognition (CVPR)</hi>, 2016, pp.
                    770–78,<ref target="https://doi.org/10.1109/CVPR.2016.90"> Available at: </ref>
            <ref target="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</ref>.</p>
         <p>[Holland et al. 2018] Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski, K. “The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.” <hi rend="italic">ArXiv:1805.03677 [Cs]</hi>, May 9,
                    2018.<ref target="http://arxiv.org/abs/1805.03677" xml:space="preserve"> </ref>
            <ref target="http://arxiv.org/abs/1805.03677">http://arxiv.org/abs/1805.03677</ref> (Accessed 29 July 2021).</p>
         <p>[Hitchcock 2013] Hitchcock, T. “Confronting the Digital,” <hi rend="italic">Cultural and Social
                    History</hi> 10:1. pp. 9–23 (2013). Available at:<ref target="https://doi.org/10.2752/147800413X13515292098070" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.2752/147800413X13515292098070">https://doi.org/10.2752/147800413X13515292098070</ref>.</p>
         <p>[ImageNet 2020] “ImageNet,” Available
                    at:<ref target="http://image-net.org/index" xml:space="preserve"> </ref>
            <ref target="http://image-net.org/index">http://image-net.org/index</ref> (Accessed: 8 June 2020).</p>
         <p>[ImageNet: What about the Images? 2020] “What about the images?” Available at: <ref target="http://image-net.org/download-faq">http://image-net.org/download-faq</ref> (Accessed: 8 June 2020).</p>
         <p>[Intel 2020a] “Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product Specifications,” Available at: <ref target="https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html">https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html</ref> (Accessed: 5 June 2020).</p>
         <p>[Intel 2020b] “Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product Specifications,” Available at: <ref target="https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html">https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html</ref> (Accessed: June 5, 2020).</p>
         <p>[Iowa State Bystander 1910] Iowa state
                    bystander.
                    [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling
                    America: Historic American
                    Newspapers. Library of Congress. Available at: <ref target="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</ref>
         </p>
         <p>[Lacoste et al. 2019] Lacoste, A., Luccioni, A.,
                    Schmidt, V., and
                    Dandres, T. “Quantifying the Carbon Emissions of Machine Learning,” <hi rend="italic">ArXiv:1910.09700
                    [Cs]</hi> (2019). Available at: <ref target="http://arxiv.org/abs/1910.09700">http://arxiv.org/abs/1910.09700</ref>. </p>
         <p>[LC Labs 2017a]  LC Labs, “Beyond Words: Mark” Available at: <ref target="http://beyondwords.labs.loc.gov/#/mark">http://beyondwords.labs.loc.gov/#/mark</ref>  (Accessed 5 June, 2020).</p>
         <p>[LC Labs 2017b]  LC Labs, “Beyond Words: Transcribe,” Available at: <ref target="http://beyondwords.labs.loc.gov/#/transcribe">http://beyondwords.labs.loc.gov/#/transcribe</ref> (Accessed 5 June, 2020).</p>
         <p>[LC Labs 2017c] LC Labs, “Beyond Words: Veriffy,” Available at: <ref target="http://beyondwords.labs.loc.gov/#/verify">http://beyondwords.labs.loc.gov/#/verify</ref> (Accessed 5 June, 2020).</p>
         <p>[LC Labs no date] LC Labs, Beyond Words | Experiments. Available at: <ref target="https://labs.loc.gov/work/experiments/beyond-words/">https://labs.loc.gov/work/experiments/beyond-words/</ref> (Accessed 5 June, 2020).</p>
         <p>[LC Labs and Digital Strategy Directorate 2020] LC Labs and Digital Strategy Directorate, “Machine Learning + Libraries Summit Event Summary” (2020). Available at: <ref target="https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf">https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</ref>. </p>
         <p>[Lee 2019] Lee, B. “Machine Learning, Template Matching, and the International Tracing Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards from 40 Million Document Scans,” <hi rend="italic">Digital Scholarship in the
                    Humanities</hi> 34:3, pp. 513-535 (2019). <ref target="https://doi.org/10.1093/llc/fqy063">Available at: </ref>
            <ref target="https://doi.org/10.1093/llc/fqy063">https://doi.org/10.1093/llc/fqy063</ref>.</p>
         <p>[Lee 2020] Lee, B. <hi rend="italic">LibraryOfCongress/Newspaper-Navigator</hi>, GitHub
                    Repository ( Library of Congress, 2020).<ref target="https://github.com/LibraryOfCongress/newspaper-navigator"> Available at: </ref>
            <ref target="https://github.com/LibraryOfCongress/newspaper-navigator">https://github.com/LibraryOfCongress/newspaper-navigator</ref>.</p>
         <p>[Lee et al. 2020] Lee, B., Mears, J., Jakeway, E.,
                    Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., and Weld,
                    D. “The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America,” <ref target="https://dl.acm.org/doi/proceedings/10.1145/3340531">CIKM '20: Proceedings of the
                        29th ACM International Conference on Information &amp; Knowledge
                        Management</ref>
            <hi rend="italic">, </hi>pp. 3055–3062 (2020). Available
                    at:<ref target="http://arxiv.org/abs/2005.01583" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1145/3340531.3412767">https://doi.org/10.1145/3340531.3412767</ref>.</p>
         <p>[Lee and Weld 2020] Lee, B., and Weld, D. “Newspaper Navigator: Open Faceted Search for 1.5 Million Images,” <ref target="https://dl.acm.org/doi/proceedings/10.1145/3379350">UIST '20 Adjunct: Adjunct Publication of the 33rd
                        Annual ACM Symposium on User Interface Software and
                    Technology</ref>, pp. 120-122 (2020). Available at: <ref target="https://doi.org/10.1145/3379350.3416143">https://doi.org/10.1145/3379350.3416143</ref>.</p>
         <p>[Lee, Berson, and Berson 2021] Lee, B., Berson, I., and Berson, M. “Machine Learning and the Social Studies,” <hi rend="italic">Social
                    Education</hi> 85:2, pp. 88-92 (2021). Available at: <ref target="https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies">https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</ref>.</p>
         <p>[Lee et al. 2021] Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A. “Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,” <hi rend="italic">EuropeanaTech
                    Insight</hi> 16 (2021). Available at: <ref target="https://pro.europeana.eu/page/issue-16-newspapers">https://pro.europeana.eu/page/issue-16-newspapers</ref>.</p>
         <p>[Library of Congress 2019] “Digital Strategy | Library of Congress,” Library of Congress (2019). Available at: <ref target="https://www.loc.gov/digital-strategy/">https://www.loc.gov/digital-strategy/</ref> (Accessed: 30 May 2020).</p>
         <p>[Lincoln et al. 2019]  Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019) “National Neighbors: Distant Viewing the National Gallery of Art's Collection of Collections” (2019) Available at: <ref target="https://nga-neighbors.library.cmu.edu/">https://nga-neighbors.library.cmu.edu</ref>. (Accessed: 30 May 2020).</p>
         <p>[Lonij and Weavers 2017]  Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague (2017). Available at: <ref target="http://lab.kb.nl/tool/siamese">http://lab.kb.nl/tool/siamese</ref>.</p>
         <p>[Lorang et al 2020]  Lorang, E., Soh, L., Liu, Y., and Pack, C. “Digital Libraries, Intelligent Data Analytics, and Augmented Description: A Demonstration Project” (2020). Available at: <ref target="https://digitalcommons.unl.edu/libraryscience/396/">https://digitalcommons.unl.edu/libraryscience/396/</ref>.</p>
         <p>[Mak 2017]  Mak, B. “Archaeology of a Digitization,” <hi rend="italic">Journal of the Association for Information
                    Science and
                    Technology</hi> 65:8, pp. 1515–26 (2014). Available at: <ref target="https://doi.org/10.1002/asi.23061">https://doi.org/10.1002/asi.23061</ref>.</p>
         <p>[Manovich 2012] Manovich, L. “How to Compare One Million Images?,” in <hi rend="italic">Understanding Digital Humanities</hi>, ed. David M. Berry (London: Palgrave Macmillan UK,
                    2012), pp. 249–78. Available
                    at:<ref target="https://doi.org/10.1057/9780230371934_14" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1057/9780230371934_14">https://doi.org/10.1057/9780230371934_14</ref>.</p>
         <p>[Maxwell 2017] Maxwell, M. “WVU Today | WVRHC Seeking Copies of Rare African-American Newspapers” (2017). Available at: <ref target="https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers">https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</ref>. (Accessed 11 July 2020).</p>
         <p>[Mears 2014] Mears, J. <hi rend="italic">National Digital Newspaper Program Impact
                    Study
                    2004-2014</hi>, National Endowment for the Humanities (2014). Available at: <ref target="https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study">https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</ref>. (Accessed 29 May 2020).</p>
         <p>[Meta | Morphosis no date] “Meta | Morphosis: Tutorials,” National Digital Newspaper Program and the University of Kentucky Libraries. Available at: <ref target="https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html">https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html</ref> (Accessed 3 July 2020).</p>
         <p>[Milligan 2013] Milligan, I. “Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010,” <hi rend="italic">Canadian Historical
                    Review</hi> 94:4, pp. 540–69 (2013). Available at: <ref target="https://doi.org/10.3138/chr.694">https://doi.org/10.3138/chr.694</ref>.</p>
         <p>[Mitchell et al. 2019] Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T. “Model Cards for Model Reporting.” <hi rend="italic">Proceedings of the Conference on Fairness,
                    Accountability, and Transparency</hi>, January 29,
                    2019,
                    220–29.<ref target="https://doi.org/10.1145/3287560.3287596" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1145/3287560.3287596">https://doi.org/10.1145/3287560.3287596</ref>.</p>
         <p>[Nasjonalmuseet 2017] “Project: «Principal Components»,” Nasjonalmuseet (2018). Available at: <ref target="https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/">https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management — -behind-the-scenes/digital-collection-management/project-principal-components/</ref> (Accessed 11 June 2020).</p>
         <p>[National Digital Newspaper Program 2019] “About the Program - National Digital Newspaper Program (Library of Congress),” (2019). Available at: <ref target="https://www.loc.gov/ndnp/about.html">https://www.loc.gov/ndnp/about.html</ref> (Accessed 3 July 2020).<ref target="https://www.loc.gov/ndnp/about.html" xml:space="preserve"> </ref>
         </p>
         <p>[National Digital Newspaper Program 2020] The National Digital Newspaper Program (NDNP) Technical Guidelines for Applicants 2020-22 Awards (2020). Available at: <ref target="https://www.loc.gov/ndnp/guidelines/">https://www.loc.gov/ndnp/guidelines/</ref> (Accessed 28 June 2020).</p>
         <p>[National Digital Newspaper Program no date] “Content Selection - National Digital Newspaper Program (Library of Congress)” (2020). Available at: <ref target="https://www.loc.gov/ndnp/guidelines/selection.html">https://www.loc.gov/ndnp/guidelines/selection.html</ref> (Accessed 3 July 2020).<ref target="https://www.loc.gov/ndnp/guidelines/selection.html" xml:space="preserve"> </ref>
         </p>
         <p>[NEH Division of Preservation and Access 2020] Division of Preservation and Access (NEH), “Notice of Funding Opportunity, National Digital Newspaper Program” (2020). Available at: <ref target="https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf">https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</ref>. (Accessed 28 June 2020).</p>
         <p>[Newspaper Navigator 1910a] Image of W.E.B. Du Bois from the <hi rend="italic">Iowa State
                    Bystander</hi> (14 October 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America. Available at: </p>
         <p>
            <ref target="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg</ref>.</p>
         <p>[Newspaper Navigator 1910b] <hi rend="italic">Newspaper
                    Navigator</hi> metadata for the <hi rend="italic">Iowa State
                    Bystander</hi> (14 October 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America. Available at: </p>
         <p>
            <ref target="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json">https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</ref>.</p>
         <p>[Newspaper Navigator 1910c] Image of W.E.B. Du Bois
                    from<hi rend="italic"> Franklin’s Paper the Statesman</hi> (15 October 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America.
                    Available at:
                <ref target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg</ref>
         </p>
         <p>[Newspaper Navigator 1910d] <hi rend="italic">Newspaper Navigator </hi>metadata for <hi rend="italic">Franklin’s Paper the
                    Statesman</hi> (15 October 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America.
                    Available at:
                <ref target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json">https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272</ref>
            <ref target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json">.json</ref>
         </p>
         <p>[Newspaper Navigator 1910e] Image of W.E.B. Du Bois from <hi rend="italic">The Broad Ax </hi>(15 October 1910).
                    From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America. Available at: <ref target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg</ref>
         </p>
         <p>[Newspaper Navigator 1910f] <hi rend="italic">Newspaper Navigator </hi>metadata for <hi rend="italic">The Broad Ax </hi>(15 October 1910).
                    From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America. Available at: <ref target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json</ref>
         </p>
         <p>[Newspaper Navigator 1910g] Image of W.E.B. Du Bois from The Broad Ax (26 November 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America.
                    Available at:
                <ref target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg</ref>
         </p>
         <p>[Newspaper Navigator 1910h] <hi rend="italic">Newspaper Navigator </hi>metadata for
                The Broad Ax (26 November 1910). From the Library of Congress, Newspaper
                    Navigator dataset: Extracted Visual Content from Chronicling
                    America.
                    Available at:
                <ref target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json">https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json</ref>
         </p>
         <p>[Noble 2018] Noble, S. <hi rend="italic">Algorithms of Oppression: How Search
                    Engines Reinforce Racism</hi>. NYU Press, New York
                    (2018).</p>
         <p>[Oceanic Exchanges Project 2017] Oceanic Exchanges Project Team. Oceanic Exchanges:
                    Tracing Global Information Networks In Historical Newspaper Repositories,
                    1840-1914 (2017). Available at: 10.17605/OSF.IO/WA94S.</p>
         <p>[Owens 2018] Owens, T. The Theory and Craft of
                    Digital Preservation.
                    Johns Hopkins University Press, Baltimore (2018).</p>
         <p>[Owens and Padilla 2020]  Owens, T., and Padilla, T. “Digital Sources and Digital Archives: Historical Evidence in the Digital Age,” <hi rend="italic">International Journal of Digital Humanities </hi>(2020). Available
                    at:<ref target="https://doi.org/10.1007/s42803-020-00028-7" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</ref>. </p>
         <p>[Padilla 2019]  Padilla, T. <hi rend="italic">Responsible Operations: Data Science,
                    Machine Learning, and AI in
                    Libraries</hi> (2019). Available at: <ref target="https://doi.org/10.25333/xk7z-9g97">https://doi.org/10.25333/xk7z-9g97</ref>.</p>
         <p>[Reidsma 2019] Reidsma, M. <hi rend="italic">Masked by Trust: Bias in Library
                    Discovery.</hi> Litwin Books, Sacramento (2019).</p>
         <p>[Reisman et al. 2018] Reisman, D., Schultz, J., Crawford, K., Whittaker, M. <hi rend="italic">Algorithmic Impact Assessments: A Practical
                    Framework for Public Agency
                    Accountability</hi> (2018). Available at: <ref target="https://ainowinstitute.org/aiareport2018.pdf">https://ainowinstitute.org/aiareport2018.pdf</ref>.</p>
         <p>[Russakovsky et al. 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L.“ImageNet Large Scale Visual Recognition Challenge,” <hi rend="italic">International Journal of Computer
                    Vision</hi> 115:3, pp. 211-252 (2015). Available at:<ref target="https://doi.org/10.1007/s11263-015-0816-y" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</ref>. </p>
         <p>[Schwartz et al. 2019] Schwartz, R., Dodge, J., Smith,
                    N., and Etzioni, O. “Green AI,” ArXiv:1907.10597
                    [Cs, Stat], (2019). Available
                    at:<ref target="http://arxiv.org/abs/1907.10597" xml:space="preserve"> </ref>
            <ref target="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</ref>.</p>
         <p>[Strange et al. 2014] Strange, C., McNamara, D., Wodak, J., and Wood, I. “Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers,” <hi rend="italic">Digital Humanities
                    Quarterly</hi> 8:1 (2014). Available at: <ref target="http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html">http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</ref>.</p>
         <p>[Strubell et al. 2019] Strubell, E., Ganesh, A., and McCallum, A. “Energy and Policy Considerations for Deep Learning in NLP,” ArXiv:1906.02243 [Cs] (2019). Available at: <ref target="http://arxiv.org/abs/1906.02243">http://arxiv.org/abs/1906.02243</ref>.</p>
         <p>[The Broad Ax 1910a] 
                The broad
                    ax.
                    [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling
                    America: Historic American
                    Newspapers. Library of Congress. Available at: <ref target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</ref>
         </p>
         <p>[The Broad Ax 1910b] The broad
                    ax.
                    [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling
                    America: Historic American
                    Newspapers. Library of Congress. Available at: <ref target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</ref>
         </p>
         <p>[Traub, van Ossenbruggen, and Hardman 2015] Traub, M., van Ossenbruggen, J., and Hardman, L.“Impact Analysis of OCR Quality on Research Tasks in Digital Archives,” in <hi rend="italic">Research and Advanced Technology for
                    Digital Libraries</hi>, ed. Sarantos Kapidakis,
                    Cezary Mazurek, and Marcin Werla (Cham: Springer International Publishing,
                    2015), 252–263.</p>
         <p>[van der Maaten and Hinton 2009].  van der Maaten, L., and Hinton, G. “Visualizing Data Using T-SNE,” <hi rend="italic">Journal of Machine Learning
                    Research</hi> 9, pp. 2579-2605 (2008). Available at: <ref target="http://www.jmlr.org/papers/v9/vandermaaten08a.html">http://www.jmlr.org/papers/v9/vandermaaten08a.html</ref>.</p>
         <p>[Vane 2018]  Vane, O. “Visualising the Royal Photographic Society Collection: Part 2 • V&amp;A Blog,” <hi rend="italic">V&amp;A
                    Blog</hi> (2018). Available at: <ref target="https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2">https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</ref>.</p>
         <p>[Wattenberg, Viégas, and Johnson 2016] Wattenberg, M., Viégas, F., and Johnson, I. “How to Use T-SNE Effectively,” <hi rend="italic">Distill</hi> 1:10 (2016). Available at:<ref target="https://doi.org/10.23915/distill.00002" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.23915/distill.00002">https://doi.org/10.23915/distill.00002</ref>.</p>
         <p>[Weavers and Smits 2020]  Wevers, M., and Smits, T. “The Visual Digital Turn: Using Neural Networks to Study Historical Images,” <hi rend="italic">Digital Scholarship in the
                    Humanities</hi> 35:1, pp. 194-207 (2020). Available at:<ref target="https://doi.org/10.1093/llc/fqy085" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1093/llc/fqy085">https://doi.org/10.1093/llc/fqy085</ref>.</p>
         <p>[Weld and Bansal 2019] Weld, D., and Bansal, G. 2019. The challenge of crafting intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at: <ref target="https://doi.org/10.1145/3282486">https://doi.org/10.1145/3282486</ref>.</p>
         <p>[Williams 2019] Williams, L. “What Computational Archival Science Can Learn from Art History and Material Culture Studies,” in <hi rend="italic">2019 IEEE International Conference on Big
                    Data (Big Data)</hi>, 2019, pp. 3153–55. Available
                    at:<ref target="https://doi.org/10.1109/BigData47090.2019.9006527" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1109/BigData47090.2019.9006527">https://doi.org/10.1109/BigData47090.2019.9006527</ref>. </p>
         <p>[Wright 2019] Wright, R. “Typewriting Mass Observation Online: Media Imprints on the Digital Archive,” <hi rend="italic">History Workshop
                    Journal</hi> 87, pp. 118–38 (2019). Available at:<ref target="https://doi.org/10.1093/hwj/dbz005" xml:space="preserve"> </ref>
            <ref target="https://doi.org/10.1093/hwj/dbz005">https://doi.org/10.1093/hwj/dbz005</ref>.</p>
         <p>[Yale Digital Humanities Lab 2017] “Yale Digital Humanities Lab - PixPlot” (2020). Available at: <ref target="https://dhlab.yale.edu/projects/pixplot/">https://dhlab.yale.edu/projects/pixplot/</ref> (Accessed 11 June 2020).</p>
         <p>Appendix:</p>
         <figure>
            </figure>
         <p>Iowa state
                    bystander.
                    [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling
                    America: Historic American Newspapers. Lib. of
                    Congress. &lt;<ref target="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/">https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</ref>&gt;</p>
         <figure>
            </figure>
         <p>Franklin's
                    paper the
                    statesman. (Denver, Colo.), 15 Oct. 1910. Chronicling
                    America: Historic American Newspapers. Lib. of
                    Congress. &lt;<ref target="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/">https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</ref>&gt;</p>
         <figure>
            </figure>
         <p>The broad
                    ax.
                    [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling
                    America: Historic American Newspapers. Lib. of
                    Congress. &lt;<ref target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</ref>&gt;</p>
         <figure>
            </figure>
         <p>The broad
                    ax.
                    [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling
                    America: Historic American Newspapers. Lib. of
                    Congress. &lt;<ref target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/">https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</ref>&gt;</p>
         <p>Endnotes</p>
      </body>
      <back>
         <listBibl>
            <bibl/>
         </listBibl>
      </back>
   </text>
</TEI>
