<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English-->Compounded Mediation:
               A Data Archaeology of the Newspaper Navigator Dataset</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Benjamin <dhq:family>Lee</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>The Library of Congress &amp; The University of Washington</dhq:affiliation>
               <email>bcgl@cs.washington.edu</email>
               <dhq:bio>
                  <p>Ben Lee is a fourth year Ph.D.
                     candidate in the Paul G. Allen School for Computer Science &amp; Engineering at the University of Washington, where he studies human-AI interaction with his advisor, Professor Daniel Weld. He was also a 2020 Innovator in Residence at the Library of Congress, where
                     he created Newspaper Navigator. Ben served as the 2020-2021 Richard and Ina Willner Memorial Fellow in the Stroum Center for Jewish Studies at the University of Washington, the inaugural Digital Humanities Associate Fellow at the United States Holocaust Memorial
                     Museum, and a Visiting Fellow in Harvard’s History Department. He is currently a National Science Foundation Graduate Research Fellow in machine learning.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000578</idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->015</idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->4</idno>
            <date when="2021-12-07">07 December 2021</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC0">
               <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change when="2022-08-02" who="BRG">resolved malformed xml tags</change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>The increasing roles of machine learning and artificial intelligence in the construction
               of cultural heritage and humanities datasets necessitate critical examination of the
               myriad biases introduced by machines, algorithms, and the humans who build and deploy
               them. From image classification to optical character recognition, the effects of
               decisions ostensibly made by machines compound through the digitization pipeline and
               redouble in each step, mediating our interactions with digitally-rendered artifacts
               through the search and discovery process. As a result, scholars within the digital
               humanities community have begun advocating for the proper contextualization of cultural
               heritage datasets within the socio-technical systems in which they are created and
               utilized. One such approach to this contextualization is the <term>data
                  archaeology</term>, a form of humanistic excavation of a dataset that Paul Fyfe defines
               as <quote rend="inline">recover[ing] and reconstitut[ing] media objects within their changing ecologies</quote>
               <ptr target="#fyfe2016"/>. Within critical data studies, this excavation of a dataset - including its
               construction and mediation via machine learning - has proven to be a capacious approach.
               However, the data archaeology has yet to be adopted as standard practice among cultural
               heritage practitioners who produce such datasets with machine learning. 
            </p>
            <p>In this article, I present a data archaeology of the Library of Congress’s <title
               rend="italic">Newspaper Navigator </title>dataset, which I created as part of the
               Library of Congress’s Innovator in Residence program <ptr target="#lee2020b"/>. The dataset
               consists of visual content extracted from 16 million historic newspaper pages in the <title
                  rend="italic">Chronicling America</title> database using machine learning techniques. In
               this case study, I examine the manifold ways in which a <title rend="italic">Chronicling
                  America</title> newspaper page is transmuted and decontextualized during its journey
               from a physical artifact to a series of probabilistic photographs, illustrations, maps,
               comics, cartoons, headlines, and advertisements in the<title rend="italic"> Newspaper
                  Navigator</title> dataset <ptr target="#fyfe2016"/>. Accordingly, I draw from fields of scholarship
               including media archaeology, critical data studies, science and technology studies, and
               the autoethnography throughout. 
            </p>
            <p>To excavate the <title rend="italic">Newspaper Navigator</title> dataset, I consider
               the digitization journeys of four different pages in Black newspapers included in <title
                  rend="italic">Chronicling America</title>, all of which reproduce the same photograph of
               W.E.B. Du Bois in an article announcing the launch of <title rend="italic">The Crisis</title>,
               the official magazine of the NAACP. In tracing the newspaper pages’ journeys, I unpack
               how each step in the <title rend="italic">Chronicling America</title> and <title rend="italic"
                  >Newspaper Navigator</title> pipelines, such as the imaging process and the construction
               of training data, not only imprints bias on the resulting <title rend="italic">Newspaper
                  Navigator</title> dataset but also propagates the bias through the pipeline via the
               machine learning algorithms employed. Along the way, I investigate the limitations of
               the <title rend="italic">Newspaper Navigator</title> dataset and machine learning techniques
               more generally as they relate to cultural heritage, with a particular focus on
               marginalization and erasure via algorithmic bias, which implicitly rewrites the archive
               itself. </p>
            <p>In presenting this case study, I argue for the value of the data archaeology as a
               mechanism for contextualizing and critically examining cultural heritage datasets within
               the communities that create, release, and utilize them. I offer this autoethnographic
               investigation of the <title rend="italic">Newspaper Navigator </title>dataset in the hope that
               it will be considered not only by users of this dataset in particular but also by
               digital humanities practitioners and end users of cultural heritage datasets writ large. 
            </p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>In this article, I consider the Library of Congress’s <title rend="italic">Newspaper
                  Navigator</title> dataset, which I created as part of the Library of Congress’s
               Innovator-in-Residence program.</p>
         </dhq:teaser>
      </front>
      <body>
         <div>
         <head>I. An Introduction to the Newspaper Navigator Dataset</head>
         <p>In partnership with LC Labs, the National Digital Newspaper Program, and IT Design &amp;
            Development at the Library of Congress, as well as Professor Daniel Weld at the
            University of Washington, I constructed the <title rend="italic">Newspaper Navigator</title>
            dataset as the first phase of my Library of Congress Innovator in Residence project,<title 
               rend="italic"> Newspaper Navigator</title>.<note> More on the organizational
               considerations surrounding <title rend="italic">Newspaper Navigator </title>can be found in
                  <ptr target="#lee2021b"/>.</note> The project has its origins in <title rend="italic"
               >Chronicling America</title>, a database of digitized historic American newspapers
            created and maintained by the National Digital Newspaper Program, itself a partnership
            between the Library of Congress and the National Endowment for the Humanities. Content
            in <title rend="italic">Chronicling America</title>is contributed by state partners of the National Digital
            Newspaper Program who have applied for and received awards from the Division of
            Preservation and Access at the National Endowment for the Humanities <ptr target="#mears2014"/>. At
            the time of the construction of the <title rend="italic">Newspaper Navigator</title> dataset in March, 2020,
            <title rend="italic">Chronicling America</title> contained approximately 16.3 million digitized historic newspaper
            pages published between 1789 and 1963, covering 47 states as well as Washington, D.C.
            and Puerto Rico. The technical specifications of the National Digital Newspaper Program
            require that each digitized page in <title rend="italic">Chronicling America</title> comprises the following digital
            artifacts <ptr target="#national2020"/>: </p>
         <list type="ordered">
            <item>A page image in two raster formats:<list type="ordered">
                  <item>Grayscale, scanned for maximum resolution possible between 300-400 DPI,
                     relative to the original material, uncompressed TIFF 6.0 </item>
                  <item>Same image, compressed as JPEG2000</item>
               </list>
            </item>
            <item>Optical character recognition (OCR) text and associated bounding boxes for words
               (one file per page image)</item>
            <item>PDF Image with Hidden Text, i.e., with text and image correlated</item>
            <item>Structural metadata (a) to relate pages to title, date, and edition; (b) to
               sequence pages within issue or section; and (c) to identify associated image and OCR
               files</item>
            <item>Technical metadata to support the functions of a trusted repository</item>
         </list>
            
            
         <p>Additional artifacts and metadata are contributed for each digitized newspaper issue and
            microfilm reel. All digitized pages are in the public domain and are available online
            via a public search user interface,<note> The public search interface is available at:
                  <ref target="https://chroniclingamerica.loc.gov/"
                  >https://chroniclingamerica.loc.gov/</ref>
            </note> making <title rend="italic">Chronicling America</title> an immensely rich resource for the American
            public.</p> 
         <p>The central goal of <title rend="italic">Newspaper Navigator </title>is to re-imagine how the
            American public explores <title rend="italic">Chronicling America</title> by utilizing
            emerging machine learning techniques to extract, categorize, and search over the visual
            content and headlines in <title rend="italic">Chronicling America</title>’s 16.3 million pages
            of digitized historic newspapers. <title rend="italic">Newspaper Navigator</title> was both
            inspired and directly enabled by the Library of Congress’s <title rend="italic">Beyond
               Words</title> crowdsourcing initiative <ptr target="#ferriter2017"/>. Launched by LC Labs in 2017, <title
               rend="italic">Beyond Words</title> engages the American public by asking volunteers to
            identify and draw boxes around photographs, illustrations, maps, comics, and editorial
            cartoons on World War I-era pages in <title rend="italic">Chronicling America</title>, note
            the visual content categories, and transcribe the relevant textual information such as
            titles and captions.<note> For more information on the <title rend="italic">Beyond Words</title> workflow, see 
               <ptr target="#lclabs"/>, as well as <ptr target="#lee2020b"/>.</note> The thousands of annotations
            created by <title rend="italic">Beyond Words </title>volunteers are in the public domain and
            available for download online. <title rend="italic">Newspaper Navigator </title>directly
            builds on <title rend="italic">Beyond Words</title> by utilizing these annotations, as well as
            additional annotations of headlines and advertisements, to train a machine learning
            model to detect visual content in historic newspapers.<note> In particular, the
               annotations were used to finetune an object detection model that had been pre-trained
               on Common Objects in Context, a common dataset for benchmarking object detection
               algorithms.</note> Because <title rend="italic">Beyond Words</title> volunteers were asked
            to draw bounding boxes to include any relevant textual content, such as a photograph’s
            title, this machine learning model learns during training to include relevant textual
            content when predicting bounding boxes.<note> A screenshot of the workflow can be found
               later in this article in Figure 4. </note> Furthermore, in the <title rend="italic"
               >Transcribe </title>step of <title rend="italic">Beyond Words</title>, the system provided the
            OCR with each bounding box as an initial transcription for the volunteer to correct;
            inspired by this, the <title rend="italic">Newspaper Navigator</title> pipeline automatedly
            extracts the OCR falling within each predicted bounding box in order to provide noisy
            textual metadata for each image. In the case of headlines, this method enables the
            headline text to be directly extracted from the bounding box predictions. Lastly, the
            pipeline generates image embeddings for the extracted visual content using an image
            classification model trained on ImageNet.<note> For those who are not familiar with
               image embeddings, a detailed description is provided in Section IX.</note> A diagram
            of the full <title rend="italic">Newspaper Navigator</title> pipeline can be found in Figure
            1. </p> 
            
            <figure>
               <head>A diagram showing the <title rend="italic">Newspaper Navigator</title> pipeline,
                  which processed over 16.3 million historic newspaper pages in <title rend="italic"
                     >Chronicling America</title>, resulting in the <title rend="italic">Newspaper
                        Navigator</title> dataset.</head>
               <graphic url="resources/images/image1.png"/>
               <figDesc>A diagram of newspaper screenshots</figDesc>
            </figure>
            
      
         <p>Over the course of 19 days from late March to early April of 2020, the <title rend="italic"
               >Newspaper Navigator</title> pipeline processed 16.3 million pages in <title rend="italic">Chronicling America</title>; the
            resulting <title rend="italic">Newspaper Navigator </title>dataset was publicly released in
            May, 2020. The full dataset, as well as all code written for this project, are available
            online and have been placed in the public domain for unrestricted re-use.<note> For the
               dataset, see: <ref target="https://news-navigator.labs.loc.gov"
                  >https://news-navigator.labs.loc.gov</ref>; for the code, see <ref
                  target="https://github.com/LibraryOfCongress/newspaper-navigator"
                  >https://github.com/LibraryOfCongress/newspaper-navigator</ref>.</note> Currently,
            the <title rend="italic">Newspaper Navigator</title> dataset can be queried using HTTPS and
            Amazon S3 requests. Furthermore, hundreds of pre-packaged datasets have been made
            available for download, along with associated metadata. These pre-packaged datasets
            consist of different types of visual content for each year, from 1850 to 1963, allowing
            users to download, for example, all of the maps from 1863 or all of the photographs from
            1910. For more information on the technical aspects of the pipeline and the construction
            of the <title rend="italic">Newspaper Navigator</title> dataset, I refer the reader to <ptr target="#lee2020b"/></p>
         </div>
         <div>
         <head>II. Why a Data Archaeology?</head>
         <p>As machine learning and artificial intelligence play increasing roles in digitization
            and digital content stewardship, the Libraries, Archives, and Museums (<q>LAM</q>) community
            has repeatedly emphasized the importance of ensuring that these emerging methodologies
            are incorporated ethically and responsibly. Indeed, a major theme that emerged from the
            <title rend="quotes">Machine Learning + Libraries Summit</title> hosted by LC Labs in September, 2019, was that
            <quote rend="inline">there is much more <soCalled>human</soCalled> in machine learning than the name conveys</quote> and that
            transparency and communication are first steps toward addressing the <quote rend="inline">human
               subjectivities, biases, and distortions</quote> embedded within machine learning systems <ptr target="#lclabs2020"/>. 
            This data archaeology has been written in
            support of this call for transparency and responsible stewardship, which is echoed in
            the Library of Congress’s Digital Strategy, as well as the recommendations in Ryan
            Cordell’s report to the Library of Congress <title rend="quotes">ML + Libraries: A Report on the State of
            the Field,</title> Thomas Padilla’s OCLC position paper <title rend="quotes">Responsible Operations: Science,
            Machine Learning, and AI in Libraries</title>, and the University of Nebraska-Lincoln’s report
            on machine learning to the Library of Congress <ptr target="#congress2019"/>; <ptr target="#cordell2020"/>;
            <ptr target="#padilla2019"/>; <ptr target="#lorang2020"/>. I write this data archaeology from my
            perspective of having created the dataset, and although I am not without my own biases,
            I have attempted to represent my work as honestly as possible. Accordingly, I seek not
            only to document the construction of the <title rend="italic">Newspaper Navigator
            </title>dataset through the lens of data stewardship but also to critically examine the
            dataset’s limitations. In doing so, I advocate for the importance of autoethnographic
            approaches to documenting a cultural heritage dataset’s construction from a humanistic perspective.
         </p>
         <p>This article draws inspiration from recent works in media and data archaeology,
            including Paul Fyfe’s <title rend="quotes">An Archaeology of Victorian Newspapers</title>; Bonnie Mak’s
            <title rend="quotes">Archaeology of a Digitization</title>; Kate Crawford and Trevor Paglen’s <title rend="quotes">Excavating AI: The
            Politics of Images in Machine Learning Training Sets</title>; and, most directly, Ryan
            Cordell’s <title rend="quotes">Qi-jtb the Raven: Taking Dirty OCR Seriously,</title> in which Cordell traces the
            digitization of a single issue of the <title rend="italic">Lewisburg Chronicle</title> from
            its selection by the Pennsylvania Digital Newspaper Project to its ingestion into the
               <title rend="italic">Chronicling America </title>online database, with a focus on the
            distortive effects of OCR <ptr target="#fyfe2016"/>; <ptr target="#mak2017"/>; <ptr target="#crawford2019"/>; <ptr target="#cordell2017"/>.
            As argued by Trevor Owens and Thomas Padilla, it is essential to <quote rend="inline">document how
            digitization practices and how the affordances of particular sources … produce
            unevenness in the discoverability and usability of collections</quote> <ptr target="#owens2020"/>.
            Recent works within the machine learning literature have analogously emphasized
            the importance of documenting the collection and curation efforts underpinning community
            datasets and machine learning models. Reporting mechanisms include <title rend="quotes">Datasheets for
            Datasets,</title> <title rend="quotes">Dataset Nutrition Labels,</title> <title rend="quotes">Data Statements for NLP,</title> <title rend="quotes">Model Cards for Model
               Reporting,</title> and <title rend="quotes">Algorithmic Impact Assessments</title> <ptr target="#gebru2020"/>; 
            <ptr target="#holland2018"/>; <ptr target="#bender2018"/>; <ptr target="#mitchell2019"/>; <ptr target="#reisman2018"/>. This
            case study adopts a similar framing in stressing the importance of reporting mechanisms,
            with a particular focus on the data archaeology in the context of cultural heritage datasets.
         </p>
         <p> In the following sections, I trace the digitization process and data flow for <title
               rend="italic">Newspaper Navigator</title>, beginning with the physical artifact of the
            newspaper itself and ending with the machine learning predictions that constitute the
               <title rend="italic">Newspaper Navigator </title>dataset, reflecting on each step through
            the lens of discoverability and erasure. In particular, I study four different <title
               rend="italic">Chronicling America </title>Black newspaper pages published in 1910, each
            depicting the same photograph of W.E.B. Du Bois, as the pages move through the <title
               rend="italic">Chronicling America</title> and <title rend="italic">Newspaper Navigator</title>
            pipelines. All four pages reproduce the same article by Franklin F. Johnson, a reporter
            from <title rend="italic">The Baltimore Afro-American </title> <ptr target="#farrar1998"/>; the headline is
            as follows: </p>
         <p>NEW MOVEMENT </p>
         <p>BEGINS WORK</p>
         <p>Plan and Scope of the Asso-</p>
         <p>ciation Briefly Told. </p>
         <p>Will Publish the Crisis. </p>
         <p>Review of Causes Which Led to the </p>
         <p>Organization of the Association in </p>
         <p>New York and What Its Policy Will </p>
         <p>Be-Career and Work of Professor </p>
         <p>W.E.B. Du Bois</p>
         <p>The article describes the creation of the National Association for the Advancement of
            Colored People (NAACP), details W.E.B. Du Bois’s background, and announces the launch of
               <title rend="italic">The Crisis</title>, the official magazine of the NAACP, with Du Bois
            as Editor-in-Chief. The four pages comprise the front page of the October 14th, 1910,
            issue of the <title rend="italic">Iowa State Bystander </title> <ptr target="#iowa1910"/>;
            the 16th page of the October 15th, 1910, issue of <title rend="italic">Franklin’s Paper the
               Statesman </title> <ptr target="#franklin1910"/>; and the 2nd and 3rd pages of
            the October 15th, 1910, and November 26th, 1910, issues of <title rend="italic">The Broad
               Ax</title>, respectively <ptr target="#ax1910a"/>; <ptr target="#ax1910b"/>. All four digitized
            pages are reproduced in the Appendix. </p>
         </div>
         <div>
         <head>III. <title rend="italic">Chronicling America</title>: A Genealogy of Collecting, Microfilming, and Digitizing</head>
         <p>Any examination of <title rend="italic">Newspaper Navigator </title>must begin with the
            genealogy of collecting, microfilming, and digitizing that dictates which newspapers
            have been ingested into the <title rend="italic">Chronicling America </title>database. The
            question of what to digitize is, in practice, answered and realized incrementally over
            decades, beginning at its most fundamental level with the question of which newspapers
            have survived and which have been reduced to lacunae in the historical record <ptr target="#hardy2019"/>.
            <note> Indeed, compiling bibliographies of serials published after 1820
               remains an immensely difficult task <ptr target="#hardy2019"/>.</note> Historic
            newspapers present challenges for digitization in part due to the ephemerality of the
            physical printed newspaper itself: many newspapers were microfilmed and immediately
            discarded due to a fear that the physical pages would deteriorate.<note> The extent to
               which newspaper microfilming was driven by credible fear of deterioration versus
               other factors, such as microfilm marketing, is an important question that is rightly
               debated. For more on this topic, see <ptr target="#baker2001"/>. </note> Indeed, almost all of the
            pages included in <title rend="italic">Chronicling America</title> have been digitized
            directly from microfilm. In the next section, I will examine the microfilm imaging
            process in more detail; however, in most cases, librarians selected newspapers for
            collecting and microfilming decades before the National Digital Newspaper Program was
            launched in 2004. These selections were informed by a range of factors including
            historical significance - itself a subjective, nebulous, and ever-evolving notion that
            has historically served as the basis for perpetuating oppression within the historical
            record. In “Chronicling White America,” Benjamin Fagan highlights the paucity of Black
            newspapers in <title rend="italic">Chronicling America</title>, in particular in relation to
            pre-Civil War era newspapers [Fagan 2016]. It is imperative to remember that this
            paucity can directly be traced back decades to the collecting and preserving
               stages.<note> For example, a 2017 article describing the West Virginia University
               Libraries’ West Virginia &amp; Regional History Center and its participation in the
               National Digital Newspaper Program states:<quote rend="inline"> By August 2017, all known issues of West
               Virginia’s African-American newspapers from the 19th and early 20th centuries will
               have been digitized </quote> <ptr target="#maxwell2017"/>. The article describes Curator Stewart Plein’s
               efforts to locate surviving copies of three Black West Virginia newspapers in order
               to digitize and include them in <title rend="italic">Chronicling America</title>.</note>
         </p>
         <p>In regard to collecting, the newspaper page is both an informational object (i.e., the
            newspaper page as defined by its content) and a material object (i.e., the specific
            printed copy of the newspaper page) <ptr target="#owens2018"/>. At some point in time, librarians
            accessioned a specific copy of each printed page and microfilmed it or contracted out
            the microfilming. The materiality of that specific printed page is a confluence of
            unique ink smudges, rips, creases, and page alignment, much of which is captured in the
            microfilm imaging process. Though we may not make much of a crease or a smudge on a
            digitized page when we find it in the <title rend="italic">Chronicling America</title>
            database, it can very well take on a life of its own with a machine learning algorithm
            in <title rend="italic">Newspaper Navigator</title>. The machine learning algorithm might deem
            two newspaper photographs as similar simply due to the presence of creases or smudges,
            even if the photographs are easily discernible to the naked eye, or the smudges are of
            entirely different origin (i.e., a printing imperfection versus a smudge from a dirty
            hand). </p>
         <p>It is only by foregrounding these subtleties of the collection, preservation, and
            microfilming processes that we can understand the selection process for <title
               rend="italic">Chronicling America</title> in its proper context. The grant-seeking
            process dictates selection criteria for <title rend="italic">Chronicling America </title>by
            which state-level institutions including state libraries, historical societies, and
            universities apply for two years of grant funding from the National Digital Newspaper
            Program via the Division of Preservation and Access at the National Endowment for the
            Humanities. With the awarding of a grant, a state-level awardee then digitizes
            approximately 100,000 newspaper pages published in their state for inclusion in
            <title rend="italic">Chronicling America</title> <ptr target="#national2020"/>; <ptr target="#neh2020"/>.
            The grant-seeking and awarding process is nuanced, but
            salient points include that state-level applicants must assemble an advisory board
            including scholars, teachers, librarians, and archivists to aid in the selection of
            newspapers, and grants are reviewed by National Endowment for the Humanities staff, as
            well as peer reviewers.<note> For a thorough case study of this process, I direct the
               reader to <title rend="quotes">Qi-jtb the Raven,</title> in which Ryan Cordell walks through an example with the
               Pennsylvania Digital Newspaper Program <ptr target="#cordell2017"/>. </note>
         </p>
         <p>Regarding selection criteria for newspaper titles, the National Digital Newspaper
            Program defines the following factors for state-level awardees to consider for content
            selection after a newspaper is determined to be in the public domain <ptr target="#national"/>: </p>
         <list type="unordered">
            <item>image quality in the selection of microfilm</item>
            <item>research value</item>
            <item>geographic representation</item>
            <item>temporal coverage</item>
            <item>bibliographic completeness of microfilm copy</item>
            <item>diversity (i.e., “newspaper titles that document a significant minority community
               at the state or regional level”)</item>
            <item>whether the title is orphaned (i.e., whether the newspaper has “ceased publication
               and lack[s] active ownership” [Chronicling America no date])</item>
            <item>whether the title has already been digitized.</item>
         </list>
            
         <p>Though factors such as research value are considered by each state awardee’s advisory
            board, as well as by the National Endowment for the Humanities and peer review experts,
            the titles included in <title rend="italic">Chronicling America</title> are largely dictated
            by which exist on microfilm and are of sufficient image quality within a state-level
            grantee’s collection. Thus, the significance of the collection and microfilming
            practices of decades prior cannot be understated.</p>
         <p>I also highlight that assessing microfilmed titles based on image quality is a complex
            procedure in its own right. The National Digital Newspaper Program has made publicly
            available a number of resources devoted specifically to this task, including documents
            and video tutorials <ptr target="#barrall2005"/>; <ptr target="#meta"/>. They
            articulate factors such as the microfilm generation (archive master, print master, or
            review copy), the material (polyester or acetate), the reduction ratio, and the physical
            condition. The detailed resources made available by the National Digital Newspaper
            Program, the Library of Congress, and the National Endowment for the Humanities for
            navigating this process are testaments to the multidimensional complexity of the
            selection process for <title rend="italic">Chronicling America </title> <ptr target="#national2019"/>;
            <ptr target="#national"/>; <ptr target="#neh2020"/>.</p>
         <p>We have not yet investigated the topic of digitization, and we have already encountered
            a profusion of factors from collection to digitization that mediate which artifacts
            appear in <title rend="italic">Chronicling America</title> and thus <title rend="italic"
               >Newspaper Navigator</title>. Let us now examine the microfilm itself.</p>
         </div>
         <div>
         <head>IV. The Microfilm</head>
         <p>In <title rend="quotes">What Computational Archival Science Can Learn from Art History and Material Culture
            Studies,</title> Lyneise Williams shares a powerful anecdote of coming across a physical copy
            of a 1927 issue of the French sports newspaper <title rend="italic">Match L’Intran</title>
            that featured accomplished Black Panamanian boxer, Alfonso Teofilo Brown, on the front
            cover <ptr target="#williams2019"/>. Williams describes Brown as <quote rend="inline">glowing. He looked like a 1920s film
               star rather than a boxer</quote> <ptr target="#williams2019"/>. Curious to learn more about the printing
            process, Williams discovered that the issue of <title rend="italic">Match L’Intran</title> was
            produced using rotogravure, a specific printing process that could <quote rend="inline">capture details in
               dark tones</quote> <ptr target="#williams2019"/>. However, when Williams found a version of the same
            newspaper cover that had been digitized from microfilm, it was apparent that the
            microfilming process had washed out the detail of the rotogravure, reducing Brown to a
            <quote rend="inline">flat black, cartoonish form</quote> <ptr target="#williams2019"/>. Williams relays the anecdote to
            articulate that the microfilming process itself is thus a form of erasure for
            communities of color <ptr target="#williams2019"/>.</p>
         <p>The grayscale saturation of photographs induced by microfilming is widely documented and
            recognizable to most researchers who have ever worked with the medium <ptr target="#baker2001"/>;
            however, Lyneise Williams’s article affords us a lens into what precisely is lost
            amongst the distortive effects of the microfilming process. This erasure via
            microfilming can be seen in <title rend="italic">Chronicling America </title>directly. In
            Figure 2, I show the same photograph of W.E.B. Du Bois as it appears in 4 different <title
               rend="italic">Chronicling America</title> newspaper pages published during October and
            November of 1910 and digitized from microfilm <ptr target="#iowa1910"/>; <ptr target="#franklin1910"/>; 
            <ptr target="#ax1910a"/>; <ptr target="#ax1910b"/>. The phenomenon
            described by Williams is immediately recognizable in these four images: Du Bois’s facial
            features are distorted by the grayscale saturation. In the case of the <title rend="italic"
               >Iowa State Bystander</title>, Du Bois has been rendered into a silhouette.</p>
         <p>Moreover, each digitized reproduction reveals unique visual qualities, varying in
            contrast, sharpness, and noise - a testament to the confluence of mediating conditions
            from printing through digitization that have rendered each newspaper photograph in
            digital form. Even in the case of the two images reproduced in the <title rend="italic">The
               Broad Ax</title>, which were digitized from the very same microfilm reel (reel
            #00280761059) by the University of Illinois at Urbana-Champaign Library, variations are
            still apparent. To understand how these subtle differences between images are amplified
            through digitization, we now turn to optical character recognition.</p>
            
         
            
            <figure>
               <head>The same image of W.E.B. Du Bois reproduced in 4 different digitized Black
                  newspapers in <title rend="italic">Chronicling America</title> from 1910. Note that the
                  combined effects of printing, microfilming, and digitizing have led to different visual
                  effects in each image, ranging from contrast to sharpness.</head>
               <graphic url="resources/images/image2.jpg"/>
               <figDesc>Images of W.E.B. Du Bois</figDesc>
            </figure>
            
         </div>
         <div>
         <head>V. OCR</head>
         <p>Optical character recognition, commonly called OCR, refers to machine learning
            algorithms that are trained to read images of typewritten text and output
            machine-readable text, thereby providing the bridge between an image of typewritten text
            and the transcribed text itself. Because OCR algorithms are <quote rend="inline">trained and evaluated using
            labeled data: examples with ground-truth classification labels that have been assigned
            by another means,</quote> the algorithms are considered a form of <term>supervised learning</term> in the
            machine learning literature <ptr target="#lee2019"/>. OCR engines are remarkably powerful in their
            ability to improve access to historic texts. Indeed, OCR is a crucial form of metadata
            for <title rend="italic">Chronicling America</title>, enabling keyword search in the search
            portal and making possible scholarship with the newspaper text at large scales.<note>
               For exemplary research collaborations that utilize the <title rend="italic">Chronicling
                  America </title>bulk OCR, see the Viral Text Project and the Oceanic Exchanges
               Project <ptr target="#cordell2017"/>; <ptr target="#oceanic2017"/>. </note> However,
            OCR is not perfect. Although humans are able to discern an <q>E</q> from an <q>R</q> on a
            digitized page even if the type has been smudged, an OCR engine is not always able to do
            so: its performance is dependent on factors ranging from the sharpness of text in an
            image to printing imperfections to the specific typography on the page. </p>
         <p>In Figure 3, I show the same four images shown in Figure 2, along with OCR
            transcriptions of the captions provided by <title rend="italic">Chronicling America</title>.
            All four transcriptions fail to reproduce the true caption with 100% accuracy, differing
            from one another by at least one character. Consequently, a keyword search of <q>W. E. B.
            Du Bois</q> over the raw text would not register the caption for any of the four
            photographs (the <title rend="italic">Chronicling America</title> search portal utilizes a
            form of relevance search to alleviate this problem). These examples reveal how sensitive
            OCR engines are to slight perturbations, or <soCalled>noise,</soCalled> in the digitized images, from ink
            smudges to text sharpness to page contrast. Though the NDNP awardees who contributed
            these pages may have utilized different OCR engines or chosen different OCR settings,
            the OCR for the two image captions from <title rend="italic">The Broad Ax </title>that have
            been digitized from the very same microfilm reel was in all likelihood generated using the same
            OCR engine and settings. Put succinctly, OCR engines amplify the noise from both the
            material page and the digitization pipeline.<note> For other examinations of how OCR
               mediates our interactions with digital archives, see <ptr target="#hitchcock2013"/>; <ptr target="#milligan2013"/>;
               <ptr target="#strange2014"/>; <ptr target="#traub2015"/>; <ptr target="#wright2019"/>.</note>
         </p>
            
            
            
            <figure>
               <head>The OCR transcriptions of the caption “W. E. B. DU BOIS, PH. D.” appearing in
                  the image of W.E.B. Du Bois reproduced in 4 different digitized Black newspapers in <title
                     rend="italic">Chronicling America</title>. These OCR transcriptions are provided by <title
                        rend="italic">Chronicling America</title>.</head>
               <graphic url="resources/images/image3.jpg" />
               <figDesc>four images of W.E.B. Du Bois</figDesc>
            </figure>
            
           
         <p>Though OCR engines have become standard components of digitization pipelines, it is
            important to remember that OCR engines are themselves machine learning models that have
            been trained on sets of transcribed typewritten pages. Like any machine learning model,
            OCR predictions are thus subject to biases encoded not only in the OCR engine’s
            architecture but also in the training data itself. Though it is often called 
            <term>algorithmic bias</term>, this bias is undeniably human, in that the
            construction of training data machine learning models are imprinted with countless human
            decisions and judgment calls. For example, if an OCR engine is trained on transcriptions
            that consistently misspell a word, the OCR engine will amplify this misspelling across
            all transcriptions of processed pages.<note> For a concrete example of a similar
               phenomenon in the image domain, see <ptr target="#lee2019"/>, in which a machine learning algorithm
               was trained to classify digitized images but consistently misclassified images that
               had been misoriented 180 degrees in the scanning bed - a consequence of the
               classifier not having seen enough instances of these misoriented scans during
               training.</note> A recurring theme of algorithmic bias is that it is a force for
            marginalization, especially in the context of how we navigate information digitally. In
               <title rend="italic">Algorithms of Oppression</title>, Safiya Noble describes how Google’s
            search engine consistently marginalizes women and people of color by displaying search
            results that reinforce racism <ptr target="#noble2018"/>. This bias is not restricted to Google: in
               <title rend="italic">Masked by Trust: Bias in Library Discovery</title>, Matthew Reidsma
            articulates how library search engines suffer from similar biases <ptr target="#reidsma2019"/>.
            Despite the fact that knowledge of algorithmic bias in relation to search engines and
            image recognition tools is becoming increasingly widespread among the cultural heritage
            community, the errors introduced by OCR engines are often accepted as inevitable without
            critical inquiry from this perspective. However, algorithmic bias is a useful framework
            for examining OCR engines <ptr target="#alpertabrams2016"/>. </p>
         <p>Perhaps the most significant challenge to studying OCR engines is that the
            best-performing and most widely-used OCR engines are proprietary. Though ABBYY
            FineReader and Google Cloud Vision API offer high performance, the systems fundamentally
            are black boxes: we have no access to the underlying algorithms or the training data.
            The ability to audit a system is crucial to developing an understanding of how it works
            and the biases it encodes. The fact that many OCR engines are opaque prevents us from
            disentangling whether poor performance on a particular page is due to algorithmic
            limitations or due to a lack of relevant training data. The distinction is significant:
            the former may reflect an algorithmic upper bound, whereas the latter reflects decisions
            made by humans.</p>
         <p>Indeed, algorithmic bias distorts and occludes the historical record, as it is made
            discoverable through OCR. Discrepancies in OCR performance for different languages and
            scripts is a consequence of human prioritization, from the collection of training data
            and lexicons to the development of the algorithms themselves. As articulated by Hannah
            Alpert-Abrams in <title rend="quotes">Machine Reading the <title rend="italic">Primeros Libros</title>,</title> 
            <quote rend="inline">the machine-recognition of printed characters is a historically charged event, in which the
            system and its data conspire to embed cultural biases in the output, or to affix them as
            supplementary information hidden behind the screen</quote> <ptr target="#alpertabrams2016"/>.
            Alpert-Abrams’s work reveals how the OCR inaccuracies for indigenous languages recorded
            in colonial scripts perpetuate colonialism. For other languages such as Ladino,
            typically typeset in Rashi script, the lack of high-performing OCR has presented
            consistent challenges for digitization and scholarship.</p>
         <p>In the case of <title rend="italic">Chronicling America</title>, the National Digital
            Newspaper Program is exemplary in its efforts to support OCR for non-English languages.
            In the Notice of Funding Opportunity for the National Digital Newspaper Program produced
            by the Division of Preservation of Access at the National Endowment for the Humanities,
            OCR performance in different languages is explicitly addressed: <quote rend="inline">Applicants proposing to
            digitize titles in languages other than English must include staff with the relevant
            language expertise to review the quality of the converted content and related metadata</quote>
            <ptr target="#neh2020"/>. I have included this discussion of OCR
            and algorithmic bias to offer a broader provocation regarding machine learning and
            digitization: how much text in digitized sources has been transmuted by this effect and
            thus effectively erased due to inaccessibility when using search and discovery
            platforms?</p>
         </div>
         <div>
         <head>VI. The Visual Content Recognition Model</head>
         <p>I will now turn to the <title rend="italic">Newspaper Navigator</title> pipeline itself, in
            particular the visual content recognition model. Trained on annotations from the <title
               rend="italic">Beyond Words </title>crowdsourcing initiative, as well as additional
            annotations of headlines and advertisements, the visual content recognition model
            detects photographs, illustrations, maps, comics, editorial cartoons, headlines, and
            advertisements on historic newspaper pages. </p>
         <p>As described in the previous section, examining training data is an essential component
            of auditing any machine learning model, from understanding how the dataset was
            constructed to uncovering any biases in the composition of the dataset itself. For the
            visual content recognition model, this examination begins with <title rend="italic">Beyond
               Words</title>. Launched in 2017 by LC Labs, <title rend="italic">Beyond Words </title>has
            collected to-date over 10,000 verified annotations of visual content in World War 1-era
            newspaper pages from <title rend="italic">Chronicling America</title>. The <title rend="italic"
               >Beyond Words </title>workflow consists of the three steps listed below:</p>
         <list type="ordered">
            <item><p>A <q>Mark</q> step, in which volunteers are asked to draw bounding boxes around visual
               content on the page <ptr target="#lclabs2017a"/>. The instructions read as follows:</p>
        
         <p><quote rend="block">In the Mark step, your task is to identify and select pictures in newspaper pages. For
            our project, <q>pictures</q> means illustrations, photographs, comics, and cartoons. You'll
            use the marking tool to draw a box around the picture using your mouse. After you have
            marked all pictures on the newspaper page, click the ‘DONE’ button. Skip the page
            altogether by clicking the <q>Skip this page</q> button. If no illustrations, photographs, or
            cartoons appear on the page, click the <q>DONE</q> button. Not sure if a picture should be
            marked? Select the <q>Done for now, more left to mark</q> button so another volunteer can
            help finish that page. Please do not select pictures within advertisements.</quote></p></item>
         
            <item><p>A <q>Transcribe</q> step, in which volunteers are asked to transcribe the caption of
               the highlighted visual content, as well as note the artist and visual content
               category (<q>Photograph,</q> <q>Illustration,</q> <q>Map,</q> <q>Comics/Cartoon,</q> <q>Editorial Cartoon</q>)
               <ptr target="#lclabs2017b"/>. The transcription is pre-populated with the OCR falling within the
               bounding box in question. The instructions for this step state:</p>
         
         <p><quote rend="block">Most pictures have captions or descriptions. Enter the text exactly as you see it.
            Include capitalization and punctuation, but remove hyphenation that breaks words at the
            end of the line. Use new lines to separate different parts of captions and descriptions.
            You can zoom in for better looks at the page. You can also select <q>View the original
            page</q> in the upper right corner of the screen to view the original high resolution image
            of the newspaper.</quote></p>
            
         <p>An example of this step can be seen in Figure 4.</p>
            </item>
         
            <item><p>A <q>Verify</q> step, in which volunteers are asked to select the best caption for an
               identified region of visual content from at least two examples; alternatively, a
               volunteer can add another caption <ptr target="#lclabs2017c"/>. The instructions state:</p>
         
         <p> <quote rend="block">Choose the transcription that most accurately captures the text as written. If
            multiple transcriptions appear valid, choose the first one. If the selected region isn't
            appropriate for the prompt, click <q>Bad region</q>.</quote></p>
            </item>
         </list>
            
            
            <figure>
               <head>A screenshot showing an example of the <q>Transcribe</q> step of the <title
                  rend="italic">Beyond Words </title>workflow. Note that the photograph caption is
                  pre-populated using the OCR falling within the bounding box <ptr target="#lclabs2017b"/>.</head>
               <graphic url="resources/images/image4.png"/>
               <figDesc>screenshot of a newspaper article and a text box where the article has been transcribed</figDesc>
            </figure>
 
            
            
         <p>For the purposes of <title rend="italic">Newspaper Navigator</title>, only the bounding boxes
            from the <q>Mark</q> step and the category labels from the <q>Transcribe</q> step were utilized as
            training data; however, understanding the full workflow is essential because annotations
            are considered <q>verified</q> only if they have passed through the full workflow.</p>
         <p>A number of factors contribute to which <title rend="italic">Chronicling America </title>pages
            were processed by volunteers in <title rend="italic">Beyond Words</title>. First, the temporal
            restriction to World War 1-era pages affects the ability of the visual content
            recognition model to generalize: after all, if the model is trained on World War 1-era
            pages, how well should we expect it to perform on 19th century pages? I will return to
            this question later in the section. Moreover, <title rend="italic">Beyond Words
            </title>volunteers could select either an entirely random page or a random page from a
            specific state, an important affordance from an engagement perspective, as volunteers
            could explore the local histories of states in which they are interested. But this
            affordance is also imprinted on the training data, as certain states - and thus, certain
            newspapers - appear at a higher frequency than if the World War-1 era <title rend="italic"
               >Chronicling America</title> pages had been drawn randomly from this temporal range in
               <title rend="italic">Chronicling America</title>.</p>
         <p>Furthermore, it should be noted that the <q>Mark</q> and <q>Transcribe</q> steps - specifically,
            drawing bounding boxes and labeling the visual content category - are complex tasks.
            Because newspaper pages are remarkably heterogenous, ambiguities and edge-cases abound.
            Should a photo collage be marked as one unit or segmented into constituent parts? What
            precisely is the distinction between an editorial cartoon and an illustration? How much
            relevant textual content should be included in a bounding box? Naturally, volunteers did
            not always agree on these choices. In this regard, the notion of a ground-truth, a set
            of perfect annotations against which we can assess performance, is itself called into
            question. Moreover, with thousands of annotations, mistakes in the form of missed visual
            content, as well as misclassifications, are inevitable.<note> It should be noted that
                  <title rend="italic">Beyond Words </title>was introduced by LC Labs as an experiment,
               with no interventions in workflow or community management.</note> These ambiguities
            and errors are natural components of <emph>any</emph> training dataset and
            must be taken into account when analyzing a machine learning model’s predictions.</p>
         <p>A breakdown of <title rend="italic">Beyond Words</title> annotations included in the training
            data can be found in the second column of Table 1. I downloaded these 6,732
            publicly-accessible annotations as a JSON file on December 1, 2019. Table 1 reveals an
            imbalance between the number of examples for each category; in the language of machine
            learning, this is called <emph>class imbalance</emph>. While the discrepancy
            between maps and photographs is to be expected, the fact that so few maps were included
            was concerning from a machine learning standpoint: a machine learning algorithm’s
            ability to generalize to new data is dependent on having many diverse training examples.
            To address this concern, I searched <title rend="italic">Chronicling America </title>and
            identified 134 pages published between January 1st, 1914, and December 31st, 1918, that
            contain maps. I then annotated these pages myself.</p>
         <p>In addition, during the development of the <title rend="italic">Newspaper Navigator</title>
            pipeline, I realized the value in training the visual content recognition model to
            identify headlines and advertisements. Consequently, I added annotations of headlines
            and advertisements for all 3,559 pages included in the training data. The statistics for
            this augmented set of annotations can be found in the third column of Table 1. Though I
            attempted to use a consistent approach to annotating the headlines and advertisements,
            my interpretation of what constitutes a headline is certainly not unimpeachable: I am
            not a trained scholar of periodicals or of print culture; even if I were, the task
            itself is inevitably subjective. Furthermore, I made decisions to annotate large grids
            of classified ads as a single ad to expedite the annotation process. Whether this was a
            correct judgment call can be debated. Lastly, annotating all 3,559 pages for headlines
            and advertisements required a significant amount of time, and there are inevitably
            mistakes and inconsistencies embedded within the annotations. My own decisions in terms
            of how to annotate, as well as my mistakes and inconsistencies, are embedded within the
            visual content recognition model through training. For those interested in examining the
            training data directly, the data can be found in the GitHub repository for this project
            <ptr target="#lee2020a"/>.</p>
         <table>
            <head>A breakdown of <title rend="italic">Beyond Words </title>annotations included in the
               training data for the visual content recognition model, as well as all annotations
               constituting the training data.</head>
            <row role="label">
               <cell role="label">Category</cell>
               <cell role="label">Beyond Words Annotations</cell>
               <cell role="label">Total Annotations</cell>
            </row>
            <row>
               <cell>Photograph</cell>
               <cell>4,193</cell>
               <cell>4,254</cell>
            </row>
            <row>
               <cell>Illustration</cell>
               <cell>1,028</cell>
               <cell>1,048</cell>
            </row>
            <row>
               <cell>Map</cell>
               <cell>79</cell>
               <cell>215</cell>
            </row>
            <row>
               <cell>Comic/Cartoon</cell>
               <cell>1,139</cell>
               <cell>1,150</cell>
            </row>
            <row>
               <cell>Editorial Cartoon</cell>
               <cell>293</cell>
               <cell>293</cell>
            </row>
            <row>
               <cell>Headline</cell>
               <cell>-</cell>
               <cell>27,868</cell>
            </row>
            <row>
               <cell>Advertisement</cell>
               <cell>-</cell>
               <cell>13,581</cell>
            </row>
            <row>
               <cell>
                  <title rend="italic">Total</title>
               </cell>
               <cell>6,732</cell>
               <cell>48,409</cell>
            </row>
         </table> 

         <p>Beyond the construction of the training data, I made manifold decisions regarding the
            selection of the correct model architecture and the training of the model. Because this
            discussion surrounding these choices is quite technical, I refer the reader to <ptr target="#lee2020b"/>
            for an in-depth examination. However, I will state
            that the choice of model, the number of iterations for which the model was trained, and
            the choice of model parameters are all of significant import for the resulting trained
            model and consequently, the <title rend="italic">Newspaper Navigator </title>dataset.</p>
         <p>I will now turn to the visual content recognition model’s outputs in relation to the <title
               rend="italic">Newspaper Navigator </title>pipeline. The model itself consumes a
            lower-resolution version of a <title rend="italic">Chronicling America </title>page as input
            and then outputs a JSON file containing predictions, each of which consists of bounding
            box coordinates,<note> Bounding box coordinates refer to the positions of the corners of
               the predicted bounding box, relative to the image coordinates.</note> the predicted
            class (i.e., <q>photograph</q>, <q>map</q>, etc.), and a confidence score generated by the machine
            learning model.<note> The confidence score is examined in more detail in the next
               section.</note> Cropping out and saving the visual content required extra code to be
            written. Because the high-resolution images of the <title rend="italic">Chronicling
               America</title> pages, in addition to the METS/ALTO OCR, amount to many tens of
            terabytes of data, questions of data storage became major considerations in the
            pipeline. I chose to save the extracted visual content as lower-resolution JPEG images
            in order to reduce the upload time and lessen the storage burden. Though the <title
               rend="italic">Newspaper Navigator</title> dataset retains identifiers to all
            high-resolution pages in <title rend="italic">Chronicling America, </title>the images in the
               <title rend="italic">Newspaper Navigator </title>dataset are altered by the downsampling
            procedure. This downsampling procedure should be free of any significant biasing
            effects. </p>
         <p>For visual content recognition, <title rend="quotes">Newspaper Navigator</title> utilized an object detection model,
            which is a type of widely-used computer vision technique for identifying objects in
            images. The performance for computer vision techniques is regularly measured using
            metrics such as average precision. For <title rend="quotes">Newspaper Navigator</title>, the model’s performance on a
            specific page, as measured by average precision, is dependent on a confluence of
            factors. These factors include the page’s layout, artifacts and distortions introduced
            in the microfilming and digitization process, and - most importantly - the composition
            of the training data. Thus, each image is <q>seen</q> differently by the visual content
            recognition model. In Figure 5, I show the four images of W.E.B. Du Bois, as identified
            by the visual content recognition model and saved in the <title rend="italic">Newspaper
               Navigator </title>dataset. Each image is cropped slightly differently. In the case of
            the image from the <title rend="italic">Iowa State Bystander</title>, extra text is included,
            while in the case of the images from <title rend="italic">The Broad Ax</title>, the captions
            are partially cut off. The loss in image quality is due to the aforementioned
            downsampling performed by the pipeline. This downsampling leads to artifacts such as the
            dots appearing on Du Bois’s face in the image from the <title rend="italic">Iowa State
               Bystander</title>, as well as the streaks in the image from <title rend="italic">Franklin’s
               Paper the Statesman</title>, that are not present in Figure 2.</p>
         <p>Returning to the question of the visual content recognition model’s performance on pages
            published outside of the temporal range of the training data (1914-1918), it is possible
            to provide a quantitative answer by measuring average precision on test sets of
            annotated pages from different periods of time. In <ptr target="#lee2020b"/>, I describe this
            analysis in detail and demonstrate that the performance declines for pages published
            between 1875 and 1900 and further declines for pages published between 1850 and 1875.
            This confirms that the composition of the training data directly manifests in the
            model’s performance. While it is certainly the case that the <title rend="italic">Newspaper
               Navigator</title> dataset can still be used for scholarship related to 19th century
            newspapers in <title rend="italic">Chronicling America</title>, any scholarship with the 19th
            century visual content in the <title rend="italic">Newspaper Navigator </title>dataset must
            consider how the dataset may skew what visual content is represented.</p>
            
            
            
            
            <figure>
               <head>The four images of W.E.B. Du Bois, as identified by the visual content
                  recognition model and included in the <title rend="italic">Newspaper Navigator </title>dataset
                  <ptr target="#navigator1910a"/>; <ptr target="#navigator1910c"/>; <ptr target="#navigator1910e"/>;
                  <ptr target="#navigator1910g"/>.</head>
               <graphic url="resources/images/image5.jpg"/>
               <figDesc>four images of W.E.B. Du Bois</figDesc>
            </figure>
         
        
         <p>Let me conclude this section with a discussion of the act of visual content extraction
            itself in relation to digitization. While this extraction enables a wide range of
            affordances for searching <title rend="italic">Chronicling America</title>, it is also an act
            of decontextualization: visual content no longer appears in relation to the <title
               rend="italic">mise-en-page</title>. In the Appendix, the full pages containing the
            photographs of W.E.B. Du Bois are reproduced, showing each photograph in context. Only
            by examining the full pages does it become clear that the article featuring W.E.B. Du
            Bois was printed with a second article in the <title rend="italic">Iowa State
               Bystander</title> and <title rend="italic">The Broad Ax</title>, the headline of which reads:
            <title rend="quotes">ANTI-LYNCHING SOCIETY ORGANIZED IN BOSTON — Afro-American Women Unite For Active
            Campaign Against Injustice.</title> Furthermore, upon examination, the <title rend="italic">Iowa
               State Bystander </title>front page features the article on <title rend="italic">The
               Crisis</title> and W.E.B. Du Bois as the most prominent article of the issue. Though
            links between the extracted visual content and the original <title rend="italic"
               >Chronicling America</title> pages are always retained, this decontextualization
            inevitably transmutes <emph>how</emph> we perceive and interact with the
            visual content in <title rend="italic">Chronicling America</title>. Indeed, all uses of
            machine learning for metadata enhancement are a form of decontextualization, centering
            the user’s discovery and analysis of content around the metadata itself. 
         </p>
         </div>
         <div>
         
         <head>VII. Prediction Uncertainty</head>
         <p>Perhaps the most fundamental question to ask of the <title rend="italic">Newspaper
               Navigator </title>dataset is: <q>How many photographs does the dataset contain?</q> Because
            the dataset has been constructed using a machine learning model, predictions are
            ultimately probabilistic in nature, quantified by the confidence score returned by the
            model. This begs the question of what counts as an identified unit of visual content: a
            user is much more inclined to tally a prediction of a map if it has an associated
            confidence score of 99% rather than 1%. However, choosing this cut is fundamentally a
            subjective decision, informed by the user’s end goals with the dataset. In the language
            of machine learning, picking a stringent confidence cut (i.e., only counting predictions
            with high confidence scores) emphasizes <emph>precision</emph>: a prediction
            of a photograph likely corresponds to a true photograph, but the predictions will suffer
            from false negatives. Conversely, picking a loose confidence cut (i.e., counting
            predictions with low confidence scores) emphasizes <emph>recall</emph>: most
            true photographs are identified as such, but the predictions will suffer from many false
            positives. In this regard, the total number of images in the <title rend="italic">Newspaper
               Navigator </title>dataset is dependent on one’s desired tradeoff between precision and
            recall. In Table 2, I show the dynamic range of the dataset size, as induced by three
            different cuts on confidence score: 90%, 70%, and 50%. Figure 6 shows the effects of
            different cuts on confidence score for the page featuring W.E.B. Du Bois in the November
            26,1910, issue of <title rend="italic">The Broad Ax</title>.</p>
         <table>
            <head>The number of occurrences of each category of visual content in the <title
               rend="italic">Newspaper Navigator</title> dataset with confidence scores above the
               listed thresholds (0.9, 0.7, 0.5).</head>
            <row role="label">
               <cell role="label">Category</cell>
               <cell role="label">≥ 90%</cell>
               <cell role="label">≥ 70%</cell>
               <cell role="label">≥ 50%</cell>
            </row>
            <row>
               <cell>Photograph</cell>
               <cell>1.59 x 10<hi rend="superscript">6</hi></cell>
               <cell>2.63 x 10<hi rend="superscript">6</hi></cell>
               <cell>3.29 x 10<hi rend="superscript">6</hi></cell>
            </row>
            <row>
               <cell>Illustration</cell>
               <cell>8.15 x 10<hi rend="superscript">5</hi></cell>
               <cell>2.52 x 10<hi rend="superscript">6</hi></cell>
               <cell>4.36 x 10<hi rend="superscript">6</hi></cell>
            </row>
            <row>
               <cell>Map</cell>
               <cell>2.07 x 10<hi rend="superscript">5</hi></cell>
               <cell>4.59 x 10<hi rend="superscript">5</hi></cell>
               <cell>7.54 x 10<hi rend="superscript">5</hi></cell>
            </row>
            <row>
               <cell>Comic/Cartoon</cell>
               <cell>5.35 x 10<hi rend="superscript">5</hi></cell>
               <cell>1.23 x 10<hi rend="superscript">6</hi></cell>
               <cell>2.06 x 10<hi rend="superscript">6</hi></cell>
            </row>
            <row>
               <cell>Editorial Cartoon</cell>
               <cell>2.09 x 10<hi rend="superscript">5</hi></cell>
               <cell>6.67 x 10<hi rend="superscript">5</hi></cell>
               <cell>1.27 x 10<hi rend="superscript">6</hi></cell>
            </row>
            <row>
               <cell>Headline</cell>
               <cell>3.44 x 10<hi rend="superscript">7</hi></cell>
               <cell>5.37 x 10<hi rend="superscript">7</hi></cell>
               <cell>6.95 x 10<hi rend="superscript">7</hi></cell>
            </row>
            <row>
               <cell>Advertisement</cell>
               <cell>6.42 x 10<hi rend="superscript">7</hi></cell>
               <cell>9.48 x 10<hi rend="superscript">7</hi></cell>
               <cell>1.17 x 10<hi rend="superscript">8</hi></cell>
            </row>
            <row>
               <cell>
                  <emph>Total</emph>
               </cell>
               <cell>1.02 x 10<hi rend="superscript">8</hi></cell>
               <cell>1.56 x 10<hi rend="superscript">8</hi></cell>
               <cell>1.98 x 10<hi rend="superscript">8</hi></cell>
            </row>
         </table>
            

            <figure>
               <head>The same page of <title rend="italic">The Broad Ax </title>from November 26, 1910,
                  along with predictions from the visual content recognition model, thresholded on
                  confidence score at 5%, 50%, 70%, and 90% <ptr target="#navigator1910g"/>; <ptr target="#navigator1910h"/>.
                  Note that red corresponds to a prediction of <q>photograph</q>, cyan
                  corresponds to a prediction of <q>headline</q>, and blue corresponds to a prediction of
                  <q>advertisement</q>.</head>
               <graphic url="resources/images/Figure_6.jpg"/>
               <figDesc>screenshot of four newspaper pages</figDesc>
            </figure>

         <p>Rather than pre-selecting a confidence score threshold, the <title rend="italic">Newspaper
               Navigator </title>dataset contains all predictions with confidence scores greater than
               5%,<note> This modest cut is provided to remove the large number of predictions with
               confidence scores between 0% and 5%, which have high false-positive rates, and thus
               reduce the size of the <title rend="italic">Newspaper Navigator </title>dataset.</note>
            allowing the user to define their own confidence cut when querying the dataset. However,
            the website for the <title rend="italic">Newspaper Navigator</title> dataset also includes
            hundreds of pre-packaged datasets in order to make it easier for users to work with the
            dataset. In particular, users can download zip files containing all of the visual
            content of a specific type with confidence scores greater than or equal to 90%, for any
            year from 1850 to 1963. I made this choice of 90% as the threshold cut for these
            pre-packaged datasets based on heuristic evidence from inspecting sample pre-packaged
            datasets by eye. However, as articulated above, based on different use cases, this cut
            of 90% may be too restrictive or permissive: relevant visual content may be absent from
            the pre-packaged dataset or lost in a sea of other examples. In Figure 7, I show the
            visual content recognition model’s confidence scores for the four images of W.E.B. Du
            Bois described throughout this data archaeology. The effect of a cut on confidence score
            can be seen here: selecting a cut of 95% would exclude the image from <title rend="quotes"
               >Franklin’s Paper the Statesman</title>. I raise this point to emphasize that even this
            seemingly innocuous choice of 90% for the pre-packaged datasets alters the discovery
            process and thus can have an impact on scholarship. </p>
            
            
         
            <figure>
               <head>The visual content recognition model’s confidence score for each of the four
                  images of W.E.B. Du Bois. Note how the model assigns a different confidence score to
                  each identified image <ptr target="#navigator1910b"/>; <ptr target="#navigator1910d"/>;
                  <ptr target="#navigator1910f"/>; <ptr target="#navigator1910h"/>.</head>
               <graphic url="resources/images/image8.jpg"/>
               <figDesc>Four images of W.E.B. Du Bois</figDesc>
            </figure>
            
        
         <p>Just as the bounding box predictions themselves are affected by the training data, as
            well as newspaper page layout, date of publication, and noise from the digitization
            pipeline, so too are the confidence scores. In particular, the visual content
            recognition model suffers from high-confidence misclassifications, for example,
            crossword puzzles that are identified as maps with confidence scores greater than 90%.
            High-confidence misclassifications pose challenges for machine learning writ large, and
            the field of explainable artificial intelligence is largely devoted to developing tools
            for understanding this type of misclassification <ptr target="#weld2019"/>. However, these
            high-confidence misclassifications can often be traced back to the composition of the
            training set. For example, the fact that the visual content recognition model sometimes
            identifies crossword puzzles as maps with high confidence is likely due to the fact that
            the training data did not contain enough labeled examples of maps and crossword puzzles
            for the visual content recognition model to differentiate them with high accuracy. </p>
         <p>The questions surrounding confidence scores and probabilistic descriptions of items is
            by no means restricted to the <title rend="italic">Newspaper Navigator </title>dataset. I echo
            Thomas Padilla’s assertion that <quote rend="inline">attempts to use algorithmic methods to describe
            collections must embrace the reality that, like human descriptions of collections,
            machine descriptions come with varying measure of certainty</quote> <ptr target="#padilla2019"/>.
            Machine-generated metadata such as OCR are also fundamentally probabilistic in nature;
            this fact is not immediately apparent to end users of cultural heritage collections
            because cuts on confidence score are typically chosen before surfacing the metadata.
            Effectively communicating confidence scores, probabilistic descriptions, and the
            decisions surrounding them to end users remains a challenge for content stewards.</p>
         </div>
         <div>
         <head>VIII. OCR Extraction</head>
         <p>In the <title rend="italic">Newspaper Navigator </title>pipeline, a textual description of
            each prediction is obtained by extracting the OCR within each predicted bounding box.
            The resulting textual description is thus dependent on not only the OCR provided by <title
               rend="italic">Chronicling America </title>but also the exact coordinates of the bounding
            box: if the coordinates of a word in the localized OCR extend beyond the bounds of the
            box, the word is excluded. I experimented with utilizing tolerance limits to allow words
            that extend just beyond the bounds of the boxes to be included, but doing so ultimately
            introduces false positives as well, as words from neighboring articles or visual content
            were inevitably included some fraction of the time. Once again, the tradeoff between
            false positives and false negatives is manifest.</p>
         <p>In Figure 8, I show the textual descriptions of the four images of W.E.B. Du Bois, as
            identified by the <title rend="italic">Newspaper Navigator</title> pipeline. Significantly, in
            the <title rend="italic">Newspaper Navigator </title>dataset, the OCR is stored as a list of
            words, with line breaks removed; these lists are what appear in Figure 8. These four
            examples provide intuition as to how the captions are altered. While the examples from
            the <title rend="italic">Iowa State Bystander </title>and <title rend="italic">Franklin’s Paper
               the Statesman</title> both have very similar captions as shown in Figure 3, the captions for
            both of the examples from <title rend="italic">The Broad Ax</title> are unrecognizable.
            Because the bounding boxes have clipped the caption, none of the characters from the
            proper OCR captions from Figure 3 are present. Furthermore, the captions contain OCR
            noise due to the OCR engine attempting to read text from the photographs. Consequently,
            the mentions of W.E.B. Du Bois are erased from the textual descriptions in the <title
               rend="italic">Newspaper Navigator </title>dataset. The visual content in the <title
               rend="italic">Newspaper Navigator </title>dataset is thus decontextualized not only in
            the sense that the visual content is extracted from the newspaper pages but also in the
            sense that the OCR extraction method further alters the textual descriptions. While the
            images from the <title rend="italic">Iowa State Bystander</title> and <title rend="italic"
               >Franklin’s Paper the Statesman</title> are still recoverable with fuzzy keyword search,
            the two images from <title rend="italic">The Broad Ax </title>are impossible to retrieve with
               <title rend="italic">any </title>form of keyword search, revealing another instance in
            which employing automated techniques for collections processing affects
            discoverability.</p>
            
            <figure>
               <head>The textual descriptions of each image, as extracted from the OCR and saved in
                  the <title rend="italic">Newspaper Navigator </title>dataset <ptr target="#navigator1910b"/>;
                  <ptr target="#navigator1910d"/>; <ptr target="#navigator1910f"/>; <ptr target="#navigator1910h"/>.</head>
               <graphic url="resources/images/image9.jpg"/>
               <figDesc>four images of W.E.B. Du Bois</figDesc>
            </figure>
            
      
         <p>Fortunately, visual content can still be recovered using similarity search over the
            images themselves; these methods are discussed in detail in the next section. However,
            in the case of headlines, the errors introduced by OCR engines and the subsequent OCR
            extraction have no recourse, as similarity search for images of headlines would only
            capture similar typography and text layout.<note> The<title rend="italic"> Newspaper
                  Navigator </title>dataset does not retain the cropped images of headlines, as the
               textual content is more salient than visual snippets in the case of headlines.</note>
         </p>
         <p>To illustrate the effects of this OCR extraction on headlines, I reproduce in Table 3
            the extracted OCR as it appears in the <title rend="italic">Newspaper Navigator
            </title>dataset for Franklin F. Johnson’s headline: </p>
         <p>NEW MOVEMENT </p>
         <p>BEGINS WORK</p>
         <p>Plan and Scope of the Asso-</p>
         <p>ciation Briefly Told. </p>
         <p>Will Publish the Crisis. </p>
         <p>Review of Causes Which Led to the </p>
         <p>Organization of the Association in </p>
         <p>New York and What Its Policy Will </p>
         <p>Be-Career and Work of Professor </p>
         <p>W.E.B. Du Bois</p>
         <table>
            <head>The extracted OCR associated with each of the four photographs of W.E.B. Du
               Bois <ptr target="#navigator1910b"/>; <ptr target="#navigator1910d"/>; <ptr target="#navigator1910f"/>; <ptr target="#navigator1910h"/>.</head>
            <row role="label">
               <cell role="label">
                  
                     <title rend="italic">Iowa State Bystander </title>
                  
                 (14 Oct. 1910)
               </cell>
               <cell role="label">
                  
                     <title rend="italic">Franklin’s Paper the Statesman</title>
                  
                  (15 Oct. 1910)
               </cell>
               <cell role="label">
                 
                     <title rend="italic">The Broad Ax </title>
                  
                  (15 Oct. 1910)
               </cell>
               <cell role="label">
                 
                     <title rend="italic">The Broad Ax </title>
                  
                  (26 Nov. 1910)
               </cell>
            </row>
            <row>
               <cell>98.72%</cell>
               <cell>99.57%</cell>
               <cell>99.76%</cell>
               <cell>99.70%</cell>
            </row>
            <row>
               <cell>
                  [<q>NEW </q>, <q>MOVEMENT </q>, <q>BEGINS </q>, <q>WORK </q>, <q>and </q>, <q>Plan </q>, <q>Scope </q>, <q>of </q>, <q>the </q>,
                     <q>Asso\u00ad </q>, <q>ciation </q>, <q>Briefly </q>, <q>Told. </q>, <q>WILL </q>, <q>PUBLISH </q>, <q>THE </q>,
                     <q>CRISIS. </q>, <q>Review </q>, <q>of </q>, <q>Causae </q>, <q>Which </q>, <q>Lad </q>, <q>to </q>, <q>the </q>,
                     <q>Organisation </q>, <q>of </q>, <q>the </q>, <q>Auooiation </q>, <q>In </q>, <q>Naw </q>, <q>York </q>, <q>and </q>, <q>JWhat </q>,
                     <q>It* </q>, <q>Polioy </q>, <q>Will </q>, <q>Ba\u2014Career </q>, <q>and </q>, <q>Wark </q>, <q>of </q>,
                     <q>Profeasor</q>]
               </cell>
               <cell>
                  [<q>NEW </q>, <q>MOVEMENT </q>, <q>BEGINS </q>, <q>WORK </q>, <q>Plan </q>, <q>and </q>, <q>Scope </q>, <q>of </q>, <q>the </q>,
                     <q>Asso </q>, <q>ciation </q>, <q>Briefly </q>, <q>Told. </q>, <q>WILL </q>, <q>PUBLISH </q>, <q>THE </q>, <q>CRISIS.</q>]
               </cell>
               <cell>
                  [<q>NEW </q>, <q>MOVEMENT </q>, <q>BEGINS </q>, <q>WORK </q>, <q>Plan </q>, <q>and </q>, <q>Sep </q>, <q>if </q>, <q>the </q>,
                     <q>Asso </q>, <q>ciation </q>, <q>Briefly </q>, <q>Told. </q>, <q>WILL </q>, <q>PUBLISH </q>, <q>THE </q>, <q>CRISIS, </q>,
                     <q>Be </q>, <q>Career </q>, <q>nnd </q>, <q>Work </q>, <q>of </q>, <q>Professor </q>, <q>W. </q>, <q>E. </q>, <q>B. </q>, <q>Du </q>,
                     <q>Bois. </q>, <q>Review </q>, <q>of </q>, <q>Causes </q>, <q>Which </q>, <q>Led </q>, <q>to </q>, <q>the </q>, <q>Oraanteallon </q>,
                     <q>of </q>, <q>th. </q>, <q>A.Me!.!?n </q>, <q>i </q>, <q>i </q>, <q>New </q>, <q>York </q>, <q>and </q>, <q>What </q>, <q>IU </q>,
                     <q>Policy </q>, <q>Will</q>]
               </cell>
               <cell>
                  [<q>NEW </q>, <q>MOVEMENT </q>, <q>BEGINS </q>, <q>WORK </q>, <q>Plan </q>, <q>and </q>, <q>Scope </q>, <q>of </q>, <q>the </q>,
                     <q>Asso </q>, <q>ciation </q>, <q>Briefly </q>, <q>Told. </q>, <q>WILL </q>, <q>PUBLISH </q>, <q>THE </q>, <q>CRISIS. </q>,
                     <q>Review </q>, <q>of </q>, <q>Causes </q>, <q>Which </q>, <q>Lad </q>, <q>to </q>, <q>tha </q>, <q>Organization </q>, <q>of </q>,
                     <q>the\" </q>, <q>Association </q>, <q>In </q>, <q>New </q>, <q>York </q>, <q>and </q>, <q>What </q>, <q>Its </q>, <q>Policy </q>,
                     <q>Will</q>]
               </cell>
            </row>
         </table>
            
            
  
         <p>The full pages are reproduced in the appendix for reference. Notably, all four extracted
            headlines contain OCR errors, as well as missing words due to the OCR extraction. The
            visual content recognition model consistently fails to include the last line of the
            headline, <quote rend="inline">W.E.B. Du Bois,</quote> revealing another case in which Du Bois’s name is rendered
            inaccessible by keyword search in the <title rend="italic">Newspaper Navigator
            </title>dataset. </p>
         </div>
         <div>
         <head>IX. Image Embeddings</head>
         <p>An <term>image embedding</term> canonically refers to a low-dimensional
            representation of an image, often a list of a few hundred or a few thousand numbers,
            that captures much of the image’s semantic content. Image embeddings are typically
            generated by feeding an image into a pre-trained neural image classification model
            (i.e., a model that takes in an image and outputs a label of <q>dog</q> or <q>cat</q>) and
            extracting a representation of the image from one of the model’s hidden layers, often
            the penultimate layer.<note> If these words are unfamiliar, the three takeaways listed
               are more important.</note> Image embeddings are valuable for three reasons:</p>
         <list type="ordered">
            <item>Image embeddings are remarkably adept at capturing semantic similarity between
               images. For example, images of dogs tend to be clustered together in embedding space,
               with images of bicycles in another cluster and images of buildings in yet another.
               These clusters can be fine-grained: sometimes, the red bicycles are grouped closer
               together than the blue bicycles.</item>
            <item>Image embeddings can be constructed by feeding images into an image classification
               model already trained on another dataset (such as ImageNet), meaning that generating
               image embeddings is a useful method for comparing images without having to construct
               training data by labeling images. </item>
            <item>Image embeddings are low-dimensional and thus much smaller in size than the images
               themselves (i.e., on the order of kilobytes instead of megabytes). As a result, image
               embeddings are much less computationally expensive to compare to one another when
               conducting similarity search, clustering, or related tasks. In short, image
               embeddings speed up image comparison.</item>
         </list>
         <p>Utilizing image embeddings to visualize and explore large collections of images has
            become an increasingly common approach among cultural heritage practitioners. Projects
            and institutions that have utilized image embeddings for visualizing cultural heritage
            collections include the Yale Digital Humanities Lab’s PixPlot interface <ptr target="#yale2017"/>,
            the National Neighbors project <ptr target="#lincoln2019"/>, Google Arts
            and Culture <ptr target="#googlearts2018"/>, The Norwegian National Museum’s Principal
            Components project <ptr target="#nasjonalmuseet2017"/>, the State Library of New South Wales’s Aero
            Project <ptr target="#geraldo2020"/>, the Royal Photographic Society <ptr target="#vane2018"/>, The American Museum
            of Natural History <ptr target="#foo2019"/>, and The National Library of the Netherlands <ptr target="#lonij2017"/>;
            <ptr target="#weavers2020"/>. These visualizations provide insights into
            broader themes in the collections, thereby allowing curators, researchers, and the
            public to explore collections at a scale previously only possible by organizing images
            by color or other low-level features.<note> For an introduction to some of these methods
               with lower-level features, see <ptr target="#manovich2012"/>.</note> In this regard, image
            embeddings provide new affordances for searching over images that complement canonical
            faceted and keyword search.</p>
         <p>Because these image embeddings enable these visualization approaches and open the door
            to similarity search and recommendation, I opted to include image embeddings as part of
            the <title rend="italic">Newspaper Navigator</title> pipeline. Indeed, these image embeddings
            power the similarity search functionality in the <title rend="italic">Newspaper
               Navigator</title> user interface and, in this regard, are crucial to the broader vision
            of the project <ptr target="#lee2020c"/>.<note> The search application can be found at: <ref
                  target="https://news-navigator.labs.loc.gov/search"
                  >https://news-navigator.labs.loc.gov/search</ref>.</note> To generate the
            embeddings, I utilized ResNet-18 and ResNet-50, two variants of a prominent deep
            learning architecture for image classification, both of which had already been
            pre-trained on ImageNet <ptr target="#he2016"/>. </p>
         <p>ImageNet is perhaps the most well-known image dataset in the history of machine
            learning. Constructed by scraping publicly available images from the internet and
            recruiting Amazon Mechanical Turk workers to annotate the images, ImageNet contains
            approximately 14 million images across 20,000 categories <ptr target="#deng2009"/>; <ptr target="#imagenet2020a"/>.
            Kate Crawford and Trevor Paglen’s essay <title rend="quotes">Excavating AI: The Politics of Images in
            Machine Learning Training Sets</title> offers a history and incisive critique of the
            classification schema of ImageNet; here, I will summarize the most salient critiques.
            First, many of the categories in the taxonomy utilized are themselves marginalizing
            <ptr target="#crawford2019"/>. Though many of the classes relating to people were removed
            in 2019, ImageNet had previously bifurcated the <q>Natural Object <code>&gt;</code> Body <code>&gt;</code> Adult
            Body</q> category into <q>Male Body</q> and <q>Female Body</q> subcategories. Second, ethnic classes
            were included, implying that 1) classification into rigid categories of ethnicity is
            possible and appropriate and 2) a machine learning system could learn how to classify
            ethnicity from these images. Diving deeper, the classifications become horrifying in
            their supposed granularity: until 2019, an image of a woman in a bikini was accompanied
            with the tags <quote rend="inline">slattern, slut, slovenly woman, trollop</quote> <ptr target="#crawford2019"/>.
            Though many embedding models are pre-trained on subsets of ImageNet categories included
            in the ImageNet Large Scale Visual Recognition Challenge that elide these particularly
            troubling classifications, these classifications nonetheless necessitate a reckoning
            with our use of ImageNet writ large, especially in regard to how the semantics of
            ImageNet are projected onto any image embedding generated with such a model <ptr target="#russakovsky2015"/>.
            <note> The specific categories used in the challenge can be found at: <ref
                  target="http://image-net.org/challenges/LSVRC/2010/browse-synsets"
                  >http://image-net.org/challenges/LSVRC/2010/browse-synsets</ref>.</note>
         </p>
         <p>However, questions probing the data in ImageNet fail to critique the ethically
            questionable practices on which ImageNet is built. Though the researchers responsible
            for the dataset scraped all 14 million images from public URLs, ImageNet does not
            provide any guarantees on image copyright, as only the URLs are provided in the
            database: <quote rend="inline">The images in their original resolutions may be subject to copyright, so we
               do not make them publicly available on our server</quote> <ptr target="#imagenet2020b"/>.
            It is highly unlikely that a photographer with an image in the dataset could have
            known that a photograph could be used this way, much less actively consent to the
            image’s inclusion, as is the case with subjects in the photographs. Furthermore, the
            labels themselves were collected using Amazon’s Mechanical Turk platform, which has been
            repeatedly criticized for its exploitative labor practices: as of 2017, workers earned a
            median wage of approximately $2 an hour on the platform <ptr target="#haro2018"/>. Scholars
            including Natalia Cecire, Bonnie Mak, and Paul Fyfe have highlighted how outsourced
            marginalized labor underpins digitization efforts, and the reliance on Mechanical Turk
            for the production of ImageNet further entrenches the digitization and discovery process
            within a system of labor exploitation <ptr target="#cecire2011"/>; <ptr target="#mak2017"/>; <ptr target="#fyfe2016"/>. As
            cultural heritage practitioners and humanities researchers, we must acknowledge these
            exploitative practices, and we must reckon with how we perpetuate them through the use
            of ImageNet as a training source for image search and discovery. </p>
         <p>In offering these critiques, my intention is not to dismiss ImageNet in a wholesale
            manner. Certainly, the benefits of utilizing ImageNet are manifold, as evidenced by
            widespread community adoption, as well as new affordances for searching cultural
            heritage collections enabled by the dataset that are shaping the contours of digital
            scholarship. In the case of my own scholarship with Newspaper Navigator, I have elected
            to utilize machine learning models pre-trained on ImageNet precisely for these reasons.
            I offer these provocations instead to question how we can do better as a community, not
            only in imagining alternatives but in bringing them to fruition. Classification is an
            act of interpretive reduction, whether by human or machine, and thus manifests all too
            often as an act of oppression.<note> For more reading on this topic, see <ptr target="#bowker2000"/>.
            </note> And yet, the structure imposed by classification constitutes the
            very basis for search and discovery systems. The salient question is thus not how we
            dispense of these systems but rather how we progressively realize a more inclusive
            vision of these systems, from the labor practices behind their construction to the very
            classification taxonomies themselves.</p>
         <p>How, then, do image embeddings derived from ImageNet mediate our interactions with the
            photographs in <title rend="italic">Newspaper Navigator</title>? Figure 9 shows a
            visualization of 1,000 photographs from the <title rend="italic">Newspaper Navigator
            </title>dataset published during the year 1910. This visualization was created using the
            ResNet-50 image embeddings, as well as a dimensionality reduction algorithm known as
            T-SNE <ptr target="#vandermaaten2009"/>. With T-SNE, a cluster of photographs indicates
            that the photographs are likely semantically similar, but the size of the cluster and
            distances from other clusters bear no meaning <ptr target="#wattenberg2016"/>.
            With this in mind, we can examine the clusters. Despite the fact that the high-contrast,
            grayscale photographs in <title rend="italic">Newspaper Navigator</title> are markedly
            different, or <soCalled>out-of-sample,</soCalled> in comparison to the clear, color images in ImageNet, the
            clusters nonetheless capture semantic similarity. In Figure 9, we observe the clustering
            of photographs depicting crowds of people, as well as photographs depicting ships and
            the sea. This visualization technique with the image embeddings is thus powerful in
            helping to navigate large collections of photographs by their semantic content.</p>
         
         
         
         <figure>
            <head>A visualization of 1,000 photographs from the year 1910 in the <title
               rend="italic">Newspaper Navigator </title>dataset, generated using the <title rend="italic"
                  >Newspaper Navigator</title> ResNet-50 image embeddings.</head>
            <graphic url="resources/images/image10.jpg"/>
            <figDesc>image of a network graph</figDesc>
         </figure>
         
         
         <p>What about the photographs of W.E.B. Du Bois? In Figure 10, I show the clusters
            containing these four photographs. This visualization affords us a lens into the
            limitations of image embeddings. First, it is evident that image embeddings are directly
            impacted by the distortions of the digitization process: while the three photographs
            from <title rend="italic">Franklin’s Paper the Statesman </title>and <title rend="italic">The
               Broad Ax</title> are clustered together with other portraits, the photograph from the
               <title rend="italic">Iowa State Bystander</title> is located in an entirely different
            cluster - a consequence of the fact that the <title rend="italic">Iowa State Bystander</title>
            photograph is saturated and that W.E.B. Du Bois’s facial features are obscured (notably,
            neighboring photographs suffer from similar distortions). A search engine powered with
            these image embeddings would in all likelihood return the three photographs from <title
               rend="italic">Franklin’s Paper the Statesman</title> and <title rend="italic">The Broad
               Ax</title> together, but the fourth photograph would effectively be lost. This
            algorithmic mediation is particularly troubling because, as described in Section IV, the
            microfilming digitization process causes newspaper photographs of darker-skinned people
            to lose contrast. While this loss in image quality is marginalizing in its own right,
            image embeddings perpetuate this marginalization: digitized newspaper portraits of
            darker-skinned individuals are more likely to suffer from saturated facial features, in
            turn resulting in these photographs being lost during the discovery and retrieval
            process, as is the case with the saturated <title rend="italic">Iowa State Bystander
            </title>photograph of W.E.B. Du Bois in Figure 10. Understanding these limitations of image
            embeddings are particularly relevant in the case of <title rend="italic">Newspaper
               Navigator</title>, as these image embeddings power the visual similarity search
            affordance within the publicly-deployed <title rend="italic">Newspaper Navigator
            </title>search application <ptr target="#lee2020c"/>. Though machine learning methods are often
            offered as panaceas for automation, this algorithmic erasure reminds us that traditional
            methods of scholarship and historiography, such as detailed analyses and close readings
            of Black newspapers in <title rend="italic">Chronicling America</title>, are more important
            than ever to counter algorithmic bias.</p>
         
         
         <figure>
            <head>The same visualization as in Figure 9, this time showing the locations of the
               four photographs of W.E.B. Du Bois.</head>
            <graphic url="resources/images/image11.jpg"/>
            <figDesc>image of a network graph focused on W.E.B. Du Bois</figDesc>
         </figure>
         
      </div>
         <div>
         <head>X. Environmental Impact</head>
         <p>Any examination of a dataset whose construction required large-scale computing would be
            remiss in not investigating the environmental impact of the computation itself. The
            carbon emissions generated from training a state-of-the-art machine learning model such
            as BERT is comparable to a single flight across the United States; however, factoring in
            experimentation and tuning, the carbon emissions can quickly amount to the carbon
            emissions of a car over its entire lifetime, including fuel <ptr target="#strubell2019"/>.
            OpenAI’s GPT-3 model required several thousand petaflop/s-days to train; without
            specific numbers, the carbon emissions are not possible to calculate exactly, but they
            are nonetheless substantial <ptr target="#brown2020"/>. In response, machine learning
            researchers have recommended ideas such as <term>Green AI,</term> with the goal of encouraging the
            community to value computational efficiency and not just accuracy <ptr target="#schwartz2019"/>.</p>
         <p>In the case of <title rend="italic">Newspaper Navigator</title>, most of the compute time was
            devoted to processing all 16.3 million <title rend="italic">Chronicling America</title> pages
            with the visual content recognition model, as opposed to training the model itself. In
            Tables 4 and 5, I report details on training the model and running the pipeline, as well
            as the carbon emissions generated by each step, computed using the Machine Learning
            Impact Calculator <ptr target="#lacoste2019"/>. In total, approximately 380 kg CO2 were emitted
            during the construction of the <title rend="italic">Newspaper Navigator </title>dataset,
            including development, experimentation, training, pipeline processing, and
            post-processing. It should be noted that this number is an estimate, as the statistics
            for experimentation and post-processing are difficult to quantify exactly. Nonetheless,
            this is approximately equivalent to the carbon emissions incurred by a single person
            flying from Washington, D.C. to Boston <ptr target="#carbon"/>. I include
            these numbers in the hope that cultural heritage practitioners will consider the
            environmental impact of utilizing machine learning and artificial intelligence for
            digital content stewardship. Doing so is essential to the data archaeology: given that
            climate change will disproportionately affect cultural heritage institutions in regions
            unable to develop proper infrastructure to withstand rapid temperature fluctuations and
            unprecedented flooding, even the environmental impacts of utilizing machine learning
            within digital content stewardship has the capacity to contribute to erasure and
            marginalization. 
         </p>
         <table>
            <head>Carbon emissions from the GPU usage for <title rend="italic">Newspaper
               Navigator</title>, broken down by project component. Note that all computation was done
               on Amazon AWS g4dn instances in the zone <q>us-east-2</q>. The carbon emissions were
               calculated using the Machine Learning Impact Calculator <ptr target="#lacoste2019"/>.</head>
            <row role="label">
               <cell role="label">Activity</cell>
               <cell role="label"># of NVIDIA T4 GPUs</cell>
               <cell role="label">GPU Hours (each)</cell>
               <cell role="label">Carbon Emissions</cell>
            </row>
            <row>
               <cell>Training</cell>
               <cell>1</cell>
               <cell>19</cell>
               <cell>0.96 kg CO2</cell>
            </row>
            <row>
               <cell>Pipeline Processing</cell>
               <cell>8</cell>
               <cell>456</cell>
               <cell>144.56 kg CO2</cell>
            </row>
            <row>
               <cell>Experimentation for Training and Pipeline Processing (estimate)</cell>
               <cell>8</cell>
               <cell>24</cell>
               <cell>7.66 kg CO2</cell>
            </row>
            <row>
               <cell>
                  <emph>Total</emph>
               </cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>153.18 kg CO2</cell>
            </row>
         </table>
         

         <table>
            <head>Carbon emissions from the CPU usage for <title rend="italic">Newspaper
               Navigator</title>, broken down by project component. Note that all computation was done
               on Amazon AWS g4dn instances in the zone <q>us-east-2</q>. The CPU processors are all 2nd
               generation Intel Xeon Scalable Processors (Cascade Lake) <ptr target="#amazon2020"/>.
               The 48-core processor outputs approximately 350 W; the 4-core processor outputs
               approximately 104 W <ptr target="#intel2020a"/>; <ptr target="#intel2020b"/>. The carbon emissions were calculated
               using the Machine Learning Impact Calculator <ptr target="#lacoste2019"/>. Note that the energy
               consumption by RAM is not factored in, but it is insignificant in comparison to the CPU
               and GPU energy consumption.</head>
            <row role="label">
               <cell role="label">Activity</cell>
               <cell role="label">CPU Processor (#)</cell>
               <cell role="label"># Processor CPU Cores</cell>
               <cell role="label">CPU Hours (each)</cell>
               <cell role="label">Carbon Emissions</cell>
            </row>
            <row>
               <cell>Training</cell>
               <cell>1</cell>
               <cell>4 CPUs</cell>
               <cell>19</cell>
               <cell>1.13 kg CO2</cell>
            </row>
            <row>
               <cell>Pipeline Processing</cell>
               <cell>2</cell>
               <cell>48 CPUs</cell>
               <cell>456</cell>
               <cell>181.9 kg CO2</cell>
            </row>
            <row>
               <cell>Experimentation for Training and Pipeline Processing (<emph
                     >estimate</emph>)</cell>
               <cell>2</cell>
               <cell>48 CPUs</cell>
               <cell>24</cell>
               <cell>9.57 kg CO2</cell>
            </row>
            <row>
               <cell>Extra Computation (dataset post-processing, etc., <emph
                     >estimate</emph>)</cell>
               <cell>1</cell>
               <cell>48 CPUs</cell>
               <cell>168</cell>
               <cell>33.52 kg CO2</cell>
            </row>
            <row>
               <cell>
                  <emph>Total</emph>
               </cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>-</cell>
               <cell>226.12 kg CO2</cell>
            </row>
         </table>
         </div>
         <div>

         <head> XI. Conclusion</head>
         <p>In this data archaeology, I have traced four <title rend="italic">Chronicling America</title>
            pages reproducing the same photograph of W.E.B. Du Bois as they have traveled through
            the <title rend="italic">Chronicling America</title> and <title rend="italic">Newspaper
               Navigator</title> pipelines. The excavated genealogy of digital artifacts has revealed
            the imprintings of the complex interactions between humans and machines. Indeed, the
            journey of each newspaper page through the <title rend="italic">Chronicling America
            </title>and <title rend="italic">Newspaper Navigator</title> pipelines is one of refraction,
            mediation, and decontextualization that is compounded upon with each step. Decisions
            made decades ago when microfilming a newspaper page inevitably affect how the machine
            learning models employed for OCR, visual content extraction, and image embedding
            generation ultimately process the pages, render them as digital artifacts in the <title
               rend="italic">Newspaper Navigator </title>dataset, and mediate their
            discoverability.</p>
         <p>As articulated by Trevor Owens in <title rend="italic">The Theory and Craft of Digital
               Preservation</title>, machine learning and artificial intelligence are the <quote rend="inline">underlying
                  sciences for digital preservation</quote> <ptr target="#owens2018"/>. Though machine learning techniques
            provide us with new affordances for searching and studying cultural heritage materials,
            they have the power to perpetuate and amplify the marginalization and erasure of entire
            communities within the archive. This erasure, coupled with the labor practices involved
            in creating training data as well as the environmental impact of training and deploying
            machine learning models in large-scale digitization pipelines, necessitates that we
            continue to examine the broader socio-technical ecosystems in which we participate. In
            doing so, we can work toward a more inclusive vision of the digital collection and the
            ways in which we render its contents discoverable.</p>
         <p>How, then, is <title rend="italic">Newspaper Navigator</title> situated within this vision? In
            reimagining how we search over the visual content in <title rend="italic">Chronicling
               America</title>, one explicit goal of the project is to engage the public with the rich
            history preserved within historic American periodicals and thus build on <title
               rend="italic">Chronicling America</title> as a free-to-use, public domain resource for
            scholars, educators, students, journalists, genealogists, and beyond <ptr target="#lee2021a"/>. <ptr target="#lee2021b"/>.
            With <title rend="italic">Newspaper Navigator</title>, it is
            my belief that the new modes of interacting with <title rend="italic">Chronicling
               America</title> have the capacity to not only enable a breadth of new scholarship but
            also foster engagement in and reckoning with America’s multilayered history of
            oppression. In documenting the different components of the project with this data
            archaeology and corresponding technical paper <ptr target="#lee2020b"/>, as well as releasing
            the full dataset and all code into the public domain, I have intended to be as
            transparent as possible with the tools and methodologies employed. <title rend="italic"
               >Newspaper Navigator </title>is not without its shortcomings, but my hope is that the
            project contributes to this vision of the digital collection through transparency and
            inclusivity, as well as the scholarship and pedagogy that it has enabled.</p>
         <p>I offer this case study not only to contextualize the <title rend="italic">Newspaper
               Navigator </title>dataset but also to advocate for the autoethnographic data archaeology
            as a valuable apparatus for reflecting on a cultural heritage dataset from a humanistic
            perspective. Though the digital humanities community has yet to adopt the data
            archaeology as standard practice when creating and releasing cultural heritage datasets,
            doing so has the capacity to improve accountability and context surrounding applications
            of machine learning for both practitioners and end users. Given the manifold ways in
            which machine learning mediates access to the archive and perpetuates erasure,
            reflecting critically on these systems is not only urgent but essential for transparency
            and inclusivity. 
               
         </p>
         </div>
         <div>
         <head> Sources of Funding</head>
         <p>This material is based upon work supported by the National Science Foundation Graduate
            Research Fellowship under Grant DGE-1762114, as well as the Library of Congress
            Innovator in Residence Position.</p>
      </div>
         <div>
         <head>Acknowledgments</head>
         <p>I would like to thank Eileen Jakeway, Jaime Mears, Laurie Allen, Meghan Ferriter, Robin
            Butterhof, and Nathan Yarasavage at the Library of Congress, as well as Molly Hardy and
            Joshua Ortiz Baco at the National Endowment for the Humanities, for their thoughtful and
            enlightening feedback on drafts of this article. I am grateful to my Ph.D. advisor,
            Daniel Weld, at the University of Washington, for his support, guidance, and invaluable
            advice with <title rend="italic">Newspaper Navigator</title>. In addition, I would like to
            thank Kurtis Heimerl and Esther Jang at the University of Washington for the opportunity
            to formulate and write early sections of this data archaeology as part of this Spring’s
            CSE 599: <title rend="quotes">Computing for Social Good</title> course. </p>
         <p>Lastly, I would like to thank the following people who have shaped <title rend="italic"
               >Newspaper Navigator</title>: Kate Zwaard, Leah Weinryb Grohsgal, Abbey Potter, Chris
            Adams, Tong Wang, John Foley, Brian Foo, Trevor Owens, Mark Sweeney, and the entire
            National Digital Newspaper Program staff at the Library of Congress; Devin Naar, Stephen
            Portillo, Daniel Gordon, and Tim Dettmers at the University of Washington; Michael Haley
            Goldman, Robert Ehrenreich, Eric Schmalz, and Elliott Wrenn at the United States
            Holocaust Memorial Museum; Jim Casey at The Pennsylvania State University; Sarah Salter
            at Texas A&amp;M University-Corpus Christi; and Gabriel Pizzorno at Harvard University. </p>
            
         </div>
         
         <div>
            <head>Appendix:</head>
            <figure>
               <head><title rend="italic"> Iowa state bystander</title>. [volume] (Des Moines, Iowa), 14 Oct. 1910. Chronicling America:
                  Historic American Newspapers. Lib. of Congress. <ref
                     target="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/"
                     >https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</ref></head>
               <graphic url="resources/images/image12.png"/>
               <figDesc>screenshot of a newspaper page</figDesc>
            </figure>
 
            <figure>
               <head><title rend="italic">Franklin's paper the statesman</title>. (Denver, Colo.), 15 Oct. 1910. Chronicling America:
                  Historic American Newspapers. Lib. of Congress. <ref
                     target="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/"
                     >https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</ref></head>
               <graphic url="resources/images/image13.png"/>
               <figDesc>screenshot of a newspaper page</figDesc>
            </figure>

            <figure>
               <head><title rend="italic">The broad ax.</title> [volume] (Salt Lake City, Utah), 15 Oct. 1910. Chronicling America:
                  Historic American Newspapers. Lib. of Congress. <ref
                     target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/"
                     >https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</ref></head>
               <graphic url="resources/images/image14.png"/>
               <figDesc>screenshot of a newspaper page</figDesc>
            </figure>
            
            <figure>
               <head><title rend="italic">The broad ax.</title> [volume] (Salt Lake City, Utah), 26 Nov. 1910. Chronicling America:
                  Historic American Newspapers. Lib. of Congress. <ref
                     target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/"
                     >https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</ref></head>
               <graphic url="resources/images/image15.png"/>
               <figDesc>screenshot of a newspaper page</figDesc>
            </figure>
            
         </div>
      </body>
      <back>
         <listBibl>
         <bibl xml:id="alpertabrams2016" label="Alpert-Adams 2016"> Alpert-Abrams, H.<title rend="quotes">Machine Reading the Primeros Libros,</title><title
               rend="italic">Digital Humanities Quarterly</title> 10:4 (2016).</bibl>
            <bibl xml:id="amazon2020" label="Amazon Web Services, Inc. 2020"> <title rend="quotes">Amazon EC2 Instance Types - Amazon Web Services,</title>
            (2020) Amazon Web Services, Inc.<ref target="https://aws.amazon.com/ec2/instance-types/"
               > Available at: </ref>
            <ref target="https://aws.amazon.com/ec2/instance-types/"
               >https://aws.amazon.com/ec2/instance-types/</ref>. (Accessed: 5 June 2020).</bibl>
         <bibl xml:id="bailey2015" label="Bailey 2015"> Bailey, M. <title rend="quotes">#transform(Ing)DH Writing and Research: An Autoethnography of
            Digital Humanities and Feminist Ethics,</title> <title rend="italic">Digital Humanities
               Quarterly</title> 9:2 (2015).</bibl>
         <bibl xml:id="baker2001" label="Baker 2001"> Baker, N. <title rend="italic">Double Fold: Libraries and the Assault on
               Paper</title>. Random House (2001).</bibl>
         <bibl xml:id="barrall2005" label="Barrall and Guenther 2005"> Barrall, K. and Guenther, C. <title rend="quotes">Microfilm Selection for
            Digitization,</title> (2005). Available at: <ref
               target="https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf"
               >https://www.loc.gov/ndnp/guidelines/NEH_MicrofilmSelectionNDNP.pdf</ref>.</bibl>
         <bibl xml:id="bender2018" label="Bender and Friedman 2018"> Bender, E., and Friedman, B. <title rend="quotes">Data Statements for Natural
            Language Processing: Toward Mitigating System Bias and Enabling Better Science.</title> <title
               rend="italic">Transactions of the Association for Computational Linguistics</title> 6
            (2018): 587–604.
            <ref target="https://doi.org/10.1162/tacl_a_00041"
               >https://doi.org/10.1162/tacl_a_00041</ref> (Accessed 29 July 2021).</bibl>
         <bibl xml:id="bowker2000" label="Bowker and Star 2000"> Bowker, G., and Star, S. <title rend="italic">Sorting Things Out:
               Classification and Its Consequences</title>. MIT Press, Cambridge (2000).</bibl>
            <bibl xml:id="brown2020" label="Brown et al. 2020">Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei D. <title rend="quotes">Language Models Are Few-Shot
            Learners,</title><title rend="italic">ArXiv:2005.14165 [Cs]</title> (2020),<ref
               target="http://arxiv.org/abs/2005.14165"> Available at: </ref>
            <ref target="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</ref>
            (Accessed: 6 June 2020).</bibl>
         <bibl xml:id="carbon" label="Carbon Footprint Calculator no date"> <title rend="quotes">Carbon Footprint Calculator,</title> <ref
               target="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3"
               >Available at: </ref>
            <ref
               target="https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3"
               >https://calculator.carbonfootprint.com/calculator.aspx?lang=en-GB&amp;tab=3</ref>.
            (Accessed: 6 June 2020).</bibl>
         <bibl xml:id="cecire2011" label="Cecire 2011"> Cecire, N.<title rend="quotes">Works Cited: The Visible Hand,</title><title rend="italic"> Works
               Cited</title> (blog) (2011). Available at: <ref
               target="http://nataliacecire.blogspot.com/2011/05/visible-hand.html"
               >http://nataliacecire.blogspot.com/2011/05/visible-hand.html</ref>.</bibl>
            <bibl xml:id="chronicling" label="Chronicling America no date"> <title rend="quotes">Chronicling America | Library of Congress,</title> Available at:
               <ref target="https://chroniclingamerica.loc.gov/about/"
               >https://chroniclingamerica.loc.gov/about/</ref> (Accessed 3 July 2020).</bibl>
         <bibl xml:id="cordell2017" label="Cordell 2017"> Cordell, R. <title rend="quotes">‘Q i-Jtb the Raven’: Taking Dirty OCR Seriously,</title> <title
               rend="italic">Book History</title> 20:1, pp. 188–225 (2017). Available at:
            <ref target="https://doi.org/10.1353/bh.2017.0006"
               >https://doi.org/10.1353/bh.2017.0006</ref>.</bibl>
         <bibl xml:id="cordell2020" label="Cordell 2020"> Cordell, R. <title rend="quotes">Machine Learning + Libraries: A Report on the State of the
            Field</title> (2020). Available at: <ref
               target="https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig"
               >https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig</ref>.</bibl>
         <bibl xml:id="cordellsmith2017" label="Cordell and Smith 2017"> Cordell, R., and Smith, D. <title rend="italic">Viral Texts:
               Mapping Networks of Reprinting in 19th-Century Newspapers and Magazines</title> (2017),
            Available at: <ref target="http://viraltexts.org/">http://viraltexts.org</ref>.</bibl>
         <bibl xml:id="crawford2019" label="Crawford and Paglen 2019"> Crawford, K., and Paglen, T. <title rend="quotes">Excavating AI: The Politics of
            Training Sets for Machine Learning</title> (2019). Available at: <ref
               target="https://excavating.ai">https://excavating.ai</ref> (Accessed: 19 September
            2019).</bibl>
            <bibl xml:id="deng2009" label="Deng et al. 2009"> Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L.
           <title rend="quotes">ImageNet: A Large-Scale Hierarchical Image Database,</title>in <title rend="italic">2009 IEEE
               Conference on Computer Vision and Pattern Recognition</title> (2009), pp. 248–55,<ref
               target="https://doi.org/10.1109/CVPR.2009.5206848"> Available at: </ref>
            <ref target="https://doi.org/10.1109/CVPR.2009.5206848"
               >https://doi.org/10.1109/CVPR.2009.5206848</ref>.</bibl>
         <bibl xml:id="fagan2016" label="Fagan 2016"> Fagan, B. <title rend="quotes">Chronicling White America.</title>American Periodicals: A Journal of
            History &amp; Criticism 26:1, pp. 10-13 (2016). Available at: <ref
               target="https://muse.jhu.edu/article/613375"
               >https://www.muse.jhu.edu/article/613375</ref>.</bibl>
         <bibl xml:id="farrar1998" label="Farrar 1998"> Farrar, H. <title rend="italic">The Baltimore Afro-American, 1892-1950</title>.
            Greenwood Publishing Group (1998). </bibl>
         <bibl xml:id="ferriter2017" label="Ferriter 2017"> Ferriter, M. <title rend="quotes">Introducing Beyond Words | The Signal,</title> (2017). Available
            at: <ref
               target="https://doi.org/blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/"
               >//blogs.loc.gov/thesignal/2017/09/introducing-beyond-words/</ref>. (Accessed: 13
            July 2020).</bibl>
         <bibl xml:id="foo2019" label="Foo 2019"> Foo, B. <title rend="quotes">AMNH Photographic Collection,</title> (2020). Available at: <ref
               target="https://amnh-sciviz.github.io/image-collection/about.html"
               >https://amnh-sciviz.github.io/image-collection/about.html</ref> (Accessed: 11 June
            2020).</bibl>
            <bibl xml:id="franklin1910" label="Franklin’s Paper the Statesman 1910"> Franklin's paper the statesman. (Denver, Colo.),
            15 Oct. 1910. <title rend="italic">Chronicling America: Historic American Newspapers</title>. Library of Congress.
            Available at: <ref
               target="https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/"
               >https://chroniclingamerica.loc.gov/lccn/sn91052311/1910-10-15/ed-1/seq-16/</ref>
         </bibl>
         <bibl xml:id="fyfe2016" label="Fyfe 2016"> Fyfe, P. <title rend="quotes">An Archaeology of Victorian Newspapers,</title> <title rend="italic">Victorian Periodicals
            Review</title> 49:4, pp. 546–77 (2016). Available at: <ref
               target="https://doi.org/10.1353/vpr.2016.0039"
               >https://doi.org/10.1353/vpr.2016.0039</ref>.</bibl>
            <bibl xml:id="gebru2020" label="Gebru et al. 2020"> Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H., Daumé III, H., and Crawford, K. <title rend="quotes">Datasheets for Datasets.</title><title rend="italic"
               >ArXiv:1803.09010 [Cs]</title>, March 19, 2020.
            <ref target="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</ref>
            (Accessed: July 29 2021).</bibl>
         <bibl xml:id="geraldo2020" label="Geraldo 2020"> Giraldo, M. <title rend="quotes">Building Aereo,</title> DX Lab | State Library of NSW (2020).
            Available at: <ref target="https://dxlab.sl.nsw.gov.au/blog/building-aereo"
               >https://dxlab.sl.nsw.gov.au/blog/building-aereo</ref> (Accessed: 2 July 2020).</bibl>
            <bibl xml:id="googlearts2018" label="Google Arts and Culture 2018"> <title rend="quotes">Google Arts &amp; Culture Experiments - t-SNE Map
            Experiment</title> (2018). Available at:
            <ref target="https://artsexperiments.withgoogle.com/tsnemap/"
               >https://artsexperiments.withgoogle.com/tsnemap/</ref> (Accessed: 11 June 2020).</bibl>
            <bibl xml:id="hardy2019" label="Hardy and DiCuirci 2019"> Hardy, M., and DiCuirci, L. <title rend="quotes">Critical Cataloging and the
            Serials Archive: The Digital Making of ‘Mill Girls in Nineteenth-Century Print,’</title>
            Archive Journal, Available at: <ref target="http://www.archivejournal.net/?p=8073"
               >http://www.archivejournal.net/?p=8073</ref>.</bibl>
            <bibl xml:id="haro2018" label="Haro et al. 2018"> Hara, K. et al.<title rend="quotes">A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk,</title>in
               <title rend="italic">Proceedings of the 2018 CHI Conference on Human Factors in
               Computing Systems</title>, CHI ’18 (Montreal QC, Canada: Association for Computing
            Machinery, 2018), pp. 1–14. Available at: <ref
               target="https://doi.org/10.1145/3173574.3174023"
               >https://doi.org/10.1145/3173574.3174023</ref>.</bibl>
            <bibl xml:id="he2016" label="He et al. 2016"> He, K., Zhang, X., Ren, S., and Sun, J.<title rend="quotes">Deep Residual Learning for
            Image Recognition,</title> in <title rend="italic">2016 IEEE Conference on Computer Vision and
               Pattern Recognition (CVPR)</title>, 2016, pp. 770–78,<ref
               target="https://doi.org/10.1109/CVPR.2016.90"> Available at: </ref>
            <ref target="https://doi.org/10.1109/CVPR.2016.90"
               >https://doi.org/10.1109/CVPR.2016.90</ref>.</bibl>
            <bibl xml:id="holland2018" label="Holland et al. 2018"> Holland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski,
            K.<title rend="quotes">The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards.</title>
               <title rend="italic">ArXiv:1805.03677 [Cs]</title>, May 9, 2018.
            <ref target="http://arxiv.org/abs/1805.03677">http://arxiv.org/abs/1805.03677</ref>
            (Accessed 29 July 2021).</bibl>
         <bibl xml:id="hitchcock2013" label="Hitchcock 2013"> Hitchcock, T. <title rend="quotes">Confronting the Digital,</title> <title rend="italic">Cultural and
               Social History</title> 10:1. pp. 9–23 (2013). Available at:
            <ref target="https://doi.org/10.2752/147800413X13515292098070"
               >https://doi.org/10.2752/147800413X13515292098070</ref>.</bibl>
         <bibl xml:id="imagenet2020a" label="ImageNet 2020"> <title rend="quotes">ImageNet, </title>Available at:
            <ref target="http://image-net.org/index">http://image-net.org/index</ref> (Accessed: 8
            June 2020).</bibl>
            <bibl xml:id="imagenet2020b" label="ImageNet: What about the Images? 2020"> <title rend="quotes">What about the images?</title>Available at: <ref
               target="http://image-net.org/download-faq">http://image-net.org/download-faq</ref>
            (Accessed: 8 June 2020).</bibl>
            <bibl xml:id="intel2020a" label="Intel 2020a"> <title rend="quotes">Intel® Xeon® Platinum 9242 Processor (71.5M Cache, 2.30 GHz) Product
            Specifications,</title> Available at: <ref
               target="https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html"
               >https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html</ref>
            (Accessed: 5 June 2020).</bibl>
            <bibl xml:id="intel2020b" label="Intel 2020b"> <title rend="quotes">Intel® Xeon® Platinum 8256 Processor (16.5M Cache, 3.80 GHz) Product
            Specifications,</title> Available at: <ref
               target="https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html"
               >https://ark.intel.com/content/www/us/en/ark/products/192467/intel-xeon-platinum-8256-processor-16-5m-cache-3-80-ghz.html</ref>
            (Accessed: June 5, 2020).</bibl>
         <bibl xml:id="iowa1910" label="Iowa State Bystander 1910"> Iowa state bystander. [volume] (Des Moines, Iowa), 14 Oct.
            1910. <title rend="italic">Chronicling America: Historic American Newspapers</title>. Library of Congress. Available
            at: <ref
               target="https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/"
               >https://chroniclingamerica.loc.gov/lccn/sn83025186/1910-10-14/ed-1/seq-1/</ref>
         </bibl>
         <bibl xml:id="lacoste2019" label="Lacoste et al. 2019"> Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T.
           <title rend="quotes">Quantifying the Carbon Emissions of Machine Learning,</title> <title rend="italic"
               >ArXiv:1910.09700 [Cs]</title> (2019). Available at: <ref
               target="http://arxiv.org/abs/1910.09700">http://arxiv.org/abs/1910.09700</ref>. </bibl>
         <bibl xml:id="lclabs2017a" label="LC Labs 2017a"> LC Labs, <title rend="quotes">Beyond Words: Mark</title>Available at: <ref
               target="http://beyondwords.labs.loc.gov/#/mark"
               >http://beyondwords.labs.loc.gov/#/mark</ref> (Accessed 5 June, 2020).</bibl>
         <bibl xml:id="lclabs2017b" label="LC Labs 2017b"> LC Labs, <title rend="quotes">Beyond Words: Transcribe,</title> Available at: <ref
               target="http://beyondwords.labs.loc.gov/#/transcribe"
               >http://beyondwords.labs.loc.gov/#/transcribe</ref> (Accessed 5 June, 2020).</bibl>
         <bibl xml:id="lclabs2017c" label="LC Labs 2017c"> LC Labs, <title rend="quotes">Beyond Words: Veriffy,</title> Available at: <ref
               target="http://beyondwords.labs.loc.gov/#/verify"
               >http://beyondwords.labs.loc.gov/#/verify</ref> (Accessed 5 June, 2020).</bibl>
            <bibl xml:id="lclabs" label="LC Labs no date"> LC Labs, Beyond Words | Experiments. Available at: <ref
               target="https://labs.loc.gov/work/experiments/beyond-words/"
               >https://labs.loc.gov/work/experiments/beyond-words/</ref> (Accessed 5 June,
            2020).</bibl>
            <bibl xml:id="lclabs2020" label="LC Labs and Digital Strategy Directorate 2020"> LC Labs and Digital Strategy
            Directorate, <title rend="quotes">Machine Learning + Libraries Summit Event Summary</title>(2020). Available at:
               <ref
               target="https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf"
               >https://labs.loc.gov/static/labs/meta/ML-Event-Summary-Final-2020-02-13.pdf</ref>. </bibl>
         <bibl xml:id="lee2019" label="Lee 2019"> Lee, B. <title rend="quotes">Machine Learning, Template Matching, and the International Tracing
            Service Digital Archive: Automating the Retrieval of Death Certificate Reference Cards
            from 40 Million Document Scans,</title> <title rend="italic">Digital Scholarship in the
               Humanities</title> 34:3, pp. 513-535 (2019). <ref
               target="https://doi.org/10.1093/llc/fqy063">Available at: </ref>
            <ref target="https://doi.org/10.1093/llc/fqy063"
               >https://doi.org/10.1093/llc/fqy063</ref>.</bibl>
         <bibl xml:id="lee2020a" label="Lee 2020"> Lee, B. <title rend="italic">LibraryOfCongress/Newspaper-Navigator</title>, GitHub
            Repository ( Library of Congress, 2020).<ref
               target="https://github.com/LibraryOfCongress/newspaper-navigator"> Available at: </ref>
            <ref target="https://github.com/LibraryOfCongress/newspaper-navigator"
               >https://github.com/LibraryOfCongress/newspaper-navigator</ref>.</bibl>
         <bibl xml:id="lee2020b" label="Lee et al. 2020"> Lee, B., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage,
            N., Thomas, D., Zwaard, K., and Weld, D. <title rend="quotes">The Newspaper Navigator Dataset: Extracting
            And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling
            America,</title> <ref target="https://dl.acm.org/doi/proceedings/10.1145/3340531">CIKM '20:
               Proceedings of the 29th ACM International Conference on Information &amp; Knowledge
               Management</ref>
            <title rend="italic">, </title>pp. 3055–3062 (2020). Available at:
            <ref target="https://doi.org/10.1145/3340531.3412767"
               >https://doi.org/10.1145/3340531.3412767</ref>.</bibl>
         <bibl xml:id="lee2020c" label="Lee and Weld 2020"> Lee, B., and Weld, D. <title rend="quotes">Newspaper Navigator: Open Faceted Search for
            1.5 Million Images,</title> <ref target="https://dl.acm.org/doi/proceedings/10.1145/3379350"
               >UIST '20 Adjunct: Adjunct Publication of the 33rd Annual ACM Symposium on User
               Interface Software and Technology</ref>, pp. 120-122 (2020). Available at: <ref
               target="https://doi.org/10.1145/3379350.3416143"
               >https://doi.org/10.1145/3379350.3416143</ref>.</bibl>
            <bibl xml:id="lee2021a" label="Lee, Berson, and Berson 2021"> Lee, B., Berson, I., and Berson, M. <title rend="quotes">Machine Learning and
            the Social Studies,</title> <title rend="italic">Social Education</title> 85:2, pp. 88-92 (2021).
            Available at: <ref
               target="https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies"
               >https://www.socialstudies.org/social-education/85/2/machine-learning-and-social-studies</ref>.</bibl>
         <bibl xml:id="lee2021b" label="Lee et al. 2021"> Lee, B., Mears, J., Jakeway, E., Ferriter, M., and Potter, A.
           <title rend="quotes">Newspaper Navigator: Putting Machine Learning in the Hands of Library Users,</title><title
               rend="italic">EuropeanaTech Insight</title> 16 (2021). Available at: <ref
               target="https://pro.europeana.eu/page/issue-16-newspapers"
               >https://pro.europeana.eu/page/issue-16-newspapers</ref>.</bibl>
         <bibl xml:id="congress2019" label="Library of Congress 2019"> <title rend="quotes">Digital Strategy | Library of Congress,</title> Library of Congress
            (2019). Available at: <ref target="https://www.loc.gov/digital-strategy/"
               >https://www.loc.gov/digital-strategy/</ref> (Accessed: 30 May 2020).</bibl>
         <bibl xml:id="lincoln2019" label="Lincoln et al. 2019"> Lincoln, M., Levin, G., Conell, S., and Huang, L. (2019) <title rend="quotes">National
            Neighbors: Distant Viewing the National Gallery of Art's Collection of Collections</title>
            (2019) Available at: <ref target="https://nga-neighbors.library.cmu.edu/"
               >https://nga-neighbors.library.cmu.edu</ref>. (Accessed: 30 May 2020).</bibl>
            <bibl xml:id="lonij2017" label="Lonij and Weavers 2017"> Lonij, J., and Wevers, M. (2017) SIAMESE. KB Lab: The Hague
            (2017). Available at: <ref target="http://lab.kb.nl/tool/siamese"
               >http://lab.kb.nl/tool/siamese</ref>.</bibl>
            <bibl xml:id="lorang2020" label="Lorang et al 2020"> Lorang, E., Soh, L., Liu, Y., and Pack, C. <title rend="quotes">Digital Libraries,
            Intelligent Data Analytics, and Augmented Description: A Demonstration Project</title> (2020).
            Available at: <ref target="https://digitalcommons.unl.edu/libraryscience/396/"
               >https://digitalcommons.unl.edu/libraryscience/396/</ref>.</bibl>
         <bibl xml:id="mak2017" label="Mak 2017"> Mak, B. <title rend="quotes">Archaeology of a Digitization,</title> <title rend="italic">Journal of the
               Association for Information Science and Technology</title> 65:8, pp. 1515–26 (2014).
            Available at: <ref target="https://doi.org/10.1002/asi.23061"
               >https://doi.org/10.1002/asi.23061</ref>.</bibl>
         <bibl xml:id="manovich2012" label="Manovich 2012"> Manovich, L. <title rend="quotes">How to Compare One Million Images?,</title> in <title rend="italic"
               >Understanding Digital Humanities</title>, ed. David M. Berry (London: Palgrave
            Macmillan UK, 2012), pp. 249–78. Available at:
            <ref target="https://doi.org/10.1057/9780230371934_14"
               >https://doi.org/10.1057/9780230371934_14</ref>.</bibl>
         <bibl xml:id="maxwell2017" label="Maxwell 2017"> Maxwell, M. <title rend="quotes">WVU Today | WVRHC Seeking Copies of Rare African-American
            Newspapers</title> (2017). Available at: <ref
               target="https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers"
               >https://wvutoday.wvu.edu/stories/2017/01/19/wvrhc-seeking-copies-of-rare-african-american-newspapers</ref>.
            (Accessed 11 July 2020).</bibl>
         <bibl xml:id="mears2014" label="Mears 2014"> Mears, J. <title rend="italic">National Digital Newspaper Program Impact Study
               2004-2014</title>, National Endowment for the Humanities (2014). Available at: <ref
               target="https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study"
               >https://www.neh.gov/divisions/preservation/featured-project/neh-releases-national-digital-newspaper-program-impact-study</ref>.
            (Accessed 29 May 2020).</bibl>
            <bibl xml:id="meta" label="Meta | Morphosis no date"> <title rend="quotes">Meta | Morphosis: Tutorials,</title> National Digital Newspaper
            Program and the University of Kentucky Libraries. Available at: <ref
               target="https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html"
               >https://www.uky.edu/Libraries/NDNP/metamorphosis/tutorials.html</ref> (Accessed 3
            July 2020).</bibl>
         <bibl xml:id="milligan2013" label="Milligan 2013"> Milligan, I. <title rend="quotes">Illusionary Order: Online Databases, Optical Character
            Recognition, and Canadian History, 1997–2010,</title> <title rend="italic">Canadian Historical
               Review</title> 94:4, pp. 540–69 (2013). Available at: <ref
               target="https://doi.org/10.3138/chr.694">https://doi.org/10.3138/chr.694</ref>.</bibl>
         <bibl xml:id="mitchell2019" label="Mitchell et al. 2019"> Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L.,
            Hutchinson, B., Spitzer, E., Raji, I., and Gebru, T. <title rend="quotes">Model Cards for Model Reporting.</title>
               <title rend="italic">Proceedings of the Conference on Fairness, Accountability, and
               Transparency</title>, January 29, 2019, 220–29.
            <ref target="https://doi.org/10.1145/3287560.3287596"
               >https://doi.org/10.1145/3287560.3287596</ref>.</bibl>
            <bibl xml:id="nasjonalmuseet2017" label="Nasjonalmuseet 2017"> <title rend="quotes">Project: «Principal Components»,</title> Nasjonalmuseet (2018).
            Available at: <ref
               target="https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management---behind-the-scenes/digital-collection-management/project-principal-components/"
               >https://www.nasjonalmuseet.no/en/about-the-national-museum/collection-management —
               -behind-the-scenes/digital-collection-management/project-principal-components/</ref>
            (Accessed 11 June 2020).</bibl>
            <bibl xml:id="national2019" label="National Digital Newspaper Program 2019"> <title rend="quotes">About the Program - National Digital
            Newspaper Program (Library of Congress),</title> (2019). Available at: <ref
               target="https://www.loc.gov/ndnp/about.html"
               >https://www.loc.gov/ndnp/about.html</ref> (Accessed 3 July
            2020).
         </bibl>
            <bibl xml:id="national2020" label="National Digital Newspaper Program 2020"> The National Digital Newspaper Program (NDNP)
            Technical Guidelines for Applicants 2020-22 Awards (2020). Available at: <ref
               target="https://www.loc.gov/ndnp/guidelines/"
               >https://www.loc.gov/ndnp/guidelines/</ref> (Accessed 28 June 2020).</bibl>
            <bibl xml:id="national" label="National Digital Newspaper Program no date"> <title rend="quotes">Content Selection - National Digital
            Newspaper Program (Library of Congress)</title>(2020). Available at: <ref
               target="https://www.loc.gov/ndnp/guidelines/selection.html"
               >https://www.loc.gov/ndnp/guidelines/selection.html</ref> (Accessed 3 July
            2020).
         </bibl>
            <bibl xml:id="neh2020" label="NEH Division of Preservation and Access 2020"> Division of Preservation and Access
            (NEH), <title rend="quotes">Notice of Funding Opportunity, National Digital Newspaper Program</title> (2020).
            Available at: <ref
               target="https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf"
               >https://www.neh.gov/sites/default/files/inline-files/National-Digital-Newspaper-Program-NOFO-January-2020.pdf</ref>.
            (Accessed 28 June 2020).</bibl>
            <bibl xml:id="navigator1910a" label="Newspaper Navigator 1910a"> Image of W.E.B. Du Bois from the <title rend="italic">Iowa
               State Bystander</title> (14 October 1910). From the Library of Congress, Newspaper
            Navigator dataset: Extracted Visual Content from Chronicling America. Available at: 
            <ref
               target="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg"
               >https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015/001_0_95.jpg</ref>.</bibl>
            <bibl xml:id="navigator1910b" label="Newspaper Navigator 1910b">[Newspaper Navigator 1910b] <title rend="italic">Newspaper Navigator </title> metadata for the
               <title rend="italic">Iowa State Bystander</title> (14 October 1910). From the Library of
            Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling
            America. Available at: 
            <ref
               target="https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json"
               >https://news-navigator.labs.loc.gov/data/iahi_ames_ver01/data/sn83025186/00202198417/1910101401/1015.json</ref>.</bibl>
            <bibl xml:id="navigator1910c" label="Newspaper Navigator 1910c"> Image of W.E.B. Du Bois from<title rend="italic"> Franklin’s
               Paper the Statesman</title> (15 October 1910). From the Library of Congress, Newspaper
            Navigator dataset: Extracted Visual Content from Chronicling America. Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg"
               >https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272/001_0_93.jpg</ref>
         </bibl>
            <bibl xml:id="navigator1910d" label="Newspaper Navigator 1910d"> <title rend="italic">Newspaper Navigator </title> metadata for <title
               rend="italic">Franklin’s Paper the Statesman</title> (15 October 1910). From the Library
            of Congress, Newspaper Navigator dataset: Extracted Visual Content from Chronicling
            America. Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json"
               >https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272</ref>
            <ref
               target="https://news-navigator.labs.loc.gov/data/cohi_abbeyville_ver01/data/sn91052311/00279550730/1910101501/2272.json"
               >.json</ref>
         </bibl>
            <bibl xml:id="navigator1910e" label="Newspaper Navigator 1910e"> Image of W.E.B. Du Bois from <title rend="italic">The Broad Ax
            </title> (15 October 1910). From the Library of Congress, Newspaper Navigator dataset:
            Extracted Visual Content from Chronicling America. Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg"
               >https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538/002_0_98.jpg</ref>
         </bibl>
            <bibl xml:id="navigator1910f" label="Newspaper Navigator 1910f"> <title rend="italic">Newspaper Navigator </title>metadata for <title
               rend="italic">The Broad Ax </title>(15 October 1910). From the Library of Congress,
            Newspaper Navigator dataset: Extracted Visual Content from Chronicling America.
            Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json"
               >https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910101501/0538.json</ref>
         </bibl>
            <bibl xml:id="navigator1910g" label="Newspaper Navigator 1910g"> Image of W.E.B. Du Bois from The Broad Ax (26 November
            1910). From the Library of Congress, Newspaper Navigator dataset: Extracted Visual
            Content from Chronicling America. Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg"
               >https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564/004_0_98.jpg</ref>
         </bibl>
            <bibl xml:id="navigator1910h" label="Newspaper Navigator 1910h"> <title rend="italic">Newspaper Navigator </title>metadata for The
            Broad Ax (26 November 1910). From the Library of Congress, Newspaper Navigator dataset:
            Extracted Visual Content from Chronicling America. Available at: <ref
               target="https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json"
               >https://news-navigator.labs.loc.gov/data/iune_charlie_ver01/data/sn84024055/00280761059/1910112601/0564.json</ref>
         </bibl>
         <bibl xml:id="noble2018" label="Noble 2018"> Noble, S. <title rend="italic">Algorithms of Oppression: How Search Engines
               Reinforce Racism</title>. NYU Press, New York (2018).</bibl>
            <bibl xml:id="oceanic2017" label="Oceanic Exchanges Project 2017"> Oceanic Exchanges Project Team. Oceanic Exchanges:
            Tracing Global Information Networks In Historical Newspaper Repositories, 1840-1914
            (2017). Available at: 10.17605/OSF.IO/WA94S.</bibl>
         <bibl xml:id="owens2018" label="Owens 2018"> Owens, T. <title rend="italic">The Theory and Craft of Digital Preservation</title>. Johns Hopkins
            University Press, Baltimore (2018).</bibl>
            <bibl xml:id="owens2020" label="Owens and Padilla 2020"> Owens, T., and Padilla, T. <title rend="quotes">Digital Sources and Digital
            Archives: Historical Evidence in the Digital Age,</title> <title rend="italic">International
               Journal of Digital Humanities </title>(2020). Available at:<ref target="https://doi.org/10.1007/s42803-020-00028-7">https://doi.org/10.1007/s42803-020-00028-7</ref>
            <ref target="https://doi.org/10.1007/s42803-020-00028-7"
               >https://doi.org/10.1007/s42803-020-00028-7</ref>. </bibl>
         <bibl xml:id="padilla2019" label="Padilla 2019"> Padilla, T. <title rend="italic">Responsible Operations: Data Science,
               Machine Learning, and AI in Libraries</title> (2019). Available at: <ref
               target="https://doi.org/10.25333/xk7z-9g97"
            >https://doi.org/10.25333/xk7z-9g97</ref>.</bibl>
         <bibl xml:id="reidsma2019" label="Reidsma 2019"> Reidsma, M. <title rend="italic">Masked by Trust: Bias in Library
               Discovery.</title> Litwin Books, Sacramento (2019).</bibl>
         <bibl xml:id="reisman2018" label="Reisman et al. 2018"> Reisman, D., Schultz, J., Crawford, K., Whittaker, M. <title
               rend="italic">Algorithmic Impact Assessments: A Practical Framework for Public Agency
               Accountability</title> (2018). Available at: <ref
               target="https://ainowinstitute.org/aiareport2018.pdf"
               >https://ainowinstitute.org/aiareport2018.pdf</ref>.</bibl>
            <bibl xml:id="russakovsky2015" label="Russakovsky et al. 2015"> Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
            Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei,
            L. <title rend="quotes">ImageNet Large Scale Visual Recognition Challenge,</title><title rend="italic">International
               Journal of Computer Vision</title> 115:3, pp. 211-252 (2015). Available at:<ref target="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</ref>
            <ref target="https://doi.org/10.1007/s11263-015-0816-y"
               >https://doi.org/10.1007/s11263-015-0816-y</ref>. </bibl>
            <bibl xml:id="schwartz2019" label="Schwartz et al. 2019"> Schwartz, R., Dodge, J., Smith, N., and Etzioni, O. <title rend="quotes">Green AI,</title>
               ArXiv:1907.10597 [Cs, Stat], (2019). Available at:<ref target="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</ref>
            <ref target="http://arxiv.org/abs/1907.10597">http://arxiv.org/abs/1907.10597</ref>.</bibl>
         <bibl xml:id="strange2014" label="Strange et al. 2014"> Strange, C., McNamara, D., Wodak, J., and Wood, I. <title rend="quotes">Mining for the
            Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical
            Newspapers,</title> <title rend="italic">Digital Humanities Quarterly</title> 8:1 (2014). Available
            at: <ref target="http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html"
               >http://www.digitalhumanities.org/dhq/vol/8/1/000168/000168.html</ref>.</bibl>
         <bibl xml:id="strubell2019" label="Strubell et al. 2019"> Strubell, E., Ganesh, A., and McCallum, A. <title rend="quotes">Energy and Policy
            Considerations for Deep Learning in NLP,</title> ArXiv:1906.02243 [Cs] (2019). Available at:
               <ref target="http://arxiv.org/abs/1906.02243"
            >http://arxiv.org/abs/1906.02243</ref>.</bibl>
            <bibl xml:id="ax1910a" label="The Broad Ax 1910a"> The broad ax. [volume] (Salt Lake City, Utah), 15 Oct. 1910.
            <title rend="italic">Chronicling America: Historic American Newspapers</title>. Library of Congress. Available at:
               <ref
               target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/"
               >https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-10-15/ed-1/seq-2/</ref>
         </bibl>
            <bibl xml:id="ax1910b" label="The Broad Ax 1910b"> The broad ax. [volume] (Salt Lake City, Utah), 26 Nov. 1910.
            <title rend="italic">Chronicling America: Historic American Newspapers</title>. Library of Congress. Available at:
               <ref
               target="https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/"
               >https://chroniclingamerica.loc.gov/lccn/sn84024055/1910-11-26/ed-1/seq-3/</ref>
         </bibl>
            <bibl xml:id="traub2015" label="Traub, van Ossenbruggen, and Hardman 2015"> Traub, M., van Ossenbruggen, J., and
            Hardman, L. <title rend="quotes">Impact Analysis of OCR Quality on Research Tasks in Digital Archives,</title> in
               <title rend="italic">Research and Advanced Technology for Digital Libraries</title>, ed.
            Sarantos Kapidakis, Cezary Mazurek, and Marcin Werla (Cham: Springer International
            Publishing, 2015), 252–263.</bibl>
            <bibl xml:id="vandermaaten2009" label="Van der Maaten and Hinton 2009"> van der Maaten, L., and Hinton, G. <title rend="quotes">Visualizing Data
            Using T-SNE,</title> <title rend="italic">Journal of Machine Learning Research</title> 9, pp.
            2579-2605 (2008). Available at: <ref
               target="http://www.jmlr.org/papers/v9/vandermaaten08a.html"
               >http://www.jmlr.org/papers/v9/vandermaaten08a.html</ref>.</bibl>
         <bibl xml:id="vane2018" label="Vane 2018"> Vane, O. <title rend="quotes">Visualising the Royal Photographic Society Collection: Part 2 •
            V&amp;A Blog,</title> <title rend="italic">V&amp;A Blog</title> (2018). Available at: <ref
               target="https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2"
               >https://www.vam.ac.uk/blog/digital/visualising-the-royal-photographic-society-collection-part-2</ref>.</bibl>
            <bibl xml:id="wattenberg2016" label="Wattenberg, Viégas, and Johnson 2016"> Wattenberg, M., Viégas, F., and Johnson, I. <title rend="quotes">How
            to Use T-SNE Effectively,</title> <title rend="italic">Distill</title> 1:10 (2016). Available at:
            <ref target="https://doi.org/10.23915/distill.00002"
               >https://doi.org/10.23915/distill.00002</ref>.</bibl>
            <bibl xml:id="weavers2020" label="Weavers and Smits 2020"> Wevers, M., and Smits, T. <title rend="quotes">The Visual Digital Turn: Using
            Neural Networks to Study Historical Images,</title> <title rend="italic">Digital Scholarship in
               the Humanities</title> 35:1, pp. 194-207 (2020). Available at:
            <ref target="https://doi.org/10.1093/llc/fqy085"
               >https://doi.org/10.1093/llc/fqy085</ref>.</bibl>
            <bibl xml:id="weld2019" label="Weld and Bansal 2019"> Weld, D., and Bansal, G. 2019. The challenge of crafting
            intelligible intelligence. Commun. ACM 62: 6, pp. 70–79 (2019). Available at: <ref
               target="https://doi.org/10.1145/3282486">https://doi.org/10.1145/3282486</ref>.</bibl>
         <bibl xml:id="williams2019" label="Williams 2019"> Williams, L. <title rend="quotes">What Computational Archival Science Can Learn from Art
            History and Material Culture Studies,</title> in <title rend="italic">2019 IEEE International
               Conference on Big Data (Big Data)</title>, 2019, pp. 3153–55. Available at:
            <ref target="https://doi.org/10.1109/BigData47090.2019.9006527"
               >https://doi.org/10.1109/BigData47090.2019.9006527</ref>. </bibl>
         <bibl xml:id="wright2019" label="Wright 2019"> Wright, R. <title rend="quotes">Typewriting Mass Observation Online: Media Imprints on the
            Digital Archive,</title> <title rend="italic">History Workshop Journal</title> 87, pp. 118–38 (2019).
            Available at:
            <ref target="https://doi.org/10.1093/hwj/dbz005"
               >https://doi.org/10.1093/hwj/dbz005</ref>.</bibl>
            <bibl xml:id="yale2017" label="Yale Digital Humanities Lab 2017"> <title rend="quotes">Yale Digital Humanities Lab - PixPlot</title> (2020).
            Available at: <ref target="https://dhlab.yale.edu/projects/pixplot/"
               >https://dhlab.yale.edu/projects/pixplot/</ref> (Accessed 11 June 2020).</bibl>
        </listBibl>

      </back>
   </text>
</TEI>
