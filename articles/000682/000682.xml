<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English-->Automated
               Transcription of Gə'əz Manuscripts Using Deep Learning</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Samuel <dhq:family>Grieggs</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Notre Dame</dhq:affiliation>
               <email>sgrieggs@nd.edu</email>
               <dhq:bio>
                  <p>Samuel Grieggs is currently pursuing a PhD in Computer Science and Engineering at the University of Notre Dame. His research focuses on using computer vision and machine learning to create tools that benefit humanities researchers, as well as studying and improving how machine learning models handle novelty. Samuel earned his undergraduate degree in Computer Science from Indiana University of Pennsylvania in 2017. In the fall of 2023, he will return to IUP as an Assistant Professor of Computer Science.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Jessica <dhq:family>Lockhart</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Toronto</dhq:affiliation>
               <email>jessica.lockhart@utoronto.ca</email>
               <dhq:bio>
                  <p>Jessica Lockhart is Head of Research for the Old Books New Science lab at the University of Toronto, under the direction of Alexandra Gillespie. Her research facilitates collaborations concerning the humanistic and scientific study of premodern book technologies. Lockhart has authored or co-authored articles in <title rend="italic">Chaucer Review</title> (2015), <title rend="italic">Digital Humanities Quarterly</title> (2020), <title rend="italic">Digital Philology</title> (2022), and the volume <title rend="italic">Cultural Translations in Medieval Romance</title> (2022), and is co-editing with Michelle Brown a special issue of Ancient Narrative (2023), alongside other publications emerging from the lab’s research. Lockhart earned a PhD in Medieval Studies from the University of Toronto in 2017.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Alexandra <dhq:family>Atiya</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Toronto</dhq:affiliation>
               <email>alexandra.atiya@mail.utoronto.ca</email>
               <dhq:bio>
                  <p>Alexandra Atiya is a PhD candidate at the Centre for Medieval Studies at the University of Toronto. Her research and teaching focuses on late-medieval English and Iberian drama as well as contemporary literature and digital humanities. Atiya has also been a research assistant for <title rend="quotes">The Book and the Silk Roads</title> and <title rend="quotes">Hidden Stories: New Approaches to the Local and Global History of the Book</title> since 2021.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Gelila <dhq:family>Tilahun</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Toronto</dhq:affiliation>
               <email>gelila.tilahun@utoronto.ca</email>
               <dhq:bio>
                  <p>Gelila Tilahun is a research fellow at the DEEDS Centre and the Department of Historical and Cultural Studies, University of Toronto. She works in the area of statistical and computational text analysis methods. Her focus is on understanding changes in document production over time and how charter language changes in response to large-scale historical events. Previously, she worked in the bioinformatics research area applying text mining and language model techniques to identify regulatory elements in the non-coding regions of the DNA that are involved in gene expression.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Suzanne <dhq:family>Akbari</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Institute for Advanced Study, Princeton, NJ</dhq:affiliation>
               <email>sakbari@ias.edu</email>
               <dhq:bio>
                  <p>Suzanne Conklin Akbari is Professor of Medieval Studies at the Institute for Advanced Study in Princeton, New Jersey. Her books are on optics and allegory (<title rend="italic">Seeing Through the Veil</title>) and European views of Islam and the Orient (<title rend="italic">Idols in the East</title>), plus edited volumes on travel literature, Mediterranean Studies, and somatic histories. Her most recent publication is <title rend="italic">Practices of Commentary: Medieval Traditions and Transmissions</title> (<title rend="italic">The Medieval Globe</title> 8.2 [2022]). Akbari is co-PI on <title rend="quotes">The Book and the Silk Roads,</title> and <title rend="quotes">Hidden Stories: New Approaches to the Local and Global History of the Book.</title> She co-hosts a literature podcast called The Spouter-Inn.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Eyob <dhq:family>Derillo</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>SOAS, University of London</dhq:affiliation>
               <email>eyob.derillo@bl.uk</email>
               <dhq:bio>
                  <p>Eyob Derillo is currently working at the British Library as Reference Specialist in the department of Asia and Africa Studies, where he has served as curator for Ethiopic and Ethiopian collections. In 2018 he curated the British Library’s exhibition <title rend="italic">African Scribes: Manuscript Culture of Ethiopia</title> which was the first exhibition to be held at the Library devoted entirely to Ethiopian manuscripts. He also co-curated the British Library’s highly acclaimed exhibition <title rend="italic">Harry Potter: History of Magic</title>. Eyob is also completing his doctorate at SOAS (Department of Religions and Philosophies). His research focuses on the nature and historical development of the concept of Ethiopian ‘magic’ and its use within a specifically Christian context.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Jarod <dhq:family>Jacobs</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>Warner Pacific College</dhq:affiliation>
               <email>jarod.jacobs@gmail.com</email>
               <dhq:bio>
                  <p>Jarod Jacobs received his PhD from the University of Manchester in 2015. Jacobs’ academic work centres around statistics and language, with a specific focus on biblical texts. His book, entitled Statistics, Linguistics, and the <q>Biblical</q> Dead Sea Scrolls, was published in OUP’s Journal of Semitic Studies Supplement Series in 2018. He is currently working as a Manager of Data Engineering at Providence Health &amp; Services’ Analytics Center of Excellence, where he applies his experience with machine learning models and language processing to tell the story of (un)structured data.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Christine <dhq:family>Kwon</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Notre Dame</dhq:affiliation>
               <email>ckwon@nd.edu</email>
               <dhq:bio>
                  <p>Christine Kwon is currently pursuing PhD in Human Computer Interaction at Carnegie Mellon University. Her research focuses on learning sciences, specifically building educational technologies for underserved communities. At the time of this research, she was an undergraduate student at the University of Notre Dame where she earned her bachelor’s degree of science in mathematics with a concentration in computing. </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Michael <dhq:family>Gervers</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Toronto</dhq:affiliation>
               <email>m.gervers@utoronto</email>
               <dhq:bio>
                  <p>Michael Gervers is Professor of Medieval History at the University of Toronto, where he focuses on digital diplomatics and the international integration of databases. Working in collaboration with Dr. Gelila Tilahun, he has been a pioneer in the application of statistics to the analysis of medieval charters, including topic modeling and network analysis. He is currently investigating the diplomatic differences between Anglo-Saxon and Norman charters, while simultaneously testing methods to confirm the dating of the former. He is also working with colleagues in France on Handwritten Text Recognition technology (HTR), training the open source software <title rend="italic">eScriptorium</title> to read medieval Latin scripts. In 2017, he established the regular teaching of Old Ethiopic (Ge’ez) at the University of Toronto.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Steve <dhq:family>Delamarter</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>George Fox University</dhq:affiliation>
               <email>sdelamar@georgefox.edu</email>
               <dhq:bio>
                  <p>Steve Delamarter is Emeritus Research Professor in Residence at George Fox University, and serving as co-director of the Textual History of the Ethiopic Old Testament (THEOT) Project. Founded in 2012, this project employs a method and workflow that fully integrates statistical and philological analyses to tell the story of the transmission history of the Ethiopic Old Testament. In 2005, Delamarter founded the Ethiopic Manuscript Imaging Project (EMIP) and has located and digitised just over 12,000 manuscripts since then. He has worked since then to create metadata and make the images and metadata available in Beta maṣāḥǝft, a digital manuscript studies environment operated by the University of Hamburg’s Hiob Ludolf Centre for Eritrean and Ethiopian Manuscript Studies.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Alexandra <dhq:family>Gillespie</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Toronto</dhq:affiliation>
               <email>alexandra.gillespie@utoronto.ca</email>
               <dhq:bio>
                  <p>​​Alexandra Gillespie is Vice-President of the University of Toronto and Principal of the University of Toronto Mississauga, where she has worked as Professor of English and Medieval Studies for the past twenty years. Her research ranges widely: from the poetics of Chaucer’s <title rend="italic">Canterbury Tales</title> to the history of text technologies, from scientific approaches to book history to literary theory and philosophy. On these topics she <ref target="https://scholar.google.ca/citations?user=FSxJY0MAAAAJ&amp;hl=en">has published</ref> more than fifty articles and six co-edited volumes, including most recently <title rend="italic">The Unfinished Book</title> (Oxford, 2021) with Deidre Lynch. Her first monograph, <title rend="italic">Print Culture and the Medieval Author</title> (Oxford, 2006) remains one of the most cited in the field; her current project extends this work in new directions in a study of <title rend="italic">Chaucer’s Books</title>.
                  </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Walter <dhq:family>Scheirer</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"
                  ><!--if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000--></idno>
               <dhq:affiliation>University of Notre Dame</dhq:affiliation>
               <email>walter.scheirer@nd.edu</email>
               <dhq:bio>
                  <p>Walter J. Scheirer received the M.S. degree in computer science from Lehigh University, in 2006, and the Ph.D. degree in engineering from the University of Colorado, Colorado Springs, CO, USA, in 2009. He is the Dennis O. Doughty Collegiate Associate Professor with the Department of Computer Science and Engineering, University of Notre Dame. Prior to joining the University of Notre Dame, he was a Postdoctoral Fellow with Harvard University from 2012 to 2015, and the Director of Research and Development with Securics, Inc., from 2007 to 2012.  He serves as the Chair of the IEEE Computer Society Technical Community on Pattern Analysis and Machine Intelligence and serves on the board of the Computer Vision Foundation. His research interests include artificial intelligence, computer vision, machine learning, and digital humanities.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000682</idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date/>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/projects.xml"
                     >http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref
               target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000682/000682.xml"
               >GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>This paper describes a collaborative project designed to meet the needs of
               communities interested in Gə'əz language texts – and other under-resourced manuscript
               traditions – by developing an easy-to-use open-source tool that converts images of
               manuscript pages into a transcription using optical character recognition (OCR). Our
               computational tool incorporates a custom data curation process to address the
               language-specific facets of Gə'əz coupled with a Convolutional Recurrent Neural
               Network to perform the transcription. An open-source OCR transcription tool for
               digitized Gə'əz manuscripts can be used by students and scholars of Ethiopian
               manuscripts to create a substantial and computer-searchable corpus of transcribed and
               digitized Gə'əz texts, opening access to vital resources for sustaining the history
               and living culture of Ethiopia and its people. With suitable ground-truth, our
               open-source OCR transcription tool can also be retrained to read other
               under-resourced scripts. The tool we developed can be run without a graphics
               processing unit (GPU), meaning that it requires much less computing power than most
               other modern AI systems. It can be run offline from a personal computer, or accessed
               via a web client and potentially in the web browser of a smartphone. The paper
               describes our team’s collaborative development of this first open-source tool for
               Gə'əz manuscript transcription that is both highly accurate and accessible to
               communities interested in Gə'əz books and the texts they contain.</p>
         </dhq:abstract>
         <dhq:abstract xml:land="am">
            <p> ጥልቅ እውቀትን ለረቂቅ ጽሁፎች ስለመጠቀም</p>


            <p>ሳሙኤል ግሪግስ፡ ኖተርዳም ዩኒቨርሲቲ፤ ጀሲካ ሎክሀርት፡ቶሮንቶ ዩኒቨርሲቲ፤ አሌክሳንደራ አትያ፡ ቶሮንቶ ዩኒቨርሲቲ፤ ገሊላ ጥላሁን፡
               ቶሮንቶ ዩኒቨርሲቲ፤ ሱዛን ኮንክሊን አክባሪ፡ አድቫንስድ ጥናት ኢንስቲትዩት፡ ፕሪንስተን ኒው ጀርሲ፤ ኢዮብ ደሪሎ ሶ.አ.ስ. ለንደን
               ዩኒቨርሲቲ፤ ጃሮድ ጃኮብስ፡ ዋርነር ፓሲፊክ ኮሌጅ፤ ክሪስቲን ኮን፡ ኖተርዳም ዩኒቨርሲቲ፤ ሚካኤል ጀርቨርስ፡ ቶሮንቶ ዩኒቨርሲቲ፤ ስቲቭ
               ደላማርተር፡ ጆርጅ ፎክስ ዩኒቨርሲቲ፤ አሌክሳንድራ ግለስፒ፡ ቶሮንቶ ዩኒቨርሲቲ፤ ዋልተር ሸሪር፡ ኖተርዳም ዩኒቨርሲቲ። </p>


            <p> መግለጫ</p>
            <p>ይህ ጥናት የሚገልፀው የግዕዝ ቋንቋ ፅሁፍን እና ሌሎች መሰል ትኩረት ያልተሰጣቸውን፣ ባህላዊና እና ጥንታዊ ሥሁፎችን ለመማር ወይም
               ለጥናት የሚፈልጉ ማህበረሰቦችን ፍላጎት ለማርካት የጥምር የጥናት ቡድናችን ስለቀረፀው ቀላል እና ሁሉም ሊጠቀምበት ስለሚችል
               መሣሪያ(ዘዴ) ነው።፡ይህ መሣሪያ የብራና ፅሁፍን የመሰሉ ረቂቅ ፅሁፎች የተፃፉባቸውን ገፆች ምሥል በማንሳት እና ፊደላትን ለይቶ
               በሚገነዘብ ጨረር (optical character recognition (OCR)) በመጠቀም ምሥሉን ወደ መደበኛ ወይም ሁለተኛ ፅሁፍነት
               የመቀየር ችሎታ ያለው ነው። ይህ ኮምፒዩተር ላይ የተመሰረተ ዘዴ ወይም መሣሪያ የግዕዝ ቋንቋን ልዩ ባህርዮች ለይቶ እንዲያውቅ ሲባል
               ስለቋንቋው ያገኘውን መረጃ ወይም ዳታ የመንከባከብ እና የማከም ሂደቶችን አልፎ እንደ አንጎል ነርቮች መረብ እሽክርክሪት የሚመስል
               ኮንቮሉሽናል ሪከረንት ነውራል ኔትዎርክ (Convolutional Recurrent Neural Network) በመያዙ ገጽታዎችን እና
               ምሥሎችን ወደ ፅሁፍ ይቀይራል። ይህ ለሁሉም ተጠቃሚዎች ክፍት የሆነው ጽሁፍ ለተማሪዎች እንዲሁም ለኢትዮጵያ ጽሁፍ ጥናት ተመራማሪዎች
               የሚጠቅም ብቃት ያለው እና በቀላሉ በኮምፒዩተር ተፈልጎ ሊገኝ የሚችል ከመሆኑም በተጨማሪ የግዕዝ ጽሁፎቹ የኢትዮጵያን እና የኢትዮጵያን
               ህዝብ ታሪክና ባህል ግዕዝን በዲጂታል/በኮምፑተር ቀርፆ በማስቀመጥ በቀጣይነት እንዲኖር ያስችላል። አመቺ የሆነ ተጨባጭ ሁኔታ ሲኖር
               ደግሞ ይህ ለሁሉም ክፍት የሆነ የ OCR የግዕዝን ምስልን ወደ ፅሁፍ የሚቀይር መሣሪያ ወይም ዘዴ ሌሎች ትኩረት ያላገኙ ረቂቅ
               ፅሁፎችንም እንዲያነብ ተደርጎ ሊሰለጥን ወይም ዲዛይን ሊደረግ ይችላል። ይህ የፈጠርነው መሣሪያ/ዘዴ የተለመደውን ግራፊክስ ፕሮሰሲንግ
               ዩኒት (GPU) የተባለውን በኮምፕዩተር ምሥሎችን የማንበቢያ እና ማሳለጫ ዘዴ መጠቀም አያስፈልገውም። በዚህም ምክንያት ከሌሎች ዘመናዊ
               የአርቲፊሻል ኢንተሊጀንስ (AI systems ) ዘዴዎች አንፃር ሲታይ ሃይለኛ የኮምፒዩተር አቅም አይፈልግም። ይህንን መሣሪያ/ዘዴ ያለ
               ኢንተርኔት ወይም በይነ-መረብ ከግል ኮምፒዩተር፣ በኢንተርኔት እንዲሁም ወደፊት ኢንተርኔት ባለው የእጅ ሥልክን በመጠቀም ማስኬድ
               ይቻላል። ይህ ጥናት የሚገልጸው በአይነቱ የመጀመሪያ የሆነው እና ለሁሉም ክፍት የሆነ እንዲሁም በተገቢ ሁኔታ ጥራቱን ጠብቆ በጥምር
               ተመራማሪዎቻችን የበለፀገው መሣሪያ/ዘዴ ለማናቸውም በግዕዝ መጽሀፍቶች እና ውስጣቸው በያዙት ፅሁፎች ላይ ጥናት ለማድረግ ለሚፈልጉ
               ግለሰቦችም ሆኑ ማህበረሰቦች ሁሉ ጠቃሚ መሆኑን ለማስገንዘብ ነው።</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>

         <div>
            <head>Introduction</head>
            <p>This paper summarizes an interdisciplinary collaborative project to create an
               easy-to-use open-source tool that converts an image of a manuscript page written in
               the historical Ethiopic script of Gə'əz into a transcription using Optical Character
               Recognition (OCR). </p>
            <p>OCR tools can transform the life of a text in a digitized manuscript by rendering its
               images readable and searchable. This ability allows readers to engage directly with
               the contents of manuscripts and significantly eases the tasks of those describing or
               cataloging endangered collections or obsolescing files. OCR thus has a pivotal role
               in the preservation of historical texts into the future. However, recent years and
               new users have brought into focus a range of ethical challenges in the evolving field
               of digital preservation of cultural heritage, emphasizing the need for any
               digitization to serve first and foremost the needs and wishes of the communities who
               created and safeguard the cultural heritage being preserved <ptr
                  target="#manzuch2017"/>
               <ptr target="#liuzzo2019" loc="239"/>
               <ptr target="#sutherland2021"/>. </p>
            <p>With this in mind we have designed an open-source tool that is accessible outside of
               a university setting, that can transcribe batches of images with no requirement that
               the text generated through transcription leave the control or the home environment of
               the person running the program. Our tool has low operational requirements in terms of
               computing power, and it does not require an internet connection to run, although we
               have also created a web interface to demonstrate it on a line-by-line basis. It can
               be used broadly by students and scholars of Ethiopian manuscripts to create a
               substantial and computer-searchable corpus of digitized and transcribed Gə'əz texts. </p>
            <p>Our tool advances AI-driven methods of OCR for manuscripts, through adaptable
               strategies that can be used to enhance research on other under-resourced textual
               traditions. The software itself is agnostic to language, and thus, although we have
               developed it for the specific context of cultural heritage preservation of Gə'əz
               language texts, we hope it will be of interest to other groups, as it can be
               retrained easily for other languages and scripts given a new JSON file. In this paper
               we thus describe our own process for preparing ground-truth, and we include some
               discussion of how to adapt it to a new context in the final stages of the paper. </p>
            <div>
               <head>Background on the Gə'əz Language</head>
               <p>Gə'əz is largely considered to fall within the North Ethiopic subgroup of
                  Ethiopian Semitic languages <ptr target="#weninger2005" loc="732"/>. Other
                  languages in the Ethiopian Semitic language family include Amharic, Tigre, and
                  Tigrinya <ptr target="#appleyard2005" loc="51"/>
                  <ptr target="#weninger2005" loc="732"/>. Gə'əz has a distinctive script, which ––
                  unlike some other Semitic languages, such as Hebrew and Arabic –– is read from
                  left to right. The script’s characters are phonetic, and arranged in a table
                  called by members of the Ethiopian community a fidäl, rather than in a sequential
                  alphabet. We reproduce a sample table in Appendix A of a fidäl consisting of 245
                  characters laid out on thirty-five rows across seven columns with the inclusion of
                  characters specific to Amharic script used in some personal and place names, and
                  additional characters representing labiovelars or semi-vowel glides. Words in
                  Gə'əz script are formed by concatenating characters, and words are separated by a
                  character that looks very similar to the colon punctuation mark, ‘፡’. Sentences
                  are separated by a ‘double-colon’ looking character, ‘።’. Sometimes, the end of a
                  sentence is indicated by ‘፨’. Semicolons are indicated by the character ‘፤’. This
                  punctuation system developed over time, and thus older documents often lack the
                  usage found in later documents. The Gə'əz numerical system has different
                  characters for digits from one to nine, separate characters for numbers that are
                  multiples of ten, and a character for the value ‘hundred’.</p>
               <p>Gə'əz served as the dominant written language in the Christian kingdom of Ethiopia
                  from the 3rd century until the mid-16th <ptr target="#kelly2020" loc="25"/>.
                  Today, Gə'əz continues to be used as a liturgical language and tongue of poetic
                  expression both in the Horn of Africa and in the Ethiopian diaspora. Gə'əz is also
                  associated with a living, though endangered, manuscript tradition; manuscript
                  production in Ethiopia continued robustly into the mid- to late-20th century <ptr
                     target="#winslow2015" loc="7–12"/>. An estimated 200,000 Gə'əz manuscripts,
                  primarily from the 14th century onwards, still survive in Ethiopia <ptr
                     target="#nosnitsin2020" loc="289"/>, and are kept by the churches and
                  monasteries of the Ethiopian Orthodox Täwahǝdo Church, as well as public
                  institutions such as the Addis Ababa University’s Institute of Ethiopian Studies
                  (IES) and the National Archives and Library of Ethiopia <ptr target="#bausi2015"
                     loc="47"/>.</p>
               <p>The continuity of this living manuscript tradition, however, has become critically
                  endangered in Ethiopia.<note>The precarity of what Eyob Derillo describes as
                     Ethiopia’s <quote rend="inline">distinctive, extraordinary and irreplaceable
                        traditional school and academic system [...] which is still little known and
                        [...] for which support is rapidly dwindling in the 21st century</quote>
                     <ptr target="#derillo2019" loc="112"/> is illustrated in descriptions of the
                     past forty years. In 1981, Sergew Hable Selassie reported that <quote
                        rend="inline">[b]ookmaking is alive and well in Ethiopia,</quote> but noted
                     a decline in the demand for scribal work as more church books came into print
                     and the cost of materials rose <ptr target="#hableselassie1981" loc="3, 33–34"
                     />. In 2002, John Mellors and Anne Parsons interviewed 30 of <q>around one
                        hundred</q> scribes active in South Gondar, <quote rend="inline">the only
                        area where [manuscripts] are now produced in any quantity</quote>
                     <ptr target="#mellors2002" loc="4"/>. In 2015, Sean Winslow found and
                     interviewed just over 30 active scribes across multiple regions including
                     Gondar, and reported that <quote rend="inline">pockets of scribal activity
                        nevertheless survive throughout the country</quote>
                     <ptr target="#winslow2015" loc="23, 140–141"/>. In 2018, Gezae Haile identified
                     a number of factors contributing to the <q>alarming rate</q> of decline of the
                     tradition, especially in remote rural areas <ptr target="#haile2018"
                        loc="35–37"/>. How the situation may have changed further as a consequence
                     of Ethiopia’s current civil war will become apparent in time.</note>
                  Recognizing the need to document it, a range of international collaborations since
                  the 1960s have endeavored to preserve records and create facsimile surrogates of
                  Gə'əz manuscripts<emph> in situ</emph> through cataloging, photography,
                  microfilming, and digital images <ptr target="#stewart2017"/>. Our project
                  consulted digital facsimiles of 17 manuscripts imaged in Ethiopia by Donald Davies
                  (1968); the Ethiopian Manuscript Microfilm Library (EMML) (1973–1994); the
                  Endangered Archives Programme EAP432, led by Mersha Alehegne Mengistie (2011); the
                  Ethiopian Manuscript Imaging Project (EMIP, 2005) directed by Steve Delamarter; Michael
                  Gervers and Ewa Balicka-Witakowska’s digitizations of books at Gunda Gundē
                  Monastery (2006) in association with HMML; and the Ethio-SPaRe project (2009–2015)
                  led by Principal Investigator Denis Nosnitsin of the Universität Hamburg’s Hiob
                  Ludolf Centre for Ethiopian and Eritrean Studies (HLCEES). The digitized images we
                  accessed are now under the care of the Hill Museum and Manuscript Library (6
                  manuscripts); EAP (1 manuscript); EMIP (4 manuscripts), and Ethio-SPaRe (6
                  manuscripts). See Appendix B for further details on these manuscripts and the
                  availability of their digital surrogates.</p>
               <p>The digital age has ushered in a new wave of international collaborations and
                  research trips focussing on manuscript documentation and image preservation in
                  Ethiopia. Meanwhile, the digitizing of historical microfilms has come to be a
                  separate and essential source of manuscript images. Some of the most important and
                  early collections, including the UNESCO and Ernst Hammerschmidt–Tanasee projects
                  of the 1960s and some of the EMML projects, are in microfilm form. The pace of
                  microfilm digitization has taken decades longer than expected, and the most
                  important and largest of these collections (the EMML collection) still has a
                  significant percentage of image sets unavailable in any digital form. In certain
                  cases, decades after these projects, the whereabouts of the manuscripts the
                  microfilms represent cannot be confirmed, rendering the microfilm effectively the
                  book’s only extant witness. Other important microfilm collections represent Gə'əz
                  manuscripts historically based outside Ethiopia, such as the Library of Congress’s
                  microfilmed collection of manuscripts held at Sinai, or the University of Utah’s
                  microfilms of the manuscripts of the Ethiopian Orthodox Church in Jerusalem.
                  Digitization of microfilm collections greatly increases their accessibility; it
                  has been a priority for institutions such as HMML and the Bibliothèque nationale
                  de France (BnF), and continues to be a pressing need in the field –– bringing
                  attendant needs for metadata and appropriate cataloging. </p>
            </div>
            <div>
               <head>Gə'əz OCR and Our Contribution</head>
               <p>A range of resources and digital tools are under development to build upon the
                  above imaging efforts and to facilitate broad access to the precious cultural
                  heritage these manuscripts represent –– for a recent overview, see <ptr
                     target="#liuzzo2019" loc="xxv–xxxii"/>. To support these and other projects our
                  work contributes a tool: an open-source OCR transcription algorithm that can be
                  widely used by students and scholars of Ethiopian manuscripts to create a
                  computer-searchable corpus of transcribed and digitized Gə'əz texts, and which can
                  be retrained to serve other historical scripts and languages.</p>
               <p>Our open-source tool is the first to facilitate end-to-end AI-driven transcription
                  of Gə'əz, but our team has learned from previous projects in developing its unique
                  affordances. Daniel Yacob’s work on Unicode standards for Ethiopic languages
                  defined the text characters used for automatic transcriptions of Gə'əz
                  manuscripts, making this entire endeavor feasible <ptr target="#yacob2005"/>.
                  Siranesh Getu Endalamaw’s 2016 thesis, which introduced a deep learning-based
                  artificial neural network approach to recognize text in Gə'əz manuscripts <ptr
                     target="#endalamaw2016"/> and recent work by Fitehalew Ashagrie Demilew and
                  Boran Sekeroglu [<ref target="#demilew2019">2019</ref>] on Gə'əz character
                  recognition were constrained to the classification of pre-segmented characters.
                  While this is an important step in a text recognition tool, it does not lead to
                  readable text output, and the results are not directly comparable to what we
                  present in this paper. Daniel Mahetot Kassa and Hani Hagras [<ref
                     target="#kassa2018">2018</ref>] discuss the issue of segmentation of digitized
                  images into individual Gə'əz characters. Segmentation plays an important role in
                  transcription accuracy, but our tool does not require character-level
                  segmentation. With respect to languages close to Gə'əz, OCR for modern Amharic has
                  also been explored by Fitsum Demissie [<ref target="#demissie2011"
                  >2011</ref>].</p>
               <p>Looking beyond Gə'əz, a number of projects have focused on the transcription of
                  handwritten manuscripts and documents of medieval European and colonial origin.
                  For example, Alrasheed et al. use object detection networks to find and classify
                  individual characters in handwritten seventeenth-century Spanish American notary
                  records <ptr target="#alrasheed2019"/>. The medieval Latin script known as
                  Caroline Minuscule has been often explored due to the high availability of
                  creative commons scans from collections such as <ref
                     target="https://www.e-codices.unifr.ch/en">e-codices - Virtual Manuscript
                     Library of Switzerland</ref>. Long short-term memory recurrent models look at
                  raw image data to transcribe these texts <ptr target="#hawk2019"/>. A psychometric
                  loss that measures the samplewise reading performance of paleographers and
                  incorporates it into the training process improves accuracy on transcription of
                  Latin text in Caroline Minuscule script across a variety of models <ptr
                     target="#grieggs2021"/>. Calamari is a high performance OCR tool that
                  transcribes historical printed texts in both English and German with a high degree
                  of accuracy using a CRNN <ptr target="#wick2020"/>.</p>
               <p>Transkribus is a more complete computer-aided transcription platform that does
                  have some Gə'əz support <ptr target="#kahle2017"/>. While it offers a similar
                  feature set to our tool, there are several key differences. In a broad sense, our
                  software can be run locally, and is free and open-source, allowing users to add
                  functionality as they see fit. Transkribus offers more sophisticated layout
                  analysis tools, but requires the user to upload documents to an external server,
                  raising questions about intellectual property. Transkribus’s performance is
                  comparable to the algorithm proposed in this paper under certain conditions. We
                  have included a more detailed comparison of the two tools and performance on our
                  data in the results section below.</p>
               <p>In building our tool we were aware of some of the ethical challenges that Gə'əz
                  digitization projects face more broadly –– particularly with respect to issues of
                  selection, copyright and access, short- and long-term stewardship, and appropriate
                  uses of those images once created.<note> See e.g., <ptr target="#stewart2009"
                        loc="606–13"/>
                     <ptr target="#tomaszewski2015" loc="92"/>
                     <ptr target="#kominko2015" loc="liii"/>
                     <ptr target="#derillo2019" loc="104–105"/>
                     <ptr target="#woldeyes2020"/>
                     <ptr target="#loyer2021"/> on challenges encountered during the EMML project
                     see <ptr target="#stewart2017" loc="453, 458–467"/>. It is worth noting that
                     collaborations surrounding digitization of Gə'əz manuscripts are further
                     complicated by a history of bad-faith interactions from the 18th century
                     onwards with western scholars and collectors seeking to appropriate Ethiopian
                     manuscripts and cultural patrimony from their original owners <ptr
                        target="#hableselassie1981" loc="35"/>
                     <ptr target="#stewart2009"/>
                     <ptr target="#kominko2015"/>
                     <ptr target="#derillo2019"/>
                     <ptr target="#woldeyes2020"/> and by the disjunction of perspective between
                     western scholars seeing manuscripts as historical artifacts, and the Ethiopian
                     religious communities for whom the manuscripts were created, who continue to
                     venerate and employ the manuscripts for their original cultural and religious
                     significance <ptr target="#kominko2015"/>
                     <ptr target="#winslow2015"/>
                     <ptr target="#haile2018"/>. On some principles guiding our own development, see
                     e.g., Liuzzo’s <ref target="#liuzzo2019">2019</ref> chapters <title
                        rend="quotes">Introduction</title> and <title rend="quotes">Openness and
                        Collaboration,</title> in particular on the need for any digital tools to be
                     useful in Ethiopia and in the manuscripts’ home environments: <quote
                        rend="inline">It might well be that a laudable software methodology produces
                        an output which is from the perspective of the users useless or wrong. [...]
                        The possibility to work digitally should benefit Eritrean and Ethiopian
                        scholars in the first place.</quote>
                     <ptr target="#liuzzo2019" loc="xvii, 234"/>.</note> As Lara Putnam argues,
                     <quote rend="inline">the digitized revolution is not inherently egalitarian,
                     open, or cost-free</quote> and digital corpora can threaten the <quote
                     rend="inline">place-specific learning that historical research in a pre-digital
                     world required</quote>
                  <ptr target="#putnam2016" loc="389, 377"/>. A nuanced discussion of these issues
                  is not possible here and we do not attempt it, although navigating them is the
                  central work of several of our team members, who have published on this elsewhere.
                     <note>See <ptr target="#derillo2019"/>
                     <ptr target="#akbari2019"/>
                     <ptr target="#delamarter2023"/>.</note> In this project we have endeavored to
                  operate under principles of community-driven development and open collaboration,
                  focussed on issues of access. This work too is still ongoing –– we describe some
                  next steps, including accessibility mechanisms in the technology for further
                  community engagement, at the conclusion to this paper.</p>
            </div>
         </div>
         <div>
            <head>Data Collection and Collaborative Workflow</head>
            <p>Collecting ground-truth data, or labeled examples from which the algorithm can learn,
               is always one of the biggest challenges when implementing a machine learning-based
               tool, but this rings especially true for specialized handwritten text transcription
               tasks. Historical documents often require significant domain expertise in ancient
               languages and scripts to read, meaning that the data collection process cannot simply
               be outsourced to human intelligence crowdsourcing services like Amazon Mechanical
               Turk. Furthermore, documents such as these, when transcribed by humanists, are
               typically not in a format that is suitable for use as ground-truth. When
               consolidating into an edition, edits are made that make the documents more
               accessible, but also make the result unsuitable for use as ground-truth for an
               algorithm, since it no longer represents a one-to-one (diplomatic) transcription of
               the original text.</p>
            <p>One of the most important aspects of our collaboration was facilitating the
               collection of ground-truth data with which to train the algorithm. We experimented
               with two different workflows for this collection. In one method, co-author Eyob
               Derillo (SOAS, University of London; Asian &amp; African Collections Reference
               Specialist at the British Library) transcribed samples of whole pages of text from
               two manuscripts from Gunda Gundē Monastery, viewable on vHMML’s Reading Room
               interface as <ref target="https://www.vhmml.org/readingRoom/view/500155">GG
                  00004</ref> and <ref target="https://www.vhmml.org/readingRoom/view/133110">GG
                  00016</ref> (see Appendix B). These manuscripts were chosen essentially at random
               with respect to content, but with pages screened for clear and readable
               appearance.</p>
            <p>Our second method, developed and largely executed by co-authors Gelila Tilahun
               (University of Toronto) and Sam Grieggs (Notre Dame), emerged from meetings of the
               University of Toronto and Notre Dame teams with the <title rend="inline">Cannibal of
                  Qəmər</title> project team led by co-author Steve Delamarter in collaboration with
               Wendy Belcher (Princeton University). The <title rend="quotes">Cannibal of
                  Qəmər</title> project is a research collaboration between EMIP and Princeton
               University, based on the foundation of decades of imaging and cultural heritage
               preservation work performed by Delamarter and others, as detailed above. Workers in
               the Qəmər team including Delamarter, Jeremy Brown, Jonah Sandford, and Ashlee Benson
               had made diplomatic transcriptions of 95 manuscript witnesses of one of the
               best-known Täˀammərä Maryam (the Miracles of Mary) stories of the Ethiopian Orthodox
               Church –– the tale of <title rend="quotes">The Cannibal of Qəmər.</title> The Qəmər
               team shared these transcriptions with the University of Toronto and Notre Dame
               collaborators as they did with Pietro Liuzzo of Hamburg University. With this
               contribution, our teams were able to make significantly faster progress towards an
               OCR reader, as Tilahun and Grieggs coordinated with Sindayo Robel (University of
               Toronto) and Enkenyelesh Bekele (volunteer) to lead the work to reformat the Qəmər
               team’s transcription data into a format suitable for training.</p>
            <p>Thus, the experimental data upon which we ultimately drew consist of images and
               transcriptions of lines of handwritten text from seventeen manuscripts written by
               scribes in Ethiopian church communities and monasteries between the sixteenth and
               twentieth centuries. As stated above, these manuscripts were originally imaged in
               Ethiopia with the consent and collaboration of their home institutions in a series of
               cultural heritage preservation projects between the 1960s and 2000s –– see Appendix B
               for further details on these manuscripts and the availability of their digital
               surrogates.</p>
            <p>The manuscripts are written on parchment folios, with text blocks divided into two or
               three columns depending on the manuscripts’ size. The majority contain compilations
               of Täˀammərä Maryam (the Miracles of Mary) stories of the Ethiopian Orthodox Church.
               We drew our data from copies of a single text from this collection, specifically, the
               tale of <title rend="quotes">The Cannibal of Qəmər.</title> The full tale of <quote
                  rend="inline">The Cannibal of Qəmər</quote> is described in approximately twelve
               to eighteen columns of writing. The sequential reading of the story starts from the
               top of the leftmost column and continues down to the end of the column. The story
               then continues from the top of the closest right column. Below is an image of two
               pages/folio sides from our manuscript image repository.</p>

            <figure>
               <head>Ethiopia, Tegrāy Province, Dabra Śāhel Agwazā Monastery, HMML <ref
                     target="https://w3id.org/vhmml/readingRoom/view/501278">Pr. No. DSAE
                     00014</ref>, fols. 40v–41r, from the Cannibal of Qəmər. Image courtesy of the
                  Dabra Śāhel Agwazā Monastery and the Hill Museum &amp; Manuscript Library.
                  Published with permission of the owners. All rights reserved. An example of Gə'əz
                  text used in our experiments. </head>
               <graphic url="resources/images/figure01.jpg"/>
               <figDesc>Scan of a manuscript. The text is written in Amharic and includes both black and red font.</figDesc>
            </figure>


            <div>
               <head>Data Pre-Processing</head>
               <p>For all of the manuscript samples, we have digital images of the handwritten texts
                  and corresponding transcriptions in standard unicode. These transcriptions have
                  been prepared by human experts and the accuracy of the transcriptions has been
                  thoroughly verified. For each manuscript, we prepared two files: an image file and
                  a text file. For each manuscript, starting from the column and line at which the
                  Qəmər text begins, we sequentially worked our way down the column by manually
                  cropping an image of each of the lines separately and copying them onto the image
                  file. These cropped line images were sequentially numbered. In the text file, we
                  copied the transcribed text that corresponded exactly to the cropped-and-copied
                  line images. This way, the line number of a cropped line image corresponds exactly
                  to the line number of a transcribed text. Since the goal of the project is to
                  train a handwriting transcription algorithm, we note that if the scribe of a
                  manuscript inadvertently misspelled or omitted a word, or left out a punctuation
                  mark, the training transcription does not make a correction. </p>
               <p>When pre-processing the data, there were a few things that were left out from
                  consideration. For example, when we encounter rubricated words, such as a mention
                  of <q>Mary</q> (<hi rend="bold">ማርያም</hi>) or <q>Son of Christ</q> (<hi rend="bold">ወልደ ክርስቶስ</hi>), the textual information
                  is recorded in a homogenous color, and we thus do not capture the rubrication.
                  Similarly, the rubricated parts of punctuations are also recorded in a homogenous
                  color. In addition, pictorial illustrations, as well as their associated text
                  bubbles, are omitted in the transcribed texts. Therefore, scribal information
                  conveyed through the emphasis of rubricated words or pictorial illustration will
                  not be available. Below is an excerpt of a pictorial reproduction of the <title
                     rend="quotes">The Cannibal of Qəmər</title> tale.</p>

               <figure>
                  <head>Ethiopia, Tǝgray Province, Däbrä Dammo ˀAbunä ˀArägawi, MS C3-IV-229
                     (digitized through the Ethio-SPaRe Project as <ref
                        target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00001852"
                        >EthioSPaRe DD-001</ref>), fols. 85v–86r. 1632–1664. Cataloged by Susanne
                     Hummel. Accessed 2 Feb 2022 at <ref
                        target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00001852"
                        >https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00001852</ref>.
                     An example from the Cannibal of Qəmər showing a page with an illustration
                     containing text. </head>
                  <graphic url="resources/images/figure02.png"/>
                  <figDesc>Scanned manuscript written in Amharic which includes both black and red text. The image on the right features images of people being beheaded</figDesc>
               </figure>


            </div>
            <div>
               <head>Synthetic Ground-Truth Generation</head>
               <p>One of the reasons that automatic document transcription is such a worthwhile task
                  is that it is a very expensive process to transcribe documents manually –– in both
                  time and money. Unfortunately, this also means that the process of collecting
                  ground-truth data is similarly resource-intensive. This is especially true for
                  texts like Ethiopic manuscripts, which require specialized knowledge to read.
                  Therefore we also make use of synthetic ground-truth data –– i.e., images
                  generated using transcriptions for which we don’t possess a corresponding original
                  image, for training purposes. Such data helps us account for variance in the
                  documents that will ultimately be transcribed by the trained neural network model
                  in the transcription tool. </p>

               <figure>
                  <head>Examples of synthetic ground-truth. The images are created by taking
                     transcribed text, splitting the lines up to match the length of the text lines
                     in the target dataset, and then generating corresponding images for that text
                     using different fonts, backgrounds, and colors to resemble possible
                     configurations of the real documents. </head>
                  <graphic url="resources/images/figure03.png"/>
                  <figDesc>Screenshot of three separate lines in manuscripts. The top and middle texts are written in red font and the last is in black</figDesc>
               </figure>



               <p>21,873 lines transcribed from the legacy of the hagiographic Christian tradition
                  in Ethiopia were used as the basis for generating the synthetic data. To better
                  match the formatting of our real data, the text lines were assembled into 106,616
                  images, since the real data we were working with were in a 2-column format, with
                  significantly fewer words per image than using the lines associated with the
                  formatting in this separate transcribed text. Therefore, we added additional line
                  breaks, such that the resulting images contained a similar distribution of the
                  number of words per line as the dataset of real documents.</p>
               <p>This data was provided by the Textual History of the Ethiopic Old Testament
                  Project, and represents a gold standard for direct transcription data <ptr target="#assefa2020"/>. As their goals were textual analysis, they went to great lengths to
                  ensure the transcription recorded was directly what was written on the page, which
                  increases its value as a representation of handwritten Gə'əz. The synthetic
                  ground-truth text was generated using the <ref
                     target="https://github.com/Belval/TextRecognitionDataGenerator">Text
                     Recognition Data Generator</ref>, with special fonts added to account for the
                  unicode characters used in Gə'əz script. Examples are shown in Figure 3.</p>
            </div>
         </div>
         <div>
            <head>A Deep Learning-Based Transcription Algorithm for Gə'əz</head>
            <p>Convolutional Recurrent Neural Networks (CRNNs) are commonly used for the task of
               handwritten text recognition <ptr target="#shi2016"/>. A CRNN is an artificial neural
               network architecture that is specifically designed for sequential text processing and
               meant to be trained over many different image samples of writing. The resulting
               trained network is commonly referred to as a model, which represents the core
               functionality of any transcription tool. CRNNs have recently been used in
               transcription tools that have achieved state-of-the-art results on the IAM and RIMES
               datasets of handwritten text <ptr target="#xiao2020"/>. The English IAM and French
               RIMES datasets are commonly used by the computer vision and document processing
               communities to benchmark handwritten text recognition tasks. Despite the
               significantly expanded character set, the CRNN technique can be applied to Gə'əz
               without a significant processing penalty. </p>
            <p>CRNNs offer good performance, while not requiring excessive resources. In this case,
               we have found that it is reasonable to run a trained CRNN model at inference time
               without GPU acceleration, with CPU speeds of up to 4 lines of text per second on
               modern hardware from the past several years. However, even with the older hardware
               commonly found at under-resourced institutions, performance is still very good. For
               instance, on a 2013 Thinkpad T440p running Microsoft Windows we averaged 2.2 lines
               per second. While this is significantly slower than when processing on a GPU (usually
               more than 20 lines per second), it makes it possible to run this software to
               transcribe documents on just an old laptop. A GPU is still recommended to train
               models.</p>
            <p>CRNNs work by passing the learned feature representations of a convolutional neural
               network into a series of recurrent layers that can analyze the text as a sequence. In
               essence, this means that the first convolutional layers of the network learn the
               parts of the image that are most important for identifying individual characters,
               encoding them into a sequence. This sequence is then analyzed by a series of
               recurrent layers that can understand context. This allows the model to understand, to
               some extent, which characters are likely to appear together, and to utilize that
               information when making a prediction about what an individual character should
               be.</p>
            <p>While the specifics of the network can vary, we have drawn inspiration from the work
               of Joan Puigcerver [<ref target="#2017">2017</ref>] and Shanyu Xiao et al. [<ref
                  target="#shanyuxiao2020">2020</ref>], who use a base CRNN model with 5
               convolutional layers which pass into 5 Bi-directional Long Short Term Memory (BLSTM)
               recurrent layers. We do not use any of the image rectification of Xiao et al.’s
               project. Additionally, we did not use dropout, a regularization technique that zeros
               out random parameters within the network to improve generalization <ptr
                  target="#srivastava2014"/>, as extensively as they did. Only a pooling layer was
               dropped. Removing that layer appeared to marginally improve performance in our
               implementation. The exact architecture is shown in Figure 4. The code was written in
               Python using the PyTorch framework and is available on github <ref
                  target="https://github.com/grieggs/Ge-ez-HWR">here</ref>.</p>


            <figure>
               <head>A diagram of the neural network architecture used for transcription. The image
                  passes through 5 convolutional layers into 5 Bidirectional Long Short Term Memory
                  (BLSTM) recurrent layers. The output size is specified at each layer, and as
                  images of text lines are inherently of variable length, ℓ refers to the length of
                  the input image. </head>
               <graphic url="resources/images/figure04.png"/>
               <figDesc>A diagram illustrating how the transcription process. The image is uploaded,
                  and then passed through a 5 layered convolutional BLSTM, then character
                  probabilities are calculated which produce a transcription</figDesc>
            </figure>

            <p>The output of the network is not a string of text, but a matrix containing character
               probabilities for timesteps that roughly (but not exactly) correlate with a length of
               4 pixels in the original image. For example, when the ground-truth is ስተ፡ጕርዔሁ፡ዘእንበለ,
               and we just take the highest probability character for each time step, the output
               looks something like this (the ‘~’ character represents a blank space):</p>
            <dhq:example><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ስስስ~~~~~~~~~~~~ተተተ~~~~~~~~~~~~~፡፡~~~~~~~~~~~~~~~~ጕጕጕጕ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ርር~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ዔዔዔ~~~~~~~~~~~~~~~~~~ሁሁሁሁሁ~~~~~~~~~~~~፡፡~~~~~~~~~~~~~~~~~~~~~ዘዘ~~~~~~~~~~~~~~~~~~~~~~~~~~እእ~~~~~~~~~~~~~~~~~~~ንንን~~~~~~~~~~~~~~~~~~በበበበ~~~~~~~~~~~~~~~~~~~~~~ለለ~~~~~~~~~~~</p></dhq:example>
            <p>This turns out to be a perfect transcription, because if we merge all the repeating
               characters not separated by blank characters (which to the network represent the end
               of a character) we get a matching transcription candidate of ስተ፡ጕርዔሁ፡ዘእንበለ. This same
               example is shown graphically in Figure 5.</p>


            <figure>
               <head>An example of the model output for one line of text. The first plot is a chart
                  showing the probability of a specific character occurring at each time step, and
                  the second plot is the same, but without the ‘blank’ character that specifies that
                  a particular timestep doesn’t correspond with a character. </head>
               <graphic url="resources/images/figure05.png"/>
               <figDesc>Two plots where the first plot is almost entirely filled with red lines
                  which represent the blank character and the second is the same plot but with blank
                  removed to show a few additional characters</figDesc>
            </figure>


            <p>A loss function is the measure of performance that a machine learning model is
               optimized against. It is a differentiable formula that can be used to calculate the
               model’s accuracy. With this, we can measure the gradient of this function when we
               change the model’s parameters, ideally changing them to move towards a more optimal
               result.</p>
            <p>For this work, we use a Connectionist Temporal Classification (CTC) loss, the most
               commonly used loss function in handwritten text recognition <ptr target="#graves2018"
               />. The CTC loss attempts to evaluate the network’s prediction by taking all possible
               alignments of the ground-truth and the output matrix and finding the sum of the
               probabilities of the correct alignments. For a simple synthetic example, assume the
               network output shown in the top-most table of Figure 6. If the ground-truth
               transcription is <q>ዘ,</q> then the possible alignments are <q>ዘዘ,</q>
               <q>~ዘ,</q> and <q>ዘ~</q>. This gives the probabilities shown in the center-left
               table, with the transcription receiving a CTC loss value of 0.76. The same process is
               illustrated for <q>ካ</q> and the blank character in the remaining two tables of the
               figure. Note that the blank character’s only possible alignment is <q>~~,</q> based
               on the information in the other two tables. The training objective is to maximize the
               probability that we have the correct transcription, a probability which is well
               represented by taking the CTC loss of the network prediction.</p>




            <figure>
               <head>Example calculations for the CTC Loss Function. The top table represents the
                  raw network output, i.e., the probability that a character is present at each
                  timestep. The lower tables show the calculation of the CTC loss for each possible
                  character transcription. </head>
               <graphic url="resources/images/figure06.png"/>
               <figDesc>Four charts. The top chart shows the network output with columns
                  representing character and probability at timestep. The one on the left is titled
                  Probability that the Transcription is "H" with the columns "possible solutions,"
                  "calculation," and "result." The one on the right is titled "Probability that the
                  Transcription is "n" with the column headings "possiuble solutions,"
                  "calculation," and "result." The chart at the bottom is titled "Probability that
                  the Transcription is 'Blank'" with the column headings "Possible Solutions,"
                  "calculation," "result"</figDesc>
            </figure>



         </div>
         <div>
            <head>Experiments</head>
            <p>To evaluate the proposed transcription tool, a series of controlled experiments were
               performed over relevant Gə'əz texts drawn from the datasets that were prepared for
               the project. These experiments are described below. </p>
            <div>
               <head>Data Augmentation For Training Dataset</head>
               <p>When performing character frequency analysis, we found that some of the characters
                  in our dataset appeared very infrequently. Not only does this negatively affect
                  the model’s ability to identify these characters, but it also limits our ability
                  to assess the transcription performance on individual characters. To address this
                  class imbalance, we generated new synthetic lines of text for the training and
                  evaluation (validation and test) datasets. These lines were created by collecting
                  all the individual words from both sets of texts and compiling a list of all the
                  words containing each character. We then randomly selected words from this list
                  such that each character appears a specified number of times in the training and
                  evaluation sets.</p>
               <p>Since we wanted to make sure that we did not overemphasize synthetic examples
                  during training, we generated synthetic data for the training set until each
                  character appeared 10 times. We did not add any synthetic data to the validation
                  set used for model tuning during training, since the model is trained until
                  performance plateaus on it, and we wanted to emphasize performance on images drawn
                  from real manuscript pages. For the testing dataset, we added data until there
                  were 100 examples of each character. While the model performs better on synthetic
                  data because there is less variance (due to the limited number of fonts), it is
                  trivial to evaluate the model both with and without the added synthetic examples.
                  Thus with this methodology, we could get a more accurate depiction of performance
                  on individual characters, without diluting performance on real manuscripts.
                  Additionally, all images were augmented on-the-fly using the random elastic
                  distortion technique of Wigington et al. <ptr target="#wigington2017"/>.
                  On-the-fly augmentation ensures that each image shown to the model is unique, and
                  greatly reduces the opportunity for the model to simply memorize each image, or
                     <q>overfit.</q></p>
            </div>
            <div>
               <head>CRNN Training</head>
               <p>For a rigorous evaluation, we trained three CRNN models in two stages using
                  different random initializations of the parameters that are adjusted as the model
                  learns. First, each model was trained on a dataset made completely from
                  synthetically generated text. These models are used as a starting point for the
                  three models that will be used for the transcription task; each is then fine-tuned
                  on the combination of real and synthetic data mentioned in the paragraph above.
                  Specifically, these models were trained on 106,616 line images of Gə'əz text
                  generated from transcriptions provided by the Textual History of the Ethiopic Old
                  Testament Project <ptr target="#assefa2020"/>, with an additional 9,593 images
                  created using disjoint text used as a validation set only during the initial
                  training phase. This selection of text contained 234 different characters. We used
                  an RMSprop optimizer with a learning rate of 3*10^-4, and each model was trained
                  from a random initialization until it did not improve for 80 epochs.</p>


               <figure>
                  <head>Examples of the various types of manuscripts in the evaluation
                     datasets.</head>
                  <graphic url="resources/images/figure07.png"/>
                  <figDesc>Three scans of three lines of text from three manuscripts. The first image is slightly more grainy while the second and last are of slightly better quality</figDesc>
               </figure>



               <p>We then proceeded to repeat this training process on our dataset of real Gə'əz
                  manuscript images augmented with a selection of synthetic examples of rare
                  characters using the three candidate models trained on the synthetic data as the
                  starting parameters. This process is commonly known as fine-tuning, and is helpful
                  to overcome domain mismatch between real and synthetic data. For the dataset
                  consisting of real manuscript images, we have a collection of 1,510 lines from
                  various manuscripts, including both high and low resolution color images, and also
                  lower quality images scanned from microfilm media in black and white. Examples of
                  these images are pictured above in Figure 7. 224 images were withheld as a
                  validation set for this additional training run, which was used to evaluate the
                  model after each epoch. Finally, 394 line images were held out as a test set,
                  which were only used for a final evaluation of a model after all training was
                  completed. This procedure was designed to ensure that the model is not overfit to
                  the data that it has seen in training, and can generalize to unseen data as well.
               </p>
            </div>
            <div>
               <head>CRNN Evaluation</head>
               <p>The metric commonly used to measure transcription performance, Character Error
                  Rate (CER) is calculated by counting the number of edits required to make the
                  predicted string match the ground-truth string, and dividing that number by the
                  length of the string. An example of this in English would be if a model outputs
                     <q>Helo,</q> when the ground-truth is <q>Hello,</q> which would require a
                  single edit, adding an <q>l</q> to correct it, making its <q>edit distance</q> 1.
                  This is known as a <q>Deletion Error.</q> Similarly, if the model outputs
                     <q>Helllo,</q> we reach the ground-truth string by removing a single <q>l,</q>
                  which also gives us an edit distance of 1. This is an <q>Insertion Error</q>.
                  Finally, if the model outputs <q>Helfo,</q> we can achieve the target string by
                  changing the <q>f</q> into an <q>l.</q> This also has an edit distance of 1, and
                  is considered a <q>Substitution Error.</q>
               </p>
               <p>To further emphasize what this means, some example mistranscriptions of the
                  English string <q>The Quick Brown Fox Jumps Over The Lazy Dog</q> and their
                  respective CERs are shown in Table 1.</p>
               <table>
                  <head>Examples of Character Error Rates (CER) of varying degrees. Note that some
                     types of errors are much more legible than others.</head>
                  <row role="label">
                     <cell>Proposed Transcription</cell>
                     <cell>CER</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <quote rend="inline">The Quick Brown Fox Jumps Over The Lazy Dog</quote>
                     </cell>
                     <cell>0%</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <quote rend="inline">The Quick Brown Fox Jumps Over The Lazy</quote>
                     </cell>
                     <cell>9.3%</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <quote rend="inline">Th Qick Brwn Fx Jmps vr Th Lzy Dg</quote>
                     </cell>
                     <cell>23.2%</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <quote rend="inline">the quick brown fox jumps over the lazy dog</quote>
                     </cell>
                     <cell>20.9%</cell>
                  </row>
                  <row role="data">
                     <cell>
                        <quote rend="inline">th aick dlak fao bump the dog</quote>
                     </cell>
                     <cell>60.4%</cell>
                  </row>

               </table>
               <p>The results listed in Table 2 below as <q>Test</q> are a reflection of performance
                  on data completely unseen during training time. These data are only used after
                  choosing a model based on the results on the validation set, and help demonstrate
                  model generalizability to unseen data. As a reminder, in order to evaluate
                  performance across all characters, we balanced the test set using synthetic
                  augmentations, so that each character appears at least 100 times. Since typeset
                  characters are easier for our model, we included the results with and without
                  these augmentations. While the results are not perfect, the average CER on the
                  Test set is quite low, meaning the models are able to reliably recognize
                  individual characters.</p>
               <table>
                  <head>Character Error Rate (CER) of our model on the Validation and Test sets of
                     the data we collected. The performance is aggregated across 3 models. The <q>Best</q>
                     row refers to the model we trained that had the best performance, which is the
                     one that would be selected for further use.</head>
                  <row role="label">
                     <cell/>
                     <cell>Validation CER (%)</cell>
                     <cell>Test CER (%)</cell>
                     <cell>Test w/o Augmentations CER (%)</cell>
                  </row>
                  <row role="data">
                     <cell>Average</cell>
                     <cell>8.25%</cell>
                     <cell>6.35%</cell>
                     <cell>10.62%</cell>
                  </row>
                  <row role="data">
                     <cell>Error</cell>
                     <cell>±0.31%</cell>
                     <cell>±0.88%</cell>
                     <cell>±0.32%</cell>
                  </row>
                  <row role="data">
                     <cell>Best</cell>
                     <cell>7.71%</cell>
                     <cell>4.60%</cell>
                     <cell>10.08%</cell>
                  </row>

               </table>
            </div>
            <div>
               <head>Analysis of Transcription Mistakes</head>
               <p>While CER is a useful tool to measure performance across transcription models, it
                  is somewhat lacking in assessing the readability of model output. A key question
                  is whether or not the output will be useful to the reader. The human brain does
                  not just sequentially classify each character individually when it reads, so some
                  incorrect transcriptions are more problematic than others, since they make the
                  transcription less legible. We can further analyze a model’s performance by
                  looking at exactly what kinds of mistakes it is making. For this we consider the
                  output of the best performing model. While there can be multiple shortest paths to
                  edit a line of text into a specified target, we systematically generated sets of
                  the minimum edits required to map the predicted strings onto the ground-truth
                  text, and found the characters most commonly missed. </p>
               <p>Considering the most commonly missed characters we find an interesting trend. The
                  cutoff for the 90th percentile of most missed characters is a CER of 14.8% in
                  aggregate. Table 3 shows the characters that make up the 90th percentile of most
                  frequently misidentified characters. With the exception of two punctuation
                  characters, we see that the vast majority of errors are substitution errors, and
                  for many, greater than 30% of the total errors comes from one specific character
                  substitution.</p>
               <table>
                  <head>The most commonly missed characters, and the characters most commonly
                     substituted (MCS) with them using our highest performing model.</head>
                  <row role="label">
                     <cell>Ground-Truth Character</cell>
                     <cell>MCS</cell>
                     <cell>Freq. of MCS</cell>
                     <cell>MCS % of Total Errors</cell>
                     <cell>Other Common Subs.</cell>
                     <cell>Total Sub. Errors</cell>
                     <cell>Total % of Sub. Errors</cell>
                     <cell>Total Errors</cell>
                  </row>
                  <row role="data">
                     <cell>ስ</cell>
                     <cell>ሰ</cell>
                     <cell>39</cell>
                     <cell>29.32%</cell>
                     <cell>ዕ,ሱ,ለ,ሶ,እ,ፅ,ል,፼,አ</cell>
                     <cell>123</cell>
                     <cell>92.48%</cell>
                     <cell>133</cell>
                  </row>
                  <row role="data">
                     <cell>የ</cell>
                     <cell>ያ</cell>
                     <cell>21</cell>
                     <cell>17.80%</cell>
                     <cell>፻,ም,ዖ,ዋ,ፆ,ዩ,ዬ,ዮ,ት</cell>
                     <cell>113</cell>
                     <cell>95.76%</cell>
                     <cell>118</cell>
                  </row>
                  <row role="data">
                     <cell>ዘ</cell>
                     <cell>ወ</cell>
                     <cell>39</cell>
                     <cell>17.89%</cell>
                     <cell>አ,ዝ,በ,ለ,ዞ,እ,ዚ,ኢ,፱</cell>
                     <cell>209</cell>
                     <cell>95.87%</cell>
                     <cell>218</cell>
                  </row>
                  <row role="data">
                     <cell>ሰ</cell>
                     <cell>ስ</cell>
                     <cell>55</cell>
                     <cell>55.56%</cell>
                     <cell>ዕ,ሱ,ለ,ሶ,ጎ,ኀ,ይ,ገ,ሕ</cell>
                     <cell>93</cell>
                     <cell>93.94%</cell>
                     <cell>99</cell>
                  </row>
                  <row role="data">
                     <cell>ቤ</cell>
                     <cell>ቢ</cell>
                     <cell>5</cell>
                     <cell>31.25%</cell>
                     <cell>ሊ,ጤ,ር,ጌ,ኬ,ሲ</cell>
                     <cell>13</cell>
                     <cell>81.25%</cell>
                     <cell>16</cell>
                  </row>
                  <row role="data">
                     <cell>ዓ</cell>
                     <cell>ፃ</cell>
                     <cell>8</cell>
                     <cell>19.05%</cell>
                     <cell>ዲ,ት,ዔ,ዒ,ኆ,ባ,ጓ,ጎ,ን</cell>
                     <cell>41</cell>
                     <cell>97.62%</cell>
                     <cell>42</cell>
                  </row>
                  <row role="data">
                     <cell>ገ</cell>
                     <cell>ን</cell>
                     <cell>9</cell>
                     <cell>16.98%</cell>
                     <cell>ጉ,ግ,ጓ,ፒ,ፐ,ፓ,ጌ,ጔ,ፔ</cell>
                     <cell>48</cell>
                     <cell>90.57%</cell>
                     <cell>53</cell>
                  </row>
                  <row role="data">
                     <cell>ዳ</cell>
                     <cell>ደ</cell>
                     <cell>4</cell>
                     <cell>13.79%</cell>
                     <cell>ዱ,ድ,ዴ,ጻ,ጼ,ት,ላ,ኆ,ፋ</cell>
                     <cell>28</cell>
                     <cell>96.55%</cell>
                     <cell>29</cell>
                  </row>
                  <row role="data">
                     <cell>ጸ</cell>
                     <cell>ጻ</cell>
                     <cell>18</cell>
                     <cell>23.38%</cell>
                     <cell>ጾ,ጽ,ጼ,ጳ,ጹ,፳,፰,ጲ,፷</cell>
                     <cell>74</cell>
                     <cell>96.10%</cell>
                     <cell>77</cell>
                  </row>
                  <row role="data">
                     <cell>ሀ</cell>
                     <cell>ሠ</cell>
                     <cell>4</cell>
                     <cell>14.81%</cell>
                     <cell>ህ,ሁ,፼,ቤ,፱,ኔ,ይ,ዕ,ዐ</cell>
                     <cell>23</cell>
                     <cell>85.19%</cell>
                     <cell>27</cell>
                  </row>
                  <row role="data">
                     <cell>፪</cell>
                     <cell>ቋ</cell>
                     <cell>4</cell>
                     <cell>14.29%</cell>
                     <cell>፸,፶,፩,፭,፱,፼,ዪ,፻,ዴ</cell>
                     <cell>23</cell>
                     <cell>82.14%</cell>
                     <cell>28</cell>
                  </row>
                  <row role="data">
                     <cell>፩</cell>
                     <cell>፳</cell>
                     <cell>21</cell>
                     <cell>61.76%</cell>
                     <cell>፯,፶,፴,፰,፺,፸,፹,፭</cell>
                     <cell>31</cell>
                     <cell>91.18%</cell>
                     <cell>34</cell>
                  </row>
                  <row role="data">
                     <cell>ጠ</cell>
                     <cell>ጡ</cell>
                     <cell>7</cell>
                     <cell>36.84%</cell>
                     <cell>ጣ,ጤ,፷,ጢ</cell>
                     <cell>17</cell>
                     <cell>89.47%</cell>
                     <cell>19</cell>
                  </row>
                  <row role="data">
                     <cell>ቍ</cell>
                     <cell>ቊ</cell>
                     <cell>34</cell>
                     <cell>85.00%</cell>
                     <cell>ቁ,ኈ</cell>
                     <cell>38</cell>
                     <cell>95.00%</cell>
                     <cell>40</cell>
                  </row>
                  <row role="data">
                     <cell>፯</cell>
                     <cell>፮</cell>
                     <cell>11</cell>
                     <cell>44.00%</cell>
                     <cell>፰,፪,፺,፩,፫,፭</cell>
                     <cell>22</cell>
                     <cell>88.00%</cell>
                     <cell>25</cell>
                  </row>
                  <row role="data">
                     <cell>:</cell>
                     <cell>፡</cell>
                     <cell>1</cell>
                     <cell>4.76%</cell>
                     <cell>n/a</cell>
                     <cell>1</cell>
                     <cell>4.76%</cell>
                     <cell>21</cell>
                  </row>
                  <row role="data">
                     <cell>፶</cell>
                     <cell>፵</cell>
                     <cell>10</cell>
                     <cell>41.67%</cell>
                     <cell>፺,፱,፲,፭</cell>
                     <cell>21</cell>
                     <cell>87.50%</cell>
                     <cell>24</cell>
                  </row>
                  <row role="data">
                     <cell>፬</cell>
                     <cell>፴</cell>
                     <cell>9</cell>
                     <cell>25.71%</cell>
                     <cell>፹,፩,፵,፱,፰,፯,፻,፸</cell>
                     <cell>32</cell>
                     <cell>91.43%</cell>
                     <cell>35</cell>
                  </row>
                  <row role="data">
                     <cell>ኍ</cell>
                     <cell>ኊ</cell>
                     <cell>19</cell>
                     <cell>100%</cell>
                     <cell>n/a</cell>
                     <cell>19</cell>
                     <cell>100%</cell>
                     <cell>19</cell>
                  </row>
                  <row role="data">
                     <cell>.</cell>
                     <cell>:</cell>
                     <cell>1</cell>
                     <cell>1.85%</cell>
                     <cell>n/a</cell>
                     <cell>1</cell>
                     <cell>1.85%</cell>
                     <cell>54</cell>
                  </row>
                  <row role="data">
                     <cell>፵</cell>
                     <cell>፶</cell>
                     <cell>5</cell>
                     <cell>50.00%</cell>
                     <cell>፴</cell>
                     <cell>6</cell>
                     <cell>60.00%</cell>
                     <cell>10</cell>
                  </row>
                  <row role="data">
                     <cell>፺</cell>
                     <cell>፲</cell>
                     <cell>7</cell>
                     <cell>18.42%</cell>
                     <cell>፷,፰,፶,፳,፯,፭,፮,፵</cell>
                     <cell>35</cell>
                     <cell>92.11%</cell>
                     <cell>38</cell>
                  </row>
                  <row role="data">
                     <cell>ኊ</cell>
                     <cell>ኍ</cell>
                     <cell>28</cell>
                     <cell>84.85%</cell>
                     <cell>ኁ,ኈ</cell>
                     <cell>31</cell>
                     <cell>93.94%</cell>
                     <cell>33</cell>
                  </row>
                  <row role="data">
                     <cell>ፒ</cell>
                     <cell>ፕ</cell>
                     <cell>27</cell>
                     <cell>65.85%</cell>
                     <cell>ፔ,ጊ</cell>
                     <cell>37</cell>
                     <cell>90.24%</cell>
                     <cell>41</cell>
                  </row>

               </table>
               <p>Looking at the situations where characters are missed, we see some encouraging
                  signs. We found a large portion of the model’s errors were substitution errors
                  amongst characters that are visually similar. This tends to be one of the more
                  legible errors, and means in aggregate that it should be easier to understand the
                  resulting transcription. Furthermore, of the commonly missed characters that are
                  not frequently substituted with one visually similar character, most are
                  punctuation. Another trend we see in the data is that the model performs poorly on
                  numbers. This is almost certainly due to the fact that our real training data was
                  a collection of biblical stories, so these characters did not occur frequently.
                  Digits tend to be an illegible error, since they would be difficult to correct
                  without looking at the source. But looking at the commonly missed characters, we
                  see that when the model misses digits it is mostly confusing them with other
                  digits (see Appendix A for a listing of the numerical characters).</p>
               <p>This means that in the worst case, our model will produce transcriptions that will
                  be usable right away, with any errors being easily correctable by the human
                  readers. The accuracy of our tool’s reading of Gə'əz might, however, be improved
                  in future. In state-of-the-art transcription tools for modern handwritten text,
                  decoders based on language models are often used to improve transcription quality
                  over the naïve <quote rend="inline">highest probability character at each time
                     step</quote>’ approach. Implementing this, however, creates a significant
                  challenge when historical documents are the source texts: it is notoriously
                  difficult to model their language, because few diplomatic transcriptions of such
                  documents exist, and producing them specifically for the purpose of OCR is
                  laborious. While the language models that are used for decoding are typically very
                  simple n-gram probability-based models, which do not require data on the scale of
                  state-of-the-art transformer-based language models, finding good representations
                  of the target data can be difficult, since most of this type of data is not
                  standardized across time and space. We could address some of these challenges by
                  working collaboratively with our team on more specific language models. In
                  addition, some of the characteristics of the Gə'əz language could be exploited in
                  the structure of the CRNN. The character family information could potentially be
                  incorporated into the loss function to allow for transcriptions that are more
                  readable than the ones that are currently being produced, if not perfect.</p>
            </div>
            <div>
               <head>Comparison to Transkribus</head>
               <p>Transkribus <ptr target="#kahle2017"/> is another tool that can be used not only
                  to automatically transcribe handwritten text, but also to collect ground-truth
                  data for it. The Transkribus project has an Ethiopic text model available; thus we
                  have decided to include a brief comparison to the results from that tool.</p>
               <p>It can be somewhat difficult to compare transcription performance across
                  algorithms and datasets, for a number of reasons, including the difficulty of the
                  evaluation dataset, the differences in training data, and differences in string
                  tokenization. String tokenization refers to the <q>rules</q> on how to handle
                  possible edge cases when making ground-truth data. One example of this is that our
                  data do not have spaces between words, only the Gə'əz word separator, <q>፡,</q>
                  while the Transkribus model’s data includes a space <q>፡,</q> which would be
                  counted as an error with a direct comparison to our tool. Because of this, we
                  manually retokenized the model output to match our data. For the purposes of this
                  comparison we share Transkribus’s reported error rate. However, since the dataset
                  the Transkribus model is trained and validated on is different from the dataset we
                  put together, and that data is not publicly available, we also provide results on
                  the test set for our data. This may not be a fair comparison, but it allows us to
                  give better context for the results.</p>
               <p>In order to give Transkribus the best chance on our data, we uploaded each line
                  image of the unaugmented test set to the project’s server and manually drew text
                  region boxes around the whole text, lining boxes tightly around the text for each
                  image. This is not quite the intended use case for Transkribus, since the platform
                  supports whole pages, and has a number of automated segmentation tools, but it
                  ensured a more direct comparison, with both tools seeing the same images.
                  Additionally, some of our images contain cropping artifacts, that is, some black
                  space around the edge of the images. These tended to break Transkribus’s line
                  segmentation, which is unsurprising, because it would be out of scope for what
                  that tool has been designed to work with. The manual bounding boxes were intended
                  to help give it the best chance at dealing with these as well.</p>
               <table>
                  <head>Comparison in performance on our Test Set using only real manuscript
                     images.</head>
                  <row role="label">
                     <cell/>
                     <cell>Character Error Rate</cell>
                     <cell></cell>
                  </row>
                  <row role="data">
                     <cell>Transkribus on that project’s Validation Set</cell>
                     <cell>5.16%</cell>
                     <cell></cell>
                  </row>
                  <row role="data">
                     <cell>Transkribus on this paper’s Test Set</cell>
                     <cell>27.6%</cell>
                     <cell></cell>
                  </row>
                  <row role="data">
                     <cell>Proposed tool on this paper’s Test Set</cell>
                     <cell>10.6%</cell>
                     <cell>±0.32%</cell>
                  </row>
               </table>
               <p>From the results in the above table, we can see that Transkribus performs better
                  on that project’s own data, but doesn’t generalize to our data with a similar
                  level of performance. Note that our result is the average performance over three
                  trained neural networks, but Transkribus only has one model for Gə'əz. By diving
                  further into these results we see a pattern where the Transkribus model has
                  trouble with cropping artifacts in some of our images where the lines were cropped
                  using a Lasso selection tool to prevent overlapping text from being included in
                  the image. The image itself is still a rectangle, so the areas outside of the
                  selection are filled with a solid black color. This accounts for a great deal of
                  the Transkribus model’s underperformance on our data because our model sees this
                  at training time and knows how to handle it, while Transkribus does not.</p>
               <p>We also found that the pattern of character-level errors was different between our
                  model and Transkribus. On the unaugmented test set, we found that Transkribus had
                  166 replacement errors, 1034 deletion errors, and 18 insertion errors. Our model
                  had 305 replacement errors, 22 deletion errors, and 152 insertion errors. A large
                  portion of the deletion errors from Transkribus were due to an inability to
                  process cropped microfilm images, where the black borders interfered with the
                  transcription process.</p>
               <p>The most important distinction between the two approaches is price. Our software
                  is open-source and freely available to anyone who wants to use it, while
                  Transkribus is a service that requires tokens. While they give a generous free
                  allocation of tokens, because the transcription is run on the project’s own
                  servers, there are maintenance costs associated with the upkeep. Thus additional
                  tokens must be purchased for large tasks.</p>
               <p>While we would recommend a GPU to train a model from scratch, our software can be
                  run on just a laptop to transcribe the text, so it offers additional portability.
                  Keeping the images local is also of some value. Situations often occur where
                  uploading the images to an external server may cause intellectual property
                  concerns. This is an issue we have encountered frequently when working with
                  digitized manuscripts.</p>
               <p>The Transkribus platform offers more sophisticated layout analysis software than
                  our tool provides, and contains a variety of pretrained models for multiple
                  languages available out of the box. Thus there are definitely use cases where it
                  could be a more appealing option. For instance, the narrow question of OCR
                  accuracy is not the only point of comparison that represents factors crucially
                  important to the needs of scholars. The functionality of joint access to image and
                  transcription provided by Transkribus, as well as its interface that connects
                  (highlights) the location in the images with the location in the transcription,
                  improves ease of use and the accuracy of the workflow. However, as our tool is
                  completely free and open-source, it might be a more attractive option for people
                  with internal tooling, since all of the source code is available to be integrated
                  into a custom pipeline.</p>
            </div>
         </div>
         <div>
            <head>Web Client and User Interface</head>
            <p>In order to showcase the functionality of our transcription tool, we designed and
               implemented a publicly accessible web demo which anyone can use to transcribe images
               of Gə’əz text. We prioritized making this website simple and user-friendly in order
               for scholars without extensive experience in computer science to use it. Software
               that utilizes neural networks is often seen as intimidating and prohibitively
               resource-intensive: we wanted to make sure that ours was accessible to end-users with
               more limited resources, and we kept in mind that many of these manuscripts are
               located in remote areas of Ethiopia, where it may be difficult to acquire and run
               large workstations with expensive GPUs.</p>
            <p>The neural network that directly transcribes text was originally coded in Python, so
               we decided to program the website through Flask, a web framework that Python
               provides, in combination with HyperText Markup Language (HTML). Through Flask’s web
               framework, we were able to code input and output pages that allow for the
               presentation of clear results for the transcription of an input text, as well as
               efficient methods of inputting an image into the website. Specifically, we coded the
               functions of the opening and resulting pages in combination with HTML templates that
               arrange and outline each page. Using Flask’s Dropzone package, it was possible to
               support dragging an image into one of the site’s pages as a method of inputting it
               into the tool. Though we programmed a Dropzone area for a user to drag in an image of
               a piece of text, we also coded in an option for the user to upload an image to cater
               towards different circumstances. Once an image is successfully uploaded, the
               resulting page will display the uploaded image and the resulting transcription in a
               formatted table. The original image and the resulting transcription will be adjacent
               to one another in the output.</p>
            <p>The website tool currently only accepts images of lines of text. We are currently
               working on a graphical user interface (GUI) feature that permits a user to input an
               entire page of text, similar to what our standalone tool supports (described below).
               The user would input the text in the same fashion. Utilizing contour analysis and
               thresholding functions within Python, the GUI feature permits the user to clearly
               distinguish text and eliminate noise within the image by adjusting different
               parameters on trackbars. Python’s OpenCV library includes trackbar functions that can
               be coded in the GUI feature. With these additional features, transcription accuracy
               and accessibility of the demo will be greatly enhanced.</p>
            <p>This web interface is written in such a way that it can interface with a PyTorch
               backend. The server running this tool has no GPU, which demonstrates our software’s
               flexibility. The tool can be accessed at <ref target="http://docturk.crc.nd.edu/"
                  >http://docturk.crc.nd.edu/</ref> and the code used to run it can be found at <ref
                  target="https://github.com/grieggs/Ge-ez-HWR"
                  >https://github.com/grieggs/Ge-ez-HWR</ref>.</p>


            <figure>
               <head>Screenshots of the transcription site. A user can just drag a line of text into
                  the upload box (top), and the tool will transcribe it automatically
                  (bottom).</head>
               <graphic url="resources/images/figure08.png"/>
               <figDesc>Screenshot of the digitized text that is very blurry. Beneath the image
                  is a chart which includes the model (using crnn), gpu state (no gpu detected),
                  validation set size (1), and predicted string</figDesc>
            </figure>





            <p>Additionally, we have a fully offline version that can be run locally on a user’s
               computer. It can process segmented line images in batches of any size, transcribing
               entire directories of images all at once. This tool can be found at <ref
                  target="https://github.com/grieggs/Ge-ez-HWR"
                  >https://github.com/grieggs/Ge-ez-HWR</ref>, and instructions on how to run it are
               included in the repository. The offline tool also includes some simple software for
               segmenting lines semi-automatically. The offline version offers users the ability to
               train a new model from scratch, or to fine-tune an existing model on a specific
               dataset of interest. This feature works generically on handwritten text regardless of
               language, but we recommend using a CUDA capable GPU for training. All of this
               software is fully open-source and available under an MIT License. Thus anyone
               interested in using this software is welcome to branch these repositories and modify
               the code as needed without any restrictions. </p>
         </div>
         <div>
            <head>Conclusion </head>
            <p>The past decade has witnessed significant growth in the study and conservation of
               Ethiopian manuscripts and cultural history <ptr target="#nosnitsin2020" loc="282"/>,
               including the large scale project <ref
                  target="https://www.betamasaheft.uni-hamburg.de/">Beta maṣāḥǝft: Manuscripts of
                  Ethiopia and Eritrea</ref> at the Hiob Ludolf Centre for Ethiopian Studies at the
               University of Hamburg (2016–2040, PI Alessandro Bausi). Yet compared with Western
               heritage materials, such as European manuscripts, Gə'əz manuscripts need more and
               better tools to aid their study and preservation. Gezae Haile argues there is <quote
                  rend="inline">a greater need than ever for systematic recording/cataloging,
                  conservation and digitization of Ethiopian manuscripts in Ethiopia</quote>
               <ptr target="#haile2018" loc="41"/>, and Liuzzo meanwhile notes a <quote
                  rend="inline">major shift in focus</quote> in the field of digital Ethiopian
               studies, <quote rend="inline">to the aim of creating resources available
                  online</quote>
               <ptr target="#liuzzo2019" loc="xxiv"/>. </p>
            <p>We hope that our OCR tool will make a significant contribution to the broader project
               of resourcing digital Ethiopian studies. We also hope it will have significance for
               research into other historical manuscript cultures. Our tool fits an important niche
               in the automated document transcription space: a lightweight, offline, and
               open-source algorithm, with a high degree of accuracy, that both simplifies the
               process of building a transcription tool from scratch, and allows for keeping the
               data in-house. It can run at reasonable speeds without a GPU, meaning it will work on
               most modern laptops, even in contexts where users have poor internet access. </p>
            <p>While our focus has been Gə’əz manuscripts, the tool as designed could also be used
               for other languages and scripts. Reworking the tool itself for a new language and
               script simply requires new ground-truth –– the more the better, but a minimum of
               1,000 lines –– plus a GPU and computer scientist for the training process. We’ve
               included a case study for retraining on modern English text in our github repo, using
               the same steps that can be generalized to other languages. Our team recently met with
               a group of Sanskrit scholars led by Ajay Rao at the University of Toronto
               Mississauga, to discuss OCR –– and the potential reworking of our tool –– for
               transcription of Sanskrit manuscripts. Now that the core of this tool exists and is
               open-source, it is available for a new phase of development along these lines:
               scholars working in other fields on digitized manuscripts written in under-resourced
               scripts can tailor and frame the technology to suit their particular needs. </p>
         </div>

         <div>
            <head>Acknowledgments</head>
            <p>Toronto members of the project team received funding through The Andrew W. Mellon
               Foundation (2019–2021); and the University of Toronto Mississauga Research and
               Scholarly Activity Fund (RSAF) 2019.</p>
            <p>vHMML images courtesy of the Dabra Śāhel Agwazā Monastery, Tegrāy Province; Qundi
               Giyorgis Church, Šawā Province; Ḥayq Esṭifānos Monastery, Wallo Province; Darafo
               Māryām Church, Šawā Province; Gunda Gundē Monastery, Tegrāy Province, Ethiopia, and
               the Hill Museum &amp; Manuscript Library. Published with permission of the owners.
               All rights reserved. </p>
            <p>All EMIP images were accessed courtesy of Ethiopic Manuscript Imaging Project (EMIP),
               Steve Delamarter, Director.</p>
            <p>Ethio-SPaRe images accessed courtesy of Ethio-SPaRe, EU 7th Framework Programme, ERC
               Starting Grant 240720, PI Denis Nosnitsin, 2009–2015, <ref
                  target="https://www.aai.uni-hamburg.de/en/ethiostudies/research/ethiospare.html"
                  >https://www.aai.uni-hamburg.de/en/ethiostudies/research/ethiospare.html</ref>.</p>
            <p>For searchability, place names and other manuscript identifiers are listed with the
               transcription systems used by the manuscripts’ respective digital platforms. </p>
            <p>The <title rend="italic">Fidäl</title> table in Appendix A was reproduced with kind
               permission of Alessandro Bausi, and originally made for Samantha Kelly, ed., <title
                  rend="italic">A Companion to Medieval Ethiopia and Eritrea</title>, Brill, Leiden
               (2020). Freely accessible at <ref
                  target="https://brill.com/view/book/9789004419582/front-11.xml"
                  >https://brill.com/view/book/9789004419582/front-11.xml</ref>. </p>
         </div>
         <div>
            <head>Appendix A: Fidäl Table </head>

            <table>
               <head>ETHIOPIAN SYLLABARY (transcription based on the system adopted by the <title
                     rend="italic">Encyclopaedia Aethiopica</title>). Reproduced with permission of
                  Alessandro Bausi.</head>
               <row role="label">
                  <cell>1st order</cell>
                  <cell>2nd order</cell>
                  <cell>3rd order</cell>
                  <cell>4th order</cell>
                  <cell>5th order</cell>
                  <cell>6th order</cell>
                  <cell> 7th order </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ሀ</emph>
                     <emph> ha</emph>
                  </cell>
                  <cell>
                     <emph>ሁ</emph>
                     <emph> hu</emph>
                  </cell>
                  <cell>
                     <emph>ሂ</emph>
                     <emph> hi</emph>
                  </cell>
                  <cell>
                     <emph>ሃ</emph>
                     <emph> ha</emph>
                  </cell>
                  <cell>
                     <emph>ሄ</emph>
                     <emph> he</emph>
                  </cell>
                  <cell>
                     <emph>ህ</emph>
                     <emph> hǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሆ</emph>
                     <emph> ho</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ለ</emph>
                     <emph> lä</emph>
                  </cell>
                  <cell>
                     <emph>ሉ</emph>
                     <emph> lu</emph>
                  </cell>
                  <cell>
                     <emph>ሊ</emph>
                     <emph> li</emph>
                  </cell>
                  <cell>
                     <emph>ላ</emph>
                     <emph> la</emph>
                  </cell>
                  <cell>
                     <emph>ሌ</emph>
                     <emph> le</emph>
                  </cell>
                  <cell>
                     <emph>ል</emph>
                     <emph> lǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሎ</emph>
                     <emph> lo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ሐ</emph>
                     <emph> ḥa</emph>
                  </cell>
                  <cell>
                     <emph>ሑ</emph>
                     <emph> ḥu</emph>
                  </cell>
                  <cell>
                     <emph>ሒ</emph>
                     <emph> ḥi</emph>
                  </cell>
                  <cell>
                     <emph>ሓ</emph>
                     <emph> ḥa</emph>
                  </cell>
                  <cell>
                     <emph>ሔ</emph>
                     <emph> ḥe</emph>
                  </cell>
                  <cell>
                     <emph>ሕ</emph>
                     <emph> ḥǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሖ</emph>
                     <emph> ḥo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>መ</emph>
                     <emph> mä</emph>
                  </cell>
                  <cell>
                     <emph>ሙ</emph>
                     <emph> mu</emph>
                  </cell>
                  <cell>
                     <emph>ሚ</emph>
                     <emph> mi</emph>
                  </cell>
                  <cell>
                     <emph>ማ</emph>
                     <emph> ma</emph>
                  </cell>
                  <cell>
                     <emph>ሜ</emph>
                     <emph> me</emph>
                  </cell>
                  <cell>
                     <emph>ም</emph>
                     <emph> mǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሞ</emph>
                     <emph> mo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ሠ</emph>
                     <emph> śä</emph>
                  </cell>
                  <cell>
                     <emph>ሡ</emph>
                     <emph> śu</emph>
                  </cell>
                  <cell>
                     <emph>ሢ</emph>
                     <emph> śi</emph>
                  </cell>
                  <cell>
                     <emph>ሣ</emph>
                     <emph> śa</emph>
                  </cell>
                  <cell>
                     <emph>ሤ</emph>
                     <emph> śe</emph>
                  </cell>
                  <cell>
                     <emph>ሥ</emph>
                     <emph> śǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሦ</emph>
                     <emph> śo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ረ</emph>
                     <emph> rä</emph>
                  </cell>
                  <cell>
                     <emph>ሩ</emph>
                     <emph> ru</emph>
                  </cell>
                  <cell>
                     <emph>ሪ</emph>
                     <emph> ri</emph>
                  </cell>
                  <cell>
                     <emph>ራ</emph>
                     <emph> ra</emph>
                  </cell>
                  <cell>
                     <emph>ሬ</emph>
                     <emph> re</emph>
                  </cell>
                  <cell>
                     <emph>ር</emph>
                     <emph> rǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሮ</emph>
                     <emph> ro</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ሰ</emph>
                     <emph> sä</emph>
                  </cell>
                  <cell>
                     <emph>ሱ</emph>
                     <emph> su</emph>
                  </cell>
                  <cell>
                     <emph>ሲ</emph>
                     <emph> si</emph>
                  </cell>
                  <cell>
                     <emph>ሳ</emph>
                     <emph> sa</emph>
                  </cell>
                  <cell>
                     <emph>ሴ</emph>
                     <emph> se</emph>
                  </cell>
                  <cell>
                     <emph>ስ</emph>
                     <emph> sǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሶ</emph>
                     <emph> so</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ሸ</emph>
                     <emph> šä</emph>
                  </cell>
                  <cell>
                     <emph>ሹ</emph>
                     <emph> šu</emph>
                  </cell>
                  <cell>
                     <emph>ሺ</emph>
                     <emph> ši</emph>
                  </cell>
                  <cell>
                     <emph>ሻ</emph>
                     <emph> ša</emph>
                  </cell>
                  <cell>
                     <emph>ሼ</emph>
                     <emph> še</emph>
                  </cell>
                  <cell>
                     <emph>ሽ</emph>
                     <emph> šǝ</emph>
                  </cell>
                  <cell>
                     <emph>ሾ</emph>
                     <emph> šo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ቀ</emph>
                     <emph> qä</emph>
                  </cell>
                  <cell>
                     <emph>ቁ</emph>
                     <emph> qu</emph>
                  </cell>
                  <cell>
                     <emph>ቂ</emph>
                     <emph> qi</emph>
                  </cell>
                  <cell>
                     <emph>ቃ</emph>
                     <emph> qa</emph>
                  </cell>
                  <cell>
                     <emph>ቄ</emph>
                     <emph> qe</emph>
                  </cell>
                  <cell>
                     <emph>ቅ</emph>
                     <emph> qǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቆ</emph>
                     <emph> qo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ቐ</emph>
                     <emph> q̱ä</emph>
                  </cell>
                  <cell>
                     <emph>ቑ</emph>
                     <emph> q̱u</emph>
                  </cell>
                  <cell>
                     <emph>ቒ</emph>
                     <emph> q̱i</emph>
                  </cell>
                  <cell>
                     <emph>ቓ</emph>
                     <emph> q̱a</emph>
                  </cell>
                  <cell>
                     <emph>ቔ</emph>
                     <emph> q̱e</emph>
                  </cell>
                  <cell>
                     <emph>ቕ</emph>
                     <emph> q̱ǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቖ</emph>
                     <emph> q̱o</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>በ</emph>
                     <emph> bä</emph>
                  </cell>
                  <cell>
                     <emph>ቡ</emph>
                     <emph> bu</emph>
                  </cell>
                  <cell>
                     <emph>ቢ</emph>
                     <emph> bi</emph>
                  </cell>
                  <cell>
                     <emph>ባ</emph>
                     <emph> ba</emph>
                  </cell>
                  <cell>
                     <emph>ቤ</emph>
                     <emph> be</emph>
                  </cell>
                  <cell>
                     <emph>ብ</emph>
                     <emph> bǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቦ</emph>
                     <emph> bo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ተ</emph>
                     <emph> tä</emph>
                  </cell>
                  <cell>
                     <emph>ቱ</emph>
                     <emph> tu</emph>
                  </cell>
                  <cell>
                     <emph>ቲ</emph>
                     <emph> ti</emph>
                  </cell>
                  <cell>
                     <emph>ታ</emph>
                     <emph> ta</emph>
                  </cell>
                  <cell>
                     <emph>ቴ</emph>
                     <emph> te</emph>
                  </cell>
                  <cell>
                     <emph>ት</emph>
                     <emph> tǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቶ</emph>
                     <emph> to</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ቸ</emph>
                     <emph> čä</emph>
                  </cell>
                  <cell>
                     <emph>ቹ</emph>
                     <emph> ču</emph>
                  </cell>
                  <cell>
                     <emph>ቺ</emph>
                     <emph> či</emph>
                  </cell>
                  <cell>
                     <emph>ቻ</emph>
                     <emph> ča</emph>
                  </cell>
                  <cell>
                     <emph>ቼ</emph>
                     <emph> če</emph>
                  </cell>
                  <cell>
                     <emph>ች</emph>
                     <emph> čǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቾ</emph>
                     <emph> čo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ኀ</emph>
                     <emph> ḫa</emph>
                  </cell>
                  <cell>
                     <emph>ኁ</emph>
                     <emph> ḫu</emph>
                  </cell>
                  <cell>
                     <emph>ኂ</emph>
                     <emph> ḫi</emph>
                  </cell>
                  <cell>
                     <emph>ኃ</emph>
                     <emph> ḫa</emph>
                  </cell>
                  <cell>
                     <emph>ኄ</emph>
                     <emph> ḫe</emph>
                  </cell>
                  <cell>
                     <emph>ኅ</emph>
                     <emph> ḫǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኆ</emph>
                     <emph> ḫo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ነ</emph>
                     <emph> nä</emph>
                  </cell>
                  <cell>
                     <emph>ኑ</emph>
                     <emph> nu</emph>
                  </cell>
                  <cell>
                     <emph>ኒ</emph>
                     <emph> ni</emph>
                  </cell>
                  <cell>
                     <emph>ና</emph>
                     <emph> na</emph>
                  </cell>
                  <cell>
                     <emph>ኔ</emph>
                     <emph> ne</emph>
                  </cell>
                  <cell>
                     <emph>ን</emph>
                     <emph> nǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኖ</emph>
                     <emph> no</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ኘ</emph>
                     <emph> ñä</emph>
                  </cell>
                  <cell>
                     <emph>ኙ</emph>
                     <emph> ñu</emph>
                  </cell>
                  <cell>
                     <emph>ኚ</emph>
                     <emph> ñi</emph>
                  </cell>
                  <cell>
                     <emph>ኛ</emph>
                     <emph> ña</emph>
                  </cell>
                  <cell>
                     <emph>ኜ</emph>
                     <emph> ñe</emph>
                  </cell>
                  <cell>
                     <emph>ኝ</emph>
                     <emph> ñǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኞ</emph>
                     <emph> ño</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>አ</emph>
                     <emph> ʾa </emph>
                  </cell>
                  <cell>
                     <emph>ኡ</emph>
                     <emph> ʾu</emph>
                  </cell>
                  <cell>
                     <emph>ኢ</emph>
                     <emph> ʾi</emph>
                  </cell>
                  <cell>
                     <emph>ኣ</emph>
                     <emph> ʾa</emph>
                  </cell>
                  <cell>
                     <emph>ኤ</emph>
                     <emph> ʾe</emph>
                  </cell>
                  <cell>
                     <emph>እ</emph>
                     <emph> ʾǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኦ</emph>
                     <emph> ʾo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ከ</emph>
                     <emph> kä </emph>
                  </cell>
                  <cell>
                     <emph>ኩ</emph>
                     <emph> ku</emph>
                  </cell>
                  <cell>
                     <emph>ኪ</emph>
                     <emph> ki</emph>
                  </cell>
                  <cell>
                     <emph>ካ</emph>
                     <emph> ka</emph>
                  </cell>
                  <cell>
                     <emph>ኬ</emph>
                     <emph> ke</emph>
                  </cell>
                  <cell>
                     <emph>ክ</emph>
                     <emph> kǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኮ</emph>
                     <emph> ko</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ኸ</emph>
                     <emph> ḵä </emph>
                  </cell>
                  <cell>
                     <emph>ኹ</emph>
                     <emph> ḵu</emph>
                  </cell>
                  <cell>
                     <emph>ኺ</emph>
                     <emph> ḵi</emph>
                  </cell>
                  <cell>
                     <emph>ኻ</emph>
                     <emph> ḵa</emph>
                  </cell>
                  <cell>
                     <emph>ኼ</emph>
                     <emph> ḵe</emph>
                  </cell>
                  <cell>
                     <emph>ኽ</emph>
                     <emph> ḵǝ</emph>
                  </cell>
                  <cell>
                     <emph>ኾ</emph>
                     <emph> ḵo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ወ</emph>
                     <emph> wä</emph>
                  </cell>
                  <cell>
                     <emph>ዉ</emph>
                     <emph> wu</emph>
                  </cell>
                  <cell>
                     <emph>ዊ</emph>
                     <emph> wi</emph>
                  </cell>
                  <cell>
                     <emph>ዋ</emph>
                     <emph> wa</emph>
                  </cell>
                  <cell>
                     <emph>ዌ</emph>
                     <emph> we</emph>
                  </cell>
                  <cell>
                     <emph>ው</emph>
                     <emph> wǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዎ</emph>
                     <emph> wo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ዐ</emph>
                     <emph> ʿa</emph>
                  </cell>
                  <cell>
                     <emph>ዑ</emph>
                     <emph> ʿu</emph>
                  </cell>
                  <cell>
                     <emph>ዒ</emph>
                     <emph> ʿi</emph>
                  </cell>
                  <cell>
                     <emph>ዓ</emph>
                     <emph> ʿa</emph>
                  </cell>
                  <cell>
                     <emph>ዔ</emph>
                     <emph> ʿe</emph>
                  </cell>
                  <cell>
                     <emph>ዕ</emph>
                     <emph> ʿǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዖ</emph>
                     <emph> ʿo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ዘ</emph>
                     <emph> zä</emph>
                  </cell>
                  <cell>
                     <emph>ዙ</emph>
                     <emph> zu</emph>
                  </cell>
                  <cell>
                     <emph>ዚ</emph>
                     <emph> zi</emph>
                  </cell>
                  <cell>
                     <emph>ዛ</emph>
                     <emph> za</emph>
                  </cell>
                  <cell>
                     <emph>ዜ</emph>
                     <emph> ze</emph>
                  </cell>
                  <cell>
                     <emph>ዝ</emph>
                     <emph> zǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዞ</emph>
                     <emph> zo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ዠ</emph>
                     <emph> žä</emph>
                  </cell>
                  <cell>
                     <emph>ዡ</emph>
                     <emph> žu</emph>
                  </cell>
                  <cell>
                     <emph>ዢ</emph>
                     <emph> ži</emph>
                  </cell>
                  <cell>
                     <emph>ዣ</emph>
                     <emph> ža</emph>
                  </cell>
                  <cell>
                     <emph>ዤ</emph>
                     <emph> že</emph>
                  </cell>
                  <cell>
                     <emph>ዥ</emph>
                     <emph> žǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዦ</emph>
                     <emph> žo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>የ</emph>
                     <emph> yä</emph>
                  </cell>
                  <cell>
                     <emph>ዩ</emph>
                     <emph> yu</emph>
                  </cell>
                  <cell>
                     <emph>ዪ</emph>
                     <emph> yi</emph>
                  </cell>
                  <cell>
                     <emph>ያ</emph>
                     <emph> ya</emph>
                  </cell>
                  <cell>
                     <emph>ዬ</emph>
                     <emph> ye</emph>
                  </cell>
                  <cell>
                     <emph>ይ</emph>
                     <emph> yǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዮ</emph>
                     <emph> yo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ደ</emph>
                     <emph> dä</emph>
                  </cell>
                  <cell>
                     <emph>ዱ</emph>
                     <emph> du</emph>
                  </cell>
                  <cell>
                     <emph>ዲ</emph>
                     <emph> di</emph>
                  </cell>
                  <cell>
                     <emph>ዳ</emph>
                     <emph> da</emph>
                  </cell>
                  <cell>
                     <emph>ዴ</emph>
                     <emph> de</emph>
                  </cell>
                  <cell>
                     <emph>ድ</emph>
                     <emph> dǝ</emph>
                  </cell>
                  <cell>
                     <emph>ዶ</emph>
                     <emph> do</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጀ</emph>
                     <emph> ǧä</emph>
                  </cell>
                  <cell>
                     <emph>ጁ</emph>
                     <emph> ǧu</emph>
                  </cell>
                  <cell>
                     <emph>ጂ</emph>
                     <emph> ǧi</emph>
                  </cell>
                  <cell>
                     <emph>ጃ</emph>
                     <emph> ǧa</emph>
                  </cell>
                  <cell>
                     <emph>ጄ</emph>
                     <emph> ǧe</emph>
                  </cell>
                  <cell>
                     <emph>ጅ</emph>
                     <emph> ǧǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጆ</emph>
                     <emph> ǧo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ገ</emph>
                     <emph> gä</emph>
                  </cell>
                  <cell>
                     <emph>ጉ</emph>
                     <emph> gu</emph>
                  </cell>
                  <cell>
                     <emph>ጊ</emph>
                     <emph> gi</emph>
                  </cell>
                  <cell>
                     <emph>ጋ</emph>
                     <emph> ga</emph>
                  </cell>
                  <cell>
                     <emph>ጌ</emph>
                     <emph> ge</emph>
                  </cell>
                  <cell>
                     <emph>ግ</emph>
                     <emph> gǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጎ</emph>
                     <emph> go</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጠ</emph>
                     <emph> ṭä</emph>
                  </cell>
                  <cell>
                     <emph>ጡ</emph>
                     <emph> ṭu</emph>
                  </cell>
                  <cell>
                     <emph>ጢ</emph>
                     <emph> ṭi</emph>
                  </cell>
                  <cell>
                     <emph>ጣ</emph>
                     <emph> ṭa</emph>
                  </cell>
                  <cell>
                     <emph>ጤ</emph>
                     <emph> ṭe</emph>
                  </cell>
                  <cell>
                     <emph>ጥ</emph>
                     <emph> ṭǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጦ</emph>
                     <emph> ṭo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጨ</emph>
                     <emph> č̣ä</emph>
                  </cell>
                  <cell>
                     <emph>ጩ</emph>
                     <emph> č̣u</emph>
                  </cell>
                  <cell>
                     <emph>ጪ</emph>
                     <emph> č̣i</emph>
                  </cell>
                  <cell>
                     <emph>ጫ</emph>
                     <emph> č̣a</emph>
                  </cell>
                  <cell>
                     <emph>ጬ</emph>
                     <emph> č̣e</emph>
                  </cell>
                  <cell>
                     <emph>ጭ</emph>
                     <emph> č̣ǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጮ</emph>
                     <emph> č̣o</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጰ</emph>
                     <emph> ṗä</emph>
                  </cell>
                  <cell>
                     <emph>ጱ</emph>
                     <emph> ṗu</emph>
                  </cell>
                  <cell>
                     <emph>ጲ</emph>
                     <emph> ṗi</emph>
                  </cell>
                  <cell>
                     <emph>ጳ</emph>
                     <emph> ṗa</emph>
                  </cell>
                  <cell>
                     <emph>ጴ</emph>
                     <emph> ṗe</emph>
                  </cell>
                  <cell>
                     <emph>ጵ</emph>
                     <emph> ṗǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጶ</emph>
                     <emph> ṗo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጸ</emph>
                     <emph> ṣä</emph>
                  </cell>
                  <cell>
                     <emph>ጹ</emph>
                     <emph> ṣu</emph>
                  </cell>
                  <cell>
                     <emph>ጺ</emph>
                     <emph> ṣi</emph>
                  </cell>
                  <cell>
                     <emph>ጻ</emph>
                     <emph> ṣa</emph>
                  </cell>
                  <cell>
                     <emph>ጼ</emph>
                     <emph> ṣe</emph>
                  </cell>
                  <cell>
                     <emph>ጽ</emph>
                     <emph> ṣǝ</emph>
                  </cell>
                  <cell>
                     <emph>ጾ</emph>
                     <emph> ṣo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ፀ</emph>
                     <emph> ṣ́ä</emph>
                  </cell>
                  <cell>
                     <emph>ፁ</emph>
                     <emph> ṣ́u</emph>
                  </cell>
                  <cell>
                     <emph>ፂ</emph>
                     <emph> ṣ́i</emph>
                  </cell>
                  <cell>
                     <emph>ፃ</emph>
                     <emph> ṣ́a</emph>
                  </cell>
                  <cell>
                     <emph>ፄ</emph>
                     <emph> ṣ́e</emph>
                  </cell>
                  <cell>
                     <emph>ፅ</emph>
                     <emph> ṣ́ǝ</emph>
                  </cell>
                  <cell>
                     <emph>ፆ</emph>
                     <emph> ṣ́o</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ፈ</emph>
                     <emph> fä</emph>
                  </cell>
                  <cell>
                     <emph>ፉ</emph>
                     <emph> fu</emph>
                  </cell>
                  <cell>
                     <emph>ፊ</emph>
                     <emph> fi</emph>
                  </cell>
                  <cell>
                     <emph>ፋ</emph>
                     <emph> fa</emph>
                  </cell>
                  <cell>
                     <emph>ፌ</emph>
                     <emph> fe</emph>
                  </cell>
                  <cell>
                     <emph>ፍ</emph>
                     <emph> fǝ</emph>
                  </cell>
                  <cell>
                     <emph>ፎ</emph>
                     <emph> fo</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ፐ</emph>
                     <emph> pä</emph>
                  </cell>
                  <cell>
                     <emph>ፑ</emph>
                     <emph> pu</emph>
                  </cell>
                  <cell>
                     <emph>ፒ</emph>
                     <emph> pi</emph>
                  </cell>
                  <cell>
                     <emph>ፓ</emph>
                     <emph> pa</emph>
                  </cell>
                  <cell>
                     <emph>ፔ</emph>
                     <emph> pe</emph>
                  </cell>
                  <cell>
                     <emph>ፕ</emph>
                     <emph> pǝ</emph>
                  </cell>
                  <cell>
                     <emph>ፖ</emph>
                     <emph> po</emph>
                  </cell>
               </row>
               <row role="data">
                  <cell>
                     <emph>ቨ</emph>
                     <emph> vä</emph>
                  </cell>
                  <cell>
                     <emph>ቩ</emph>
                     <emph> vu</emph>
                  </cell>
                  <cell>
                     <emph>ቪ</emph>
                     <emph> vi</emph>
                  </cell>
                  <cell>
                     <emph>ቫ</emph>
                     <emph> va</emph>
                  </cell>
                  <cell>
                     <emph>ቬ</emph>
                     <emph> ve</emph>
                  </cell>
                  <cell>
                     <emph>ቭ</emph>
                     <emph> vǝ</emph>
                  </cell>
                  <cell>
                     <emph>ቮ</emph>
                     <emph> vo</emph>
                  </cell>
               </row>
            </table>


            <table>
               <head>
                  <title rend="italic">Labiovelars</title>
               </head>
               <row role="data">
                  <cell>
                     <emph>ቈ </emph>
                     <emph>q</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ቊ</emph>
                     <emph> q</emph>w<emph>i</emph>
                  </cell>
                  <cell>
                     <emph> ቋ</emph>
                     <emph> q</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ቌ</emph>
                     <emph> q</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ቍ</emph>
                     <emph> q</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
               <row role="data">
                  <cell>
                     <emph>ኈ</emph>
                     <emph> ḫ</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ኊ</emph>
                     <emph> ḫ</emph>w<emph>i</emph>
                  </cell>
                  <cell>
                     <emph>ኋ</emph>
                     <emph> ḫ</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ኌ</emph>
                     <emph> ḫ</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ኍ</emph>
                     <emph> ḫ</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
               <row role="data">
                  <cell>
                     <emph>ኰ</emph>
                     <emph> k</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ኲ</emph>
                     <emph> k</emph>w<emph>i</emph>
                  </cell>
                  <cell>
                     <emph>ኳ</emph>
                     <emph> k</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ኴ</emph>
                     <emph> k</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ኵ</emph>
                     <emph> k</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
               <row role="data">
                  <cell>
                     <emph>ጐ</emph>
                     <emph> g</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ጒ</emph>
                     <emph> g</emph>w<emph>i</emph>
                  </cell>
                  <cell>
                     <emph>ጓ</emph>
                     <emph> g</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ጔ</emph>
                     <emph> g</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ጕ</emph>
                     <emph> g</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
               <row role="data">
                  <cell>
                     <emph>ዀ</emph>
                     <emph> ḵ</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ዂ</emph>
                     <emph> ḵ</emph>w<emph>i</emph>
                  </cell>
                  <cell>
                     <emph>ዃ</emph>
                     <emph> ḵ</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ዄ</emph>
                     <emph> ḵ</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ዅ</emph>
                     <emph> ḵ</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
               <row role="data">
                  <cell>
                     <emph>ቘ</emph>
                     <emph> q̱</emph>w<emph>ä</emph>
                  </cell>
                  <cell/>
                  <cell>
                     <emph>ቚ</emph>
                     <emph> q̱</emph>wi</cell>
                  <cell>
                     <emph>ቛ</emph>
                     <emph> q̱</emph>w<emph>a</emph>
                  </cell>
                  <cell>
                     <emph>ቜ</emph>
                     <emph> q̱</emph>w<emph>e</emph>
                  </cell>
                  <cell>
                     <emph>ቝ</emph>
                     <emph> q̱</emph>w<emph>ǝ</emph>
                  </cell>
                  <cell/>
               </row>
            </table>



            <table>
               <head>
                  <emph>Numerals</emph>
               </head>
               <row role="data">
                  <cell>፩ 1</cell>
                  <cell>፰ 8</cell>
                  <cell>፷ 60</cell>
               </row>
               <row role="data">
                  <cell>፪ 2</cell>
                  <cell>፱ 9</cell>
                  <cell>፸ 70</cell>
               </row>
               <row role="data">
                  <cell>፫ 3</cell>
                  <cell>፲ 10</cell>
                  <cell>፹ 80</cell>
               </row>
               <row role="data">
                  <cell>፬ 4</cell>
                  <cell>፳ 20</cell>
                  <cell>፺ 90</cell>
               </row>
               <row role="data">
                  <cell>፭ 5</cell>
                  <cell>፴ 30</cell>
                  <cell>፻ 100</cell>
               </row>
               <row role="data">
                  <cell>፮ 6</cell>
                  <cell>፵ 40</cell>
                  <cell>፲፻ 1,000</cell>
               </row>
               <row role="data">
                  <cell>፯ 7</cell>
                  <cell>፶ 50</cell>
                  <cell>፻፻ 10,000</cell>
               </row>
            </table>

         </div>
         <div>
            <head>Appendix B: Manuscript Images Consulted for Ground-Truth Data</head>
            <listBibl>
               <bibl label="EMIP 01154"> Ethiopia, Addis Alem, Adbarat Debretsion Mariam Church, MS
                  Addis Alem 112 digitized as EMIP 01154, ff. 30v–31v. <title rend="italic">Täˀamrä
                     Maryam <q>Miracles of Mary</q></title>, 1868–1913. </bibl>
               <bibl label="EAP432/1/42 a"> Ethiopia, East Gojjam, Debre Koreb We Qeraneyo
                  Medhanealem Monastery, MS digitized by Hamburg University as <ref
                     target="https://eap.bl.uk/archive-file/EAP432-1-42">EAP432/1/42</ref>, Images
                  180–181. </bibl>
               <bibl label="EAP432/1/42 b"> Digitization: <title rend="italic">Mengel Tsion [17th
                     century]</title>, British Library, EAP432/1/42, <ref
                     target="https://eap.bl.uk/archive-file/EAP432-1-42"
                     >https://eap.bl.uk/archive-file/EAP432-1-42</ref>. Digitized through the
                  Endangered Archives Programme supported by Arcadia. Accessed 3 Feb 2022.</bibl>
               <bibl label="MS fols. 73r–76v"> Ethiopia, East Gojjam (?), Dabra Wark (?), Debre Werk
                  Saint Mary Church (?), MS fols. 73r–76v. Contents include <title rend="italic"
                     >Täˀamrä Maryam <q>Miracles of Mary</q></title>, 17th century. Microfilmed
                  images held at the Institute of Ethiopian Studies (Addis Ababa), digitized in 2010
                  at the request of the director of the Manuscripts Department. Microfilm among a
                  collection attributed to Donald Davies. Images courtesy of Ethiopic Manuscript
                  Imaging Project (EMIP), Steve Delamarter, Director.</bibl>
               <bibl label="EMML 2275"> Ethiopia, Šawā Province, Darafo Māryām Church, HMML Pr. No.
                     <ref target="https://w3id.org/vhmml/readingRoom/view/204104">EMML 2275</ref>,
                  fols. 157r–161r. Contents including <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>,<hi rend="italic">”</hi> 1508/1535. Not yet
                  fully cataloged. Accessed 3 Feb 2022.</bibl>
               <bibl label="EMML 1978"> Ethiopia, Šawā Province, Qundi Giyorgis Church, HMML Pr. No.
                     <ref target="https://w3id.org/vhmml/readingRoom/view/203808">EMML 1978</ref>,
                  fols. 30v–31v. <title rend="italic">Täˀamrä Maryam <q>Miracles of
                     Mary</q>,</title> 1813. Cataloged by Getatchew Haile and William Macomber;
                  metadata added by Ted Erho. Accessed 3 Feb 2022.</bibl>
               <bibl label="MS C3-IV-229"> Ethiopia, Tǝgray Province, Däbrä Dammo ˀAbunä ˀArägawi,
                  MS C3-IV-229 digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00001852"
                     >EthioSPaRe DD-001</ref>, fols. 85v–86r. Täˀamrä Maryam “Miracles of Mary,”
                  1632–1664. Cataloged by Susanne Hummel, description accessed 2 Feb 2022.</bibl>
               <bibl label="MS C3-IV-258"> Ethiopia, Tǝgray Province, Däbrä Dammo ˀAbunä ˀArägawi,
                  MS C3-IV-258 digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00002300"
                     >EthioSPaRe DD-010</ref>, fols. 56r–68v. Täˀamrä Maryam <q>Miracles of Mary</q>
                  / Täˀamrä Gäbrä Mänfäs Qəddus <title rend="quotes">Miracles of Gäbrä Mänfäs
                     Qəddus,</title> 1772. Cataloged by Susanne Hummel, description accessed 2 Feb
                  2022.</bibl>
               <bibl label="DSAE 00014"> Ethiopia, Tegrāy Province, Dabra Śāhel Agwazā Monastery, MS
                  digitized as HMML Pr. No. <ref
                     target="https://w3id.org/vhmml/readingRoom/view/501278">DSAE 00014</ref>, fols.
                  40v–42v. Täˀamrä Maryam <q>Miracles of Mary,</q> 20th century(?). Digitized by Ewa
                  Balicka-Witakowska and Michael Gervers. Metadata supplied by Ted Erho. Accessed 3
                  Feb 2022.</bibl>
               <bibl label="EthioSPaRe DZ-003"> Ethiopia, Tǝgray Province, Däbrä Zäyt Qәddәst Maryam
                  / DZ, MS digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00000176"
                     >EthioSPaRe DZ-003</ref>, imgs 47–48r, 63, 64. Haymanotä ˀabäw <q>Faith of the
                     Fathers</q> / Täˀamrä Maryam <q>Miracles of Mary,</q> 1550–1650. Cataloged by
                  Stéphane Ancel, description accessed 2 Feb 2022.</bibl>
               <bibl label="C3-IV-163"> Ethiopia, Tegrāy Province, Gunda Gundē Monastery, MS
                  C3-IV-163 digitized as HMML Pr. No. <ref
                     target="https://www.vhmml.org/readingRoom/view/500155">GG 00004</ref>, fols.
                  5r–7r. <title rend="italic">Life of Alexis</title>;<title rend="italic"> Life of
                     Yāsāy, the orthodox king of Rome</title>, 15th–16th century (?). Digitized by
                  Ewa Balicka-Witakowska and Michael Gervers. Cataloged by Ted Erho. Accessed 3 Feb
                  2022. Also accessible at <q>Gunda Gunde Manuscripts</q> Digital Scholarship Unit
                  Islandora site hosted by the University of Toronto Scarborough at <ref
                     target="https://gundagunde.digital.utsc.utoronto.ca/islandora/object/gundagunde%3A3307#page/1/mode/2up"
                     >https://gundagunde.digital.utsc.utoronto.ca/islandora/object/gundagunde%3A3307#page/1/mode/2up</ref>.
                  Accessed 7 April 2022.</bibl>
               <bibl label="C3-IV-154"> Ethiopia, Tegrāy Province, Gunda Gundē Monastery, MS
                  C3-IV-154 digitized as HMML Pr. No. <ref
                     target="https://www.vhmml.org/readingRoom/view/133110">GG 00016</ref>, fols.
                  1r, 8r–15. Homilies and Testaments, 18th century (?). Digitized by Ewa
                  Balicka-Witakowska and Michael Gervers. Cataloged by Ted Erho. Accessed 3 Feb
                  2022. Also accessible at <q>Gunda Gunde Manuscript</q>s Digital Scholarship Unit
                  Islandora site hosted by the University of Toronto Scarborough at <ref
                     target="https://gundagunde.digital.utsc.utoronto.ca/islandora/object/gundagunde%3A12438#page/1/mode/2up"
                     >https://gundagunde.digital.utsc.utoronto.ca/islandora/object/gundagunde%3A12438#page/1/mode/2up</ref>.
                  Accessed 7 April 2022.</bibl>
               <bibl label="EthioSPaRe GBI-011"> Ethiopia, Tegrāy Province, Gwaḥgot ˀIyäsus / GBI,
                  MS digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00002349"
                     >EthioSPaRe GBI-011</ref>, ff. 102r–104v. <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>, 17th century. Cataloged by Susanne Hummel,
                  description accessed 2 Feb 2022.</bibl>
               <bibl label="EthioSPaRe NSM-007"> Ethiopia, Tegrāy Province, Nәḥbi Qәddus Mikaˀel /
                  NSM, MS digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00001878"
                     >EthioSPaRe NSM-007</ref>, ff. 90v–92r. <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>, 1900–1950. Cataloged by Susanne Hummel,
                  description accessed 2 Feb 2022.</bibl>
               <bibl label="EthioSPaRe QSM-017"> Ethiopia, Tegrāy Province, Qärsäbär Qәddus Mikaˀel
                  / QSM, MS digitized as <ref
                     target="https://mycms-vs03.rrz.uni-hamburg.de/domlib/receive/domlib_document_00000339"
                     >EthioSPaRe QSM-017</ref>, ff. 48r–50v. <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>, 1721–1735. Cataloged by Stéphane Ancel,
                  description accessed 2 Feb 2022.</bibl>
               <bibl label="EMIP0761"> Ethiopia, Tegrāy Province, Selassie Chelekot Church, MS 20
                  digitized as EMIP0761, ff. 227v–229r. <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>, 17th century. </bibl>
               <bibl label="EMIP0832, f"> Ethiopia, Tegrāy Province, Selassie Chelekot Church, MS 93
                  digitized as EMIP0832, ff. 98r–98v. <title rend="quotes">A major collection of 318
                        <title rend="italic">Täˀamrä Maryam <q>Miracles of Mary</q></title>,</title>
                  1750–1849. </bibl>
               <bibl label="EMML 2058"> Ethiopia, Wallo Province, Ḥayq Esṭifānos Monastery, HMML Pr.
                  No. <ref target="https://w3id.org/vhmml/readingRoom/view/203887">EMML 2058</ref>,
                  fol. 97rv. Homily on the glory of Mary and <title rend="italic">Täˀamrä Maryam
                        <q>Miracles of Mary</q></title>, 18th century (?). Cataloged by Getatchew
                  Haile and William Macomber; metadata added by Ted Erho. Accessed 3 Feb
                  2022.</bibl>
            </listBibl>
         </div>


      </body>
      <back>
         <listBibl>
            <bibl xml:id="abebe2019" label="Abebe 2019"> Abebe, A. <title rend="italic">Launch of
                  Ethiopic Studies Program at the University of Toronto.</title>
               <title rend="italic">Tadias Magazine</title>, 19 December 2019. Available at: <ref
                  target="http://www.tadias.com/12/29/2015/launch-of-ethiopic-studies-program-at-university-of-toronto/"
                  >http://www.tadias.com/12/29/2015/launch-of-ethiopic-studies-program-at-university-of-toronto/</ref>
            </bibl>
            <bibl xml:id="akbari2019" label="Akbari 2019"> Akbari, S.C. <title rend="italic">Where
                  is Medieval Ethiopia? Mapping Ethiopic Studies within Medieval Studies.</title> In
               B. Keene (ed), <title rend="italic">Toward a Global Middle Ages: Encountering the
                  World through Illuminated Manuscripts</title>, The J. Paul Getty Museum, Los
               Angeles (2019), pp. 80–91. </bibl>
            <bibl xml:id="alrasheed2019" label="Alrasheed et al. 2019"> Alrasheed, N., Rao, P.,, and
               Grieco, V. <title rend="italic">Character Recognition Of Seventeenth-Century Spanish
                  American Notary Records Using Deep Learning.</title>
               <title rend="italic">Digital Humanities Quarterly</title> 15.4 (2021). Available at:
                  <ref target="http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html"
                  >http://www.digitalhumanities.org/dhq/vol/15/4/000581/000581.html</ref>
            </bibl>
            <bibl xml:id="appleyard2005" label="Appleyard 2005"> Appleyard, D. <title rend="italic"
                  >Definite Markers in Modern Ethiopian Semitic Languages.</title> In G. Khan (ed),
                  <title rend="italic">Semitic Studies in Honour of Edward Ullendorff</title>,
               Brill, Leiden (2005), pp. 51–61. DOI: <ref
                  target="https://doi.org/10.1163/9789047415756_007"
               >10.1163/9789047415756_007</ref>.</bibl>
            <bibl xml:id="assefa2020" label="Assefa et al. 2020"> Assefa, D., Delamarter, S., Jost,
               G., Lee, R. and Niccum, C. <title rend="italic">The Textual History of the Ethiopic
                  Old Testament Project (THEOT): Goals and Initial Findings.</title>
               <title rend="italic">Textus</title>, 29 (2020): 80–110. DOI: <ref
                  target="https://doi.org/10.1163/2589255X-02901002"
                  >https://doi.org/10.1163/2589255X-02901002</ref>
            </bibl>
            <bibl xml:id="bausi2005" label="Bausi 2005"> Bausi, A. <title rend="italic">Ancient
                  features of Ancient Ethiopic.</title>
               <title rend="italic">Aethiopica</title>, 8 (2005):149–169. DOI: <ref
                  target="https://doi.org/10.15460/aethiopica.8.1.331"
                  >10.15460/aethiopica.8.1.331</ref>.</bibl>
            <bibl xml:id="bausi2015" label="Bausi 2015"> Bausi, A. ed. <title rend="italic"
                  >Comparative Oriental Manuscript Studies: An Introduction</title>. Tradition,
               Hamburg (2015).</bibl>
            <bibl xml:id="bausi2020" label="Bausi 2020"> Bausi, A. <title rend="italic">Ethiopia and
                  the Christian Ecumene: Cultural Transmission, Translation, and Reception.</title>
               In S. Kelly (ed), <title rend="italic">A Companion to Medieval Ethiopia and
                  Eritrea</title>, Brill, Leiden (2020), pp. 217–251. DOI: <ref
                  target="https://doi.org/10.1163/9789004419582_010"
                  >https://doi.org/10.1163/9789004419582_010</ref>. </bibl>
            <bibl xml:id="delamarter2023" label="Delamarter 2023"> Delamarter, S. <title
                  rend="italic">Relationships, Technology, Money, and Luck: The Back Story of Six
                  Collections Containing Ethiopian Arabic Manuscripts and How They Were
                  Digitized.</title> In A. Butts (ed), <title rend="italic">The Qurʾān and Ethiopia:
                  Context and Reception,</title> proceedings of the conference The Qurʾān and
               Ethiopia, Context and Reception, April 8, 2019, Catholic University of America
               (forthcoming).</bibl>
            <bibl xml:id="demilew2019" label="Demilew and Sekeroglu 2019"> Demilew, F.A. and
               Sekeroglu, B. <title rend="italic">Ancient Geez script recognition using deep
                  learning.</title>
               <title rend="italic">SN Applied Sciences</title>, 1 (2019): Article Number 1315. DOI:
                  <ref target="https://doi.org/10.1007/s42452-019-1340-4"
                  >10.1007/s42452-019-1340-4</ref>.</bibl>
            <bibl xml:id="demissie2011" label="Demissie 2011"> Demissie, F. <title rend="italic"
                  >Developing Optical Character Recognition for Ethiopic Scripts</title>. Masters
               thesis, Dalarna University (2011). Available at:<ref
                  target="http://du.diva-portal.org/smash/record.jsf?pid=diva2%3A519067&amp;dswid=2398"
                  >http://du.diva-portal.org/smash/record.jsf?pid=diva2%3A519067&amp;dswid=2398</ref>
               <ref
                  target="http://du.diva-portal.org/smash/record.jsf?pid=diva2%3A519067&amp;dswid=2398"
                  >http://du.diva-portal.org/smash/record.jsf?pid=diva2%3A519067&amp;dswid=2398</ref>.</bibl>
            <bibl xml:id="derat2020" label="Derat 2020"> Derat, M.-L. <title rend="italic">Before
                  the Solomonids: Crisis, Renaissance and the Emergence of the Zagwe Dynasty
                  (Seventh–Thirteenth Centuries).</title> In S. Kelly (ed), <title rend="italic">A
                  Companion to Medieval Ethiopia and Eritrea</title>, Brill, Leiden (2020), pp.
               31–56. DOI: <ref target="https://doi.org/10.1163/9789004419582_003"
                  >https://doi.org/10.1163/9789004419582_003</ref>.</bibl>
            <bibl xml:id="endalamaw2016" label="Endalamaw 2016"> Endalamaw, S. G. <title
                  rend="italic">Ancient Ethiopic Manuscript Recognition Using Deep Learning
                  Artificial Neural Network</title> (Doctoral dissertation, Addis Ababa University,
               2016).</bibl>
            <bibl xml:id="derillo2019" label="Derillo 2019"> Eyob Derillo. <title rend="italic"
                  >Exhibiting the Maqdala Manuscripts: African Scribes: Manuscript Culture of
                  Ethiopia.</title>
               <title rend="italic">African Research &amp; Documentation</title>, 135 (2019):
               102–116.</bibl>
            <bibl xml:id="graves2018" label="Graves 2018"> Graves, A., Fernández, S., Gomez, F. and
               Schmidhuber, J. <title rend="italic">Connectionist temporal classification: labeling
                  unsegmented sequence data with recurrent neural networks.</title> In W. Cohen and
               A. Moore (eds), Proceedings of the 23rd International Conference on Machine Learning,
               Association for Computing Machinery, New York (2006), pp. 369–376. DOI: <ref
                  target="https://doi.org/10.1145/1143844.1143891"
               >10.1145/1143844.1143891</ref>.</bibl>
            <bibl xml:id="grieggs2021" label="Grieggs et al. 2021"> Grieggs, S., Shen, B., Rauch,
               G., Li, P., Ma, J., Chiang, D., Price, B., Scheirer, W. <title rend="italic"
                  >Measuring human perception to improve handwritten document transcription.</title>
               <title rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence
               </title>44.10 (2021): 6594-6601. DOI: <ref
                  target="https://doi.org/10.1109/TPAMI.2021.3092688"
                  >10.1109/TPAMI.2021.3092688</ref>. </bibl>
            <bibl xml:id="hableselassie1981" label="Hable Selassie 1981"> Hable Selassie, S. <title
                  rend="italic">Bookmaking in Ethiopia</title>. Leiden, Netherlands: Karstens
               Drukkers B. V., 1981.</bibl>
            <bibl xml:id="haile2018" label="Haile 2018"> Gezae Haile. <title rend="italic">The
                  Limits of Traditional Methods of Preserving Ethiopian Gə'əz Manuscripts.</title>
               <title rend="italic">Libri</title>, 68.1 (2018): 33–42. DOI: <ref
                  target="https://doi.org/10.1515/libri-2017-0004"
               >10.1515/libri-2017-0004</ref>.</bibl>
            <bibl xml:id="hawk2019" label="Hawk et al. 2019"> Hawk, B,, Karaisl, A., and White, N.
                  <title rend="italic">Modelling medieval hands: practical OCR for Caroline
                  minuscule.</title>
               <title rend="italic">Digital Humanities Quarterly </title>13.1 (2019). Available at:
                  <ref target="http://www.digitalhumanities.org/dhq/vol/13/1/000412/000412.html"
                  >http://www.digitalhumanities.org/dhq/vol/13/1/000412/000412.html</ref>
            </bibl>
            <bibl xml:id="kahle2017" label="Kahle et al. 2017"> Kahle, P., Colutto, S., Hackl, G.,
               and Mühlberger, G. <title rend="italic">Transkribus - A Service Platform for
                  Transcription, Recognition and Retrieval of Historical Documents.</title>
               <title rend="italic">2017 14th IAPR International Conference on Document Analysis and
                  Recognition (ICDAR)</title>, Vol. 4, IEEE, Kyoto, Japan, 2017: 19–24. DOI: <ref
                  target="https://doi.org/10.1109/ICDAR.2017.307">10.1109/ICDAR.2017.307</ref>
            </bibl>
            <bibl xml:id="kassa2018" label="Kassa and Hagras 2018"> Kassa, D. M. and Hagras, H.
                  <title rend="italic">An Adaptive Segmentation Technique For the Ancient Ethiopian
                  Gə'əz Language Digital Manuscripts.</title>
               <title rend="italic">2018 10th Computer Science and Electronic Engineering Conference
                  (CEEC)</title>, IEEE, University of Essex, United Kingdom, 2018: 83–88. DOI: <ref
                  target="https://doi.org/10.1109/CEEC.2018.8674218"
               >10.1109/CEEC.2018.8674218</ref>.</bibl>
            <bibl xml:id="kelly2020" label="Kelly 2020"> Kelly, S. "Introduction." In S. Kelly (ed),
                  <title rend="italic">A Companion to Medieval Ethiopia and Eritrea</title>, Brill,
               Leiden (2020), pp. 1–30. DOI: <ref target="https://doi.org/10.1163/9789004419582_002"
                  >https://doi.org/10.1163/9789004419582_002</ref>.</bibl>
            <bibl xml:id="kominko2015" label="Kominko 2015"> Kominko, M, ed. <title rend="italic"
                  >From Dust to Digital: Ten years of the Endangered Archives Programme</title>.
               Cambridge, UK: Open Book Publishers, 2015. <ref
                  target="http://dx.doi.org/10.11647/OBP.0052"
                  >http://dx.doi.org/10.11647/OBP.0052</ref>
            </bibl>
            <bibl xml:id="liuzzo2019" label="Liuzzo 2019"> Liuzzo, P. M. <title rend="italic"
                  >Digital Approaches to Ethiopian and Eritrean Studies</title>. Wiesbaden:
               Harrassowitz Verlag, 2019.</bibl>
            <bibl xml:id="loe2005" label="Lor et al. 2005"> Lor, P. J. and Britz, J. "Knowledge
               production from an African perspective: International information flows and
               intellectual property." <title rend="italic">International Information &amp; Library
                  Review</title>, 37.2 (2005): 61–76. DOI: <ref
                  target="https://doi.org/10.1080/10572317.2005.10762667"
                  >10.1080/10572317.2005.10762667</ref>.</bibl>
            <bibl xml:id="loyer2021" label="Loyer 2021"> Loyer, J. <title rend="italic"><ref
                     target="https://mru.arcabc.ca/islandora/object/mru%3A793/datastream/PDF/view"
                     >Collections Are Our Relatives: Disrupting the Singular, White Man’s Joy That
                     Shaped Collections</ref>.</title> In Meagan Browndorf, Erin Pappas, and Anna
               Arays, eds., <title rend="italic">The Collector and the Collected: Decolonizing Area
                  Studies Librarianship, </title>Sacramento, CA: Library Juice Press, 2021. (<ref
                  target="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND
               4.0</ref>)</bibl>
            <bibl xml:id="manzuch2017" label="Manžuch 2017"> Manžuch, Z. "Ethical Issues In
               Digitization Of Cultural Heritage." <title rend="italic">Journal of Contemporary
                  Archival Studies</title>: 4.4 (2017): Available at: <ref
                  target="http://elischolar.library.yale.edu/jcas/vol4/iss2/4"
                  >http://elischolar.library.yale.edu/jcas/vol4/iss2/4</ref>. </bibl>
            <bibl xml:id="mellors2002" label="Mellors et al. 2002"> Mellors, J. and Parsons, A.
                  <title rend="italic">Scribes of South Gondar: Bookmaking in Rural Ethiopia in the
                  Twenty-First Century</title>, London, UK: New Cross Books, 2002. </bibl>
            <bibl xml:id="nosnitsin2020" label="Nosnitsin 2020"> Nosnitsin, D. <title rend="italic"
                  >Christian Manuscript Culture of the Ethiopian-Eritrean Highlands: Some Analytical
                  Insights.</title> In S. Kelly (ed), <title rend="italic">A Companion to Medieval
                  Ethiopia and Eritrea</title>, Brill, Leiden (2020), pp. 282–321. DOI: <ref
                  target="https://doi.org/10.1163/9789004419582_012"
                  >https://doi.org/10.1163/9789004419582_012</ref>.</bibl>
            <bibl xml:id="ondariokemwa2014" label="Ondari-Okemwa 2014"> Ondari-Okemwa, E. <title
                  rend="italic">Ethical Issues and Indigenous Knowledge Production and Use in
                  Sub-Saharan Africa in the 21st Century.</title>
               <title rend="italic">Mediterranean Journal of Social Sciences</title>, 5.23 (2014):
               2389–2396. DOI: <ref target="https://doi.org/10.5901/mjss.2014.v5n23p2389"
                  >10.5901/mjss.2014.v5n23p2389</ref>.</bibl>
            <bibl xml:id="puigcerver2017" label="Puigcerver 2017"> Puigcerver, J. <title
                  rend="italic">Are Multidimensional Recurrent Layers Really Necessary for
                  Handwritten Text Recognition?</title>
               <title rend="italic">2017 14th IAPR International Conference on Document Analysis and
                  Recognition (ICDAR)</title>, Vol. 1, IEEE, Kyoto, Japan, 2017: 67–72. DOI:
               10.1109/ICDAR.2017.20.</bibl>
            <bibl xml:id="putnam2016" label="Putnam 2016"> Putnam, L. <title rend="italic">The
                  Transnational and the Text-Searchable: Digitized Sources and the Shadows They
                  Cast.</title>
               <title rend="italic">The American Historical Review</title>, 121.2 (2016): 377–402.
               DOI: <ref target="https://doi.org/10.1093/ahr/121.2.377"
               >10.1093/ahr/121.2.377</ref>.</bibl>
            <bibl xml:id="shi2016" label="Shi et al. 2016"> Shi, B., Bai, X. and Yao, C. <title
                  rend="italic">An end-to-end trainable neural network for image-based sequence
                  recognition and its application to scene text recognition.</title>
               <title rend="italic">IEEE Transactions on Pattern Analysis and Machine
                  Intelligence</title>, 39.11 (2016): 2298–2304. DOI:
               10.1109/TPAMI.2016.2646371.</bibl>
            <bibl xml:id="srivastava2014" label="Srivastava et al. 2014"> Srivastava, N., Hinton,
               G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. <title rend="italic">Dropout:
                  A Simple Way to Prevent Neural Networks from Overfitting.</title> The Journal of
               Machine Learning Research, 15.1 (2014): 1929–1958. Available at: <ref
                  target="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"
                  >https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</ref>.</bibl>
            <bibl xml:id="stewart2009" label="Stewart 2009"> Stewart, C. <title rend="italic">Yours,
                  Mine, or Theirs? Historical Observations on the Use, Collection and Sharing of
                  Manuscripts in Western Europe and the Christian Orient</title>. Piscataway, NJ,
               USA: Gorgias Press, 2009. DOI: <ref
                  target="https://doi-org.myaccess.library.utoronto.ca/10.31826/9781463216801"
                  >10.31826/9781463216801</ref>.</bibl>
            <bibl xml:id="stewart2017" label="Stewart 2017"> Stewart, C. <title rend="italic">A
                  Brief History of the Ethiopian Manuscript Microfilm Library (EMML).</title> In
               A.C. McCollum (ed), <title rend="italic">Studies in Ethiopian Languages, Literature,
                  and History: Festschrift for Getatchew Haile Presented by his Friends and
                  Colleagues</title>, Harrassowitz Verlag, Wiesbaden (2017), pp. 447–472.</bibl>
            <bibl xml:id="sutherland2021" label="Sutherland and Purcell 2021"> Sutherland, T. and
               Purcell, A. <title rend="italic">A Weapon and A Tool: Decolonizing Descriptoion and
                  Embracing Rediscription as Liberatory Archival Praxis.</title> The International
               Journal of Information, Diversity, &amp; Inclusion, 5.1 (2021). DOI: <ref
                  target="https://jps.library.utoronto.ca/index.php/ijidi/article/view/34669"
                  >10.33137/ijidi.v5i1.346669</ref>.</bibl>
            <bibl xml:id="tadiasstaff2020" label="Tadias Staff 2020">
               <title rend="italic">Tadias Magazine</title>, <title rend="italic">Ethiopic Studies
                  at the University of Toronto Becomes Permanent,</title> 4 November 2020. Available
               at: <ref
                  target="http://www.tadias.com/11/04/2020/ethiopic-studies-at-university-of-toronto-becomes-permanent-update/"
                  >http://www.tadias.com/11/04/2020/ethiopic-studies-at-university-of-toronto-becomes-permanent-update/</ref>.</bibl>
            <bibl xml:id="tomaszewski2015" label="Tomaszewski et al. 2015"> Tomaszewski, J., and
               Gervers, M. <title rend="italic">Technological aspects of the monastic manuscript
                  collection at May Wäyni, Ethiopia.</title> In Kominko, M. ed. <title rend="italic"
                  >From Dust to Digital: Ten years of the Endangered Archives Programme</title>,
               Cambridge, UK: Open Book Publishers, 2015, pp. 89–133. DOI: <ref
                  target="http://dx.doi.org/10.11647/OBP.0052">10.11647/OBP.0052</ref>.</bibl>
            <bibl xml:id="weninger2005" label="Weninger 2005"> Weninger, S. "‘Gə'əz." In S. Uhlig
               and A. Bausi (eds), <title rend="italic">Encyclopaedia Aethiopica</title>, Vol. 2,
               Harrassowitz, Wiesbaden (2005), pp. 732–735.</bibl>
            <bibl xml:id="wick2020" label="Wick et al. 2020"> Wick, C., Reul, C., and Puppe, F.
                  <title rend="italic">Calamari - A High-Performance Tensorflow-based Deep Learning
                  Package for Optical Character Recognition.</title><title rend="italic"> Digital
                  Humanities Quarterly</title> 14.1 (2020). Available at: <ref
                  target="http://www.digitalhumanities.org/dhq/vol/14/2/000451/000451.html"
                  >http://www.digitalhumanities.org/dhq/vol/14/2/000451/000451.html</ref>. </bibl>
            <bibl xml:id="wigington2017" label="Wigington et al. 2017"> Wigington, C., Stewart, S.,
               Davis, B., Barrett, B., Price, B. and Cohen, S. <title rend="italic">Data
                  Augmentation for Recognition of Handwritten Words and Lines Using a CNN-LSTM
                  Network.</title>
               <title rend="italic">2017 14th IAPR International Conference on Document Analysis and
                  Recognition (ICDAR)</title>, Vol. 1, IEEE, Kyoto, Japan, 2017: 639–645. DOI: <ref
                  target="https://doi-org.myaccess.library.utoronto.ca/10.1109/ICDAR.2017.110"
                  >10.1109/ICDAR.2017.110</ref>. </bibl>
            <bibl xml:id="winslow2015" label="Winslow 2015"> Winslow, S.M. <title rend="italic"
                  >Ethiopian Manuscript Culture: Practices and Contexts</title>. PhD thesis,
               University of Toronto (2015).</bibl>
            <bibl xml:id="woldeyes2020" label="Woldeyes 2020"> Woldeyes, Y. G. <title rend="italic"
                  >‘Holding Living Bodies in Graveyards’: The Violence of Keeping Ethiopian
                  Manuscripts in Western Institutions.</title>
               <title rend="italic">M/C Journal</title>, 23.2 (2020). DOI: <ref
                  target="https://doi.org/10.5204/mcj.1621">10.5204/mcj.1621</ref>.</bibl>
            <bibl xml:id="xiao2020" label="Xiao et al. 2020"> Xiao, S., Peng, L., Yan, R. and Wang,
               S. <title rend="italic">Deep network with pixel-level rectification and robust
                  training for handwriting recognition.</title>
               <title rend="italic">SN Computer Science</title>, 1.3 (2020): 1–-13. DOI: <ref
                  target="https://doi.org/10.1007/s42979-020-00133-y"
                  >10.1007/s42979-020-00133-y</ref>.</bibl>
            <bibl xml:id="yacob2005" label="Yacob 2005"> Yacob, D. <title rend="italic">Ethiopic at
                  the End of the 20th Century.</title>
               <title rend="italic">International Journal of Ethiopian Studies</title>, 2.1/2
               (2005): 121–140.</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
