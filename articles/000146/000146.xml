<?xml version="1.0" encoding="UTF-8"?><?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?><?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!-- Author should supply the title and personal information-->
            <title type="article">Sounding for Meaning: Using Theories of Knowledge Representation
               to Analyze Aural Patterns in Texts</title>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Tanya <dhq:family>Clement</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>University of Texas, Austin</dhq:affiliation>
               <email>tclement@ischool.utexas.edu</email>
               <dhq:bio>
                  <p>Tanya Clement is an Assistant Professor in the School of Information at the
                     University of Texas at Austin. She has a PhD in English Literature and Language
                     and an MFA in fiction. Her primary area of research is the role of scholarly
                     information infrastructure as it impacts academic research libraries and
                     digital collections, research tools and (re)sources in the context of future
                     applications, humanities informatics, and humanities data curation. Her
                     research is informed by theories of knowledge representation, information
                     theory, mark-up theory, social text theory, and theories of information
                     visualization. She has published pieces on digital humanities and digital
                     literacies in several books and on digital scholarly editing, text mining and
                     modernist literature in the <title rend="italic">Journal of the Text Encoding
                        Initiative</title>, <title rend="italic">Literary and Linguistic
                        Computing</title>, <title rend="italic">Texas Studies in Literature and
                        Language</title>, and <title rend="italic">DS/CN</title>. </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>David <dhq:family>Tcheng</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>University of Illinois, Urbana-Champaign</dhq:affiliation>
               <email>davidtcheng@gmail.com,</email>
               <dhq:bio>
                  <p>David Tcheng works as a Research Scientist for the Illinois Informatics
                     Institute (I3) at the University of Illinois at Urbana Champaign (UIUC). David
                     received a BS from Illinois State University and is currently pursuing a Ph.D.
                     in Informatics at UIUC. David is a machine learning (ML) specialist and has
                     applied ML to many difficult real world problems from domains ranging from art
                     to science and involving media types ranging from sound, image, and symbolic
                     sequences. Prior to I3, David worked many years with NCSA and co-founded the
                     Automated Learning Group. Backed by venture funding from I-Ventures, David took
                     a one year hiatus from UIUC to start up a music analysis and recommendation
                     company called One Llama Media, Inc.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Loretta <dhq:family>Auvil</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>University of Illinois, Urbana-Champaign</dhq:affiliation>
               <email>lauvil@illinois.edu</email>
               <dhq:bio>
                  <p>Loretta Auvil works at the Illinois Informatics Institute (I3) at the
                     University of Illinois at Urbana Champaign. She received a MS in Computer
                     Science from Virginia Tech and a BS in Applied Mathematics and Computer Science
                     from Alderson-Broaddus College. She has worked with a diverse set of
                     application drivers to integrate machine learning and information visualization
                     techniques to solve the needs of research partners. She has led software
                     development and research projects for many years. Prior to working for I3, she
                     spent many years at NCSA on machine learning and information visualization
                     projects and several years creating tools for visualizing performance data of
                     parallel computer programs at Rome Laboratory and Oak Ridge National
                     Laboratory. </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Boris <dhq:family>Capitanu</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>University of Illinois, Urbana-Champaign</dhq:affiliation>
               <email>capitanu@ncsa.uiuc.edu</email>
               <dhq:bio>
                  <p>Boris Capitanu is a Research Programmer working in the Illinois Informatics
                     Institute at the University of Illinois at Urbana Champaign. Boris holds a B.S.
                     and M.S. in Computer Science from University of Illinois at Urbana-Champaign.
                     His research interests include data mining, machine learning, and educational
                     technologies. Boris is currently working on the SEASR project creating software
                     platforms for the advancement of scholarly research.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!-- Include a separate <dhq:authorInfo> element for each author -->
               <dhq:author_name>Megan <dhq:family>Monroe</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>University of Maryland, College Park</dhq:affiliation>
               <email>madey.j@gmail.com</email>
               <dhq:bio>
                  <p>Megan Monroe is a Ph.D. student in the Computer Science Department at the
                     University of Maryland. She currently works in the Human-Computer Interaction
                     Lab (HCIL) on Professor Ben Shneiderman's medical visualization team. Her focus
                     is in temporal event search and analysis.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association of Computers and the Humanities</publisher>
            <!-- This information will be completed at publication -->
            <idno type="DHQarticle-id">000146</idno>
            <idno type="volume">007</idno>
            <idno type="issue">1</idno>
            <date when="2013-07-01">1 July 2013</date>
            <dhq:articleType>article</dhq:articleType>
            <availability>
               <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>

         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!-- Authors may include one or more keywords of their choice -->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Each change should include @who and @when as well as a brief note on what was done. -->
         <change when="2013-02-17" who="JHF">Encoded file</change>
      </revisionDesc>
   </teiHeader>

   <text xml:lang="en">
      <front>
         <dhq:abstract>
            <!-- Include a brief abstract of the article -->
            <p>Computational literary analytics that include frequency trends and collocation, topic
               modeling, and network analysis have relied on rapid and large-scale analysis of the
               word or strings of words. This essay shows that there are many other features of
               literary texts by which humanists make meaning other than the word, such as prosody
               and sound, and how computational methods allow us to do what has historically been a
               more difficult method of analysis — trying to understand how literary texts make
               meaning with these features. This paper will discuss a case study that uses theories
               of knowledge representation and research on phonetic and prosodic symbolism to
               develop analytics and visualizations that help readers discover aural and prosodic
               patterns in literary texts. To this end, this paper has two parts: (I) We describe
               the theories of knowledge representation and research into phonetic and prosodic
               symbolism that underpin the logics and ontologies of aurality incorporated in our
               project. This basic theory of aurality is reflected in our use of OpenMary, a
               text-to-speech application tool for extracting aural features; in the <q>flow</q> we
               coordinated to pre-process texts in SEASR’s Meandre, a data flow environment; in the
               instance-based predictive modeling procedure that we developed for the project; and
               in <title rend="italic">ProseVis</title>, the reader interface that we created to
               allow readers to discover aural features across literary texts. And (II), we discuss
               readings of several works by Gertrude Stein (the portraits <title rend="quotes"
                  >Matisse</title> and <title rend="quotes">Picasso</title> and the prose poem
                  <title rend="italic">Tender Buttons</title>) that were facilitated by this
               work.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!-- Include a brief teaser, no more than a phrase or a single sentence -->
            <p>Literary meaning from aurality</p>
         </dhq:teaser>
      </front>
      <body>
         <head>Sounding for Meaning: Using Theories of Knowledge Representation to Analyze Aural
            Patterns in Texts</head>
         <div>
            <head>Introduction</head>
            <p>Humanities data, for which cultural institutions such as libraries and museums are
               becoming progressively more responsible, is like all data: increasing exponentially.
               Many scholars have responded to this expanded access by augmenting their fields of
               study with theories and practices that correspond to methodologies that use advanced
               computational analysis. The very popular Digging into Data challenge is a testament
               to the wide array of perspectives and methodologies digital projects can encompass.
               In particular, the first (2009) and second (2011) rounds of awards include projects
               that are using machine learning and visualization to provide new methods of
               discovery. Some analyze image files (<title rend="quotes">Digging into Image Data to
                  Answer Authorship Related Questions</title>) and the word (<title rend="quotes"
                  >Mapping the Republic of Letters</title> and <title rend="quotes">Using Zotero and
                  TAPoR on the Old Bailey Proceedings: Data Mining with Criminal Intent</title>).
               Others provide new methods for discovery with audio files by analyzing <q>large
                  amounts of music information</q> (the <title rend="italic">Structural Analysis of
                  Large Amounts of Music</title> and <title rend="quotes">the Electronic Locator of
                  Vertical Interval Successions (ELVIS)</title> project) and <title rend="quotes"
                  >large scale data analysis of audio -- specifically the spoken word</title> (the
                  <title rend="quotes">Mining a Year of Speech</title> and the <title rend="quotes"
                  >Harvesting Speech Datasets for Linguistic Research on the Web</title> projects).
                  <note> Both projects seek to analyze prosodic elements in audio files and in
                  corresponding text files. In any case, they are studying everyday speech rather
                  than the more crafted language that typically informs literary texts.</note> At
               this time, however, none of these projects is looking at how we can analyze literary
               texts for patterns of prosody and sound; none are looking at the sound of text as it
               contributes to how we make meaning or interpret literature. </p>
            <p>At a time when digital humanities scholars are enthusiastic about <title
                  rend="quotes">Big Data</title> and are also struggling to make ties between theory
               and methodology, this paper discusses theories and research tools that allow scholars
               to analyze sound patterns in large collections of literary texts. For the most part,
               researchers interested in investigating large collections of text are using analytics
               such as frequency trending and collocation, topic modeling, and network analysis that
               ultimately rely on word occurrence. The use case discussed here, which is supported
               by the Andrew W. Mellon Foundation through a grant titled <title rend="quotes">SEASR
                  Services,</title>
               <note> Other collaborators include humanities professors from Stanford, George Mason
                  University, and University of Illinois at Urbana-Champaign. All of the use cases
                  within the project include research on how humanities scholars can use textual
                  analytics and predictive modeling with visualizations to help scholars interpret
                  large digital collections. In particular, we are demonstrating how humanities
                  scholars can use the open-source <ref target="../customXml/item1.xml">SEASR</ref>
                  environment developed by the informatics experts at the University of Illinois for
                  research.</note> seeks to identify other features than the <q>word</q> to analyze
               literary texts — specifically those features that comprise sound including
               parts-of-speech, accent, phoneme, stress, tone, and phrase units. To this end, this
               discussion includes a case study that uses theories of knowledge representation and
               research on phonetic and prosodic symbolism to develop analytics and visualizations
               that help readers of literary texts to negotiate large data sets and interpret aural
               and prosodic patterns in text. </p>
            <p>In this piece, we describe how computational analysis, predictive modeling, and
               visualization facilitated our discovery process in three texts by Gertrude Stein, the
               word portraits <title rend="quotes">Matisse</title> and <title rend="quotes"
                  >Picasso</title> (first published in Alfred Stieglitz’s <title rend="italic"
                  >Camera Work</title> in 1912 and in her collection <title rend="italic">Geography
                  and Plays</title>, 1922) and the prose poem <title rend="italic">Tender
                  Buttons</title> (1914). The following discussion focuses primarily on the
               theories, research, and methodologies that underpin this discovery process. First, we
               discuss the theories of knowledge representation and research into phonetic and
               prosodic symbolism that underpin the logics and ontologies of aurality incorporated
               in this project. This basic theory of aurality is reflected in our use of OpenMary, a
               text-to-speech application tool for extracting aural features; in the <q>flow</q> we
               coordinated to pre-process texts in SEASR’s Meandre,<note>Meandre is explained more
                  fully later in the discussion. In brief, it is an environment that allows for
                  assembling and executing data flows. A data flow is a software application
                  consisting of software components that process data. Processing can include, for
                  example, an application that accesses a data store, one that transforms the data
                  from that store and analyzes it with textual analysis, and one that visualizes the
                  transformed results.</note> a data flow environment; in the instance-based
               predictive modeling procedure that we developed for the project; and in <title
                  rend="italic">ProseVis</title>, the reader interface that we created to allow
               readers to discover aural features across literary texts. Second, this discussion
               addresses new readings of the word portraits <title rend="quotes">Matisse</title> and
                  <title rend="quotes">Picasso</title> and the prose poem <title rend="italic"
                  >Tender Buttons</title> by Gertrude Stein that have been facilitated by these
               modes of inquiry. This article outlines the theoretical underpinnings and the
               technical infrastructure that influenced our process of discovery such that
               humanities scholars may consider the efficacy of analyzing sound in literary texts
               with computational methods. </p>
            <div>
               <head>Knowledge Representation</head>
               <p>Theories of knowledge representation can facilitate our ability to express how we
                  are modeling sound in a computational environment. Before defining what we mean by
                     <q>the logics and ontologies of aurality,</q> however, it is useful to discuss
                  why these definitions are necessary at all. John F. Sowa writes in his seminal
                  book on computational foundations that theories of knowledge representation are
                  particularly useful <cit>
                     <quote rend="inline" source="#sowa2000">for anyone whose job is to analyze
                        knowledge about the real world and map it to a computable form</quote>
                     <ptr target="#sowa2000" loc="xi"/>
                  </cit>. He defines knowledge representation as <cit>
                     <quote rend="inline" source="#sowa2000">the application of logic and ontology
                        to the task of constructing computable models for some domain</quote>
                     <ptr target="#sowa2000" loc="xi"/>
                  </cit>. In other words, theories of knowledge representation are transparent about
                  the fact that computers do not afford representations of <q>truth</q> but rather
                  of how we think about the world in a certain context (the <emph>domain</emph>).
                  For Sowa, <emph>logic</emph> is <q>pure form</q> and <emph>ontology</emph> is <cit>
                     <quote rend="inline" source="#sowa2000">the content that is expressed in that
                        form</quote>
                     <ptr target="#sowa2000" loc="xiii"/>
                  </cit>. When developing projects that include computational analytics but lack
                  logic, <quote rend="inline" source="#sowa2000">knowledge representation is vague,
                     with no criteria for determining whether statements are redundant or
                     contradictory;</quote> similarly, <q>without ontology</q> (or a clear sense of
                  what the content represents), Sowa writes, <cit>
                     <quote rend="inline" source="#sowa2000">the terms and symbols are ill-defined,
                        confused, and confusing</quote>
                     <ptr target="#sowa2000" loc="xii"/>
                  </cit>. Accordingly, if researchers and developers are unclear about
                     <emph>what</emph> we mean and <emph>how</emph> we mean when we seek to
                  represent <q>sound,</q> it is difficult for literary scholars to read or
                  understand the results of any computational analytics we apply to that model.</p>
               <p>In his seminal article, <title rend="quotes">What is Humanities Computing and What
                     is not?</title> John Unsworth completes the very useful exercise of identifying
                  various digital humanities projects that adhere to the aspects of knowledge
                  representation put forth by AI scientists Davis, Shrobe, and Szolovits <ptr
                     target="#davis1993"/>. Namely, the authors claim that knowledge representation <cit>
                     <quote rend="inline" source="#davis1993">can best be understood in terms of
                        five distinct roles it plays</quote>
                     <ptr target="#davis1993"/>
                  </cit>. In the interest of defining and explaining our logic and ontology for this
                  project, we will likewise map the development of our methodology project to these
                  same parameters. Listed below is each of the five roles that knowledge
                  representation plays in a project according to Davis, et al. </p>
               <cit>
                  <quote rend="block" source="#davis1993">
                     <list type="ordered">
                        <item>A knowledge representation is most fundamentally a surrogate, a
                           substitute for the thing itself, used to enable an entity to determine
                           consequences by thinking rather than acting, i.e., by reasoning about the
                           world rather than taking action in it.</item>
                        <item>It is a set of ontological commitments, i.e., an answer to the
                           question: In what terms should I think about the world? </item>
                        <item>It is a fragmentary theory of intelligent reasoning, expressed in
                           terms of three components: (i) the representation's fundamental
                           conception of intelligent reasoning; (ii) the set of inferences the
                           representation sanctions; and (iii) the set of inferences it
                           recommends.</item>
                        <item>It is a medium for pragmatically efficient computation, i.e., the
                           computational environment in which thinking is accomplished. One
                           contribution to this pragmatic efficiency is supplied by the guidance a
                           representation provides for organizing information so as to facilitate
                           making the recommended inferences.</item>
                        <item> It is a medium of human expression, i.e., a language in which we say
                           things about the world.</item>
                     </list>
                  </quote>
                  <ptr target="#davis1993"/>
               </cit>
               <p>After defining the first and second roles of knowledge representation in more
                  detail in the first part of this piece, we aggregate a discussion of the next two
                  aspects in the second part. Finally, part three of this piece includes the final
                  role and a more comprehensive discussion of how all five roles are at play within
                  our specific readings of texts written by Gertrude Stein.</p>
            </div>
         </div>
         <div>
            <head>Defining surrogates and ontologies</head>
            <p>The first role of knowledge representation, as Davis describes it, is <cit>
                  <quote rend="inline" source="#davis1993">most fundamentally a surrogate, a
                     substitute for the thing itself, used to enable an entity to determine
                     consequences by thinking rather than acting, i.e., by reasoning about the world
                     rather than taking action in it</quote>
                  <ptr target="#davis1993"/>
               </cit>. This surrogacy is essential to our understanding of aurality and the ways
               that text operates as a surrogate for sound systems. In this project, we are defining
               aurality as <emph>the pre-speech potential of sound</emph> as it is signified within
               the structure and syntax of text.</p>
            <div>
               <head>Aurality and the Subjective Nature of Sound Surrogacy</head>
               <p>We call this surrogate an <emph>aural</emph> representation in order to emphasize
                  the relationship that the written text bears on how sound contributes to meaning
                  making practices in literary texts. While Walter Ong famously focuses on the
                     <q>orality</q> of text as a testament to the history of oral cultures, Charles
                  Bernstein focuses on the <q>aurality</q> of text, which he calls the <cit>
                     <quote rend="inline" source="#bernstein1998">sounding of the writing</quote>
                     <ptr target="#bernstein1998" loc="13"/>
                  </cit>. Bernstein explains that <q>orality</q> has an <cit>
                     <quote rend="inline" source="#bernstein1998">emphasis on breath, voice, and
                        speech ... <title rend="italic">Aurality precedes orality</title>, just as
                        language precedes speech</quote>
                     <ptr target="#bernstein1998" loc="13"/>
                  </cit>. Bernstein makes further distinctions that frame pre-speech aurality as a
                  perspective that focuses on the poem (the writing) rather than the poet (the
                  performance). Bernstein considers the aurality of text as a kind of music in
                  amorphous shapes of patterns and in grand sweeps. This is not to say that aurality
                  depicts the exactness of traditional metered scansion. In contrast, when we read
                  words, we say them differently depending on our regional dialects, our proficiency
                  with the language, or even the physical differences of our mouths.<note>A
                     wonderful example of the different ways that people can say the phrase <q>than
                        I did</q> can be found as part of the <title rend="quotes">Prosody
                        Datasets</title> (<ref
                        target="https://confluence.cornell.edu/display/prosody/Prosody+Datasets"
                        >https://confluence.cornell.edu/display/prosody/Prosody+Datasets</ref>) from
                     the <title rend="quotes">Harvesting Speech Datasets for Linguistic Research on
                        the Web</title> project.</note> Correspondingly, sound is not represented in
                  text as the representation of one particular utterance or scansion. In fact,
                  textual structures present an imperfect representation of sound that inevitably
                  incorporates the chaos, inexactitude, and confusion of aurality <ptr
                     target="#bernstein1998"/>. By couching this sound surrogate within the context
                  of <emph>aurality</emph> instead of <emph>orality</emph>, we are acknowledging
                  that our sound surrogate gestures towards many potential utterances, to any of
                  many noise-profuse, context-driven, reading performances. </p>
               <p>Ultimately, understanding and defining sonic phenomena is a subjective practice.
                  In a recent special issue of <title rend="italic">differences</title> titled
                     <title rend="quotes">Sound Senses,</title> editors Rey Chow and James
                  Steintrager ask what is the <title rend="quotes">Object of Sound</title> for study
                  and they note the slippery, diffusive nature of sound as <cit>
                     <quote rend="inline" source="#chow2012">something not obviously
                        divisible</quote>
                     <ptr target="#chow2012" loc="2"/>
                  </cit>. <quote rend="inline" source="#chow2012">Objects as sonic phenomena are
                     points of diffusion that in listening we attempt to gather,</quote> they write.
                  Most significant for this discussion, they articulate the work of sonic
                  interpretation as this <quote rend="inline" source="#chow2012">work of
                     gathering</quote> in <cit>
                     <quote rend="inline" source="#chow2012">an effort to unify and make
                        cohere</quote>
                     <ptr target="#chow2012" loc="2"/>
                  </cit>. Likewise, Walter Ong sees the work of interpreting or reading texts as a
                  gathering of sounds: <quote rend="block" source="#ong2002">Written texts all have
                     to be related somehow, directly or indirectly, to the world of sound, the
                     natural habitat of language, to yield their meanings. ‘<q>Reading</q>’ a text
                     means converting it to sound, aloud or in the imagination, syllable-by-syllable
                     in slow reading or sketchily in the rapid reading common to high-technology
                     cultures. <ptr target="#ong2002" loc="8"/>
                  </quote>
               </p>
               <p>While Ong essentializes the relationship between written texts and sound in terms
                  of his study of literacy in oral cultures, Charles Bernstein points back to the
                  difficult work of identifying the osmotic relationship between sound and meaning
                  when interpreting poetry, arguing, <cit>
                     <quote rend="inline" source="#bernstein1998">[t]he relation of sound to meaning
                        is something like the relation of the soul (or mind) to the body. They are
                        aspects of each other, neither prior, neither independent</quote>
                     <ptr target="#bernstein1998" loc="17"/>
                  </cit>. Finally, in writing her word portraits, Gertrude Stein also notes the
                  subjective nature of her own <q>gatherings</q> of sound and how that work
                  influenced her creation of literary texts: <quote rend="inline"
                     source="#stein1988c">I had the habit,</quote> she writes, <cit>
                     <quote rend="inline" source="#stein1988c">of conceiving myself as completely
                        talking and listening, listening was talking and talking was listening and
                        in so doing I conceived what I at the time called the rhythm of anybody’s
                        personality</quote>
                     <ptr target="#stein1988c" loc="174"/>
                  </cit>.</p>
               <p>Interpreting or representing sound is also a subjective practice, however. Dwight
                  Bolinger notes that this subjective work of gathering is a <q>best guess</q> that
                  is based on what can be considered divisible and measurable syntactical units.
                     <quote rend="inline" source="#bolinger1986">In the total absence of all
                     phonological and visual cues,</quote> he writes, <cit>
                     <quote rend="inline" source="#bolinger1986">the psychological tendency to
                        impose an accent is so strong that it will be done as a <q>best guess</q>
                        from the syntax</quote>
                     <ptr target="#bolinger1986" loc="17"/>
                  </cit>. In other words, when we seek to <q>sound out</q> a written word, we make
                     <q>best guesses</q> for those sounds based on the possibilities of sound that
                  are represented by the structural features of a word including parts-of-speech,
                  the position of a word in a phrase (e.g., consecutive verbs or nouns), sentence
                  type (e.g., a declaration or a question), and information structure (e.g., given
                  and inferable information in a dependent clause is frequently de-accented) within
                  its syntactical context. This discussion emphasizes the fact that identifying
                  sound surrogates that represent a literary text’s aurality remains subjective,
                  especially within a computational system like ours that relies on <q>best
                     guesses</q> for gathering syntactical units.</p>
            </div>
            <div>
               <head>An Ontology of Sound in which Sound is a Meaningful Aspect of Literary
                  Texts</head>
               <p>Davis’s second role for knowledge representation is as <cit>
                     <quote rend="inline" source="#davis1993">a set of ontological commitments,
                        i.e., an answer to the question: In what terms should I think about the
                        world? The commitments are in effect a strong pair of glasses that determine
                        what we can see, bringing some part of the world into sharp focus, at the
                        expense of blurring other parts</quote>
                     <ptr target="#davis1993"/>
                  </cit>. In this project, we are committed to an ontology of sound in which sound
                  is a meaningful aspect of literary texts. </p>
               <p>The debate concerning whether or not sound contributes to how we interpret written
                  texts has a long history.<note>See Shrum and Lowrey 2007, 40-47 for an extensive
                     discussion concerning this history.</note> While French theorist Ferdinand de
                  Saussure argued that the relationship between sound and meaning was essentially
                  arbitrary (1916), Socrates previously argued there was a significant (i.e.
                     <emph>signifying</emph>) relationship there <ptr target="#ong2002" loc="78"/>.
                  Roland Barthes identified two aspects of sound that contribute to meaning: the
                     <emph>pheno-song</emph>, which maps to our concern with aurality in this
                  project, and refers to <quote rend="inline" source="#barthes1978">all the
                     phenomena, all the features which belong to the structure of the language being
                     sung, the rules of the genre, the coded form of the melisma, the composer’s
                     idiolect, the style of the interpretation: in short everything in the
                     performance which is in the service of communication, representation,
                     expression</quote>; and the <emph>geno-song</emph>, which is the <cit>
                     <quote rend="inline" source="#barthes1978">volume of the singing and speaking
                        voice, the space where significations germinate</quote>
                     <ptr target="#barthes1978" loc="182"/>
                  </cit>. </p>
               <p>In order to create aural surrogates with computational modeling we include textual
                  features of sound that research has shown correspond to how we interpret texts.
                  Reuven Tsur, for example, works backwards from long held beliefs about specific
                  meanings of sound in poetry to arrive at rules that can support or refute these
                  meanings. We are also interested in those features of text that create
                  possibilities for interpretation. For instance, Tsur notes that a reader’s sense
                  that sounds make meaning is an abstract or impressionistic regard, but Tsur seeks
                  to <quote rend="inline" source="#tsur1992">use phonetic and phonological
                     generalizations in an attempt <emph>to determine the rules</emph> on which
                     certain impressionistic generalizations are founded</quote> based on <cit>
                     <quote rend="inline" source="#tsur1992">widespread beliefs concerning the
                           ‘<q>aesthetic</q>’ quality of speech sounds</quote>
                     <ptr target="#tsur1992" loc="64"/>
                  </cit>. In this regard, Tsur attempts to balance <quote rend="inline"
                     source="#tsur1992">two aims</quote>: (1) to legitimate <q>impressions</q> of
                  sound as an <quote rend="inline" source="#tsur1992">integral part of
                     criticism</quote> and (2) to define rules that harken toward <q>scientific
                     impartiality</q> or empirical analysis <ptr target="#tsur1992" loc="64"/>.
                  Where we differ with Tsur is in his attempt to <cit>
                     <quote rend="inline" source="#tsur1992">claim back the largest possible areas
                        of criticism from arbitrary impressionism</quote>
                     <ptr target="#tsur1992" loc="64"/>
                  </cit>. Our emphasis in this discussion, in contrast, is to expose the
                     <emph>partial</emph> nature of determining such rules by exposing how our
                  ontological commitments — those <cit>
                     <quote rend="inline" source="#davis1993">aspects of the world we believe to be
                        relevant</quote>
                     <ptr target="#davis1993"/>
                  </cit> — influence our choices to expose particular sound features. </p>
               <p>In particular, we adhere to the idea that all meaning making is an act of
                  abstraction that is dependent not only on the objects of study (words
                     <emph>or</emph> sound) but also on the context in which we find them (words
                     <emph>and</emph> sounds). Quoting the work of Hrushovski, Tsur claims that
                  there are four kinds of relations between sound and meaning: <cit>
                     <quote rend="inline" source="#tsur1992">(a) Onomatopoeia; (b) Expressive
                        Sounds; (c) Focusing Sound Patterns; (d) Neutral Sound Patterns</quote>
                     <ptr target="#tsur1992" loc="2"/>
                  </cit>. Benjamin Hrushovski describes as <title rend="quotes">Expressive
                     Sounds</title> as <cit>
                     <quote rend="inline" source="#tsur1992">[a] sound combination [that] is grasped
                        as expressive of the tone, mood, or some general quality of meaning</quote>
                     <ptr target="#tsur1992" loc="2"/>
                  </cit>. Most important for Tsur and our project is the comparison that Hrushovski
                  draws between the act of making meaning with sound and the act of making meaning
                  with words: <cit>
                     <quote rend="inline" source="#x_hrushovski1968">an abstraction from the sound
                        pattern (i.e., some kind of tone or <q>quality</q> of the sounds is parallel
                        to an abstraction from the meaning of the words (tone, mood etc.)</quote>
                     <ptr target="#hrushovski1968" loc="444"/>
                  </cit>; <ptr target="#tsur1992" loc="2"/>. The same sounds can make meaning by
                  reflecting different relations, and as a result, the same sounds can evoke
                  different moods. For instance, Tsur cites the example of a line by Poe (<q>And the
                     silken, sad, uncertain rustling, of each purple curtain</q>), which uses
                  onomatopoeia and a quatrain by Shakespeare that begins <q>When to the sessions of
                     sweet, silent thought…</q> that uses expressive sounds. Both of these examples
                  use the sibilants /s/ and /š/ to evoke, respectively, <quote rend="inline"
                     source="#tsur1992">noisy potential</quote> and <cit>
                     <quote rend="inline" source="#tsur1992">hushing potential</quote>
                     <ptr target="#tsur1992" loc="2"/>
                  </cit>. Tsur denies claims that <quote rend="inline" source="#tsur1992">all speech
                     sounds are equal</quote> since <cit>
                     <quote rend="inline" source="#tsur1992">for readers of poetry it is difficult
                        to escape the feeling that some speech sounds are more equal … more musical,
                        more emotional, or more beautiful than others</quote>
                     <ptr target="#tsur1992" loc="53"/>
                  </cit>.</p>
               <p>Sound also contributes to meaning with prosodic and phonetic elements. Prosody has
                  been defined by linguists as comprising intonation, stress, and rhythm to convey
                  linguistic meaning through phrasing and prominence <ptr target="#cole2011"/>.
                  Prosody makes meaning in part by reflecting a speaker’s identity, gender, regional
                  dialect, ethnolect, affect and emotional engagement, and cognitive process.
                  Consequently, prosody can be used to study human behavior, culture, and society
                     <ptr target="#cole2011"/>. In terms of prosody, Dwight Bolinger defines the
                  term <q>intonation</q> to include not only accents and stress but also symbolic
                  meaning since <cit>
                     <quote rend="inline" source="#bolinger1986">it is generally used to refer to
                        the overall landscape, the wider ups and downs that show greater or lesser
                        degrees of excitement, boredom, curiosity, positiveness, etc.</quote>
                     <ptr target="#bolinger1986" loc="11"/>
                  </cit>. Moreover, there is a body of research in linguistics and psychology called
                  phonetic symbolism that dates back to controlled studies done by Edward Sapir
                  (1929) and his student Stanley Newman (1933). In these studies and subsequent
                  ones, links are established between the sounds of vowels and consonants and
                  readers’ perceptions of size (big and small), volume (full or empty), speed (fast
                  and slow), intensity (dull and sharp) and value (pleasant and unpleasant) <ptr
                     target="#shrum2007" loc="40–47"/>. These ideas that the meaning of sound
                  correlates to the structure of the text are significant to how we have chosen to
                  develop an infrastructure for analyzing sound in text. With this project, we are
                  identifying and modeling these potentially meaningful aspects of literary texts
                  for interpretive analysis. </p>
            </div>
         </div>
         <div>
            <head>Intelligent Reasoning and Pragmatically Efficient Computation</head>
            <p>The above theories in aurality and research in phonetic and prosodic symbolism
               undergird the choices we have made in developing a technical, computational
               infrastructure for analyzing the sound of literary texts. Shifting our attention to
               consider two more of Davis’s roles of knowledge representation including knowledge
               representation as a <q>fragmentary theory of intelligent reasoning</q> and as a
                  <q>medium for pragmatically efficient computation,</q> this section will discuss
               three essential parts of the infrastructure that represents the sound of text in our
               project. First we consider our decision to use OpenMary, a text-to-speech application
               tool, that extracts aural features from literary texts; next, we discuss the data
               flow we developed in SEASR’s data flow environment (Meandre) to produce a
               representation of the data for modeling as well as the predictive modeling procedure
               we implemented to analyze patterns across these extracted features; and finally, we
               introduce <title rend="italic">ProseVis</title>, the reader interface we created to
               allow readers to discover and interpret these extracted aural features and patterns
                  <emph>in conversation with</emph> (not a replacements for) the literary texts.</p>
            <div>
               <head>OpenMary</head>
               <p>In this project, we use OpenMary,<note> See <ref target="http://mary.dfki.de/"
                        >http://mary.dfki.de/</ref>.</note> an open source, text-to-speech system,
                  to create a text-based surrogate of sound. Developed as a collaborative project of
                  Das Deutsche Forschungszentrum für Künstliche Intelligenz (German Research Center
                  for Artificial Intelligence) Language Technology Lab and the Institute of
                  Phonetics at Saarland University, OpenMary captures information about the
                  structure of the text that make it possible for a computer to read text in
                  multiple languages (German, British and American English, Telugu, Turkish, and
                  Russian; more languages are in preparation) and create spoken text. We chose
                  OpenMary as a useful analytic routine for analyzing these texts after first
                  parsing our texts against the CMU (Carnegie Mellon University) Pronouncing
                  Dictionary and then validating sections of the Mary XML Output against
                  human-parsed sections. In a simple comparison based on analyzing Gertrude Stein’s
                  novel <title rend="italic">The Making of Americans</title>, we noticed that many
                     <q>unknown</q> words were returned in the CMU comparison. That is, many of the
                  words that Stein used in her lexicon, though common words, were returned as
                     <q>unknown</q> words in the results (such as <q>insensibility,</q>
                  <q>meekness,</q>
                  <q>well-meaning,</q> and <q>slinks</q>). OpenMary’s recommendation, on the other
                  hand, incorporates a <q>best guess</q> model in any given prosodic situation —
                  that is, it is based on an algorithm or a set of stringent rules that draws on the
                  kind of research that Tsur, Bolinger and others have mapped for how we make
                  meaning with sound, which includes part-of-speech, accent, phoneme, stress, tone,
                  the position of a word in a phrase (e.g., consecutive verbs or multiple nouns),
                  sentence type (e.g., a declaration or a question), and information structure
                  (e.g., given and inferable information in a dependent clause is frequently
                  de-accented) <ptr target="#becker2006"/>. </p>
               <p>The documentation explains OpenMary’s system for Natural Language Processing
                  (NLP): <cit>
                     <quote rend="block" source="#mary">In a first NLP step, part of speech
                        labelling [sic] and shallow parsing (chunking) is performed. Then, a lexicon
                        lookup is performed in the pronounciation [sic] lexicon; unknown tokens are
                        morphologically decomposed and phonemised by grapheme to phoneme (letter to
                        sound) rules. Independently from the lexicon lookup, symbols for the
                        intonation and phrase structure are assigned by rule, using punctuation,
                        part of speech info, and the local syntactic info provided by the chunker.
                        Finally, postlexical phonological rules are applied, modifying the phone
                        symbols and/or the intonation symbols as a function of their
                        context.</quote>
                     <ptr target="#mary"/>
                  </cit>
               </p>
               <p>Further intelligent reasoning is reflected in OpenMary’s folksonomic technique for
                  representing words that are not in the CMU Pronouncing dictionary lexicon; this
                  technique involves generating a lexicon of known pronunciations from the most
                  common words in Wikipedia and allowing developers to enter new words manually
                     (<q>Adding support for a new language to MARY TTS</q>). OpenMary will make a
                     <q>best guess</q> at words that are not part of the CMU lexicon because its
                  rule set or algorithm — its <soCalled>intelligent reasoning</soCalled> — for how
                  it generates audio files is based on the research of both linguists and computer
                  scientists. As such, this highly technical description speaks to the deeply
                  interdisciplinary work that has formed the rules by which OpenMary represents the
                  sound of literary texts in a digital file — as an interface between
                  human-perceived rules for reading and methods for machine processing. </p>
               <p>As a byproduct of this process, OpenMary outputs a representation of the sound of
                  text in XML that reflects a set of possibilities for speech that are important
                  indicators of how the text could potentially be read aloud by a reader.
                  Specifically, OpenMary accepts text input and creates an XML document (MaryXML) as
                  output with attributes like those shown in Figure 1. This example represents the
                  phrase <q>A kind in glass and a cousin, a spectacle and nothing strange</q> from
                  Gertrude Stein’s text <title rend="italic">Tender Buttons</title>.</p>
               <figure>
                  <graphic url="resources/images/figure01.png"/>
                  <figDesc>Figure 1: A sample from the OpenMary XML output shows the phrase <q>A
                        kind in glass and a cousin, a spectacle nothing strange,</q> from Gertrude
                     Stein’s Tender Buttons</figDesc>
               </figure>
               <p>As shown above sentences (&lt;s&gt;) are broken into prosodic units and then
                  phrases (&lt;prosody&gt; and &lt;phrase&gt;), which are, in turn, broken into
                  words or tokens (&lt;t&gt;). These word elements hold the attributes that mark
                     <q>accent</q>, part of speech (<q>pos</q>), and <q>ph</q> — phonetic spellings
                  (transcribed in SAMPA),) broken into what we refer to as <q>sounds</q> separated
                  by <q>–</q>, with an apostrophe (<q> ' </q>) preceding stressed syllables. Other
                  information is included at the phrase level such as <q>tone</q> and
                     <q>breakindex.</q>
                  <note>More information on these elements may be needed: in <q>accent</q> the
                        <q>L</q> indicates a low pitch and <q>H</q> indicates a high pitch; a
                     character followed by * gives the pitch of the stressed syllable; pitches
                     preceding and following the stressed pitch are separated from the stressed
                     pitch by <q>+</q> and <q>!</q> represents a downstep onto the following pitch;
                        <q>g2p_method</q> indicates how the phonetics were found; <q>breakindex</q>
                     indicates the type of boundary: <q>2</q> = a potential boundary location;
                        <q>3</q> = an intermediate phrase break; <q>4</q> = an intra-sentential
                     phrase break; <q>5</q>= a sentence-final boundary; <q>6</q> = a paragraph-final
                     boundary; <q>tone</q> indicates the tone ending the previous phrase</note>
               </p>
            </div>
            <div>
               <head>Meandre Data Flow Environment</head>
               <p>The SEASR (Software Environment for the Advancement of Scholarly Research) team at
                  the University of Illinois at Urbana-Champaign has been working on creating a
                  computational environment in which users who are interested in analyzing large
                  data sets can develop data flows that push these data sets through various textual
                  analytics and visualizations.<note>For an example, see <ptr target="#clement2008"
                     />.</note> This environment, called Meandre, provides tools for assembling and
                  executing data flows. A data flow is a software application consisting of software
                  components that process data. Processing can include, for example, an application
                  that accesses a data store, one that transforms the data from that store and
                  analyzes it with textual analysis, and one that visualizes the transformed
                  results. Within Meandre, each flow is represented as a graph that shows components
                  as icons linked through their input and output connections (see Figure 2). Based
                  on the inputs and properties of a component, an output is generated upon
                  execution. Meandre provides basic infrastructure for data-intensive computation by
                  providing tools for creating, linking, and executing components and flows. As
                  such, Meandre facilitates a user’s ability to choose how her information will be
                  organized and ultimately the kinds of inferences that can be made from the
                  resulting data. </p>
               <figure>
                  <graphic url="resources/images/figure02.png"/>
                  <figDesc>A sample Meandre dataflow for processing a document through the OpenMary
                     System</figDesc>
               </figure>
               <p>The ability to explore a text’s aurality was not represented within SEASR until we
                  added a Meandre component to use OpenMary (shown as the green box module in Figure
                  2). Meandre components were used to segment the book into smaller chunks of text
                  before passing it to OpenMary for feature extraction, because sending large
                  amounts of text to OpenMary created memory problems associated with processing the
                  complete document. Consequently, the flow processes each document in our
                  collection through the OpenMary web service at a paragraph level. Meandre is also
                  used to create a tabular representation of the data (see Figure 3). The features
                  represented from the MaryXML are part of speech, accent, phoneme, stress, tone,
                  and break index, because research shows that these features have a significant
                  impact on how we make meaning with sound.<note>Please see <ptr
                        target="#plamondon2006"/>; <ptr target="#sapir1929"/>; <ptr
                        target="#saussure1916"/>; <ptr target="#shrum2007"/>; <ptr
                        target="#smolinsky2006"/> on phonetic symbolism; <ptr target="#bolinger1986"
                     /> and <ptr target="#cole2011"/> on prosody and intonation; and <ptr
                        target="#tsur1992"/> on speech perception.</note> We also include
                  information that is useful in terms of framing the context of the sounds within
                  the document’s structure (chapter id, section id, paragraph id, sentence id,
                  phrase id, and word id). This allows words to be associated with accent, phoneme,
                  and part-of-speech within the context of the phrase, sentence and paragraph
                  boundary. Figure 2 shows the flow with the components that are used for executing
                  OpenMary and for post-processing the data to create the database tables. Green
                  components are for computing (i.e. the OpenMary processing component), the blue
                  components are transformation components (i.e. XSL transformation), the red
                  components are input components (i.e. loading the xml file), the dark gray
                  component is an output component (i.e. writing a file) and the yellow component
                  are control flow components, (i.e. forking - duplicating an output). Another
                  benefit of creating this flow in Meandre is that readers who wish to analyze these
                  results or who wish to produce data for their documents will have access to the
                  same flow<note>This flow and ProseVis are available for trial at
                     http://tclement.ischool.utexas.edu/ProseVis/.</note>. </p>
               <figure>
                  <graphic url="resources/images/figure03.png"/>
                  <figDesc>A sample of the tabular output of <title rend="italic">Tender
                        Buttons</title> by Gertrude Stein created within Meandre</figDesc>
               </figure>
               <p>Once the features for aurality were extracted for a collection of documents, we
                  wanted to compare the aurality between the documents and identify the documents
                  that had similar prosody patterns. This comparison was framed as a predictive
                  problem, where we used the features from one document to predict similar
                  documents. We developed an instance-based, machine-learning algorithm for the
                  predictive analysis that can be broken into the following steps: </p>
               <list type="ordered">
                  <item>
                     <label>Defining a prediction problem</label>:  <p>Our hypothesis is that
                        several books in our collection have similar prosody patterns and should
                           <q>sound</q> more alike. </p>
                  </item>
                  <item>
                     <label>Defining examples for machine learning</label>: <p>Figure 4 shows the
                        process we follow to create <q>examples</q> for machine learning, starting
                        with the OpenMary output, and transformation to a database table in Meandre.
                        Next, we use our predictive analysis algorithms to derive a <q>symbol</q>
                        from the OpenMary output at the sound level (i.e., each row of the tabular
                        data). This symbol is an id that represents a unique combination of
                           <emph>just</emph> those features we associate with prosody including part
                        of speech, accent, stress, tone, and break index. There are over six
                        thousand symbols because there are over six thousand combinations of these
                        attribute values. Once symbols are defined, we create a moving window — a
                        phrase window — across the sounds to create the examples we use for
                        comparison. We define the window size of this phrase window to be the
                        average phrase length produced by the OpenMary analysis. We select the
                        average length of a phrase in the data set, not in order to maximize
                        classification accuracy, but in order to best simulate how readers perceive
                        sound at the phrase level (Soderstrom, et. al). Shorter or longer phrase
                        windows are possible and window size does affect accuracy — these choices,
                        again, reflect the <q>intelligent reasoning</q> and <q>pragmatically
                           efficient computation</q> aspects of knowledge representation that Davis,
                        et. al have identified.</p>
                     <p>For our collection, the size of the phrase window is fourteen so the set of
                        input features are the fourteen symbol ids for the given phrase. In total,
                        there are 1,434,588 phrase windows of fourteen symbols from nine books.
                        Finally, we added the <q>class</q> attribute, which is an id for the book in
                        which the phrase window exists. The class attribute (the book) is the
                        attribute that we predict.</p>
                  </item>
                  <item>
                     <label>Modeling</label>: <p>For the predictive analysis we use an
                        instance-based approach, which is based on learning algorithms that
                        systemically compare new problem instances with training instances. In this
                        project, we use a full, leave-one-out cross validation. That is, for each
                        prediction, the phrase window is compared to each phrase window from all
                        other books.<note> A simple approach works well if all books were of the
                           same size resulting in the same number of phrase windows, but they are
                           not.  If we do not modify the instance-based algorithm, it will tend to
                           predict the class of the largest book. To address this problem, we
                           normalize the predicted probabilities under the assumption that all
                           classes are equally likely.</note> The prediction is the probability that
                        the phrase window is in each class (or book), so the probability
                        distribution over all classes sums to 1.0 (as seen by the row values of the
                        bottom table in Figure 4). In our collection, there are nine class
                        attributes, one for each book.</p>
                     <p>To predict which book a given phrase window exists, this phrase window is
                        compared to all other phrase windows from all other books by computing a
                        distance function.<note>Our distance function is simply a count of the
                           number of mismatches between the windows, which ranges from 0 to 14. The
                           function that computes example weights on the basis of distance is:
                           weight = 1 / (distance p) where p is the <q>distance weighting power</q>.
                           When p is set to a maximum, only the single nearest neighbor is used to
                           make predictions and when p is set to a minimum, all memorized examples
                           contributed equally.  For any given problem there is a <q>sweet spot</q>
                           where the highest accuracy is achieved. This optimal parameter setting is
                           different for each variation of the problem and is also affected by the
                           number of training examples.</note> In order to build the best prosody
                        model, one must systematically optimize the control parameters of the
                        machine-learning algorithm to maximize accuracy. There are over one million
                        phrase windows that need to be compared with each other requiring
                        twenty-eight trillion window comparisons. This amount of computation needs
                        to be done for each bias parameter setting considered during bias
                        optimization. Each bias is a new experiment to run with different parameters
                        so adding a book is a new parameter and changing the phrase window from 14
                        to 15 is a new bias.<note>Doing the trillions of comparisons per bias point
                           is too much work to be quickly solved using a single CPU. Fortunately, we
                           now have access to highly parallel computing system with GPUs (Graphical
                           Processing Units). The fastest GPU used in this project is a GTX 580,
                           which has 512 cores (tiny CPUs), all of which are simultaneously used to
                           solve the problem. Using GPUs has made it possible for us to approach a
                           problem of this scale. To make these comparisons efficiently, we
                           implemented the instance-based algorithm on a GPU using NVIDIA’s CUDA
                           framework. Using the GTX 580 with 512 GPU cores simultaneously, the
                           analysis only took 47 minutes.</note>
                     </p>
                  </item>
               </list>
               <figure>
                  <graphic url="resources/images/figure04.png"/>
                  <figDesc>Process of data transformation for predictive modeling.</figDesc>
               </figure>
               <p>We describe these extensive processes to show that intelligent reasoning and
                  pragmatically efficient computation require extensive amounts of processing power.
                  As such, these are not experiments that can be run on a home computer. Changing
                  the way we analyze text (moving away from the grapheme and towards the phoneme) is
                  complicated by the need to collaborate across disciplinary realms (such as an
                  English Department or School of Information collaborating with a Supercomputing
                  Center and Visualization Lab). Further, the results produced by these processes
                  comprise another set of large amounts of data that must be made comprehensible to
                  readers or scholars interested in analyzing sound patterns in text.</p>
            </div>
            <div>
               <head>ProseVis</head>
               <p>An essential aspect of this project is ProseVis, a visualization tool we developed
                  to allow a reader to map the features extracted from OpenMary and the predictive
                  classification data to the words in the contexts to which readers are
                     familiar.<note>ProseVis has been developed in a two-stage process, first as
                     VerseVis: Visualizing Spoken Language Features in Text by graduate students
                     Christine Lu, Leslie Milton, and Austin Myers as part of a graduate course in
                     visualization with Ben Shneiderman at the University of Maryland, College Park.
                     Megan Monroe further developed the prototype as ProseVis under the auspices of
                     this grant.</note> We developed this project with the ultimate goal of
                  facilitating a reader’s ability to analyze sonic features of text and research has
                  shown that mapping the data to the text in its original form allows for the kind
                  of reading that literary scholars engage: they read words and features of language
                  situated within the contexts of phrases, sentences, lines, stanzas, and paragraphs
                     <ptr target="#clement2008"/>. Recreating the contexts of the word not only
                  allows for the simultaneous consideration of multiple representations of knowledge
                  or readings (since every reader’s perspective on the context will be different)
                  but it also allows for a more transparent view of the underlying data. If a reader
                  can see the data (such as sounds and parts of speech) within the contexts of the
                  text with which they are familiar and well-versed, then the reader is empowered
                  within this familiar context to read what might otherwise be an unfamiliar,
                  tabular representation of the text. </p>
               <p>Using the data produced by Meandre, ProseVis highlights features of a text. Figure
                  5, for example, shows two short prose pieces by Gertrude Stein called <q>word
                     portraits</q> and titled <title rend="quotes">Matisse</title> and <title
                     rend="quotes">Picasso.</title> Stein’s word portraits were writing projects in
                  which character development progresses without narrative, much like still-life
                  portraits of a person that also <quote rend="inline" source="#x_stein1988c">do not
                     tell a story</quote>
                  <ptr target="#stein1988c" loc="184"/>. Rather, portraits provide a telling
                  snapshot in time. Stein draws the comparison to portraits because her attempt to
                  create written portraits was much like what she considered a painter’s ought to be
                  — to create <quote rend="inline" source="#stein1990">a picture that exists for and
                     in itself</quote> using <quote rend="inline" source="#stein1990">objects
                     landscapes and people</quote> without being <cit>
                     <quote rend="inline" source="#stein1990">deeply conscious of these things as a
                        subject</quote>
                     <ptr target="#stein1990" loc="497"/>
                  </cit>. In this first ProseVis example, we see Stein’s portraits <title
                     rend="quotes">Matisse</title> and <title rend="quotes">Picasso</title> rendered
                  as a series of rows with colored blocks. Because ProseVis maintains a list of
                  unique occurrences of each attribute extracted by OpenMary, the reader can choose
                  to color the visualization by any of these attributes such as part-of-speech,
                  tone, accent, word, and phonetic sound. (Figure 6 shows the same information,
                  zoomed to illustrate how the words are legible beneath the blocks of color.) The
                  panel on the right is the control panel where the reader can choose how to display
                  the text and prosody features. Options include the ability to show lines by
                  phrases or sentences or chapter or stanza group. In these ways, the reader can
                  examine prosodic patterns as they occur at the beginning or end of these text
                  divisions. </p>
               <figure>
                  <graphic url="resources/images/figure05.png"/>
                  <figDesc>Full sounds in Gertrude Stein’s <title rend="quotes">Matisse</title> and
                        <title rend="quotes">Picasso</title> word portraits visualized in
                     ProseVis</figDesc>
               </figure>
               <figure>
                  <graphic url="resources/images/figure06.png"/>
                  <figDesc>Full sounds in Gertrude Stein’s <title rend="quotes">Matisse</title> and
                        <title rend="quotes">Picasso</title> word portraits visualized in ProseVis,
                     zoomed</figDesc>
               </figure>
               <p>When visualizing the text at the sound level, we encountered three primary issues:
                  (1) The set of unique sounds in a given text is too large to assign each one an
                  easily discernible color in the display; (2) When doing a string-based comparison
                  of one complete sound to another, it is not possible to detect subtler, and
                  potentially critical similarities that form patterns such as alliteration and
                     rhyming.<note>Another display issue we discovered concerns that fact that while
                     each syllable takes approximately the same amount of time to speak, variations
                     in letter count and letter width can render one sound as significantly longer
                     than another in the display. To address the final issue of variable syllable
                     width, we built a display option that normalizes every syllable to a uniform
                     width. Using this option, syllables are displayed without text, using a series
                     of equally sized blocks. This gives the user a more normalized view that does
                     not confuse syllable size in terms of actual character-size on the page with
                     syllable size in terms of timing and cadence.</note> To address the second
                  issue, we broke each syllable down into three primary constituents, and allowed
                  for the display to target each of these constituents individually.  The
                  constituents that we identified as the most informative were the leading consonant
                  sound, the vowel sound, and the ending consonant sound:<note>A list of the
                     consonant and vowel sounds is available at <ref
                        target="http://mary.opendfki.de/wiki/USEnglishSAMPA"
                        >http://mary.opendfki.de/wiki/USEnglishSAMPA</ref>. However, over the course
                     of testing ProseVis, we uncovered two additional vowel components, @U and EI,
                     which are now included in the implementation. Nearly every syllable in the data
                     contains a vowel component. Across the sample Stein files (<title rend="quotes"
                        >Matisse</title>, <title rend="quotes">Picasso</title>, and <title
                        rend="quotes">Miss Furr and Miss Skeene</title>) that we analyzed during
                     testing, we only found four words that contained a syllable that did not
                     include a vowel component. The words were <q>Struggle,</q>
                     <q>Skeene,</q>
                     <q>Stay,</q>
                     <q>Stayed.</q> In all four of these words (which accounted for 67 instances
                     across all files), the leading <q>S</q> is separated into its own sound. The
                     reason for this breakdown is that sounds in the OpenMary data are broken into
                     parts to correspond to how words are spoken, not necessarily according to
                     syllables. Some sounds lack one or more of these components, and the absence of
                     any such component will be assigned a color as well.</note>
               </p>
               <table>
                  <row role="label">
                     <cell>Word</cell>
                     <cell>Sound</cell>
                     <cell>Lead Consonant</cell>
                     <cell>Vowel Sound</cell>
                     <cell>End Consonant</cell>
                  </row>
                  <row>
                     <cell>Strike</cell>
                     <cell>s tr I ke</cell>
                     <cell>S</cell>
                     <cell>AI</cell>
                     <cell>k</cell>
                  </row>
               </table>
               <p>This breakdown provides the reader with a finer-grained level of analysis, as well
                  as a simplified coloring scheme. As a result, if the reader chooses to color the
                  visualization by the sound, they have the additional option of coloring by the
                  full sound, or by a component of sound such as a leading or ending consonant or a
                  vowel sound. Figure 7, Figure 8, and Figure 9 show these alternate views. Further,
                  a reader can render all the words as phonetic spellings (<q>sound</q>),
                  parts-of-speech (<q>POS</q>), or take out the underlying information altogether
                  (to leave just color) instead of text.</p>
               <figure>
                  <graphic url="resources/images/figure07.png"/>
                  <figDesc>Beginning consonant sounds in Gertrude Stein’s <title rend="quotes"
                        >Matisse</title> and <title rend="quotes">Picasso</title> word portraits
                     visualized in ProseVis</figDesc>
               </figure>
               <figure>
                  <graphic url="resources/images/figure08.png"/>
                  <figDesc>Vowel sounds in Gertrude Stein’s <title rend="quotes">Matisse</title> and
                        <title rend="quotes">Picasso</title> word portraits visualized in
                     ProseVis</figDesc>
               </figure>
               <figure>
                  <graphic url="resources/images/figure09.png"/>
                  <figDesc>Ending sounds in Gertrude Stein’s <title rend="quotes">Matisse</title>
                     and <title rend="quotes">Picasso</title> word portraits visualized in
                     ProseVis</figDesc>
               </figure>
               <p>Finally, under the <title rend="quotes">Comparison</title> menu, readers can see
                  the predictive modeling data layered on top of the text. Here, each color
                  represents a different book (listed on the right) and each sound is highlighted
                  according to which book it is most like. When all the books are selected, the
                  color reflects which book has the highest probability or comparison for a given
                     sound.<note> In this example, the <title rend="quotes">Picasso</title> and
                        <title rend="quotes">Matisse</title> portraits are deselected. Otherwise,
                     their <q>colors</q> would override the others. This is explained in further
                     detail below.</note>
               </p>
               <figure>
                  <graphic url="resources/images/figure10.png"/>
                  <figDesc>Comparison data in Gertrude Stein’s <title rend="quotes">Matisse</title>
                     and <title rend="quotes">Picasso</title> word portraits visualized in
                     ProseVis</figDesc>
               </figure>
            </div>
         </div>
         <div>
            <head>Reading the portraits <title rend="quotes">Matisse</title> and <title
                  rend="quotes">Picasso</title> and <title rend="italic">Tender Buttons</title> in
               ProseVis</head>
            <p>As discussed, one of Gertrude Stein’s early modes of experimentation was to create
               word portraits in the modernist mode. At the same time, she sensed an immediate
               connection between the acts of speech (talking and listening) and her work creating
               portraits of people in words. Derrida minimizes the distinction between writing and
               speech or voice (and therefore sound) by showing how both are perceived by the
                  <foreign>différance</foreign> that is signification. In reading Gertrude Stein’s
               work, however, Scott Pound argues that <cit>
                  <quote rend="inline" source="#pound2007">Derrida obscures a distinction between
                     written and spoken language that a discussion of poetics cannot do without.
                     Poststructuralism’s demonstration of the difference writing makes must
                     therefore be set in relation to the difference sound makes</quote>
                  <ptr target="#pound2007" loc="26–27"/>
               </cit>. As discussed in the first part of this essay, in order to investigate the
               difference that sound makes, we are transparent about the fact that a representation
               of sound is subjective. What is most significant for this discussion, therefore, is
               Derrida’s claim that <cit>
                  <quote rend="inline" source="#derrida1991">[i]n order to function, that is, to be
                     readable, a signature must have a repeatable, iterable, imitable form; it must
                     be able to be detached from the present and singular intention of its
                     production</quote>
                  <ptr target="#derrida1991" loc="106"/>
               </cit>. In this project, the form of the <q>signature</q> or sound of text is the
               iterable, repeatable data that is produced by computational analysis.</p>
            <p>Further, we can imagine this imitable form as a layer of data (a reading or another
                  <q>text</q>) that we are using as an overlay on the <q>originary</q> text as a
               means or a lens to read the literary text differently. This <q>new</q> perspective on
               Stein’s texts is not only important for understanding her creative work; it is
               important for reconsidering what we have learned not to consider. For instance, Craig
               Monk argues that Gertrude Stein lost favor with Eugene Jolas, founding editor of
                  <emph>transition</emph>, for political and personal reasons. Yet, the history can
               be and has been read differently: that Jolas preferred James Joyce’s writing to
               Stein’s because Joyce was held up as the revolutionary writer of his time. According
               to Monk, Jolas laid down a gauntlet in 1929 when he published his <title
                  rend="quotes">The Revolution of the Word Proclamation</title> (issue 16/17 of
                  <emph>transition</emph>). This revolution, Jolas writes, requires <q>[t]he
                  literary creator</q> or writer <q>to disintegrate the primal matter of words
                  imposed on him by textbooks and dictionaries</q> (<q>Introduction</q>). Joyce,
               argues Monk, epitomized Jolas’s revolution with his <cit>
                  <quote rend="inline" source="#monk1998">neologisms and portmanteau words</quote>
                  <ptr target="#monk1998" loc="29"/>
               </cit> while Stein’s <q>little household words so dear to Sherwood Anderson never
                  impressed [Jolas]</q> ([unpublished autobiography], 201 qtd. in Monk 32). As a
               result, while Jolas would publish much of Joyce’s work including a serialization of
               his ‘Work in Progress’ (which subsequently became <title rend="italic">Finnegans
                  Wake</title>) as well as essays by prominent authors who wrote about Joyce’s work,
               he only published one more piece of Stein’s after his 1929 manifesto. Finally, in
               1935, Jolas publicly denounced her writing in a <title rend="italic">Testimony
                  Against Gertrude Stein</title> (a supplement to <emph>transition</emph>, July 23).
               Perhaps the most salient observation Monk makes for this discussion is his conclusion
               that <cit>
                  <quote rend="inline" source="#monk1998">it was only as Jola’s preference for the
                     verbal in poetry began to emerge clearly that that the discussion of the visual
                     analogies used often to describe Stein’s works might be read, in hindsight, as
                     implicitly derogatory</quote>
                  <ptr target="#monk1998" loc="30"/>
               </cit>. </p>
            <p>In fact, the idea that James Joyce’s mode of experimentation incorporated elements
               from music while Stein’s works, in contrast, reflected influences from the visual
               arts has been debunked and explored and complicated by too many scholars to rehearse
               again in this space, but the fact remains that as a culture, we are not far removed
               from the situation in which <emph>transition</emph>’s audience found itself: we have
               been summarily prohibited from reading sound patterns by a system of production that
               favors one mode of interpretation over another: the grapheme over the phoneme. Sound
               patterns are difficult to discern. Using computational analysis to mark (to make
               imitable and repeatable and <emph>visual</emph>) expressions that correspond to sound
               is a step in attempting to discern the relationships between all the various features
               of text that contribute to meaning. Stein describes this confluence of features this
               way: <cit>
                  <quote rend="inline" source="#stein1988c">I began to wonder at about this time
                     just what one saw when one looked at anything really looked at anything. Did
                     one see sound, and what was the relation between color and sound, did it make
                     itself by description by a word that meant it or did it make itself by a word
                     in itself</quote>
                  <ptr target="#stein1988c" loc="91"/>
               </cit>. </p>
            <p>Primarily, the last part of this discussion is an exploration, using ProseVis and the
               data from OpenMary and Meandre’s processes in reading sound patterns in Gertrude
               Stein’s portraits <title rend="quotes">Matisse</title> and <title rend="quotes"
                  >Picasso</title> and her prose poem <title rend="italic">Tender Buttons</title>.
               In this exploration, we are concerned above all with Davis’s final role of knowledge
               representation, namely as <cit>
                  <quote rend="inline" source="#davis1993">a medium of human expression, i.e., a
                     language in which we say things about the world</quote>
                  <ptr target="#davis1993"/>
               </cit>. What is at stake in this section is not to create new readings of Stein’s
               texts (this would take much more deliberation and space) as much as it is to
               demonstrate how we have come to analyze literary texts in digital environments as
               visual texts that are divorced, quite often, from attributes of sound. In
               computational environments, productive and critical representations of knowledge
               should show a consideration for the multiplicity of ways that humans express and
               understand themselves through how we say things about the world with literature.
               Considering how to represent and analyze the sound of text in these readings
               represents a step towards pushing computational discovery practices past singular
               representations of the word and, thus, singular modes of interpretation. </p>
            <div>
               <head>
                  <title rend="quotes">Matisse</title> and <title rend="quotes">Picasso</title>
                  (1912)</head>
               <p>The relative success of Stein’s methods for creating the rhythm of a character is
                  evident in the response of scholars. Wendy Salkind argues that with her portraits
                     <title rend="quotes">Matisse</title> and <title rend="quotes">Picasso</title>,
                  Stein expresses a <q>disenchant[ment] with Matisse and his painting</q> and a
                  sustained <q>belief in the genius of Picasso.</q> In particular, Salkind notes the
                  ways in which sounds and rhythms work to create these readings:</p>
               <cit>
                  <quote rend="block" source="#salkind2011">
                     <p>We can <emph>hear </emph>that adulation and disappointment in the phrase
                        repetitions she uses in both pieces. She writes about the effort of creating
                        art, the struggle to be constantly working, to be consistently expressing
                        something, and to find greatness among your followers… When the <title
                           rend="italic">Picasso</title> description above is <emph>spoken
                           aloud</emph>, the repetition of the <q>w</q> sound continuously brings
                        your lips forward, as if in a kiss. The monosyllabic sentence flows and
                        arrives on the emphasis of the <emph>double syllable</emph> resolution of
                        the final word. Although also <emph>monosyllabic</emph>, the <title
                           rend="italic">Matisse</title> description is pedestrian, lacking
                        fluidity. When<emph> spoken</emph>, her words describing him don't feel
                        nearly as good in your mouth.</p>
                  </quote>
                  <dhq:citRef>
                     <ptr target="#salkind2011"/> emphasis added.</dhq:citRef>
               </cit>
               <p>These same patterns are evident in the ProseVis visualization in Figure 7 in which
                  the beginning consonant sound <q>w</q> of words like <q>was,</q>
                  <q>one,</q> and <q>were</q> is represented in red. Clearly, there are fewer
                  concentrated patterns in the <title rend="quotes">Matisse</title> portrait on the
                  left than in the <title rend="quotes">Picasso</title> portrait on the right but
                     <title rend="quotes">Matisse</title> has 283 <q>w</q> sounds out of 2129 total
                  sounds (7.5%), which is actually more than <title rend="quotes">Picasso,</title>
                  which has 271 <q>w</q> sounds out of 1246 sounds (4.5%). The visualization
                  suggests that rather than volume of sounds, Salkind’s reading may have more to do
                  with the close repetition of the <q>w</q> sounds in <title rend="quotes"
                     >Picasso</title> — the successive opening and closing of the lips to make these
                  sounds could mimic kissing more readily than the sporadic lone <q>w</q> sounds
                  used in <title rend="quotes">Matisse.</title> Further, if we color the text
                  according to the <q>accent</q> data (see Figure 11 and Figure 12), we see the dark
                  blue areas that indicate high pitch or accented words that are more prevalent in
                     <title rend="quotes">Picasso</title> than in Matisse. These representations
                  invite us to ask more questions: what is accent doing in the text to contribute to
                  readings like the one Salkind proposes? What is the role of sound and prosody in
                  this text? </p>
               <figure>
                  <graphic url="resources/images/figure11.png"/>
                  <figDesc>Accents highlighted in Gertrude Stein’s <title rend="quotes"
                        >Matisse</title> and <title rend="quotes">Picasso</title> word portraits
                     visualized in ProseVis, vertical tile</figDesc>
               </figure>
               <figure>
                  <graphic url="resources/images/figure12.png"/>
                  <figDesc>Accents highlighted in Gertrude Stein’s <title rend="quotes"
                        >Matisse</title> and <title rend="quotes">Picasso</title> word portraits
                     visualized in ProseVis, horizontal tile, zoomed</figDesc>
               </figure>
               <p>Other comparative patterns are clear as well. In Figure 5, in which full sounds
                  are represented, and each vertical line is a phrase, there is an inversion of
                  patterns between the two pieces. In <title rend="quotes">Picasso,</title> phrases
                  (represented in each line) begin with the yellow/red pattern and evolve into the
                  blue/green pattern at the end of the phrase (or line). The reverse is true of the
                  color sequences in <title rend="quotes">Matisse.</title> A closer look at these
                  patterns in Figure 6 shows that Stein starts phrases about Picasso with specific
                  referents to him such as <q>This one,</q> while phrases about Matisse begin with
                  more general referents such as <q>Some</q> as in <q>Some of a few.</q> Conversely,
                  while the <title rend="quotes">Picasso</title> phrases evolve into expressing an
                  abstract notion of a thing as in <q>something</q> and end again with the specific
                  reference to him again in <q>one,</q> the <title rend="quotes">Matisse</title>
                  phrases start with vague language (<q>Some</q>) and get more specific in the
                  middle of the phrase (referring to <q>he</q>) and ending with vague terms
                  referring to an abstract <q>thing</q>. These patterns or expression are
                  highlighted in this visualization because they are emphasized by certain sounds.
                  In the <title rend="quotes">Picasso</title> phrases, ending sounds are ones
                  created by first opening and then closing your lips such as the <q>o</q> and
                     <q>m</q> and <q>n</q> sounds in <q>some</q> and <q>one</q> — nonetheless with
                  the <q>om</q> sound, your throat remains open. The prevalent Matisse sounds are
                  one would make by beginning with closed lips and ending with widened or more
                  opened lips such as the <q>i</q> and <q>e</q> sounds in <q>thing</q> and <q>he</q>
                  — in this case the reader is closing off the breath, squeezing it with her mouth.
                  One could argue that Stein’s play with sounds shows how she represents these
                  artists: the <title rend="quotes">Picasso</title> sounds are open, contributing to
                  the sense of <q>fluidity</q> upon which Salkind remarks; the <title rend="quotes"
                     >Matisse</title> sounds, on the other hand, shorten the breath and restrict the
                  mouth’s movement into the next sound. The visualizations facilitate our ability to
                  examine how the words in context correspond to these sound patterns. </p>
            </div>
            <div>
               <head>Tender Buttons (1914)</head>
               <p>For our predictive modeling study, we compared the sounds of Gertrude Stein’s
                     <title rend="italic">Tender Buttons</title> to that of <title rend="italic">The
                     New England Cook Book</title>
                  <ptr target="#turner1905"/>. Margueritte S. Murphy hypothesizes that <title
                     rend="italic">Tender Buttons</title>
                  <cit>
                     <quote rend="inline" source="#murphy1991">takes the form of domestic guides to
                        living: cookbooks, housekeeping guides, books of etiquette, guides to
                        entertaining, maxims of interior design, fashion advice</quote>
                     <ptr target="#murphy1991" loc="389"/>
                  </cit>. By writing in this style, Murphy argues, Stein <quote rend="inline"
                     source="#murphy1991">exploits the vocabulary, syntax, rhythms, and cadences of
                     conventional women's prose and talk</quote> to <cit>
                     <quote rend="inline" source="#murphy1991">[explain] her own idiosyncratic
                        domestic arrangement by using and displacing the authoritative discourse of
                        the conventional woman's world</quote>
                     <ptr target="#murphy1991" loc="383–384"/>
                  </cit>. Murphy sites <title rend="italic">The New England Cook Book</title>
                     (<title rend="italic">NECB</title>) as a possible source with which to compare
                  the prosody of <title rend="italic">Tender Buttons</title>: </p>
               <cit>
                  <quote rend="block" source="#murphy1991">Toklas, of course, collected recipes, and
                     she later published two cookbooks, <title rend="italic">The Alice B. Toklas
                        Cookbook</title> (1954) and <title rend="italic">Aromas of Past and
                        Present</title> (1958). Through Toklas then, at least, Stein was familiar
                     with the genre of the cookbook or recipe collection and would appropriately
                        <q>adopt</q> and parody that genre in writing of their growing intimacy.
                     Significantly, Toklas’s name as <q>alas</q> appears repeatedly in <title
                        rend="quotes">Cooking</title> as well as elsewhere in <title rend="italic"
                        >Tender Buttons</title>.</quote>
                  <ptr target="#murphy1991" loc="391"/>
               </cit>
               <p>It is immediately clear from a simple frequency analysis that the word <q>Alas</q>
                  only appears in the one <title rend="quotes">Cooking</title> section in <title
                     rend="italic">Tender Buttons</title>, albeit it appears there thirteen times.
                  In order to analyze whether the texts had similar prosodic elements, however, we
                  attempted to make this comparison evident with predictive modeling.</p>
               <p>To focus the machine learning on this hypothesis, we chose nine texts for
                  comparison and only used features that research has shown reflect prosody such as
                  part of speech, accent, stress, tone, and break index. The nine texts we chose
                  were <title rend="quotes">Picasso</title> (1912), <title rend="quotes"
                     >Matisse</title> (1912), <title rend="italic">Three Lives</title> (1909),
                     <title rend="italic">The Making of Americans</title> (1923), <title
                     rend="italic">Ulysses</title> by James Joyce (based on the pre-1923 print
                  editions), <title rend="italic">The Iliad</title>, translated by Andrew Lang,
                  Walter Leaf, and Ernest Myers (1882), <title rend="italic">The Odyssey</title>,
                  translated by S.H. Butcher and Andrew Lang (1882), and of course, <title
                     rend="italic">Tender Buttons</title> (1914) and <title rend="italic">The New
                     England Cook Book</title> (1905).<note>The texts by Stein were works that were
                     written during the same time period as <title rend="italic">Tender
                        Buttons</title>. The works by Homer and Joyce were chosen based on
                     preliminary work Clement has done comparing the repetition patterns in <title
                        rend="italic">The Making of Americans</title> to these texts <ptr
                        target="#clement2008"/>, <ptr target="#clement2012"/>. The translations of
                     the <title rend="italic">Iliad</title> and the <title rend="italic"
                        >Odyssey</title> represent those editions that scholars have identified in
                     both Joyce <ptr target="#schork1998" loc="122"/> and Stein’s <ptr
                        target="#watson2005"/> libraries.</note> The features we included from the
                  OpenMary data do not include the word or the sound. The break index, which marks
                  the boundaries of syntactic units such as an intermediate phrase break, an
                  intra-sentential phrase break, a sentence-final boundary, and a paragraph-final
                  boundary, is particularly important because readers (and correspondingly the
                  OpenMary system) use phrasal boundaries to determine the rise and fall or emphases
                  of particular words based on their context within the phrase (Soderstrom, et. al).
                  As mentioned, to further bias the system towards the manner in which readers make
                  decisions on sound, we selected a window size that represented the average size of
                  a phrase across the nine texts. We also hypothesized, in order to measure the
                  tool’s efficacy, that<title rend="italic"> The Odyssey</title> and <title
                     rend="italic">The Iliad</title> are most similar.</p>
               <figure>
                  <graphic url="resources/images/figure13.png"/>
                  <figDesc>Shown are the results of a predictive modeling algorithm that was run on
                     nine texts. Each color represents the probability the algorithm predicts one of
                     the nine in its attempt to pick a given text.</figDesc>
               </figure>
               <p>First, we defined a prediction problem for machine learning to solve: Predict from
                  which book the window of prosody features comes. Figure 13 visualizes the results
                  of our predictive analysis. The analysis results show that machine learning makes
                  the same similarity judgment that Murphy had made: <title rend="italic">Tender
                     Buttons</title> and <title rend="italic">NECB</title> sound more similar to
                  each other than they do to any of the other books in the set. In the
                  visualization, row four shows the results for <title rend="italic">Tender
                     Buttons</title> in which the analysis has chosen <title rend="italic"
                     >NECB</title> as the matching text more often (at 14%) than any other book
                  including others by Stein (<title rend="quotes">Picasso</title> is chosen 10% of
                  the time). As well, row two, which shows the results for <title rend="italic"
                     >NECB</title>, shows that <title rend="italic">Tender Buttons</title> is chosen
                  more often (10%) than any other book when trying to predict the actual class or
                  the book itself. Another prediction that shows the algorithm’s success is
                  expressed in rows six and seven, which indicate that the computer confuses <title
                     rend="italic">The Odyssey</title> and <title rend="italic">The Iliad</title> –
                  texts that are known to be very similar in terms of prosody — the highest
                  percentage of times. Interestingly enough, while both the results for the <title
                     rend="italic">Iliad</title> and the results for the <title rend="italic"
                     >Odyssey</title> show a high correlation with the cookbook (14% and 12%
                  respectively), neither the results for <title rend="italic">Tender Buttons</title>
                  or for the cookbook show a high correlation with Homer’s texts. This seems to
                  indicate that the aspects of <title rend="italic">Tender Buttons</title> and the
                  cookbook that make them sound like each other are not those that make the cookbook
                  sound like Homer’s texts. Further, the fact that <title rend="italic">Tender
                     Buttons</title> has very little correlation with texts that are seen as similar
                  to the <title rend="italic">NECB</title>, shows how strongly <title rend="italic"
                     >Tender Buttons</title> is correlated with the other texts in the set.
                  Consequently, its consistent correlation with the cookbook is a much stronger
                  match than might otherwise be indicated.</p>
               <p>Using the ProseVis interface, we can see within the context of the text where
                  these associations have been made. Figure 14 shows <title rend="italic">Tender
                     Buttons</title> and <title rend="italic">NECB</title> in ProseVis. In both
                  panes, each sound is highlighted according to which book it is most like. When all
                  the books are selected, the color of the book that has the highest probability for
                  a given sound is shown. As well, sounds are brighter or less so depending on the
                  level of probability. Figure 15 shows <title rend="italic">Tender Buttons</title>
                  compared to <title rend="quotes">Picasso</title> with only a subset of texts
                     <q>turned on</q> including NECB, <title rend="italic">The Making of
                     Americans</title>, <title rend="quotes">Matisse,</title> and <title
                     rend="italic">Three Lives</title>. In this view, <title rend="italic">Tender
                     Buttons</title> again shows a comparatively higher number <title rend="italic"
                     >NECB</title> (pink) matches than the shorter <title rend="quotes"
                     >Picasso</title> text. In the close-up view in Figure 16, it is easier to see
                  the lighter and darker shades of pink (in the line <q>alas the back shape of
                     mussle</q>) and yellow (in the line <q>alas a dirty third alas a dirty
                     third</q>). The darker shades indicate that the probability that the sound is
                  more like <title rend="italic">Tender Buttons</title> or <title rend="italic"
                     >NECB</title> is greater. </p>
               <figure>
                  <graphic url="resources/images/figure14.png"/>
                  <figDesc>Results of predictive modeling shown in ProseVis for Stein’s <title
                        rend="italic">Tender Buttons</title> and Turner’s NECB </figDesc>
               </figure>
               <note>This figure shows ProseVis opened twice because if <title rend="italic">Tender
                     Buttons</title> and <title rend="italic">NECB </title>were both open in the
                  same view, comparing them would be difficult. For example, by selecting a book
                  like the cook book in which most of the sounds correlate to that book, the
                  presentation is overwhelmed by the colors representing that book. Deselecting
                     <title rend="italic">NECB</title> would mean that it would also be deselected
                  in the <title rend="italic">Tender Buttons</title> view. We plan to address this
                  in future development but in this case it is most efficient to open the tool
                  twice.</note>
               <figure>
                  <graphic url="resources/images/figure15.png"/>
                  <figDesc>Results of predictive modeling shown in ProseVis for Stein’s <title
                        rend="italic">Tender Buttons</title> and <title rend="quotes"
                        >Picasso</title> portrait</figDesc>
               </figure>
               <figure>
                  <graphic url="resources/images/figure16.png"/>
                  <figDesc>The use of the word <q>alas</q> and
                     results of predictive modeling shown in ProseVis for Stein’s <title
                        rend="italic">Tender Buttons</title>
                  </figDesc>
               </figure>
               <p>The story the visualizations tell is two-fold. First, these visualizations are
                  useful in allowing us to test or generate hypotheses about prosody and sound in
                  the texts. Figure 16 shows us that the section surrounding <q>alas</q> is, in
                  fact, more like <title rend="italic">NECB</title> than the other books, supporting
                  Murphy’s hypothesis that the area around <q>alas</q> has the rhythm of <title
                     rend="italic">NECB</title>. At the same time, we can see in Figure 15 that all
                  of the sections of <title rend="italic">Tender Buttons</title> are
                     <emph>not</emph> most like <title rend="italic">NECB</title>. In this figure,
                  the top of the view is colored red, grey, and light blue, indicating that this
                  area is more like <title rend="italic">Three Lives</title>, <title rend="italic"
                     >The Making of Americans</title>, and <title rend="quotes">Matisse</title>
                  respectively. A subsequent research question could concern the nature of this
                  difference. Further, Figure 17 shows two views of <title rend="italic">Tender
                     Buttons</title> visualized in ProseVis. On the left, the same list is divided
                  into two colors showing that half the list is correlated with <title rend="italic"
                     >NECB</title> and half the list is more strongly correlated with <title
                     rend="italic">The Making of Americans</title>. On the right, there is another
                  list with half of the list correlating more strongly to the <title rend="italic"
                     >Odyssey</title> and the bottom half correlating to <title rend="quotes"
                     >Picasso.</title> This visualization immediately engenders questions concerning
                  why the first part of the list is different than the second half when the two
                  halves seem remarkably similar. </p>
               <figure>
                  <graphic url="resources/images/figure17.png"/>
                  <figDesc>Two views of two different sections of <title rend="italic">Tender
                        Buttons</title> showing predictive modeling probabilities visualized in
                     ProseVis</figDesc>
               </figure>
               <p>Second, other questions and hypotheses may be raised concerning how the algorithm
                  and ProseVis work together to generate these visualizations. These latter kinds of
                  questions can be considered in terms of the data sets and the documentation we are
                  providing as well as in respect to articles such as this one. In other words, the
                  goal is not accurate text identification using prosody features, but rather to
                  test hypotheses that consider the sound and prosodic similarities of texts. Part
                  of what we interested in digital humanities are the mistakes we perceive are made
                  by the computer and what these errors reveal about algorithms we are using to
                  gauge the significance of textual features. In other words, one benefit to
                  scholarship represented in this research is determining where the model breaks
                  down and where the ontology must be tweaked. For example, currently, the machine
                  learning system is not being tuned to produce the most accurate classifications:
                  using more context such as a larger window size (i.e., a larger number of phonemes
                  to consider as part of a window) increases the classification accuracy
                     dramatically.<note> We ran experiments with the window sizes of 1, 2, 4, 8, 16,
                     32, and 64. Figure 18 shows the accuracy of each of these experiments. The
                     accuracy is the ability of each window to predict the book it came from for all
                     books. As an example from Figure 13, with a window size of 14, the accuracy of
                     predicting each book varies from 76% for <title rend="quotes">Picasso</title>
                     to 29% for <title rend="italic">Ulysses</title>.</note> As well, if we take
                  parts of speech out of our analysis, our results are less clear. Keeping in mind
                  that we are modeling the possibility of sound <emph>as it could be
                     perceived</emph> opens space for discovery and illumination since what we are
                  not only identifying in this process which text is more like the other (though
                  this is interesting). Rather, by focusing on where the ontology breaks down under
                  the weight of computation, we are learning more about how knowledge
                  representations (our modeling of sound, for instance) are productive for critical
                  inquiry in literary texts.</p>
               <figure>
                  <graphic url="resources/images/figure18.jpeg"/>
                  <figDesc>Relationship between window size and classification accuracy.</figDesc>
               </figure>
            </div>
         </div>
         <div>
            <head>Conclusion</head>
            <p>Previously, digital humanities scholars have also used phonetic symbolic research to
               create tools that mark and analyze sound in poetry. For instance, Marc Plamondon
               created AnalysePoems to analyze the Representative Poetry Online (RPO) website
               (http://rpo.library.utoronto.ca). Plamondon’s goal with AnalysePoems was to <cit><quote
                  rend="inline" source="#plamondon2006">automate the identification of the
                  dominant metrical pattern of a poem and to describe some basic elements of the
                  structure of the poem such as the number of syllables per line and whether all
                  lines are of the same syllabic length or whether there are variations in the
                  syllabic length of the lines in an identifiable pattern</quote>
               <ptr target="#plamondon2006" loc="128"/></cit>. Like our project, AnalysePoems is not a
               tool that attempts to represent the <q>reality</q> of a spoken poem, a feat that is
               impossible because of the ephemeral elements of performance of which a poetry reading
               comprises. Instead, AnalysePoems is “built on the prosodic philosophy that a full
               scansion of a poe<ptr target="#plamondon2006" loc="129"/>. Plamondon’s work has been
               important for the development of the processes described here as it creates a
               precedent and model for analyzing sound from a perspective that also values
               pre-speech (aurality) and phonetic symbolism. </p>
            <p>Another tool that was built to examine how the <quote rend="inline"
                  source="#smolinsky2006">phonetic/phonological structure of a poem may contribute
                  to its meaning and emotional power</quote> is Pattern-Finder <ptr
                  target="#smolinsky2006" loc="339"/>. Smolinksky and Sokoloff’s hypothesis — <cit><quote
                  rend="inline" source="#smolinsky2006">that feature-patterning is the driving
                  force in the <q>music</q> of the poetry</quote>
               <ptr target="#smolinsky2006" loc="340"/></cit> is also important for the creation of our
               visualization tool, ProseVis. With this tool, we are also interested in allowing
               readers to identify patterns in the analyzed texts by facilitating their ability to
               highlight different aspects or <q>features</q> of data such as parts of speech,
               syllables, stress, and groups of vowel and consonant sounds. Like the creators of
               Pattern-Finder, we are also interested in allowing readers to make groupings of
               consonant sounds that include plosives, frications, and affricates and groupings of
               vowels that include those formed in the front or the back of the mouth. Phonetic
               symbolic research and the creation of these tools demonstrate a precedent for
               facilitating readings that use these features to analyze text for meaning.</p>
            <p>Practically speaking, our system for predictive modeling and the ProseVis tool are in
               the early stages of development but we are encouraged with the results so far. We
               predicted that <title rend="italic">Tender Buttons</title> and <title rend="italic"
                  >The New England Cook Book</title> would be most similar and that<title
                  rend="italic"> The Odyssey</title> and <title rend="italic">The Iliad</title>
               should be most similar and these predictions were confirmed on our first attempt, but
               our work in determining whether or not analyzing sound or interpreting with sound in
               these ways is critically productive requires much more research, development, and
               experimentation. Future development plans include collecting more use cases from
               multiple experts and doing cross validation studies before we can have high
               confidence that we have a useful system that compares well to expert predictions.
               Similarly, ProseVis has only been used by a few scholars. Use case studies are needed
               to establish if and how examining sound in this way can be useful to or change the
               nature of scholarship in areas of text and sound. </p>
            <p>At the same time, while usability studies are still a future goal in the project,
               developing the algorithm and the ProseVis interface has already been productive in
               terms of interrogating the efficacy of our underlying theories of aurality. Our work
               is a new and promising approach to comparing texts based on prosody, but what is
               equally promising is that we are ultimately basing our ontology for creating a
               machine learning algorithm on an underlying logic of potential and inexact sounds as
               they are anticipated in text. Further, the <q>success</q> of the comparison of sounds
               between texts is based on the extent to which the computer is <q>confused</q> about
               these possibilities. The fact that this is a <q>best guess</q> methodology, which
               stems from theories in artificial intelligence and knowledge representation and is
               based on potentials and probabilities, suggests that the algorithm and the tool
               incorporate and function within a space that invites hypothesis generation and
               discoveries in the sound of text. </p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl label="MARY TTS" xml:id="mary" key="mary">Adding support for a new language to
               MARY TTS. MARY Text To Speech. Accessed September 8, 2011. <ref
                  target="http://mary.opendfki.de/wiki/NewLanguageSupport"
                  >http://mary.opendfki.de/wiki/NewLanguageSupport</ref>.</bibl>
            <bibl label="Barthes 1978" xml:id="barthes1978" key="barthes1978">Barthes, Roland.
                  <title rend="italic">Image-Music-Text</title>. Hill and Wang, 1978. Print.</bibl>
            <bibl label="Becker et al 2006" xml:id="becker2006" key="becker2006">Becker S., Shröder
               M., and Barry, W. <title rend="quotes">
                  <title rend="quotes">Rule-based Prosody Prediction for German Text-to-Speech
                     Synthesis</title>
               </title> (2006).</bibl>
            <bibl label="Bernstein 1998" xml:id="bernstein1998" key="bernstein1998b">Bernstein, C.
                  <title rend="italic">Close Listening: Poetry and the Performed Word.</title>
               Oxford University Press (1998).</bibl>
            <bibl xml:id="bolinger1986" label="Bolinger 1986" key="bolinger1986">Bolinger, D. <title
                  rend="italic">Intonation and Its Parts: Melody in Spoken English</title>.
               Stanford, Calif: Stanford University Press (1986).</bibl>
            <bibl xml:id="chow2012" label="Chow and Steintrager 2012" key="chow2012">Chow, R. and
               Steintrager, J. In Pursuit of the Object of Sound: An Introduction. <title
                  rend="italic">differences</title> 22.2-3 (2011): 1–9. Accessed April 4, 2012. <ref
                  target="http://differences.dukejournals.org/content/22/2-3/1.full.pdf"
                  >http://differences.dukejournals.org/content/22/2-3/1.full.pdf</ref>
            </bibl>
            <bibl xml:id="clement2008" label="Clement 2008" key="clement2008b">Clement, T. <title
                  rend="quotes">‘<q>A thing not beginning or ending</q>’: Using Digital Tools to
                  Distant-Read Gertrude Stein’s <title rend="italic">The Making of Americans</title>
               </title>. <title rend="italic">Literary and Linguistic Computing</title> 23.3 (2008):
               361-382.</bibl>
            <bibl label="Clement 2012" xml:id="clement2012" key="clement2012">Clement, T. <title
                  rend="quotes">The story of one: the rhetoric of narrative and composition in The
                  Making of Americans by Gertrude Stein.</title>
               <title rend="italic">Texas Studies in Literature and Language</title> 54:3 (2012):
               426-448.</bibl>
            <bibl label="Cole 2011" xml:id="cole2011" key="cole2011">Cole, J. Respondent to Rooth,
               M. and Wagner, M. <title rend="quotes">Harvesting Speech Datasets for Linguistic
                  Research on the Web. Digging Into Data Conference</title>. National Endowment for
               the Humanities. Washington, DC (June 2011).</bibl>
            <bibl label="Davis et al 1993" xml:id="davis1993" key="davis1993">Davis, R., Shrobe R.H.
               and Szolovits, P. <title rend="quotes">What is a Knowledge Representation?</title>
               <title rend="italic">AI Magazine</title>, 14.1 (1993): 17-33. August 31, 2011. <ref
                  target="http://www.medg.lcs.mit.edu/ftp/psz/k-rep.html"
                  >http://www.medg.lcs.mit.edu/ftp/psz/k-rep.html</ref>
            </bibl>
            <bibl label="Derrida 1991" xml:id="derrida1991" key="derrida1991">Derrida, J. <title
                  rend="quotes">Signature, Event, Context.</title> In <title rend="italic">A Derrida
                  Reader: Between the Blinds</title>. Ed. Peggy Kamuf. Hemel Hempstead; Harvester
               Wheatsheaf. 1991.</bibl>
            <bibl label="Drucker 2011" xml:id="drucker2011" key="drucker2011">Drucker, J. Humanities
               Approaches to Graphical Display. <title rend="italic">Digital Humanities
                  Quarterly.</title> 5.1 (Winter 2011). Accessed August 31, 2011. <ref
                  target="http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html"
                  >http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html</ref>.</bibl>
            <bibl label="Flanders 2009" xml:id="flanders2009" key="flanders2009">Flanders, J. <title
                  rend="quotes">The Productive Unease of 21st-century Digital Scholarship.</title>
               <title rend="italic">Digital Humanities Quarterly</title> 3.3 (2009). Accessed August
               31, 2011. <ref target="http://digitalhumanities.org/dhq/vol/3/3/000055/000055.html"
                  >http://digitalhumanities.org/dhq/vol/3/3/000055/000055.html</ref>.</bibl>
            <bibl label="Hrushovski 1968" xml:id="hrushovski1968" key="hrushovski1968">Hrushovski,
               B. <title rend="italic">Poetry</title>. New Haven, Conn: Yale University Press:
               1968.</bibl>
            <bibl label="Jolas 1929" xml:id="jolas1929" key="jolas1929">Jolas, E. <ref
                  target="http://www.davidson.edu/academic/english/Little_Magazines/transition/manifesto.html">
                  <title rend="quotes">Introduction</title>
               </ref>. </bibl>
            <bibl label="McGann 2005" xml:id="mcgann2005" key="mcgann2005">McGann, J. <title
                  rend="quotes">Culture and Technology: The Way We Live Now, What Is to Be
                  Done?</title>
               <title rend="italic">New Literary History</title> 36, 1 (2005): 71-82.</bibl>
            <bibl label="Meyer 2001" xml:id="meyer2001" key="meyer2001">Meyer, S. <title
                  rend="italic">Irresistible Dictation: Gertrude Stein and the Correlations of
                  Writing and Science</title>. Stanford, Calif: Stanford University Press
               (2001).</bibl>
            <bibl label="Monk 1998" xml:id="monk1998" key="monk1998">Monk, Craig. <title
                  rend="quotes">Sound Over Sight: James Joyce and Gertrude Stein in
                  Transition.</title>
               <title rend="italic">Re: Joyce: Text, Culture, Politics</title>. Ed. John Brannigan,
               Geoff Ward, &amp; Julian Wolfreys. Basingstoke, Hampshire: Macmillan, 1998. 17–59.
               Print.</bibl>
            <bibl label="Murphy 1991" xml:id="murphy1991" key="murphy1991">Murphy, M. S. <title
                  rend="quotes">Familiar Strangers</title>: The Household Words of Gertrude Stein's
                  <title rend="quotes">Tender Buttons.</title>
               <title rend="italic">Contemporary Literature</title>. 32, 3 (Autumn, 1991):
               383-402.</bibl>
            <bibl label="Ong 2002" xml:id="ong2002" key="ong2002">Ong, W. J. <title rend="italic"
                  >Orality and Literacy: The Technologizing of the Word</title>. London; New York:
               Routledge (2002).</bibl>
            <bibl label="Peterson 1996" xml:id="peterson1996" key="peterson1996">Peterson, C. L.
                  <title rend="quotes">The Remaking of Americans: Gertrude Stein’s
                  ‘<q>Melanctha</q>’ and African-American Musical Traditions</title>. In H. B.
               Wonham (ed), <title rend="italic">Criticism and the Color Line: Desegregating
                  American Literary Studies. </title>New Brunswick, NJ: Rutgers UP (1996):
               140-157.</bibl>
            <bibl label="Plamondon 2006" xml:id="plamondon2006" key="plamondon2006">Plamondon, M.R.
                  <title rend="quotes">Virtual Verse Analysis: Analysing Patterns in Poetry.</title>
               <title rend="italic">Literary and Linguistic Computing</title> 21 (March 2006):
               127-141. doi:10.1093/llc/fql011.</bibl>
            <bibl label="Pound 2007" xml:id="pound2007" key="pound2007">Pound, Scott. <title
                  rend="quotes">The Difference Sound Makes: Gertrude Stein and the Poetics of
                  Intonation.</title>” <title rend="italic">ESC: English Studies in Canada</title>
               33.4 (2007): 25–35. Web. 30 Nov. 2010.</bibl>
            <bibl label="Salkind 2011" xml:id="salkind2011" key="salkind2011">Salkind, W. <title
                  rend="quotes">Two Portraits: Matisse and Picasso</title>, <title rend="italic"
                  >Gertrude Stein Aloud</title>. January 2011 <ref
                  target="http://www.gertrudesteinaloud.com/matissepicasso.php"
                  >http://www.gertrudesteinaloud.com/matissepicasso.php</ref> Accessed December 15,
               2011. </bibl>
            <bibl label="Sapir 1929" xml:id="sapir1929" key="sapir1929">Sapir, E. <title
                  rend="quotes">A study in phonetic symbolism.</title>
               <title rend="italic">Journal of Experimental Psychology</title>, 12 (1929):
               225–239.</bibl>
            <bibl label="Saussure 1916" xml:id="saussure1916" key="saussure1916">Saussure, F. D.
                  <title rend="italic">Course in general linguistics</title> (W. Baskin, Trans.).
               New York: McGraw-Hill (1916).</bibl>
            <bibl label="Shrum and Lowrey 2007" xml:id="shrum2007" key="shrum2007">Shrum, L. J. and
               Lowrey, T.J. <title rend="quotes">‘<q>Sounds Convey Meaning</q>: The Implications of
                  Phonetic Symbolism for Brand Name Construction</title>. Tina M. Lowrey (ed.),
                  <title rend="italic">Psycholinguistic Phenomena in Marketing
                  Communications</title>. Mahwah, NJ: Lawrence Erlbaum (2007): 39-58.</bibl>
            <bibl label="Smolinsky and Solokoff 2006" xml:id="smolinsky2006" key="smolinsky2006"
               >Smolinsky, S. and Sokoloff, C. <title rend="quotes">Introducing the
                  Pattern-Finder</title>
               <title rend="italic">Conference Abstracts</title>, Digital Humanities Conference,
               Paris (2006). </bibl>
            <bibl label="Soderstrom et al 2003" xml:id="soderstrom2003" key="soderstrom2003"
               >Soderstrom, M. Seidl, Nelson, D.G.K., Jusczyk, P.W. <title rend="quotes">The
                  prosodic bootstrapping of phrases: Evidence from prelinguistic infants</title> ,
               49, 2 (2003): 249-267.</bibl>
            <bibl label="Sowa 2000" xml:id="sowa2000" key="sowa2000">Sowa, J. F. 2000. <title
                  rend="italic">Knowledge Representation: Logical, Philosophical, and Computational
                  Foundations.</title> Pacific Grove, CA: Brooks Cole Publishing Co.</bibl>
            <bibl label="Stein 1988a" xml:id="stein1988a" key="stein1988a">Stein, G. <title
                  rend="quotes">Matisse.</title>
               <title rend="italic">Writings 1903-1932</title>. New York: Library of America (1988):
               278-281.</bibl>
            <bibl label="Stein 1988b" xml:id="stein1988b" key="stein1988b">Stein, G. <title
                  rend="quotes">Picasso.</title>
               <title rend="italic">Writings 1903-1932</title>. New York: Library of America (1988):
               282.</bibl>
            <bibl label="Stein1988c" xml:id="stein1988c" key="stein1988c">Stein, G. <title
                  rend="quotes">Portraits and Repetition.</title>
               <title rend="italic">Lectures in America</title>. London: Virago (1988):
               165-206.</bibl>
            <bibl label="Stein 1990" xml:id="stein1990" key="stein1990b">Stein, G. <title
                  rend="quotes">What are Masterpieces and Why Are There So Few of Them?</title>
               <title rend="italic">The Gender of Modernism</title>. B. K. Scott and M.L. Broe
               (eds). Bloomington: Indiana University Press (1990): 495-501. </bibl>
            <bibl label="Turner 1905" xml:id="turner1905" key="turner1905">Turner, A.M. <title
                  rend="italic">The New England Cook Book: The Latest and the Best Methods for
                  Economy and Luxury at Home</title>. Boston: Chas. E. Brown (1905).</bibl>
            <bibl label="Tsur 1992" xml:id="tsur1992" key="tsur1992">Tsur, Reuven. <title
                  rend="italic">What Makes Sound Patterns Expressive?: the Poetic Mode of Speech
                  Perception</title>. Duke University Press (1992).</bibl>
            <bibl label="Unsworth 2002" xml:id="unsworth2002" key="unsworth2002">Unsworth, J. <title
                  rend="quotes">What is Humanities Computing, and What is Not?</title> In <title
                  rend="italic">Jahrbuch für Computerphilologie</title> 4, Georg Braungart, Karl
               Eibl and Fotis Jannidis, eds. Paderborn: mentis Verlag 2002. <ref
                  target="http://computerphilologie.uni-muenchen.de/jg02/unsworth.html"
                  >http://computerphilologie.uni-muenchen.de/jg02/unsworth.html</ref>. Accessed
               September 8, 2011. </bibl>
            <bibl label="Schork 1998" xml:id="schork1998" key="schork1998">Schork, R. J. <title
                  rend="italic">Greek and Hellenic culture in Joyce</title>. University Press of
               Florida, (1998).</bibl>
            <bibl label="Watson 2005" xml:id="watson2005" key="watson2005">Watson, Dana Cairns.
                  <title rend="italic">Gertrude Stein and the Essence of What Happens</title>. 1st
               ed. Nashville [Tenn.]: Vanderbilt University Press, 2005. Print.</bibl>
         </listBibl>

      </back>
   </text>
</TEI>
