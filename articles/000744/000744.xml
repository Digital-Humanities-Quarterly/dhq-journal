<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title type="article" xml:lang="en">: Deep Learning for Historical Cadastral Map and Satellite Imagery Analysis: Insights from Styria's 
               Franciscean Cadastre</title>
            <dhq:authorInfo>
               <dhq:author_name>Wolfgang Thomas <dhq:family>Göderle</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0002-9417-5316</idno>
               <dhq:affiliation/>
               <email>wolfgang.goederle@uni-graz.at</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Fabian <dhq:family>Rampetsreiter</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation/>
               <email>rampman@gmx.net</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Christian <dhq:family>Macher</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation/>
               <email>cmacher@know-center.at</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Katrin <dhq:family>Mauthner</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation/>
               <email>kmauthner@know-center.at</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <dhq:author_name>Oliver <dhq:family>Pimas</dhq:family>
               </dhq:author_name>
               <idno type="ORCID"></idno>
               <dhq:affiliation/>
               <email>opimas@know-center.at</email>
               <dhq:bio>
                  <p/>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <idno type="DHQarticle-id">000744</idno>
            <idno type="volume"><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006--></idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2--></idno>
            <date><!--include @when with ISO date and also content in the form 23 February 2024--></date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <term corresp="#geospatial"/>
               <term corresp="#archaeology"/>
               <term corresp="#machine_learning"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <list type="simple">
                  <item>map feature extraction</item>
                  <item>remote sensing</item>
                  <item>cadastral map</item>
                  <item>aerial photography</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <change>The version history for this file can be found on <ref target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000744/000744.xml">GitHub
                   </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <p>Cadastres from the 19th century are a complex as well as rich source for historians and archaeologists, the study of which presents 
               great challenges. For archaeological and historical remote sensing, we have trained several Deep Learning models, CNNs, and Vision 
               Transformers to extract large-scale data from this knowledge representation. We present the principle results of our work here and 
               demonstrate our browser-based tool that allows researchers and public stakeholders to quickly identify spots that featured buildings 
               in the 19th century Franciscean cadastre. The tool not only supports scholars and fellow researchers in building a better understanding 
               of the settlement history of the region of <name>Styria</name>; it also helps public administration and fellow citizens to swiftly 
               identify areas of heightened sensibility with regard to the cultural heritage of the region.
            </p>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p/>
         </dhq:teaser>
      </front>
      <body>
        <div>
          <head/>
           <p>Cadastral maps, produced with meticulous quality standards from the early 19th century onwards for large parts of Europe, offer a unique and complex representation of knowledge (W. T. Göderle 2017; W. Göderle 2023; Wheeler 2023; Femenia- Ribera, Mora-Navarro, and Pérez 2022). In the case of Habsburg Central Europe, where cadastral mapping occurred between 1816 and 1861, these maps provide detailed information on individual houses at a scale ranging from 1:720 to 1:5760, with a standard scale of 1:2880. They also document land use, contemporary roads and pathways, land boundaries, and other relevant data (N.N. 1824; Bundesamt für Eich- und Vermessungswesen 2017: 44). Apart from their historical significance, cadastral maps are valuable for contemporary spatial analysis, including environmental and transport history, building history, economic and social history, as well as questions related to land use and landscape transformation in the Anthropocene era, and resource extraction during the time of their creation (Ståhl and Weimann 2022).</p>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure01.jpeg"/>
            </figure>
            <p>They constitute a key resource for historical research into this region, as they enable the spatialization of historical processes, contexts and interdependencies. However, accessing and processing cadastral data pose challenges due to their multimodal structure and primary focus on image and geodata. This presents difficulties for digital humanities, which primarily operate in the textual domain(Champion 2017; Van Noord 2022). We are convinced that recent advances in the field of deep neural networks in particular open up opportunities for multimodal analysis of historical sources (Smits and Wevers 2020). This would open up entirely new opportunities in the field of analyzing those non-written sources that emerged especially (but not only) in Europe in the long 19th century. Sources such as the cadastre are unique and central representations of knowledge from this period, but have so far been insufficiently analyzed, especially in social and economic history research (Hosseini et al. 2021).</p>
            <p>Digitization has made cadastres more accessible to a wider audience, although accessing the physical volumes stored in archives remains challenging for historians and researchers (Bundesamt für Eich- und Vermessungswesen 2017; Femenia- Ribera, Mora-Navarro, and Pérez 2022; Hagemans et al. 2022; McDonough 2024). Despite digitization efforts, the comprehensive evaluation of cadastral data, particularly in Habsburg Central Europe where 300,000 square kilometers were surveyed between 1816 and 1861, remains labor-intensive due to the sheer volume of individual map sheets documenting every aspect of land use and property(Bundesamt für Eich- und Vermessungswesen 2017).</p>
            <p>The FWF project RePaSE – Reading the Past from the Surface of the Earth – 2022 aimed to use AI-assisted extraction to identify historical building outlines and locations from geo-referenced map sheets of the Franziszeische Kataster (Franciscean Cadastre). This involved training an AI model for building extraction from both historical maps and modern high-resolution aerial and satellite images. By overlaying the extracted historical and present building information, the project sought to identify potential archaeological sites, facilitating large-scale archaeological remote sensing. The hypothesis driving RePaSE is that by combining historical cadastral map data with modern satellite imagery, it's possible to identify regions where buildings once stood, aiding in the discovery of vanished structures and potentially significant archaeological sites.</p>
            <p>In addition to the cadastre, we worked with current spatial images in our study. We were supported by the relevant department of the Province of Styria, which provided us with high-resolution aerial photographs from different flight periods and two digital terrain models. It has been shown that even the transfer of information from georeferenced historical map images to different spatial representations of the present has enormous cognitive potential. We were able to draw on high-resolution aerial photographs from 1952, images from the 1970s, 1980s and 1990s, and current and extremely accurate images from recent years.</p>
            <p>The results of RePaSE, and the tools that emerge from it, should create opportunities for the digital humanities to deal analytically with the information contained in historical maps. Possibilities for automated segmentation of the information contained therein would open up opportunities for us to further process data contained in maps quantitatively, or to utilize new possibilities for qualitative analysis (Hosseini et al. 2021). In HGIS systems that contain georeferenced historical maps, it is already possible to calculate areas; automated segmentation would significantly advance historical analysis (Baeten and Lave 2020). Furthermore, new feature complexes could be defined and searched for automatically, extending the logics of distant reading to map analysis (Arnold and Tilton 2019). For example, mills not explicitly identified in the French cadastre could be automatically identified, which would allow a profound analysis of hydropower use in Central Europe in the early 19th century and enable the investigation of local or regional differences.</p>
            <p>We consider neither cadastre nor satellite and high-resolution aerial photography authentic representations of reality, but as high-profile sources providing us with several layers of research data (Gil-Fournier and Parikka 2021; Hosseini et al. 2021).</p>     
        </div>
        <div>
          <head>State of Research</head>
            <p>Despite the accessibility challenges mentioned above, there is substantial historical research that deals either with the construction of cadastral maps, or with the information stored in them(Dolejš and Forejt 2019). Already prior to digitization, cadastral maps have played an important role in historical and particularly archeological research on micro-level(Petek and Urbanc 2004). Cadastral maps have been a research resource with regard to genealogical research for a very long time, they were used as sources in agricultural and environmental history and further environmental studies, in archeological remote sensing, in social and economic history, and with regard to the development of a broader understanding of the evolution of landscapes (Drobesch 2013; Rumpler 2013; Scharr 2024; Rumpler, Scharr, and Ungureanu 2015; Hohensinner et al. 2021). Their value, however, has frequently been delimitated by the difficult accessibility of the data and the difficulties regarding the extension of analysis beyond relatively limited borders.
            </p>
            <p>Access to historical cadastral maps in Central Europe became publicly available relatively late, with data now accessible through projects like www.mapire.eu. In most cases, the digitization of cadastral maps is accompanied by georeferencing. However, these repositories often lack comprehensive metadata and may require payment for research use. Public authorities have increasingly made parts of the Franziszeische Kataster available online for research purposes, offering data at various quality levels for free (Pivac et al. 2021).Exploration of historical map data, including cadastral maps, has accelerated due to recent advancements in machine learning(Chiang et al. 2020; Budig 2017). Deep learning technologies, in particular (L.-C. Chen et al. 2016), have revolutionized feature extraction, focusing on streets (Ekim, Sertel, and Kabadayı 2021; Can, Gerrits, and Kabadayi 2021; Jiao, Heitzler, and Hurni 2021; Uhl et al. 2022; Jiao, Heitzler, and Hurni 2022) and buildings (Heitzler and Hurni 2020; Uhl et al. 2020) among other applications (Wu et al. 2023; Rémi Petitpierre, Kaplan, and Di Lenardo 2021a; Garcia-Molsosa et al. 2021). These advancements offer new possibilities for researchers in history, archaeology, and historical geography. Recent research has focused on feature extraction from cadastral maps, particularly in the context of the Venetian cadastre and the 1900 Atlas of Paris(Oliveira et al. 2019; Remi Petitpierre and Guhennec 2023). As Petitpierre and Guhennec point out, consistent annotation is the main challenge with regard to automatized vectorization.
            </p>
            <p>However, the bulk of current research is directed towards the analysis of aerial and satellite imagery (Jiao, Heitzler, and Hurni 2022). This research spans a wide range of objectives and interests, with stakeholders including municipal and urban administrations, as well as archaeological remote sensing missions. Additionally, significant attention has been given to LIDAR data and research on urban landscape transformation, as well as real-time surveillance tasks involving UAVs (J. Li et al. 2022; Ji, Wei, and Lu 2019; M. Li et al. 2023; Fiorucci et al. 2020; 2022; Bickler 2021; Ren et al. 2015; Ding and Zhang 2021; Luo, Wu, and Wang 2022; Lee, Wang, and Lee 2023; K. Chen et al. 2021; Uhl et al. 2021).
            </p>
            <p>The Franciscean Cadastre, covering an extensive area of Habsburg Central Europe spanning around 300,000 square kilometers, stands out as an exceptional historical source. Its nearly half-century production period and stringent quality standards provide historians, archaeologists, and environmental scientists with privileged insights into 19th-century land use and spatial organization (Feucht 2008). Nevertheless, the cadastre also contains inaccuracies and errors, which are magnified by the size of the operation. It represents a historical primary source and as such requires meticulous and thorough critical reading. Werner Drobesch has recently provided the first comprehensive analysis of the economics of the cadastre, and from this, and from the savings constraints he has pointed out, there are also comprehensive indications of the weaknesses of this source(Scharr 2024). Further, a great deal depended on the quality of the individual's work; the scale of the recording varied considerably (high level of detail in urban areas, medium level of detail in rural areas, very coarse details in exposed terrain and high mountains). Many of the weaknesses encountered in dealing with the Franziszeische Kataster resemble those that Hosseini et al. describe and scrutinize in detail with regard to the Ordnance Survey (Hosseini et al. 2021)
            </p>
        </div>
        <div>
          <head>Task</head>
            <p>The objective of RePaSE was twofold: RePaSE should provide a proof-of-concept that AI-assisted extraction of large amounts of data – in our use case: the location and the form of buildings – from the cadastre is already possible with existing resources and, in the best case, identify suitable models to that purpose. The second step was to make RePaSE fit for future multimodal applications, and to identify and test models that could be used to extract the location and the form of buildings also from current aerial and satellite imagery that is already available (Smits and Wevers 2023).</p>
            <p>The larger context in which our work is embedded is the question of the possibilities that deep neural networks open up for the work of historians, archaeologists and other humanities scholars. A central challenge that we are addressing with this project is to make the huge data structures that represent historical cadastres searchable at a higher level of analysis. On the one hand, we see potential for a quantitative economic and social history. Structuring and classifying the information encoded in cadastres would make it possible to analyze land use at regional or provincial level, for example. In the field of archaeology, we see possibilities in that a cost-effective type of remote sensing in the automated comparison of historical and current visual representations of space would, on the one hand, allow faster identification of potentially archaeologically relevant large-scale historical structures. On the other hand, our tool would also make it possible to extend the archaeological view to larger spatial units with relatively little effort and to better visualize translocal or regional contexts for experts.
            </p>
            <p>Our research strategy involved two main sub-tasks: task 1 focused on detecting buildings in scans of historical cadastral map data, while task 2 aimed to detect buildings in current satellite and aerial imagery. We then superimposed the output layers of both tasks, specifically targeting places where buildings were present in the cadastre but absent in the present. This overlay combined historical cadastral map data with current satellite imagery, allowing us to identify relevant locations for further analysis. These locations were then utilized for analyzing different spatial representations, including satellite and aerial imagery.</p>
        </div>
        <div>
          <head>Approach</head>
            <p>As it turned out that the size of the annotated map patches had a significant impact on the performance of our models, we resized all map patches to the uniform size 3747x2235, which was a dimension, which turned out to be working quite well. However, we employed three different zoom levels – close, medium and far.
            </p>
            <p>We used the software tool cvat to annotate the map patches (“www.cvat.ai,” n.d.). The first task to be addressed was an image segmentation problem, in order to be able to successfully extract houses from cadastral map material. In our first attempt, we followed a low-key approach and tried clustering algorithms to tackle this challenge, which turned out unsuccessful. In line with the fail-fast approach of our proof-of-concept study, we subsequently turned to a completely different method. When it comes to deep neural networks in this area of application, convolutional neural networks (CNN) have outperformed other architectures in the last years(Rawat and Wang 2017; O’Shea and Nash 2015). A CNN is a type of deep learning algorithm that is well-suited for analyzing visual data. It uses principles from linear algebra, especially convolution operations, to extract features and identify patterns within images. CNNs use a series of layers, each of which detects different features of an input image. Depending on the complexity of its intended purpose, a CNN can contain dozens, hundreds, or even thousands of layers. As building extraction can be considered a computer vision problem, fully convolutional networks are considered the most widespread state-of-the-art solution to this kind of problem (K. Chen et al. 2021). We therefore chose the Google DeeplabV3 model (L.-C. Chen et al. 2017), which is considered to be very efficient in dealing with the multi-scale problem featured in this context, due to its atrous convolution layer (L.-C. Chen et al. 2016). We finetuned DeepLabV3 with 50 annotated example images that we split into 6 squares each to give 300 images in total. The images were split into training, testing and validation set according to the following scheme:
            </p>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure02.jpeg"/>
            </figure>
            <p>The validation set was automatically populated with squares lacking buildings to teach the model the absence of structures in some map patches, explaining its larger size compared to the test and train sets. Each dataset (train, test, val) contained unique images without duplication across sets. Finetuning utilized a repository designed for transfer learning in semantic segmentation (Singh and Popescu 2021), building upon DeeplabV3 with resnet 101. Finetuning spanned 20 epochs on various computing platforms until achieving satisfactory results (“Https://Pytorch.Org/Vision/Stable/Models/Generated/Torchvision.Models.Segmentat ion.Deeplabv3_resnet101.Html#torchvision.Models.Segmentation.Deeplabv3_resnet 101” 2017).</p>
            <p>The second task to be addressed was another image segmentation problem, the extraction of buildings from high-resolution aerial and satellite data. As the quality of satellite imagery freely available has once more increased in the two years between the setup of the project idea and the project execution, we could use satellite imagery in better quality than originally expected, yet we could also add very-high-resolution aerial photography and LiDAR data to the research data under scrutiny. Unlike the 19th century cadastral map data that we were dealing with in task 1, we were addressing an entirely different problem here, as buildings in ortho- and satellite imagery can take on a wide range of very different appearances. Due to the significantly different challenge, we chose an adapted approach. Vision transformers break down input images into patches, convert each patch into a vector, and then process these vectors using a transformer encoder. This allows the model to understand the content of the image and has been used for various computer vision tasks such as image recognition, image segmentation, and object detection. Vision transformers are considered to feature relatively economic computational overhead and memory consumption, while they can achieve state-of-the-art accuracy (K. Chen et al. 2021). We therefore trained a sparse token transformer (STTnet) from scratch, using a GPU. STTnet is a state-of-the-art vision transformer with very promising results in comparable tasks(K. Chen et al. 2021). Building on Resnet50 as backbone, we selected 2.657 labelled images from the AIRS dataset. This was so that we could avoid labelling houses in the satellite images ourselves. The effort for this would have been considerable, because for training from scratch we need substantially more training examples than for finetuning. All images resembled the Alpine topography and the landscape characteristics of our target region Styria. We finally used imagery that was taken in Austin, Chicago, Kitsap (Washington State), Tyrol and Vienna. The training took place over 98 epochs, once the loss rate and the F1 score did not feature significant changes anymore, the training was considered completed. The model was then tested with 224 labelled images.</p>
            <p>Subsequently, the extracted layers were superimposed and negative profiles were created. The negative profiles contain archaeologically potentially interesting locations, which once saw buildings, yet that are not overbuilt at present.</p>
        </div>
        <div>
          <head>Results</head>
            <p>The results for task 1 – the extraction of buildings from cadastral maps via DeeplabV3 turned out very well.</p>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure03.jpeg"/>
            </figure>
            <p>As can be seen from the images, the extraction of red houses (stone houses), for which the model was finetuned worked impeccably, which is reflected in the metrics we obtained. We calculated the parameter intersection over union (IoU), where the labelled mas You are an experienced reader and reviewer of scientific literature. Read the following paragraph between ‚‘ and tell me, whether it is clear, concise, well-written, and comprehensible. The paragraph represents the final part of the section „Approach“ and is supposed to describe, how we plan to tackle the problem described earlier in the article. – thus the image that we prepared for training by labelling – is compared with the predicted mask, thus the result the model actually delivers once the training is completed. The IoU score ranges from 0 to 1, with a score of 1 indicating a perfect overlap between the predicted and ground truth bounding boxes. A high IoU score indicates that the object detection algorithm is accurately localizing the object in the image. The value is calculated per class, whereas we defined two classes here, house (class 1) and background (class 2). We calculated two values:
            </p>
            <!-- Insert equations here! -->
            <p>We yielded a mean micro average IoU of 0.990389 and a mean macro average IoU of 0.89516, which represent outstanding values. The accuracy of the model was thus sufficiently high. Among the problems we encountered were some streets that were mistaken for houses, and minor differences in the color schemes (shades) in the hand-drawn maps. These were, however, minor issues, which we believe that we could deal with quite simply by labelling more training examples and retraining the model. Further finetuning could teach the model to recognize further structures, particularly yellow houses (wooden buildings) and streets, which will be our priority in a follow-up project.</p>
            <p>The situation with task 2 was slightly different. The achieved IoUs of 0.537755 (macro) and 0.795838 (micro) proved to be satisfactory in this context from a pragmatical perspective, as we were well able to work with the obtained results. Although these are significantly below the values achieved for task 1, we consider this task successfully solved. The building outlines contained in the negative layer must be kept more generous anyway, as it is evident, especially in the terrain model, that the terrain is affected by the development beyond the actual building boundaries, through the levelling of the built-over area. The red markings in the upper layer mark false positives (a house was detected, where there was no house in reality), the green markings false negatives (the model failed to detect the house). White indicates a correct identification. The model sometimes displays difficulties correctly identifying houses at the margins of the detection area. Furthermore, it was difficult for the model to achieve good results outside a certain (optimal) zoom range. This problem could be circumvented by properly addressing the issue in the data preprocessing. As a rule, the model tended to minimally enlarge the recognized buildings, which, however, was quite convenient for our work, as already noted. From an academic perspective, we see some possibilities to optimize the model. We are certain that additional training, i.e. adding more epochs, would yield a substantial improvement of the performance.
            </p>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure04.jpeg"/>
            </figure>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure05.jpeg"/>
            </figure>
        </div>
        <div>
          <head>Next Steps</head>
           <p>RePaSE provided proof-of-concept with regard to AI driven data extraction from historical map material and extremely valuable experience in terms of a way forward. The most important finding is that structures can be extracted from historical cadastral map material well and economically in very high quality. This opens up a wide range of new possibilities for us. Therefore, this is a field, in which we plan on engaging further, building on the experience we gathered in RePaSE and further in cooperation with several other groups working in this field of research. We expect that particularly the highly relevant findings of colleagues concerning the synthetic production of suitable training data (Jiao, Heitzler, and Hurni 2022) will allow for a significant breakthrough in extracting relevant data from historical maps at scale.
           </p>
           <p>In view of the rapid progress in the extraction of graphic structures from high- resolution satellite and aerial photographs, it seems sensible to orientate on the current state of research instead of investing own resources in improving the technology. Nevertheless, we consider it important to keep an eye on this field, as digital humanities and humanities data science struggle to access state-of-the-art technologies, models and datasets to process information explainable. We consider the solution we identified in the context of RePaSE – STTnet – viable and expandable, though it will require a substantial effort to toughen up our outputs to a level, at which the quality of results will be robust enough to qualify for positive profiles. The STTnet we trained is perfectly working in a project context such as RePaSE, it will however require some adjustments to make it work in a different context where a higher IoU metric might be required.</p>
           <p>We have identified structures that can be detected very well when using computer vision, and which bear important time signatures, thus, which actually promise to render the past that is hidden in the surface of the Earth visible to the eyes of researchers. Further, as we made promising and encouraging experiences with other object detectors on related tasks recently, we believe to have identified promising paths of development in this field.</p>
           <p>Our study has shown us that, in principle, our models would also be suitable for recognising and reporting structural changes over time in a more finely graduated manner. By choosing a different model - experiments with YOLOv5 were promising - building outlines in the cadastral material could be recognised even more finely and, theoretically, buildings that may not have undergone any structural changes could be detected automatically. Even if RePaSE was not aimed at finding out what was used to replace historical buildings, the models could also contribute to clarifying such questions with a manageable adaptation effort.</p>
           <p>A follow-up to RePaSE has recently been granted by the Austrian Science Fund (FWF), it will fully integrate the two models trained and finetuned within RePaSE – DeeplabV3 and STTnet – and will set out to identify and train two further models with a focus on historical road infrastructure.</p>
        </div>
        <div>
          <head>Conclusion</head>
            <p>RePaSE provides a wide variety of stakeholders, from municipal authorities over political decision makers at the local and regional level to historians and archeologists with a powerful tool to detect sites of potential archeological relevance, as is shown on the following two figures:</p>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure06.jpeg"/>
            </figure>
            <figure>
              <head></head>
              <figDesc></figDesc>
                <graphic url="resources/images/figure07.jpeg"/>
            </figure>
            <p>Based on our findings, we are convinced that the results of RePaSE will be an important support for various stakeholders in Central Europe in many areas: Our model is likely to be used in the field of construction management when archaeologically and historically sensitive areas are to be identified in advance of major construction and infrastructure projects. It can help to identify relevant structures in cultural heritage management and make them more comprehensively visible, for example in existing GIS and HGIS systems. Primarily, however, we see RePaSE as the basis for a research tool designed to support digital humanists in identifying and extracting relevant research data from mapped sources.</p>
            <p>By archaeologically relevant in the context of RePaSE, we mean sites that featured historic buildings and may require archaeological investigation in the course of construction activities or current land use. RePaSE can support the identification of such sites in advance and thus help to prevent so-called emergency excavations, e.g. by changing the route or planned archaeological intervention before construction begins.</p>
            <p>The question arises to what extent neural networks can also be trained to detect visually relevant anomalies in different dimensions of surface representations (digital terrain model, different high-resolution aerial photographs) without having to rely on concrete knowledge from a historical knowledge representation. Such detection could be achieved by a neural network, for example by identifying complex feature clusters in multimodal visual input signals - such as the superimposition of DTM and high- resolution aerial images from different aerial surveys.</p>
            <p>The RePaSE-team is currently working to set up a server to provide full access to the above-mentioned group to the entire dataset processed in the project runtime. RePaSE is currently working for the entire region of Styria, we expect that it could easily be expanded for the adjoining regions of Carinthia, Lower Austria, Salzburg and Slovenia. As Croatia and Burgenland were part of the Transleithanian cadastre, which features slightly different optical features, we expect to meet some difficulties there, yet the system would probably be working with another round of finetuning.</p>
            <p>RePaSE clearly showed the enormous potential of machine learning, particularly with regard to computer vision, image segmentation and object detection to scale up analysis of historical map material and to combine these insights with (AI harvested) data gained from current high-resolution imagery of the surface of the Earth. Models of the latest generation feature outstanding usability and require manageable effort to successfully extract data from large-scale knowledge representations. It turned out in the context of this project, though, that domain knowledge plays an increasingly important role.</p>
            <p>Whereas image segmentation must be regarded a standard technology with regard to this type of sources now, due to its good availability, usability and accessibility, possibly the most relevant result of RePaSE with regard to the question, what digital humanists’ tools will be for working with these data in the near future, is: Based on our results, what we learnt is essentially that object detection bears enormous potential for many crucial tasks, particularly when it comes to the analysis of historical map data at scale. The object detectors we have been working with so far display a wide range of possible applications, which reach far beyond the purpose they were developed for. The fact that many object identification models combine excellent object recognition with relatively manageable hardware requirements makes them eminently suitable tools for use in the DH context.</p>
            <p>However, the most relevant potential of the tools that we were working with in this project, is their capacity to bring spatial analysis in a historical context to scale. What is more, a large set of different tools meanwhile exists to visualize such changes, which will help historians and fellow humanists to develop a deeper understanding of how space transforms over time.</p>
        </div>
      </body>
      <back>
        <listBibl>
          <bibl xml:id="arnold_tilton_2019" label="Arnold and Tilton 2019">Arnold, T. and Tilton, L. (2019) <title rend="quotes">Distant viewing: Analyzing 
             large visual corpora</title>, <title rend="italic">Digital Scholarship in the Humanities</title>, 34(Supplement 1), pp. i3-i6. 
             <ref target="https://doi.org/10.1093/llc/fqz013">https://doi.org/10.1093/llc/fqz013</ref>.
          </bibl>
          <bibl xml:id="baeten_lave_2020" label="Baeten and Lave 2020">Baeten, J. and Lave, R. (2020) <title rend="quotes">Retracing rivers and drawing 
             swamps: Using a drawing tablet to reconstruct an historical hydroscape from Army corps survey maps</title>, <title rend="italic">Historical 
             Methods</title>, 53(3), pp. 182–198. 
             <ref target="https://doi.org/10.1080/01615440.2020.1748151">https://doi.org/10.1080/01615440.2020.1748151</ref>.
          </bibl>
          <bibl xml:id="bickler_2021" label="Bickler 2021">Bickler, S.H. (2021) <title rend="quotes">Machine learning arrives in archaeology</title>, 
             <title rend="italic">Advances in Archeological Practice</title>, 9(2), pp. 186–192. 
             <ref target="https://doi.org/10.1017/aap.2021.6">https://doi.org/10.1017/aap.2021.6</ref>.
          </bibl>
          <bibl xml:id="budig_2017" label="Budig 2017">Budig, B. (2017) <title rend="italic">Extracting spatial information from historical maps: Algorithms 
             and interaction</title>. Würzburg, Germany: Würzburg University Press.
          </bibl>
          <bibl xml:id="bundesamt_2017" label="Bundesamt 2017">Bundesamt für Eich- und Vermessungswesen (ed.) (2017) <title rend="italic">Österreichisches 
             Kulturgut: 200 Jahre Kataster, 1817–2017</title>.
          </bibl>
          <bibl xml:id="can_gerrits_kabadayi_2021" label="Can, Gerrits, and Kabadayi 2021">Can, Y.S., Gerrits, P.J., and Kabadayi, M.E. (2021) 
             <title rend="quotes">Automatic detection of road types from the third military mapping survey of Austria-Hungary historical map series with deep 
             convolutional neural networks</title>, <title rend="italic">IEEE Access</title>, 9. 
             <ref target="https://doi.org/10.1109/ACCESS.2021.3074897">https://doi.org/10.1109/ACCESS.2021.3074897</ref>.
          </bibl>
          <bibl xml:id="champion_2017" label="Champion 2017">Champion, E.M. (2017) <title rend="quotes">Digital humanities is text heavy, visualization light, 
             and simulation poor</title>, <title rend="italic">Digital Scholarship in the Humanities</title>, 32(Supplement 1), pp. 25–32. 
             <ref target="https://doi.org/10.1093/llc/fqw053">https://doi.org/10.1093/llc/fqw053</ref>.
          </bibl>
          <bibl xml:id="chen_zou_shi_2021" label="Chen, Zou, and Shi 2021"></bibl>
          <bibl xml:id="chen_et_al_2016" label="Chen et al. 2016"></bibl>
          <bibl xml:id="chen_et_al_2017" label="Chen et al. 2017"></bibl>
          <bibl xml:id="chiang_et_al_2020" label="Chiang et al. 2020"></bibl>
          <bibl xml:id="ding_wei_zhang_2021" label="Ding, Wei, and Zhang 2021"></bibl>
          <bibl xml:id="dolejs_forejt_2019" label="Dolejš and Forejt 2019"></bibl>
          <bibl xml:id="drobesch_2013" label="Drobesch 2013"></bibl>
          <bibl xml:id="ekim_sertel_kabadayi_2021" label="Ekim, Sertel, and Kabadayi 2021"></bibl>
          <bibl xml:id="femenia-ribera_et_al_2022" label="Femenia-Ribera et al. 2022"></bibl>
          <bibl xml:id="feucht_2008" label="Feucht 2008"></bibl>
          <bibl xml:id="Fiorucci_et_al_2020" label="Fiorucci et al. 2020"></bibl>
          <bibl xml:id="fiorucci_et_al_2022" label="Fiorucci et al. 2022"></bibl>
          <bibl xml:id="garcia-molsosa_et_al_2021" label="Garcia-Molsosa et al. 2021"></bibl>
          <bibl xml:id="gil-fournier_abelardo_parikka_2021" label="Gil-Fournier, Abelardo, and Parikka 2021"></bibl>
          <bibl xml:id="goderle_2017" label="Göderle 2017"></bibl>
          <bibl xml:id="goderle_2021" label="Göderle 2023"></bibl> 
          <bibl xml:id="hagemans_et_al_2022" label="Hagemans et al. 2022"></bibl>
          <bibl xml:id="heitzler_hurni_2020" label="Heitzler and Hurni 2020"></bibl>
          <bibl xml:id="hohensinner_et_al_2021" label="Hohensinner et al. 2021"></bibl>
          <bibl xml:id="hosseini_et_al_2021" label="Hosseini et al. 2021"></bibl>
          <bibl xml:id="ji_wei_lu_2019" label="Ji, Wei, and Lu 2019"></bibl>
          <bibl xml:id="jiao_heitzler_hurni_2021" label="Jiao, Heitzler, and Hurni 2021"></bibl>
          <bibl xml:id="jiao_heitzler_hurni_2022" label="Jiao, Heitzler, and Hurni 2022"></bibl>
          <bibl xml:id="katastral_1824" label="Katastral-Vermessungs Instruktion 1824"></bibl>
          <bibl xml:id="lee_wang_lee_2023" label="Lee, Wang, and Lee 2023"></bibl>
          <bibl xml:id="li_et_al_2022" label="Li et al. 2022"></bibl>
          <bibl xml:id="li_et_al_2023" label="Li et al. 2023"></bibl>
          <bibl xml:id="luo_wu_wang_2022" label="Luo, Wu, and Wang 2022"></bibl>
          <bibl xml:id="mcdonough_2024" label="McDonough 2024"></bibl>
          <bibl xml:id="noord_2022" label="Noord 2022"></bibl>
          <bibl xml:id="oliveira_et_al_2019" label="Oliveira et al. 2019"></bibl>
          <bibl xml:id="oshea_nash_2015" label="O'Shea and Nash 2015"></bibl>
          <bibl xml:id="petek_urbanc_2004" label="Petek and Urbanc 2004"></bibl>
          <bibl xml:id="petitpierre_guhennec_2023" label="Petitpierre and Guhennec 2023"></bibl>
          <bibl xml:id="petitpierre_kaplan_di-lenardo_2021" label="Petitpierre, Kaplan, and Di Lenardo 2021"></bibl>
          <bibl xml:id="pivac_et_al_2021" label="Pivac et al. 2021"></bibl>
          <bibl xml:id="pytorch_n.d." label="PyTorch n.d.">PyTorch (n.d.) <title rend="italic">DEEPLABV3</title>. Available at: 
             <ref target="https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/">https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/</ref>.
          </bibl>
          <bibl xml:id="rawat_wang_2017" label="Rawat and Wang 2017"></bibl>
          <bibl xml:id="ren_et_al_2015" label="Ren et al. 2015"></bibl>
          <bibl xml:id="rumpler_2013" label="Rumpler 2013"></bibl>
          <bibl xml:id="rumpler_scharr_ungureaunu_2015" label="Rumpler, Scharr, and Ungureanu 2015"></bibl>
          <bibl xml:id="scharr_2024" label="Scharr 2024"></bibl>
          <bibl xml:id="signh_popescu_2021" label="Singh and Popescu 2021"></bibl>
          <bibl xml:id="smits_wevers_2020" label="Smits and Wevers 2020"></bibl>
          <bibl xml:id="smits_wevers_2023" label="Smits and Wevers 2023"></bibl>
          <bibl xml:id="stahl_weimann_2022" label="Ståhl and Weimann 2022"></bibl>
          <bibl xml:id="uhl_et_al_2020" label="Uhl et al. 2020"></bibl>
          <bibl xml:id="uhl_et_al_2021" label="Uhl et al. 2021"></bibl>
          <bibl xml:id="uhl_et_al_2022" label="Uhl et al. 2022"></bibl>
          <bibl xml:id="wheeler_2023" label="Wheeler 2023"></bibl>
          <bibl xml:id="wu_et_al_2023" label="Wu et al. 2023"></bibl>
          <bibl xml:id="cvat_n.d." label="CVAT n.d.">CVAT (n.d.) <title rend="italic">CVAT: Open data annotation platform</title>. Available at: 
             <ref target="https://www.cvat.ai/">https://www.cvat.ai/</ref>.
          </bibl>
        </listBibl>
      </back>
   </text>
</TEI>
