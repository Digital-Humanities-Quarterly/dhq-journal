<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI
  xmlns="http://www.tei-c.org/ns/1.0"
  xmlns:cc="http://web.resource.org/cc/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">

  <!-- BEGIN TEI HEADER ELEMENTS -->
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title type="article" xml:lang="en">Towards a User-Friendly Tool for
          Automated Sign Annotation: Identification and Annotation of Time
          Slots, Number of Hands, and Handshape
        </title>
        <dhq:authorInfo>
          <dhq:author_name>Manolis <dhq:family>Fragkiadakis</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>m.fragkiadakis@hum.leidenuniv.nl</email>
          <dhq:bio><p>PhD student in the Center for Digital Humanities and Data
            Science Research Program at Leiden University, the Netherlands.
            His current research focuses on the study of automatic annotation
            as well as variation measurement for sign language corpora and
            dictionaries.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Victoria A.S. <dhq:family>Nyst</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>v.a.s.nyst@hum.leidenuniv.nl</email>
          <dhq:bio><p>Associate professor working at the Leiden University
            Center for Linguistics in the Netherlands. Her research focuses
            on sign languages and gestures of deaf and hearing people in Africa,
            leading to the publication and analysis of a growing number of
            video corpora of West African sign languages, as well as of a
            dictionary app for Ghanaian Sign Language. </p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Peter <dhq:family>van der Putten</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>p.w.h.van.der.putten@liacs.leidenuniv.nl</email>
          <dhq:bio><p>Assistant professor at the Leiden Institute of Advanced
            Computer Science (LIACS), Leiden University, The Netherlands. His
            research borders on the intersection of machine learning and
            creative research, and he is a collaborator in the LCDS and SAILS
            university wide AI research programs; the Creative Intelligence Lab
            and the [A]social Creatures Lab, and the Media Technology MSc
            program. He is also a Director Decisioning Solutions at
            Pegasystems.</p>
          </dhq:bio>
        </dhq:authorInfo>
      </titleStmt>
      <publicationStmt>
        <publisher>Alliance of Digital Humanities Organizations</publisher>
        <publisher>Association for Computers and the Humanities</publisher>
        <idno type="DHQarticle-id">000510</idno>
        <idno type="volume">014</idno>
        <idno type="issue">4</idno>
        <date></date>
        <dhq:articleType>article</dhq:articleType>
        <availability>
          <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <p>This is the source</p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <classDecl>
      <taxonomy xml:id="dhq_keywords">
        <bibl>DHQ classification scheme; full list available at<ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
      </taxonomy>
      <taxonomy xml:id="authorial_keywords">
        <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
      </taxonomy>
      </classDecl>
    </encodingDesc>
      <profileDesc>
        <langUsage>
          <language ident="en" extent="original"/>
          </langUsage>
          <textClass>
            <keywords scheme="#dhq_keywords">
              <list type="simple">
                <item></item>
              </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
              <list type="simple">
                <item></item>
              </list>
            </keywords>
          </textClass>
      </profileDesc>
      <revisionDesc>
        <change when="2020-09-18" who="Taylor Arnold">Created file</change>
      </revisionDesc>
    </teiHeader>
    <!-- END TEI HEADER ELEMENTS -->

    <!-- BEGIN TEXT -->
    <text xml:lang="en" type="original">
      <!-- FRONT TEXT -->
      <front>
        <dhq:abstract>
          <p>The annotation process of sign language corpora in terms of
            glosses, is a highly labor-intensive task, but a condition for a
            reliable quantitative analysis. During the annotation process the
            researcher typically defines the precise time slot in which a sign
            occurs and then enters the appropriate gloss for the sign. The aim
            of this project is to develop a set of tools to assist the
            annotation of the signs and their formal features in a video
            irrespectively of its content and quality. Recent advances in the
            field of deep learning have led to the development of accurate and
            fast pose estimation frameworks. In this study, such a framework
            (namely OpenPose) has been used to develop three different methods
            and tools to facilitate the annotation process. The first tool
            estimates the span of a sign sequence and creates empty slots in an
            annotation file. The second tool detects whether a sign is one- or
            two-handed. The last tool recognizes the different handshapes
            presented in a video sample. All tools can be easily re-trained to
            fit the needs of the researcher.</p>
        </dhq:abstract>
        <dhq:teaser>
          <p>The aim
          of this project is to develop a set of tools to assist the
          annotation of the signs and their formal features in a video
          irrespectively of its content and quality.</p>
        </dhq:teaser>
      </front>

      <!-- BODY TEXT -->
      <body>
        <div><head>Introduction</head>
        <p>While the majority of the studies in the field of digital humanities have
        been mostly text oriented, the evolution in computing power and technology
        has resulted in a shift towards multimedia-oriented studies. Recently,
        advances in computer vision have started to find practical applications in
        study domains outside of computer and data science. Video is one of the most
        important time-based media as it has the ability to carry large amount of
        digital information in a condensed form, and hence it serves as a rich medium
        to capture various forms of cultural expression. Automated processing and
        annotation of large numbers of videos is now becoming feasible due to the
        evolution of computer vision and machine learning.</p>
        <p>In sign language linguistics, a transition took place from paper-based
        materials to large video corpora to facilitate the study of the languages in
        question. Sign language corpora are mainly composed of video data. The primary
        goal of these video corpora is to study sign language functioning.</p>
        <p>The processing of sign languages usually involves requires a form of textual
        representation <ptr target="#dreuw2008"/>, most notably glosses for annotation. Sign
        language glosses are words from a spoken language. Uniquely identifying glosses
        by definition refer to a specific sign. Such ID glosses are an essential
        element for the quantitative analysis of a sign language corpus
        <ptr target="#johnston2010"/>. Typically, sign language linguists add glosses
        and other annotations to
        the video recordings with the use of a software tool (namely ELAN). ELAN allows
        researchers to add time-aligned annotations to a video. However, this task
        requires a lot of time and can be prone to errors.</p>
        <p>New advances in computer vision open up additional ways of studying videos
        containing sign language data, extracting formal representations of linguistic
        phenomena, and implementing these in computer applications, such as automatic
        recognition, generation, and translation. Using computer vision and machine
        learning enables quick and new ways of processing large sets of video data,
        which in turns makes it possible to address research questions that were not
        feasible before.</p>
        <p>This study is the first part of a project aiming at the creation of tools
        to automatize part of the annotation process of sign language video data.
        This paper presents the methodologies, tools and implementations of three
        functionalities: the detection of 1) manual activation 2) the number of hands
        involved and 3) the handshape distribution on sign language corpora.</p>
        <p>Recent developments in sign language recognition illustrate the
        advantages of machine and deep learning for tasks related to recognition and
        classification
        <ptr target="#agha2018"/>
        <ptr target="#cao2017"/>
        <ptr target="#pigou2015"/>.
        Nevertheless, current approaches are restricted in various ways, limiting
        their applicability in current sign language research. For example, training
        deep learning networks requires a vast amount of data as well as adequate
        computational power. These networks are usually trained in one sign language
        and they do not generalize well in other sign languages.</p>
        <p>Additionally, current approaches in sign language automatic annotation
        need manual annotation of the hands and body joints for the training of the
        recognizer models
        <ptr target="#aitpayev2016"/>
        <ptr target="#dreuw2008"/>. Moreover,
        the application of color and motion detection algorithms <ptr target="#kumar2017"/>, as
        feature extraction methods, can be susceptible to errors and possibly skin
        color bias. Finally, several hand tracking models only work on a particular
        type of recordings, for example, a signer wearing gloves, or recordings made
        with Microsoft’s Kinect <ptr target="#pigou2015"/>. As a result, these models are not
        usable for the majority of the existing sign language corpora which have been
        recorded with standard RGB cameras.</p>
        <p>Our methods have been developed and tested on two West African sign language
        corpora containing natural conditions with non-Caucasian signers. While most
        studies in the sign language recognition field have mainly concerned signers
        with light skin tones, little research has been conducted using darker skin
        tones. With the emergence of corpora compiled in African countries under
        challenging real-world conditions, and their contribution to the overall sign
        language community, it is of utmost importance to test how methods perform
        in such a domain. Alleviating biases and increasing diversity should be a top
        priority of any computer assisted study.</p>
        <p>In this study, a pre-trained deep learning pose estimation library developed
        by Cao et al. <ptr target="#cao2017"/> named OpenPose has been used to extract body and finger
        key-points. OpenPose has been trained and evaluated on two large datasets for
        multi-person pose estimation. The MPII human multi-person dataset
        <ptr target="#andriluka2014"/> and the COCO 2016 keypoints challenge dataset
        <ptr target="#lin2014"/>
        contain images of people of different age groups and ethnicities in diverse
        scenarios. As a result, OpenPose does not have a bias toward skin color.
        Additionally, its easy-to-use implementation makes it an ideal framework to be
        used by linguists with limited coding experience.</p>
        <p>The combination of the aforementioned pose estimation framework as well
        as the machine and deep learning architectures tested in this study, provides
        a robust approach towards automatic annotation. Current models and tools can
        be used in any sign language or gestural corpus independently of its quality,
        length and number of people in the video. These tools have been developed as
        python modules that can run automatically in a video and produce the relevant
        annotation files requiring minimal effort from the user. More generally, as
        large parts of our cultures nowadays are captured in video, our study serves
        as a case example of how intelligent machine learning techniques can serve
        digital humanities researchers by extracting semantics from large video
        collections.</p>
        <p>This article is structured as follows: Section 2 introduces the developments
        on the sign language recognition and automatic annotation fields. Section 3
        describes the materials used in this study and the methodologies developed
        and applied for each tool separately. Section 4 presents the results for each
        experimental setup and tool. Section 5 contains the discussion and future
        work while Section 6 presents our conclusions. Finally, Appendix A presents
        the architecture and technical details of the Long-Short-Term-Memory Network
        trained for this study.</p>
        </div>
        <div><head>2. Literature review</head>

        <p>In this section we present the studies conducted on the sign language
        recognition and automatic annotation field developed with depth sensors as
        well as standard RGB cameras. Additionally, we describe the developments of the
        human pose estimation field and we introduce the OpenPose framework that will
        be used in this article.</p>

        <div><head>2.1 Sign Language Recognition and Automatic Annotation</head>

        <p>The primary goal of sign language recognition is to develop methods and
        algorithms to accurately identify a series of produced signs and to discern
        their meaning. The majority of studies have focused on recognizing those
        features and methods that can properly identify a sign out of a given set of
        possible signs. However, such methods can only be used on a particular set of
        signs and, thus, a specific sign language, which makes it harder to study the
        relationships between and evolution of various sign languages.</p>
        <p>An additional motivation behind Sign Language Recognition (SLR) is to
        build automatic sign language to speech or text translation systems to
        assist the communication between the deaf and hearing community <ptr target="#fang2004"/>.
        Moreover, SLR plays an important role in developing gesture-based
        human–computer interaction systems <ptr target="#kelly2009"/>. Sign language
        linguists have used such systems to facilitate the annotation process of sign
        language corpora in order to discern the different signs in a video recording
        and further study the linguistic phenomena presented.</p>
        <p>There are numerous studies dealing with the automated recognition of sign
        languages as clearly presented by Cooper et al. <ptr target="#cooper2007"/>, in their review study
        on the state-of-the-art in sign language recognition. However, the experiments
        presented in most of these studies are either hard to replicate, or they pose
        limitations as far as their applicability is concerned. For instance, most of
        these studies use depth sensors, most notably MS Kinect, to capture 3D images
        of the environment
        <ptr target="#aitpayev2016"/>
        <ptr target="#pigou2015"/>
        <ptr target="#zhang2013"/>.
        As a result, using the frameworks developed in these studies requires
        a machine with similar features as the one used for testing and most probably
        will only work for that sign language on which they have been trained.</p>
        <p>Recently, computer vision techniques have been applied to sign language
        recognition to overcome the aforementioned limitations. Roussos et al. <ptr target="#roussos2012"/>
        created a skin color probabilistic model to detect and track the hands of a
        signer on a video, while Cooper et al. <ptr target="#cooper2007"/> use this model to segment the
        hands and apply a classifier based on Markov models. However, systems based
        on skin color
        <ptr target="#buehler2009"/>
        <ptr target="#cooper2007"/>
        <ptr target="#farhadi2007"/>
        <ptr target="#starner1998"/>
        are prone to errors and have difficulties on
        tracking the hands and the signer’s features against cluttered backgrounds and
        in noisy conditions in general. Also, they do not work in videos with multiple
        signers.</p>
        </div>
        <div><head>2.2 Human Pose Estimation</head>

        <p>Human pose estimation has been extensively studied due to its numerous
        applications on a number of different fields <ptr target="#moeslund2006"/>. Due to
        low computational complexity during inference, pictorial structures have been
        commonly used
        <ptr target="#felzenszwalb2005"/>
        <ptr target="#ramanan2007"/>
        <ptr target="#sivic2006"/>
        to model human pose. Recently, studies have focused on improving
        the appearance models used in these structures by modelling the individual body
        parts
        <ptr target="#eichner2009"/>
        <ptr target="#eichner2012"/>
        <ptr target="#johnson2009"/>
        <ptr target="#sapp2010"/>. Felzenszwalb and Huttenlocher <ptr target="#felzenszwalb2005"/>,
        relying on
        the pictorial structure framework recommended a deformable part-based model.
        Additionally, Yang and Ramanan <ptr target="#yang2011"/>
        showed that a tree-structured model using
        a combination of deformable parts can be used in order to achieve accurate
        pose estimation. Furthermore, Charles et al. <ptr target="#charles2014"/>
        showed that human body
        joint positions can be predicted using a random forest regressor based on a
        co-segmentation process over all video frames.</p>
        <p>In general, most of the vision-based approaches developed for sign language
        recognition tasks utilizing pose estimation, have used the RWTH-PHOENIX-Weather
        data set <ptr target="#forster2012"/> to validate their models. This data set
        consists of weather forecast airings from the German public tv-station PHOENIX
        along with transcribed gloss annotations. However, it is a question to what
        extent such systems tested in this data set can be replicated with real-life
        conditions in the corpora. It is often the case that sign language and gestural
        corpora, especially the ones filmed outside of studio conditions, have bad
        quality, low brightness and often contain more than one person in the frame.
        These characteristics create an additional challenge to the tracking and
        prediction task.</p>

        <div><head>2.2.1 OpenPose</head>

        <p>OpenPose is a real-time, open-source library for academic purposes for
        multi-person 2D pose estimation. It can detect body, foot, hand and facial
        keypoints <ptr target="#cao2017"/>. Following a bottom-up approach (from an entire
        image as input to full body poses as output), it outperforms similar 2D body
        pose estimation libraries.</p>
        <p>A major advantage of the library is that it achieves high accuracy and
        performance regardless of the number of people in the image. Its high accuracy
        is performed by using a non-parametric representation of 2D vector fields.
        These fields encode the position and orientation of body parts over the image
        domain and their degree of association in order to learn to relate them to each
        individual.</p>
        <p>OpenPose is able to run on different operating systems and multiple hardware
        architectures . Additionally, it provides tools for visualization and output
        file generation. The output can be multiple json files containing all the pixel
        x, y coordinates of the body, hand and face joints. In this study the DEMO
        version on a CPU-only mode has been used to train our models. This choice was
        made in order to ensure that reproducibility can be easily achieved without the
        need for powerful computers from the linguist’s side.</p>

        </div></div></div>

        <div><head>3. Materials and Methods</head>

        <p>This section describes the datasets used in our study as well as the
        pre-processing stage using OpenPose to extract the body joints’ pixel
        coordinates. Furthermore, we introduce the methods applied in the development
        of each tool. Special consideration is given on the handshape recognition
        module as an additional normalization part has been developed. </p>

        <div><head>3.1 Data</head>

        <p>A data set of 7,805 frames in total (approximately 4 minutes) labeled as
        signing or not signing has been compiled for the first part of the study. The
        dimensions of the frames were 352 by 288 pixels and were extracted from the
        Adamorobe and Berbey Sign Language corpora <ptr target="#nyst2012a"/>
        <ptr target="#nyst2012b"/>.
        These corpora portray an additional challenge as the signers have been filmed
        in and around their homes, in natural conditions, outside of a studio, with
        strongly varying brightness and background noise. Furthermore, they may contain
        signing from one and two people at the same time. As a result, they can be
        considered as one of the hardest corpora to perform classification tasks. It
        is arguable that if the methods developed in this study can perform reasonably
        well on corpora of such poor conditions, then they can be applied to any sign
        language corpus under better settings.</p>
        <p>Additional videos from YouTube with higher quality have been selected
        for testing purposes too
        <note><ref target="https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s">https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s</ref></note>.
        For the first task of this study, the
        original data set was split into a training and testing set of 6,150 and 1,655
        frames respectively and the labels were one hot encoded (i.e. signing as 1 and
        not-signing as 0).</p>
        <p>After a successful training of the first prediction model, the tool was
        applied on a different part of the corpora. The predicted signing sequences
        were manually labeled as one- or two-handed signs. Together with randomly
        selected not-signing sequences (as predicted by the first tool), they formed
        a second data set. The size of this data set was slightly larger than the
        previous one: 10,120 frames in total.</p>
        </div>
        <div><head>3.2 Pre-processing</head>

        <p>Using OpenPose, the pixel coordinates of the hands, elbows, shoulders and
        head were extracted from each frame. In the case of the handshape recognition
        module, the fingers joints coordinates were additionally extracted. We avoided
        using the finger extraction module of OpenPose on the first two parts of the
        study as that would have increased the computational time significantly. The
        positions of the rest of the body joints were disregarded as most of the time
        they were out of the frame bounds. Although the quality of the frames was
        poor, it created an advantage for the pose estimation framework, reducing the
        computational time to a reasonable level.</p>

        </div>
        <div><head>3.3 Tool 1: Manual Activation</head>

        <p>The first tool is a temporal segmentation method to predict the begin and
        end frames of a sign sequence in a video sample. Thus, it is important to
        compare the performance of multiple different machine learning algorithms
        consistently. Four classification methods were used, namely: Support Vector
        Machines (SVM), Random Forests (RF), Artificial Neural Networks (ANN) and
        Extreme Gradient Boosting (XGBoost). The majority of these algorithms have
        been extensively used in machine learning studies as well as in sign language
        applications <ptr target="#agha2018"/>. Performance was measured using the Area
        Under the Receiver Operating Characteristics (AUROC) to validate each model.
        The AUC is a performance measurement specifically designed for binary (i.e.
        two class) classification problems. In general, it expresses how well a model
        is capable of distinguishing between classes, for example whether someone
        is signing in a video fragment or not. A model that makes random predictions
        will have an AUC of 0.5, a perfect model will have an AUC of 1. AUC stands
        for 'Area under the Receiver Operating Characteristic (ROC) Curve’, the curve
        of True Positive Rate (probability of detection) versus False Positive Rate
        (probability of false alarms). It is better than just accuracy, i.e. percentage
        of correct predictions by the model, because it is not dependent on the
        relative amount of positives, i.e. percentage of total videos with signs in our
        case. We searched for the optimal setting of the various classification method
        parameters by exhaustive testing of the possible parameter settings and testing
        the performance on a validation set ('grid search’, searching the 'grid’ of
        possible parameter values).</p>
        </div>
        <div><head>3.4 Tool 2: Number of Hands</head>
        <p>The second tool’s goal is to predict not only if a person is signing or
        not, but also to identify the number of hands involved (one- or two-handed).
        We hypothesized that this task is more complex than before, thus we considered
        it as a time-series problem. By using a sliding window technique, the original
        data set was parsed to form new training sets, where different possible
        frame intervals (1,2,3,5 and 10) were tested. Furthermore, similar (to some
        extend) classification methods with Tool 1 have been used
        <note>Linear Regression (LR), Decision Trees (CART), Support Vector Machines
        (SVM), Random Forest (RF) and Gradient Boosting (GBM).</note>.</p>

        <p>Moreover, recent studies in the sign language recognition field suggest that
        the use of Long-Short-Term-Memory (LSTM) networks can yield accurate results.
        LSTM is an artificial recurrent neural network (RNN) architecture used in the
        field of deep learning. Unlike standard feedforward neural networks (like the
        one tested in Tool 1) LSTM has feedback connections. It can not only process
        single data points, but also understand patterns in entire sequences of data,
        by combining its internal state resulting from previous input with a new
        input data item. In our case, instead of predicting whether a specific pose
        belongs into a class, we investigate whether a sequence of poses can be used
        for the same purpose. In this part of the study an LSTM network with different
        layer units as well as sliding window intervals has also been tested and
        compared with the above traditional machine learning classifiers. The overall
        architecture and technical details of the LSTM network can be found in Appendix
        A.</p>
        </div>
        <div><head>3.5 Tool 3: Handshape</head>
        <p>The
        handshape recognition module was considered a so-called unsupervised learning
        problem as no ground truth information regarding this feature was available
        prior to the experiment, i.e. in contrast to the previous two problems we did
        not know what classes (handshapes) to detect. Such an unsupervised learning
        method can be useful in other newly compiled sign language or gestural corpora
        where there is no information regarding the different handshapes presented by
        the signers in the video. Additionally, an unsupervised learning method can be
        useful in other newly compiled sign language or gestural corpora where there
        is no information regarding the different handshapes presented by the signers
        in the video. We approached this as a clustering task: can we find groups of
        signs that were similar. Two different clustering methods have been tested:
        K-means and DBSCAN. The first clustering method was chosen for its simplicity
        as well as its fast implementation on the Python library that was utilized
        (namely scikit-learn). However, as the complexity of the data is unknown and
        it is case sensitive, it was decided to employ Density-Based Spatial Clustering
        of Applications with Noise (DBSCAN) as an alternative option. Given a set of
        points in some space, DBSCAN groups together points that are closely packed
        together, marking as outliers the ones that lie alone in low-density regions.
        This clustering method is one of the most common clustering algorithms.</p>
        <p>Determining the optimal number of clusters (i.e. total number of expected
        handshapes) is a crucial issue in clustering methods such as K-means, which
        requires the user to specify the number of clusters <emph>k</emph> to be
        generated. The definition of clusters is done so that the total within-cluster
        sum of square (WSS) is minimized, hence, in this study the <emph>elbow
        method</emph> was utilized to estimate the number of clusters.</p>

        <div><head>3.5.1 Hand Normalization</head>

        <p>Since the output of OpenPose contains the raw x, y pixel positions for the
        different finger joints, it is important to normalize them before applying the
        clustering method. To do so, the angle of the vector between the elbow and the
        wrist of the right hand is calculated. Subsequently, the coordinates of the
        finger joints positions are rotated to be in parallel on the horizontal axis
        and normalized so that their averaged location is at the origin.
        <ref target="figure01">Figure 1</ref> shows
        the output of the overall normalization process. All
        experiments were conducted using one machine with a hexa-core processor
        (Intel Core i7-3930K) and 4GB RAM. The models are implemented using the Python
        libraries scikit-learn <ptr target="#pedregosa2011"/> and Keras
        <ptr target="#chollet2015"/> for
        their fast and easy implementation.</p>

        <figure xml:id="figure01">
          <head>Signer's hand normalization is done based on the angle between the horizontal axis and the vector of the elbow and wrist coordinates. The finger joints are rotated according to that angle in order to be in parallel to the horizontal axis and scaled so that their average location is at the origin.</head>
          <graphic url="resources/images/figure01.jpg"/>
        </figure>

        </div></div></div>

        <div><head>4. Results</head>

        <p>The results section consists of three parts, the first part (Section
        4.1) discusses the results of the analysis regarding the manual activation
        prediction. ﻿Section 4.2 discusses the results regarding the classification of
        one- and two-handed signing sequences. Last but not least, Section 4.3 presents
        the result regarding the handshape distribution using different clustering
        methods.</p>

        <div><head>4.1 Tool 1: Manual Activation</head>
        <p>All classifiers performed adequately well, apart from the Support Vector
        Machines (AUC: 0.80) (<ref target="table01">Table 1</ref>). Extreme Gradient Boosting (XGBoost) showed
        the highest AUC score at 0.92<note>eta: 0.23, gamma: 3, lambda: 2, max.
        delta step: 4, max. depth: 37 and min. child weight: 4</note>. <ref target="figure02">Figure 2</ref>
        presents the AUROC curve after
        a 10-fold cross-validation. The Artificial Neural Network was found to perform
        sufficiently well (AUC: 0.88). By exploring the importance of each feature on
        the prediction of the model we observe that the y and x pixel coordinates of
        the dominant (i.e. right) hand are on the top two positions (<ref target="table02">Table 2</ref>).</p>

        <figure xml:id="figure02">
          <head>10-fold cross validation of Extreme Gradient Boosting (XGBoost) classifier for the prediction of manual activation.</head>
          <graphic url="resources/images/figure02.jpg"/>
        </figure>

        <p>The fact that the Artificial Neural Network turned out to be a less
        efficient approach than the XGBoost can be accounted to the small training
        data set. Typically, Neural Networks require a lot more training data than
        traditional machine learning algorithms. Additionally, designing a network
        that correctly encodes a domain specific problem is challenging. In most cases,
        competent architectures are only reached when a whole research community is
        working on those problems, without short-term time constraints. Fine-tuning
        such a network would require time and effort that reach beyond the scope of
        this study.</p>
        <p>To account for multiple people signing in one frame, an extra module was
        added. This module creates bounding boxes around each person recognized by
        OpenPose, normalizes the positions of the body joints and runs the classifier.
        This process makes it possible to classify sign occurrences for multiple people
        irrespective of their positions in a frame (<ref target="figure04">Figure 4</ref>).</p>

        <p>Once all the frames have been classified, the "cleaning up" and annotation
        phase starts. A sign occurrence is annotated only if at least 12 consecutive
        frames have been classified as "signing". That way we account for the false
        positive errors. This sets the stage for the annotation step. Using the PyMpi
        python library <ptr target="#lubbers2013"/> the classifications are translated
        into annotations that can be imported directly to ELAN, a standard audio and
        video annotation tool <ptr target="#sloetjes2008"/>. <ref target="figure03">Figure 3</ref> shows the
        result of the overall outcome.</p>



        <table xml:id="table01">
          <head>AUC scores of all the classifiers tested for manual activation prediction</head>
          <row>
          <cell><hi rend="bold">Classifier</hi></cell>
          <cell><hi rend="bold">AUC score</hi></cell>
          </row>
          <row>
            <cell>Artificial Neural Network (ANN)</cell>
            <cell>0.88</cell>
          </row>
          <row>
            <cell>Random Forest (RF)</cell>
            <cell>0.87</cell>
          </row>
          <row>
            <cell>Support Vector Machines (SVM)</cell>
            <cell>0.78</cell>
          </row>
          <row>
            <cell>Extreme Gradient Boosting (XGBoost)</cell>
            <cell>0.92</cell>
          </row>
        </table>

        <table xml:id="table02">
          <head>Importance of each feature during manual activation as predicted by the Extreme Gradient Boosting classifier</head>
          <row>
            <cell><hi rend="bold">Weight</hi></cell>
            <cell><hi rend="bold">Feature</hi></cell>
          </row>
          <row>
            <cell>0.1410</cell>
            <cell>Right wrist y</cell>
          </row>
          <row>
            <cell>0.1281</cell>
            <cell>Right wrist x</cell>
          </row>
          <row>
            <cell>0.0928</cell>
            <cell>Left wrist y</cell>
          </row>
          <row>
            <cell>0.0917</cell>
            <cell>Left wrist x</cell>
          </row>
          <row>
            <cell>0.0717</cell>
            <cell>Nose x</cell>
          </row>
          <row>
            <cell>0.0658</cell>
            <cell>Left shoulder x</cell>
          </row>
          <row>
            <cell>0.0623</cell>
            <cell>Left elbow y</cell>
          </row>
          <row>
            <cell>0.0588</cell>
            <cell>Right elbow y</cell>
          </row>
          <row>
            <cell>0.0552</cell>
            <cell>Nose y</cell>
          </row>
          <row>
            <cell>0.0517</cell>
            <cell>Left shoulder y</cell>
          </row>
          <row>
            <cell>0.0482</cell>
            <cell>Left elbow x</cell>
          </row>
          <row>
            <cell>0.0482</cell>
            <cell>Right elbow x</cell>
          </row>
          <row>
            <cell>0.0482</cell>
            <cell>Right shoulder y</cell>
          </row>
          <row>
            <cell>0.0364</cell>
            <cell>Right shoulder x</cell>
          </row>
        </table>

        <figure xml:id="figure03">
          <head>Final output of the manual activation tool as seen in ELAN. Signing sequences have been given a 'signing' gloss for readability. This attribute can be easily changed to produce empty glosses.</head>
          <graphic url="resources/images/figure03.jpg"/>
        </figure>

        <figure xml:id="figure04">
          <head>Bounding boxes are calculated in order to normalize the body joint coordinates for each signer. After this process the normalized coordinates are passed to the XGBoost classifier (here tested on a YouTube video: <ref target="https://www.youtube.com/watch?v=NR">https://www.youtube.com/watch?v=NR</ref>).</head>
          <graphic url="resources/images/figure04.jpg"/>
        </figure>

        </div>

        <div><head>4.2 Tool 2: Number of Hands</head>

        <p>The second tool is responsible for not only recognizing whether a person
        in a video is signing but also if the sign is one or two-handed. We have
        previously hypothesized that this is a more complex task than the previous
        binary classification. Results on the accuracy of all the classifiers suggest
        that it is not as intricate as initially thought of; the higher the sliding
        window interval, the lower the accuracy of the model. As seen in <ref target="figure05">Figure 5</ref> of
        all classifiers tested, Random forest had the highest accuracy at the sliding
        window interval of 1 frame at a time. Similarly to the previous experiment, a
        frame-to-frame prediction can produce the highest results.</p>
        <p>Furthermore, the results regarding the Long-Short-Term-Memory networks
        (<ref target="figure06">Figure 6</ref>) suggest that the highest accuracy can be achieved at a sliding
        window interval of 56 frames and at a hidden layer size of 8 units. However,
        such a high window interval contains more than one sign, as the average length
        of a sign is approximately 14 frames. This discrepancy can be caused due to the
        architectural properties of the LSTM network. The average length of the signs
        is too small for the network to converge. The LSTM units needed more timesteps
        in order to prevent overfitting to the data. This property in addition to the
        small dataset used to train the network caused this anomaly.</p>
        <p>Although the tool performs well on predicting whether a sign is one-
        or two-handed (using a Random Forest classifier) there are cases were the
        output is not as expected. In particular, cases where there is a two-handed
        symmetrical sign produced, the tool fails to accurately predict the correct
        class. It is likely that such signs were under-presented in our data set, thus
        resulting in poor classification.</p>
        <p><img src="media/image6.png" style="width:6.56458in;height:3.33264in" /></p>

        <table>
        <tbody>
        <tr class="odd">
        <td></td>
        <td></td>
        <td></td>
        </tr>
        <tr class="even">
        <td></td>
        <td></td>
        <td></td>
        </tr>
        </tbody>
        </table>

        <figure xml:id="figure05">
          <head>Accuracy of different classifiers in various sliding window intervals in order to predict whether a sequence contains a one- or a two- handed sign or not signing at all. At a sliding window interval of 1 (a), Random Forest shows the highest accuracy.</head>
          <graphic url="resources/images/figure05.png"/>
        </figure>

        <p><img src="media/image12.jpeg"
        style="width:5.43661in;height:3.29352in" /></p>

        <figure xml:id="figure06">
          <head>Accuracy of Long-Short-Term-Memory networks on the test set with different sliding window intervals (x) and hidden layer sizes (8,16,32,64,256).</head>
          <graphic url="resources/images/figure06.jpg"/>
        </figure>


        </div>

        <div><head>4.3 Tool 3: Handshape</head>

        <p>In order to understand the distribution of the different handshapes
        presented in a video, Principal Component Analysis (PCA) was utilized on all
        the normalized finger joint coordinates for all the frames at once (<ref target="figure07">Figure 7a</ref>).
        This process allows us to reduce the dimensionality of the data while retaining
        as much as possible of the variance in the dataset. Each multidimensional array
        of the extracted finger joints positions, for each frame, has been reduced to
        a single x,y coordinate. The result already suggests that there are regions
        dense enough to be considered different clusters. The utilized elbow method
        suggested that at k=5 the highest classification could be achieved (<ref target="figure07">Figure 7b</ref>).
        On the video sample used in our study that number seemed to reflect the proper
        amount of discerned handshapes. However, as OpenPose captures all the finger
        configurations in each frame it is at the linguist’s discretion to decide
        on when a handshape is significantly different from another. Additionally,
        experiments to optimize the hyperparameters (eta, min samples and leaf
        size) for the DBSCAN failed to create an accurate clustering (<ref target="figure07">Figure 7c</ref>).
        Subsequently, the module creates annotation slots for the different handshapes
        in the video and adds an overlay containing the number of the predicted cluster
        on each frame.</p>
        <p>However, special consideration must be given to the overall handshape
        recognition module. Although the hand normalization process prepares the finger
        joints adequately enough to be used in the clustering methods, it fails to
        account for hands perpendicular to the camera’s point of view. Additionally,
        handshapes that are similar to each other but are rotated towards or outwards
        of the signer’s body will most probably clustered differently. Some of these
        limitations can be solved by manually editing the cluster numbers prior to the
        annotation process.</p>
        <p>In its current form, this method can already be used to either fully
        annotate the handshapes in a video sample or be used in different samples
        and treated as weakly annotated data in order to be used in other handshape
        classifiers similarly to Koller’s et al. study <ptr target="#koller2016"/>.</p>

        <p><img src="media/image13.png" style="width:6.64815in;height:2.0346in" /></p>
        <table>
        <tbody>
        <tr class="odd">
        <td></td>
        <td></td>
        <td></td>
        </tr>
        </tbody>
        </table>

        <figure xml:id="figure07">
          <head>Visualizations produced by the handshape recognition module. Principal component analysis (a) can be used to reduce the dimensionality of the finger joints coordinates. Two clustering methods, namely K-means (b) and DBSCAN (c), can be used to detect the different handshapes presented in a video sample.</head>
          <graphic url="resources/images/figure07.png"/>
        </figure>

        <figure xml:id="figure08">
          <head>Different handshapes recognized by the handshape recognition module using K-means.</head>
          <graphic url="resources/images/figure08.png"/>
        </figure>

        </div></div>
        <div><head>5. Discussion</head>

        <p>In this study we have presented three different tools that can be used
        to assist the annotation process of sign language corpora. The first tool
        proved to be robust on the task of classification of manual activation even
        when the corpora are noisy, of poor quality and most importantly containing
        more than one signer. This eliminates the preprocessing stage that many sign
        language corpora have to endure where either dedicated cameras per signer are
        utilized or manually cropping the original video. As a result, a more natural
        filming process can be applied. One limitation regarding our methodology
        is that at its current state is not possible to account for individual sign
        temporal classification. Reaching such level would require to fuse additional
        information into the training sets which in most cases might be language
        specific. However, it is possible to get a per sign prediction when the "number
        of hands involved" feature changes.</p>
        <p>The most striking observation to emerge from our methodology is that there
        is no necessity of having massive training sets for the classification of
        low-level features (such as manual activation and number of hands involved).
        In contrast to earlier studies using neural networks for sign language
        recognition
        <ptr target="#aitpayev2016"/>
        <ptr target="#pigou"/>
        <ptr target="#zhang2013"/>,
        we used a proportionally smaller dataset. Additionally, this is the first
        time to our knowledge where corpora outside of studio conditions have been
        used to train and most importantly test models and tools for sign language
        automatic annotation. Furthermore, such findings can be applied in other
        studies as well. It is a common misconception that only large data sets can
        be used for analysis. Such a trend, although true for deep learning purposes,
        can be daunting for digital humanities researchers without in depth data
        science knowledge. In our study, we have shown that even with a small and noisy
        dataset of visual materials, researchers can use machine learning algorithms
        to effectively extract meaningful information. Our testing in West African
        Sign Language corpora showed that such frameworks can work effectively in
        different skin color participants lifting possible bias by previously developed
        algorithms.</p>
        <p>There are few limitations regarding our methodologies, particularly
        with respect to the handshape distribution module. Low quality video and
        consequently framerate seem to affect the robustness of OpenPose. As a result,
        finger joint prediction can be noisy and of low confidence. Additionally,
        we observed that finger joints could not be predicted when the elbow was
        not visible in the frame, and thus, losing that information. In our study we
        treated all predicted joints equally but it is necessary for future research
        to include the prediction confidence interval as an additional variable.
        Furthermore, on the current output from OpenPose it is difficult to extract
        the palm orientation attribute meaning that differently rotated handshapes
        might result in the same cluster. Future research will concentrate on fixing
        that issue as well as creating an additional tool for the annotation of this
        feature.</p>
        <p>In the sign language domain, researchers can use our tools to recognize
        the times of interest and basic phonological features on newly compiled
        corpora. Additionally, such extracted features can be further used to measure
        variation on different sign languages or signers, for example, to measure the
        distribution of one- and two-handed signs or particular handshapes. Moreover,
        other machine or deep learning experiments can benefit from our tools by using
        them to extract only the meaningful information from the corpora during the
        data gathering process, thus reducing possible noise in the datasets. Our tools
        can also be used towards automatic gloss suggestion. A future model can search
        only the signing sequences predicted by our tool rather than "scanning" the
        whole video corpus, and consequently making it more efficient.</p>
        <p>Outside the sign language domain, the results have further strengthened our
        confidence that pre-trained frameworks can be used to help extract meaningful
        information from audio-visual materials. In particular, OpenPose can be a
        useful asset when human activity needs to be tracked and recognized in a video
        without the need of special hardware setups. Its accurate tracking allows
        researchers to use it in videos compiled outside studio conditions. As a
        result, studies in the audio-visual domain can benefit from community-created
        materials involving natural and unbiased communication. Using our tools,
        these study areas can analyze and classify human activity beyond the sign
        language discipline in large scale cultural archives or specific domains
        such as gestural research, dance or theater and cinema related studies, to
        name but a few. For example, video analyses in gestural and media studies can
        benefit from such an automatic approach to find relevant information regarding
        user-generated data on social media and other popular platforms.</p>
        <p>Finally, due to the cumbersome installation process of OpenPose for
        the majority of SL linguists, we have decided to implement part of the
        tools in an online collaborative environment on a cloud service provided
        by Google (i.e. Google Colab). In this environment a temporary instance
        of OpenPose can be installed along with our developed python modules. In
        a simple step-based manner, the researcher can upload the relevant videos
        and download the automatically generated annotation files. Find the link
        to this Colab in the footnote below
        <note><ref target="https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA">https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA</ref></note>.
        Additionally, we have
        a created another environment for re-training purposes. By doing so, the
        researcher can re-train the models on his or her particular data and ensure the
        aforementioned accuracy on
        them<note><ref target="https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y">https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y</ref></note>.</p>

        </div>
        <div><head>Conclusion</head>
        <p>To summarise, glossing sign language corpora is a cumbersome and
        time-consuming task. Current approaches to automatize parts of this process
        need special video recording devices (such as Microsoft Kinect), large amount
        of data in order to train deep learning architectures to recognize a set of
        signs and can be prone to skin-color bias. In this study we explored the use of
        a pre-trained pose estimation framework created by Cao et al. <ptr target="#cao2017"/> in order
        to create three tools and methods to predict sign occurrences, number of hands
        involved, and handshapes. The results show that four minutes of annotated data
        are adequate enough to train a classifier (namely XGBoost) to predict whether
        one or more persons are signing or not as well as the number of hands used
        (using Random Forest). Additionally, we examined the use of K-means and DBSCAN
        as clustering methods to detect the different handshapes presented in the
        video. Because of the low complexity of the finger joint data extracted from
        the pose estimation library, K-means was found to produce accurate results.</p>
        <p>The significance of this study lies in the fact that the tools created
        do not rely on specialized cameras nor require large amount of information
        to be trained. Additionally, they can be easily used by researchers without
        developing skills and adjusted to work in any kind of sign language corpus
        irrespective of its quality or the number of people in the video. Finally, they
        have the potential to be extended and used in other audio-visual material that
        involve human activity such as gestural corpora.</p>
        </div>

        <div><head>Appendix A</head>
        <p>
        The input shape of the LSTM network trained to recognize the "number of hands”
        (Tool 2) feature is a three dimensional array that can be defined as:
        [samples × timesteps × features] where features is a 2 dimensional array of
        [21 × 2] containing the pixel x,y coordinates of the finger joints and timesteps
        are the sliding window interval. Two Dense layers of 7 and 1 unit respectively
        follow the previous Bidirectional LSTM layer. The activation function used is
        ’ReLU’ and the dropout rate at 0.4. The architecture that produced the highest
        results for this network can be seen in <ref target="figure09">Figure 9</ref>.
        </p>
        <figure xml:id="figure09">
          <head>Architecture of the Long-Short-Term-Memory network trained for Tool 2.</head>
          <graphic url="resources/images/figure09.png"/>
        </figure>
        </div>
      </body>

      <!-- BACK TEXT -->
      <back>
        <listBibl>
          <bibl xml:id="agha2018" label="Agha et al. 2018">Agha, R. A. A. R., Sefer, M. N. and Fattah, P. (2018). <title rend="quote">A Comprehensive Study on Sign Languages Recognition Systems Using (SVM, KNN, CNN and ANN)</title>. <title rend="italic">First International Conference on Data Science, E-Learning and Information Systems</title>. (DATA ’18). Madrid, Spain: ACM, pp. 28:1–28:6.</bibl>
          <bibl xml:id="aitpayev2016" label="Aitpayev et al. 2016">Aitpayev, K., Islam, S. and Imashev, A. (2016). <title rend="quote">Semi-automatic annotation tool for sign languages</title>. <title rend="italic">2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)</title>. Baku, Azerbaijan: IEEE, pp. 1–4.</bibl>
          <bibl xml:id="andriluka2014" label="Andriluka et al. 2014">Andriluka, M., Pishchulin, L., Gehler, P. and Schiele, B. (2014). <title rend="quote">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>. <title rend="italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>.</bibl>
          <bibl xml:id="buehler2009" label="Buehler et al. 2009">Buehler, P., Zisserman, A. and Everingham, M. (2009). <title rend="quote">Learning sign language by watching TV (using weakly aligned subtitles)</title>. <title rend="italic">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>. pp. 2961–68.</bibl>
          <bibl xml:id="cao2017" label="Cao et al. 2017">Cao, Z., Simon, T., Wei, S.-E. and Sheikh, Y. (2017). <title rend="quote">Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields</title>. <title rend="italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>. Honolulu, HI: IEEE, pp. 1302–10.</bibl>
          <bibl xml:id="charles2014" label="Charles et al. 2014">Charles, J., Pfister, T., Everingham, M. and Zisserman, A. (2014). <title rend="quote">Automatic and Efficient Human Pose Estimation for Sign Language Videos</title>. <title rend="italic">International Journal of Computer Vision</title>, 110(1): 70–90.</bibl>
          <bibl xml:id="chollet2015" label="Chollet 2015">Chollet, F. and others (2015). <title rend="italic">Keras</title>. <ref target="https://keras.io">https://keras.io</ref>.</bibl>
          <bibl xml:id="cooper2007" label="Cooper and Bowden 2007">Cooper, H. and Bowden, R. (2007). <title rend="quote">Large Lexicon Detection of Sign Language</title>. In Lew, M., Sebe, N., Huang, T. S. and Bakker, E. M. (eds), <title rend="italic">Human–Computer Interaction</title>. (Lecture Notes in Computer Science). Springer Berlin Heidelberg, pp. 88–97.</bibl>
          <bibl xml:id="cooper2011" label="Cooper et al. 2011">Cooper, H., Holt, B. and Bowden, R. (2011). <title rend="quote">Sign Language Recognition</title>. In Moeslund, T. B., Hilton, A., Krüger, V. and Sigal, L. (eds), <title rend="italic">Visual Analysis of Humans: Looking at People</title>. London: Springer London, pp. 539–62.</bibl>
          <bibl xml:id="dreuw2008" label="Dreuw and Ney 2008">Dreuw, P. and Ney, H. (2008). <title rend="quote">Towards automatic sign language annotation for the ELAN tool</title>. <title rend="italic">LREC Workshop: Representation and Processing of Sign Languages</title>. Marrakech, Morocco, pp. 50–53.</bibl>
          <bibl xml:id="eichner2009" label="Eichner and Ferrari 2009">Eichner, M. and Ferrari, V. (2009). <title rend="quote">Better appearance models for pictorial structures</title>. <title rend="italic">British Machine Vision Conference 2009</title>. London, UK: British Machine Vision Association, pp. 3.1-3.11.</bibl>
          <bibl xml:id="eichner2012" label="Eichner et al. 2012">Eichner, M., Marin-Jimenez, M., Zisserman, A. and Ferrari, V. (2012). <title rend="quote">2D Articulated Human Pose Estimation and Retrieval in (Almost) Unconstrained Still Images</title>. <title rend="italic">International Journal of Computer Vision</title>, 99(2): 190–214.</bibl>
          <bibl xml:id="fang2004" label="Fang et al. 2004">Fang, G., Gao, W. and Zhao, D. (2004). <title rend="quote">Large Vocabulary Sign Language Recognition Based on Fuzzy Decision Trees</title>. <title rend="italic">Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions On</title>, 34: 305–14 doi:10.1109/TSMCA.2004.824852.</bibl>
          <bibl xml:id="farhadi2007" label="Farhadi et al. 2007">Farhadi, A., Forsyth, D. and White, R. (2007). <title rend="quote">Transfer Learning in Sign language</title>. <title rend="italic">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>. Minneapolis, MN, USA, pp. 1–8.</bibl>
          <bibl xml:id="felzenszwalb2005" label="Felzenszwalb and Huttenlocher 2005">Felzenszwalb, P. F. and Huttenlocher, D. P. (2005). <title rend="quote">Pictorial Structures for Object Recognition</title>. <title rend="italic">International Journal of Computer Vision</title>, 61(1): 55–79.</bibl>
          <bibl xml:id="forster2012" label="Forster 2012">Forster, J., Schmidt, C., Hoyoux, T., Koller, O., Zelle, U., Piater, J. and Ney, H. (2012). <title rend="quote">RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus</title>. Istanbul, Turkey, pp. 3785–3789.</bibl>
          <bibl xml:id="johnson2009" label="Johnson and Everingham 2009">Johnson, S. and Everingham, M. (2009). <title rend="quote">Combining discriminative appearance and segmentation cues for articulated human pose estimation</title>. <title rend="italic">2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</title>. pp. 405–12.</bibl>
          <bibl xml:id="johnston2010" label="Johnston 2010">Johnston, T. (2010). <title rend="quote">From archive to corpus: Transcription and annotation in the creation of signed language corpora</title>. <title rend="italic">International Journal of Corpus Linguistics</title>, 15(1): 106–31 doi:10.1075/ijcl.15.1.05joh. <ref target="http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh">http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh</ref> (accessed 19 May 2020).</bibl>
          <bibl xml:id="kelly2009" label="Kelly et al. 2009">Kelly, D., Reilly Delannoy, J., Mc Donald, J. and Markham, C. (2009). <title rend="quote">A framework for continuous multimodal sign language recognition</title>. <title rend="italic">Proceedings of the 2009 International Conference on Multimodal Interfaces</title>. pp. 351–358.</bibl>
          <bibl xml:id="koller2016" label="Koller et al. 2016">Koller, O., Ney, H. and Bowden, R. (2016). <title rend="quote">Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data is Continuous and Weakly Labelled</title>. <title rend="italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>. Las Vegas, NV, USA: IEEE, pp. 3793–802.</bibl>
          <bibl xml:id="kumar2017" label="Kumer 2017">Kumar, N. (2017). <title rend="quote">Motion trajectory based human face and hands tracking for sign language recognition</title>. <title rend="italic">2017 4th IEEE Uttar Pradesh Section International Conference on Electrical, Computer and Electronics (UPCON)</title>. Mathura: IEEE, pp. 211–16.</bibl>
          <bibl xml:id="lin2014" label="Lin 2014">Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L. and Dollár, P. (2014). <title rend="italic">Microsoft COCO: Common Objects in Context</title>.</bibl>
          <bibl xml:id="lubbers2013" label="Lubbers and Torreira 2013">Lubbers, M. and Torreira, F. (2013). <title rend="italic">A Python Module for Processing ELAN and Praat Annotation Files: Dopefishh/Pympi</title>. Python <ref target="https://github.com/dopefishh/pympi">https://github.com/dopefishh/pympi</ref>.</bibl>
          <bibl xml:id="moeslund2006" label="Moselund et al. 2006">Moeslund, T. B., Hilton, A. and Krüger, V. (2006). <title rend="quote">A survey of advances in vision-based human motion capture and analysis</title>. <title rend="italic">Computer Vision and Image Understanding</title>, 104(2): 90–126.</bibl>
          <bibl xml:id="nyst2012a" label="Nyst 2012">Nyst, V. A. S. (2012). <title rend="quote">A Reference Corpus of Adamorobe Sign Language</title>. A digital, annotated video corpus of the sign language used in the village of Adamorobe, Ghana.</bibl>
          <bibl xml:id="nyst2012b" label="Nyst et al. 2012">Nyst, V. A. S., Magassouba, M. M. and Sylla, K. (2012). <title rend="quote">Un Corpus de reference de la Langue des Signes Malienne II</title>. A digital, annotated video corpus of local sign language use in the Dogon area of Mali.</bibl>
          <bibl xml:id="pedregosa2011" label="Pedregosa et al. 2011">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). <title rend="quote">Scikit-learn: Machine Learning in Python</title>. <title rend="italic">Journal of Machine Learning Research</title>, 12: 2825–2830.</bibl>
          <bibl xml:id="pigou2015" label="Pigou et al. 2015">Pigou, L., Dieleman, S., Kindermans, P.-J. and Schrauwen, B. (2015). <title rend="quote">Sign Language Recognition Using Convolutional Neural Networks</title>. In Agapito, L., Bronstein, M. M. and Rother, C. (eds), <title rend="italic">Computer Vision - ECCV 2014 Workshops</title>. (Lecture Notes in Computer Science). Cham: Springer International Publishing, pp. 572–78.</bibl>
          <bibl xml:id="ramanan2007" label="Ramanan et al. 2007">Ramanan, D., Forsyth, D. A. and Zisserman, A. (2007). <title rend="quote">Tracking People by Learning Their Appearance</title>. <title rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>, 29(1): 65–81.</bibl>
          <bibl xml:id="roussos2012" label="Roussos et al. 2012">Roussos, A., Theodorakis, S., Pitsikalis, V. and Maragos, P. (2012). <title rend="quote">Hand Tracking and Affine Shape-Appearance Handshape Sub-units in Continuous Sign Language Recognition</title>. In Kutulakos, K. N. (ed), <title rend="italic">Trends and Topics in Computer Vision</title>, vol. 6553. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 258–72.</bibl>
          <bibl xml:id="sapp2010" label="Sapp et al. 2010">Sapp, B., Jordan, C. and Taskar, B. (2010). Adaptive pose priors for pictorial structures. <title rend="italic">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>. pp. 422–29.</bibl>
          <bibl xml:id="sivic2006" label="Sivic et al. 2006">Sivic, J., Zitnick, C. L. and Szeliski, R. (2006). <title rend="quote">Finding people in repeated shots of the same scene</title>. <title rend="italic">British Machine Vision Conference 2006</title>, vol. 3. Edinburgh: British Machine Vision Association, pp. 909–18.</bibl>
          <bibl xml:id="sloetjes2008" label="Sloetjes and Wittenburg 2008">Sloetjes, H. and Wittenburg, P. (2008). <title rend="quote">Annotation by category - ELAN and ISO DCR. Marrakech, Morocco</title>, p. 5 <ref target="https://tla.mpi.nl/tools/tla-tools/elan/">https://tla.mpi.nl/tools/tla-tools/elan/</ref>.</bibl>
          <bibl xml:id="starner1998" label="Starner et al. 1998">Starner, T., Weaver, J. and Pentland, A. (1998). <title rend="quote">Real-time American sign language recognition using desk and wearable computer based video</title>. <title rend="italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>, 20(12): 1371–75.</bibl>
          <bibl xml:id="yang2011" label="Yang and Ramanan 2011">Yang, Y. and Ramanan, D. (2011). <title rend="quote">Articulated pose estimation with flexible mixtures-of-parts</title>. <title rend="italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>. pp. 1385–92.</bibl>
          <bibl xml:id="zhang2013" label="Zhang et al. 2013">Zhang, C., Yang, X. and Tian, Y. (2013). <title rend="quote">Histogram of 3D Facets: A characteristic descriptor for hand gesture recognition.</title> <title rend="italic">2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>. pp. 1–8.</bibl>
        </listBibl>
      </back>
    </text>
    <!-- END TEXT -->

</TEI>
