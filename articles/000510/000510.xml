<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">

  <!-- BEGIN TEI HEADER ELEMENTS -->
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title type="article" xml:lang="en">Towards a User-Friendly Tool for Automated Sign
          Annotation: Identification and Annotation of Time Slots, Number of Hands, and Handshape </title>
        <dhq:authorInfo>
          <dhq:author_name>Manolis <dhq:family>Fragkiadakis</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>m.fragkiadakis@hum.leidenuniv.nl</email>
          <dhq:bio>
            <p>PhD student in the Center for Digital Humanities and Data Science Research Program at
              Leiden University, the Netherlands. His current research focuses on the study of
              automatic annotation as well as variation measurement for sign language corpora and
              dictionaries.</p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Victoria <dhq:family>Nyst</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>v.a.s.nyst@hum.leidenuniv.nl</email>
          <dhq:bio>
            <p>Associate professor working at the Leiden University Center for Linguistics in the
              Netherlands. Her research focuses on sign languages and gestures of deaf and hearing
              people in Africa, leading to the publication and analysis of a growing number of video
              corpora of West African sign languages, as well as of a dictionary app for Ghanaian
              Sign Language. </p>
          </dhq:bio>
        </dhq:authorInfo>
        <dhq:authorInfo>
          <dhq:author_name>Peter <dhq:family>van der Putten</dhq:family></dhq:author_name>
          <dhq:affiliation>Leiden University</dhq:affiliation>
          <email>p.w.h.van.der.putten@liacs.leidenuniv.nl</email>
          <dhq:bio>
            <p>Assistant professor at the Leiden Institute of Advanced Computer Science (LIACS),
              Leiden University, The Netherlands. His research borders on the intersection of
              machine learning and creative research, and he is a collaborator in the LCDS and SAILS
              university wide AI research programs; the Creative Intelligence Lab and the [A]social
              Creatures Lab, and the Media Technology MSc program. He is also a Director Decisioning
              Solutions at Pegasystems.</p>
          </dhq:bio>
        </dhq:authorInfo>
      </titleStmt>
      <publicationStmt>
        <publisher>Alliance of Digital Humanities Organizations</publisher>
        <publisher>Association for Computers and the Humanities</publisher>
        <idno type="DHQarticle-id">000510</idno>
        <idno type="volume">015</idno>
        <idno type="issue">1</idno>
        <date when="2021-03-05">05 March 2021</date>
        <dhq:articleType>article</dhq:articleType>
        <availability status="CC-BY-ND">
          <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <p>This is the source</p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <classDecl>
        <taxonomy xml:id="dhq_keywords">
          <bibl>DHQ classification scheme; full list available at<ref
              target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
              >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
        </taxonomy>
        <taxonomy xml:id="authorial_keywords">
          <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
        </taxonomy>
      </classDecl>
    </encodingDesc>
    <profileDesc>
      <langUsage>
        <language ident="en" extent="original"/>
      </langUsage>
      <textClass>
        <keywords scheme="#dhq_keywords">
          <list type="simple">
            <item/>
          </list>
        </keywords>
        <keywords scheme="#authorial_keywords">
          <list type="simple">
            <item/>
          </list>
        </keywords>
      </textClass>
    </profileDesc>
    <revisionDesc>
      <change when="2020-09-18" who="Taylor Arnold">Created file</change>
    </revisionDesc>
  </teiHeader>
  <!-- END TEI HEADER ELEMENTS -->

  <!-- BEGIN TEXT -->
  <text xml:lang="en" type="original">
    <!-- FRONT TEXT -->
    <front>
      <dhq:abstract>
        <p>The annotation process of sign language corpora in terms of glosses, is a highly
          labor-intensive task, but a condition for a reliable quantitative analysis. During the
          annotation process the researcher typically defines the precise time slot in which a sign
          occurs and then enters the appropriate gloss for the sign. The aim of this project is to
          develop a set of tools to assist the annotation of the signs and their formal features in
          a video irrespectively of its content and quality. Recent advances in the field of deep
          learning have led to the development of accurate and fast pose estimation frameworks. In
          this study, such a framework (namely OpenPose) has been used to develop three different
          methods and tools to facilitate the annotation process. The first tool estimates the span
          of a sign sequence and creates empty slots in an annotation file. The second tool detects
          whether a sign is one- or two-handed. The last tool recognizes the different handshapes
          presented in a video sample. All tools can be easily re-trained to fit the needs of the
          researcher.</p>
      </dhq:abstract>
      <dhq:teaser>
        <p>The aim of this project is to develop a set of tools to assist the annotation of the
          signs and their formal features in a video irrespectively of its content and quality.</p>
      </dhq:teaser>
    </front>

    <!-- BODY TEXT -->
    <body>
      <div>
        <head>Introduction</head>
        <p>While the majority of the studies in the field of digital humanities have been mostly
          text oriented, the evolution in computing power and technology has resulted in a shift
          towards multimedia-oriented studies. Recently, advances in computer vision have started to
          find practical applications in study domains outside of computer and data science. Video
          is one of the most important time-based media as it has the ability to carry large amount
          of digital information in a condensed form, and hence it serves as a rich medium to
          capture various forms of cultural expression. Automated processing and annotation of large
          numbers of videos is now becoming feasible due to the evolution of computer vision and
          machine learning.</p>
        <p>In sign language linguistics, a transition took place from paper-based materials to large
          video corpora to facilitate the study of the languages in question. Sign language corpora
          are mainly composed of video data. The primary goal of these video corpora is to study
          sign language functioning.</p>
        <p>The processing of sign languages usually involves requires a form of textual
          representation <ptr target="#dreuw2008"/>, most notably glosses for annotation. Sign
          language glosses are words from a spoken language. Uniquely identifying glosses by
          definition refer to a specific sign. Such ID glosses are an essential element for the
          quantitative analysis of a sign language corpus <ptr target="#johnston2010"/>. Typically,
          sign language linguists add glosses and other annotations to the video recordings with the
          use of a software tool (namely ELAN). ELAN allows researchers to add time-aligned
          annotations to a video. However, this task requires a lot of time and can be prone to
          errors.</p>
        <p>New advances in computer vision open up additional ways of studying videos containing
          sign language data, extracting formal representations of linguistic phenomena, and
          implementing these in computer applications, such as automatic recognition, generation,
          and translation. Using computer vision and machine learning enables quick and new ways of
          processing large sets of video data, which in turns makes it possible to address research
          questions that were not feasible before.</p>
        <p>This study is the first part of a project aiming at the creation of tools to automatize
          part of the annotation process of sign language video data. This paper presents the
          methodologies, tools and implementations of three functionalities: the detection of 1)
          manual activation 2) the number of hands involved and 3) the handshape distribution on
          sign language corpora.</p>
        <p>Recent developments in sign language recognition illustrate the advantages of machine and
          deep learning for tasks related to recognition and classification <ptr target="#agha2018"/>
          <ptr target="#cao2017"/>
          <ptr target="#pigou2015"/>. Nevertheless, current approaches are restricted in various
          ways, limiting their applicability in current sign language research. For example,
          training deep learning networks requires a vast amount of data as well as adequate
          computational power. These networks are usually trained in one sign language and they do
          not generalize well in other sign languages.</p>
        <p>Additionally, current approaches in sign language automatic annotation need manual
          annotation of the hands and body joints for the training of the recognizer models <ptr
            target="#aitpayev2016"/>
          <ptr target="#dreuw2008"/>. Moreover, the application of color and motion detection
          algorithms <ptr target="#kumar2017"/>, as feature extraction methods, can be susceptible
          to errors and possibly skin color bias. Finally, several hand tracking models only work on
          a particular type of recordings, for example, a signer wearing gloves, or recordings made
          with Microsoft’s Kinect <ptr target="#pigou2015"/>. As a result, these models are not
          usable for the majority of the existing sign language corpora which have been recorded
          with standard RGB cameras.</p>
        <p>Our methods have been developed and tested on two West African sign language corpora
          containing natural conditions with non-Caucasian signers. While most studies in the sign
          language recognition field have mainly concerned signers with light skin tones, little
          research has been conducted using darker skin tones. With the emergence of corpora
          compiled in African countries under challenging real-world conditions, and their
          contribution to the overall sign language community, it is of utmost importance to test
          how methods perform in such a domain. Alleviating biases and increasing diversity should
          be a top priority of any computer assisted study.</p>
        <p>In this study, a pre-trained deep learning pose estimation library developed by Cao et
          al. <ptr target="#cao2017"/> named OpenPose has been used to extract body and finger
          key-points. OpenPose has been trained and evaluated on two large datasets for multi-person
          pose estimation. The MPII human multi-person dataset <ptr target="#andriluka2014"/> and
          the COCO 2016 keypoints challenge dataset <ptr target="#lin2014"/> contain images of
          people of different age groups and ethnicities in diverse scenarios. As a result, OpenPose
          does not have a bias toward skin color. Additionally, its easy-to-use implementation makes
          it an ideal framework to be used by linguists with limited coding experience.</p>
        <p>The combination of the aforementioned pose estimation framework as well as the machine
          and deep learning architectures tested in this study, provides a robust approach towards
          automatic annotation. Current models and tools can be used in any sign language or
          gestural corpus independently of its quality, length and number of people in the video.
          These tools have been developed as python modules that can run automatically in a video
          and produce the relevant annotation files requiring minimal effort from the user. More
          generally, as large parts of our cultures nowadays are captured in video, our study serves
          as a case example of how intelligent machine learning techniques can serve digital
          humanities researchers by extracting semantics from large video collections.</p>
        <p>This article is structured as follows: Section 2 introduces the developments on the sign
          language recognition and automatic annotation fields. Section 3 describes the materials
          used in this study and the methodologies developed and applied for each tool separately.
          Section 4 presents the results for each experimental setup and tool. Section 5 contains
          the discussion and future work while Section 6 presents our conclusions. Finally, Appendix
          A presents the architecture and technical details of the Long-Short-Term-Memory Network
          trained for this study.</p>
      </div>
      <div>
        <head>2. Literature review</head>

        <p>In this section we present the studies conducted on the sign language recognition and
          automatic annotation field developed with depth sensors as well as standard RGB cameras.
          Additionally, we describe the developments of the human pose estimation field and we
          introduce the OpenPose framework that will be used in this article.</p>

        <div>
          <head>2.1 Sign Language Recognition and Automatic Annotation</head>

          <p>The primary goal of sign language recognition is to develop methods and algorithms to
            accurately identify a series of produced signs and to discern their meaning. The
            majority of studies have focused on recognizing those features and methods that can
            properly identify a sign out of a given set of possible signs. However, such methods can
            only be used on a particular set of signs and, thus, a specific sign language, which
            makes it harder to study the relationships between and evolution of various sign
            languages.</p>
          <p>An additional motivation behind Sign Language Recognition (SLR) is to build automatic
            sign language to speech or text translation systems to assist the communication between
            the deaf and hearing community <ptr target="#fang2004"/>. Moreover, SLR plays an
            important role in developing gesture-based human–computer interaction systems <ptr
              target="#kelly2009"/>. Sign language linguists have used such systems to facilitate
            the annotation process of sign language corpora in order to discern the different signs
            in a video recording and further study the linguistic phenomena presented.</p>
          <p>There are numerous studies dealing with the automated recognition of sign languages as
            clearly presented by Cooper et al. <ptr target="#cooper2007"/>, in their review study on
            the state-of-the-art in sign language recognition. However, the experiments presented in
            most of these studies are either hard to replicate, or they pose limitations as far as
            their applicability is concerned. For instance, most of these studies use depth sensors,
            most notably MS Kinect, to capture 3D images of the environment <ptr
              target="#aitpayev2016"/>
            <ptr target="#pigou2015"/>
            <ptr target="#zhang2013"/>. As a result, using the frameworks developed in these studies
            requires a machine with similar features as the one used for testing and most probably
            will only work for that sign language on which they have been trained.</p>
          <p>Recently, computer vision techniques have been applied to sign language recognition to
            overcome the aforementioned limitations. Roussos et al. <ptr target="#roussos2012"/>
            created a skin color probabilistic model to detect and track the hands of a signer on a
            video, while Cooper et al. <ptr target="#cooper2007"/> use this model to segment the
            hands and apply a classifier based on Markov models. However, systems based on skin
            color <ptr target="#buehler2009"/>
            <ptr target="#cooper2007"/>
            <ptr target="#farhadi2007"/>
            <ptr target="#starner1998"/> are prone to errors and have difficulties on tracking the
            hands and the signer’s features against cluttered backgrounds and in noisy conditions in
            general. Also, they do not work in videos with multiple signers.</p>
        </div>
        <div>
          <head>2.2 Human Pose Estimation</head>

          <p>Human pose estimation has been extensively studied due to its numerous applications on
            a number of different fields <ptr target="#moeslund2006"/>. Due to low computational
            complexity during inference, pictorial structures have been commonly used <ptr
              target="#felzenszwalb2005"/>
            <ptr target="#ramanan2007"/>
            <ptr target="#sivic2006"/> to model human pose. Recently, studies have focused on
            improving the appearance models used in these structures by modelling the individual
            body parts <ptr target="#eichner2009"/>
            <ptr target="#eichner2012"/>
            <ptr target="#johnson2009"/>
            <ptr target="#sapp2010"/>. Felzenszwalb and Huttenlocher <ptr target="#felzenszwalb2005"
            />, relying on the pictorial structure framework recommended a deformable part-based
            model. Additionally, Yang and Ramanan <ptr target="#yang2011"/> showed that a
            tree-structured model using a combination of deformable parts can be used in order to
            achieve accurate pose estimation. Furthermore, Charles et al. <ptr target="#charles2014"
            /> showed that human body joint positions can be predicted using a random forest
            regressor based on a co-segmentation process over all video frames.</p>
          <p>In general, most of the vision-based approaches developed for sign language recognition
            tasks utilizing pose estimation, have used the RWTH-PHOENIX-Weather data set <ptr
              target="#forster2012"/> to validate their models. This data set consists of weather
            forecast airings from the German public tv-station PHOENIX along with transcribed gloss
            annotations. However, it is a question to what extent such systems tested in this data
            set can be replicated with real-life conditions in the corpora. It is often the case
            that sign language and gestural corpora, especially the ones filmed outside of studio
            conditions, have bad quality, low brightness and often contain more than one person in
            the frame. These characteristics create an additional challenge to the tracking and
            prediction task.</p>

          <div>
            <head>2.2.1 OpenPose</head>

            <p>OpenPose is a real-time, open-source library for academic purposes for multi-person
              2D pose estimation. It can detect body, foot, hand and facial keypoints <ptr
                target="#cao2017"/>. Following a bottom-up approach (from an entire image as input
              to full body poses as output), it outperforms similar 2D body pose estimation
              libraries.</p>
            <p>A major advantage of the library is that it achieves high accuracy and performance
              regardless of the number of people in the image. Its high accuracy is performed by
              using a non-parametric representation of 2D vector fields. These fields encode the
              position and orientation of body parts over the image domain and their degree of
              association in order to learn to relate them to each individual.</p>
            <p>OpenPose is able to run on different operating systems and multiple hardware
              architectures. Additionally, it provides tools for visualization and output file
              generation. The output can be multiple json files containing all the pixel x, y
              coordinates of the body, hand and face joints. In this study the DEMO version on a
              CPU-only mode has been used to train our models. This choice was made in order to
              ensure that reproducibility can be easily achieved without the need for powerful
              computers from the linguist’s side.</p>

          </div>
        </div>
      </div>

      <div>
        <head>3. Materials and Methods</head>

        <p>This section describes the datasets used in our study as well as the pre-processing stage
          using OpenPose to extract the body joints’ pixel coordinates. Furthermore, we introduce
          the methods applied in the development of each tool. Special consideration is given on the
          handshape recognition module as an additional normalization part has been developed. </p>

        <div>
          <head>3.1 Data</head>

          <p>A data set of 7,805 frames in total (approximately 4 minutes) labeled as signing or not
            signing has been compiled for the first part of the study. The dimensions of the frames
            were 352 by 288 pixels and were extracted from the Adamorobe and Berbey Sign Language
            corpora <ptr target="#nyst2012a"/>
            <ptr target="#nyst2012b"/>. These corpora portray an additional challenge as the signers
            have been filmed in and around their homes, in natural conditions, outside of a studio,
            with strongly varying brightness and background noise. Furthermore, they may contain
            signing from one and two people at the same time. As a result, they can be considered as
            one of the hardest corpora to perform classification tasks. It is arguable that if the
            methods developed in this study can perform reasonably well on corpora of such poor
            conditions, then they can be applied to any sign language corpus under better
            settings.</p>
          <p>Additional videos from YouTube with higher quality have been selected for testing
            purposes too. For the first task of this study, the original data set was split into a
            training and testing set of 6,150 and 1,655 frames respectively and the labels were one
            hot encoded (i.e. signing as 1 and not-signing as 0).</p>
          <p>After a successful training of the first prediction model, the tool was applied on a
            different part of the corpora. The predicted signing sequences were manually labeled as
            one- or two-handed signs. Together with randomly selected not-signing sequences (as
            predicted by the first tool), they formed a second data set. The size of this data set
            was slightly larger than the previous one: 10,120 frames in total.</p>
        </div>
        <div>
          <head>3.2 Pre-processing</head>

          <p>Using OpenPose, the pixel coordinates of the hands, elbows, shoulders and head were
            extracted from each frame. In the case of the handshape recognition module, the fingers
            joints coordinates were additionally extracted. We avoided using the finger extraction
            module of OpenPose on the first two parts of the study as that would have increased the
            computational time significantly. The positions of the rest of the body joints were
            disregarded as most of the time they were out of the frame bounds. Although the quality
            of the frames was poor, it created an advantage for the pose estimation framework,
            reducing the computational time to a reasonable level.</p>

        </div>
        <div>
          <head>3.3 Tool 1: Manual Activation</head>

          <p>The first tool is a temporal segmentation method to predict the begin and end frames of
            a sign sequence in a video sample. Thus, it is important to compare the performance of
            multiple different machine learning algorithms consistently. Four classification methods
            were used, namely: Support Vector Machines (SVM), Random Forests (RF), Artificial Neural
            Networks (ANN) and Extreme Gradient Boosting (XGBoost). The majority of these algorithms
            have been extensively used in machine learning studies as well as in sign language
            applications <ptr target="#agha2018"/>. Performance was measured using the Area Under
            the Receiver Operating Characteristics (AUC) to validate each model. The AUC is a
            performance measurement specifically designed for binary (i.e. two class) classification
            problems. In general, it expresses how well a model is capable of distinguishing between
            classes, for example whether someone is signing in a video fragment or not. A model that
            makes random predictions will have an AUC of 0.5, a perfect model will have an AUC of 1.
            AUC stands for <soCalled>Area under the Receiver Operating Characteristic (ROC)
              Curve</soCalled>, the curve of True Positive Rate (probability of detection) versus
            False Positive Rate (probability of false alarms). It is better than just accuracy, i.e.
            percentage of correct predictions by the model, because it is not dependent on the
            relative amount of positives, i.e. percentage of total videos with signs in our case. We
            searched for the optimal setting of the various classification method parameters by
            exhaustive testing of the possible parameter settings and testing the performance on a
            validation set (<soCalled>grid search</soCalled>, searching the
              <soCalled>grid</soCalled> of possible parameter values).</p>
        </div>
        <div>
          <head>3.4 Tool 2: Number of Hands</head>
          <p>The second tool’s goal is to predict not only if a person is signing or not, but also
            to identify the number of hands involved (one- or two-handed). We hypothesized that this
            task is more complex than before, thus we considered it as a time-series problem. By
            using a sliding window technique, the original data set was parsed to form new training
            sets, where different possible frame intervals (1,2,3,5 and 10) were tested.
            Furthermore, similar (to some extend) classification methods with Tool 1 have been used
              <note>Linear Regression (LR), Decision Trees (CART), Support Vector Machines (SVM),
              Random Forest (RF) and Gradient Boosting (GBM).</note>.</p>

          <p>Moreover, recent studies in the sign language recognition field suggest that the use of
            Long-Short-Term-Memory (LSTM) networks can yield accurate results. LSTM is an artificial
            recurrent neural network (RNN) architecture used in the field of deep learning. Unlike
            standard feedforward neural networks (like the one tested in Tool 1) LSTM has feedback
            connections. It can not only process single data points, but also understand patterns in
            entire sequences of data, by combining its internal state resulting from previous input
            with a new input data item. In our case, instead of predicting whether a specific pose
            belongs into a class, we investigate whether a sequence of poses can be used for the
            same purpose. In this part of the study an LSTM network with different layer units as
            well as sliding window intervals has also been tested and compared with the above
            traditional machine learning classifiers. The overall architecture and technical details
            of the LSTM network can be found in Appendix A.</p>
        </div>
        <div>
          <head>3.5 Tool 3: Handshape</head>
          <p>The handshape recognition module was considered a so-called unsupervised learning
            problem as no ground truth information regarding this feature was available prior to the
            experiment, i.e. in contrast to the previous two problems we did not know what classes
            (handshapes) to detect. Such an unsupervised learning method can be useful in other
            newly compiled sign language or gestural corpora where there is no information regarding
            the different handshapes presented by the signers in the video. Additionally, an
            unsupervised learning method can be useful in other newly compiled sign language or
            gestural corpora where there is no information regarding the different handshapes
            presented by the signers in the video. We approached this as a clustering task: can we
            find groups of signs that were similar. Two different clustering methods have been
            tested: K-means and DBSCAN. The first clustering method was chosen for its simplicity as
            well as its fast implementation on the Python library that was utilized (namely
            scikit-learn). However, as the complexity of the data is unknown and it is case
            sensitive, it was decided to employ Density-Based Spatial Clustering of Applications
            with Noise (DBSCAN) as an alternative option. Given a set of points in some space,
            DBSCAN groups together points that are closely packed together, marking as outliers the
            ones that lie alone in low-density regions. This clustering method is one of the most
            common clustering algorithms.</p>
          <p>Determining the optimal number of clusters (i.e. total number of expected handshapes)
            is a crucial issue in clustering methods such as K-means, which requires the user to
            specify the number of clusters <emph>k</emph> to be generated. The definition of
            clusters is done so that the total within-cluster sum of square (WSS) is minimized,
            hence, in this study the <emph>elbow method</emph> was utilized to estimate the number
            of clusters.</p>

          <div>
            <head>3.5.1 Hand Normalization</head>

            <p>Since the output of OpenPose contains the raw x, y pixel positions for the different
              finger joints, it is important to normalize them before applying the clustering
              method. To do so, the angle of the vector between the elbow and the wrist of the right
              hand is calculated. Subsequently, the coordinates of the finger joints positions are
              rotated to be in parallel on the horizontal axis and normalized so that their averaged
              location is at the origin. <ref target="#figure01">Figure 1</ref> shows the output of
              the overall normalization process. All experiments were conducted using one machine
              with a hexa-core processor (Intel Core i7-3930K) and 4GB RAM. The models are
              implemented using the Python libraries scikit-learn <ptr target="#pedregosa2011"/> and
              Keras <ptr target="#chollet2015"/> for their fast and easy implementation.</p>

            <figure xml:id="figure01">
              <head>Signer's hand normalization is done based on the angle between the horizontal
                axis and the vector of the elbow and wrist coordinates. The finger joints are
                rotated according to that angle in order to be in parallel to the horizontal axis
                and scaled so that their average location is at the origin.</head>
              <graphic url="resources/images/figure01.png"/>
            </figure>

          </div>
        </div>
      </div>

      <div>
        <head>4. Results</head>

        <p>The results section consists of three parts, the first part (Section 4.1) discusses the
          results of the analysis regarding the manual activation prediction. ﻿Section 4.2 discusses
          the results regarding the classification of one- and two-handed signing sequences. Last
          but not least, Section 4.3 presents the result regarding the handshape distribution using
          different clustering methods.</p>

        <div>
          <head>4.1 Tool 1: Manual Activation</head>
          <p>All classifiers performed adequately well, apart from the Support Vector Machines (AUC:
            0.80) (<ref target="#table01">Table 1</ref>). Extreme Gradient Boosting (XGBoost) showed
            the highest AUC score at 0.92<note>eta: 0.23, gamma: 3, lambda: 2, max. delta step: 4,
              max. depth: 37 and min. child weight: 4</note>. <ref target="#figure02">Figure 2</ref>
            presents the ROC curve after a 10-fold cross-validation. The Artificial Neural Network
            was found to perform sufficiently well (AUC: 0.88). By exploring the importance of each
            feature on the prediction of the model we observe that the y and x pixel coordinates of
            the dominant (i.e. right) hand are on the top two positions (<ref target="#table02"
              >Table 2</ref>).</p>

          <figure xml:id="figure02">
            <head>10-fold cross validation of Extreme Gradient Boosting (XGBoost) classifier for the
              prediction of manual activation.</head>
            <graphic url="resources/images/figure02.jpg"/>
          </figure>

          <p>The fact that the Artificial Neural Network turned out to be a less efficient approach
            than the XGBoost can be accounted to the small training data set. Typically, Neural
            Networks require a lot more training data than traditional machine learning algorithms.
            Additionally, designing a network that correctly encodes a domain specific problem is
            challenging. In most cases, competent architectures are only reached when a whole
            research community is working on those problems, without short-term time constraints.
            Fine-tuning such a network would require time and effort that reach beyond the scope of
            this study.</p>
          <p>To account for multiple people signing in one frame, an extra module was added. This
            module creates bounding boxes around each person recognized by OpenPose, normalizes the
            positions of the body joints and runs the classifier. This process makes it possible to
            classify sign occurrences for multiple people irrespective of their positions in a frame
              (<ref target="#figure04">Figure 4</ref>).</p>

          <p>Once all the frames have been classified, the "cleaning up" and annotation phase
            starts. A sign occurrence is annotated only if at least 12 consecutive frames have been
            classified as "signing". That way we account for the false positive errors. This sets
            the stage for the annotation step. Using the PyMpi python library <ptr
              target="#lubbers2013"/> the classifications are translated into annotations that can
            be imported directly to ELAN, a standard audio and video annotation tool <ptr
              target="#sloetjes2008"/>. <ref target="#figure03">Figure 3</ref> shows the result of
            the overall outcome.</p>



          <table xml:id="table01">
            <head>AUC scores of all the classifiers tested for manual activation prediction</head>
            <row>
              <cell><hi rend="bold">Classifier</hi></cell>
              <cell><hi rend="bold">AUC score</hi></cell>
            </row>
            <row>
              <cell>Artificial Neural Network (ANN)</cell>
              <cell>0.88</cell>
            </row>
            <row>
              <cell>Random Forest (RF)</cell>
              <cell>0.87</cell>
            </row>
            <row>
              <cell>Support Vector Machines (SVM)</cell>
              <cell>0.78</cell>
            </row>
            <row>
              <cell>Extreme Gradient Boosting (XGBoost)</cell>
              <cell>0.92</cell>
            </row>
          </table>

          <table xml:id="table02">
            <head>Importance of each feature during manual activation as predicted by the Extreme
              Gradient Boosting classifier</head>
            <row>
              <cell><hi rend="bold">Weight</hi></cell>
              <cell><hi rend="bold">Feature</hi></cell>
            </row>
            <row>
              <cell>0.1410</cell>
              <cell>Right wrist y</cell>
            </row>
            <row>
              <cell>0.1281</cell>
              <cell>Right wrist x</cell>
            </row>
            <row>
              <cell>0.0928</cell>
              <cell>Left wrist y</cell>
            </row>
            <row>
              <cell>0.0917</cell>
              <cell>Left wrist x</cell>
            </row>
            <row>
              <cell>0.0717</cell>
              <cell>Nose x</cell>
            </row>
            <row>
              <cell>0.0658</cell>
              <cell>Left shoulder x</cell>
            </row>
            <row>
              <cell>0.0623</cell>
              <cell>Left elbow y</cell>
            </row>
            <row>
              <cell>0.0588</cell>
              <cell>Right elbow y</cell>
            </row>
            <row>
              <cell>0.0552</cell>
              <cell>Nose y</cell>
            </row>
            <row>
              <cell>0.0517</cell>
              <cell>Left shoulder y</cell>
            </row>
            <row>
              <cell>0.0482</cell>
              <cell>Left elbow x</cell>
            </row>
            <row>
              <cell>0.0482</cell>
              <cell>Right elbow x</cell>
            </row>
            <row>
              <cell>0.0482</cell>
              <cell>Right shoulder y</cell>
            </row>
            <row>
              <cell>0.0364</cell>
              <cell>Right shoulder x</cell>
            </row>
          </table>

          <figure xml:id="figure03">
            <head>Final output of the manual activation tool as seen in ELAN. Signing sequences have
              been given a 'signing' gloss for readability. This attribute can be easily changed to
              produce empty glosses.</head>
            <graphic url="resources/images/figure03.jpg"/>
          </figure>

          <figure xml:id="figure04">
            <head>Bounding boxes are calculated in order to normalize the body joint coordinates for
              each signer. After this process the normalized coordinates are passed to the XGBoost
                  classifier.<note><ref
                  target="https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s"
                  >https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s</ref>.</note></head>
            <graphic url="resources/images/figure04.jpg"/>
          </figure>

        </div>

        <div>
          <head>4.2 Tool 2: Number of Hands</head>

          <p>The second tool is responsible for not only recognizing whether a person in a video is
            signing but also if the sign is one or two-handed. We have previously hypothesized that
            this is a more complex task than the previous binary classification. Results on the
            accuracy of all the classifiers suggest that it is not as intricate as initially thought
            of; the higher the sliding window interval, the lower the accuracy of the model. As seen
            in <ref target="#figure05">Figure 5</ref> of all classifiers tested, Random forest had
            the highest accuracy at the sliding window interval of 1 frame at a time. Similarly to
            the previous experiment, a frame-to-frame prediction can produce the highest
            results.</p>
          <p>Furthermore, the results regarding the Long-Short-Term-Memory networks (<ref
              target="#figure06">Figure 6</ref>) suggest that the highest accuracy can be achieved
            at a sliding window interval of 56 frames and at a hidden layer size of 8 units.
            However, such a high window interval contains more than one sign, as the average length
            of a sign is approximately 14 frames. This discrepancy can be caused due to the
            architectural properties of the LSTM network. The average length of the signs is too
            small for the network to converge. The LSTM units needed more timesteps in order to
            prevent overfitting to the data. This property in addition to the small dataset used to
            train the network caused this anomaly.</p>
          <p>Although the tool performs well on predicting whether a sign is one- or two-handed
            (using a Random Forest classifier) there are cases were the output is not as expected.
            In particular, cases where there is a two-handed symmetrical sign produced, the tool
            fails to accurately predict the correct class. It is likely that such signs were
            under-presented in our data set, thus resulting in poor classification.</p>
          <figure xml:id="figure05">
            <head>Accuracy of different classifiers in various sliding window intervals in order to
              predict whether a sequence contains a one- or a two- handed sign or not signing at
              all. At a sliding window interval of 1 (a), Random Forest shows the highest
              accuracy.</head>
            <graphic url="resources/images/figure05.png"/>
          </figure>
          <figure xml:id="figure06">
            <head>Accuracy of Long-Short-Term-Memory networks on the test set with different sliding
              window intervals (x) and hidden layer sizes (8,16,32,64,256).</head>
            <graphic url="resources/images/figure06.jpg"/>
          </figure>
        </div>
        <div>
          <head>4.3 Tool 3: Handshape</head>

          <p>In order to understand the distribution of the different handshapes presented in a
            video, Principal Component Analysis (PCA) was utilized on all the normalized finger
            joint coordinates for all the frames at once (<ref target="#figure07">Figure 7a</ref>).
            This process allows us to reduce the dimensionality of the data while retaining as much
            as possible of the variance in the dataset. Each multidimensional array of the extracted
            finger joints positions, for each frame, has been reduced to a single x,y coordinate.
            The result already suggests that there are regions dense enough to be considered
            different clusters. The utilized elbow method suggested that at k=5 the highest
            classification could be achieved (<ref target="#figure07">Figure 7b</ref>). On the video
            sample used in our study that number seemed to reflect the proper amount of discerned
            handshapes. However, as OpenPose captures all the finger configurations in each frame it
            is at the linguist’s discretion to decide on when a handshape is significantly different
            from another. Additionally, experiments to optimize the hyperparameters (eta, min
            samples and leaf size) for the DBSCAN failed to create an accurate clustering (<ref
              target="#figure07">Figure 7c</ref>). Subsequently, the module creates annotation slots
            for the different handshapes in the video and adds an overlay containing the number of
            the predicted cluster on each frame.</p>
          <p>However, special consideration must be given to the overall handshape recognition
            module. Although the hand normalization process prepares the finger joints adequately
            enough to be used in the clustering methods, it fails to account for hands perpendicular
            to the camera’s point of view. Additionally, handshapes that are similar to each other
            but are rotated towards or outwards of the signer’s body will most probably clustered
            differently. Some of these limitations can be solved by manually editing the cluster
            numbers prior to the annotation process.</p>
          <p>In its current form, this method can already be used to either fully annotate the
            handshapes in a video sample or be used in different samples and treated as weakly
            annotated data in order to be used in other handshape classifiers similarly to Koller’s
            et al. study <ptr target="#koller2016"/>.</p>
          <figure xml:id="figure07">
            <head>Visualizations produced by the handshape recognition module. Principal component
              analysis (a) can be used to reduce the dimensionality of the finger joints
              coordinates. Two clustering methods, namely K-means (b) and DBSCAN (c), can be used to
              detect the different handshapes presented in a video sample.</head>
            <graphic url="resources/images/figure07.png"/>
          </figure>

          <figure xml:id="figure08">
            <head>Different handshapes recognized by the handshape recognition module using
              K-means.</head>
            <graphic url="resources/images/figure08.png"/>
          </figure>

        </div>
      </div>
      <div>
        <head>5. Discussion</head>

        <p>In this study we have presented three different tools that can be used to assist the
          annotation process of sign language corpora. The first tool proved to be robust on the
          task of classification of manual activation even when the corpora are noisy, of poor
          quality and most importantly containing more than one signer. This eliminates the
          preprocessing stage that many sign language corpora have to endure where either dedicated
          cameras per signer are utilized or manually cropping the original video. As a result, a
          more natural filming process can be applied. One limitation regarding our methodology is
          that at its current state is not possible to account for individual sign temporal
          classification. Reaching such level would require to fuse additional information into the
          training sets which in most cases might be language specific. However, it is possible to
          get a per sign prediction when the "number of hands involved" feature changes.</p>
        <p>The most striking observation to emerge from our methodology is that there is no
          necessity of having massive training sets for the classification of low-level features
          (such as manual activation and number of hands involved). In contrast to earlier studies
          using neural networks for sign language recognition <ptr target="#aitpayev2016"/>
          <ptr target="#pigou2015"/>
          <ptr target="#zhang2013"/>, we used a proportionally smaller dataset. Additionally, this
          is the first time to our knowledge where corpora outside of studio conditions have been
          used to train and most importantly test models and tools for sign language automatic
          annotation. Furthermore, such findings can be applied in other studies as well. It is a
          common misconception that only large data sets can be used for analysis. Such a trend,
          although true for deep learning purposes, can be daunting for digital humanities
          researchers without in depth data science knowledge. In our study, we have shown that even
          with a small and noisy dataset of visual materials, researchers can use machine learning
          algorithms to effectively extract meaningful information. Our testing in West African Sign
          Language corpora showed that such frameworks can work effectively in different skin color
          participants lifting possible bias by previously developed algorithms.</p>
        <p>There are few limitations regarding our methodologies, particularly with respect to the
          handshape distribution module. Low quality video and consequently framerate seem to affect
          the robustness of OpenPose. As a result, finger joint prediction can be noisy and of low
          confidence. Additionally, we observed that finger joints could not be predicted when the
          elbow was not visible in the frame, and thus, losing that information. In our study we
          treated all predicted joints equally but it is necessary for future research to include
          the prediction confidence interval as an additional variable. Furthermore, on the current
          output from OpenPose it is difficult to extract the palm orientation attribute meaning
          that differently rotated handshapes might result in the same cluster. Future research will
          concentrate on fixing that issue as well as creating an additional tool for the annotation
          of this feature.</p>
        <p>In the sign language domain, researchers can use our tools to recognize the times of
          interest and basic phonological features on newly compiled corpora. Additionally, such
          extracted features can be further used to measure variation on different sign languages or
          signers, for example, to measure the distribution of one- and two-handed signs or
          particular handshapes. Moreover, other machine or deep learning experiments can benefit
          from our tools by using them to extract only the meaningful information from the corpora
          during the data gathering process, thus reducing possible noise in the datasets. Our tools
          can also be used towards automatic gloss suggestion. A future model can search only the
          signing sequences predicted by our tool rather than "scanning" the whole video corpus, and
          consequently making it more efficient.</p>
        <p>Outside the sign language domain, the results have further strengthened our confidence
          that pre-trained frameworks can be used to help extract meaningful information from
          audio-visual materials. In particular, OpenPose can be a useful asset when human activity
          needs to be tracked and recognized in a video without the need of special hardware setups.
          Its accurate tracking allows researchers to use it in videos compiled outside studio
          conditions. As a result, studies in the audio-visual domain can benefit from
          community-created materials involving natural and unbiased communication. Using our tools,
          these study areas can analyze and classify human activity beyond the sign language
          discipline in large scale cultural archives or specific domains such as gestural research,
          dance or theater and cinema related studies, to name but a few. For example, video
          analyses in gestural and media studies can benefit from such an automatic approach to find
          relevant information regarding user-generated data on social media and other popular
          platforms.</p>
        <p>Finally, due to the cumbersome installation process of OpenPose for the majority of SL
          linguists, we have decided to implement part of the tools in an online collaborative
          environment on a cloud service provided by Google (i.e. Google Colab). In this environment
          a temporary instance of OpenPose can be installed along with our developed python modules.
          In a simple step-based manner, the researcher can upload the relevant videos and download
          the automatically generated annotation files. Find the link to this Colab in the footnote
          below <note><ref
              target="https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA"
              >https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA</ref>.</note>.
          Additionally, we have a created another environment for re-training purposes. By doing so,
          the researcher can re-train the models on his or her particular data and ensure the
          aforementioned accuracy on them<note><ref
              target="https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y"
              >https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y</ref>.</note>.</p>

      </div>
      <div>
        <head>Conclusion</head>
        <p>To summarise, glossing sign language corpora is a cumbersome and time-consuming task.
          Current approaches to automatize parts of this process need special video recording
          devices (such as Microsoft Kinect), large amount of data in order to train deep learning
          architectures to recognize a set of signs and can be prone to skin-color bias. In this
          study we explored the use of a pre-trained pose estimation framework created by Cao et al.
            <ptr target="#cao2017"/> in order to create three tools and methods to predict sign
          occurrences, number of hands involved, and handshapes. The results show that four minutes
          of annotated data are adequate enough to train a classifier (namely XGBoost) to predict
          whether one or more persons are signing or not as well as the number of hands used (using
          Random Forest). Additionally, we examined the use of K-means and DBSCAN as clustering
          methods to detect the different handshapes presented in the video. Because of the low
          complexity of the finger joint data extracted from the pose estimation library, K-means
          was found to produce accurate results.</p>
        <p>The significance of this study lies in the fact that the tools created do not rely on
          specialized cameras nor require large amount of information to be trained. Additionally,
          they can be easily used by researchers without developing skills and adjusted to work in
          any kind of sign language corpus irrespective of its quality or the number of people in
          the video. Finally, they have the potential to be extended and used in other audio-visual
          material that involve human activity such as gestural corpora.</p>
      </div>

      <div>
        <head>Appendix A</head>
        <p> The input shape of the LSTM network trained to recognize the <soCalled>number of
            hands</soCalled> (Tool 2) feature is a three dimensional array that can be defined as:
          [samples × timesteps × features] where features is a 2 dimensional array of [21 × 2]
          containing the pixel x,y coordinates of the finger joints and timesteps are the sliding
          window interval. Two Dense layers of 7 and 1 unit respectively follow the previous
          Bidirectional LSTM layer. The activation function used is <soCalled>ReLU</soCalled> and
          the dropout rate at 0.4. The architecture that produced the highest results for this
          network can be seen in <ref target="#figure09">Figure 9</ref>. </p>
        <figure xml:id="figure09">
          <head>Architecture of the Long-Short-Term-Memory network trained for Tool 2.</head>
          <graphic url="resources/images/figure09.png"/>
        </figure>
      </div>
    </body>

    <!-- BACK TEXT -->
    <back>
      <listBibl>
        <bibl xml:id="agha2018" label="Agha et al. 2018">Agha, R. A. A. R., Sefer, M. N. and Fattah,
          P. (2018). <title rend="quotes">A Comprehensive Study on Sign Languages Recognition
            Systems Using (SVM, KNN, CNN and ANN)</title>. <title rend="italic">First International
            Conference on Data Science, E-Learning and Information Systems</title>. (DATA ’18).
          Madrid, Spain: ACM, pp. 28:1–28:6.</bibl>
        <bibl xml:id="aitpayev2016" label="Aitpayev et al. 2016">Aitpayev, K., Islam, S. and
          Imashev, A. (2016). <title rend="quotes">Semi-automatic annotation tool for sign
            languages</title>. <title rend="italic">2016 IEEE 10th International Conference on
            Application of Information and Communication Technologies (AICT)</title>. Baku,
          Azerbaijan: IEEE, pp. 1–4.</bibl>
        <bibl xml:id="andriluka2014" label="Andriluka et al. 2014">Andriluka, M., Pishchulin, L.,
          Gehler, P. and Schiele, B. (2014). <title rend="quotes">2D Human Pose Estimation: New
            Benchmark and State of the Art Analysis</title>. <title rend="italic">IEEE Conference on
            Computer Vision and Pattern Recognition (CVPR)</title>.</bibl>
        <bibl xml:id="buehler2009" label="Buehler et al. 2009">Buehler, P., Zisserman, A. and
          Everingham, M. (2009). <title rend="quotes">Learning sign language by watching TV (using
            weakly aligned subtitles)</title>. <title rend="italic">2009 IEEE Conference on Computer
            Vision and Pattern Recognition</title>. pp. 2961–68.</bibl>
        <bibl xml:id="cao2017" label="Cao et al. 2017">Cao, Z., Simon, T., Wei, S.-E. and Sheikh, Y.
          (2017). <title rend="quotes">Realtime Multi-person 2D Pose Estimation Using Part Affinity
            Fields</title>. <title rend="italic">IEEE Conference on Computer Vision and Pattern
            Recognition (CVPR)</title>. Honolulu, HI: IEEE, pp. 1302–10.</bibl>
        <bibl xml:id="charles2014" label="Charles et al. 2014">Charles, J., Pfister, T., Everingham,
          M. and Zisserman, A. (2014). <title rend="quotes">Automatic and Efficient Human Pose
            Estimation for Sign Language Videos</title>. <title rend="italic">International Journal
            of Computer Vision</title>, 110(1): 70–90.</bibl>
        <bibl xml:id="chollet2015" label="Chollet 2015">Chollet, F. and others (2015). <title
            rend="italic">Keras</title>. <ref target="https://keras.io"
          >https://keras.io</ref>.</bibl>
        <bibl xml:id="cooper2007" label="Cooper and Bowden 2007">Cooper, H. and Bowden, R. (2007).
            <title rend="quotes">Large Lexicon Detection of Sign Language</title>. In Lew, M., Sebe,
          N., Huang, T. S. and Bakker, E. M. (eds), <title rend="italic">Human–Computer
            Interaction</title>. (Lecture Notes in Computer Science). Springer Berlin Heidelberg,
          pp. 88–97.</bibl>
        <bibl xml:id="cooper2011" label="Cooper et al. 2011">Cooper, H., Holt, B. and Bowden, R.
          (2011). <title rend="quotes">Sign Language Recognition</title>. In Moeslund, T. B.,
          Hilton, A., Krüger, V. and Sigal, L. (eds), <title rend="italic">Visual Analysis of
            Humans: Looking at People</title>. London: Springer London, pp. 539–62.</bibl>
        <bibl xml:id="dreuw2008" label="Dreuw and Ney 2008">Dreuw, P. and Ney, H. (2008). <title
            rend="quotes">Towards automatic sign language annotation for the ELAN tool</title>.
            <title rend="italic">LREC Workshop: Representation and Processing of Sign
            Languages</title>. Marrakech, Morocco, pp. 50–53.</bibl>
        <bibl xml:id="eichner2009" label="Eichner and Ferrari 2009">Eichner, M. and Ferrari, V.
          (2009). <title rend="quotes">Better appearance models for pictorial structures</title>.
            <title rend="italic">British Machine Vision Conference 2009</title>. London, UK: British
          Machine Vision Association, pp. 3.1-3.11.</bibl>
        <bibl xml:id="eichner2012" label="Eichner et al. 2012">Eichner, M., Marin-Jimenez, M.,
          Zisserman, A. and Ferrari, V. (2012). <title rend="quotes">2D Articulated Human Pose
            Estimation and Retrieval in (Almost) Unconstrained Still Images</title>. <title
            rend="italic">International Journal of Computer Vision</title>, 99(2): 190–214.</bibl>
        <bibl xml:id="fang2004" label="Fang et al. 2004">Fang, G., Gao, W. and Zhao, D. (2004).
            <title rend="quotes">Large Vocabulary Sign Language Recognition Based on Fuzzy Decision
            Trees</title>. <title rend="italic">Systems, Man and Cybernetics, Part A: Systems and
            Humans, IEEE Transactions On</title>, 34: 305–14 doi:10.1109/TSMCA.2004.824852.</bibl>
        <bibl xml:id="farhadi2007" label="Farhadi et al. 2007">Farhadi, A., Forsyth, D. and White,
          R. (2007). <title rend="quotes">Transfer Learning in Sign language</title>. <title
            rend="italic">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>.
          Minneapolis, MN, USA, pp. 1–8.</bibl>
        <bibl xml:id="felzenszwalb2005" label="Felzenszwalb and Huttenlocher 2005">Felzenszwalb, P.
          F. and Huttenlocher, D. P. (2005). <title rend="quotes">Pictorial Structures for Object
            Recognition</title>. <title rend="italic">International Journal of Computer
            Vision</title>, 61(1): 55–79.</bibl>
        <bibl xml:id="forster2012" label="Forster 2012">Forster, J., Schmidt, C., Hoyoux, T.,
          Koller, O., Zelle, U., Piater, J. and Ney, H. (2012). <title rend="quotes"
            >RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation
            Corpus</title>. Istanbul, Turkey, pp. 3785–3789.</bibl>
        <bibl xml:id="johnson2009" label="Johnson and Everingham 2009">Johnson, S. and Everingham,
          M. (2009). <title rend="quotes">Combining discriminative appearance and segmentation cues
            for articulated human pose estimation</title>. <title rend="italic">2009 IEEE 12th
            International Conference on Computer Vision Workshops, ICCV Workshops</title>. pp.
          405–12.</bibl>
        <bibl xml:id="johnston2010" label="Johnston 2010">Johnston, T. (2010). <title rend="quotes"
            >From archive to corpus: Transcription and annotation in the creation of signed language
            corpora</title>. <title rend="italic">International Journal of Corpus
            Linguistics</title>, 15(1): 106–31 doi:10.1075/ijcl.15.1.05joh. <ref
            target="http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh"
            >http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh</ref> (accessed 19
          May 2020).</bibl>
        <bibl xml:id="kelly2009" label="Kelly et al. 2009">Kelly, D., Reilly Delannoy, J., Mc
          Donald, J. and Markham, C. (2009). <title rend="quotes">A framework for continuous
            multimodal sign language recognition</title>. <title rend="italic">Proceedings of the
            2009 International Conference on Multimodal Interfaces</title>. pp. 351–358.</bibl>
        <bibl xml:id="koller2016" label="Koller et al. 2016">Koller, O., Ney, H. and Bowden, R.
          (2016). <title rend="quotes">Deep Hand: How to Train a CNN on 1 Million Hand Images When
            Your Data is Continuous and Weakly Labelled</title>. <title rend="italic">IEEE
            Conference on Computer Vision and Pattern Recognition (CVPR)</title>. Las Vegas, NV,
          USA: IEEE, pp. 3793–802.</bibl>
        <bibl xml:id="kumar2017" label="Kumer 2017">Kumar, N. (2017). <title rend="quotes">Motion
            trajectory based human face and hands tracking for sign language recognition</title>.
            <title rend="italic">2017 4th IEEE Uttar Pradesh Section International Conference on
            Electrical, Computer and Electronics (UPCON)</title>. Mathura: IEEE, pp. 211–16.</bibl>
        <bibl xml:id="lin2014" label="Lin 2014">Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L.,
          Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L. and Dollár, P. (2014).
            <title rend="italic">Microsoft COCO: Common Objects in Context</title>.</bibl>
        <bibl xml:id="lubbers2013" label="Lubbers and Torreira 2013">Lubbers, M. and Torreira, F.
          (2013). <title rend="italic">A Python Module for Processing ELAN and Praat Annotation
            Files: Dopefishh/Pympi</title>. Python <ref target="https://github.com/dopefishh/pympi"
            >https://github.com/dopefishh/pympi</ref>.</bibl>
        <bibl xml:id="moeslund2006" label="Moselund et al. 2006">Moeslund, T. B., Hilton, A. and
          Krüger, V. (2006). <title rend="quotes">A survey of advances in vision-based human motion
            capture and analysis</title>. <title rend="italic">Computer Vision and Image
            Understanding</title>, 104(2): 90–126.</bibl>
        <bibl xml:id="nyst2012a" label="Nyst 2012">Nyst, V. (2012). <title rend="italic">A Reference
            Corpus of Adamorobe Sign Language</title>. A digital, annotated video corpus of the sign
          language used in the village of Adamorobe, Ghana.</bibl>
        <bibl xml:id="nyst2012b" label="Nyst et al. 2012">Nyst, V., Magassouba, M. M. and Sylla, K.
          (2012). <title rend="italic">Un Corpus de reference de la Langue des Signes Malienne
            II</title>. A digital, annotated video corpus of local sign language use in the Dogon
          area of Mali.</bibl>
        <bibl xml:id="pedregosa2011" label="Pedregosa et al. 2011">Pedregosa, F., Varoquaux, G.,
          Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). <title
            rend="quotes">Scikit-learn: Machine Learning in Python</title>. <title rend="italic"
            >Journal of Machine Learning Research</title>, 12: 2825–2830.</bibl>
        <bibl xml:id="pigou2015" label="Pigou et al. 2015">Pigou, L., Dieleman, S., Kindermans,
          P.-J. and Schrauwen, B. (2015). <title rend="quotes">Sign Language Recognition Using
            Convolutional Neural Networks</title>. In Agapito, L., Bronstein, M. M. and Rother, C.
          (eds), <title rend="italic">Computer Vision - ECCV 2014 Workshops</title>. (Lecture Notes
          in Computer Science). Cham: Springer International Publishing, pp. 572–78.</bibl>
        <bibl xml:id="ramanan2007" label="Ramanan et al. 2007">Ramanan, D., Forsyth, D. A. and
          Zisserman, A. (2007). <title rend="quotes">Tracking People by Learning Their
            Appearance</title>. <title rend="italic">IEEE Transactions on Pattern Analysis and
            Machine Intelligence</title>, 29(1): 65–81.</bibl>
        <bibl xml:id="roussos2012" label="Roussos et al. 2012">Roussos, A., Theodorakis, S.,
          Pitsikalis, V. and Maragos, P. (2012). <title rend="quotes">Hand Tracking and Affine
            Shape-Appearance Handshape Sub-units in Continuous Sign Language Recognition</title>. In
          Kutulakos, K. N. (ed), <title rend="italic">Trends and Topics in Computer Vision</title>,
          vol. 6553. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 258–72.</bibl>
        <bibl xml:id="sapp2010" label="Sapp et al. 2010">Sapp, B., Jordan, C. and Taskar, B. (2010).
          Adaptive pose priors for pictorial structures. <title rend="italic">2010 IEEE Computer
            Society Conference on Computer Vision and Pattern Recognition</title>. pp.
          422–29.</bibl>
        <bibl xml:id="sivic2006" label="Sivic et al. 2006">Sivic, J., Zitnick, C. L. and Szeliski,
          R. (2006). <title rend="quotes">Finding people in repeated shots of the same
          scene</title>. <title rend="italic">British Machine Vision Conference 2006</title>, vol.
          3. Edinburgh: British Machine Vision Association, pp. 909–18.</bibl>
        <bibl xml:id="sloetjes2008" label="Sloetjes and Wittenburg 2008">Sloetjes, H. and
          Wittenburg, P. (2008). <title rend="quotes">Annotation by category - ELAN and ISO DCR.
            Marrakech, Morocco</title>, p. 5 <ref target="https://tla.mpi.nl/tools/tla-tools/elan/"
            >https://tla.mpi.nl/tools/tla-tools/elan/</ref>.</bibl>
        <bibl xml:id="starner1998" label="Starner et al. 1998">Starner, T., Weaver, J. and Pentland,
          A. (1998). <title rend="quotes">Real-time American sign language recognition using desk
            and wearable computer based video</title>. <title rend="italic">IEEE Transactions on
            Pattern Analysis and Machine Intelligence</title>, 20(12): 1371–75.</bibl>
        <bibl xml:id="yang2011" label="Yang and Ramanan 2011">Yang, Y. and Ramanan, D. (2011).
            <title rend="quotes">Articulated pose estimation with flexible
          mixtures-of-parts</title>. <title rend="italic">IEEE Conference on Computer Vision and
            Pattern Recognition (CVPR)</title>. pp. 1385–92.</bibl>
        <bibl xml:id="zhang2013" label="Zhang et al. 2013">Zhang, C., Yang, X. and Tian, Y. (2013).
            <title rend="quotes">Histogram of 3D Facets: A characteristic descriptor for hand
            gesture recognition.</title>
          <title rend="italic">2013 10th IEEE International Conference and Workshops on Automatic
            Face and Gesture Recognition (FG)</title>. pp. 1–8.</bibl>
      </listBibl>
    </back>
  </text>
  <!-- END TEXT -->

</TEI>
