<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 015 Number 1
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol/15/1/000510.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol/15/1/000510/000510.html#disqus_thread" data-disqus-identifier="000510">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Towards a User-Friendly Tool for Automated Sign
                  Annotation: Identification and Annotation of Time Slots, Number of Hands, and Handshape
                  
               </h1>
               
               <div class="author"><span style="color: grey">Manolis Fragkiadakis</span> &lt;<a href="mailto:m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl'); return false;">m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               <div class="author"><span style="color: grey">Victoria Nyst</span> &lt;<a href="mailto:v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl'); return false;">v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               <div class="author"><span style="color: grey">Peter van der Putten</span> &lt;<a href="mailto:p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl'); return false;">p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Towards%20a%20User-Friendly%20Tool%20for%20Automated%20Sign%20Annotation%3A%20Identification%20and%20Annotation%20of%20Time%20Slots,%20Number%20of%20Hands,%20and%20Handshape&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=015&amp;rft.issue=1&amp;rft.aulast=Fragkiadakis&amp;rft.aufirst=Manolis&amp;rft.au=Manolis%20Fragkiadakis&amp;rft.au=Victoria%20Nyst&amp;rft.au=Peter%20van der Putten"> </span></div>
            
            
            
            
            <div id="DHQtext">
               
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>The annotation process of sign language corpora in terms of glosses, is a highly
                     labor-intensive task, but a condition for a reliable quantitative analysis. During
                     the
                     annotation process the researcher typically defines the precise time slot in which
                     a sign
                     occurs and then enters the appropriate gloss for the sign. The aim of this project
                     is to
                     develop a set of tools to assist the annotation of the signs and their formal features
                     in
                     a video irrespectively of its content and quality. Recent advances in the field of
                     deep
                     learning have led to the development of accurate and fast pose estimation frameworks.
                     In
                     this study, such a framework (namely OpenPose) has been used to develop three different
                     methods and tools to facilitate the annotation process. The first tool estimates the
                     span
                     of a sign sequence and creates empty slots in an annotation file. The second tool
                     detects
                     whether a sign is one- or two-handed. The last tool recognizes the different handshapes
                     presented in a video sample. All tools can be easily re-trained to fit the needs of
                     the
                     researcher.
                  </p>
                  
               </div>
               
               
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">While the majority of the studies in the field of digital humanities have been mostly
                     text oriented, the evolution in computing power and technology has resulted in a shift
                     towards multimedia-oriented studies. Recently, advances in computer vision have started
                     to
                     find practical applications in study domains outside of computer and data science.
                     Video
                     is one of the most important time-based media as it has the ability to carry large
                     amount
                     of digital information in a condensed form, and hence it serves as a rich medium to
                     capture various forms of cultural expression. Automated processing and annotation
                     of large
                     numbers of videos is now becoming feasible due to the evolution of computer vision
                     and
                     machine learning.
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">In sign language linguistics, a transition took place from paper-based materials to
                     large
                     video corpora to facilitate the study of the languages in question. Sign language
                     corpora
                     are mainly composed of video data. The primary goal of these video corpora is to study
                     sign language functioning.
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The processing of sign languages usually involves requires a form of textual
                     representation [<a class="ref" href="#dreuw2008">Dreuw and Ney 2008</a>], most notably glosses for annotation. Sign
                     language glosses are words from a spoken language. Uniquely identifying glosses by
                     definition refer to a specific sign. Such ID glosses are an essential element for
                     the
                     quantitative analysis of a sign language corpus [<a class="ref" href="#johnston2010">Johnston 2010</a>]. Typically,
                     sign language linguists add glosses and other annotations to the video recordings
                     with the
                     use of a software tool (namely ELAN). ELAN allows researchers to add time-aligned
                     annotations to a video. However, this task requires a lot of time and can be prone
                     to
                     errors.
                  </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">New advances in computer vision open up additional ways of studying videos containing
                     sign language data, extracting formal representations of linguistic phenomena, and
                     implementing these in computer applications, such as automatic recognition, generation,
                     and translation. Using computer vision and machine learning enables quick and new
                     ways of
                     processing large sets of video data, which in turns makes it possible to address research
                     questions that were not feasible before.
                  </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">This study is the first part of a project aiming at the creation of tools to automatize
                     part of the annotation process of sign language video data. This paper presents the
                     methodologies, tools and implementations of three functionalities: the detection of
                     1)
                     manual activation 2) the number of hands involved and 3) the handshape distribution
                     on
                     sign language corpora.
                  </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Recent developments in sign language recognition illustrate the advantages of machine
                     and
                     deep learning for tasks related to recognition and classification [<a class="ref" href="#agha2018">Agha et al. 2018</a>]
                     [<a class="ref" href="#cao2017">Cao et al. 2017</a>]
                     [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]. Nevertheless, current approaches are restricted in various
                     ways, limiting their applicability in current sign language research. For example,
                     training deep learning networks requires a vast amount of data as well as adequate
                     computational power. These networks are usually trained in one sign language and they
                     do
                     not generalize well in other sign languages.
                  </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Additionally, current approaches in sign language automatic annotation need manual
                     annotation of the hands and body joints for the training of the recognizer models
                     [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                     [<a class="ref" href="#dreuw2008">Dreuw and Ney 2008</a>]. Moreover, the application of color and motion detection
                     algorithms [<a class="ref" href="#kumar2017">Kumer 2017</a>], as feature extraction methods, can be susceptible
                     to errors and possibly skin color bias. Finally, several hand tracking models only
                     work on
                     a particular type of recordings, for example, a signer wearing gloves, or recordings
                     made
                     with Microsoft’s Kinect [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]. As a result, these models are not
                     usable for the majority of the existing sign language corpora which have been recorded
                     with standard RGB cameras.
                  </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Our methods have been developed and tested on two West African sign language corpora
                     containing natural conditions with non-Caucasian signers. While most studies in the
                     sign
                     language recognition field have mainly concerned signers with light skin tones, little
                     research has been conducted using darker skin tones. With the emergence of corpora
                     compiled in African countries under challenging real-world conditions, and their
                     contribution to the overall sign language community, it is of utmost importance to
                     test
                     how methods perform in such a domain. Alleviating biases and increasing diversity
                     should
                     be a top priority of any computer assisted study.
                  </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">In this study, a pre-trained deep learning pose estimation library developed by Cao
                     et
                     al. [<a class="ref" href="#cao2017">Cao et al. 2017</a>] named OpenPose has been used to extract body and finger
                     key-points. OpenPose has been trained and evaluated on two large datasets for multi-person
                     pose estimation. The MPII human multi-person dataset [<a class="ref" href="#andriluka2014">Andriluka et al. 2014</a>] and
                     the COCO 2016 keypoints challenge dataset [<a class="ref" href="#lin2014">Lin 2014</a>] contain images of
                     people of different age groups and ethnicities in diverse scenarios. As a result,
                     OpenPose
                     does not have a bias toward skin color. Additionally, its easy-to-use implementation
                     makes
                     it an ideal framework to be used by linguists with limited coding experience.
                  </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">The combination of the aforementioned pose estimation framework as well as the machine
                     and deep learning architectures tested in this study, provides a robust approach towards
                     automatic annotation. Current models and tools can be used in any sign language or
                     gestural corpus independently of its quality, length and number of people in the video.
                     These tools have been developed as python modules that can run automatically in a
                     video
                     and produce the relevant annotation files requiring minimal effort from the user.
                     More
                     generally, as large parts of our cultures nowadays are captured in video, our study
                     serves
                     as a case example of how intelligent machine learning techniques can serve digital
                     humanities researchers by extracting semantics from large video collections.
                  </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">This article is structured as follows: Section 2 introduces the developments on the
                     sign
                     language recognition and automatic annotation fields. Section 3 describes the materials
                     used in this study and the methodologies developed and applied for each tool separately.
                     Section 4 presents the results for each experimental setup and tool. Section 5 contains
                     the discussion and future work while Section 6 presents our conclusions. Finally,
                     Appendix
                     A presents the architecture and technical details of the Long-Short-Term-Memory Network
                     trained for this study.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2. Literature review</h1>
                  
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">In this section we present the studies conducted on the sign language recognition
                     and
                     automatic annotation field developed with depth sensors as well as standard RGB cameras.
                     Additionally, we describe the developments of the human pose estimation field and
                     we
                     introduce the OpenPose framework that will be used in this article.
                  </div>
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.1 Sign Lanfiguage Recognition and Automatic Annotation</h2>
                     
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">The primary goal of sign language recognition is to develop methods and algorithms
                        to
                        accurately identify a series of produced signs and to discern their meaning. The
                        majority of studies have focused on recognizing those features and methods that can
                        properly identify a sign out of a given set of possible signs. However, such methods
                        can
                        only be used on a particular set of signs and, thus, a specific sign language, which
                        makes it harder to study the relationships between and evolution of various sign
                        languages.
                     </div>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">An additional motivation behind Sign Language Recognition (SLR) is to build automatic
                        sign language to speech or text translation systems to assist the communication between
                        the deaf and hearing community [<a class="ref" href="#fang2004">Fang et al. 2004</a>]. Moreover, SLR plays an
                        important role in developing gesture-based human–computer interaction systems [<a class="ref" href="#kelly2009">Kelly et al. 2009</a>]. Sign language linguists have used such systems to facilitate
                        the annotation process of sign language corpora in order to discern the different
                        signs
                        in a video recording and further study the linguistic phenomena presented.
                     </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">There are numerous studies dealing with the automated recognition of sign languages
                        as
                        clearly presented by Cooper et al. [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>], in their review study on
                        the state-of-the-art in sign language recognition. However, the experiments presented
                        in
                        most of these studies are either hard to replicate, or they pose limitations as far
                        as
                        their applicability is concerned. For instance, most of these studies use depth sensors,
                        most notably MS Kinect, to capture 3D images of the environment [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                        [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]
                        [<a class="ref" href="#zhang2013">Zhang et al. 2013</a>]. As a result, using the frameworks developed in these studies
                        requires a machine with similar features as the one used for testing and most probably
                        will only work for that sign language on which they have been trained.
                     </div>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">Recently, computer vision techniques have been applied to sign language recognition
                        to
                        overcome the aforementioned limitations. Roussos et al. [<a class="ref" href="#roussos2012">Roussos et al. 2012</a>]
                        created a skin color probabilistic model to detect and track the hands of a signer
                        on a
                        video, while Cooper et al. [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>] use this model to segment the
                        hands and apply a classifier based on Markov models. However, systems based on skin
                        color [<a class="ref" href="#buehler2009">Buehler et al. 2009</a>]
                        [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>]
                        [<a class="ref" href="#farhadi2007">Farhadi et al. 2007</a>]
                        [<a class="ref" href="#starner1998">Starner et al. 1998</a>] are prone to errors and have difficulties on tracking the
                        hands and the signer’s features against cluttered backgrounds and in noisy conditions
                        in
                        general. Also, they do not work in videos with multiple signers.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">2.2 Human Pose Estimation</h2>
                     
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">Human pose estimation has been extensively studied due to its numerous applications
                        on
                        a number of different fields [<a class="ref" href="#moeslund2006">Moselund et al. 2006</a>]. Due to low computational
                        complexity during inference, pictorial structures have been commonly used [<a class="ref" href="#felzenszwalb2005">Felzenszwalb and Huttenlocher 2005</a>]
                        [<a class="ref" href="#ramanan2007">Ramanan et al. 2007</a>]
                        [<a class="ref" href="#sivic2006">Sivic et al. 2006</a>] to model human pose. Recently, studies have focused on
                        improving the appearance models used in these structures by modelling the individual
                        body parts [<a class="ref" href="#eichner2009">Eichner and Ferrari 2009</a>]
                        [<a class="ref" href="#eichner2012">Eichner et al. 2012</a>]
                        [<a class="ref" href="#johnson2009">Johnson and Everingham 2009</a>]
                        [<a class="ref" href="#sapp2010">Sapp et al. 2010</a>]. Felzenszwalb and Huttenlocher [<a class="ref" href="#felzenszwalb2005">Felzenszwalb and Huttenlocher 2005</a>], relying on the pictorial structure framework recommended a deformable part-based
                        model. Additionally, Yang and Ramanan [<a class="ref" href="#yang2011">Yang and Ramanan 2011</a>] showed that a
                        tree-structured model using a combination of deformable parts can be used in order
                        to
                        achieve accurate pose estimation. Furthermore, Charles et al. [<a class="ref" href="#charles2014">Charles et al. 2014</a>] showed that human body joint positions can be predicted using a random forest
                        regressor based on a co-segmentation process over all video frames.
                     </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">In general, most of the vision-based approaches developed for sign language recognition
                        tasks utilizing pose estimation, have used the RWTH-PHOENIX-Weather data set [<a class="ref" href="#forster2012">Forster 2012</a>] to validate their models. This data set consists of weather
                        forecast airings from the German public tv-station PHOENIX along with transcribed
                        gloss
                        annotations. However, it is a question to what extent such systems tested in this
                        data
                        set can be replicated with real-life conditions in the corpora. It is often the case
                        that sign language and gestural corpora, especially the ones filmed outside of studio
                        conditions, have bad quality, low brightness and often contain more than one person
                        in
                        the frame. These characteristics create an additional challenge to the tracking and
                        prediction task.
                     </div>
                     
                     
                     <div class="div div2">
                        
                        <h3 class="head">2.2.1 OpenPose</h3>
                        
                        
                        <div class="counter"><a href="#p19">19</a></div>
                        <div class="ptext" id="p19">OpenPose is a real-time, open-source library for academic purposes for multi-person
                           2D pose estimation. It can detect body, foot, hand and facial keypoints [<a class="ref" href="#cao2017">Cao et al. 2017</a>]. Following a bottom-up approach (from an entire image as input
                           to full body poses as output), it outperforms similar 2D body pose estimation
                           libraries.
                        </div>
                        
                        <div class="counter"><a href="#p20">20</a></div>
                        <div class="ptext" id="p20">A major advantage of the library is that it achieves high accuracy and performance
                           regardless of the number of people in the image. Its high accuracy is performed by
                           using a non-parametric representation of 2D vector fields. These fields encode the
                           position and orientation of body parts over the image domain and their degree of
                           association in order to learn to relate them to each individual.
                        </div>
                        
                        <div class="counter"><a href="#p21">21</a></div>
                        <div class="ptext" id="p21">OpenPose is able to run on different operating systems and multiple hardware
                           architectures. Additionally, it provides tools for visualization and output file
                           generation. The output can be multiple json files containing all the pixel x, y
                           coordinates of the body, hand and face joints. In this study the DEMO version on a
                           CPU-only mode has been used to train our models. This choice was made in order to
                           ensure that reproducibility can be easily achieved without the need for powerful
                           computers from the linguist’s side.
                        </div>
                        
                        
                     </div>
                     
                  </div>
                  
               </div>
               
               
               <div class="div div0">
                  
                  <h1 class="head">3. Materials and Methods</h1>
                  
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">This section describes the datasets used in our study as well as the pre-processing
                     stage
                     using OpenPose to extract the body joints’ pixel coordinates. Furthermore, we introduce
                     the methods applied in the development of each tool. Special consideration is given
                     on the
                     handshape recognition module as an additional normalization part has been developed.
                     
                  </div>
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.1 Data</h2>
                     
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">A data set of 7,805 frames in total (approximately 4 minutes) labeled as signing or
                        not
                        signing has been compiled for the first part of the study. The dimensions of the frames
                        were 352 by 288 pixels and were extracted from the Adamorobe and Berbey Sign Language
                        corpora [<a class="ref" href="#nyst2012a">Nyst 2012</a>]
                        [<a class="ref" href="#nyst2012b">Nyst et al. 2012</a>]. These corpora portray an additional challenge as the signers
                        have been filmed in and around their homes, in natural conditions, outside of a studio,
                        with strongly varying brightness and background noise. Furthermore, they may contain
                        signing from one and two people at the same time. As a result, they can be considered
                        as
                        one of the hardest corpora to perform classification tasks. It is arguable that if
                        the
                        methods developed in this study can perform reasonably well on corpora of such poor
                        conditions, then they can be applied to any sign language corpus under better
                        settings.
                     </div>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">Additional videos from YouTube with higher quality have been selected for testing
                        purposes too. For the first task of this study, the original data set was split into
                        a
                        training and testing set of 6,150 and 1,655 frames respectively and the labels were
                        one
                        hot encoded (i.e. signing as 1 and not-signing as 0).
                     </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">After a successful training of the first prediction model, the tool was applied on
                        a
                        different part of the corpora. The predicted signing sequences were manually labeled
                        as
                        one- or two-handed signs. Together with randomly selected not-signing sequences (as
                        predicted by the first tool), they formed a second data set. The size of this data
                        set
                        was slightly larger than the previous one: 10,120 frames in total.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.2 Pre-processing</h2>
                     
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">Using OpenPose, the pixel coordinates of the hands, elbows, shoulders and head were
                        extracted from each frame. In the case of the handshape recognition module, the fingers
                        joints coordinates were additionally extracted. We avoided using the finger extraction
                        module of OpenPose on the first two parts of the study as that would have increased
                        the
                        computational time significantly. The positions of the rest of the body joints were
                        disregarded as most of the time they were out of the frame bounds. Although the quality
                        of the frames was poor, it created an advantage for the pose estimation framework,
                        reducing the computational time to a reasonable level.
                     </div>
                     
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.3 Tool 1: Manual Activation</h2>
                     
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">The first tool is a temporal segmentation method to predict the begin and end frames
                        of
                        a sign sequence in a video sample. Thus, it is important to compare the performance
                        of
                        multiple different machine learning algorithms consistently. Four classification methods
                        were used, namely: Support Vector Machines (SVM), Random Forests (RF), Artificial
                        Neural
                        Networks (ANN) and Extreme Gradient Boosting (XGBoost). The majority of these algorithms
                        have been extensively used in machine learning studies as well as in sign language
                        applications [<a class="ref" href="#agha2018">Agha et al. 2018</a>]. Performance was measured using the Area Under
                        the Receiver Operating Characteristics (AUC) to validate each model. The AUC is a
                        performance measurement specifically designed for binary (i.e. two class) classification
                        problems. In general, it expresses how well a model is capable of distinguishing between
                        classes, for example whether someone is signing in a video fragment or not. A model
                        that
                        makes random predictions will have an AUC of 0.5, a perfect model will have an AUC
                        of 1.
                        AUC stands for “Area under the Receiver Operating Characteristic (ROC)
                        Curve”, the curve of True Positive Rate (probability of detection) versus
                        False Positive Rate (probability of false alarms). It is better than just accuracy,
                        i.e.
                        percentage of correct predictions by the model, because it is not dependent on the
                        relative amount of positives, i.e. percentage of total videos with signs in our case.
                        We
                        searched for the optimal setting of the various classification method parameters by
                        exhaustive testing of the possible parameter settings and testing the performance
                        on a
                        validation set (“grid search”, searching the
                        “grid” of possible parameter values).
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.4 Tool 2: Number of Hands</h2>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">The second tool’s goal is to predict not only if a person is signing or not, but also
                        to identify the number of hands involved (one- or two-handed). We hypothesized that
                        this
                        task is more complex than before, thus we considered it as a time-series problem.
                        By
                        using a sliding window technique, the original data set was parsed to form new training
                        sets, where different possible frame intervals (1,2,3,5 and 10) were tested.
                        Furthermore, similar (to some extend) classification methods with Tool 1 have been
                        used
                        <a class="noteRef" href="#d4e429">[1]</a>.
                     </div>
                     
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">Moreover, recent studies in the sign language recognition field suggest that the use
                        of
                        Long-Short-Term-Memory (LSTM) networks can yield accurate results. LSTM is an artificial
                        recurrent neural network (RNN) architecture used in the field of deep learning. Unlike
                        standard feedforward neural networks (like the one tested in Tool 1) LSTM has feedback
                        connections. It can not only process single data points, but also understand patterns
                        in
                        entire sequences of data, by combining its internal state resulting from previous
                        input
                        with a new input data item. In our case, instead of predicting whether a specific
                        pose
                        belongs into a class, we investigate whether a sequence of poses can be used for the
                        same purpose. In this part of the study an LSTM network with different layer units
                        as
                        well as sliding window intervals has also been tested and compared with the above
                        traditional machine learning classifiers. The overall architecture and technical details
                        of the LSTM network can be found in Appendix A.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.5 Tool 3: Handshape</h2>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">The handshape recognition module was considered a so-called unsupervised learning
                        problem as no ground truth information regarding this feature was available prior
                        to the
                        experiment, i.e. in contrast to the previous two problems we did not know what classes
                        (handshapes) to detect. Such an unsupervised learning method can be useful in other
                        newly compiled sign language or gestural corpora where there is no information regarding
                        the different handshapes presented by the signers in the video. Additionally, an
                        unsupervised learning method can be useful in other newly compiled sign language or
                        gestural corpora where there is no information regarding the different handshapes
                        presented by the signers in the video. We approached this as a clustering task: can
                        we
                        find groups of signs that were similar. Two different clustering methods have been
                        tested: K-means and DBSCAN. The first clustering method was chosen for its simplicity
                        as
                        well as its fast implementation on the Python library that was utilized (namely
                        scikit-learn). However, as the complexity of the data is unknown and it is case
                        sensitive, it was decided to employ Density-Based Spatial Clustering of Applications
                        with Noise (DBSCAN) as an alternative option. Given a set of points in some space,
                        DBSCAN groups together points that are closely packed together, marking as outliers
                        the
                        ones that lie alone in low-density regions. This clustering method is one of the most
                        common clustering algorithms.
                     </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">Determining the optimal number of clusters (i.e. total number of expected handshapes)
                        is a crucial issue in clustering methods such as K-means, which requires the user
                        to
                        specify the number of clusters <em class="emph">k</em> to be generated. The definition of
                        clusters is done so that the total within-cluster sum of square (WSS) is minimized,
                        hence, in this study the <em class="emph">elbow method</em> was utilized to estimate the number
                        of clusters.
                     </div>
                     
                     
                     <div class="div div2">
                        
                        <h3 class="head">3.5.1 Hand Normalization</h3>
                        
                        
                        <div class="counter"><a href="#p32">32</a></div>
                        <div class="ptext" id="p32">Since the output of OpenPose contains the raw x, y pixel positions for the different
                           finger joints, it is important to normalize them before applying the clustering
                           method. To do so, the angle of the vector between the elbow and the wrist of the right
                           hand is calculated. Subsequently, the coordinates of the finger joints positions are
                           rotated to be in parallel on the horizontal axis and normalized so that their averaged
                           location is at the origin. <a href="#figure01">Figure 1</a> shows the output of
                           the overall normalization process. All experiments were conducted using one machine
                           with a hexa-core processor (Intel Core i7-3930K) and 4GB RAM. The models are
                           implemented using the Python libraries scikit-learn [<a class="ref" href="#pedregosa2011">Pedregosa et al. 2011</a>] and
                           Keras [<a class="ref" href="#chollet2015">Chollet 2015</a>] for their fast and easy implementation.
                        </div>
                        
                        
                        <div id="figure01" class="figure">
                           
                           
                           <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt="" /></a></div>
                           
                           <div class="caption">
                              <div class="label">Figure 1. </div>Signer's hand normalization is done based on the angle between the horizontal
                              axis and the vector of the elbow and wrist coordinates. The finger joints are
                              rotated according to that angle in order to be in parallel to the horizontal axis
                              and scaled so that their average location is at the origin.
                           </div>
                        </div>
                        
                        
                     </div>
                     
                  </div>
                  
               </div>
               
               
               <div class="div div0">
                  
                  <h1 class="head">4. Results</h1>
                  
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">The results section consists of three parts, the first part (Section 4.1) discusses
                     the
                     results of the analysis regarding the manual activation prediction. ﻿Section 4.2 discusses
                     the results regarding the classification of one- and two-handed signing sequences.
                     Last
                     but not least, Section 4.3 presents the result regarding the handshape distribution
                     using
                     different clustering methods.
                  </div>
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.1 Tool 1: Manual Activation</h2>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">All classifiers performed adequately well, apart from the Support Vector Machines
                        (AUC:
                        0.80) (<a href="#table01">Table 1</a>). Extreme Gradient Boosting (XGBoost) showed
                        the highest AUC score at 0.92<a class="noteRef" href="#d4e498">[2]</a>. <a href="#figure02">Figure 2</a>
                        presents the ROC curve after a 10-fold cross-validation. The Artificial Neural Network
                        was found to perform sufficiently well (AUC: 0.88). By exploring the importance of
                        each
                        feature on the prediction of the model we observe that the y and x pixel coordinates
                        of
                        the dominant (i.e. right) hand are on the top two positions (<a href="#table02">Table
                           2</a>).
                     </div>
                     
                     
                     <div id="figure02" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.jpg" rel="external"><img src="resources/images/figure02.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 2. </div>10-fold cross validation of Extreme Gradient Boosting (XGBoost) classifier for the
                           prediction of manual activation.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">The fact that the Artificial Neural Network turned out to be a less efficient approach
                        than the XGBoost can be accounted to the small training data set. Typically, Neural
                        Networks require a lot more training data than traditional machine learning algorithms.
                        Additionally, designing a network that correctly encodes a domain specific problem
                        is
                        challenging. In most cases, competent architectures are only reached when a whole
                        research community is working on those problems, without short-term time constraints.
                        Fine-tuning such a network would require time and effort that reach beyond the scope
                        of
                        this study.
                     </div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">To account for multiple people signing in one frame, an extra module was added. This
                        module creates bounding boxes around each person recognized by OpenPose, normalizes
                        the
                        positions of the body joints and runs the classifier. This process makes it possible
                        to
                        classify sign occurrences for multiple people irrespective of their positions in a
                        frame
                        (<a href="#figure04">Figure 4</a>).
                     </div>
                     
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Once all the frames have been classified, the "cleaning up" and annotation phase
                        starts. A sign occurrence is annotated only if at least 12 consecutive frames have
                        been
                        classified as "signing". That way we account for the false positive errors. This sets
                        the stage for the annotation step. Using the PyMpi python library [<a class="ref" href="#lubbers2013">Lubbers and Torreira 2013</a>] the classifications are translated into annotations that can
                        be imported directly to ELAN, a standard audio and video annotation tool [<a class="ref" href="#sloetjes2008">Sloetjes and Wittenburg 2008</a>]. <a href="#figure03">Figure 3</a> shows the result of
                        the overall outcome.
                     </div>
                     
                     
                     
                     
                     <div id="table01" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Classifier</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">AUC score</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Artificial Neural Network (ANN)</td>
                              
                              <td valign="top" class="cell">0.88</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Random Forest (RF)</td>
                              
                              <td valign="top" class="cell">0.87</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Support Vector Machines (SVM)</td>
                              
                              <td valign="top" class="cell">0.78</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Extreme Gradient Boosting (XGBoost)</td>
                              
                              <td valign="top" class="cell">0.92</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>AUC scores of all the classifiers tested for manual activation prediction
                        </div>
                     </div>
                     
                     
                     <div id="table02" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Weight</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Feature</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.1410</td>
                              
                              <td valign="top" class="cell">Right wrist y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.1281</td>
                              
                              <td valign="top" class="cell">Right wrist x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0928</td>
                              
                              <td valign="top" class="cell">Left wrist y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0917</td>
                              
                              <td valign="top" class="cell">Left wrist x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0717</td>
                              
                              <td valign="top" class="cell">Nose x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0658</td>
                              
                              <td valign="top" class="cell">Left shoulder x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0623</td>
                              
                              <td valign="top" class="cell">Left elbow y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0588</td>
                              
                              <td valign="top" class="cell">Right elbow y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0552</td>
                              
                              <td valign="top" class="cell">Nose y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0517</td>
                              
                              <td valign="top" class="cell">Left shoulder y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Left elbow x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Right elbow x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Right shoulder y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0364</td>
                              
                              <td valign="top" class="cell">Right shoulder x</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Importance of each feature during manual activation as predicted by the Extreme
                           Gradient Boosting classifier
                        </div>
                     </div>
                     
                     
                     <div id="figure03" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.jpg" rel="external"><img src="resources/images/figure03.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Final output of the manual activation tool as seen in ELAN. Signing sequences have
                           been given a 'signing' gloss for readability. This attribute can be easily changed
                           to
                           produce empty glosses.
                        </div>
                     </div>
                     
                     
                     <div id="figure04" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.jpg" rel="external"><img src="resources/images/figure04.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Bounding boxes are calculated in order to normalize the body joint coordinates for
                           each signer. After this process the normalized coordinates are passed to the XGBoost
                           classifier.<a class="noteRef" href="#d4e747">[3]</a></div>
                     </div>
                     
                     
                  </div>
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.2 Tool 2: Number of Hands</h2>
                     
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">The second tool is responsible for not only recognizing whether a person in a video
                        is
                        signing but also if the sign is one or two-handed. We have previously hypothesized
                        that
                        this is a more complex task than the previous binary classification. Results on the
                        accuracy of all the classifiers suggest that it is not as intricate as initially thought
                        of; the higher the sliding window interval, the lower the accuracy of the model. As
                        seen
                        in <a href="#figure05">Figure 5</a> of all classifiers tested, Random forest had
                        the highest accuracy at the sliding window interval of 1 frame at a time. Similarly
                        to
                        the previous experiment, a frame-to-frame prediction can produce the highest
                        results.
                     </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Furthermore, the results regarding the Long-Short-Term-Memory networks (<a href="#figure06">Figure 6</a>) suggest that the highest accuracy can be achieved at
                        a sliding window interval of 56 frames and at a hidden layer size of 8 units. However,
                        such a high window interval contains more than one sign, as the average length of
                        a sign
                        is approximately 14 frames. This discrepancy can be caused due to the architectural
                        properties of the LSTM network. The average length of the signs is too small for the
                        network to converge. The LSTM units needed more timesteps in order to prevent
                        overfitting to the data. This property in addition to the small dataset used to train
                        the network caused this anomaly.
                     </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">Although the tool performs well on predicting whether a sign is one- or two-handed
                        (using a Random Forest classifier) there are cases were the output is not as expected.
                        In particular, cases where there is a two-handed symmetrical sign produced, the tool
                        fails to accurately predict the correct class. It is likely that such signs were
                        under-presented in our data set, thus resulting in poor classification.
                     </div>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41"></div>
                     
                     
                     <div class="table">
                        <table class="table"></table>
                        <div class="caption-no-label">
                           <div class="label">Table 3. </div>
                        </div>
                     </div>
                     
                     
                     <div id="figure05" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 5. </div>Accuracy of different classifiers in various sliding window intervals in order to
                           predict whether a sequence contains a one- or a two- handed sign or not signing at
                           all. At a sliding window interval of 1 (a), Random Forest shows the highest
                           accuracy.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42"></div>
                     
                     
                     <div id="figure06" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.jpg" rel="external"><img src="resources/images/figure06.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 6. </div>Accuracy of Long-Short-Term-Memory networks on the test set with different sliding
                           window intervals (x) and hidden layer sizes (8,16,32,64,256).
                        </div>
                     </div>
                     
                     
                     
                  </div>
                  
                  
                  <div class="div div1">
                     
                     <h2 class="head">4.3 Tool 3: Handshape</h2>
                     
                     
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">In order to understand the distribution of the different handshapes presented in a
                        video, Principal Component Analysis (PCA) was utilized on all the normalized finger
                        joint coordinates for all the frames at once (<a href="#figure07">Figure 7a</a>).
                        This process allows us to reduce the dimensionality of the data while retaining as
                        much
                        as possible of the variance in the dataset. Each multidimensional array of the extracted
                        finger joints positions, for each frame, has been reduced to a single x,y coordinate.
                        The result already suggests that there are regions dense enough to be considered
                        different clusters. The utilized elbow method suggested that at k=5 the highest
                        classification could be achieved (<a href="#figure07">Figure 7b</a>). On the video
                        sample used in our study that number seemed to reflect the proper amount of discerned
                        handshapes. However, as OpenPose captures all the finger configurations in each frame
                        it
                        is at the linguist’s discretion to decide on when a handshape is significantly different
                        from another. Additionally, experiments to optimize the hyperparameters (eta, min
                        samples and leaf size) for the DBSCAN failed to create an accurate clustering (<a href="#figure07">Figure 7c</a>). Subsequently, the module creates annotation slots
                        for the different handshapes in the video and adds an overlay containing the number
                        of
                        the predicted cluster on each frame.
                     </div>
                     
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">However, special consideration must be given to the overall handshape recognition
                        module. Although the hand normalization process prepares the finger joints adequately
                        enough to be used in the clustering methods, it fails to account for hands perpendicular
                        to the camera’s point of view. Additionally, handshapes that are similar to each other
                        but are rotated towards or outwards of the signer’s body will most probably clustered
                        differently. Some of these limitations can be solved by manually editing the cluster
                        numbers prior to the annotation process.
                     </div>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">In its current form, this method can already be used to either fully annotate the
                        handshapes in a video sample or be used in different samples and treated as weakly
                        annotated data in order to be used in other handshape classifiers similarly to Koller’s
                        et al. study [<a class="ref" href="#koller2016">Koller et al. 2016</a>].
                     </div>
                     
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46"></div>
                     
                     <div class="table">
                        <table class="table"></table>
                        <div class="caption-no-label">
                           <div class="label">Table 4. </div>
                        </div>
                     </div>
                     
                     
                     <div id="figure07" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 7. </div>Visualizations produced by the handshape recognition module. Principal component
                           analysis (a) can be used to reduce the dimensionality of the finger joints
                           coordinates. Two clustering methods, namely K-means (b) and DBSCAN (c), can be used
                           to
                           detect the different handshapes presented in a video sample.
                        </div>
                     </div>
                     
                     
                     <div id="figure08" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 8. </div>Different handshapes recognized by the handshape recognition module using
                           K-means.
                        </div>
                     </div>
                     
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5. Discussion</h1>
                  
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">In this study we have presented three different tools that can be used to assist the
                     annotation process of sign language corpora. The first tool proved to be robust on
                     the
                     task of classification of manual activation even when the corpora are noisy, of poor
                     quality and most importantly containing more than one signer. This eliminates the
                     preprocessing stage that many sign language corpora have to endure where either dedicated
                     cameras per signer are utilized or manually cropping the original video. As a result,
                     a
                     more natural filming process can be applied. One limitation regarding our methodology
                     is
                     that at its current state is not possible to account for individual sign temporal
                     classification. Reaching such level would require to fuse additional information into
                     the
                     training sets which in most cases might be language specific. However, it is possible
                     to
                     get a per sign prediction when the "number of hands involved" feature changes.
                  </div>
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">The most striking observation to emerge from our methodology is that there is no
                     necessity of having massive training sets for the classification of low-level features
                     (such as manual activation and number of hands involved). In contrast to earlier studies
                     using neural networks for sign language recognition [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                     [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]
                     [<a class="ref" href="#zhang2013">Zhang et al. 2013</a>], we used a proportionally smaller dataset. Additionally, this
                     is the first time to our knowledge where corpora outside of studio conditions have
                     been
                     used to train and most importantly test models and tools for sign language automatic
                     annotation. Furthermore, such findings can be applied in other studies as well. It
                     is a
                     common misconception that only large data sets can be used for analysis. Such a trend,
                     although true for deep learning purposes, can be daunting for digital humanities
                     researchers without in depth data science knowledge. In our study, we have shown that
                     even
                     with a small and noisy dataset of visual materials, researchers can use machine learning
                     algorithms to effectively extract meaningful information. Our testing in West African
                     Sign
                     Language corpora showed that such frameworks can work effectively in different skin
                     color
                     participants lifting possible bias by previously developed algorithms.
                  </div>
                  
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">There are few limitations regarding our methodologies, particularly with respect to
                     the
                     handshape distribution module. Low quality video and consequently framerate seem to
                     affect
                     the robustness of OpenPose. As a result, finger joint prediction can be noisy and
                     of low
                     confidence. Additionally, we observed that finger joints could not be predicted when
                     the
                     elbow was not visible in the frame, and thus, losing that information. In our study
                     we
                     treated all predicted joints equally but it is necessary for future research to include
                     the prediction confidence interval as an additional variable. Furthermore, on the
                     current
                     output from OpenPose it is difficult to extract the palm orientation attribute meaning
                     that differently rotated handshapes might result in the same cluster. Future research
                     will
                     concentrate on fixing that issue as well as creating an additional tool for the annotation
                     of this feature.
                  </div>
                  
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">In the sign language domain, researchers can use our tools to recognize the times
                     of
                     interest and basic phonological features on newly compiled corpora. Additionally,
                     such
                     extracted features can be further used to measure variation on different sign languages
                     or
                     signers, for example, to measure the distribution of one- and two-handed signs or
                     particular handshapes. Moreover, other machine or deep learning experiments can benefit
                     from our tools by using them to extract only the meaningful information from the corpora
                     during the data gathering process, thus reducing possible noise in the datasets. Our
                     tools
                     can also be used towards automatic gloss suggestion. A future model can search only
                     the
                     signing sequences predicted by our tool rather than "scanning" the whole video corpus,
                     and
                     consequently making it more efficient.
                  </div>
                  
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">Outside the sign language domain, the results have further strengthened our confidence
                     that pre-trained frameworks can be used to help extract meaningful information from
                     audio-visual materials. In particular, OpenPose can be a useful asset when human activity
                     needs to be tracked and recognized in a video without the need of special hardware
                     setups.
                     Its accurate tracking allows researchers to use it in videos compiled outside studio
                     conditions. As a result, studies in the audio-visual domain can benefit from
                     community-created materials involving natural and unbiased communication. Using our
                     tools,
                     these study areas can analyze and classify human activity beyond the sign language
                     discipline in large scale cultural archives or specific domains such as gestural research,
                     dance or theater and cinema related studies, to name but a few. For example, video
                     analyses in gestural and media studies can benefit from such an automatic approach
                     to find
                     relevant information regarding user-generated data on social media and other popular
                     platforms.
                  </div>
                  
                  <div class="counter"><a href="#p52">52</a></div>
                  <div class="ptext" id="p52">Finally, due to the cumbersome installation process of OpenPose for the majority of
                     SL
                     linguists, we have decided to implement part of the tools in an online collaborative
                     environment on a cloud service provided by Google (i.e. Google Colab). In this environment
                     a temporary instance of OpenPose can be installed along with our developed python
                     modules.
                     In a simple step-based manner, the researcher can upload the relevant videos and download
                     the automatically generated annotation files. Find the link to this Colab in the footnote
                     below <a class="noteRef" href="#d4e914">[4]</a>.
                     Additionally, we have a created another environment for re-training purposes. By doing
                     so,
                     the researcher can re-train the models on his or her particular data and ensure the
                     aforementioned accuracy on them<a class="noteRef" href="#d4e918">[5]</a>.
                  </div>
                  
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Conclusion</h1>
                  
                  <div class="counter"><a href="#p53">53</a></div>
                  <div class="ptext" id="p53">To summarise, glossing sign language corpora is a cumbersome and time-consuming task.
                     Current approaches to automatize parts of this process need special video recording
                     devices (such as Microsoft Kinect), large amount of data in order to train deep learning
                     architectures to recognize a set of signs and can be prone to skin-color bias. In
                     this
                     study we explored the use of a pre-trained pose estimation framework created by Cao
                     et al.
                     [<a class="ref" href="#cao2017">Cao et al. 2017</a>] in order to create three tools and methods to predict sign
                     occurrences, number of hands involved, and handshapes. The results show that four
                     minutes
                     of annotated data are adequate enough to train a classifier (namely XGBoost) to predict
                     whether one or more persons are signing or not as well as the number of hands used
                     (using
                     Random Forest). Additionally, we examined the use of K-means and DBSCAN as clustering
                     methods to detect the different handshapes presented in the video. Because of the
                     low
                     complexity of the finger joint data extracted from the pose estimation library, K-means
                     was found to produce accurate results.
                  </div>
                  
                  <div class="counter"><a href="#p54">54</a></div>
                  <div class="ptext" id="p54">The significance of this study lies in the fact that the tools created do not rely
                     on
                     specialized cameras nor require large amount of information to be trained. Additionally,
                     they can be easily used by researchers without developing skills and adjusted to work
                     in
                     any kind of sign language corpus irrespective of its quality or the number of people
                     in
                     the video. Finally, they have the potential to be extended and used in other audio-visual
                     material that involve human activity such as gestural corpora.
                  </div>
                  
               </div>
               
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix A</h1>
                  
                  <div class="counter"><a href="#p55">55</a></div>
                  <div class="ptext" id="p55"> The input shape of the LSTM network trained to recognize the “number of
                     hands” (Tool 2) feature is a three dimensional array that can be defined as:
                     [samples × timesteps × features] where features is a 2 dimensional array of [21 ×
                     2]
                     containing the pixel x,y coordinates of the finger joints and timesteps are the sliding
                     window interval. Two Dense layers of 7 and 1 unit respectively follow the previous
                     Bidirectional LSTM layer. The activation function used is “ReLU” and
                     the dropout rate at 0.4. The architecture that produced the highest results for this
                     network can be seen in <a href="#figure09">Figure 9</a>. 
                  </div>
                  
                  <div id="figure09" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 9. </div>Architecture of the Long-Short-Term-Memory network trained for Tool 2.
                     </div>
                  </div>
                  
               </div>
               
               
               
               
               
               
               
            </div>
            
            
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e429"><span class="noteRef lang en">[1] Linear Regression (LR), Decision Trees (CART), Support Vector Machines (SVM),
                     Random Forest (RF) and Gradient Boosting (GBM).</span></div>
               <div class="endnote" id="d4e498"><span class="noteRef lang en">[2] eta: 0.23, gamma: 3, lambda: 2, max. delta step: 4,
                     max. depth: 37 and min. child weight: 4</span></div>
               <div class="endnote" id="d4e747"><span class="noteRef lang en">[3] <a href="https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s" onclick="window.open('https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s'); return false" class="ref">https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s</a></span></div>
               <div class="endnote" id="d4e914"><span class="noteRef lang en">[4] <a href="https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA" onclick="window.open('https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA'); return false" class="ref">https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA</a></span></div>
               <div class="endnote" id="d4e918"><span class="noteRef lang en">[5] <a href="https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y" onclick="window.open('https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y'); return false" class="ref">https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y</a></span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="agha2018">
                     <!-- close -->Agha et al. 2018</span> Agha, R. A. A. R., Sefer, M. N. and Fattah,
                  P. (2018). “A Comprehensive Study on Sign Languages Recognition
                  Systems Using (SVM, KNN, CNN and ANN)”. <cite class="title italic">First International
                     Conference on Data Science, E-Learning and Information Systems</cite>. (DATA ’18).
                  Madrid, Spain: ACM, pp. 28:1–28:6.
               </div>
               <div class="bibl"><span class="ref" id="aitpayev2016">
                     <!-- close -->Aitpayev et al. 2016</span> Aitpayev, K., Islam, S. and
                  Imashev, A. (2016). “Semi-automatic annotation tool for sign
                  languages”. <cite class="title italic">2016 IEEE 10th International Conference on
                     Application of Information and Communication Technologies (AICT)</cite>. Baku,
                  Azerbaijan: IEEE, pp. 1–4.
               </div>
               <div class="bibl"><span class="ref" id="andriluka2014">
                     <!-- close -->Andriluka et al. 2014</span> Andriluka, M., Pishchulin, L.,
                  Gehler, P. and Schiele, B. (2014). “2D Human Pose Estimation: New
                  Benchmark and State of the Art Analysis”. <cite class="title italic">IEEE Conference on
                     Computer Vision and Pattern Recognition (CVPR)</cite>.
               </div>
               <div class="bibl"><span class="ref" id="buehler2009">
                     <!-- close -->Buehler et al. 2009</span> Buehler, P., Zisserman, A. and
                  Everingham, M. (2009). “Learning sign language by watching TV (using
                  weakly aligned subtitles)”. <cite class="title italic">2009 IEEE Conference on Computer
                     Vision and Pattern Recognition</cite>. pp. 2961–68.
               </div>
               <div class="bibl"><span class="ref" id="cao2017">
                     <!-- close -->Cao et al. 2017</span> Cao, Z., Simon, T., Wei, S.-E. and Sheikh, Y.
                  (2017). “Realtime Multi-person 2D Pose Estimation Using Part Affinity
                  Fields”. <cite class="title italic">IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)</cite>. Honolulu, HI: IEEE, pp. 1302–10.
               </div>
               <div class="bibl"><span class="ref" id="charles2014">
                     <!-- close -->Charles et al. 2014</span> Charles, J., Pfister, T., Everingham,
                  M. and Zisserman, A. (2014). “Automatic and Efficient Human Pose
                  Estimation for Sign Language Videos”. <cite class="title italic">International Journal
                     of Computer Vision</cite>, 110(1): 70–90.
               </div>
               <div class="bibl"><span class="ref" id="chollet2015">
                     <!-- close -->Chollet 2015</span> Chollet, F. and others (2015). <cite class="title italic">Keras</cite>. <a href="https://keras.io" onclick="window.open('https://keras.io'); return false" class="ref">https://keras.io</a>.
               </div>
               <div class="bibl"><span class="ref" id="cooper2007">
                     <!-- close -->Cooper and Bowden 2007</span> Cooper, H. and Bowden, R. (2007).
                  “Large Lexicon Detection of Sign Language”. In Lew, M., Sebe,
                  N., Huang, T. S. and Bakker, E. M. (eds), <cite class="title italic">Human–Computer
                     Interaction</cite>. (Lecture Notes in Computer Science). Springer Berlin Heidelberg,
                  pp. 88–97.
               </div>
               <div class="bibl"><span class="ref" id="cooper2011">
                     <!-- close -->Cooper et al. 2011</span> Cooper, H., Holt, B. and Bowden, R.
                  (2011). “Sign Language Recognition”. In Moeslund, T. B.,
                  Hilton, A., Krüger, V. and Sigal, L. (eds), <cite class="title italic">Visual Analysis of
                     Humans: Looking at People</cite>. London: Springer London, pp. 539–62.
               </div>
               <div class="bibl"><span class="ref" id="dreuw2008">
                     <!-- close -->Dreuw and Ney 2008</span> Dreuw, P. and Ney, H. (2008). “Towards automatic sign language annotation for the ELAN tool”.
                  <cite class="title italic">LREC Workshop: Representation and Processing of Sign
                     Languages</cite>. Marrakech, Morocco, pp. 50–53.
               </div>
               <div class="bibl"><span class="ref" id="eichner2009">
                     <!-- close -->Eichner and Ferrari 2009</span> Eichner, M. and Ferrari, V.
                  (2009). “Better appearance models for pictorial structures”.
                  <cite class="title italic">British Machine Vision Conference 2009</cite>. London, UK: British
                  Machine Vision Association, pp. 3.1-3.11.
               </div>
               <div class="bibl"><span class="ref" id="eichner2012">
                     <!-- close -->Eichner et al. 2012</span> Eichner, M., Marin-Jimenez, M.,
                  Zisserman, A. and Ferrari, V. (2012). “2D Articulated Human Pose
                  Estimation and Retrieval in (Almost) Unconstrained Still Images”. <cite class="title italic">International Journal of Computer Vision</cite>, 99(2): 190–214.
               </div>
               <div class="bibl"><span class="ref" id="fang2004">
                     <!-- close -->Fang et al. 2004</span> Fang, G., Gao, W. and Zhao, D. (2004).
                  “Large Vocabulary Sign Language Recognition Based on Fuzzy Decision
                  Trees”. <cite class="title italic">Systems, Man and Cybernetics, Part A: Systems and
                     Humans, IEEE Transactions On</cite>, 34: 305–14 doi:10.1109/TSMCA.2004.824852.
               </div>
               <div class="bibl"><span class="ref" id="farhadi2007">
                     <!-- close -->Farhadi et al. 2007</span> Farhadi, A., Forsyth, D. and White,
                  R. (2007). “Transfer Learning in Sign language”. <cite class="title italic">2007 IEEE Conference on Computer Vision and Pattern Recognition</cite>.
                  Minneapolis, MN, USA, pp. 1–8.
               </div>
               <div class="bibl"><span class="ref" id="felzenszwalb2005">
                     <!-- close -->Felzenszwalb and Huttenlocher 2005</span> Felzenszwalb, P.
                  F. and Huttenlocher, D. P. (2005). “Pictorial Structures for Object
                  Recognition”. <cite class="title italic">International Journal of Computer
                     Vision</cite>, 61(1): 55–79.
               </div>
               <div class="bibl"><span class="ref" id="forster2012">
                     <!-- close -->Forster 2012</span> Forster, J., Schmidt, C., Hoyoux, T.,
                  Koller, O., Zelle, U., Piater, J. and Ney, H. (2012). “RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation
                  Corpus”. Istanbul, Turkey, pp. 3785–3789.
               </div>
               <div class="bibl"><span class="ref" id="johnson2009">
                     <!-- close -->Johnson and Everingham 2009</span> Johnson, S. and Everingham,
                  M. (2009). “Combining discriminative appearance and segmentation cues
                  for articulated human pose estimation”. <cite class="title italic">2009 IEEE 12th
                     International Conference on Computer Vision Workshops, ICCV Workshops</cite>. pp.
                  405–12.
               </div>
               <div class="bibl"><span class="ref" id="johnston2010">
                     <!-- close -->Johnston 2010</span> Johnston, T. (2010). “From archive to corpus: Transcription and annotation in the creation of signed language
                  corpora”. <cite class="title italic">International Journal of Corpus
                     Linguistics</cite>, 15(1): 106–31 doi:10.1075/ijcl.15.1.05joh. <a href="http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh" onclick="window.open('http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh'); return false" class="ref">http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh</a> (accessed 19
                  May 2020).
               </div>
               <div class="bibl"><span class="ref" id="kelly2009">
                     <!-- close -->Kelly et al. 2009</span> Kelly, D., Reilly Delannoy, J., Mc
                  Donald, J. and Markham, C. (2009). “A framework for continuous
                  multimodal sign language recognition”. <cite class="title italic">Proceedings of the
                     2009 International Conference on Multimodal Interfaces</cite>. pp. 351–358.
               </div>
               <div class="bibl"><span class="ref" id="koller2016">
                     <!-- close -->Koller et al. 2016</span> Koller, O., Ney, H. and Bowden, R.
                  (2016). “Deep Hand: How to Train a CNN on 1 Million Hand Images When
                  Your Data is Continuous and Weakly Labelled”. <cite class="title italic">IEEE
                     Conference on Computer Vision and Pattern Recognition (CVPR)</cite>. Las Vegas, NV,
                  USA: IEEE, pp. 3793–802.
               </div>
               <div class="bibl"><span class="ref" id="kumar2017">
                     <!-- close -->Kumer 2017</span> Kumar, N. (2017). “Motion
                  trajectory based human face and hands tracking for sign language recognition”.
                  <cite class="title italic">2017 4th IEEE Uttar Pradesh Section International Conference on
                     Electrical, Computer and Electronics (UPCON)</cite>. Mathura: IEEE, pp. 211–16.
               </div>
               <div class="bibl"><span class="ref" id="lin2014">
                     <!-- close -->Lin 2014</span> Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L.,
                  Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L. and Dollár, P. (2014).
                  <cite class="title italic">Microsoft COCO: Common Objects in Context</cite>.
               </div>
               <div class="bibl"><span class="ref" id="lubbers2013">
                     <!-- close -->Lubbers and Torreira 2013</span> Lubbers, M. and Torreira, F.
                  (2013). <cite class="title italic">A Python Module for Processing ELAN and Praat Annotation
                     Files: Dopefishh/Pympi</cite>. Python <a href="https://github.com/dopefishh/pympi" onclick="window.open('https://github.com/dopefishh/pympi'); return false" class="ref">https://github.com/dopefishh/pympi</a>.
               </div>
               <div class="bibl"><span class="ref" id="moeslund2006">
                     <!-- close -->Moselund et al. 2006</span> Moeslund, T. B., Hilton, A. and
                  Krüger, V. (2006). “A survey of advances in vision-based human motion
                  capture and analysis”. <cite class="title italic">Computer Vision and Image
                     Understanding</cite>, 104(2): 90–126.
               </div>
               <div class="bibl"><span class="ref" id="nyst2012a">
                     <!-- close -->Nyst 2012</span> Nyst, V. (2012). <cite class="title italic">A Reference
                     Corpus of Adamorobe Sign Language</cite>. A digital, annotated video corpus of the sign
                  language used in the village of Adamorobe, Ghana.
               </div>
               <div class="bibl"><span class="ref" id="nyst2012b">
                     <!-- close -->Nyst et al. 2012</span> Nyst, V., Magassouba, M. M. and Sylla, K.
                  (2012). <cite class="title italic">Un Corpus de reference de la Langue des Signes Malienne
                     II</cite>. A digital, annotated video corpus of local sign language use in the Dogon
                  area of Mali.
               </div>
               <div class="bibl"><span class="ref" id="pedregosa2011">
                     <!-- close -->Pedregosa et al. 2011</span> Pedregosa, F., Varoquaux, G.,
                  Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). “Scikit-learn: Machine Learning in Python”. <cite class="title italic">Journal of Machine Learning Research</cite>, 12: 2825–2830.
               </div>
               <div class="bibl"><span class="ref" id="pigou2015">
                     <!-- close -->Pigou et al. 2015</span> Pigou, L., Dieleman, S., Kindermans,
                  P.-J. and Schrauwen, B. (2015). “Sign Language Recognition Using
                  Convolutional Neural Networks”. In Agapito, L., Bronstein, M. M. and Rother, C.
                  (eds), <cite class="title italic">Computer Vision - ECCV 2014 Workshops</cite>. (Lecture Notes
                  in Computer Science). Cham: Springer International Publishing, pp. 572–78.
               </div>
               <div class="bibl"><span class="ref" id="ramanan2007">
                     <!-- close -->Ramanan et al. 2007</span> Ramanan, D., Forsyth, D. A. and
                  Zisserman, A. (2007). “Tracking People by Learning Their
                  Appearance”. <cite class="title italic">IEEE Transactions on Pattern Analysis and
                     Machine Intelligence</cite>, 29(1): 65–81.
               </div>
               <div class="bibl"><span class="ref" id="roussos2012">
                     <!-- close -->Roussos et al. 2012</span> Roussos, A., Theodorakis, S.,
                  Pitsikalis, V. and Maragos, P. (2012). “Hand Tracking and Affine
                  Shape-Appearance Handshape Sub-units in Continuous Sign Language Recognition”. In
                  Kutulakos, K. N. (ed), <cite class="title italic">Trends and Topics in Computer Vision</cite>,
                  vol. 6553. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 258–72.
               </div>
               <div class="bibl"><span class="ref" id="sapp2010">
                     <!-- close -->Sapp et al. 2010</span> Sapp, B., Jordan, C. and Taskar, B. (2010).
                  Adaptive pose priors for pictorial structures. <cite class="title italic">2010 IEEE Computer
                     Society Conference on Computer Vision and Pattern Recognition</cite>. pp.
                  422–29.
               </div>
               <div class="bibl"><span class="ref" id="sivic2006">
                     <!-- close -->Sivic et al. 2006</span> Sivic, J., Zitnick, C. L. and Szeliski,
                  R. (2006). “Finding people in repeated shots of the same
                  scene”. <cite class="title italic">British Machine Vision Conference 2006</cite>, vol.
                  3. Edinburgh: British Machine Vision Association, pp. 909–18.
               </div>
               <div class="bibl"><span class="ref" id="sloetjes2008">
                     <!-- close -->Sloetjes and Wittenburg 2008</span> Sloetjes, H. and
                  Wittenburg, P. (2008). “Annotation by category - ELAN and ISO DCR.
                  Marrakech, Morocco”, p. 5 <a href="https://tla.mpi.nl/tools/tla-tools/elan/" onclick="window.open('https://tla.mpi.nl/tools/tla-tools/elan/'); return false" class="ref">https://tla.mpi.nl/tools/tla-tools/elan/</a>.
               </div>
               <div class="bibl"><span class="ref" id="starner1998">
                     <!-- close -->Starner et al. 1998</span> Starner, T., Weaver, J. and Pentland,
                  A. (1998). “Real-time American sign language recognition using desk
                  and wearable computer based video”. <cite class="title italic">IEEE Transactions on
                     Pattern Analysis and Machine Intelligence</cite>, 20(12): 1371–75.
               </div>
               <div class="bibl"><span class="ref" id="yang2011">
                     <!-- close -->Yang and Ramanan 2011</span> Yang, Y. and Ramanan, D. (2011).
                  “Articulated pose estimation with flexible
                  mixtures-of-parts”. <cite class="title italic">IEEE Conference on Computer Vision and
                     Pattern Recognition (CVPR)</cite>. pp. 1385–92.
               </div>
               <div class="bibl"><span class="ref" id="zhang2013">
                     <!-- close -->Zhang et al. 2013</span> Zhang, C., Yang, X. and Tian, Y. (2013).
                  “Histogram of 3D Facets: A characteristic descriptor for hand
                  gesture recognition.”
                  <cite class="title italic">2013 10th IEEE International Conference and Workshops on Automatic
                     Face and Gesture Recognition (FG)</cite>. pp. 1–8.
               </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>