<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 014 Number 4
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol/14/4/000510.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol/14/4/000510/000510.html#disqus_thread" data-disqus-identifier="000510">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            
            
            <div class="DHQheader">
               
               
               
               <h1 class="articleTitle lang en">Towards a User-Friendly Tool for
                  Automated Sign Annotation: Identification and Annotation of Time
                  Slots, Number of Hands, and Handshape
                  
               </h1>
               
               <div class="author"><span style="color: grey">Manolis Fragkiadakis</span> &lt;<a href="mailto:m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl'); return false;">m_dot_fragkiadakis_at_hum_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               <div class="author"><span style="color: grey">Victoria A.S. Nyst</span> &lt;<a href="mailto:v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl'); return false;">v_dot_a_dot_s_dot_nyst_at_hum_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               <div class="author"><span style="color: grey">Peter van der Putten</span> &lt;<a href="mailto:p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl'); return false;">p_dot_w_dot_h_dot_van_dot_der_dot_putten_at_liacs_dot_leidenuniv_dot_nl</a>&gt;, Leiden University
               </div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Towards%20a%20User-Friendly%20Tool%20for%20Automated%20Sign%20Annotation%3A%20Identification%20and%20Annotation%20of%20Time%20Slots,%20Number%20of%20Hands,%20and%20Handshape&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=014&amp;rft.issue=4&amp;rft.aulast=Fragkiadakis&amp;rft.aufirst=Manolis&amp;rft.au=Manolis%20Fragkiadakis&amp;rft.au=Victoria A.S.%20Nyst&amp;rft.au=Peter%20van der Putten"> </span></div>
            
            
            
            
            <div id="DHQtext">
               
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>The annotation process of sign language corpora in terms of
                     glosses, is a highly labor-intensive task, but a condition for a
                     reliable quantitative analysis. During the annotation process the
                     researcher typically defines the precise time slot in which a sign
                     occurs and then enters the appropriate gloss for the sign. The aim
                     of this project is to develop a set of tools to assist the
                     annotation of the signs and their formal features in a video
                     irrespectively of its content and quality. Recent advances in the
                     field of deep learning have led to the development of accurate and
                     fast pose estimation frameworks. In this study, such a framework
                     (namely OpenPose) has been used to develop three different methods
                     and tools to facilitate the annotation process. The first tool
                     estimates the span of a sign sequence and creates empty slots in an
                     annotation file. The second tool detects whether a sign is one- or
                     two-handed. The last tool recognizes the different handshapes
                     presented in a video sample. All tools can be easily re-trained to
                     fit the needs of the researcher.
                  </p>
                  
               </div>
               
               
               
               
               
               
               <div class="div div0">
                  <h1 class="head">Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">While the majority of the studies in the field of digital humanities have
                     been mostly text oriented, the evolution in computing power and technology
                     has resulted in a shift towards multimedia-oriented studies. Recently,
                     advances in computer vision have started to find practical applications in
                     study domains outside of computer and data science. Video is one of the most
                     important time-based media as it has the ability to carry large amount of
                     digital information in a condensed form, and hence it serves as a rich medium
                     to capture various forms of cultural expression. Automated processing and
                     annotation of large numbers of videos is now becoming feasible due to the
                     evolution of computer vision and machine learning.
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">In sign language linguistics, a transition took place from paper-based
                     materials to large video corpora to facilitate the study of the languages in
                     question. Sign language corpora are mainly composed of video data. The primary
                     goal of these video corpora is to study sign language functioning.
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The processing of sign languages usually involves requires a form of textual
                     representation [<a class="ref" href="#dreuw2008">Dreuw and Ney 2008</a>], most notably glosses for annotation. Sign
                     language glosses are words from a spoken language. Uniquely identifying glosses
                     by definition refer to a specific sign. Such ID glosses are an essential
                     element for the quantitative analysis of a sign language corpus
                     [<a class="ref" href="#johnston2010">Johnston 2010</a>]. Typically, sign language linguists add glosses
                     and other annotations to
                     the video recordings with the use of a software tool (namely ELAN). ELAN allows
                     researchers to add time-aligned annotations to a video. However, this task
                     requires a lot of time and can be prone to errors.
                  </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">New advances in computer vision open up additional ways of studying videos
                     containing sign language data, extracting formal representations of linguistic
                     phenomena, and implementing these in computer applications, such as automatic
                     recognition, generation, and translation. Using computer vision and machine
                     learning enables quick and new ways of processing large sets of video data,
                     which in turns makes it possible to address research questions that were not
                     feasible before.
                  </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">This study is the first part of a project aiming at the creation of tools
                     to automatize part of the annotation process of sign language video data.
                     This paper presents the methodologies, tools and implementations of three
                     functionalities: the detection of 1) manual activation 2) the number of hands
                     involved and 3) the handshape distribution on sign language corpora.
                  </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Recent developments in sign language recognition illustrate the
                     advantages of machine and deep learning for tasks related to recognition and
                     classification
                     [<a class="ref" href="#agha2018">Agha et al. 2018</a>]
                     [<a class="ref" href="#cao2017">Cao et al. 2017</a>]
                     [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>].
                     Nevertheless, current approaches are restricted in various ways, limiting
                     their applicability in current sign language research. For example, training
                     deep learning networks requires a vast amount of data as well as adequate
                     computational power. These networks are usually trained in one sign language
                     and they do not generalize well in other sign languages.
                  </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Additionally, current approaches in sign language automatic annotation
                     need manual annotation of the hands and body joints for the training of the
                     recognizer models
                     [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                     [<a class="ref" href="#dreuw2008">Dreuw and Ney 2008</a>]. Moreover,
                     the application of color and motion detection algorithms [<a class="ref" href="#kumar2017">Kumer 2017</a>], as
                     feature extraction methods, can be susceptible to errors and possibly skin
                     color bias. Finally, several hand tracking models only work on a particular
                     type of recordings, for example, a signer wearing gloves, or recordings made
                     with Microsoft’s Kinect [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]. As a result, these models are not
                     usable for the majority of the existing sign language corpora which have been
                     recorded with standard RGB cameras.
                  </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Our methods have been developed and tested on two West African sign language
                     corpora containing natural conditions with non-Caucasian signers. While most
                     studies in the sign language recognition field have mainly concerned signers
                     with light skin tones, little research has been conducted using darker skin
                     tones. With the emergence of corpora compiled in African countries under
                     challenging real-world conditions, and their contribution to the overall sign
                     language community, it is of utmost importance to test how methods perform
                     in such a domain. Alleviating biases and increasing diversity should be a top
                     priority of any computer assisted study.
                  </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">In this study, a pre-trained deep learning pose estimation library developed
                     by Cao et al. [<a class="ref" href="#cao2017">Cao et al. 2017</a>] named OpenPose has been used to extract body and finger
                     key-points. OpenPose has been trained and evaluated on two large datasets for
                     multi-person pose estimation. The MPII human multi-person dataset
                     [<a class="ref" href="#andriluka2014">Andriluka et al. 2014</a>] and the COCO 2016 keypoints challenge dataset
                     [<a class="ref" href="#lin2014">Lin 2014</a>]
                     contain images of people of different age groups and ethnicities in diverse
                     scenarios. As a result, OpenPose does not have a bias toward skin color.
                     Additionally, its easy-to-use implementation makes it an ideal framework to be
                     used by linguists with limited coding experience.
                  </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">The combination of the aforementioned pose estimation framework as well
                     as the machine and deep learning architectures tested in this study, provides
                     a robust approach towards automatic annotation. Current models and tools can
                     be used in any sign language or gestural corpus independently of its quality,
                     length and number of people in the video. These tools have been developed as
                     python modules that can run automatically in a video and produce the relevant
                     annotation files requiring minimal effort from the user. More generally, as
                     large parts of our cultures nowadays are captured in video, our study serves
                     as a case example of how intelligent machine learning techniques can serve
                     digital humanities researchers by extracting semantics from large video
                     collections.
                  </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">This article is structured as follows: Section 2 introduces the developments
                     on the sign language recognition and automatic annotation fields. Section 3
                     describes the materials used in this study and the methodologies developed
                     and applied for each tool separately. Section 4 presents the results for each
                     experimental setup and tool. Section 5 contains the discussion and future
                     work while Section 6 presents our conclusions. Finally, Appendix A presents
                     the architecture and technical details of the Long-Short-Term-Memory Network
                     trained for this study.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  <h1 class="head">2. Literature review</h1>
                  
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">In this section we present the studies conducted on the sign language
                     recognition and automatic annotation field developed with depth sensors as
                     well as standard RGB cameras. Additionally, we describe the developments of the
                     human pose estimation field and we introduce the OpenPose framework that will
                     be used in this article.
                  </div>
                  
                  
                  <div class="div div1">
                     <h2 class="head">2.1 Sign Language Recognition and Automatic Annotation</h2>
                     
                     
                     <div class="counter"><a href="#p13">13</a></div>
                     <div class="ptext" id="p13">The primary goal of sign language recognition is to develop methods and
                        algorithms to accurately identify a series of produced signs and to discern
                        their meaning. The majority of studies have focused on recognizing those
                        features and methods that can properly identify a sign out of a given set of
                        possible signs. However, such methods can only be used on a particular set of
                        signs and, thus, a specific sign language, which makes it harder to study the
                        relationships between and evolution of various sign languages.
                     </div>
                     
                     <div class="counter"><a href="#p14">14</a></div>
                     <div class="ptext" id="p14">An additional motivation behind Sign Language Recognition (SLR) is to
                        build automatic sign language to speech or text translation systems to
                        assist the communication between the deaf and hearing community [<a class="ref" href="#fang2004">Fang et al. 2004</a>].
                        Moreover, SLR plays an important role in developing gesture-based
                        human–computer interaction systems [<a class="ref" href="#kelly2009">Kelly et al. 2009</a>]. Sign language
                        linguists have used such systems to facilitate the annotation process of sign
                        language corpora in order to discern the different signs in a video recording
                        and further study the linguistic phenomena presented.
                     </div>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">There are numerous studies dealing with the automated recognition of sign
                        languages as clearly presented by Cooper et al. [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>], in their review study
                        on the state-of-the-art in sign language recognition. However, the experiments
                        presented in most of these studies are either hard to replicate, or they pose
                        limitations as far as their applicability is concerned. For instance, most of
                        these studies use depth sensors, most notably MS Kinect, to capture 3D images
                        of the environment
                        [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                        [<a class="ref" href="#pigou2015">Pigou et al. 2015</a>]
                        [<a class="ref" href="#zhang2013">Zhang et al. 2013</a>].
                        As a result, using the frameworks developed in these studies requires
                        a machine with similar features as the one used for testing and most probably
                        will only work for that sign language on which they have been trained.
                     </div>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">Recently, computer vision techniques have been applied to sign language
                        recognition to overcome the aforementioned limitations. Roussos et al. [<a class="ref" href="#roussos2012">Roussos et al. 2012</a>]
                        created a skin color probabilistic model to detect and track the hands of a
                        signer on a video, while Cooper et al. [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>] use this model to segment the
                        hands and apply a classifier based on Markov models. However, systems based
                        on skin color
                        [<a class="ref" href="#buehler2009">Buehler et al. 2009</a>]
                        [<a class="ref" href="#cooper2007">Cooper and Bowden 2007</a>]
                        [<a class="ref" href="#farhadi2007">Farhadi et al. 2007</a>]
                        [<a class="ref" href="#starner1998">Starner et al. 1998</a>]
                        are prone to errors and have difficulties on
                        tracking the hands and the signer’s features against cluttered backgrounds and
                        in noisy conditions in general. Also, they do not work in videos with multiple
                        signers.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     <h2 class="head">2.2 Human Pose Estimation</h2>
                     
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">Human pose estimation has been extensively studied due to its numerous
                        applications on a number of different fields [<a class="ref" href="#moeslund2006">Moselund et al. 2006</a>]. Due to
                        low computational complexity during inference, pictorial structures have been
                        commonly used
                        [<a class="ref" href="#felzenszwalb2005">Felzenszwalb and Huttenlocher 2005</a>]
                        [<a class="ref" href="#ramanan2007">Ramanan et al. 2007</a>]
                        [<a class="ref" href="#sivic2006">Sivic et al. 2006</a>]
                        to model human pose. Recently, studies have focused on improving
                        the appearance models used in these structures by modelling the individual body
                        parts
                        [<a class="ref" href="#eichner2009">Eichner and Ferrari 2009</a>]
                        [<a class="ref" href="#eichner2012">Eichner et al. 2012</a>]
                        [<a class="ref" href="#johnson2009">Johnson and Everingham 2009</a>]
                        [<a class="ref" href="#sapp2010">Sapp et al. 2010</a>]. Felzenszwalb and Huttenlocher [<a class="ref" href="#felzenszwalb2005">Felzenszwalb and Huttenlocher 2005</a>],
                        relying on
                        the pictorial structure framework recommended a deformable part-based model.
                        Additionally, Yang and Ramanan [<a class="ref" href="#yang2011">Yang and Ramanan 2011</a>]
                        showed that a tree-structured model using
                        a combination of deformable parts can be used in order to achieve accurate
                        pose estimation. Furthermore, Charles et al. [<a class="ref" href="#charles2014">Charles et al. 2014</a>]
                        showed that human body
                        joint positions can be predicted using a random forest regressor based on a
                        co-segmentation process over all video frames.
                     </div>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">In general, most of the vision-based approaches developed for sign language
                        recognition tasks utilizing pose estimation, have used the RWTH-PHOENIX-Weather
                        data set [<a class="ref" href="#forster2012">Forster 2012</a>] to validate their models. This data set
                        consists of weather forecast airings from the German public tv-station PHOENIX
                        along with transcribed gloss annotations. However, it is a question to what
                        extent such systems tested in this data set can be replicated with real-life
                        conditions in the corpora. It is often the case that sign language and gestural
                        corpora, especially the ones filmed outside of studio conditions, have bad
                        quality, low brightness and often contain more than one person in the frame.
                        These characteristics create an additional challenge to the tracking and
                        prediction task.
                     </div>
                     
                     
                     <div class="div div2">
                        <h3 class="head">2.2.1 OpenPose</h3>
                        
                        
                        <div class="counter"><a href="#p19">19</a></div>
                        <div class="ptext" id="p19">OpenPose is a real-time, open-source library for academic purposes for
                           multi-person 2D pose estimation. It can detect body, foot, hand and facial
                           keypoints [<a class="ref" href="#cao2017">Cao et al. 2017</a>]. Following a bottom-up approach (from an entire
                           image as input to full body poses as output), it outperforms similar 2D body
                           pose estimation libraries.
                        </div>
                        
                        <div class="counter"><a href="#p20">20</a></div>
                        <div class="ptext" id="p20">A major advantage of the library is that it achieves high accuracy and
                           performance regardless of the number of people in the image. Its high accuracy
                           is performed by using a non-parametric representation of 2D vector fields.
                           These fields encode the position and orientation of body parts over the image
                           domain and their degree of association in order to learn to relate them to each
                           individual.
                        </div>
                        
                        <div class="counter"><a href="#p21">21</a></div>
                        <div class="ptext" id="p21">OpenPose is able to run on different operating systems and multiple hardware
                           architectures . Additionally, it provides tools for visualization and output
                           file generation. The output can be multiple json files containing all the pixel
                           x, y coordinates of the body, hand and face joints. In this study the DEMO
                           version on a CPU-only mode has been used to train our models. This choice was
                           made in order to ensure that reproducibility can be easily achieved without the
                           need for powerful computers from the linguist’s side.
                        </div>
                        
                        
                     </div>
                  </div>
               </div>
               
               
               <div class="div div0">
                  <h1 class="head">3. Materials and Methods</h1>
                  
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">This section describes the datasets used in our study as well as the
                     pre-processing stage using OpenPose to extract the body joints’ pixel
                     coordinates. Furthermore, we introduce the methods applied in the development
                     of each tool. Special consideration is given on the handshape recognition
                     module as an additional normalization part has been developed. 
                  </div>
                  
                  
                  <div class="div div1">
                     <h2 class="head">3.1 Data</h2>
                     
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">A data set of 7,805 frames in total (approximately 4 minutes) labeled as
                        signing or not signing has been compiled for the first part of the study. The
                        dimensions of the frames were 352 by 288 pixels and were extracted from the
                        Adamorobe and Berbey Sign Language corpora [<a class="ref" href="#nyst2012a">Nyst 2012</a>]
                        [<a class="ref" href="#nyst2012b">Nyst et al. 2012</a>].
                        These corpora portray an additional challenge as the signers have been filmed
                        in and around their homes, in natural conditions, outside of a studio, with
                        strongly varying brightness and background noise. Furthermore, they may contain
                        signing from one and two people at the same time. As a result, they can be
                        considered as one of the hardest corpora to perform classification tasks. It
                        is arguable that if the methods developed in this study can perform reasonably
                        well on corpora of such poor conditions, then they can be applied to any sign
                        language corpus under better settings.
                     </div>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">Additional videos from YouTube with higher quality have been selected
                        for testing purposes too
                        <a class="noteRef" href="#d4e377">[1]</a>.
                        For the first task of this study, the
                        original data set was split into a training and testing set of 6,150 and 1,655
                        frames respectively and the labels were one hot encoded (i.e. signing as 1 and
                        not-signing as 0).
                     </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">After a successful training of the first prediction model, the tool was
                        applied on a different part of the corpora. The predicted signing sequences
                        were manually labeled as one- or two-handed signs. Together with randomly
                        selected not-signing sequences (as predicted by the first tool), they formed
                        a second data set. The size of this data set was slightly larger than the
                        previous one: 10,120 frames in total.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     <h2 class="head">3.2 Pre-processing</h2>
                     
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">Using OpenPose, the pixel coordinates of the hands, elbows, shoulders and
                        head were extracted from each frame. In the case of the handshape recognition
                        module, the fingers joints coordinates were additionally extracted. We avoided
                        using the finger extraction module of OpenPose on the first two parts of the
                        study as that would have increased the computational time significantly. The
                        positions of the rest of the body joints were disregarded as most of the time
                        they were out of the frame bounds. Although the quality of the frames was
                        poor, it created an advantage for the pose estimation framework, reducing the
                        computational time to a reasonable level.
                     </div>
                     
                     
                  </div>
                  
                  <div class="div div1">
                     <h2 class="head">3.3 Tool 1: Manual Activation</h2>
                     
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">The first tool is a temporal segmentation method to predict the begin and
                        end frames of a sign sequence in a video sample. Thus, it is important to
                        compare the performance of multiple different machine learning algorithms
                        consistently. Four classification methods were used, namely: Support Vector
                        Machines (SVM), Random Forests (RF), Artificial Neural Networks (ANN) and
                        Extreme Gradient Boosting (XGBoost). The majority of these algorithms have
                        been extensively used in machine learning studies as well as in sign language
                        applications [<a class="ref" href="#agha2018">Agha et al. 2018</a>]. Performance was measured using the Area
                        Under the Receiver Operating Characteristics (AUROC) to validate each model.
                        The AUC is a performance measurement specifically designed for binary (i.e.
                        two class) classification problems. In general, it expresses how well a model
                        is capable of distinguishing between classes, for example whether someone
                        is signing in a video fragment or not. A model that makes random predictions
                        will have an AUC of 0.5, a perfect model will have an AUC of 1. AUC stands
                        for 'Area under the Receiver Operating Characteristic (ROC) Curve’, the curve
                        of True Positive Rate (probability of detection) versus False Positive Rate
                        (probability of false alarms). It is better than just accuracy, i.e. percentage
                        of correct predictions by the model, because it is not dependent on the
                        relative amount of positives, i.e. percentage of total videos with signs in our
                        case. We searched for the optimal setting of the various classification method
                        parameters by exhaustive testing of the possible parameter settings and testing
                        the performance on a validation set ('grid search’, searching the 'grid’ of
                        possible parameter values).
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     <h2 class="head">3.4 Tool 2: Number of Hands</h2>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">The second tool’s goal is to predict not only if a person is signing or
                        not, but also to identify the number of hands involved (one- or two-handed).
                        We hypothesized that this task is more complex than before, thus we considered
                        it as a time-series problem. By using a sliding window technique, the original
                        data set was parsed to form new training sets, where different possible
                        frame intervals (1,2,3,5 and 10) were tested. Furthermore, similar (to some
                        extend) classification methods with Tool 1 have been used
                        <a class="noteRef" href="#d4e410">[2]</a>.
                     </div>
                     
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">Moreover, recent studies in the sign language recognition field suggest that
                        the use of Long-Short-Term-Memory (LSTM) networks can yield accurate results.
                        LSTM is an artificial recurrent neural network (RNN) architecture used in the
                        field of deep learning. Unlike standard feedforward neural networks (like the
                        one tested in Tool 1) LSTM has feedback connections. It can not only process
                        single data points, but also understand patterns in entire sequences of data,
                        by combining its internal state resulting from previous input with a new
                        input data item. In our case, instead of predicting whether a specific pose
                        belongs into a class, we investigate whether a sequence of poses can be used
                        for the same purpose. In this part of the study an LSTM network with different
                        layer units as well as sliding window intervals has also been tested and
                        compared with the above traditional machine learning classifiers. The overall
                        architecture and technical details of the LSTM network can be found in Appendix
                        A.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     <h2 class="head">3.5 Tool 3: Handshape</h2>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">The
                        handshape recognition module was considered a so-called unsupervised learning
                        problem as no ground truth information regarding this feature was available
                        prior to the experiment, i.e. in contrast to the previous two problems we did
                        not know what classes (handshapes) to detect. Such an unsupervised learning
                        method can be useful in other newly compiled sign language or gestural corpora
                        where there is no information regarding the different handshapes presented by
                        the signers in the video. Additionally, an unsupervised learning method can be
                        useful in other newly compiled sign language or gestural corpora where there
                        is no information regarding the different handshapes presented by the signers
                        in the video. We approached this as a clustering task: can we find groups of
                        signs that were similar. Two different clustering methods have been tested:
                        K-means and DBSCAN. The first clustering method was chosen for its simplicity
                        as well as its fast implementation on the Python library that was utilized
                        (namely scikit-learn). However, as the complexity of the data is unknown and
                        it is case sensitive, it was decided to employ Density-Based Spatial Clustering
                        of Applications with Noise (DBSCAN) as an alternative option. Given a set of
                        points in some space, DBSCAN groups together points that are closely packed
                        together, marking as outliers the ones that lie alone in low-density regions.
                        This clustering method is one of the most common clustering algorithms.
                     </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">Determining the optimal number of clusters (i.e. total number of expected
                        handshapes) is a crucial issue in clustering methods such as K-means, which
                        requires the user to specify the number of clusters <em class="emph">k</em> to be
                        generated. The definition of clusters is done so that the total within-cluster
                        sum of square (WSS) is minimized, hence, in this study the <em class="emph">elbow
                           method</em> was utilized to estimate the number of clusters.
                     </div>
                     
                     
                     <div class="div div2">
                        <h3 class="head">3.5.1 Hand Normalization</h3>
                        
                        
                        <div class="counter"><a href="#p32">32</a></div>
                        <div class="ptext" id="p32">Since the output of OpenPose contains the raw x, y pixel positions for the
                           different finger joints, it is important to normalize them before applying the
                           clustering method. To do so, the angle of the vector between the elbow and the
                           wrist of the right hand is calculated. Subsequently, the coordinates of the
                           finger joints positions are rotated to be in parallel on the horizontal axis
                           and normalized so that their averaged location is at the origin.
                           <a href="figure01" onclick="window.open('figure01'); return false" class="ref">Figure 1</a> shows
                           the output of the overall normalization process. All
                           experiments were conducted using one machine with a hexa-core processor
                           (Intel Core i7-3930K) and 4GB RAM. The models are implemented using the Python
                           libraries scikit-learn [<a class="ref" href="#pedregosa2011">Pedregosa et al. 2011</a>] and Keras
                           [<a class="ref" href="#chollet2015">Chollet 2015</a>] for
                           their fast and easy implementation.
                        </div>
                        
                        
                        <div id="figure01" class="figure">
                           
                           
                           <div class="ptext"><a href="resources/images/figure01.jpg" rel="external"><img src="resources/images/figure01.jpg" alt="" /></a></div>
                           
                           <div class="caption">
                              <div class="label">Figure 1. </div>Signer's hand normalization is done based on the angle between the horizontal axis
                              and the vector of the elbow and wrist coordinates. The finger joints are rotated according
                              to that angle in order to be in parallel to the horizontal axis and scaled so that
                              their average location is at the origin.
                           </div>
                        </div>
                        
                        
                     </div>
                  </div>
               </div>
               
               
               <div class="div div0">
                  <h1 class="head">4. Results</h1>
                  
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">The results section consists of three parts, the first part (Section
                     4.1) discusses the results of the analysis regarding the manual activation
                     prediction. ﻿Section 4.2 discusses the results regarding the classification of
                     one- and two-handed signing sequences. Last but not least, Section 4.3 presents
                     the result regarding the handshape distribution using different clustering
                     methods.
                  </div>
                  
                  
                  <div class="div div1">
                     <h2 class="head">4.1 Tool 1: Manual Activation</h2>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">All classifiers performed adequately well, apart from the Support Vector
                        Machines (AUC: 0.80) (<a href="table01" onclick="window.open('table01'); return false" class="ref">Table 1</a>). Extreme Gradient Boosting (XGBoost) showed
                        the highest AUC score at 0.92<a class="noteRef" href="#d4e474">[3]</a>. <a href="figure02" onclick="window.open('figure02'); return false" class="ref">Figure 2</a>
                        presents the AUROC curve after
                        a 10-fold cross-validation. The Artificial Neural Network was found to perform
                        sufficiently well (AUC: 0.88). By exploring the importance of each feature on
                        the prediction of the model we observe that the y and x pixel coordinates of
                        the dominant (i.e. right) hand are on the top two positions (<a href="table02" onclick="window.open('table02'); return false" class="ref">Table 2</a>).
                     </div>
                     
                     
                     <div id="figure02" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.jpg" rel="external"><img src="resources/images/figure02.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 2. </div>10-fold cross validation of Extreme Gradient Boosting (XGBoost) classifier for the
                           prediction of manual activation.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">The fact that the Artificial Neural Network turned out to be a less
                        efficient approach than the XGBoost can be accounted to the small training
                        data set. Typically, Neural Networks require a lot more training data than
                        traditional machine learning algorithms. Additionally, designing a network
                        that correctly encodes a domain specific problem is challenging. In most cases,
                        competent architectures are only reached when a whole research community is
                        working on those problems, without short-term time constraints. Fine-tuning
                        such a network would require time and effort that reach beyond the scope of
                        this study.
                     </div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">To account for multiple people signing in one frame, an extra module was
                        added. This module creates bounding boxes around each person recognized by
                        OpenPose, normalizes the positions of the body joints and runs the classifier.
                        This process makes it possible to classify sign occurrences for multiple people
                        irrespective of their positions in a frame (<a href="figure04" onclick="window.open('figure04'); return false" class="ref">Figure 4</a>).
                     </div>
                     
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Once all the frames have been classified, the "cleaning up" and annotation
                        phase starts. A sign occurrence is annotated only if at least 12 consecutive
                        frames have been classified as "signing". That way we account for the false
                        positive errors. This sets the stage for the annotation step. Using the PyMpi
                        python library [<a class="ref" href="#lubbers2013">Lubbers and Torreira 2013</a>] the classifications are translated
                        into annotations that can be imported directly to ELAN, a standard audio and
                        video annotation tool [<a class="ref" href="#sloetjes2008">Sloetjes and Wittenburg 2008</a>]. <a href="figure03" onclick="window.open('figure03'); return false" class="ref">Figure 3</a> shows the
                        result of the overall outcome.
                     </div>
                     
                     
                     
                     
                     <div id="table01" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Classifier</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">AUC score</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Artificial Neural Network (ANN)</td>
                              
                              <td valign="top" class="cell">0.88</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Random Forest (RF)</td>
                              
                              <td valign="top" class="cell">0.87</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Support Vector Machines (SVM)</td>
                              
                              <td valign="top" class="cell">0.78</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Extreme Gradient Boosting (XGBoost)</td>
                              
                              <td valign="top" class="cell">0.92</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>AUC scores of all the classifiers tested for manual activation prediction
                        </div>
                     </div>
                     
                     
                     <div id="table02" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><span class="hi bold">Weight</span></td>
                              
                              <td valign="top" class="cell"><span class="hi bold">Feature</span></td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.1410</td>
                              
                              <td valign="top" class="cell">Right wrist y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.1281</td>
                              
                              <td valign="top" class="cell">Right wrist x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0928</td>
                              
                              <td valign="top" class="cell">Left wrist y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0917</td>
                              
                              <td valign="top" class="cell">Left wrist x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0717</td>
                              
                              <td valign="top" class="cell">Nose x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0658</td>
                              
                              <td valign="top" class="cell">Left shoulder x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0623</td>
                              
                              <td valign="top" class="cell">Left elbow y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0588</td>
                              
                              <td valign="top" class="cell">Right elbow y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0552</td>
                              
                              <td valign="top" class="cell">Nose y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0517</td>
                              
                              <td valign="top" class="cell">Left shoulder y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Left elbow x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Right elbow x</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0482</td>
                              
                              <td valign="top" class="cell">Right shoulder y</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">0.0364</td>
                              
                              <td valign="top" class="cell">Right shoulder x</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Importance of each feature during manual activation as predicted by the Extreme Gradient
                           Boosting classifier
                        </div>
                     </div>
                     
                     
                     <div id="figure03" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.jpg" rel="external"><img src="resources/images/figure03.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Final output of the manual activation tool as seen in ELAN. Signing sequences have
                           been given a 'signing' gloss for readability. This attribute can be easily changed
                           to produce empty glosses.
                        </div>
                     </div>
                     
                     
                     <div id="figure04" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.jpg" rel="external"><img src="resources/images/figure04.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Bounding boxes are calculated in order to normalize the body joint coordinates for
                           each signer. After this process the normalized coordinates are passed to the XGBoost
                           classifier (here tested on a YouTube video: <a href="https://www.youtube.com/watch?v=NR" onclick="window.open('https://www.youtube.com/watch?v=NR'); return false" class="ref">https://www.youtube.com/watch?v=NR</a>).
                        </div>
                     </div>
                     
                     
                  </div>
                  
                  
                  <div class="div div1">
                     <h2 class="head">4.2 Tool 2: Number of Hands</h2>
                     
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">The second tool is responsible for not only recognizing whether a person
                        in a video is signing but also if the sign is one or two-handed. We have
                        previously hypothesized that this is a more complex task than the previous
                        binary classification. Results on the accuracy of all the classifiers suggest
                        that it is not as intricate as initially thought of; the higher the sliding
                        window interval, the lower the accuracy of the model. As seen in <a href="figure05" onclick="window.open('figure05'); return false" class="ref">Figure 5</a> of
                        all classifiers tested, Random forest had the highest accuracy at the sliding
                        window interval of 1 frame at a time. Similarly to the previous experiment, a
                        frame-to-frame prediction can produce the highest results.
                     </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Furthermore, the results regarding the Long-Short-Term-Memory networks
                        (<a href="figure06" onclick="window.open('figure06'); return false" class="ref">Figure 6</a>) suggest that the highest accuracy can be achieved at a sliding
                        window interval of 56 frames and at a hidden layer size of 8 units. However,
                        such a high window interval contains more than one sign, as the average length
                        of a sign is approximately 14 frames. This discrepancy can be caused due to the
                        architectural properties of the LSTM network. The average length of the signs
                        is too small for the network to converge. The LSTM units needed more timesteps
                        in order to prevent overfitting to the data. This property in addition to the
                        small dataset used to train the network caused this anomaly.
                     </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">Although the tool performs well on predicting whether a sign is one-
                        or two-handed (using a Random Forest classifier) there are cases were the
                        output is not as expected. In particular, cases where there is a two-handed
                        symmetrical sign produced, the tool fails to accurately predict the correct
                        class. It is likely that such signs were under-presented in our data set, thus
                        resulting in poor classification.
                     </div>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41"></div>
                     
                     
                     <div class="table">
                        <table class="table"></table>
                        <div class="caption-no-label">
                           <div class="label">Table 3. </div>
                        </div>
                     </div>
                     
                     
                     <div id="figure05" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 5. </div>Accuracy of different classifiers in various sliding window intervals in order to
                           predict whether a sequence contains a one- or a two- handed sign or not signing at
                           all. At a sliding window interval of 1 (a), Random Forest shows the highest accuracy.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42"></div>
                     
                     
                     <div id="figure06" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.jpg" rel="external"><img src="resources/images/figure06.jpg" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 6. </div>Accuracy of Long-Short-Term-Memory networks on the test set with different sliding
                           window intervals (x) and hidden layer sizes (8,16,32,64,256).
                        </div>
                     </div>
                     
                     
                     
                  </div>
                  
                  
                  <div class="div div1">
                     <h2 class="head">4.3 Tool 3: Handshape</h2>
                     
                     
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">In order to understand the distribution of the different handshapes
                        presented in a video, Principal Component Analysis (PCA) was utilized on all
                        the normalized finger joint coordinates for all the frames at once (<a href="figure07" onclick="window.open('figure07'); return false" class="ref">Figure 7a</a>).
                        This process allows us to reduce the dimensionality of the data while retaining
                        as much as possible of the variance in the dataset. Each multidimensional array
                        of the extracted finger joints positions, for each frame, has been reduced to
                        a single x,y coordinate. The result already suggests that there are regions
                        dense enough to be considered different clusters. The utilized elbow method
                        suggested that at k=5 the highest classification could be achieved (<a href="figure07" onclick="window.open('figure07'); return false" class="ref">Figure 7b</a>).
                        On the video sample used in our study that number seemed to reflect the proper
                        amount of discerned handshapes. However, as OpenPose captures all the finger
                        configurations in each frame it is at the linguist’s discretion to decide
                        on when a handshape is significantly different from another. Additionally,
                        experiments to optimize the hyperparameters (eta, min samples and leaf
                        size) for the DBSCAN failed to create an accurate clustering (<a href="figure07" onclick="window.open('figure07'); return false" class="ref">Figure 7c</a>).
                        Subsequently, the module creates annotation slots for the different handshapes
                        in the video and adds an overlay containing the number of the predicted cluster
                        on each frame.
                     </div>
                     
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">However, special consideration must be given to the overall handshape
                        recognition module. Although the hand normalization process prepares the finger
                        joints adequately enough to be used in the clustering methods, it fails to
                        account for hands perpendicular to the camera’s point of view. Additionally,
                        handshapes that are similar to each other but are rotated towards or outwards
                        of the signer’s body will most probably clustered differently. Some of these
                        limitations can be solved by manually editing the cluster numbers prior to the
                        annotation process.
                     </div>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">In its current form, this method can already be used to either fully
                        annotate the handshapes in a video sample or be used in different samples
                        and treated as weakly annotated data in order to be used in other handshape
                        classifiers similarly to Koller’s et al. study [<a class="ref" href="#koller2016">Koller et al. 2016</a>].
                     </div>
                     
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46"></div>
                     
                     <div class="table">
                        <table class="table"></table>
                        <div class="caption-no-label">
                           <div class="label">Table 4. </div>
                        </div>
                     </div>
                     
                     
                     <div id="figure07" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 7. </div>Visualizations produced by the handshape recognition module. Principal component analysis
                           (a) can be used to reduce the dimensionality of the finger joints coordinates. Two
                           clustering methods, namely K-means (b) and DBSCAN (c), can be used to detect the different
                           handshapes presented in a video sample.
                        </div>
                     </div>
                     
                     
                     <div id="figure08" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 8. </div>Different handshapes recognized by the handshape recognition module using K-means.
                        </div>
                     </div>
                     
                     
                  </div>
               </div>
               
               <div class="div div0">
                  <h1 class="head">5. Discussion</h1>
                  
                  
                  <div class="counter"><a href="#p47">47</a></div>
                  <div class="ptext" id="p47">In this study we have presented three different tools that can be used
                     to assist the annotation process of sign language corpora. The first tool
                     proved to be robust on the task of classification of manual activation even
                     when the corpora are noisy, of poor quality and most importantly containing
                     more than one signer. This eliminates the preprocessing stage that many sign
                     language corpora have to endure where either dedicated cameras per signer are
                     utilized or manually cropping the original video. As a result, a more natural
                     filming process can be applied. One limitation regarding our methodology
                     is that at its current state is not possible to account for individual sign
                     temporal classification. Reaching such level would require to fuse additional
                     information into the training sets which in most cases might be language
                     specific. However, it is possible to get a per sign prediction when the "number
                     of hands involved" feature changes.
                  </div>
                  
                  <div class="counter"><a href="#p48">48</a></div>
                  <div class="ptext" id="p48">The most striking observation to emerge from our methodology is that there
                     is no necessity of having massive training sets for the classification of
                     low-level features (such as manual activation and number of hands involved).
                     In contrast to earlier studies using neural networks for sign language
                     recognition
                     [<a class="ref" href="#aitpayev2016">Aitpayev et al. 2016</a>]
                     <span class="error"><a class="ref" href="#pigou">#pigou</a></span>
                     [<a class="ref" href="#zhang2013">Zhang et al. 2013</a>],
                     we used a proportionally smaller dataset. Additionally, this is the first
                     time to our knowledge where corpora outside of studio conditions have been
                     used to train and most importantly test models and tools for sign language
                     automatic annotation. Furthermore, such findings can be applied in other
                     studies as well. It is a common misconception that only large data sets can
                     be used for analysis. Such a trend, although true for deep learning purposes,
                     can be daunting for digital humanities researchers without in depth data
                     science knowledge. In our study, we have shown that even with a small and noisy
                     dataset of visual materials, researchers can use machine learning algorithms
                     to effectively extract meaningful information. Our testing in West African
                     Sign Language corpora showed that such frameworks can work effectively in
                     different skin color participants lifting possible bias by previously developed
                     algorithms.
                  </div>
                  
                  <div class="counter"><a href="#p49">49</a></div>
                  <div class="ptext" id="p49">There are few limitations regarding our methodologies, particularly
                     with respect to the handshape distribution module. Low quality video and
                     consequently framerate seem to affect the robustness of OpenPose. As a result,
                     finger joint prediction can be noisy and of low confidence. Additionally,
                     we observed that finger joints could not be predicted when the elbow was
                     not visible in the frame, and thus, losing that information. In our study we
                     treated all predicted joints equally but it is necessary for future research
                     to include the prediction confidence interval as an additional variable.
                     Furthermore, on the current output from OpenPose it is difficult to extract
                     the palm orientation attribute meaning that differently rotated handshapes
                     might result in the same cluster. Future research will concentrate on fixing
                     that issue as well as creating an additional tool for the annotation of this
                     feature.
                  </div>
                  
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">In the sign language domain, researchers can use our tools to recognize
                     the times of interest and basic phonological features on newly compiled
                     corpora. Additionally, such extracted features can be further used to measure
                     variation on different sign languages or signers, for example, to measure the
                     distribution of one- and two-handed signs or particular handshapes. Moreover,
                     other machine or deep learning experiments can benefit from our tools by using
                     them to extract only the meaningful information from the corpora during the
                     data gathering process, thus reducing possible noise in the datasets. Our tools
                     can also be used towards automatic gloss suggestion. A future model can search
                     only the signing sequences predicted by our tool rather than "scanning" the
                     whole video corpus, and consequently making it more efficient.
                  </div>
                  
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">Outside the sign language domain, the results have further strengthened our
                     confidence that pre-trained frameworks can be used to help extract meaningful
                     information from audio-visual materials. In particular, OpenPose can be a
                     useful asset when human activity needs to be tracked and recognized in a video
                     without the need of special hardware setups. Its accurate tracking allows
                     researchers to use it in videos compiled outside studio conditions. As a
                     result, studies in the audio-visual domain can benefit from community-created
                     materials involving natural and unbiased communication. Using our tools,
                     these study areas can analyze and classify human activity beyond the sign
                     language discipline in large scale cultural archives or specific domains
                     such as gestural research, dance or theater and cinema related studies, to
                     name but a few. For example, video analyses in gestural and media studies can
                     benefit from such an automatic approach to find relevant information regarding
                     user-generated data on social media and other popular platforms.
                  </div>
                  
                  <div class="counter"><a href="#p52">52</a></div>
                  <div class="ptext" id="p52">Finally, due to the cumbersome installation process of OpenPose for
                     the majority of SL linguists, we have decided to implement part of the
                     tools in an online collaborative environment on a cloud service provided
                     by Google (i.e. Google Colab). In this environment a temporary instance
                     of OpenPose can be installed along with our developed python modules. In
                     a simple step-based manner, the researcher can upload the relevant videos
                     and download the automatically generated annotation files. Find the link
                     to this Colab in the footnote below
                     <a class="noteRef" href="#d4e886">[4]</a>.
                     Additionally, we have
                     a created another environment for re-training purposes. By doing so, the
                     researcher can re-train the models on his or her particular data and ensure the
                     aforementioned accuracy on
                     them<a class="noteRef" href="#d4e890">[5]</a>.
                  </div>
                  
                  
               </div>
               
               <div class="div div0">
                  <h1 class="head">Conclusion</h1>
                  
                  <div class="counter"><a href="#p53">53</a></div>
                  <div class="ptext" id="p53">To summarise, glossing sign language corpora is a cumbersome and
                     time-consuming task. Current approaches to automatize parts of this process
                     need special video recording devices (such as Microsoft Kinect), large amount
                     of data in order to train deep learning architectures to recognize a set of
                     signs and can be prone to skin-color bias. In this study we explored the use of
                     a pre-trained pose estimation framework created by Cao et al. [<a class="ref" href="#cao2017">Cao et al. 2017</a>] in order
                     to create three tools and methods to predict sign occurrences, number of hands
                     involved, and handshapes. The results show that four minutes of annotated data
                     are adequate enough to train a classifier (namely XGBoost) to predict whether
                     one or more persons are signing or not as well as the number of hands used
                     (using Random Forest). Additionally, we examined the use of K-means and DBSCAN
                     as clustering methods to detect the different handshapes presented in the
                     video. Because of the low complexity of the finger joint data extracted from
                     the pose estimation library, K-means was found to produce accurate results.
                  </div>
                  
                  <div class="counter"><a href="#p54">54</a></div>
                  <div class="ptext" id="p54">The significance of this study lies in the fact that the tools created
                     do not rely on specialized cameras nor require large amount of information
                     to be trained. Additionally, they can be easily used by researchers without
                     developing skills and adjusted to work in any kind of sign language corpus
                     irrespective of its quality or the number of people in the video. Finally, they
                     have the potential to be extended and used in other audio-visual material that
                     involve human activity such as gestural corpora.
                  </div>
                  
               </div>
               
               
               <div class="div div0">
                  <h1 class="head">Appendix A</h1>
                  
                  <div class="counter"><a href="#p55">55</a></div>
                  <div class="ptext" id="p55">
                     The input shape of the LSTM network trained to recognize the "number of hands”
                     (Tool 2) feature is a three dimensional array that can be defined as:
                     [samples × timesteps × features] where features is a 2 dimensional array of
                     [21 × 2] containing the pixel x,y coordinates of the finger joints and timesteps
                     are the sliding window interval. Two Dense layers of 7 and 1 unit respectively
                     follow the previous Bidirectional LSTM layer. The activation function used is
                     ’ReLU’ and the dropout rate at 0.4. The architecture that produced the highest
                     results for this network can be seen in <a href="figure09" onclick="window.open('figure09'); return false" class="ref">Figure 9</a>.
                     
                  </div>
                  
                  <div id="figure09" class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 9. </div>Architecture of the Long-Short-Term-Memory network trained for Tool 2.
                     </div>
                  </div>
                  
               </div>
               
               
               
               
               
               
               
            </div>
            
            
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e377"><span class="noteRef lang en">[1] <a href="https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s" onclick="window.open('https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s'); return false" class="ref">https://www.youtube.com/watch?v=NRe-AxZI8Hs&amp;t=1s</a></span></div>
               <div class="endnote" id="d4e410"><span class="noteRef lang en">[2] Linear Regression (LR), Decision Trees (CART), Support Vector Machines
                     (SVM), Random Forest (RF) and Gradient Boosting (GBM).</span></div>
               <div class="endnote" id="d4e474"><span class="noteRef lang en">[3] eta: 0.23, gamma: 3, lambda: 2, max.
                     delta step: 4, max. depth: 37 and min. child weight: 4</span></div>
               <div class="endnote" id="d4e886"><span class="noteRef lang en">[4] <a href="https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA" onclick="window.open('https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA'); return false" class="ref">https://colab.research.google.com/drive/1HwXo2Tk4uHizGTpRg-simMDMD4wPOzmA</a></span></div>
               <div class="endnote" id="d4e890"><span class="noteRef lang en">[5] <a href="https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y" onclick="window.open('https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y'); return false" class="ref">https://colab.research.google.com/drive/10H2uxY7p59GrrDC5z85RugYB6PfQZs7Y</a></span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="agha2018">
                     <!-- close -->Agha et al. 2018</span> Agha, R. A. A. R., Sefer, M. N. and Fattah, P. (2018). <cite class="title quote">A Comprehensive Study on Sign Languages Recognition Systems Using (SVM, KNN, CNN and
                     ANN)</cite>. <cite class="title italic">First International Conference on Data Science, E-Learning and Information Systems</cite>. (DATA ’18). Madrid, Spain: ACM, pp. 28:1–28:6.
               </div>
               <div class="bibl"><span class="ref" id="aitpayev2016">
                     <!-- close -->Aitpayev et al. 2016</span> Aitpayev, K., Islam, S. and Imashev, A. (2016). <cite class="title quote">Semi-automatic annotation tool for sign languages</cite>. <cite class="title italic">2016 IEEE 10th International Conference on Application of Information and Communication
                     Technologies (AICT)</cite>. Baku, Azerbaijan: IEEE, pp. 1–4.
               </div>
               <div class="bibl"><span class="ref" id="andriluka2014">
                     <!-- close -->Andriluka et al. 2014</span> Andriluka, M., Pishchulin, L., Gehler, P. and Schiele, B. (2014). <cite class="title quote">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</cite>. <cite class="title italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</cite>.
               </div>
               <div class="bibl"><span class="ref" id="buehler2009">
                     <!-- close -->Buehler et al. 2009</span> Buehler, P., Zisserman, A. and Everingham, M. (2009). <cite class="title quote">Learning sign language by watching TV (using weakly aligned subtitles)</cite>. <cite class="title italic">2009 IEEE Conference on Computer Vision and Pattern Recognition</cite>. pp. 2961–68.
               </div>
               <div class="bibl"><span class="ref" id="cao2017">
                     <!-- close -->Cao et al. 2017</span> Cao, Z., Simon, T., Wei, S.-E. and Sheikh, Y. (2017). <cite class="title quote">Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields</cite>. <cite class="title italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</cite>. Honolulu, HI: IEEE, pp. 1302–10.
               </div>
               <div class="bibl"><span class="ref" id="charles2014">
                     <!-- close -->Charles et al. 2014</span> Charles, J., Pfister, T., Everingham, M. and Zisserman, A. (2014). <cite class="title quote">Automatic and Efficient Human Pose Estimation for Sign Language Videos</cite>. <cite class="title italic">International Journal of Computer Vision</cite>, 110(1): 70–90.
               </div>
               <div class="bibl"><span class="ref" id="chollet2015">
                     <!-- close -->Chollet 2015</span> Chollet, F. and others (2015). <cite class="title italic">Keras</cite>. <a href="https://keras.io" onclick="window.open('https://keras.io'); return false" class="ref">https://keras.io</a>.
               </div>
               <div class="bibl"><span class="ref" id="cooper2007">
                     <!-- close -->Cooper and Bowden 2007</span> Cooper, H. and Bowden, R. (2007). <cite class="title quote">Large Lexicon Detection of Sign Language</cite>. In Lew, M., Sebe, N., Huang, T. S. and Bakker, E. M. (eds), <cite class="title italic">Human–Computer Interaction</cite>. (Lecture Notes in Computer Science). Springer Berlin Heidelberg, pp. 88–97.
               </div>
               <div class="bibl"><span class="ref" id="cooper2011">
                     <!-- close -->Cooper et al. 2011</span> Cooper, H., Holt, B. and Bowden, R. (2011). <cite class="title quote">Sign Language Recognition</cite>. In Moeslund, T. B., Hilton, A., Krüger, V. and Sigal, L. (eds), <cite class="title italic">Visual Analysis of Humans: Looking at People</cite>. London: Springer London, pp. 539–62.
               </div>
               <div class="bibl"><span class="ref" id="dreuw2008">
                     <!-- close -->Dreuw and Ney 2008</span> Dreuw, P. and Ney, H. (2008). <cite class="title quote">Towards automatic sign language annotation for the ELAN tool</cite>. <cite class="title italic">LREC Workshop: Representation and Processing of Sign Languages</cite>. Marrakech, Morocco, pp. 50–53.
               </div>
               <div class="bibl"><span class="ref" id="eichner2009">
                     <!-- close -->Eichner and Ferrari 2009</span> Eichner, M. and Ferrari, V. (2009). <cite class="title quote">Better appearance models for pictorial structures</cite>. <cite class="title italic">British Machine Vision Conference 2009</cite>. London, UK: British Machine Vision Association, pp. 3.1-3.11.
               </div>
               <div class="bibl"><span class="ref" id="eichner2012">
                     <!-- close -->Eichner et al. 2012</span> Eichner, M., Marin-Jimenez, M., Zisserman, A. and Ferrari, V. (2012). <cite class="title quote">2D Articulated Human Pose Estimation and Retrieval in (Almost) Unconstrained Still
                     Images</cite>. <cite class="title italic">International Journal of Computer Vision</cite>, 99(2): 190–214.
               </div>
               <div class="bibl"><span class="ref" id="fang2004">
                     <!-- close -->Fang et al. 2004</span> Fang, G., Gao, W. and Zhao, D. (2004). <cite class="title quote">Large Vocabulary Sign Language Recognition Based on Fuzzy Decision Trees</cite>. <cite class="title italic">Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions On</cite>, 34: 305–14 doi:10.1109/TSMCA.2004.824852.
               </div>
               <div class="bibl"><span class="ref" id="farhadi2007">
                     <!-- close -->Farhadi et al. 2007</span> Farhadi, A., Forsyth, D. and White, R. (2007). <cite class="title quote">Transfer Learning in Sign language</cite>. <cite class="title italic">2007 IEEE Conference on Computer Vision and Pattern Recognition</cite>. Minneapolis, MN, USA, pp. 1–8.
               </div>
               <div class="bibl"><span class="ref" id="felzenszwalb2005">
                     <!-- close -->Felzenszwalb and Huttenlocher 2005</span> Felzenszwalb, P. F. and Huttenlocher, D. P. (2005). <cite class="title quote">Pictorial Structures for Object Recognition</cite>. <cite class="title italic">International Journal of Computer Vision</cite>, 61(1): 55–79.
               </div>
               <div class="bibl"><span class="ref" id="forster2012">
                     <!-- close -->Forster 2012</span> Forster, J., Schmidt, C., Hoyoux, T., Koller, O., Zelle, U., Piater, J. and Ney, H.
                  (2012). <cite class="title quote">RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation
                     Corpus</cite>. Istanbul, Turkey, pp. 3785–3789.
               </div>
               <div class="bibl"><span class="ref" id="johnson2009">
                     <!-- close -->Johnson and Everingham 2009</span> Johnson, S. and Everingham, M. (2009). <cite class="title quote">Combining discriminative appearance and segmentation cues for articulated human pose
                     estimation</cite>. <cite class="title italic">2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</cite>. pp. 405–12.
               </div>
               <div class="bibl"><span class="ref" id="johnston2010">
                     <!-- close -->Johnston 2010</span> Johnston, T. (2010). <cite class="title quote">From archive to corpus: Transcription and annotation in the creation of signed language
                     corpora</cite>. <cite class="title italic">International Journal of Corpus Linguistics</cite>, 15(1): 106–31 doi:10.1075/ijcl.15.1.05joh. <a href="http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh" onclick="window.open('http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh'); return false" class="ref">http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.1.05joh</a> (accessed 19 May 2020).
               </div>
               <div class="bibl"><span class="ref" id="kelly2009">
                     <!-- close -->Kelly et al. 2009</span> Kelly, D., Reilly Delannoy, J., Mc Donald, J. and Markham, C. (2009). <cite class="title quote">A framework for continuous multimodal sign language recognition</cite>. <cite class="title italic">Proceedings of the 2009 International Conference on Multimodal Interfaces</cite>. pp. 351–358.
               </div>
               <div class="bibl"><span class="ref" id="koller2016">
                     <!-- close -->Koller et al. 2016</span> Koller, O., Ney, H. and Bowden, R. (2016). <cite class="title quote">Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data is Continuous
                     and Weakly Labelled</cite>. <cite class="title italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</cite>. Las Vegas, NV, USA: IEEE, pp. 3793–802.
               </div>
               <div class="bibl"><span class="ref" id="kumar2017">
                     <!-- close -->Kumer 2017</span> Kumar, N. (2017). <cite class="title quote">Motion trajectory based human face and hands tracking for sign language recognition</cite>. <cite class="title italic">2017 4th IEEE Uttar Pradesh Section International Conference on Electrical, Computer
                     and Electronics (UPCON)</cite>. Mathura: IEEE, pp. 211–16.
               </div>
               <div class="bibl"><span class="ref" id="lin2014">
                     <!-- close -->Lin 2014</span> Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona,
                  P., Ramanan, D., Zitnick, C. L. and Dollár, P. (2014). <cite class="title italic">Microsoft COCO: Common Objects in Context</cite>.
               </div>
               <div class="bibl"><span class="ref" id="lubbers2013">
                     <!-- close -->Lubbers and Torreira 2013</span> Lubbers, M. and Torreira, F. (2013). <cite class="title italic">A Python Module for Processing ELAN and Praat Annotation Files: Dopefishh/Pympi</cite>. Python <a href="https://github.com/dopefishh/pympi" onclick="window.open('https://github.com/dopefishh/pympi'); return false" class="ref">https://github.com/dopefishh/pympi</a>.
               </div>
               <div class="bibl"><span class="ref" id="moeslund2006">
                     <!-- close -->Moselund et al. 2006</span> Moeslund, T. B., Hilton, A. and Krüger, V. (2006). <cite class="title quote">A survey of advances in vision-based human motion capture and analysis</cite>. <cite class="title italic">Computer Vision and Image Understanding</cite>, 104(2): 90–126.
               </div>
               <div class="bibl"><span class="ref" id="nyst2012a">
                     <!-- close -->Nyst 2012</span> Nyst, V. A. S. (2012). <cite class="title quote">A Reference Corpus of Adamorobe Sign Language</cite>. A digital, annotated video corpus of the sign language used in the village of Adamorobe,
                  Ghana.
               </div>
               <div class="bibl"><span class="ref" id="nyst2012b">
                     <!-- close -->Nyst et al. 2012</span> Nyst, V. A. S., Magassouba, M. M. and Sylla, K. (2012). <cite class="title quote">Un Corpus de reference de la Langue des Signes Malienne II</cite>. A digital, annotated video corpus of local sign language use in the Dogon area of
                  Mali.
               </div>
               <div class="bibl"><span class="ref" id="pedregosa2011">
                     <!-- close -->Pedregosa et al. 2011</span> Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
                  M., et al. (2011). <cite class="title quote">Scikit-learn: Machine Learning in Python</cite>. <cite class="title italic">Journal of Machine Learning Research</cite>, 12: 2825–2830.
               </div>
               <div class="bibl"><span class="ref" id="pigou2015">
                     <!-- close -->Pigou et al. 2015</span> Pigou, L., Dieleman, S., Kindermans, P.-J. and Schrauwen, B. (2015). <cite class="title quote">Sign Language Recognition Using Convolutional Neural Networks</cite>. In Agapito, L., Bronstein, M. M. and Rother, C. (eds), <cite class="title italic">Computer Vision - ECCV 2014 Workshops</cite>. (Lecture Notes in Computer Science). Cham: Springer International Publishing, pp.
                  572–78.
               </div>
               <div class="bibl"><span class="ref" id="ramanan2007">
                     <!-- close -->Ramanan et al. 2007</span> Ramanan, D., Forsyth, D. A. and Zisserman, A. (2007). <cite class="title quote">Tracking People by Learning Their Appearance</cite>. <cite class="title italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</cite>, 29(1): 65–81.
               </div>
               <div class="bibl"><span class="ref" id="roussos2012">
                     <!-- close -->Roussos et al. 2012</span> Roussos, A., Theodorakis, S., Pitsikalis, V. and Maragos, P. (2012). <cite class="title quote">Hand Tracking and Affine Shape-Appearance Handshape Sub-units in Continuous Sign Language
                     Recognition</cite>. In Kutulakos, K. N. (ed), <cite class="title italic">Trends and Topics in Computer Vision</cite>, vol. 6553. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 258–72.
               </div>
               <div class="bibl"><span class="ref" id="sapp2010">
                     <!-- close -->Sapp et al. 2010</span> Sapp, B., Jordan, C. and Taskar, B. (2010). Adaptive pose priors for pictorial structures.
                  <cite class="title italic">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</cite>. pp. 422–29.
               </div>
               <div class="bibl"><span class="ref" id="sivic2006">
                     <!-- close -->Sivic et al. 2006</span> Sivic, J., Zitnick, C. L. and Szeliski, R. (2006). <cite class="title quote">Finding people in repeated shots of the same scene</cite>. <cite class="title italic">British Machine Vision Conference 2006</cite>, vol. 3. Edinburgh: British Machine Vision Association, pp. 909–18.
               </div>
               <div class="bibl"><span class="ref" id="sloetjes2008">
                     <!-- close -->Sloetjes and Wittenburg 2008</span> Sloetjes, H. and Wittenburg, P. (2008). <cite class="title quote">Annotation by category - ELAN and ISO DCR. Marrakech, Morocco</cite>, p. 5 <a href="https://tla.mpi.nl/tools/tla-tools/elan/" onclick="window.open('https://tla.mpi.nl/tools/tla-tools/elan/'); return false" class="ref">https://tla.mpi.nl/tools/tla-tools/elan/</a>.
               </div>
               <div class="bibl"><span class="ref" id="starner1998">
                     <!-- close -->Starner et al. 1998</span> Starner, T., Weaver, J. and Pentland, A. (1998). <cite class="title quote">Real-time American sign language recognition using desk and wearable computer based
                     video</cite>. <cite class="title italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</cite>, 20(12): 1371–75.
               </div>
               <div class="bibl"><span class="ref" id="yang2011">
                     <!-- close -->Yang and Ramanan 2011</span> Yang, Y. and Ramanan, D. (2011). <cite class="title quote">Articulated pose estimation with flexible mixtures-of-parts</cite>. <cite class="title italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</cite>. pp. 1385–92.
               </div>
               <div class="bibl"><span class="ref" id="zhang2013">
                     <!-- close -->Zhang et al. 2013</span> Zhang, C., Yang, X. and Tian, Y. (2013). <cite class="title quote">Histogram of 3D Facets: A characteristic descriptor for hand gesture recognition.</cite> <cite class="title italic">2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture
                     Recognition (FG)</cite>. pp. 1–8.
               </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>