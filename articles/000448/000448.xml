<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en">Erasure, Misrepresentation and Confusion:
                    Investigating JSTOR Topics on Women’s and Race Histories</title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Sharon <dhq:family>Block</dhq:family></dhq:author_name>
                    <dhq:affiliation>UC-Irvine</dhq:affiliation>
                    <email>sblock@uci.edu</email>
                    <dhq:bio>
                        <p>Sharon Block is Professor of History at the University of California,
                            Irvine. She is the author of <title rend="italic">Rape and Sexual Power
                                in Early America</title> (2006), <title rend="italic">Colonial
                                Complexions: Race and Bodies in Eighteenth-Century America</title>
                            (2018), and some of the earliest articles aplying topic modeling in the
                            humanities, including <title rend="quotes">Doing More with Digitization:
                                An Introduction to Topic Modeling of Early American Sources</title>,
                                <title rend="italic">Common-place: the Interactive Journal of Early
                                American Life</title> (2006) and with D. Newman, <title
                                rend="quotes">What, Where, When and Sometimes Why: Data Mining
                                Women’s History Abstracts, 1985-2005</title>, <title rend="italic"
                                >Journal of Women’s History</title> (2011). </p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>

                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id">000448</idno>
                <idno type="volume">014</idno>
                <idno type="issue">1</idno>
                <date/>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change when="2020-01-22" who="murelj">Created file and began encoding</change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>This article investigates the topic labeling system of a widely used full-text
                    academic publication database, JSTOR, particularly in reference to colonial
                    North American history scholarship. Using insights developed by critical
                    algorithm and critical archival studies, it analyzes how JSTOR’s topics
                    repeatedly misrepresent and erase work in women’s, African diasporic/African
                    American, and Native American and settler colonial histories. The article
                    discusses concerns over the power of metadata, the need for transparent and
                    domain-expert-involved indexing processes, and digital providers’
                    responsibilities to accurately categorize scholars’ work. It particularly
                    focuses on the potentially disproportionate harm done to traditionally
                    marginalized fields of study through seemingly racist or sexist topical labeling
                    that impedes knowledge discovery.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>Examines the representation of women and minority groups in JSTOR's topical metadata</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>Introduction</head>
                <p>For at least two decades, scholars have written on the degree to which we live in
                    an <seg>algorithmic culture</seg>, a <seg>computational theocracy</seg>, or been
                    beholden to the power of computer algorithms [<ref target="#granieri2014"
                        >Granieri 2014</ref>] [<ref target="#bogost2015">Bogost 2015</ref>] [<ref
                        target="#introna2000">Introna and Nissenbaum 2000</ref>]. In recent years,
                    scholars have published book-length critiques of the sexism and racism behind
                    increasingly omnipresent and opaque search systems and mobile apps [<ref
                        target="#eubanks2018">Eubanks 2018</ref>] [<ref target="#noble2018">Noble
                        2018</ref>] [<ref target="#oneil2016">O’Neil 2016</ref>] [<ref
                        target="#wachter-boettcher2017">Wachter-Boettcher 2017</ref>]. I build on
                    these scholars’ groundbreaking works on the hazards of algorithmic bias by
                    analyzing one academic database’s topical indexing functions. Beyond a critique
                    of inaccuracies and omissions, I detail how such subject miscategorizations
                    reinforce sexist and racist belief systems, thus having a disproportionate
                    effect on marginalized groups and research.</p>
                <p>In April 2018, I attempted to use the topic indexing system in the academic
                    database, JSTOR, to prepare a state of the field presentation on early American
                    women’s history. I quickly realized that in several areas of my expertise
                    (colonial North America, women’s, race, Indigenous and African American
                    histories), JSTOR’s topic categorizations displayed concerning shortcomings.
                    Articles that were focused entirely on women’s history did not seem to be
                    categorized by the topic of <q>women</q>. Instead, some were mischaracterized
                    with <q>men</q> as the most relevant topic. JSTOR’s topics for African, African
                    American, Native American and race histories showed misapplications and erasures
                    as well, fundamentally distorting the content of scholarship in these
                    fields.</p>
                <p>Most scholars are at least passingly familiar with the Library of Congress
                    Classification System and the controlled vocabulary (a set list of terms used
                    for indexing and information retrieval) of the Library of Congress Subject
                    Headings [<ref target="#loc2019a">Library of Congress Classification</ref>]
                        [<ref target="#loc2019b">Library of Congress Subject and Genre/Form
                        Headings</ref>]. These systems have structured knowledge for well over a
                    century. Such categorization schemas have always been less than objective, as
                    the 2016 struggle between the Library of Congress and House of Representatives
                    over the subject heading <q>Illegal aliens</q> made abundantly clear [<ref
                        target="#peet2016">Peet 2016</ref>]. Scholars have pointed to anti-LGBT
                    bias, Eurocentric and anti-Afrocentric biases, outdated terminology, and the
                    limitations of discipline-based, hierarchical structure within the Library of
                    Congress classification systems for many decades [<ref target="#bethel1994"
                        >Bethel 1994</ref>] [<ref target="#christensen2008">Christensen 2008</ref>]
                        [<ref target="#diao2016">Diao and Cao 2016</ref>] [<ref
                        target="#drabinski2009">Drabinski 2009</ref>] [<ref target="#howard2018"
                        >Howard and Knowlton 2018</ref>]. Thus, concern over bias in indexing and
                    classification schemas is not a recent phenomenon.</p>
                <p>However, the rise of digital databases and accompanying machine learning
                    technologies has brought new concerns. Critical algorithm studies have
                    developed in response to the influence of unknowable algorithms on consequential
                    decisions and actions. At their most basic, algorithms are a list of programmed
                    instructions, and can be millions of lines of proprietary coding, never seen or
                    understood by end users. Scholars have called attention to the difficulty in
                        <quote rend="inline">deconstructing the black boxes of Big Data</quote> that
                    create our algorithmic culture <ptr target="#pasquale2015" loc="6"/>.
                    Proprietary algorithms that produce racist results, in particular, have been a
                    repeated concern among data scientists and social justice advocates. [<ref
                        target="#buolamwini2016">Buolamwini 2016</ref>] [<ref target="#paul2016"
                            >Paul 2016</ref>] [<ref target="#schwartzapfel2019">Schwartzapfel
                        2019</ref>] [<ref target="#ulloa2019">Ulloa 2019</ref>]. One of the most
                    influential media and library information scholars, Safiya Noble, has
                    convincingly argued that we must interrogate the <quote rend="inline"
                        >implications of the artificial intelligentsia for people who are already
                        systematically marginalized and oppressed</quote>
                    <ptr target="#noble2018" loc="3"/>. Noble’s work documenting and challenging the
                    racism reproduced by search engines (primarily Google) has been particularly
                    impactful both within and outside academic discourse.</p>
                <p>Digital Humanities scholars have argued that seemingly objective search functions
                    are anything but, and have offered varied approaches to analyzing the systems we
                    so readily adopt. Many have argued for more broad definitions of algorithms to
                    account for the entire socio-cultural process that produces them [<ref
                        target="#kitchin2017">Kitchin 2017</ref>]. Accordingly, Jamie <q>Skye</q>
                    Bianco warns that <quote rend="inline">tools don’t reflect upon their own
                        making, use or circulation or upon the constraints under which their
                        constitution becomes legible</quote>
                    <ptr target="#bianco2012" loc="99"/>. Feminist and anti-racism scholars have
                    convincingly shown how algorithmic shortcomings in search, database
                    construction, and knowledge organization can be particularly detrimental to
                    non-mainstream fields. Tara McPherson directly ties the marginalization of
                    race-related studies to <quote rend="inline">the very designs of our
                        technological systems</quote> and <quote rend="inline">post-World War II
                        computational culture</quote> that continues to <quote rend="inline"
                        >partition off consideration of race in our work in the digital
                        humanities</quote>
                    <ptr target="#mcpherson2012" loc="140"/>. Moya Z. Bailey more broadly asks about
                    the effect on diverse scholars: <quote rend="inline">How do those outside the
                        categories white and male navigate this burgeoning disciplinary
                        terrain?</quote>, and Roopika Risam questions the degree to which digital
                    humanities processes as a whole (re)produce centers and peripheries <ptr
                        target="#bailey2011"/> [<ptr target="#risam2015"/>. Such intersectional
                    approaches recognize the structural, ideological and political forces that
                    contribute to the creation and promulgation of digital library technologies.</p>
                <p>Scholars have questioned the role of the database, specifically, as its own
                    configured media object and unit of inquiry, rather than just a neutral tool
                        [<ref target="#manovich1999">Manovich 1999</ref>] [<ref target="#vesna2007"
                        >Vesna 2007</ref>]. Librarians, especially those with technology expertise
                    or digital interests, have likewise begun investigating bias in academic
                    discovery systems and scholarly databases. With the creation of massive digital
                    corpuses of scholarship and archives, providers have looked for ways to provide
                    enriching metadata. (Often described as data about data, metadata in this
                    context is added information about a document or item, often used for
                    discovery.) Unfortunately, classifying contents is rife for the introduction of
                    biases. Jeffrey Daniels noted in 2015 that an Ex Libris discovery tool returned
                    sexist results: a search on stress in the workplace returned only a Wikipedia
                    article on <title rend="quotes">Women in the workforce</title>, implying that
                    women and stress were the same thing <ptr target="#reidsma2019" loc="3–4"/>.
                    Matthew Reidsma’s recent book, <title rend="italic">Masked By Trust: Bias in
                        Library Discovery</title>, points to the additional demands on library
                    discovery systems to support a variety of intellectual inquiries that may be
                    particularly complex, including <quote rend="inline">big, challenging, often
                        contentious topics</quote> without objectively correct answers, in contrast
                    to mundane generic searches, such as <title rend="quotes">nearest gas
                        station</title>
                    <ptr target="#reidsma2019" loc="68–71"/>. Riedsma’s earlier work on Proquest, an
                    academic digital document provider, found that problematic search results
                    related to likely already-marginalized or politicized topics, including <quote
                        rend="inline">women, the LGBT community, race, Islam, and mental
                        illness</quote>
                    <ptr target="#reidsma2016"/>.</p>
                <p>As one of a relatively small subset of women’s historians with long-term
                    engagement in both machine learning and feminist scholarship, I may be
                    relatively well placed to undertake a critique of JSTOR’s possible algorithmic
                    bias [<ref target="#jockers2013" loc="123–4">Jockers 2013, 123-124</ref>] [<ref
                        target="#block2011">Block and Newman 2011</ref>] [<ref target="#newman2006"
                        >Newman and Block 2006</ref>]. Still, I write from the perspective of an
                    academic teacher and researcher with substantial knowledge in digital
                    humanities, but without training in taxonomy or library and information science.
                    Accordingly, rather than a comprehensive analysis of JSTOR’s search and
                    topic-based systems, I offer a targeted and detailed review of the topics
                    applied to scholarship in my area of expertise so that I can base all
                    quantitative and text-based analyses on my in-depth understanding of each work’s
                    content, arguments, and foci. This exploration into JSTOR’s topical indexing
                    system for women’s and race histories points to the problematic ways that
                    technology can misconstrue and marginalize scholarship and suggests areas for
                    needed improvement.</p>
            </div>
            <div>
                <head>Finding the Algorithm(s)</head>
                <p>JSTOR began as an online <q>Journal Storage</q> database, and is today a broader
                    not-for-profit digital library built for academic research. JSTOR touts its
                    availability in 10,215 institutions and 176 countries where it provides access
                    to more than twelve million pieces of academic writing in seventy-five
                    disciplines. It is an indispensable resource to the academic community in U.S.
                    and women’s history.</p>
                <p>Various online guides to JSTOR describe their topic labeling algorithms in
                    general terms. The JSTOR Thesaurus, apparently launched sometime between 2013 and 2017,
                    provides the controlled vocabulary (standardized terminology) that makes up
                    topics <ptr target="#jstor2017"/> <ptr target="#jstor2019b"/>. In 2018, Jabin White, a vice president at ITHAKA, the digital technology
                    not-for-profit that produces JSTOR, described JSTOR’s the creation of the
                    Thesaurus as a way to address the need for descriptive and semantic metadata
                    that now provides <quote rend="inline">additional value</quote> for libraries
                    and users [<ref target="#white2018">White 2018</ref>]. The Thesaurus is
                    constructed of seventeen public and corporate-produced vocabularies, and is not
                    available to users online. [<ref target="#jstor2019b">JSTOR 2019b</ref>].
                    JSTOR reports that it relied on the software company, <ref
                        target="http://www.accessinn.com/">Access Innovations</ref>, to create the
                    Thesaurus. Access Innovations touts its four decades of taxonomy development
                    experience which allows it to create classifications for customers to <quote
                        rend="inline">fit both your content and the way your users interact with
                        that content</quote> by working <quote rend="inline">closely with your
                        subject matter experts</quote>.</p>
                <p>It is not transparent precisely how JSTOR Thesaurus terms become topics for
                    specific pieces of scholarship. A JSTOR support page explains some details of
                    topic indexing its database: <quote rend="inline">If a term is present at least
                        three times, it is recognized by the thesaurus and triggers the application
                        of a topic</quote> with <quote rend="inline">up to 10 topics assigned
                        to</quote> an article or chapter <ptr target="#jstor2019a"/>. It is
                    difficult to tell from available descriptions what precise system(s) JSTOR is
                    using to create its listed topics. A JSTOR taxonmer explained that it relies on
                        <quote rend="inline">both auto &amp; manual rule creation</quote>,
                    and specified that they use Wikipedia or DBPedia (which includes over five
                    million entities of structured content from Wikipedia) to provide information
                    for and descriptions of topics. <note>The JSTOR taxonomer who tweeted in
                        response to questions about Topics has apparently deleted her Twitter
                        account. Screenshots of quoted tweets, as well as of JSTOR search result
                        images not reproduced here are on file with the author.</note> These topics
                    do not seem to be produced by probabilistic topic modeling, even though JSTOR
                    Labs’ Text Analyzer uses LDA (Latent Dirichlet Allocation), a popular topic
                        model <ptr target="#snyder2012"/>.
                    Such statistically based models are part of the larger field of probabilistic
                    modeling and automatically learn a set of topics that comprehensively describe a
                    set of documents. In the past decade-plus, topic modeling has been increasingly
                    applied to humanities research questions and texts to find themes in such large
                    corpora without <foreign xml:lang="la">a priori</foreign> subject categories
                        [<ref target="#block2006">Block 2006</ref>] [<ref target="#meeks2012">Meeks
                        and Weingart 2012</ref>]. Topic modeling is particularly good at
                    disambiguation (separating different senses of words) and thematically linking
                    words with allied meanings (e.g.: car, automobile, BMW and Ford would all likely
                    be listed in the same topic). Even though information pages on JSTOR’s Thesaurus
                    explicitly <quote rend="inline">recognize</quote> the need to distinguish among
                    homonyms, in practice, the topic identification system seems to fall short on
                    effective word disambiguation. For example, it lists the topic of <q>Charity</q>
                    first for an article by Jessica Millward about a woman named <q>Charity
                        Folks</q> [<ref target="#millward2013">Millward 2013</ref>].</p>
                <p>The end user can only guess at the precise topic-defining and ranking process.
                    For instance, does JSTOR’s topic identification system rely on Part-of-Speech
                    Tagging (identifying whether each word is a noun, verb, etc) to ascertain each
                    word’s or phrase’s role in a given sentence? Or might it rely on Shallow Parsing
                    to <q>chunk</q> parts of sentences (nouns, verbs, etc) with less specificity? Do
                    JSTOR’s topic assignment algorithms identify specific text phrases such as Named
                    Entities (proper nouns, such as individual and organization names) and select
                    other multi-word expressions?</p>
                <p>I have attempted to describe JSTOR’s topic algorithms not because it is every
                    user’s responsibility to understand controlled vocabularies, rule bases, machine
                    indexing, knowledge bases, and the principles and practices of taxonomy
                    construction. But such human and coding details – where topic information comes
                    from, whether indexing is human or machine curated – can create bias in the
                    results. For example, Jessica Parr, a historian with an MS in Archives
                    management, was <quote rend="inline">surprised</quote> at JSTOR’s use of
                    Wikipedia/DBPedia: <quote rend="inline">People have been talking about
                        Wikipedia's considerable flaws with race and gender topics for several
                        years. And these are flaws that haven't been fixed. Using a tool like
                        DBPedia means your system is full of these racial and gendered
                        biases</quote> [<ref target="#lapowsky2015">Lapowsky 2015</ref>] [<ref
                        target="#zevallos2014">Zevallos 2014</ref>]. </p>
                <p>Even without becoming expert coders and taxonomers, scholars who use databases
                    can and should ask questions about the ethics of our research tools. We can
                    learn to probe the ethics of databases in the same ways that scholars have spent
                    the past decade productively interrogating the construction of archives [<ref
                        target="#falzetti2015">Falzetti 2015</ref>] [<ref target="#fuentes2016"
                        >Fuentes 2016</ref>] [<ref target="#stoler2010">Stoler 2010</ref>]. Whose
                    stories may be silenced or misrepresented in various classification systems? How
                    can users begin to understand the impact of knowledge systems on our
                    understanding of the scholarship done in particular subject areas? Analyzing the
                    choices of programmers, taxonomers, and other database system contributors makes
                    clear that academic discvery databases are the result of human decisions rather
                    than any imagined algorithmic neutrality. By focusing on the consequences of
                    JSTOR’s algorithmic indexing choices for scholarship in women’s African
                    American, Native American, and race histories, this essay offers a first step to
                    rethinking the topical indexing of an influential scholarly database.</p>
            </div>
            <div>
                <head>JSTOR’s Topical Indexing: Erasure and Misinterpretation</head>
                <p>Jennifer Morgan’s article, <title rend="quotes"><q>Some Could Suckle over Their
                        Shoulder</q>: Male Travelers, Female Bodies, and the Gendering of Racial
                        Ideology, 1500-1770</title>, is one of the <ref
                        target="https://blog.oieahc.wm.edu/getting-lost/">most downloaded</ref> in
                    the <title rend="italic">William and Mary Quarterly</title> [<ref
                        target="#morgan1997">Morgan 1997</ref>]. Personally, I have taught it well
                    over a dozen times since its first publication. Morgan’s article focuses on
                    European travelers’ constructions of African and Native American women’s bodies,
                    showing how European print descriptions and images of women’s appearance and
                    behavior helped to build the foundations of race-based slavery.</p>
                <figure xml:id="figure01">
                    <head>JSTOR record for Jennifer Morgan, “Some Could Suckle” with Topics.</head>
                    <graphic url="resources/images/figure01.png"/>
                </figure>
                <p>JSTOR lists nine relevant topics for Morgan’s article (<ref target="#figure01"
                        >Figure 1</ref>). <note>Unless otherwise specified, the topics assigned to
                        scholarship were taken from JSTOR in June-July 2018. Over time, its topic
                        model results have seemed to change, which I have mentioned when
                        relevant.</note> The first, and presumably most relevant, is <q>Men</q>.
                        <q>Women</q> does not even appear, although <q>Mothers</q> does. JSTOR
                    topics further categorize this scholarship as being about <q>Black people</q>
                    and <q>African Americans</q>. Both the present and absent topics for Morgan’s
                    article suggest intersectionally biased impacts for historians of slavery, race,
                    African and African American history, and women’s history. (On
                    intersectionality, the idea that interlocking systems of power, such as sexism
                    and racism, work together to multiply marginality, see the foundational work of
                        <ptr target="#crenshaw1989"/>). To begin with, the absence of <q>women</q>
                    and primacy of <q>men</q> is curious, given that JSTOR states that a topic’s
                        <quote rend="inline">relevance is determined by how frequently the term
                        appears in the piece of content</quote>
                    <ptr target="#jstor2019a"/>.</p>
                <p>Given these claims, I undertook an approximate word frequency count of Morgan’s
                    article, wondering if it were possible that the text mentions <q>men</q> far
                    more than <q>women</q>.<note>Frequency counts included basic text
                        normalization: removal of punctuation; lowercasing of all text; rudimentary
                        de-pluralization; stopword omission; and ignoring of words of fewer than 3
                        characters. Since all I needed were approximate frequency counts, I did not
                        clean up the text before processing. Thanks to David Newman for technical
                        assistance.</note> As Table I shows, the occurrence of words such as
                    women/woman/female far outnumber men/man/male. <q>Women</q> alone occurs more
                    than seven times as often as <q>men</q>. So it is concerning that JSTOR
                    determined that <q>men</q> is more relevant a topic than <q>women</q>. Moreover,
                    even the category <q>Black people</q> misrepresents the article’s focus. A
                    bigram count (occurrences of meaningful two-word compounds) shows that
                        <q>African women</q> is the most frequent two-word expression in the
                    article, appearing 32 times, and <q>black women</q> is the third most frequent.
                    Calculating bigram frequencies like these offers a methodologically productive
                    way to add specification that better translates broad terms into a topic. Such
                    bigrams show how Morgan’s article focuses on women, not generic people of
                    African descent. But this intersectional identity is erased by JSTOR’s topics.
                    Other researchers have found that Ex libris tools also tended to turn searches
                    related to Africa <quote rend="inline">into topics about
                        African-Americans</quote>. This may relate to their shared use of Wikipedia
                    for subject information and metadata <ptr target="#reidsma2019" loc="129"/>.
                    Replication of popular views on race rather than discipline-specific or
                    theoretically informed ones may lead to these kinds of biased results.</p>
                <table xml:id="table01">
                    <head>Select women- and men-related term approximate frequencies in Morgan, “Some
                        Could Suckle.”</head>
                    <row role="label">
                        <cell role="label">Women-related terms</cell>
                        <cell role="label">Frequency</cell>
                        <cell role="label">Men-related terms</cell>
                        <cell role="label">Frequency</cell>
                    </row>
                    <row role="data">
                        <cell role="data">women </cell>
                        <cell role="data">169</cell>
                        <cell role="data">men</cell>
                        <cell role="data">23</cell>
                    </row>
                    <row role="data">
                        <cell role="data">woman </cell>
                        <cell role="data">50</cell>
                        <cell role="data">man</cell>
                        <cell role="data">10</cell>
                    </row>
                    <row role="data">
                        <cell role="data">female</cell>
                        <cell role="data">30</cell>
                        <cell role="data">male</cell>
                        <cell role="data">9</cell>
                    </row>
                    <row role="data">
                        <cell role="data">TOTAL</cell>
                        <cell role="data">241</cell>
                        <cell role="data"/>
                        <cell role="data">41</cell>
                    </row>
                </table>
                <p>The issue with JSTOR’s topical representation of Morgan’s article is not just the
                    absence of women, it is the minimization of an array of terms related to
                    feminist analysis. Versions of the word <q>mother</q>, which is sixth on JSTOR’s
                    topic list, appears about 22 times in this article. Yet lexemes of <q>gender</q>
                    appear more than 30 times, and <q>sex</q> related terms (sexuality, sexual,
                    sexualized) appears more than 50 times, but neither appear as relevant topics.
                    Sexuality and gender are key analytic categories to scholars who do women’s
                    history, making their absence is a notable failing of JSTOR’s topic
                    categorization system.</p>
                <p>The topics related to the analytic interrogation of racial ideologies and
                    representation of non-white historical actors show additional problems. One of
                    the most relevant analytic terms for historians, <q>race</q>, does not feature
                    as a topic, even though the article is about the construction of early modern
                    racial ideologies – as the title clearly conveys. Indeed, the word <q>race</q>
                    appears more than twenty times and other forms of the word (racial, racism,
                    racialized) occur almost as often.</p>
                <p>Even more concerning is the use of <q>African Americans</q> as a topic. This is
                    an article primarily about descriptions of African women by Europeans
                    traveling through Africa and Indigenous women in what would become America. In
                    fact, <q>Africa</q> appears almost 3 dozen times, and <q>African</q> over
                    seventy times, but neither made JSTOR’s topic list. In contrast,
                        <q>African-American</q> appears once — in footnote 36 as part of the title
                    of a cited book. Yet JSTOR lists it listed as the third most relevant topic of
                    this article. An entire continent of people has been erased through topical
                    mislabeling in an echo of the Euro-centric bias long critiqued in other library
                    information systems <ptr target="#howard2018"/>. The algorithmic erasure of
                    African and African American women has been repeatedly noted as problemamtic by
                    scholars across fields [<ref target="#noble2018">Noble 2018</ref>] [<ref
                        target="#johnson2018">Johnson 2018</ref>]. </p>
                <p>Likewise, terms frequently related to Native Americans (Native, Indian,
                    Amerindian, Indigenous) occur more than three dozen times in the text, yet did
                    not rate a topic, while <q>civility</q>, which appeared just over a dozen times,
                    did. Such topic choices raise the question of whose perspective JSTOR privileges
                    with its topics. Morgan certainly discusses <q>civility</q>, but does so in
                    terms of the ways that Europeans mobilized it as a weapon of racemaking. Notions
                    of civility are not a helpful representation of the article’s contents because
                    ideas connected to racial ideologies apparently did not merit a JSTOR topic.
                    Markers of civilization have historically marked non-Europeans as exploitable
                    heathens, but the modern meaning of civility as formal politeness elides these
                    racist and colonialist overtones. The erasure of racial ideologies, as well as
                    topics of Africans and Indigenous Americans, means that the necessary topical
                    context here is lost, fundamentally misrepresenting civility’s meaning in
                    Morgan’s work.</p>
                <p>These comparative word frequencies suggest some human-created problems with
                    JSTOR’s topics construction. Contrary to what seems to be JSTOR’s explanations
                    of its algorithmic processes, these topics are not based simply on word
                    frequency. It appears that JSTOR or outsourced staff have made decisions about
                    what should and should not be in its Thesaurus and the granularity into which
                    some topics should be divided. Unfortunately, those decisions seem to have
                    effects that are both unintended and unattended to. JSTOR’s rule base system
                    (which may involve human-curated sets of rules and/or rule based machine
                    learning systems that decide the parameters for classification based on applied
                    domain knowledge) appears to be one that minimizes women, Africans, and
                    scholarship on race as relevant topics. Analytic categorizations that seem most
                    appropriate for scholarship on gender, race, and sexuality, as well as
                    intersectional topics, seem largely absent. This appears to be suggest an
                    indexing bias, offering an example <quote rend="inline">where inaccuracy crosses
                        the line into bias</quote>
                    <ptr target="#reidsma2016"/>.</p>
            </div>
            <div>
                <head>Where are the Women in Women’s History?</head>
                <p>Other women’s history articles show similar erasures and misrepresentations. In
                    September 2017, historian Monica Mercado tweeted about the JSTOR topics applied
                    to Linda Kerber’s well known article, <title rend="quotes">Separate Spheres,
                        Female Worlds, Woman’s Place: The Rhetoric of Women’s History</title> [<ref
                        target="#kerber1988">Kerber 1988</ref>]. Especially since the words
                        <q>women</q>, <q>woman</q>, and <q>female</q> all appear in the title,
                    Mercado found it surprising that JSTOR’s most relevant topic was <q>Men</q>. A
                    JSTOR’s taxonomy manager’s response was, effectively, that it was just the
                    algorithm: the result was <quote rend="inline">relate[d] to how many times those
                        topics appear in the document</quote>. She offered as proof that <quote
                        rend="inline"><q>Men</q> appears 63x; Women 25</quote> (<ref
                        target="#figure02">Figure 2</ref>). The user cannot know exactly what
                    algorithm created those topic frequencies from this brief interaction: did men
                    appear twice as often as women as <q>topics</q> or as word frequencies? And
                        <quote rend="inline">related to</quote> suggests another mediating factors
                    such as human curation or a pre-existing Thesaurus of terms. Regardless, word
                    frequency should have some direct relationship to topic development. Certainly
                    anyone who knows Kerber’s work would wonder at JSTOR’s topical claim: does one
                    of the founders of U.S. Women’s history really focus on men more than twice as
                    often as women in her scholarship?</p>
                <figure xml:id="figure02">
                    <head>Monica Mercado 2017 tweet showing JSTOR’s top four topics for Kerber,
                            <title rend="quotes">Separate Spheres, Female Worlds</title>, article
                        with response from JSTOR Taxonomy Manager.</head>
                    <graphic url="resources/images/figure02.png"/>
                </figure>
                <p>While I cannot recreate JSTOR’s topic-producing algorithms, I can count the
                    approximate frequency of male/female-associated words in Kerber’s article as a
                    supplement to my own understanding of its women-focused content. Table 2 shows
                    that <q>women</q> appears more than five times as often as <q>men</q>. In fact,
                        <q>women</q> is by far the most frequent word in the article (after stopword
                    removal) with more than 360 mentions. In contrast, <q>men</q> appears fewer than
                    60 times. And an array of women-related words (feminine, feminism, feminist)
                    appear far more than three dozen times. There are zero appearances of any
                    masculine parallel terms (<ref target="#table02">Table 2</ref>). As anyone who
                    has read Kerber’s work would attest, it is nonsensical that the first topic for
                    this article is <q>Men</q>.</p>
                <table xml:id="table02">
                    <head>Approximate frequency of select women- and men-related terms in Linda Kerber,
                        “Separate Spheres, Female Worlds, Woman’s Place.”</head>
                    <row role="label">
                        <cell role="label">Term</cell>
                        <cell role="label">Frequency</cell>
                        <cell role="label">Name</cell>
                        <cell role="label">Frequency</cell>
                    </row>
                    <row role="data">
                        <cell role="data">women</cell>
                        <cell role="data">361</cell>
                        <cell role="data">men</cell>
                        <cell role="data">55</cell>
                    </row>
                    <row role="data">
                        <cell role="data">woman</cell>
                        <cell role="data">47</cell>
                        <cell role="data">man</cell>
                        <cell role="data">10</cell>
                    </row>
                    <row role="data">
                        <cell role="data">womanhood</cell>
                        <cell role="data">12</cell>
                        <cell role="data">manhood</cell>
                        <cell role="data">0</cell>
                    </row>
                    <row role="data">
                        <cell role="data">female</cell>
                        <cell role="data">28</cell>
                        <cell role="data">male</cell>
                        <cell role="data">23</cell>
                    </row>
                    <row role="data">
                        <cell role="data">femine/ism/ist/</cell>
                        <cell role="data">42</cell>
                        <cell role="data">masculine//ism/ist</cell>
                        <cell role="data">0</cell>
                    </row>
                    <row role="data">
                        <cell role="data">TOTAL</cell>
                        <cell role="data">490</cell>
                        <cell role="data"/>
                        <cell role="data">88</cell>
                    </row>
                </table>
                <p>Sometime between September 2017 and April 2018 JSTOR attempted to address
                    Mercado’s concern over the minimization of women’s history. Unfortunately, it
                    appeared to do so by removing <q>Women</q> as a topic for this article (See <ref
                        target="#figure03">Figure 3</ref>). It did add <q>Women’s history</q> as the
                    fifth topic – but given that this article is entirely about the state of women’s
                    history, that seems a rather substantial underrating of its importance,
                    particularly since <q>Men</q> remained the first topic.</p>
                <figure xml:id="figure03">
                    <head>JSTOR categories for Kerber, <title rend="quotes">Separate Spheres</title>
                        in June 2018.</head>
                    <graphic url="resources/images/figure03.png"/>
                </figure>
                <p>When again asked about the absence of the topic of <q>Women</q>, a JSTOR
                    taxonomist responded on social media that <quote rend="inline">We removed Women
                        as a topic due to noise a few years ago</quote>.
                    (Perhaps she was confused about dates, since Mercado posted that image in
                    September 2017.) The JSTOR staff member was referring to the computer science
                    meaning of <q>noise</q> as data that is meaningless or unable to be correctly
                    interpreted. But what does it signal that JSTOR decided that <q>Women</q> is
                        <q>noise</q>, but <q>Men</q> is not? This seeming lack of understanding of
                    the historical place of women and women-related topics in academic scholarship
                    suggests that structural power relations – a central analytic accomplishment of
                    the field — are not on JSTOR’s radar. One might argue that since <q>Men</q> is
                    the standard (or what some call the <q>null gender</q>), it might be a less
                    frequent search than <q>Women</q>, who still tend to carry non-normative
                    status <ptr target="#wagner2015"/>. When men are the subject in the vast
                    majority of historical scholarship, how is it useful for the topic of <q>Men</q>
                    to appear as such a relevant topic? And why would <q>Men</q> not appear as a
                    major topic for the majority of historical scholarship, then? The same tweet
                    claimed that JSTOR will <quote rend="inline">probably do the same with
                        Men</quote> (remove it as a too-noisy topic), but as of July 2019, that did
                    not seem to have happened and it remains first in the list of relevant topics
                    for Kerber’s article. Men still remains an outsize and inaccurate JSTOR topic in
                    many women’s history pieces of scholarship. Regarding women as <q>noise</q>
                    effectively erases the hard-won successes of women’s history, including the
                    decades of efforts to write women back into historical analysis.</p>
                <p>The Kerber article’s other assigned topics also minimize the importance of women
                    in the piece. Why would a topic like <q>US history</q> be seen as more relevant
                    to this article than <q>Women’s history</q>? Surely U.S. history is an
                    exceptionally broad, perhaps even a too noisy topical category? Moreover, as any
                    women’s historian can attest, <q>gender</q> and <q>women</q> are distinct topics
                    of inquiry – this is an article about women far more than gender (a quick
                    frequency comparison: <q>gender</q> appears about 2 dozen times, <q>women</q>
                    more than 350). Yet <q>Gender equality</q> and <q>Gender roles</q> both appear
                    as topics. At best, <q>Gender equality</q> offers a positivist gloss on Kerber’s
                    piece, which is about understanding women’s lives through the historic construct
                    of separate spheres; not about women achieving gender equality. <q>Gender
                        roles</q> is a description of culturally expected behavior. <q>Gender</q> as
                    an analytic category is how scholars have theorized the ways that structural
                    sexism allows patriarchy to flourish; in other words, an apparatus to understand
                    gender inequality and oppression. Gender as a category of historical analysis is
                    one of the major inventions of feminist scholarship <ptr target="#scott1986"/>.
                    It denotes far more than women’s roles in a given society. Instead, it is a
                    sophisticated problematization of heteroesssential and patriarchal structures of
                    power. Feminist scholars have theorized gender in terms of its performativity,
                    its relation to matrices of domination, and its intersectionality [<ref
                        target="#butler2006">Butler 2006</ref>] [<ref target="#collins2008">Collins
                        2008</ref>]. Turning gender into <q>Gender roles</q> transforms an analytic
                    concept of power relations into a descriptive term that identifies how men and
                    women are expected to behave in a given society.</p>
                <p>Other topics show additional inadvertent erasures, suggesting granularity or
                    algorithmic decisions that have substantial consequences for categorization
                    accuracy. The topic of <q>houses</q> is puzzling because Kerber’s article does
                    not generally focus on <q>houses</q> in the sense of an architectural entity,
                    nor as a woman’s workplace. A frequency count of Kerber’s article shows that
                        <q>house/houses</q> appears more than two dozen times — but almost all of
                    those mentions are in reference to <q>Hull House</q>, the Chicago settlement
                    house co-founded by Nobel peace prize winner Jane Addams and Ellen Gates Starr.
                    A bigram frequency list confirms that <q>Hull House</q> is the fourth most
                    frequent pair of terms. In this case, an algorithmic error has erased the work
                    of a Nobel-prize winning woman rather than offering metadata to promote relevant
                    discovery.</p>
                <p>Kerber’s piece is not an exception. In my review of women’s history articles,
                    these kinds of problems appeared repeatedly. For example, Terri Snyder’s 2012
                        <title rend="quotes">Refiguring Women in Early American History</title> is,
                    as its title suggests, a review of the field of early American women’s history
                        [<ref target="#snyder2012">Snyder 2012</ref>]. When I first looked at this
                    article’s topics in April 2018, women was its ninth most relevant topic, after
                    the top three of <q>Native Americans</q>, <q>History</q>, and <q>African
                        American Literature</q> (<ref target="#figure04">Figure 4</ref>). This again
                    raises questions of why <q>Women</q> would be seen as noise, but <q>History</q>
                    would not – not to mention that it is not accurate to say that Snyder’s article
                    focuses on African American literature.</p>
                <p>The social media attention to JSTOR’s shortcomings in April 2018 seems to have
                    led to ad hoc changes. The JSTOR taxonomy manager tweeted that <q>women</q> would
                    be <quote rend="inline">added back as a use case</quote> within the
                            month.
                    (In computational terms, a use case defines the relationship between actors and
                    defined steps; from the user’s perspective, it is how a system will respond to
                    their request. I’m not sure exactly what a <q>use case</q> means in this
                    context, since <q>Women</q> clearly was a possible JSTOR topic already – just
                    not deemed a highly relevant one.) And indeed, <q>Women</q> had moved up to the
                    first, and presumably most relevant topic position by July 2018, and <q>Women’s
                        history</q> was now a topic as well, which is a much more appropriate topic
                    than the April topic of <q>Working women</q> (<ref target="#figure05">Figure
                        5</ref>). Yet we might wonder what domain experts were involved with this
                    decision-making process. It is also worth noting that this shift led to other
                    negative outcomes: all mentions of marginalized groups (African American
                    literature, Native Americans) were removed as topics. In adding a focus on women
                    and gender, JSTOR’s topics eliminated all sense of the article’s intersectional
                    approach to and focus on non-white women in early American history.</p>
                <figure xml:id="figure04">
                    <head>April 2018 screenshot from JSTOR of Snyder, <title rend="quotes"
                            >Refiguring Women</title> article topics.</head>
                    <graphic url="resources/images/figure04.png"/>
                </figure>
                <figure xml:id="figure05">
                    <head>July 2018 screenshot from JSTOR of Snyder, <title rend="quotes">Refiguring
                            Women</title> article topics. This is a slightly different format from
                        figure 4 due to results list v. individual item view.</head>
                    <graphic url="resources/images/figure05.png"/>
                </figure>
                <p>While this does suggest that JSTOR investigated and revamped its topic
                    identification in response to critiques, the continued inclusion of <q>Men</q>
                    still seems concerning. Moreover, <q>Swords</q> and <q>Auctions</q> do not seem
                    to be particularly relevant topics to the main arguments of this piece,
                    especially alongside an exceptionally broad topic like <q>United States
                        history</q>. JSTOR’s application of <q>Sword</q> as a topic is a
                    recognizable curiosity – and at some point between July 2018 and July 2019,
                    JSTOR apparently recognized its erroneous application; it no longer appears as a
                    topic. However, as with Kerber’s article, the addition of <q>Gender roles</q> is a
                    less obvious and more damaging mislabeling of a field of study. JSTOR topics
                    have missed capturing crucial theoretical underpinnings and arguments in
                    Snyder’s essay, and have seemed to present women’s history with rose-colored
                    glasses that not only elide implications of struggle, conflict, and oppression,
                    but in their new formulation, further erase Indigenous and African American
                    women. While JSTOR’s responsive efforts are commendable, feminist and social
                    justice workers have long argued that impact matters far more than intent [<ref
                        target="#utt2013">Utt 2013</ref>]. Good intentions still lead to biased
                    algorithmic results when programmers and chosen domain experts do not
                    effectively or appropriately analyze scholarship.</p>
                <p>The disconnect between JSTOR’s topics and what field experts would see as the
                    significant content of these publications reflects ongoing discussions in
                    digital humanities regarding algorithmic mediation and the role of
                    classification versus content representation. With the rise of full text data
                    mining capabilities, Library of Congress subject headings and similar controlled
                    indexing vocabularies may seem to be too broad-brushed an approach for users
                    accustomed to searching the internet and for exact strings of text.
                    Full-text-based topic indexing holds the promise of using an author’s words to
                    generate precise subject categorizations rather than slotting new work into
                        <foreign xml:lang="la"> a priori</foreign> taxonomies. But as these examples
                    show, more technological mediation does not necessarily lead to better outcomes.
                    JSTOR has seemed to tinker with ways to improve its topic indexing, but it
                    continues to fall short. Women-focused histories are still being categorized as
                    focusing on men. Without topic terms that can capture sophisticated analytic
                    content analysis of race and gender, JSTOR topics continue to misrepresent
                    scholarly content.</p>
            </div>
            <div>
                <head>Book Topics: Additional Text, Added Bias</head>
                <p>In recent years, JSTOR has expanded its corpus beyond journal articles to include
                    digital versions of monograph and anthology chapters from a variety of academic
                    publishers. This means that any inadvertent sexism and racism in algorithmic
                    topic systems have potentially expanded to pollute book-length scholarship. One
                    might think that longer texts divided into multiple chapters might ameliorate
                    some of the errors of the article topic categorizations. Unfortunately, it
                    appears that similarly problematic topic indexing has propagated these new
                    genres of digital scholarship, expanding the bias presented to users.</p>
                <p>Early American historian Ann Little has explained that her biography, <title
                        rend="italic">The Many Captivities of Esther Wheelright</title>, aims to
                    move beyond privileging men’s recounting of and relationships in women’s lives.
                    Little wrote Esther Wheelright’s biography to <quote rend="inline">tell the
                        stories of the girls and women who loved her, clothed and fed her, educated
                        her, worked and prayed with her, competed with her, and buried her</quote>
                    <ptr target="#little2016" loc="12"/>. But JSTOR’s topic categories do not convey
                    Little’s women-centered approach. A word cloud made up of the JSTOR topics
                    listed for seven book chapters from <title rend="italic">The Many
                        Captivities</title> visually represent the topics assigned to the book’s
                    chapters, with the size of words correlating to the number of times the topic
                    appeared (<ref target="#figure06">Figure 6</ref>).</p>
                <p>As Figure 6 shows, neither <q>Women</q> nor <q>Women’s history</q> appear as a
                    topic for any of the chapters. Yet this is clearly a book focused on a woman. If
                    we turn to the book’s full text, the most frequent word, appearing more than
                    1300 times, is <q>her</q>. In contrast, <q>his</q> appears only about 300 times.
                    In fact, of the top-10 most frequently used words in the book manuscript, six
                    are related to women (her, Esther, she, Ursuline (an order of nuns), women,
                    mother/s) and none to men. <q>Mothers</q> was identified as the most frequent
                    JSTOR topic for Little’s book. Unfortunately, its topic frequency does not
                    disambiguate Little’s very different usages of the word: while early sections of
                    the book talk about familial mothering, most of the mentions of <q>mothers</q>
                    (more than 300 of the c. 445 occurrences) refers to the head of a female
                    religious community. Indeed, the second most common bigram in the book was
                        <q>Mother Esther</q>, and similarly, <q>Sister Marie</q> was a top-ten most
                    frequent bigram. Ideally, any meaningful topical categorization system would
                    disambiguate word sense to avoid these kinds of pitfalls and omissions. This
                    again may suggest that JSTOR has not effectively evaluated the need for
                    disambiguation to accurately represent complex topics.</p>
                <figure xml:id="figure06">
                    <head>Wordcloud of all JSTOR topic phrases in Little’s <title rend="italic"
                            >Esther Wheelright</title> book chapters.</head>
                    <graphic url="resources/images/figure06.png"/>
                </figure>
                <p>As in its topical assignments to articles, JSTOR's miscategorizations seem to be
                    particularly problematic in relation to traditionally marginalized groups.
                    Little’s book shows this erasure of Indigenous people, specifically. A
                    substantial portion of Little’s book focuses on Wheelright’s interactions with
                    the Wabanaki. The word <q>Wabanaki</q> appears over 400 times in the text, and
                        <q>Indian</q> and <q>Native</q> add another 280+ appearances. (For
                    comparison, <q>Governor</q> appears fewer than 100 times, but still appears as a
                    JSTOR topic for multiple chapters.) Yet the only ethnicity-related topic JSTOR offers
                    is <q>White people</q>, which is a topic assigned to three different chapters.
                    The book’s chapter that focuses on Wheelwright’s captivity in a Wabanaki
                    community where she was known as Mali includes extensive discussion of Wabanaki
                    gender, social, and cultural practices. Yet of the eight topics assigned to that
                    chapter, only one, <q>Wigwams</q>, relates directly to Indigenous people, even
                    though it is mentioned fewer than two dozen times. This effectively reduces the
                    central role of the Wabanaki and other Indigenous groups to mention of the
                    material object of their housing. Most of the other topics assigned to that
                    chapter relate to Catholicism, which seems to promote a Euro-centric and
                    settler-colonial bias that erases Indigenous people.</p>
                <p>These problems harm authors who have spent years thoughtfully framing their
                    scholarship and reasonably expect that databases will allow others to accurately
                    discover their content. When I shared JSTOR’s topic results and my word
                    frequency analysis with Ann Little, she responded with deep frustration <quote
                        rend="inline">that I apparently (successfully!) wrote a biography centering
                        on girls and women's lives — as your word count shows — but all of that
                        effort gets swallowed up by JSTOR's deeply flawed algorithm. How could it so
                        deeply distort my book and misapprehend its purpose?</quote> Moreover, she
                    writes that <quote rend="inline">the near-total erasure of Wabanaki (Native
                        American, First Nations), French Canadian, and Anglo-American people is also
                        deeply concerning — it's as though JSTOR has its own view of what history
                        is</quote>.<note>Little, Personal Communication, July 14, 2018.</note>
                    JSTOR’s seeming tendency to apply topics that misrepresent content does not
                    appear random. Without fully understanding JSTOR’s topical identification
                    processes, I can only guess that, as Little suggests, it seems to be rooted in a
                    lack of expert knowledge in the fields that it is seeking to topically identify.
                    It may be that JSTOR, which covers an array of disciplines, does
                    not have a system that engages experts from each field of study in the creation
                    of a controlled vocabulary. But the result may be topic terms that misrepresent
                    and misconstrue the content of publications. In this case, the focus of JSTOR’s
                    topics on a seemingly monolithic presentation of Eurocentric terms in Little’s
                    scholarship erases the intersectional and diverse communities her work presents. </p>
                <p>Other books’ JSTOR topics show similar shortcomings surrounding content on gender
                    and race. Jennifer Morgan’s <title rend="italic">Laboring Women: Reproduction
                        and Gender in New World Slavery</title>, is, as its title suggests, a study
                    of enslaved women in the 16th-18th centuries. JSTOR identified <q>Men</q> as a
                    chapter topic word seven times in but <q>Women</q> only five times – as is
                    visually striking in <ref target="#figure07">Figure 7</ref>. <q>Son</q> and
                        <q>Children</q> are topics, but <q>Daughter</q> is not. Topics do include
                    some quasi-conceptual themes, such as the terms <q>Women’s rights</q> and
                        <q>Gender equality</q>, but it is hard to see how these are relevant terms
                    for a study of enslaved women. It is also notable that JSTOR does not apply the
                    topics of <q>Race</q> or <q>Racism</q> to <title rend="italic">Laboring
                        Women</title>. Moreover, the system of <q>Slavery</q> appears as a topic
                    only once. Instead, <q>Slaves</q> and <q>Slave ownership</q> are relatively
                    frequent topics. But these two are not parallel categories. It would make more
                    sense to have a topic of <q>Slave owners</q> or <q>Enslavers</q> — not the
                    passive construction of <q>slave ownership</q> — in opposition to <q>Enslaved
                        People</q> or <q>Slaves</q>. Having such unmatched categories sidesteps the
                    reality that white people owned, traded, and tortured human beings under that
                        <q>ownership</q> category and erases the abhorrent power abuses inherent in
                    enslavement. It also suggests a lack of awareness of the current state of the
                    field. Historians of slavery have fought to recognize enslaved people as
                    individuals first, and enslavement as a condition by referring to <q>enslaved
                        people</q> rather than <q>slaves</q>
                    <ptr target="#foreman2018"/>.</p>
                <figure xml:id="figure07">
                    <head>Wordcloud of all JSTOR topic phrases in Morgan’s <title rend="italic"
                            >Laboring Women</title> book chapters.</head>
                    <graphic url="resources/images/figure07.png"/>
                </figure>
                <p>The absence of conceptual categories such as race/racism raises serious concerns
                    about the classification of historical scholarship. Historians do not just
                    describe people; we make arguments about systems of power. For example, the
                    publisher describes Daniel Livesay’s recent book, <title rend="italic">Children
                        of Uncertain Fortune: Mixed-Race Jamaicans in Britain and the Atlantic
                        Family</title> as focusing on <quote rend="inline">the largely forgotten
                        eighteenth-century migration of elite mixed-race individuals from Jamaica to
                        Great Britain, <title rend="italic">Children of Uncertain Fortune</title>
                        reinterprets the evolution of British racial ideologies as a matter of
                        negotiating family membership</quote> <ptr target="#uncpress2018"/>.
                    This is a book about mixed-race individuals, racial ideologies, and the role of
                    familial relationships across the Caribbean and Great Britain. Its assigned
                    Library of Congress subjects also make clear that the book focuses on race: all
                    10 LOC subjects assigned to the book include the word <q>race</q> or <q>racially
                        mixed</q> (<ref target="#figure08">Figure 8</ref>). Again, this is not to
                    argue that LOC classification schemas are free of bias [See, for example, <ref
                        target="#berman2014">Berman 2014</ref>, <ref target="#dudley2015">Dudley
                        2015</ref>, <ref target="#hathroc2016">Hathcoc 2016</ref>, <ref
                        target="#knowlton2005">Knowlton 2005</ref>]. But Library of Congress
                    subjects do provide another support for the centrality of race in this book.</p>
                <figure xml:id="figure08">
                    <head>Library of Congress Subject Headings for Livesay, <title rend="italic"
                            >Children of Uncertain Fortune</title> clearly show the centrality of
                        race and racial mixture to the book’s content.</head>
                    <graphic url="resources/images/figure08.png"/>
                </figure>
                <p>Unfortunately, JSTOR’s topics for <title rend="italic">Children of Uncertain
                        Fortunes</title> are problematic. Not only are race, racism, racial mixture,
                    or any related terms completely absent from JSTOR’s chapter topics, the most
                    frequent topic phrase associated with the book’s contents is <q>White People</q>
                        (<ref target="#figure09">Figure 9</ref>). Moreover, even though much of this
                    book focuses on free people of color, there is no parallel focus on <q>Black
                        People</q> or other appropriate terms to identify people of African descent.
                    Instead, non-white people are presumably represented with the JSTOR’s repeated
                    use of the topic of <q>Slaves</q>.</p>
                <figure xml:id="figure09">
                    <head>Wordcloud of all JSTOR topic phrases in Livesay’s <title rend="italic"
                            >Uncertain Fortune</title> book chapters.</head>
                    <graphic url="resources/images/figure09.png"/>
                </figure>
                <p>JSTOR does apply the topic of <q>African American</q> once – but this is a book
                    about Jamaica and Britain, not North America, and I could only find <q>African
                        American</q> in the text one time (Afro-Caribbean appears somewhat more
                    frequently). According to searches undertaken in the Kindle version of the book,
                        <q>Black</q> appears approximately 380 times, and <q>Slave</q> about 450
                    times, but <q>Black People</q> did not rate a topic while <q>Slaves</q> rated
                    one for multiple chapters. Africa/African appears in the text over 300 times,
                    but also did not rate a category, while <q>Bequests</q>, which appeared under
                    100 times, did. Neither was there any topic related to mixed-race people,
                    despite the term <q>mixed</q> (as in mixed-race, mixed heritage, mixed ancestry,
                    etc) appearing almost 800 times. Of course, it is hard to make conclusive
                    arguments about an entire book’s content from simple word counts. But between
                    the book’s description, the LOC subject headings, and the above word
                    frequencies, it does seem that JSTOR has flattened important historical
                    differences into racial binaries. As importantly, it seems to employ algorithms
                    that privilege whiteness, and can only see non-white people in terms of their
                    enslavement, rather than as multidimensional human beings with specific ethnic
                    and racial histories. By not focusing on conceptual categories that are central
                    to historical scholarship, JSTOR’s topics do not effectively allow for discovery
                    related to the central analytic accomplishments of this scholarship.</p>
            </div>
            <div>
                <head>Conclusion: Interactions, Reactions, and Ways Forward</head>
                <p>JSTOR has explained its <q>Topics</q> as experimental and welcomes feedback.
                    Every document’s topic list is accompanied by a thumbs-up/thumbs-down clickable
                    icon and a link to <q>Let us know!</q> if users see something inaccurate (<ref
                        target="#figure10">Figure 10</ref>).</p>
                <figure xml:id="figure10">
                    <head>Example of JSTOR request for feedback on topic lists.</head>
                    <graphic url="resources/images/figure10.png"/>
                </figure>
                <p>There is no question that JSTOR topic applications have an array of seemingly
                    benign errors. In the scholarship discussed above, we might question the
                    relevance of topics on <q>Swords</q> (<ref target="#figure04">Figure 4</ref>)
                    and <q>Academic libraries</q> (<ref target="#figure01">Figure 1</ref>), for
                    example. One user publicly noted that an article with the phrase <q>to bear</q>
                    erroneously was assigned a topic of <q>Bears</q>, again suggesting problems with
                    disambiguation of word senses.<note><ref target="https://twitter.com/alforrester/status/927981173049118720">
                        https://twitter.com/alforrester/status/927981173049118720</ref></note>
                    In a response to another user sharing seemingly nonsensical JSTOR tags in March
                    2018, a JSTOR representative responded that <quote rend="inline">We reviews
                        [sic] those each weed [sic] to make updates to our rules.</quote>).
                    Besides, I imagine, cursing Twitter’s no-editing-post-tweet rule, JSTOR
                    representatives do seem to take individual error notifications under advisement
                    and seek to improve the system. Matthew Reidsma reported having similar
                    experiences with offering feedback on questionable results to ProQuest <ptr
                        target="#reidsma2016"/>. </p>
                <p>The need to gather user feedback for new categorization systems is
                    understandable. At the same time, what responsibility do organizations – let
                    alone one that charges Universities hundreds of thousands of dollars - have to
                    evaluate even an admittedly experimental system for racism and sexism before
                    widely releasing it <ptr target="#jstor2019c"/>?
                    JSTOR has known that there are problems surrounding these issues since at least
                    2017 (See <ref target="#figure02">Figure 2</ref>).
                    Ad hoc attention to user concerns does not seem to be a best practice. A lacking
                    systemic response to racist or sexist algorithmic results is, unfortunately, all
                    too common <ptr target="#wachter-boettcher2017" loc="133"/>.</p>
                <p>Users clicking through topics should have reasonable expectations of accuracy and
                    lack of bias. While it may be easy to understand that the characterization of
                        <q>Bears</q> in a piece discussing bearing arms is incorrect, it is more
                    concerning when topics <emph>seem</emph> accurate, but are actually
                    marginalizing and misrepresenting women and non-white people. JSTOR has created
                    a system that can produce topics, but perhaps has not fully evaluated whether
                    those topics have appropriate disciplinary meaning, nor whether their use of
                    mainstream sources like Wikipedia or DPedia may have introduced an array of
                    racist and sexist terminology and beliefs.</p>
                <p>Moreover, when feminist scholars publicly raised these issues with JSTOR, there
                    seemed to be a sense – in line with their exclamation-pointed <quote
                        rend="inline">Let us Know!</quote> topic list suggestion – that users will
                    volunteer constructive feedback to help improve their system.
                    JSTOR states it has worked with 30 subject matter experts who <quote
                        rend="inline">volunteer their assistance</quote> and proclaims that <quote
                        rend="inline">We are also always happy to talk to Subject Matter Experts
                        about particular vocabularies. If you have suggestions or want to talk to us
                        about the thesaurus, email us</quote>
                    <ptr target="#jstor2019a"/>
                    <ptr target="#jstor2019b"/>. Given that users pay (either individually or
                    through an institution) for access to JSTOR’s various databases, this seems
                    particularly problematic. JSTOR may be not-for-profit, but it is not a volunteer
                    organization. And asking female scholars who have already identified issues of
                    sexism and racism in JSTOR’s system to spend more <q>pro bono</q> time working
                    on a solution conveys a regrettable lack of understanding of the unrecognized
                    service work regularly expected of women in academia <ptr target="#guarino2017"/>
                    <ptr target="#babcock2018"/>.</p>
                <p>Safiya Noble astutely advises that we <quote rend="inline">ask ourselves what it
                        means in practical terms to search for concepts about gender, race, and
                        ethnicity only to find information lacking or misrepresentative, whether in
                        the library database or on the open web</quote>
                    <ptr target="#noble2018" loc="142"/>. I have no reason to believe that JSTOR’s
                    problems are rooted in purposeful racism and sexism. But ultimately, that is
                    irrelevant to their results. The examples I have offered here of problematic
                    JSTOR topics suggests that its taxonomers and programmers have perhaps not
                    adequately addressed Noble’s questions. If, as it seems, these kind of biases
                    are widespread, JSTOR appears to be abdicating its responsibilities to provide a
                    non-racist and non-sexist product. Such shortcomings when programming a complex
                    system may be understandable but they should not be acceptable. They are
                    structural issues that need to be addressed beyond inviting crowd-source error
                    finding.</p>
                <p>The topics assigned to the articles and books I analyzed here suggest that JSTOR
                    fails particularly well in reference to marginalized histories: for women, for
                    Africans and African Americans, and for Native Americans. Articles on women’s
                    history are assigned the topic of <q>Men</q>, which is not even a particularly
                    relevant topic of analysis in historical study. Scholarship on Africa and people
                    of African descent are miscategorized as being about African Americans, who are then
                    assumed to be relegated to a presumed slave status. Indigenous people are
                    viewed through settler colonial and Eurocentric perspectives. Important
                    conceptual categories like race and gender are elided or erased.</p>
                <p>Shortcomings in JSTOR’s topic classifications raise an array of ethical
                    questions. JSTOR only exists because scholars’ intellectual labor fills its
                    database. What does JSTOR owe in return when it classifies and categorizes the
                    fruits of that labor? Moreover, JSTOR promotes its topic system as particularly
                    useful for students <ptr target="#jstor2018"/>.
                    I, as a scholar with decades of domain expertise, can easily look at the
                    categorizations of Jennifer Morgan's <title rend="quotes">Some Could
                        Suckle</title> article and know that it is about Africans, not African
                    Americans. Or that the topic of women is not equivalent to the analytic category
                    of gender. But for students who are an ostensible target audience for topical
                    offerings, JSTOR is providing problematic information.</p>
                <p>It matters that I came across this problem organically, in the course of
                    reviewing the field of early American women’s history because it suggests that
                    JSTOR’s racist and sexist biases may affect others interested in race and gender
                    as historical constructs. One of the challenges for ever-expanding digital
                    document providers is how to offer useful access to their contents. The staff
                    who are tasked with creating the systems that produce these topics no doubt work
                    to the best of their ability because they believe that knowledge preservation
                    and retrieval methods matter. The solutions, then, are far more complex than
                        <q>don’t be racist/sexist</q>. This is not about individual responsibility;
                    it is about structural failings. How we can search relates to the scholarship we
                    can find and the knowledge that we produce.</p>
                <p>I would not be surprised if the biases I have identified may also reflect broader
                    problems with JSTOR’s topic algorithms, relevant to those outside the fields of
                    early American history. For example, on July 5, 2019, clicking on the JSTOR
                    topic of <q>Men</q> returned <title rend="quotes">Variation in Women’s Success
                        across PhD Programs in Economics</title> as the most relevant scholarship.
                    Several days later (July 8, 2019), the most relevant article under the topic
                        <q>Men</q> was listed as <title rend="quotes">Women in the medieval wall
                        paintings of Canterbury cathedral</title>. What does it say about JSTOR’s topic-producing
                    algorithms that scholarship that very much appears to be focused on women are
                    the top results for the category of <q>Men</q>? I suspect more research would
                    show other kinds of broad classification problems that shade into bias. For
                    instance, the most relevant result when clicking on the topic <q>White
                        People</q> is a chapter from a book on <title rend="italic">Martin Luther
                        King Jr’s Theory of Political Service</title>. The top result of <q>White
                        American Culture</q> (a problematic category itself) is an article on <title
                        rend="quotes">Langston Hughes, African American Literature, and the
                        Religious Futures of Black Studies</title>. These kinds of
                    miscategorizations risk derailing inexperienced researchers, curtailing the use
                    of JSTOR’s resources, and ultimately making important scholarship less easily
                    discoverable than it should be by categorizing scholarship on women and African
                    Americans as being most relevant to topics about men and white people.</p>
                <p>I hesitate to move beyond my critique to offer concrete solutions to JSTOR’s
                    practices both because I have only a limited sense of what JSTOR practices
                    entail, and a much better sense of my own limitations, which include only
                    passing knowledge of classification systems, taxonomy, database design, and the
                    multiple needs of a massive multi-disciplinary project like JSTOR. That said, I
                    can comment as an end user on the ways that JSTOR topics do and do not serve
                    scholars’ research and teaching needs, particularly in light of recent
                    scholarship on such issues in a variety of big data systems.</p>
                <p>Algorithmic transparency and algorithmic accountability have been burgeoning
                    research areas, as theorists struggle to see the far-reaching consequences of
                    big data machine learning technologies – what some have termed the <quote
                        rend="inline">social power of algorithms</quote>
                    <ptr target="#gillespie2015"/>
                    <ptr target="#dickey2017"/>
                    <ptr target="#neyland2017"/>. Some have focused on engineering-type solutions,
                    suggesting ways to improve algorithmic decision making by identifying moments of
                    bias entry (i.e., training data bias, algorithmic focus bias, algorithmic
                    processing bias, transfer content bias, interpretation bias, non-transparency of
                    bias) <ptr target="#silva2018"/>. Others have suggested how construction of
                    ethical workflows might ameliorate inadvertently ideological outcomes or a
                        <quote rend="inline">practical algorithmic ethics</quote> that can be used
                    to analyze the virtues and consequences of individual algorithms <ptr
                        target="#hepworth2018"/>
                    <ptr target="#sandvig2016"/>. While some technology and society scholars have
                    pushed for transparency in computer algorithms, others have noted that
                    transparency does not negate biased outcomes.</p>
                <p>Some scholars have suggested that wishing for unbiased classification systems is
                    heading down a mistaken path. Feminist and queer studies writers have proposed
                    various theoretical rethinkings of how we view metadata. Librarian Emily
                    Drabinski suggests alternatives to the notion that biases can be corrected and
                    classification systems can be made objective. Instead, she employs queer theory
                    to suggest <quote rend="inline">new ways of thinking about how to be ethically
                        and politically engaged on behalf of marginal knowledge formation</quote>.
                    She argues that we should to <quote rend="inline">teach knowledge production as
                        a contested project</quote> so that users recognize and engage the bias in
                    knowledge organization systems, rather than expecting functional solutions to
                    cataloguing bias <ptr target="#drabinski2013" loc="96, 108"/>. Teaching students
                    numerous search tactics – and how to recognize problematic search results – are
                    valuable cross-disciplinary skills to impart <ptr target="#grey2012"/>. While
                    this end-user-interrogation approach is a useful one, it is not likely to be
                    entirely successful, particularly when there is not algorithmic transparency.
                    Drabinski herself recognizes that <quote rend="inline">privatized corporate
                        algorithms</quote> make information organization <quote rend="inline">less
                        and less apprehendable</quote>
                    <ptr target="#drabinski2016"/>. Similar to Drabinski’s theoretical approach,
                    Anupam Chander suggests instead a <quote rend="inline">transparency of inputs
                        and results</quote> that will make visible the discriminatory production,
                    rather than eliminate it <ptr target="#chander2017"/>. Being aware of common
                    classification biases – even if we cannot know their exact production process —
                    offers one path to becoming a more thoughtful user of academic discovery
                    systems.</p>
                <p>Matthew Reidsma, one of the leading investigators of bias in library discovery
                    systems, agrees with the need for user interrogation, and simultaneously
                    proposes several areas of specific improvements. These include paying attention
                    to the degree to which our searches are powered by proxies for the information
                    we request; less unthinking trust in algorithms; increased
                    diversity among programmers; working toward an algorithmic ethics; and
                    intentional audits of software tools <ptr target="#reidsma2019" loc="148–70"/>.
                    Similarly, I can imagine practical solutions that reshape the relationship
                    between institutional consumers and database providers. JSTOR, as a
                    not-for-profit organization, may have more obligation than privately owned
                    digital document providers to live up to academic community standards. As the
                    recent University of California resistance to publishing giant, Elsevier, has
                    shown, universities can marshall their considerable power as consumers to insist
                    on a range of standards that are in line with their own values and priorities
                        <ptr target="#mckenzie2019"/>.</p>
                <p>Just because JSTOR can create topics does not mean that it offers useful metadata
                    across fields of scholarship. It might be valuable to rethink the metadata sources
                    and subject expert review processes that JSTOR currently practices. In the era
                    of PhD training that moves beyond professorial careers, perhaps JSTOR could
                    partner with professional organizations to offer internships that pair PhD
                    students in specific fields with library and information science experts to
                    review subject-based metadata. This might help evaluate the degree to which
                    topics from the JSTOR Thesaurus’s controlled vocabulary meet standards for
                    appropriacy (is a given term appropriate for the target audience?) and currency
                    (does it reflect current common usage?). For scholars of women, race and
                    colonialism, at least, the answer currently appears to be no for too many of
                    JSTOR’s topic categorizations.</p>
                <p>A commitment to changes in and increased transparency of review processes might
                    also be a positive step. JSTOR does make individual changes when users point to
                    errors. But it seems unprepared to deal with a wholescale review of the ways
                    that their topic assignments foreground inadvertent sexism and racism, and may
                    lack structural due diligence against bias. JSTOR prides itself on <quote
                        rend="inline">enhancing its content with strong metadata</quote>
                    <ptr target="#humphrey2019" loc="slide 9"/>. It is not clear that its current
                    topic system meets that standard. Perhaps involving end users more
                    systematically (beyond thumbs up/thumbs down and ad hoc communications) would
                    promote a more transparent knowledge organization system. Scholars have pointed
                    specifically to the need to <quote rend="inline">expand the boundaries of LIS
                        [Library Information Systems]</quote> to better understand the <quote
                        rend="inline">ways in which tools impact the research process</quote>
                    <ptr target="#manoff2015" loc="526"/>. Ultimately, JSTOR is a crucial resource
                    for historians and other scholars, students, and educators. It may be easy for
                    an outsider to find shortcomings, but finding solutions is far from an
                    individual task. Raising awareness of these kinds of biases can encourage
                    academic communities to work with JSTOR and other digital providers to create
                    systems that better reflect the scholarship on which they build their
                    systems.</p>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="bailey2011" label="Bailey 20111">Bailey, Moya Z. <title rend="quotes"
                        >All the Digital Humanists Are White, All the Nerds Are Men, but Some of Us
                        Are Brave</title>. <title rend="italic">Journal of Digital
                        Humanities</title> 1, no. 1 (Winter 2011). <ref
                        target="http://journalofdigitalhumanities.org/1-1/all-the-digital-humanists-are-white-all-the-nerds-are-men-but-some-of-us-are-brave-by-moya-z-bailey/"
                        >http://journalofdigitalhumanities.org/1-1/all-the-digital-humanists-are-white-all-the-nerds-are-men-but-some-of-us-are-brave-by-moya-z-bailey/</ref></bibl>
                <bibl xml:id="babcock2018" label="Babcock et al. 2018">Babcock, Linda, Maria P.
                    Recalde, and Lise Vesterlund. <title rend="quotes">Why Women Volunteer for Tasks
                        That Don’t Lead to Promotions</title>. <title rend="italic">Harvard Business
                        Review</title>, July 16, 2018. <ref
                        target="https://hbr.org/2018/07/why-women-volunteer-for-tasks-that-dont-lead-to-promotions"
                        >https://hbr.org/2018/07/why-women-volunteer-for-tasks-that-dont-lead-to-promotions</ref>.</bibl>
                <bibl xml:id="berman2014" label="Berman 2014">Berman, Sanford. <title rend="italic"
                        >Prejudices and Antipathies: A Tract on the LC Subject Heads Concerning
                        People</title>. Jefferson, N.C.: McFarland and Co., 2014.</bibl>
                <bibl xml:id="bethel1994" label="Bethel 1994">Bethel, Kathleen E. <title
                        rend="quotes">Culture Keepers: Cataloging the Afrocentric Way</title>.
                        <title rend="italic">The Reference Librarian</title>. (July 1994) 21, vol
                    45-46. 221-240. <ref target="https://doi.org/10.1300/J120v21n45_21">
                        https://doi.org/10.1300/J120v21n45_21</ref></bibl>
                <bibl xml:id="bianco2012" label="Bianco 2012">Bianco, Jamie “Skye”. <title
                        rend="quotes">This Digital Humanities Which is Not One</title> in Gold,
                    Matthew K., ed. <title rend="italic">Debates in the Digital Humanities</title>.
                    Minneapolis: University Of Minnesota Press, 2012. 96-112.</bibl>
                <bibl xml:id="blei2012" label="Blei 2012">Blei, David M. <title rend="quotes">Topic
                        Modeling and Digital Humanities</title>. <title rend="italic">Journal of
                        Digital Humanities</title> no.1 (Winter 2012). <ref
                        target="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/"
                        >
                        http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/</ref>.</bibl>
                <bibl xml:id="block2006" label="Block 2006">Block, Sharon. <title rend="quotes"
                        >Doing More with Digitization</title>. <title rend="italic"
                            >Common-Place</title> 6, no. 2 (January 2006).
                    <ref target="http://commonplace.online/article/doing-more-with-digitization/ ">
                                http://commonplace.online/article/doing-more-with-digitization/ 
                            </ref>.</bibl>
                <bibl xml:id="block2011" label="Block and Newman 2011">Block, Sharon and David
                    Newman. <title rend="quotes">What, Where, When, and Sometimes Why: Data Mining
                        Two Decades of Women’s History Abstracts</title>. <title rend="italic"
                        >Journal of Women’s History</title> 23, no. 1 (March 2011): 81–109. <ref
                        target="https://doi.org/10.1353/jowh.2011.0001">
                        https://doi.org/10.1353/jowh.2011.0001</ref>.</bibl>
                <bibl xml:id="bogost2015" label="Bogost 2015">Bogost, Ian. <title rend="quotes">The
                        Cathedral of Computation</title>. <title rend="italic">The Atlantic</title>,
                    January 15, 2015. <ref
                        target="https://www.theatlantic.com/technology/archive/2015/01/the-cathedral-of-computation/384300/"
                        >
                        https://www.theatlantic.com/technology/archive/2015/01/the-cathedral-of-computation/384300/</ref>.</bibl>
                <bibl xml:id="buolamwini2016" label="Buolamwini 2016">Buolamwini, Joy. <title
                        rend="italic">How I’m Fighting Bias in Algorithms</title>. November 2016.
                        <ref
                        target="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/up-next"
                        >
                        https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/up-next</ref>.</bibl>
                <bibl xml:id="butler2006" label="Butler 2006">Butler, Judith. <title rend="italic"
                        >Gender Trouble: Feminism and the Subversion of Identity</title>. New York:
                    Routledge, 2006.</bibl>
                <bibl xml:id="chander2017" label="Chander 2017">Chander, Anupam. <title
                        rend="quotes">The Racist Algorithm?.</title>
                    <title rend="italic">Michigan Law Review</title>. 115, no. 6 (2017) 1023-1045.
                        <ref target="https://repository.law.umich.edu/mlr/vol115/iss6/13">
                        https://repository.law.umich.edu/mlr/vol115/iss6/13</ref>.</bibl>
                <bibl xml:id="collins2008" label="Collins 2008">Collins, Patricia Hill. <title
                        rend="italic">Black Feminist Thought: Knowledge, Consciousness, and the
                        Politics of Empowerment</title>. New York: Routledge, 2008.</bibl>
                <bibl xml:id="crenshaw1989" label="Crenshaw 1989">Crenshaw, Kimberle. <title
                        rend="quotes">Demarginalizing the Intersection of Race and Sex: A Black
                        Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and
                        Antiracist Politics</title>. <title rend="italic">University of Chicago
                            Legal Forum</title> 1 (1989) 139-167.
                    <ref target="http://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8">
                        http://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8
                    </ref>.</bibl>
                <bibl xml:id="christensen2008" label="Christensen 2008">Christensen, Ben. <title
                        rend="quotes">Minoritization vs. Universalization: Lesbianism and Male
                        Homosexuality in LCSH and LCC</title>. <title rend="italic">Knowledge
                        Organization</title> 35, no. 4 (2008) 229-238.
                    <ref target="https://doi.org/10.5771/0943-7444-2008-4-229">
                        https://doi.org/10.5771/0943-7444-2008-4-229
                    </ref>.</bibl>
                <bibl xml:id="diao2016" label="Diao and Cao 2016">Diao, Junli, and Haiyun Cao.
                        <title rend="quotes">Chronology in Cataloging Chinese Archaeological
                        Reports: An Investigation of Cultural Bias in the Library of Congress
                        Classification</title>. <title rend="italic">Cataloging &amp; Classification
                        Quarterly</title> 54, no. 4 (May 18, 2016): 244–62. <ref
                        target="https://doi.org/10.1080/01639374.2016.1150931">
                        https://doi.org/10.1080/01639374.2016.1150931</ref>.</bibl>
                <bibl xml:id="dickey2017" label="Dickey 2017">Dickey, Megan Rose. <title
                        rend="quotes">Algorithmic Accountability</title>. <title rend="italic"
                        >TechCrunch</title> (blog), 2017. <ref
                        target="http://social.techcrunch.com/2017/04/30/algorithmic-accountability/"
                        >
                    http://social.techcrunch.com/2017/04/30/algorithmic-accountability/</ref>.</bibl>
                <bibl xml:id="drabinski2009" label="Drabinski 2009">Drabinski, Emily. <title
                        rend="quotes">Gendered S(h)elves: Body and Identity in the Library</title>.
                        <title rend="italic">Women and Environments International</title>. 78/79
                    (Fall/Winter 2009) 16-18. <ref
                        target="http://www.emilydrabinski.com/wp-content/uploads/2012/06/emily_weimag.pdf"
                        >
                        http://www.emilydrabinski.com/wp-content/uploads/2012/06/emily_weimag.pdf</ref>.</bibl>
                <bibl xml:id="drabinski2013" label="Drabinski 2013">—. <title rend="quotes">Queering
                        the Catalog: Queer Theory and the Politics of Correction</title>. <title
                        rend="italic">The Library Quarterly</title> 83, no. 2 (April 2013): 94–111.
                        <ref target="https://doi.org/10.1086/669547">
                        https://doi.org/10.1086/669547</ref>.</bibl>
                <bibl xml:id="drabinski2016" label="Drabinski 2016">—. <title rend="quotes">WILU
                        2016</title>. <title rend="italic">Emily Drabinski</title> (blog), June 7,
                    2016. <ref target="http://www.emilydrabinski.com/wilu-2016/">
                        http://www.emilydrabinski.com/wilu-2016/</ref>.</bibl>
                <bibl xml:id="dudley2015" label="Dudley 2015">Dudley, Michael. <title rend="quotes"
                        >A Library Matter of Genocide, pt. II</title>. <title rend="italic">The
                        Decolonized Librarian</title>. September 30, 2015. <ref
                            target="https://web.archive.org/web/20190712212959/https://decolonizedlibrarian.wordpress.com/tag/library-of-congress/"
                            >https://web.archive.org/web/20190712212959/https://decolonizedlibrarian.wordpress.com/tag/library-of-congress/</ref>.</bibl>
                <bibl xml:id="eubanks2018" label="Eubanks 2018">Eubanks, Virginia. <title
                        rend="italic">Automating Inequality: How High-Tech Tools Profile, Police,
                        and Punish the Poor</title>. New York, NY: St. Martin’s Press, 2018.</bibl>
                <bibl xml:id="gillespie2015" label="Gillespie and Seaver 2015">Gillespie, Tarleton
                    and Nick Seaver. <title rend="quotes">Critical Algorithm Studies: A Reading
                        List</title>. <title rend="italic">Social Media Collective</title> (blog),
                    November 5, 2015. <ref
                        target="https://socialmediacollective.org/reading-lists/critical-algorithm-studies/"
                        >
                        https://socialmediacollective.org/reading-lists/critical-algorithm-studies/</ref>.</bibl>
                <bibl xml:id="falzetti2015" label="Falzetti 2015">Falzetti, Ashley Glassburn. <title
                        rend="quotes">Archival Absence: The Burden of History</title>. <title
                        rend="italic">Settler Colonial Studies</title> 5, no. 2 (April 2015):
                    128–44. <ref target="https://doi.org/10.1080/2201473X.2014.957258">
                        https://doi.org/10.1080/2201473X.2014.957258</ref>.</bibl>
                <bibl xml:id="foreman2018" label="Foreman et al.">Foreman, P. Gabrielle et al.
                        <title rend="quotes">Writing about <q>Slavery</q>? This Might Help</title>.
                    Accessed July 30, 2018. <ref
                        target="https://docs.google.com/document/d/1A4TEdDgYslX-hlKezLodMIM71My3KTN0zxRv0IQTOQs/mobilebasic"
                        >
                        https://docs.google.com/document/d/1A4TEdDgYslX-hlKezLodMIM71My3KTN0zxRv0IQTOQs/mobilebasic</ref>.</bibl>
                <bibl xml:id="fuentes2016" label="Fuentes 2016">Fuentes, Marisa J. <title
                        rend="italic">Dispossessed Lives: Enslaved Women, Violence, and the
                        Archive</title>. Philadelphia: University of Pennsylvania Press,
                    2016.</bibl>
                <bibl xml:id="guarino2017" label="Guarino and Borden 2017">Guarino, Cassandra M.,
                    and Victor M. H. Borden. <title rend="quotes">Faculty Service Loads and Gender:
                        Are Women Taking Care of the Academic Family?</title>
                    <title rend="italic">Research in Higher Education</title> 58, no. 6 (September
                    2017): 672–94. <ref target="https://doi.org/10.1007/s11162-017-9454-2">
                        https://doi.org/10.1007/s11162-017-9454-2</ref>.</bibl>
                <bibl xml:id="granieri2014" label="Granieri 2014">Granieri, Giuseppe. <title
                        rend="quotes">Algorithmic Culture. <q>Culture Now Has Two Audiences: People
                        and Machines.</q></title>
                    <title rend="italic">Medium</title> (blog), April 30, 2014. <ref
                        target="https://medium.com/futurists-views/algorithmic-culture-culture-now-has-two-audiences-people-and-machines-2bdaa404f643"
                        >
                        https://medium.com/futurists-views/algorithmic-culture-culture-now-has-two-audiences-people-and-machines-2bdaa404f643</ref>.</bibl>
                <bibl xml:id="grey2012" label="Grey and Hurko 2012">Grey, April and Christine R.
                    Hurko. <title rend="quotes">So You Think You’re an Expert: Keyword Searching vs.
                        Controlled Subject Headings</title>. <title rend="italic">Codex: the Journal
                        of the Louisiana Chapter of the ACRL</title> 1, no. 4 (2012) 15–26. <ref
                        target="http://journal.acrlla.org/index.php/codex/article/view/47/110">
                        http://journal.acrlla.org/index.php/codex/article/view/47/110</ref>. </bibl>
                <bibl xml:id="hathcock2016" label="Hathcock 2016">Hathcock, April. <title
                        rend="quotes">White Privilege — See Also Library of Congress</title>. <title
                        rend="italic">At The Intersection</title> (blog), November 5, 2016. <ref
                        target="https://aprilhathcock.wordpress.com/2016/11/05/white-privilege-the-library-of-congress/"
                        >
                        https://aprilhathcock.wordpress.com/2016/11/05/white-privilege-the-library-of-congress/</ref>.</bibl>
                <bibl xml:id="hepworth2018" label="Hepworth and Church 2018">Hepworth, Katherine and
                    Christopher Church. <title rend="quotes">Racism in the Machine: Visualization
                        Ethics in Digital Humanities Projects</title>. <title rend="italic">Digital
                        Humanities Quarterly </title>12 no. 4 (2018). <ref
                        target="http://www.digitalhumanities.org/dhq/vol/12/4/000408/000408.html">
                        http://www.digitalhumanities.org/dhq/vol/12/4/000408/000408.html</ref>.</bibl>
                <bibl xml:id="howard2018" label="Howard and Knowlton 2018">Howard, Sara A. and
                    Steven A. Knowlton. <title rend="quotes">Browsing Through Bias: The Library of
                        Congress Classification and Subject Headings for African American Studies
                        and LGBTQIA Studies</title>. <title rend="italic">Library Trends</title> 67,
                    no. 1 (2018): 74–88. <ref target="http://doi.org/10.1353/lib.2018.0026">
                        doi:10.1353/lib.2018.0026</ref></bibl>
                <bibl xml:id="humphrey2019" label="Humphrey 2019">Humphrey, Alex. <title
                        rend="quotes">Enabling New Methods of Discovery - Digital Preservation
                        Virtual Conference - Lawrence Livermore National Laboratory</title>. June
                    28, 2019. <ref
                        target="https://www.slideshare.net/AlexHumphreys1/enabling-new-methods-of-discovery-digital-preservation-virtual-conference-lawrence-livermore-national-laboratory"
                        >
                        https://www.slideshare.net/AlexHumphreys1/enabling-new-methods-of-discovery-digital-preservation-virtual-conference-lawrence-livermore-national-laboratory</ref></bibl>
                <bibl xml:id="introna2000" label="Introna and Nissenbaum 2000">Introna, Lucas D, and
                    Helen Nissenbaum. <title rend="quotes">Shaping the Web: Why the Politics of
                        Search Engines Matters</title>. <title rend="italic">The Information
                            Society</title> 16 (2000): 169-185. <ref target="https://doi.org/10.1080/01972240050133634">https://doi.org/10.1080/01972240050133634</ref></bibl>
                <bibl xml:id="jockers2013" label="Jockers 2013">Jockers, Matthew L. <title
                        rend="italic">Macroanalysis: Digital Methods and Literary History</title>.
                    Champaign: University of Illinois Press, 2013.</bibl>
                <bibl xml:id="johnson2018" label="Johnson2018">Johnson, Jessica Marie. <title
                        rend="quotes">Markup Bodies: Black [Life] Studies and Slavery [Death]
                        Studies at the Digital Crossroads</title>. <title rend="italic">Social
                        Text</title> 36, no. 4 (137) (December 2018): 57–79. <ref
                        target="https://doi.org/10.1215/01642472-7145658">
                        https://doi.org/10.1215/01642472-7145658</ref>.</bibl>
                <bibl xml:id="jstor2017" label="JSTOR 2017"><title rend="quotes">The JSTOR Thesuarus: Improving Discovery on the Platform</title>.
                    <title rend="italic">JSTOR</title>. October 12, 2017. <ref target="https://about.jstor.org/events/jstor-thesaurus-improving-discovery-platform/">
                        https://about.jstor.org/events/jstor-thesaurus-improving-discovery-platform/
                    </ref></bibl>
                <bibl xml:id="jstor2018" label="JSTOR 2018"><title rend="quotes">Searching Topic Cards</title>. <title rend="italic">JSTOR Support</title>.
                    June 30, 2018. <ref target="https://support.jstor.org/hc/en-us/articles/115005573368-The-JSTOR-Thesaurus-and-Topic-Cards">
                        https://support.jstor.org/hc/en-us/articles/115005573368-The-JSTOR-Thesaurus-and-Topic-Cards
                    </ref></bibl>
                <bibl xml:id="jstor2019a" label="JSTOR 2019a">JSTOR. <title rend="quotes">The JSTOR
                        Thesaurus and Topic Cards</title>. <title rend="italic">JSTOR
                        Support</title>. Accessed July 6, 2019. <ref
                        target="https://support.jstor.org/hc/en-us/articles/115005573368-Searching-Topic-Cards"
                        >
                        https://support.jstor.org/hc/en-us/articles/115005573368-Searching-Topic-Cards</ref>.</bibl>
                <bibl xml:id="jstor2019b" label="JSTOR 2019b">JSTOR. <title rend="quotes">JSTOR’s
                        Thesaurus</title>. <title rend="italic">JSTOR Guides</title>. Updated Dec
                    21, 2017. Accessed July 6, 2019. <ref
                        target="https://guides.jstor.org/c.php?g=743025&amp;p=5317792">
                        https://guides.jstor.org/c.php?g=743025&amp;p=5317792</ref>. </bibl>
                <bibl xml:id="jstor2019c" label="JSTOR 2019c"><title rend="quotes">U.S. Universities and Four-Year Colleges</title>. <title rend="italic">JSTOR Fees Overview</title>.
                    Accessed July 6, 2019. <ref target="https://www.jstor.org/librarians/fees/us-universities">https://www.jstor.org/librarians/fees/us-universities</ref>.</bibl>
                <bibl xml:id="kerber1988" label="Kerber 1988">Kerber, Linda K. <title rend="quotes"
                        >Separate Spheres, Female Worlds, Woman’s Place: The Rhetoric of Women’s
                        History</title>. <title rend="italic">The Journal of American
                        History</title> 75, no. 1 (1988): 9–39. <ref
                        target="https://doi.org/10.2307/1889653">
                        https://doi.org/10.2307/1889653</ref>.</bibl>
                <bibl xml:id="kitchin2017" label="Kitchin 2017">Kitchin, Rob. <title rend="quotes"
                        >Thinking Critically about and Researching Algorithms</title>. <title
                        rend="italic">Information, Communication &amp; Society</title> 20, no. 1
                    (2017). <ref
                        target="https://www.tandfonline.com/doi/abs/10.1080/1369118X.2016.1154087">
                        https://www.tandfonline.com/doi/abs/10.1080/1369118X.2016.1154087</ref>.</bibl>
                <bibl xml:id="knowlton2005" label="Knowlton 2005">Knowlton, Steven A. <title
                        rend="quotes">Three Decades Since <title rend="italic">Prejudices and
                            Antipathies</title>: A Study of Changes in the Library of Congress
                        Subject Headings</title>. <title rend="italic">Cataloging &amp;
                        Classification Quarterly</title> 40, no. 2 (August 2005): 123–45. <ref
                        target="https://doi.org/10.1300/J104v40n02_08">
                        https://doi.org/10.1300/J104v40n02_08</ref>.</bibl>
                <bibl xml:id="lapowsky2015" label="Lapowsky 2015">Lapowsky, Issie. <title
                        rend="quotes">Meet the Editors Fighting Racism and Sexism on
                        Wikipedia</title>. <title rend="italic">Wired</title>, March 5, 2015. <ref
                        target="https://www.wired.com/2015/03/wikipedia-sexism/">
                        https://www.wired.com/2015/03/wikipedia-sexism/</ref>.</bibl>
                <bibl xml:id="loc2019a" label="LoC 2019a">Library of Congress. <title rend="quotes"
                        >Library of Congress Classification</title>. Accessed July 2, 2019. <ref
                        target="https://www.loc.gov/catdir/cpso/lcc.html">
                        https://www.loc.gov/catdir/cpso/lcc.html</ref>.</bibl>
                <bibl xml:id="loc2019b" label="LoC 2019b">Library of Congress. <title rend="quotes"
                        >Subject Headings and Genre/Form Terms (Cataloging and Acquisitions at the
                        Library of Congress)</title>. Web page. Accessed July 2, 2019. <ref
                        target="https://www.loc.gov/aba/cataloging/subject/">
                        https://www.loc.gov/aba/cataloging/subject/</ref>.</bibl>
                <bibl xml:id="little2016" label="Little 2016">Little, Ann M. <title rend="italic"
                        >The Many Captivities of Esther Wheelwright</title>. New Haven: Yale
                    University Press, 2016.</bibl>
                <bibl xml:id="livesay2018" label="Livesay 2018">Livesay, Daniel. <title
                        rend="italic">Children of Uncertain Fortune: Mixed-Race Jamaicans in Britain
                        and the Atlantic Family, 1733-1833</title>. Chapel Hill: University of North
                    Carolina Press, 2018.</bibl>
                <bibl xml:id="manoff2015" label="Manoff 2015">Manoff, Marlene. <title rend="quotes"
                        >Human and Machine Entanglement in the Digital Archive: Academic Libraries
                        and Socio-Technical Change</title>. <title rend="italic">Portal: Libraries
                        and the Academy</title> 15, no. 3 (July 2015): 513–30. <ref
                        target="https://doi.org/10.1353/pla.2015.0033">
                        https://doi.org/10.1353/pla.2015.0033</ref>.</bibl>
                <bibl xml:id="manovich1999" label="Manovich 1999">Manovich, Lev. <title
                        rend="quotes">Database as Symbolic Form</title>. <title rend="italic"
                        >Convergence</title> 5, no. 2 (June 1, 1999): 80–99. <ref
                        target="https://doi.org/10.1177/135485659900500206">
                        https://doi.org/10.1177/135485659900500206</ref>.</bibl>
                <bibl xml:id="mckenzie2019" label="McKenzie 2019">McKenzie, Lindsay. <title
                        rend="quotes">University of California Cancels Deal with Elsevier after
                        Months of Negotiations</title>. <title rend="italic">Inside Higher
                        Ed</title> March 1, 2019. <ref
                        target="https://www.insidehighered.com/news/2019/03/01/university-california-cancels-deal-elsevier-after-months-negotiations"
                        >
                        https://www.insidehighered.com/news/2019/03/01/university-california-cancels-deal-elsevier-after-months-negotiations</ref>.</bibl>
                <bibl xml:id="mcpherson2012" label="McPherson 2012">McPherson, Tara. <title
                        rend="quotes">Why Are the Digital Humanities So White? or Thinking the
                        Histories of Race and Computation</title>. Gold, Matthew K., ed. <title
                        rend="italic">Debates in the Digital Humanities</title>. Minneapolis:
                    University of Minnesota Press, 2012. 189-160.</bibl>
                <bibl xml:id="meeks2012" label="Meeks and Weingart 2012">Meeks, Elijah and Scott B.
                    Weingart. <title rend="quotes">The Digital Humanities Contribution to Topic
                        Modeling</title>. <title rend="italic">Journal of Digital Humanities</title>
                    2, no. 1 (Winter 2012). <ref
                        target="http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/"
                        >
                        http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/</ref>
                    . </bibl>
                <bibl xml:id="millward2013" label="Millward 2013">Millward, Jessica. <title
                        rend="quotes">Charity Folks, Lost Royalty, and the Bishop family of Maryland
                        and New York</title>. <title rend="italic">The Journal of African American
                        History</title> 98, no. 1 (2013): 24–47. <ref
                        target="https://doi.org/10.5323/jafriamerhist.98.1.0024">
                        https://doi.org/10.5323/jafriamerhist.98.1.0024</ref>.</bibl>
                <bibl xml:id="morgan1997" label="Morgan 1997">Morgan, Jennifer L. <title
                        rend="quotes"><q>Some Could Suckle over Their Shoulder</q>: Male Travelers,
                        Female Bodies, and the Gendering of Racial Ideology, 1500-1770</title>.
                        <title rend="italic">The William and Mary Quarterly</title> 54, no. 1 (Jan
                    1997): 167–92. <ref
                        target="https://www.jstor.org/stable/2953316?seq=1">
                        https://www.jstor.org/stable/2953316?seq=1</ref>.</bibl>
                <bibl xml:id="niso2010" label="NISO 2010">National Information Standards
                    Organization. <title rend="italic">Guidelines for the Construction, Format, and
                        Management of Monolingual Controlled Vocabularies</title>. Approved July 25,
                    2005. (2010) <ref
                        target="https://groups.niso.org/apps/group_public/download.php/12591/z39-19-2005r2010.pdf"
                        >
                        https://groups.niso.org/apps/group_public/download.php/12591/z39-19-2005r2010.pdf</ref>.</bibl>
                <bibl xml:id="newman2006" label="Newman and Block 2006">Newman, David J., and Sharon
                    Block. <title rend="quotes">Probabilistic Topic Decomposition of an
                        Eighteenth-Century American Newspaper</title>. <title rend="italic">Journal
                        of the American Society for Information Science and Technology</title> 57,
                    no. 6 (April 2006): 753–67. <ref target="https://doi.org/10.1002/asi.20342">
                        https://doi.org/10.1002/asi.20342</ref>.</bibl>
                <bibl xml:id="neyland2017" label="Neyland and Möllers 2017">Neyland, Daniel, and
                    Norma Möllers. <title rend="quotes">Algorithmic IF … THEN Rules and the
                        Conditions and Consequences of Power</title>. <title rend="italic"
                        >Information, Communication &amp; Society</title> 20, no. 1 (January 2017):
                    45–62. <ref target="https://doi.org/10.1080/1369118X.2016.1156141">
                        https://doi.org/10.1080/1369118X.2016.1156141</ref>.</bibl>
                <bibl xml:id="noble2018" label="Noble 2018">Noble, Safiya Umoja. <title
                        rend="italic">Algorithms of Oppression: How Search Engines Reinforce
                        Racism</title>. New York: New York University Press, 2018.</bibl>
                <bibl xml:id="oneil2016" label="O’Neil 2016">O’Neil, Cathy. <title rend="italic"
                        >Weapons of Math Destruction: How Big Data Increases Inequality and
                        Threatens Democracy</title>. New York: Crown, 2016.</bibl>
                <bibl xml:id="pasquale2015" label="Pasquale 2015">Pasquale, Frank. <title
                        rend="italic">The Black Box Society: The Secret Algorithms that Control
                        Money and Information</title>. Cambridge: Harvard University Press,
                    2015.</bibl>
                <bibl xml:id="paul2016" label="Paul 2016">Paul, Kari. <title rend="quotes">Microsoft
                        Had to Suspend Its AI Chatbot After It Veered Into White Supremacy</title>.
                        <title rend="italic">Vice</title> (blog), March 24, 2016. <ref
                        target="https://www.vice.com/en_us/article/kb7zdw/microsoft-suspends-ai-chatbot-after-it-veers-into-white-supremacy-tay-and-you"
                        >
                        https://www.vice.com/en_us/article/kb7zdw/microsoft-suspends-ai-chatbot-after-it-veers-into-white-supremacy-tay-and-you</ref>.</bibl>
                <bibl xml:id="peet2016" label="Peet 2016">Peet, Lisa. <title rend="quotes">Library
                        of Congress Drops Illegal Alien Subject Heading, Provokes Backlash
                        Legislation</title>. <title rend="italic">Library Journal</title>. June 13,
                        2016.<ref
                        target="https://www.libraryjournal.com?detailStory=library-of-congress-drops-illegal-alien-subject-heading-provokes-backlash-legislation"
                        >
                        https://www.libraryjournal.com?detailStory=library-of-congress-drops-illegal-alien-subject-heading-provokes-backlash-legislation</ref>.</bibl>
                <bibl xml:id="reidsma2016" label="Reidsma2016">Reidsma, Matthew. <title
                        rend="quotes">Algorithmic Bias in Library Discovery Systems</title>. <title
                        rend="italic">Zenodo</title>, March 11, 2016. <ref
                        target="https://matthew.reidsrow.com/articles/173">
                        https://matthew.reidsrow.com/articles/173</ref>.</bibl>
                <bibl xml:id="reidsma2019" label="Reidsma 2019">—. <title rend="italic">Masked by
                        Trust: Bias in Library Discovery</title>. Sacramento, CA: Library Juice
                    Press, 2019.</bibl>
                <bibl xml:id="risam2015" label="Risam 2015">Risam, Roopika. <title rend="quotes"
                        >Beyond the Margins: Intersectionality and the Digital Humanities</title>.
                        <title rend="italic">Digital Humanities Quarterly</title> 9, no. 2
                    (September 2015). <ref
                        target="http://digitalhumanities.org/dhq/vol/9/2/000208/000208.html">
                        http://digitalhumanities.org/dhq/vol/9/2/000208/000208.html</ref>. </bibl>
                <bibl xml:id="sandvig2016" label="Sandvig et al. 2016">Sandvig, Christian, Kevin
                    Hamilton, Karrie Karahalios and Cedric Langbort. <title rend="quotes">When the
                        Algorithm Itself Is a Racist: Diagnosing Ethical Harm in the Basic
                        Components of Software</title>. <title rend="italic">International Journal
                        of Communication</title> 10 (2016), 4972-4990. <ref
                        target="https://ijoc.org/index.php/ijoc/article/view/6182/1807">
                        https://ijoc.org/index.php/ijoc/article/view/6182/1807</ref>.</bibl>
                <bibl xml:id="schwartzapfel2019" label="Schwartzapfel 2019">Schwartzapfel, Beth.
                        <title rend="quotes">Can Racist Algorithms Be Fixed?</title>
                    <title rend="italic">The Marshall Project</title>, July 1, 2019. <ref
                        target="https://www.themarshallproject.org/2019/07/01/can-racist-algorithms-be-fixed"
                        >
                        https://www.themarshallproject.org/2019/07/01/can-racist-algorithms-be-fixed</ref>.</bibl>
                <bibl xml:id="scott1986" label="Scott 1986">Scott, Joan W. <title rend="quotes"
                        >Gender: A Useful Category of Historical Analysis</title>. <title
                        rend="italic">The American Historical Review</title> 91, no. 5 (December
                    1986): 1053–75. <ref target="https://doi.org/10.2307/1864376">
                        https://doi.org/10.2307/1864376</ref>.</bibl>
                <bibl xml:id="silva2018" label="Silva and Kenney 2018">Silva, Selena and Martin
                    Kenney. <title rend="quotes">Algorithms, Platforms, and Ethnic Bias: An
                        Integretive Essay</title>. <title rend="italic">Phylon</title> 55, no.1
                    &amp; 2 (Summer/Winter 2018), 9-37. <ref target="https://www.jstor.org/stable/26545017?seq=1">https://www.jstor.org/stable/26545017?seq=1</ref>.</bibl>
                <bibl xml:id="snyder2012" label="Snyder 2012">Snyder, Terri L. <title rend="quotes"
                        >Refiguring Women in Early American History</title>. <title rend="italic"
                        >The William and Mary Quarterly</title> 69, no. 3 (July 2012): 421–50. <ref
                        target="http://www.jstor.org/stable/10.5309/willmaryquar.69.3.0421">
                        http://www.jstor.org/stable/10.5309/willmaryquar.69.3.0421</ref>.</bibl>
                <bibl xml:id="snyder2017" label="Snyder 2017">Snyder, Ron. <title rend="quotes">Under the Hood of the Text Analyzer</title>.
                    <title rend="italic">JSTOR Labs Blog</title>. March 7, 2017. <ref target="https://labs.jstor.org/blog/#!under_the_hood_of_text_analyzer">
                        https://labs.jstor.org/blog/#!under_the_hood_of_text_analyzer
                    </ref></bibl>
                <bibl xml:id="stoler2010" label="Stoler 2010">Stoler, Ann Laura. <title
                        rend="italic">Along the Archival Grain: Epistemic Anxieties and Colonial
                        Common Sense</title>. Princeton, NJ: Princeton University Press,
                    2010.</bibl>
                <bibl xml:id="ulloa2019" label="Ulloa 2019">Ulloa, Jennifer. <title rend="quotes"
                        >Algorithms Are Racist. Now What?</title>
                    <title rend="italic">Towards Data Science</title>, April 14, 2019. <ref
                        target="https://towardsdatascience.com/algorithms-are-racist-now-what-53fc130bb203"
                        >
                        https://towardsdatascience.com/algorithms-are-racist-now-what-53fc130bb203</ref></bibl>
                <bibl xml:id="uncpress2018" label="UNC Press 2018">Livesay, Daniel. <title rend="quotes">Children of Uncertain Fortune:
                Mixed-Race Jamaicans in Britain and the Atlantic Family, 1733-1833</title>. UNC Press Page.
                    <ref target="https://uncpress.org/book/9781469634432/children-of-uncertain-fortune/">https://uncpress.org/book/9781469634432/children-of-uncertain-fortune/</ref></bibl>
                <bibl xml:id="utt2013" label="Utt 2013">Utt, Jamie. <title rend="quotes">Intent vs.
                        Impact: Why Your Intentions Don’t Really Matter</title>. <title
                        rend="italic">Everyday Feminism</title>, July 30, 2013. <ref
                        target="https://everydayfeminism.com/2013/07/intentions-dont-really-matter/"
                        >
                    https://everydayfeminism.com/2013/07/intentions-dont-really-matter/</ref>.</bibl>
                <bibl xml:id="vesna2007" label="Vesna 2007">Vesna, Victoria, ed. <title
                        rend="italic">Database Aesthetics: Art in the Age of Information
                        Overflow</title>. Minneapolis: University of Minnesota Press, 2007.</bibl>
                <bibl xml:id="wachter-boettcher2017" label="Wachter-Boettcher 2017"
                    >Wachter-Boettcher, Sara. <title rend="italic">Technically Wrong: Sexist Apps,
                        Biased Algorithms, and Other Threats of Toxic Tech</title>. New York: W. W.
                    Norton &amp; Company, 2017.</bibl>
                <bibl xml:id="wagner2015" label="Wagner et al. 2015">Wagner, Claudia, David Garcia,
                    Mohsen Jadidi, and Markus Strohmaier. <title rend="quotes">It’s a Man’s
                        Wikipedia? Assessing Gender Inequality in an Online Encyclopedia</title>.
                        <title rend="italic">International AAAI Conference on Web and Social Media
                        (ICWSM2015), Oxford, May 2015</title>. <ref
                        target="http://arxiv.org/abs/1501.06307">
                        http://arxiv.org/abs/1501.06307</ref>.</bibl>
                <bibl xml:id="white2018" label="White 2018">White, Jabin. <title rend="quotes"
                        >JSTOR’s Metadata Story</title>. <title rend="italic">Metadata 2020</title>.
                    March 9, 2018. <ref
                        target="http://www.metadata2020.org/blog/2018-03-09-jstor-story/">
                        http://www.metadata2020.org/blog/2018-03-09-jstor-story/</ref>.</bibl>
                <bibl xml:id="zevallos2014" label="Zevallos 2014">Zevallos, Zuleyka. <title
                        rend="quotes">Sexism on Wikipedia: Why the #YesAllWomen Edits
                    Matter</title>. <title rend="italic">The Other Sociologist</title> (blog), June
                    7, 2014. <ref
                        target="https://othersociologist.com/2014/06/08/wikipedia-sexism-yesallwomen/"
                        >
                        https://othersociologist.com/2014/06/08/wikipedia-sexism-yesallwomen/</ref>.</bibl>
            </listBibl>
        </back>
    </text>
</TEI>
