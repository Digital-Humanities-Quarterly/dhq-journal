<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume 014 Number 3
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol/14/3/000481.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol/14/3/000481/000481.html#disqus_thread" data-disqus-identifier="000481">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">The Role of Critical Thinking in Humanities
                  Infrastructure: The Pipeline Concept with a Study of HaToRI (Hansard Topic Relevance
                  Identifier)
               </h1>
               
               
               <div class="author"><span style="color: grey">Ashley S. Lee
                     </span> &lt;<a href="mailto:ashley_lee_at_brown_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('ashley_lee_at_brown_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('ashley_lee_at_brown_dot_edu'); return false;">ashley_lee_at_brown_dot_edu</a>&gt;, Brown University
               </div>
               
               <div class="author"><span style="color: grey">Poom Chiarawongse
                     </span> &lt;<a href="mailto:t_dot_chia_at_brown_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('t_dot_chia_at_brown_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('t_dot_chia_at_brown_dot_edu'); return false;">t_dot_chia_at_brown_dot_edu</a>&gt;, Brown University
               </div>
               
               <div class="author"><span style="color: grey">Jo Guldi
                     </span> &lt;<a href="mailto:jguldi_at_mail_dot_smu_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('jguldi_at_mail_dot_smu_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('jguldi_at_mail_dot_smu_dot_edu'); return false;">jguldi_at_mail_dot_smu_dot_edu</a>&gt;, Southern Methodist University
               </div>
               
               <div class="author"><span style="color: grey">Andras Zsom
                     </span> &lt;<a href="mailto:andras_zsom_at_brown_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('andras_zsom_at_brown_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('andras_zsom_at_brown_dot_edu'); return false;">andras_zsom_at_brown_dot_edu</a>&gt;, Brown University
               </div>
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=The%20Role%20of%20Critical%20Thinking%20in%20Humanities%20Infrastructure%3A%20The%20Pipeline%20Concept%20with%20a%20Study%20of%20HaToRI%20(Hansard%20Topic%20Relevance%20Identifier)&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=014&amp;rft.issue=3&amp;rft.aulast=Lee&amp;rft.aufirst=Ashley S.&amp;rft.au=Ashley S.%20Lee&amp;rft.au=Poom%20Chiarawongse&amp;rft.au=Jo%20Guldi&amp;rft.au=Andras%20Zsom"> </span></div>
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>This article proposes the concept of the pipeline as a category of tool that
                     organizes a series of algorithms for users. The pipeline concept, adopted with
                     limitations by the humanities, documents how a suite of algorithms produces a
                     particular research result, with the goal of enabling interoperability, transparency,
                     and iteration by future scholars who may switch out particular algorithms within the
                     pipeline with different results. A pipeline-based application amplifies the concepts
                     of interoperability and transparency for users by allowing the researcher to toggle
                     on and off particular options, for example selecting and deselecting particular
                     topics of interest from a program of visualizations based on a topic model of a large
                     body of text. Pipelines support modular, interoperable, transparent, and documented
                     processes of research that lend themselves to Prof. Guldi's Theory of Critical Search
                     — the argument that critical thinking increasingly takes place at the design and
                     research stage of digital processes. The article presents the case of how the
                     pipeline concept influenced the development of <cite class="title italic">HaToRI</cite>
                     (Hansard Topic Relevance Identifier), an open-source pipeline-based tool for
                     identifying a cohort of thematically-linked passages in the nineteenth-century
                     debates of Britain's parliament. In our pipeline, a series of algorithms move through
                     the steps of cleaning a corpus, organizing them into topics, and selecting particular
                     topics that are used to extract a sub-corpus that matches the user’s interests. Users
                     have the option of searching based on multiple topics rather than merely keywords
                     or
                     a single topic at a time, allowing iterative searches to build upon each other. As
                     an
                     example of the Critical Search process in action, we follow an inquiry based on
                     matching parliamentary reports with material from the Hansard British Parliamentary
                     Debates. Using the pipeline, the user is able to identify multiple common topics of
                     interest, and from these topics, extract a sub-corpus specific to land use and rent
                     in the 19th century British Empire.
                  </p>
                  
               </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">In the era of web-enabled research, an enormous scholarly infrastructure of apps,
                     software, and research portals structures how many scholars go about their research.
                     This scholarly infrastructure comprises a significant portion of the “sites” (as
                     this issue of <cite class="title italic">Digital Humanities Quarterly</cite> put it) that
                     structure the modern meetings of minds. The design choices of that infrastructure
                     have direct consequences on research. 
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">The developer’s choices, vision, and values shape the experience of knowledge, the
                     movement of ideas, and the room for critical reflection on the interpretations
                     offered by any scholar. Choices made in developing infrastructure govern how easy
                     it
                     is for students to adapt or build on the work of their professors, for scholars to
                     make new discoveries about the same corpus, or members of the public to make new
                     discoveries in an entirely new corpus. 
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">A world of interactive applications facilitating distant reading of texts already
                     exists and many of these applications are organized in a way that allows a researcher
                     to toggle endpoint options on and off, but choices upstream in the application
                     development process are less available. Whether a piece of humanities infrastructure
                     is pipeline-like plays a role in how far the results of one study can be extrapolated
                     to another arena, for instance, how much effort is required to switch the British
                     parliamentary debates out with the Swedish parliamentary debates, given the same
                     infrastructure. This is not to say that pipelines are a panacea to interoperability
                     challenges in digital humanities — many algorithms are specific to language or
                     corpora. While pipelines allow scholars to apply methods to new data sets with little
                     coding effort, the interpretation of results is a human task that still requires
                     rigor.
                  </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">This article imports a concept (“the pipeline”) that has a specific meaning and
                     use in the field of computer science. The pipeline becomes, in our argument, a metric
                     for the kinds of online sites that digital humanists have built, which allows us to
                     ask important questions about how our infrastructure facilitates transparency,
                     exchange, and critical inspection of the results of research. Modularity,
                     interoperability, and transparency are values in pipeline development and guide how
                     we develop applications. 
                  </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">We argue that the pinnacle of pipeline-building is when insights developed in one
                     sphere can percolate to other spheres of knowledge with minimum translation — and
                     that how we develop and code our scholarly infrastructure directly determines the
                     options for collaboration, extrapolation, and interdisciplinary insight. We then
                     present a case study of a web application that exemplifies some of the values
                     inherent to a pipeline-based architecture. In a completely modular way, the
                     application, called “Hansard Topic Relevance Identifier (<a href="https://brown-ccv.github.io/hatori/page/home/" onclick="window.open('https://brown-ccv.github.io/hatori/page/home/'); return false" class="ref">HaToRI</a>)”,
                     ingests the Hansard data set and presents the results of topic modeling and
                     downstream analysis. 
                  </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Hansard is an archived and digitized resource of British Parliamentary speech going
                     back to 1803. It contains full transcriptions of all spoken words as well as speaker
                     and debate metadata. It is made up of hundreds of millions of words and the
                     digitization has very few errors. As such, it is a well-studied and valuable resource
                     for historical and socio-political research. 
                  </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">Since its inception in 2003, Latent Dirichlet Allocation (LDA) has become a popular
                     algorithm used to analyze large text corpora in Digital Humanities and beyond [<a class="ref" href="#blei2009">Blei and Lafferty 2009</a>]. LDA is a particular flavor of topic models, which more
                     broadly are probabilistic models for discovering the abstract concepts that occur
                     in
                     a large body of documents. The text is represented hierarchically, where a corpus
                     is
                     a collection of documents and in turn, a document is a collection of words. Topics
                     are distributions over a fixed vocabulary of words and documents are made up of a
                     mixture of topics. For example, an article in a newspaper could be about biology,
                     neuroscience, computational techniques, and biochemistry and another article could
                     be
                     about politics, corruption, and finance. There might be other articles about the
                     biological topic and more articles still about the political topic. The goal of topic
                     modeling is to figure out what a large corpus is about and the results can be used
                     for other text analysis tasks, like honing in on just the biological articles or
                     tracing the prevalence of the political topic over time (given the corpus contains
                     dates). 
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">The Pipeline Concept</h1>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">The concept of the “pipeline” is already prevalent in the literature of computer
                     science, where pipelines organize suites of algorithms [<a class="ref" href="#butterfield2016">Butterfield et al. 2016</a>]. In code, a pipeline describes a series of algorithms that handle repeated
                     processes — for example, data cleaning, stemming, topic modeling — in a series,
                     handling completion of the previous task before moving on to the subsequent task.
                     The
                     omnipresence of pipelines in coding today is a relatively recent feature of the
                     conditions of the available computational power over the last two decades, when
                     pipelines, originally having been developed for use in supercomputers, came to be
                     a
                     feature of all high-power processes. 
                  </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">The use of the term “pipeline” implied a series of steps, one leading on from
                     the next, describing a purpose-built architecture for complex tasks, originally
                     imagined after Henry Ford's assembly line [<a class="ref" href="#tucker2004">Tucker 2004</a>]. For our
                     purposes, the assembly line metaphor brings along with it the imagery of
                     “interchangeable parts”: that is, the ideal pipeline describes a process, and
                     a product, where most of the parts can be swapped in and out, allowing for rapid
                     fixes, upgrading, and comparison. When a pipeline is modular and open-source, if
                     users disagree with our particular choices of algorithms, they can try other ones
                     and
                     assess how the change in algorithm affects the analysis. 
                  </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">These qualities mark out a set of modern values appropriate to scholarly practice
                     in
                     a digital age, and they characterize coding practice around pipelines as a whole.
                     The
                     pipeline makes the process of going from raw text to topic modeling results modular,
                     subject to dissent, and interchangeable with different corpora. It is worth looking
                     at the three values in greater depth.
                  </div>
                  
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">By modular, we mean that an open-source pipeline insures that scholars can, at
                           all steps in the pipeline not just the endpoint, swap out choices or settings for
                           other ones. Modularity enables scholars to explore various results and arrive at
                           richer, more rigorous results. It also makes it possible to do this at little cost
                           to the researcher, in terms of time and resources needed to make and test the
                           changes.
                        </li>
                        <li class="item">By subject to dissent, we mean that scholars may take issue with any individual
                           choice made in the construction of the pipeline. In order to intellectually engage
                           the implications of their disagreement on the research process, scholars must be
                           able to recreate alternative results that would have happened had one of the
                           algorithms in the pipeline been switched out — for instance, had a different
                           choice of stemmer or topic modeling algorithm been employed. In this process of
                           openness to dissent, the pipeline becomes a tool for exploring best practices in
                           digital research, where scholarly disagreements about tools and interpretation can
                           be pinpointed for discussion and turned into instructive examples. 
                        </li>
                        <li class="item">By interchangeable, we mean that once an open-source pipeline is deemed useful
                           for humanities and social science knowledge, other scholars may want to apply the
                           insights from a research process to other bodies of text. Where no scholarly
                           process exists for packaging tools into a pipeline, other scholars inspired by a
                           research process must recreate the process from scratch. Where research is
                           packaged as a pipeline, however, switching a few lines of code makes it possible
                           for the scholar to apply one pipeline from the British Hansard debates to the
                           Canadian or Australian Parliamentary debates, or any other text corpora.
                           Interchangeability is a scholarly value because it enables the extension of
                           insights from one domain of humanities or social science research into another
                           domain.
                        </li>
                     </ul>
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Previous Waves of Critical Thinking about Humanities Infrastructure</h1>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">A previous wave of efforts to bring structure to digital humanities research has
                     foregrounded humanities infrastructure as a site of making. This literature has dealt
                     with both the physical and organizational infrastructure of research [<a class="ref" href="#svensson2011">Svensson 2011</a>], and data infrastructures [<a class="ref" href="#edmond2015">Edmond et al. 2015</a>]
                     [<a class="ref" href="#brooke2015">Brooke et al. 2015</a>]. Brooke’s GutenTag, for example, is a software that
                     samples sub-corpora from the Gutenberg corpus based on publication dates, gender of
                     the author, etc., and adds user-selected tags to the selection. Some, but not all
                     of
                     this research has suggested a formal set of values for analysis. Mattern argues that
                     whether infrastructure is hard (roads, railways, bridges, data centers, fiber-optic
                     cables, e-waste handlers, etc.) or soft (measurement standards, technical protocols,
                     naming conventions, etc.), digital humanists among other interfacers with
                     infrastructure should have some literacy around them, including the values that
                     underlie them including transparency and modularity. Matterns also argues that
                     infrastructure is not a revolutionary concept (though it has evolved since its
                     inception in the 1920’s), but allows people to “organize into
                     communities and share resources amongst themselves.” It is this
                     self-organizing quality that necessitates better interfaces that reflect how
                     communities want to and should interact with structures [<a class="ref" href="#mattern2016">Mattern 2016</a>].
                  </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">Discussions on the biases in the process of big data research can be found in the
                     literature of the past decade. A primary example is the idea that racist data leads
                     to racist algorithms. Noble challenges the idea that scientific research is
                     impartial, that it levels the playing field for “all forms of
                     ideas, identities, and activities”
                     [<a class="ref" href="#noble2018">Noble 2018</a>]. Negative biases are embedded in search engine results for
                     “black girls,” where suggested searches are radically different from those
                     for “white girls.” This results in biased search algorithms that privilege
                     whiteness and oppress people of color. There is an increasing awareness of this issue
                     - O’Neil et al. lay out situations where algorithms have the potential to amplify
                     the
                     biases and exacerbate the disparities present in society. Arenas such as going to
                     college, online advertising, justice and law enforcement, getting a job and job
                     performance, getting credit, and getting insurance can all be dangerously affected
                     by
                     algorithms’ increasing role in the decision making process [<a class="ref" href="#dallessandro2016">d’Alessandro et al. 2016</a>]. O’Neil calls for auditing in all steps of the data
                     science development process [<a class="ref" href="#oneil2016">O’Neill 2016</a>]. Additional research in the
                     field of bias, discrimination, and oppression in algorithms and what to do about them
                     are abundant [<a class="ref" href="#eubanks2015">Eubanks 2015</a>]
                     [<a class="ref" href="#introna2006">Introna and Nissenbaum 2006</a>]
                     [<a class="ref" href="#nissenbaum2010">Nissenbaum 2010</a>]
                     [<a class="ref" href="#vaidhyanathan2012">Vaidhyanathan 2012</a>]
                     [<a class="ref" href="#vaidhyanathan2018">Vaidhyanathan 2018</a>]. Kaplan, in his map of big data research in
                     digital humanities, briefly raises the question of bias and notes that choices need
                     to be made in the process of digitally translating from primary sources into
                     high-level human-processible insights and that the inevitable biases that result from
                     these choices apply [<a class="ref" href="#kaplan2015">Kaplan 2015</a>]. We support a solution, not by
                     codifying any particular set of standards, but by allowing these choices to be
                     questioned in all steps in the pipeline, and changed directly by the questioner. 
                  </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">As a result, the pipeline values of modularity, subjectness to dissent, and
                     interchangeability is apparent in many of the web portals available today — but not
                     in all. Perhaps the work that most closely mirrors our viewpoint is Edmond’s Data
                     Soup, which describes an architecture and pipeline for text search and selection used
                     in the [<a class="ref" href="#edmond2013">Edmond 2013</a>] project with an emphasis on modularity of the
                     processing algorithms and reproducibility. These works, however, deal mainly with
                     the
                     process of data preparation and selection, rather than the whole process of research,
                     all the while treating it primarily as something to be done, rather than something
                     to
                     think about, discuss, and debate. Often times, in Digital Humanities, the description
                     of the process is sidelined to the appendix or not discussed at all, which precludes
                     any possibility of critical review of the pipeline. In fact, in a survey of close
                     and
                     distant reading techniques, [<a class="ref" href="#janicke2015">Janicke et al. 2015</a>] found that many papers using
                     sophisticated data analysis techniques do not even provide sufficient information
                     about the preprocessing steps to be reproducible, let alone facilitate discussions
                     on
                     these processes. We argue that it is not enough for the data analysis tool to be
                     sophisticated and user-friendly, but that all steps in the data analysis pipeline
                     be
                     truly transparent and modular. Mattern argues for a collective consciousness of
                     “citizens/users” of infrastructure around both
                     awareness of and critical listening and thinking around infrastructure, implicitly
                     making an argument for these values [<a class="ref" href="#mattern2013">Mattern 2013</a>]. 
                  </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">Mattern references “path dependency,” a concept coined by
                     Edwards et al., where past decisions limit future choices. While this is true of
                     software, open source software gets around this dependency because the code is shared
                     freely, with any infinite number of branches or forks possible from the original base
                     code. If a scholar or user dislikes a past decision, they can change it so that
                     future choices are not limited by a previous developer’s choices. Thus, the software
                     not only is transparent and modular and subject to dissent, but allows scholars and
                     users to go beyond dissent and towards action and agency to diverge from the
                     path.
                  </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">It is advantageous to the community of researchers to discuss ideas about
                     infrastructural choices built into a particular pipeline. If we do collectively adopt
                     the convention of publishing articles wherein we describe the pipelines we have
                     designed — and perhaps other articles wherein we critique recent conventions in
                     building infrastructure — then the community of knowledge will benefit by
                     collectively learning. The author can describe the work that went into designing a
                     pipeline that maximizes critical thinking. Places in the research project where the
                     best algorithm is uncertain — the places that lend themselves to critical review —
                     can be described and highlighted, and the community of researchers can grow in its
                     awareness of these uncertainties and the biases they create on the research project.
                     Herein lies a major opportunity for collective critique and argumentation around the
                     ideas that structure our infrastructure.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">The Pipeline-Based Web App</h1>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16"> In most web applications in the digital humanities, some pipeline exists in the
                     background, but some of its functions are obscured from the user. The visualizations
                     displayed are based on the results of cleaning and topic modeling run behind the
                     scenes and then uploaded to the web app for the researcher to interact with. Simple
                     navigation by date or keyword allows the user to expand or explore particular aspects
                     of the results. Toggle options exist, but they are limited to later parts in the
                     pipeline. For example, in the VoyantTools web application, we can upload any text
                     we
                     want and toggle visualization settings, but there are no options for how the text
                     is
                     cleaned and tokenized and furthermore, the way these normalization steps are done
                     is
                     obscured from the user [<a class="ref" href="#sinclair2016">Sinclair and Rockwell 2016</a>]. InPhO Topic Viewer, one of the
                     HathiTrust Research Center algorithms, is a more modular pipeline than VoyantTools
                     because it provides some options in the text pre-processing and normalization parts
                     of the pipeline. However, the only two options are the choice of tokenizer (of which
                     there are only two useful selections for English texts) and whether or not to decode
                     unicode characters [<a class="ref" href="#murdock2017">Murdock et al. 2017</a>]. 
                  </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">In web applications inspired by the pipeline concept, the designers seek to highlight
                     and expand particular choices made in all steps of the design of the pipeline, giving
                     the user options for how the data is handled and presented. In an ideal
                     pipeline-based app, the user would have control over every process in the pipeline,
                     from choosing different stemming and cleaning options to visualizations, allowing
                     the
                     community to upload different possible algorithms. 
                  </div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">By giving users the option of reviewing the choices behind any given analysis and
                     visualization, the design principle of the pipeline-based app reinforces the
                     possibility of scholarly dissent in analyzing and interpreting documents. Two
                     scholars may have different instincts when it comes to how to clean the data, which
                     topics are relevant to a query, or how to visualize the data, and these different
                     instincts may lead them in different ways. To encourage a healthy climate of dissent
                     and investigation, practitioners in the digital humanities need to incorporate design
                     opportunities for dissent and counter-inspection of the evidence into the full
                     pipeline of their software. 
                  </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19"> In a sense, then, valuing scholarly dissent means that interface designers must
                     aspire to web-apps that perform like <em class="emph">code</em> in terms of their flexibility
                     and interoperability, making every choice of algorithm transparent and
                     interchangeable. The ideal pipeline-based application would, in a sense, lead the
                     user through the experience of coding, where the coder typically chooses various
                     packages that are assembled into a process of cleaning, analyzing, and visualization.
                     One example of a pipeline-based tool is Jean-Philippe Cointet’s Cortext,<a class="noteRef" href="#d4e427">[1]</a> a
                     platform that allows users to upload textual documents and queue them for cleaning
                     and various transformations including named entity recognition, topic modeling, and
                     vector analysis [<a class="ref" href="#rule2015">Rule et al. 2015</a>].
                  </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">Pure pipeline-based tools have both advantages and disadvantages. The advantage, like
                     the advantage of the pipeline itself, is the proliferation of the values of
                     modularity, openness to dissent, and interoperability. Pipeline-based tools like
                     Cortext recreate the experience of interactive, iterative coding with data for
                     non-coders and mixed classrooms or readerships where not all who want to experience
                     the data come from an equal background with respect to code. 
                  </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">The major disadvantage of existing pipeline-based tools are, generally speaking,
                     features of first-wave design. For instance, some web interfaces designed by computer
                     specialists often lack an eye for design principles that would make their use
                     transparent to users, which results in interfaces that are cumbersome to use, for
                     instance in the case of Zotero. Zotero is an application for curating, storing, and
                     organizing journal articles, but it is possible to build text mining and
                     visualization on top of it. With Zotero, a user can add or remove journal articles
                     from their hand-curated collection, topic model their corpus, and view some results.
                     Changing the inputs is trivial, but changing the parameters in modeling and
                     visualization are not. In Zotero, the emphasis is on interoperability, not
                     user-friendliness, with the result that some users may be put off an artificially
                     abstract interface. Other web interfaces command limited access to computational
                     resources, which results in the case of Cortext in long waiting times while users
                     queue for a server. These are issues that could be remedied by later design and
                     improvement. However, for designers to rebuild existing infrastructure to align with
                     new principles, they would need access to grants that emphasize extending access to
                     computational resources and usability. 
                  </div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">Tool-builders who aspire to pipeline-level transparency and easy user interface may
                     nonetheless adopt the principles of openness to dissent and interchangeability in
                     certain <em class="emph">parts</em> of the research process, with the result of creating web
                     interfaces that are more flexible, generative, and useful for scholarly debate than
                     the first generation of web applications in the digital humanities. This article
                     presents one such iteration, HaToRI, which aspires to the pipeline concept. It
                     realizes this promise by transparency — that is, by documenting the background
                     pipeline of its code for users — and by limited modularity, that is, allowing the
                     user to choose different seed topics and tweak the z-value of our sorting algorithm.
                     Transparency and open-source software give users options: if they want to change
                     other parts of the pipeline, like for example lemmatizing rather than stemming, the
                     code should be run again but with that parameter changed. 
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Introducing HaToRI, a Pipeline for modeling the British Parliamentary
                     Debates
                  </h1>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">HaToRI is the Hansard instance of the “ToRI” (Topic Relevance Identifier) tool.
                     Its creation was motivated by the research questions asked of the Hansard British
                     Parliamentary Debates data set by our scholarly collaborator, but it was built with
                     a
                     generic text analysis pipeline in mind. ToRI exemplifies the values of the pipeline
                     concept in various ways. At the highest level, it is applicable to any text corpus
                     with trivial customizations. For example, if a user likes all of the visualizations
                     and features available in the HaToRI app but wants to create an instance of ToRI
                     analyzing Twitter data, we can spin up a “TweetToRI” web app with slight
                     modifications to the underlying ToRI code pipeline. 
                  </div>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">The code pipeline consists of three steps: we prepare and identify the topics of a
                     corpus, a humanist selects topic(s) of interest, and we post-process the results.
                     Post-processing consists of ranking the documents based on how prevalent the topic(s)
                     of interest are, and using such a ranking for in-depth studies. We prepare a set of
                     visualizations to illustrate how the topics cluster based on similarity, how their
                     prevalence changed over time, etc. The choices that we made in each of the three
                     steps were informed by the historical research aims and by the content and format
                     of
                     the Hansard data, but may not be the right choices for all data. They can be adjusted
                     with ease for other data or research aims. For this reason, we believe that the
                     pipeline driven web tool can greatly aid humanists in their research.
                  </div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">The ToRI web app takes inspiration from many other useful Natural Language Processing
                     (NLP) tools, most notably the Topic Explorer [<a class="ref" href="#goldstone2014">Goldstone and Underwood 2014</a>], but is
                     distinguishable in its highly customizable end-to-end pipeline and novel document
                     ranking algorithm. Many tools, like VoyantTools and OldBaileyVoices.org, are limited
                     to basic keyword-based text analysis and corpus exploration [<a class="ref" href="#sinclair2016">Sinclair and Rockwell 2016</a>]. And while VoyantTools can accommodate various corpora,
                     it does not accept a tab-separated value format that is ideal for corpora like
                     Hansard that contain structured fields. Other tools explore and extract semantic
                     sub-corpora by ranking documents [<a class="ref" href="#tangherlini2013">Tangherlini and Leonard 2013</a>]
                     [<a class="ref" href="#goldstone2014">Goldstone and Underwood 2014</a>], but use similarity to a single document or topic as
                     the sorting value. Lengthy documents, in particular, can contain too much noise and
                     a
                     single topic does not capture the nuance that multiple topics can, resulting in a
                     sub-corpus that potentially contains many false positives and leaves out harder to
                     detect true positives. Meandre is a workflow tool that is modular and customizable
                     to
                     many corpora and offers a drag-and-drop interface for non-technical users to perform
                     many NLP tasks including data cleaning, topic modeling, sentiment analysis, etc. [<a class="ref" href="#llora2008">Llora et al. 2008</a>]. While it is user-friendly and feature-rich, it does not
                     offer sub-corpus extraction using multiple topics.
                  </div>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">The advantages of our pipeline over other corpus exploration tools are that they are
                     self-contained and complete, modular, and open source. These qualities make the code
                     generalizable to any corpus, in any format and highly adaptable to incorporate other
                     algorithms or NLP decisions. Specifically, if a user does not agree with our choice
                     of the stemming method (Snowball Stemmer), they can add a different algorithm (e.g.,
                     the Porter Stemmer) by changing two lines of code and then assess how the change
                     influences the results.
                  </div>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">Creating instances of the web app with other corpora requires the addition of a
                     single, custom pre-processing script that converts the corpus to our generic data
                     format. The generic format is a tab-separated file with three fields: <em class="term">Document
                        ID</em>, <em class="term">Text</em>, and <em class="term">Year</em>. The <em class="term">Document ID</em>
                     can be any identifier such as a sequence of integers or document titles if available
                     and it needs to uniquely identify the documents. The <em class="term">Text</em> field contains
                     the full, natural text of the documents, prior to cleaning and stemming. Then, the
                     rest of the pipeline can run without any modifications. The pipeline performs the
                     complete set of tasks needed to transform natural language to machine-readable
                     vectors, a noisy collection of words to a hierarchically sorted set of topics, and
                     an
                     impossibly large volume of text to a manageable sub-corpus of semantically relevant
                     documents. 
                  </div>
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">Additionally, the code is modular so the work required to make changes to the
                     pipeline is trivial. The decisions we made at all points in the pipeline — such as
                     our custom stopword list, our spell-checking dictionary, our decision to stem rather
                     than lemmatize, topic modeling method and implementation, number of topics, document
                     ranking algorithm, and z-value — can be customized to different corpora or user
                     interests. The code is designed this way to make space for flexibility in
                     decision-making; algorithms best suited for one corpus or domain may not be optimal
                     for another corpus or domain. Furthermore, some of these decisions are built into
                     the
                     endpoint of the pipeline, the web app user interface, with no code changes required.
                     For example, the z-value is a tune-able parameter that the user interacts with in
                     a
                     value entry bar on the detailed topic view pages of the web app. Future work will
                     incorporate more user input at the endpoint. <a class="noteRef" href="#d4e502">[2]</a></div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Comparing Applications: How Pipelines Broaden the Scholarly Implications of
                     Research 
                  </h1>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">The Hansard parliamentary debates are important information mines for scholars of
                     British history and attempts to digitize Hansard date back to at least the 1970s.
                     Open source versions of the nineteenth-century Hansard corpus have been freely
                     available in a digitized version for at least a decade, which has made them the
                     subject of a great deal of infrastructure design already. At least two applications
                     allow users access to the debates today — the Hansard Corpus site, designed by a team
                     of linguists at Glasgow and Brigham Young, and the Millbank Hansard site,
                     commissioned by parliament itself of a private developer. The two existing
                     applications have already enabled a flood of new research about language use in
                     parliament [<a class="ref" href="#blaxill2013">Blaxill 2013</a>]
                     [<a class="ref" href="#alexander2017">Alexander and Struan 2017</a>]. 
                  </div>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">However, the first generation of applications made little use of the pipeline
                     concept. In many cases, little documentation was available about how the corpus had
                     been cleaned. Users were allowed to search by keyword, year, and (in some cases)
                     speaker or other named entity, but few tools offered other ways of navigating the
                     corpus or matching a user’s interests. Hansard, thus, offers an ideal test-case for
                     building a web application that models the advantages of modularity, openness to
                     dissent, and interoperability, due, in part, to the heavy scholarly interest in the
                     subject matter; the material covered by Hansard is the subject of interest in
                     history, literature, linguistics, and geography. Moreover, an interoperable pipeline
                     or pipeline-based app for working with the British Hansards could be easily extended
                     to the Australian, Irish, and Canadian Hansards, the debates of the EU parliament,
                     or
                     any other digital corpus that represents the debates of a democratic body.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Interoperability: How to Pipe HaToRI to Other Corpora</h1>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">Unlike applications that readily ingest text data in the web browser without ever
                     having to touch the underlying code, HaToRI requires some programming knowledge to
                     adapt it to other corpora. The advantage of this approach is that the corpus can have
                     associated structure and metadata beyond a single document of plain text.
                     Additionally, minor code changes can be made in the process of spinning up another
                     instance of ToRI, making the intermediary steps between data ingest and data
                     visualization completely modular. To spin up an instance of ToRI for another corpus,
                     clone the Inquiry for Philologic Analysis repository from Github and make
                     modifications to the code in the <em class="term">src</em> folder.
                  </div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">The pipeline is set up in <span class="monospace">main.py</span>. Each step in the pipeline can be
                     toggled on or off by setting the switches at the top of the script. For example, to
                     run the first step of the pipeline on the test data given, the user can toggle the
                     <span class="monospace">to_tsv</span> switch to “True”, and the sample 10 Hansard data will be
                     converted to a tabular, tab-separated value (TSV) file. To ingest another corpus,
                     the
                     user must first convert the raw text to a TSV file. The TSV must represent a corpus
                     as one row per document, and each document must have a unique identifier associated
                     with it. The identifier is the first column in the TSV file and the text of the
                     documents should be the second column. Metadata is optional - for HaToRI the speaker
                     name and debate year were added as metadata by appending them as columns three and
                     four. This step is unique to each data set - for Hansard we wrote a code
                     (<span class="monospace">raw_corpus2tsv.py</span>) to parse the data we needed from XML to TSV. For
                     other corpora, the code will need to be custom written for the format it originates
                     in.
                  </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">The next step, data cleaning, is a single script (<span class="monospace">preprocess.py</span>) that
                     can be run as is, changed with minor code edits to accommodate different scholarly
                     choices or domain-specific corpora, or replaced by a custom preprocessing script
                     entirely. Tokenization is by word, but can be changed to ngrams (n-length word
                     phrases). Non-alphanumeric characters are removed, but perhaps punctuation and
                     special characters are important, such as with a Twitter data set. All text is
                     lower-cased, though with texts with many abbreviations like medical notes,
                     capitalization could be preserved. Spell-checking can be turned from on to off, and
                     word truncating can be changed from stemming to lemmatization, or turned off
                     completely. Common as well as custom stopwords are removed, and either or both
                     stopwords lists can be turned off or modified. Once cleaning choices have been made
                     and run, the user must download the MALLET program and run topic modeling with the
                     desired numbers of topics, k. The output of the topic models are then ready to be
                     loaded into the website. 
                  </div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">There are two steps to creating the website: uploading the corpus and putting the
                     website online. The user should clone the <em class="term">hatori</em> repository from Github
                     and change out the data in the <em class="term">serv/data/</em> folder with their own corpus.
                     Then, once the website works locally, they should host the website using one of a
                     variety of web hosting services. We suggest doing this through Github Pages<a class="noteRef" href="#d4e578">[3]</a>
                     because of its integration with Github.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Methods: the Pipeline Outlined</h1>
                  
                  <div class="counter"><a href="#p35">35</a></div>
                  <div class="ptext" id="p35">The remainder of this paper describes a full NLP pipeline for sub-corpus extraction
                     using topic-based search and tests the hypothesis that topics are better than
                     keywords for discovering a thematically coherent sub-corpus that includes
                     under-studied documents. We will describe the pipeline’s machinery in detail, which
                     includes custom data extraction and pre-processing, conversion of the text corpus
                     to
                     a numeric and machine-readable data structure, topic modeling the corpus, document
                     ranking and sub-corpus extraction, and visualization in a web app. We give an example
                     scholarly use case to show how features of the web app work, and how the pipeline
                     enables reproducible and interoperable digital humanities research and openness to
                     dissent as a way of collaboration.
                  </div>
                  
                  <div class="counter"><a href="#p36">36</a></div>
                  <div class="ptext" id="p36">In characterizing the pipeline, we give a transparent description of the entire
                     research project from raw text file (in this case; in others, it might be scanned
                     documents) to visualizations. In emulating the value of transparency, this pipeline
                     description highlights the opportunities for critical thinking along the way and how
                     scholars can engage them. Indeed, we believe that this kind of description, which
                     both illustrates the facts about the corpus and highlights the places for
                     interpretive work by scholar-users, should become the standard for scholarly
                     publications that document new scholarly infrastructures in the humanities. 
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Pre-processing</h2>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">Here, we describe the process by which we turned the raw Hansard parliamentary
                        debates into machine-readable material for downstream steps in the pipeline. The
                        process described contains choices specific to the Hansard dataset, but it is
                        modular and easily interchanged with a different suite of pre-processing
                        steps.
                     </div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">We download Hansard from the web as a series of XML files<a class="noteRef" href="#d4e605">[4]</a> and parse full text
                        and metadata from the XML into a tabular, tab-separated value (TSV) format using
                        the Python programming language. The input data could be any collection of
                        documents (news articles, tweets, political speech, etc.) in any text format
                        (cannot be images or scans of documents), so long as it is converted into a TSV
                        structured such that each row is a document. In the Hansard TSV file, each row is
                        a debate and the columns are <em class="term">debate ID</em>, <em class="term">full text</em>, and
                        <em class="term">metadata</em>. We additionally append four written reports crucial to
                        the historical analysis to our corpus. The result is a corpus made up of 45,585
                        speakers who uttered 294,203,233 words, 1,033,536 speech acts, and 111,689
                        debates. 
                     </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Figure 1 shows what parliamentary speech and the written reports look like over
                        time. The number of debates per year correlates closely with the number of
                        speakers per year, and they both increase over the course of the century with a
                        notable spike in the last quarter century. The average length of our documents
                        shows a weak inverse correlation. The number of speech acts and the number of
                        unigrams per debate are both somewhat reduced in the last quarter of the century.
                        In other words, as the number of debates and number of speakers increase, the
                        speech acts tend to get less verbose over time.
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>The number of debates and the number of speakers stayed relatively constant
                           until the last quarter of the century when there was a sharp increase in both.
                           While there are more debates and more speakers, the speakers were less verbose
                           during the last quarter century.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">As is standard in Natural Language Processing, we took steps to pre-process the
                        corpus to prepare the text for downstream analysis. These steps include
                        tokenization, removing non-alphanumeric characters, lowercasing, spell-checking,
                        replacing both common stopwords and custom stopwords with a substitute word, and
                        stemming. We walk through each step in detail during the rest of this subsection.
                        
                     </div>
                     
                     <div class="ptext">
                        <ul class="list">
                           <li class="item">We lowercase the corpus and then tokenize the documents by splitting the
                              text on whitespace (e.g. spaces, tabs, returns). There are a variety of
                              tokenizers to choose from (e.g. Penn Treebank, Punkt, Multi-Word Expression,
                              Tweet, Regular Expressions, etc.) and we chose whitespace tokenization based on
                              the structure of Hansard. This reduces our unit of text from paragraphs to
                              words. Our corpus, Hansard, consists of 198,338 unique words.
                           </li>
                           <li class="item">We remove non-alphanumeric characters from each word, so they are free from
                              punctuation, white space, and symbols. This reduces the vocabulary to 186,060
                              words. Other text corpora may require the removal of different characters, like
                              emoji from tweets or html tags from data scraped from the web. Changes to the
                              removal list are trivial to implement in the code pipeline. 
                           </li>
                           <li class="item">We spellcheck the words using a dictionary of British English words provided
                              by the Python library, <cite class="title italic">enchant</cite>. If the words in
                              our vocabulary are in the dictionary, we keep the word; if they are not, we
                              replace them with a substitute word that does not appear natively in our
                              corpus. This reduces the vocabulary to 49,789 words. We also replace common
                              stopwords (e.g. articles and conjunctions like the, and, or, it, etc. that have
                              no semantic meaning) and custom stopwords (proper nouns) with a substitute
                              word. Replacing stopwords reduces our vocabulary by less than one thousand
                              words. We can, with minimal effort, swap out the British English dictionary for
                              other language dictionaries or change the stopwords used.
                           </li>
                           <li class="item">We stem the words using Snowball Stemmer in the <cite class="title italic">NLTK</cite> Python library. Stemming further decreases the size of the
                              vocabulary by reducing inflected and derived words to their stem (base or root
                              form) to avoid duplicate counting words that may have slightly different
                              endings but the same semantic meaning. “Rent”, “rents”,
                              “rented”, and “renting” would all be reduced to the stem,
                              “rent”, and “property” and “properties” would both be reduced to
                              the stem, “properti”. We call the stemmed words unigrams because stems
                              like “properti” are not words. There are a number of different stemmers
                              and lemmatizers to choose from, should the Snowball Stemmer be called into
                              question.
                           </li>
                        </ul>
                     </div>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">By the end of the process, the original 294 million words are reduced to 97
                        million. Almost 200 million words are replaced by the substitute word because they
                        are common stopwords (for example: “a”, “an”, “the”, “I”,
                        “you”) or misspelled words. While the reduction seems significant,
                        substitute words would contribute very little to the topic modeling results and
                        thus minimal information is lost during the process. The final step in
                        pre-processing is to create a document-term matrix, which is a numerical
                        representation of the corpus that can be read and analyzed by a machine. Each
                        document is represented by a vector of word counts. Different weights and
                        normalizations can be applied to the document-term matrix, given the needs of the
                        data or research.
                     </div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell">Pre-processing Step</td>
                              
                              <td valign="top" class="cell">Number of unigrams</td>
                              
                              <td valign="top" class="cell">Number of non-substitute unigrams</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">All unigrams</td>
                              
                              <td valign="top" class="cell">198,338</td>
                              
                              <td valign="top" class="cell">196,642</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Alphanumeric characters only</td>
                              
                              <td valign="top" class="cell">186,060</td>
                              
                              <td valign="top" class="cell">184,447</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Correctly spelled unigrams</td>
                              
                              <td valign="top" class="cell">49,789</td>
                              
                              <td valign="top" class="cell">49,190</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">Stems</td>
                              
                              <td valign="top" class="cell">21,828</td>
                              
                              <td valign="top" class="cell">21,444</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>We illustrate how the various pre-processing steps reduces the number of
                           unique unigrams in Hansard by a factor of 10. We get the greatest reduction in
                           unique unigrams by spell-checking and stemming words.
                        </div>
                     </div>
                     
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Topic Modeling</h2>
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">The goal of applying NLP methods to Hansard is to answer historical questions
                        about the shifting concepts of land ownership and property in the
                        nineteenth-century British empire. Traditional historical scholarship involving
                        close reading and analysis is not suited to this task because the text corpus is
                        too large to be read by a single historian or even a team of researchers. Instead,
                        we offload the task to a computer.
                     </div>
                     
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">Topic models are probabilistic models for discovering the abstract topics that
                        occur in a large corpus of documents through a hierarchical analysis of the text
                        [<a class="ref" href="#blei2009">Blei and Lafferty 2009</a>]. The idea is that a corpus is made up of topics,
                        topics are distributions over a fixed vocabulary of terms, and documents are
                        mixtures of topics in different proportions. The vocabulary of terms can be words
                        or pre-processed tokens like unigrams or n-grams. Topic modeling can answer
                        questions that would otherwise be intractable with a large corpus: (1.) What is a
                        large, prohibitively long corpus of documents about? (2.) Can we extract a
                        human-readable sub-corpus of semantically-related documents? A random process is
                        assumed to have produced the observed data, which are the words that make up the
                        documents in the corpus. Given the observed data, the posterior distribution of
                        the hidden variables (word distributions for each topic, topic proportions for
                        each document, topic assignment per word for each document) determines the hidden
                        topical structure of the corpus which tells us what the corpus is about, answering
                        the first question we seek to solve with topic modeling; the posterior estimates
                        can be applied to tasks such as document browsing and information retrieval,
                        answering our second question. In fact, topic modeling has been used as the basis
                        for “trawling” for a smaller corpus [<a class="ref" href="#tangherlini2013">Tangherlini and Leonard 2013</a>].
                     </div>
                     
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">The evaluation of topic modeling results is subjective, requiring manual
                        inspection by subject matter experts and cannot replace the traditional textual
                        analysis. The benefit, however, is that the subject matter experts need only to
                        read through a handful of terms per topic rather than a significant part of the
                        corpus. The drawback is that we lose vital information encoded in natural language
                        when we numericize it. For example, the bag-of-words representation of a corpus
                        loses context entirely by treating documents as unordered collections of words
                        [<a class="ref" href="#harris1954">Harris 1954</a>]; n-grams cannot preserve word relationships past
                        their immediate neighbors [<a class="ref" href="#manning1999">Manning and Schutze 1999</a>]; and while vector models
                        like word2vec and doc2vec preserve far-apart word relationships, they are not
                        suitable for identifying a relevant sub-corpus within a large corpus [<a class="ref" href="#mikolov2013">Mikolov et al. 2013</a>]
                        [<a class="ref" href="#le2014">Le and Mikolov 2014</a>]. 
                     </div>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">We use a particular flavor of topic modeling called Latent Dirichlet Allocation
                        (LDA) [<a class="ref" href="#blei2003">Blei et al. 2003</a>]. The model does not have a prior notion about the
                        existence of the topics; it is given a hyperparameter, k, which describes how many
                        topics are associated with the corpus, and discovers the k topics from the
                        observed data, the words in the documents. The algorithm is implemented in a
                        number of programming languages and is simple to use with few tunable parameters,
                        but requires hand labeling of topics by subject matter experts. We use a Java
                        implementation of LDA called MAchine Learning for LanguagE Toolkit (MALLET) [<a class="ref" href="#mccallum2002">McCallum 2002</a>]. In developing the pipeline, we made topic models with
                        varying k values (50, 100, 200, 500, 1000) and different topic modeling methods
                        like Non-negative Matrix Factorization [<a class="ref" href="#berry2005">Berry and Browne 2005</a>] and Dynamic
                        Topic Models [<a class="ref" href="#blei2006">Blei and Lafferty 2006</a>] available to the users.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Extracting a Sub-corpus</h2>
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46">Within <cite class="title italic">HaToRI</cite>, the process of extracting a subcorpus
                        starts with LDA. LDA not only tells us the topics that are discussed in a corpus,
                        but also which ones each document contains. Knowing what documents are about
                        allows us to narrow our focus to a smaller sub-corpus from the whole of Hansard,
                        reading the debates and speech acts that are about our topic of interest — land
                        and property. Sub-corpus extraction using topic modeling allows us to find a
                        richer set of documents than keyword search alone because the topics are
                        discovered from the data, rather than imposing domain-specific and subjective
                        knowledge on the text. Keyword searches and term frequencies work well when the
                        topic of interest can be unambiguously identified by one word or a short phrase.
                        Documents in a corpus can be clustered based on similarity to one another and
                        subject matter experts need to analyze each cluster to decide if the clusters make
                        semantic sense and if any are about their topic of interest. This can only be done
                        if the corpus is not too large and the experts can at the very least scan through
                        each document. Sometimes it is already known that one or a few documents in a
                        corpus are about the topic of interest and the question is whether there are other
                        so far undiscovered relevant documents in the corpus? The goal then is to find
                        documents which are similar to the seed documents. A search based on seed
                        documents works if most words and terms in the seeds are about the topic of
                        interest and not other general topics. This is not often the case. Our proposed
                        method for sub-corpus extraction is a topic-model-based document ranking process
                        where users choose multiple topics of interest, rank documents by how closely they
                        match the topics of interest, explore the results, and iterate by toggling on or
                        off particular topics. We harness a code pipeline to build a highly customizable
                        web-app that makes available to the ordinary user a process of Critical Search
                        characterized by iterative interaction with large corpora [<a class="ref" href="#guldi2018">Guldi 2018</a>]. 
                     </div>
                     
                     <div class="counter"><a href="#p47">47</a></div>
                     <div class="ptext" id="p47">We use three steps in topic modeling: import, modeling, and post-processing. The
                        first two steps are fairly straightforward parts of the MALLET program. In
                        modeling, we tune the number of topics, keeping other hyperparameters (e.g. alpha
                        and beta (priors), number of iterations, sampling method, etc.) constant. We
                        created topic models with the number of topics equal to 100, 200, 500, and 1000.
                        In post-processing, we asked a group of humanists to read the ten most heavily
                        weighted words in each topic, give the topic labels based on those words, and
                        qualitatively evaluate which k number of topics is ideal for Hansard (see the
                        Results section for more details). 
                     </div>
                     
                     <div class="counter"><a href="#p48">48</a></div>
                     <div class="ptext" id="p48">Sub-corpus extraction is done by ranking the documents in a corpus using their
                        relevance to one or more topics of interest. The simplest ranking algorithm counts
                        the relative frequency of topic words, or word occurrences assigned to at least
                        one topic of interest, and ranks the documents with the highest frequencies as
                        most relevant. However, this algorithm can incorrectly down-rank longer documents.
                        We improve on this simple ranking algorithm by using the lower bound of Wilson
                        score confidence intervals for a Bernoulli parameter, z, as the sorting value [<a class="ref" href="#agresti1998">Agresti and Coull 1998</a>]. The parameter z is tuneable and a larger z
                        sporadically introduces longer documents into the top-ranked sub-corpus; in our
                        web app, the user can visually explore how increasing z changes the rankings by
                        reading the document titles and linking to the full text of the documents.
                        Different values of z may be optimal for sub-corpus extraction depending on the
                        topics of interest and the user’s goals. 
                     </div>
                     
                     <div class="counter"><a href="#p49">49</a></div>
                     <div class="ptext" id="p49">We argue that topic-based sub-corpus extraction is a highly effective way to
                        identify a relevant sub-corpus and highlight this point in the context of
                        exploring past land use and eviction in the British Parliamentary debates. 
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Use Case: Searching for Property in Hansard</h1>
                  
                  <div class="counter"><a href="#p50">50</a></div>
                  <div class="ptext" id="p50">Parliamentary speech has been carefully archived since 1803 and is available as a
                     digitized resource called the Parliamentary Debates; it contains full transcriptions
                     of all of the spoken words in the British Parliament from 1803 to 1908 as well as
                     metadata like speaker name, speaker constituency, date of speech, and debate
                     titles.<a class="noteRef" href="#d4e844">[5]</a> The collection is
                     commonly referred to as Hansard after Thomas Curson Hansard, a London publisher and
                     first official printer to the parliament. We use the terms Parliamentary Debates and
                     Hansard interchangeably throughout this paper. Hansard provides a rich corpus for
                     text analysis using Natural Language Processing methods because its digitization is
                     high quality (low errors in optical character recognition) and it is comprehensive,
                     spanning over two centuries and hundreds of millions of words of speech.
                  </div>
                  
                  <div class="figure">
                     
                     
                     <div class="ptext"><a href="resources/images/figure02.jpg" rel="external"><img src="resources/images/figure02.jpg" alt="" /></a></div>
                     
                     <div class="caption">
                        <div class="label">Figure 2. </div>Scanned PDF images of the Parliamentary Debates. The left image shows some of
                        the metadata captured in the records, such as title, volume number, date, and
                        house of parliament. The right image shows some speech acts, separated by debate
                        topic and by speaker.
                     </div>
                  </div>
                  
                  
                  <div class="counter"><a href="#p51">51</a></div>
                  <div class="ptext" id="p51">Because of the volume and breadth of topics in Hansard, it is a valuable resource
                     for
                     historical and socio-political analyses by students and scholars, alike. And while
                     there is great value in being publicly available in its entirety, it is difficult
                     to
                     narrow down to smaller excerpts of interest in its existing online forms. We create
                     a
                     pipeline that ingests the Hansard data and presents it in a new digital space, the
                     HaToRI web app, that broadens the horizons of possibility for digital humanities
                     research and collaboration. We present a case study in using the web app for a
                     historical analysis of how ideas about land and property changed in the
                     nineteenth-century British empire in the following sections. However, the tool is
                     modular and can be applied with some modification to any number of analyses by
                     selecting a different focal point other than the land topics or swapping out the
                     Hansard corpus for another collection of text.
                  </div>
                  
                  <div class="counter"><a href="#p52">52</a></div>
                  <div class="ptext" id="p52">While historical scholarship on Hansard is rich and varied due to the completeness
                     of
                     the resource, there is a lack of research about the changing discourse of property
                     in
                     Hansard. In contrast, there is extensive research on four written reports specific
                     to
                     land and property [<a class="ref" href="#bull1996">Bull 1996</a>]
                     [<a class="ref" href="#black1960">Black 1960</a>]
                     [<a class="ref" href="#campbell2005">Campbell 2005</a>]
                     [<a class="ref" href="#connelly2003">Connelly 2003</a>]
                     [<a class="ref" href="#donnelly1983">Donnelly 1983</a>]
                     [<a class="ref" href="#grigor2000">Grigor 2000</a>]
                     [<a class="ref" href="#steele1974">Steele 1974</a>]. They were commissioned by the Queen and are commonly
                     aliased by the name of the lords who wrote the report: Napier, Bessborough, Richmond,
                     and Devon. A major shift occurred between the Devon and Bessborough Reports
                     (1845-1881), from greater landlord protections to greater tenant protections, due
                     in
                     part to massive resistance in colonies of the Empire like Ireland, Scotland, and
                     India [<a class="ref" href="#sartori2014">Sartori 2014</a>]. The Encumbered Estates Act of 1849 disadvantaged
                     tenants trying to improve their holdings by moving property with outstanding debts
                     from Irish to English owners; Gladstone’s Irish Land Act of 1881 advantaged tenants
                     by introducing the first rent control law in history, in addition to redistributing
                     landlord property to Irish tenants [<a class="ref" href="#readman2008">Readman 2008</a>]
                     [<a class="ref" href="#steele1974">Steele 1974</a>]. We might attribute the large amount of scholarship on
                     the reports to both the specificity of the subject matter of the reports and also,
                     to
                     a citation feedback loop, where historians produce much research about widely cited
                     sources because of their ubiquitousness in the literature. While studying these
                     documents so extensively is valuable, it creates bias in the historical analysis by
                     leaving out more uninvestigated primary sources. 
                  </div>
                  
                  <div class="counter"><a href="#p53">53</a></div>
                  <div class="ptext" id="p53">Contemporary ideas about property that favor tenant protections, a normalized
                     cultural expectation that is often codified into law, have a long and bloody history
                     that can in part be traced back to the revolts and reforms that occurred in the
                     nineteenth-century British empire. A major shift from favoring landlords to tenants
                     in areas like land distribution, eviction, fair rent, is theorized to have occurred
                     from the first half to the second half of the century [<a class="ref" href="#sartori2014">Sartori 2014</a>].
                     The evidence presented in the following section supports this theory.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Results and Visualizations</h1>
                  
                  <div class="counter"><a href="#p54">54</a></div>
                  <div class="ptext" id="p54"> The historical analysis that follows is one of many possible threads of questioning
                     into the Hansard body of text. We demonstrate how to explore the question of how the
                     parliamentary discourse of property changed in the nineteenth-century, using the
                     choices made in developing the code pipeline and the user- and visualization-driven
                     search through the web app parameters. The threads of questioning, driven by scholars
                     and research interests, can be approached in a new way that leans on digital tools
                     for a deeper dive into large bodies of text. Historical claims are still made through
                     close reading and analysis; but using topic-based search to guide close reading and
                     analysis, we open a new and focused window into a previously under-studied primary
                     source. In the next section, we both validate our hypothesis from the secondary
                     literature and introduce new perspectives — of members of parliament — into the body
                     of knowledge about British land reform.
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Five Out of Five Hundred Topics are Relevant for Land and Property</h2>
                     
                     <div class="counter"><a href="#p55">55</a></div>
                     <div class="ptext" id="p55">An early choice made in the pipeline is what the ideal number of topics (k) is for
                        the Hansard corpus. We pre-computed topic models at different increments of k (50,
                        100, 200, 500, 1000) for review. Humanists determined that the optimal number of
                        topics is 500 because of the uniqueness and specificity of the topics. Some topics
                        are difficult to label distinctly from other topics in the 1000 topic model, while
                        most topics could be given unique labels in the 500 topic model. Five distinct
                        land-related topics, their ten most heavily weighted words, and their designated
                        labels are shown in Table 2. They are sorted by the year(s) in which they peaked,
                        with early peaks on the left and late peaks on the right. The language used to
                        describe land in Hansard mirrors the historical shifts of the period, from an
                        agriculturally valued plot in the first half of the century to a rental
                        relationship between landlords and tenants in the middle of the century and to
                        legal protections that favored tenants in the latter half of the century —
                        protections that included rent restrictions, freedom from arbitrary evictions, and
                        land redistributions [<a class="ref" href="#readman2008">Readman 2008</a>].
                     </div>
                     
                     <div class="table">
                        <table class="table">
                           <tr class="row label">
                              
                              <td valign="top" class="cell">17. Land value measured in agricultural output</td>
                              
                              <td valign="top" class="cell">46. Land value measured as a tax for the church and as rental income for
                                 landlords
                              </td>
                              
                              <td valign="top" class="cell">447. Property as a rental relationship between landlords and
                                 tenants
                              </td>
                              
                              <td valign="top" class="cell">35. Negotiating the interests of landowners vs. tenants who seek to
                                 improve their rentals
                              </td>
                              
                              <td valign="top" class="cell">180. Legal protections for tenants</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">1820, 1846</td>
                              
                              <td valign="top" class="cell">1827-1840</td>
                              
                              <td valign="top" class="cell">1838, 1892</td>
                              
                              <td valign="top" class="cell">1847-1850, 1882</td>
                              
                              <td valign="top" class="cell">1882-1904</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">farmer</td>
                              
                              <td valign="top" class="cell">tith</td>
                              
                              <td valign="top" class="cell">leas</td>
                              
                              <td valign="top" class="cell">properti</td>
                              
                              <td valign="top" class="cell">rent</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">agricultur</td>
                              
                              <td valign="top" class="cell">rent</td>
                              
                              <td valign="top" class="cell">year</td>
                              
                              <td valign="top" class="cell">land</td>
                              
                              <td valign="top" class="cell">land</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">land</td>
                              
                              <td valign="top" class="cell">clergi</td>
                              
                              <td valign="top" class="cell">rent</td>
                              
                              <td valign="top" class="cell">estat</td>
                              
                              <td valign="top" class="cell">case</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">price</td>
                              
                              <td valign="top" class="cell">charg</td>
                              
                              <td valign="top" class="cell">leasehold</td>
                              
                              <td valign="top" class="cell">proprietor</td>
                              
                              <td valign="top" class="cell">commiss</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">produc</td>
                              
                              <td valign="top" class="cell">land</td>
                              
                              <td valign="top" class="cell">lesse</td>
                              
                              <td valign="top" class="cell">owner</td>
                              
                              <td valign="top" class="cell">commission</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">farm</td>
                              
                              <td valign="top" class="cell">commut</td>
                              
                              <td valign="top" class="cell">land</td>
                              
                              <td valign="top" class="cell">interest</td>
                              
                              <td valign="top" class="cell">fix</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">cultiv</td>
                              
                              <td valign="top" class="cell">owner</td>
                              
                              <td valign="top" class="cell">term</td>
                              
                              <td valign="top" class="cell">possess</td>
                              
                              <td valign="top" class="cell">fair</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">year</td>
                              
                              <td valign="top" class="cell">collect</td>
                              
                              <td valign="top" class="cell">tenant</td>
                              
                              <td valign="top" class="cell">person</td>
                              
                              <td valign="top" class="cell">court</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">interest</td>
                              
                              <td valign="top" class="cell">landlord</td>
                              
                              <td valign="top" class="cell">properti</td>
                              
                              <td valign="top" class="cell">valu</td>
                              
                              <td valign="top" class="cell">chief</td>
                              
                           </tr>
                           <tr class="row">
                              
                              <td valign="top" class="cell">crop</td>
                              
                              <td valign="top" class="cell">payment</td>
                              
                              <td valign="top" class="cell">case</td>
                              
                              <td valign="top" class="cell">improv</td>
                              
                              <td valign="top" class="cell">applic</td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Five land-related topics from our 500 topic model of Hansard. They are
                           sorted on when they peaked. The first half of the century does not talk about
                           land as much as a rental agreement but in terms of its productivity. The second
                           half of the century considers the question of land as rental agreement and the
                           legal protections and rights for both sides of the agreement.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p56">56</a></div>
                     <div class="ptext" id="p56"> We build this process of preparing, reviewing, and labeling different k values
                        into the code pipeline. While the computational cost of preparing different models
                        is not reduced, the human effort of maintaining code and documentation on many
                        models is greatly reduced.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Spikes in the discussion of land use and rent in 1850 and 1880</h2>
                     
                     <div class="counter"><a href="#p57">57</a></div>
                     <div class="ptext" id="p57">Figure 3 shows topic 35, which we named “Negotiating the
                        interests of landowners vs. tenants who seek to improve their rentals”.
                        This topic peaks twice in the late 1840s and early 1850s and then peaks again in
                        1870 and 1880, mirroring the historical arc of this topic. The Devon and
                        Bessborough Commissions were ordered and written in 1845 and 1881 because of calls
                        for land reform from peasant farmers and tenants [<a class="ref" href="#sartori2014">Sartori 2014</a>].
                        This topic is one of several that point the scholar to the land question, and in
                        combination they can paint a fuller, richer picture of the topic than any single
                        excerpt of Hansard or written report.
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>The most frequently occurring words in the topic are ranked and displayed on
                           the left. The percentages are each word’s relative frequency in the topic and
                           add up to 100%. The line plot shows the frequency of the topic (per 10,000
                           words) as a function of time. Common and custom stopwords (such as articles and
                           conjunctions and proper nouns) are replaced with ‘substitute_word’ to
                           drastically reduce our vocabulary without much information loss.
                        </div>
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Topic modeling is better to extract a sub-corpus than word frequencies</h2>
                     
                     <div class="counter"><a href="#p58">58</a></div>
                     <div class="ptext" id="p58"> We present a land and property example that illustrates how topic modeling goes
                        beyond word frequencies for historical analysis. Figure 4 shows word frequencies
                        for the most commonly occurring words in topic 35. The word frequencies are
                        normalized by the total number of unigrams in each year and are shown per 10,000
                        words. Four peaks stand out: “person” in the early part of the period and “land”
                        in the late part of the period. “Land” and “properti” have a few smaller peaks
                        throughout the period. The pattern does have some historical significance, but it
                        is certainly noisier than the peaks in topic frequency seen in Figure 3. There is
                        no clear peak in the middle of the century where we would expect to see massive
                        activity about this topic, which could potentially lead us to miss some
                        interesting and important documents. Furthermore, if we were to search for
                        documents of interest in Hansard using these frequently occurring keywords, we
                        would likely incur many false negatives that are skewed towards the years in which
                        the word counts peaked.
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Unigram Frequencies. This figure shows the relative frequencies of the most
                           commonly occurring unigrams in topic 35. The figure shows counts per 10,000
                           unigrams, normalized by the total number of unigrams in the year, on the y-axis
                           and year on the x-axis. The diagram demonstrates a transition from a language
                           of “person” and “interest”, visible in the dominance of yellow and
                           pink lines around 1815, to a language where spikes in 1882, 1896, and 1907 were
                           marked by the use of the terms “land”, “value”, and
                           “improvement”. 
                        </div>
                     </div>
                     
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">D3 visualizations and web app</h2>
                     
                     <div class="counter"><a href="#p59">59</a></div>
                     <div class="ptext" id="p59"> The screenshots shown in Figures 3 and 4 are taken from the HaToRI web app. The
                        tool allows humanists to interact with their corpora in a user-friendly, visual,
                        iterative, and exploratory process and avoids the time and resource penalties that
                        traditional search and close reading of corpora incur. We will walk through the
                        various features of the tool with accompanying screenshots in this next section
                        with an emphasis on how the tool enhances digital humanities research by
                        incorporating some of the values of code pipelines discussed in the introduction.
                        
                     </div>
                     
                     <div class="counter"><a href="#p60">60</a></div>
                     <div class="ptext" id="p60">The tool approaches visual topic exploration at different levels or views: word,
                        document, and topic. Taken together in a single interface, these levels give the
                        user a three-dimensional view of their corpus that is less possible when searching
                        for specific words, documents, or topics in silo. The n-gram plot in Figure 5
                        shows word frequencies per year for the top six words in a topic. Each line in the
                        plot shows frequencies for a given word and each word in the plot is also shown on
                        the right as a button; they can be removed from the plot by clicking on the words
                        and the plot will dynamically re-scale based on the frequencies present in the
                        current plot window. Words can be added by searching the vocabulary in the search
                        bar. The search results filter down as the user types a query because corpora are
                        often pre-processed differently, resulting in a vocabulary made up of stems or
                        lemmas rather than actual words. The frequencies can be shown in absolute terms or
                        relative, normalized to the total number of words in the year. The word view gives
                        the user a finer-grained view of the corpus, but leaves out the greater context
                        and interaction of the words. 
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 5. </div>Word level exploration of the corpus. The default is to display the top six
                           words in each topic. Users can add or remove words with the search bar and it
                           auto-fills with unigrams in the corpus vocabulary. The user can switch between
                           absolute and relative counts.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p61">61</a></div>
                     <div class="ptext" id="p61">Figures 6 and 7 show topic views of the corpus. The home page shows the first
                        view, where each topic is a word cloud showing the top words in the topic. The
                        size of the word indicates its probability within the topic; bigger words are more
                        likely to be sampled from the topic than smaller words. The page scrolls and loads
                        one chunk of word clouds at a time. The user can filter to a smaller subset of
                        topics by searching for unigrams in the search bar at the top right corner of the
                        page. The alternate view shows a table view of the topics with an additional sort
                        functionality that will sort the topics based on any of the table fields. They can
                        be sorted by their ID, by mean peak year, alphabetically by words, or by their
                        proportion in the corpus. The recommended navigational flow of the corpus
                        exploration part of the tool is to start at one of these two topic overviews and
                        drill down closer into a single topic. Figure 8 shows a detailed topic view. The
                        view includes more top words than the overviews, a plot of the topic over time, a
                        link to the n-gram plot, and a sorted table of documents that are most associated
                        with the topic. The detail view allows the user to explore the topic from various
                        different vantage points, but does not allow multi-topic exploration. All topics
                        are shown in the plot on the left in Figure 9, and they are arranged based on
                        their similarity to each other. Topics with very similar word distributions
                        cluster together and topics with very dissimilar word distributions are very far
                        apart. 
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 6. </div>The front page of the web app for topic level corpus exploration. Each topic
                           is a word cloud and starting point with which to move into a more detailed view
                           of the topic. The word clouds show topic distribution over time on hover and
                           they are searchable by word.
                        </div>
                     </div>
                     
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 7. </div>Alternate table view of home page. Sortable by topic ID, mean peak time,
                           alphabetically by words, and by proportion in corpus.
                        </div>
                     </div>
                     
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 8. </div>Detail view of topic. Ranks words and ranks documents, and for both shows
                           relevance to the topic. Topic relevance over time plot.
                        </div>
                     </div>
                     
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 9. </div>Topic Clustering view shows how similar topics are to each other.
                        </div>
                     </div>
                     
                     
                     <div class="counter"><a href="#p62">62</a></div>
                     <div class="ptext" id="p62">Figure 10 shows ranked documents relevant to the five land-related topics shown in
                        Table 2 for z = [0, 100, 500]. For land topics, larger z values produce a better
                        ranking than smaller z values. There are documents from the early and late periods
                        of the century, near the peaks of land-related activity that we expect. Also,
                        three out of four written reports, denoted by the prefix “seed-”, are in the
                        top-ranked documents list when z = 500; we expect the written reports to be ranked
                        highly because the historical literature confirms that they are key documents
                        summarizing land use in the nineteenth-century. The low proportion of topic tokens
                        out of total tokens in the written reports — 0.031, 0.066, and 0.156 for the
                        Bessborough, Richmond, and Devon reports, respectively — indicates that the seed
                        reports contain a lot of noise and shows the weakness of an earlier document-based
                        approach compared to our topic-based approach [<a class="ref" href="#lee2018">Lee et al. 2018</a>]
                        [<a class="ref" href="#tangherlini2013">Tangherlini and Leonard 2013</a>].
                     </div>
                     
                     <div class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" alt="" /></a></div>
                        
                        <div class="caption">
                           <div class="label">Figure 10. </div>Top nine most relevant documents ranked using a Wilson-score based sorting
                           method with a single parameter, z. From top to bottom, the tables show results
                           for z = [0, 100, 500].
                        </div>
                     </div>
                     
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Summary</h1>
                  
                  <div class="counter"><a href="#p63">63</a></div>
                  <div class="ptext" id="p63">In this paper, we introduce the pipeline concept as a means for scholars to conduct
                     transparent, modular, and interoperable research and we create a web app that
                     exemplifies these values. We make the HaToRI instance of the web app digitally
                     available, and present a use case for doing enhanced historical analysis on the
                     Hansard text corpus using the app. We explored how topic modeling can be used for
                     sub-corpus extraction when other methods likely fail due to the sheer size of the
                     corpus or nuances in the topics of interest that cannot be accurately captured by
                     keywords or terms. To illustrate the advantages of topic modeling in sub-corpus
                     extraction, we extracted a sub-corpus specific to land use, rent, and property from
                     the Hansard British Parliamentary debates of the 19th century. There are two
                     historically relevant periods when land use and property was exhaustively discussed
                     in the Parliament: the 1850’s and 1880’s. We illustrate that topic-based sub-corpus
                     extraction identifies these two eras based on the frequencies of the relevant topics,
                     while keywords tend to wash out the signal and identify false peaks in the frequency
                     plot. We described our pre-processing and post-processing steps in detail and provide
                     a web app with a set of visualizations and functionalities to aid humanist research
                     on the Hansard corpus. These functionalities include topic clustering based on
                     similarity, document ranking based on topic prevalence, and frequency plots. The
                     source codes for the pipeline and the web app are publicly available and can be
                     modified to work with other corpora. We believe that topic modeling, in particular,
                     as well as our novel document ranking algorithm, are well suited to extract a
                     relevant sub-corpus for in-depth studies, especially if the corpus is prohibitively
                     long to read by humans.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgments</h1>
                  
                  <div class="counter"><a href="#p64">64</a></div>
                  <div class="ptext" id="p64">Thanks to Mark Howison at Brown University for facilitating the collaboration with
                     Prof. Jo Guldi and contributing early work to this project. This research was
                     conducted using computational resources and services at the Center for Computation
                     and Visualization, Brown University.
                  </div>
                  
               </div>
               
               
               
               
               
            </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e427"><span class="noteRef lang en">[1] <a href="https://docs.cortext.net/" onclick="window.open('https://docs.cortext.net/'); return false" class="ref">https://docs.cortext.net/</a></span></div>
               <div class="endnote" id="d4e502"><span class="noteRef lang en">[2] The code is open-sourced (MIT
                     license), so it is free to use, modify, and share. The pipeline code can be found
                     at <a href="https://github.com/brown-data-science/inquiry-for-philologic-analysis" onclick="window.open('https://github.com/brown-data-science/inquiry-for-philologic-analysis'); return false" class="ref">https://github.com/brown-data-science/inquiry-for-philologic-analysis</a>,
                     and the web-app code, both front end and back end, at <a href="https://github.com/brown-data-science/hansard_api" onclick="window.open('https://github.com/brown-data-science/hansard_api'); return false" class="ref">https://github.com/brown-data-science/hansard_api</a>. The visualizations
                     and front end are built using Javascript frameworks (<span class="monospace">d3.js</span> and
                     <span class="monospace">vue.js</span>) and the back end is deployed using Docker
                     containerization. The web app is hosted on github.io at <a href="https://eight1911.github.io/hansard/" onclick="window.open('https://eight1911.github.io/hansard/'); return false" class="ref">https://eight1911.github.io/hansard</a>.</span></div>
               <div class="endnote" id="d4e578"><span class="noteRef lang en">[3] <a href="https://pages.github.com/" onclick="window.open('https://pages.github.com/'); return false" class="ref">https://pages.github.com/</a></span></div>
               <div class="endnote" id="d4e605"><span class="noteRef lang en">[4] <a href="http://www.hansard-archive.parliament.uk/" onclick="window.open('http://www.hansard-archive.parliament.uk/'); return false" class="ref">http://www.hansard-archive.parliament.uk/</a></span></div>
               <div class="endnote" id="d4e844"><span class="noteRef lang en">[5] <a href="http://www.hansard-archive.parliament.uk/" onclick="window.open('http://www.hansard-archive.parliament.uk/'); return false" class="ref">http://www.hansard-archive.parliament.uk/</a></span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="agresti1998">
                     <!-- close -->Agresti and Coull 1998</span> Agresti, A. and Coull, B. A.
                  <cite class="title italic">Approximate is Better than “Exact” for Interval
                     Estimation of Binomial Proportions</cite>, <cite class="title italic">The American
                     Statistician</cite>, 52 (1998): 119-126.
               </div>
               <div class="bibl"><span class="ref" id="alexander2017">
                     <!-- close -->Alexander and Struan 2017</span> Alexander, M. and Struan,
                  A. “Digital Hansard: Politics and the Uncivil”, <cite class="title italic">Digital Humanities</cite>, (2017): 378-380
               </div>
               <div class="bibl"><span class="ref" id="berry2005">
                     <!-- close -->Berry and Browne 2005</span> Berry, M. W. and Browne, M.
                  “Email Surveillance Using Non-negative Matrix
                  Factorization”, <cite class="title italic">Computational and Mathematical
                     Organization Theory</cite>, 11.3 (2005): 249-264.
               </div>
               <div class="bibl"><span class="ref" id="black1960">
                     <!-- close -->Black 1960</span> Black, R. D. C. <cite class="title italic">Economic Thought and the Irish Question, 1817-1870</cite>. University Press,
                  Cambridge, MA (1960).
               </div>
               <div class="bibl"><span class="ref" id="blaxill2013">
                     <!-- close -->Blaxill 2013</span> Blaxill, L. “Quantifying the Language of British Politics, 1880–1910”, <cite class="title italic">Historical Research</cite>, 86 (2013): 313-41.
               </div>
               <div class="bibl"><span class="ref" id="blei2006">
                     <!-- close -->Blei and Lafferty 2006</span> Blei, D. M. and Lafferty, J. D.
                  “Dynamic Topic Models”, In <cite class="title italic">Proceedings of the 23rd International Conference on Machine Learning</cite>,
                  Pittsburgh, PA, June 2006.
               </div>
               <div class="bibl"><span class="ref" id="blei2009">
                     <!-- close -->Blei and Lafferty 2009</span> Blei, D. M. and Lafferty, J. D.
                  “Topic Models”, In A. Srivastava and M. Sahami (eds),
                  <cite class="title italic">Text Mining: Theory and Applications</cite>, Taylor and
                  Francis, London (2009).
               </div>
               <div class="bibl"><span class="ref" id="blei2003">
                     <!-- close -->Blei et al. 2003</span> Blei, D. M., Ng, A. Y. and Jordan, M.
                  I. “Latent Dirichlet Allocation”, <cite class="title italic">Journal of Machine Learning Research</cite>, 3 (2003): 933-1022.
               </div>
               <div class="bibl"><span class="ref" id="brooke2015">
                     <!-- close -->Brooke et al. 2015</span> Brooke, J., Hammond, A. and Hirst,
                  G. “GutenTag: An NLP-driven Tool for Digital Humanities Research
                  in the Project Gutenberg Corpus”, In <cite class="title italic">Proceedings of
                     the Fourth Workshop on Computational Linguistics for Literature</cite>, Denver,
                  CO, June 2015.
               </div>
               <div class="bibl"><span class="ref" id="bull1996">
                     <!-- close -->Bull 1996</span> Bull, P. Land, <cite class="title italic">Politics
                     and Nationalism: A Study of the Irish Land Question</cite>. Gill and Macmillan,
                  Dublin (1996).
               </div>
               <div class="bibl"><span class="ref" id="butterfield2016">
                     <!-- close -->Butterfield et al. 2016</span> Butterfield, A., Ngondi,
                  G.E. and Kerr, A. A <cite class="title italic">Dictionary of Computer Science</cite>,
                  Seventh Edition. Oxford University Press, Oxford (2016).
               </div>
               <div class="bibl"><span class="ref" id="campbell2005">
                     <!-- close -->Campbell 2005</span> Campbell, F. J. M. <cite class="title italic">Land and Revolution Nationalist Politics in the West of Ireland,
                     1891-1921</cite>. Oxford University Press, Oxford, New York (2005).
               </div>
               <div class="bibl"><span class="ref" id="connelly2003">
                     <!-- close -->Connelly 2003</span> Connelly, S. J. “Jacobites, Whiteboys, and Republicans: Varieties of Disaffection in
                  Eighteenth-Century Ireland”, <cite class="title italic">Eighteenth-Century
                     Ireland/Iris an Da Chultur</cite>, 18 (2003): 63-79.
               </div>
               <div class="bibl"><span class="ref" id="donnelly1983">
                     <!-- close -->Donnelly 1983</span> Donnelly, J. S. 1983. “Irish Agrarian Rebellion: The Whiteboys of 1769-76”, <cite class="title italic">Proceedings of the Royal Irish Academy, Section C: Archaeology,
                     Celtic Studies, History, Linguistics, Literature</cite> (1983): 293-331.
               </div>
               <div class="bibl"><span class="ref" id="edmond2013">
                     <!-- close -->Edmond 2013</span> Edmond, J. “CENDARI’s
                  Grand Challenges: Building, Contextualising and Sustaining a New Knowledge
                  Infrastructure”, <cite class="title italic">International Journal of Humanities
                     and Arts Computing</cite>, 7.1-2 (2013): 58–69. doi:<a href="https://doi.org/10.3366/ijhac.2013.0081" onclick="window.open('https://doi.org/10.3366/ijhac.2013.0081'); return false" class="ref">10.3366/ijhac.2013.0081</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="edmond2015">
                     <!-- close -->Edmond et al. 2015</span> Edmond, J., Bulatovic, N. and
                  O’Connor, A. “The Taste of ‘Data Soup’ and the Creation of a
                  Pipeline for Transnational Historical Research”, <cite class="title italic">Journal of the Japanese Association for Digital Humanities</cite> 1.1 (2015):
                  107-122. doi:<a href="https://doi.org/10.17928/jjadh.1.1_107" onclick="window.open('https://doi.org/10.17928/jjadh.1.1_107'); return false" class="ref">10.17928/jjadh.1.1_107</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="edwards2007">
                     <!-- close -->Edwards et al. 2007</span> Edwards, P.N., Jackson, S.J.,
                  Bowker, G.C., Knobel, C.P. “Understanding Infrastructure:
                  Dynamics, Tensions, and Design”, <cite class="title italic">Human and Social
                     Dynamics</cite> (2007). NSF Grant 0630263.
               </div>
               <div class="bibl"><span class="ref" id="eubanks2015">
                     <!-- close -->Eubanks 2015</span> Eubanks, V. <cite class="title italic">Automating Inequality: How High-tech Tools Profile, Police, and Punish the
                     Poor</cite>. St. Martin’s Press, New York, NY (2015).
               </div>
               <div class="bibl"><span class="ref" id="goldstone2014">
                     <!-- close -->Goldstone and Underwood 2014</span> Goldstone, A. and
                  Underwood, T. “The Quiet Transformations of Literary Studies:
                  What Thirteen Thousand Scholars Could Tell Us”, <cite class="title italic">New
                     Literary History</cite>, 45.3 (2014): 359-384. doi: <a href="https://doi.org/10.1353/nlh.2014.0025" onclick="window.open('https://doi.org/10.1353/nlh.2014.0025'); return false" class="ref">10.1353/nlh.2014.0025</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="grigor2000">
                     <!-- close -->Grigor 2000</span> Grigor, I. F. <cite class="title italic">Highland Resistance</cite>. Mainstream Publishing, Edinburgh (2000).
               </div>
               <div class="bibl"><span class="ref" id="guldi2018">
                     <!-- close -->Guldi 2018</span> Guldi, J. “Critical
                  Search: A Procedure for Guided Reading in Large-Scale Textual Corpora”,
                  <cite class="title italic">Journal of Cultural Analytics</cite> (2018). doi: <a href="https://doi.org/10.31235/osf.io/g286e" onclick="window.open('https://doi.org/10.31235/osf.io/g286e'); return false" class="ref">10.31235/osf.io/g286e</a></div>
               <div class="bibl"><span class="ref" id="harris1954">
                     <!-- close -->Harris 1954</span> Harris, Z. “Distributional Structure”, <cite class="title italic">Word</cite>, 10.2-3
                  (1954): 146-62.
               </div>
               <div class="bibl"><span class="ref" id="introna2006">
                     <!-- close -->Introna and Nissenbaum 2006</span> Introna, L.D. and
                  Nissenbaum, H. “Shaping the Web: Why the Politics of Search
                  Engines Matters”, <cite class="title italic">The Information Society: An
                     International Journal</cite> 16.3: 169-185 (2006).
               </div>
               <div class="bibl"><span class="ref" id="janicke2015">
                     <!-- close -->Janicke et al. 2015</span> Janicke, S., Franzini, G.,
                  Cheema, M. F. and Scheuermann, G. “On Close and Distant Reading
                  in Digital Humanities: A Survey and Future Challenges”, <cite class="title italic">Eurographics Conference on Visualization State of the Art
                     Report</cite> (2015): 21.
               </div>
               <div class="bibl"><span class="ref" id="kaplan2015">
                     <!-- close -->Kaplan 2015</span> Kaplan, F. “A Map for
                  Big Data Research in Digital Humanities”, <cite class="title italic">Frontiers in
                     Digital Humanities</cite>, 2 (2015). doi:<a href="https://doi.org/10.3389/fdigh.2015.00001" onclick="window.open('https://doi.org/10.3389/fdigh.2015.00001'); return false" class="ref">10.3389/fdigh.2015.00001</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="le2014">
                     <!-- close -->Le and Mikolov 2014</span> Le, Q. V. and Mikolov, T. “Distributed Representation of Sentences and Documents”, In
                  <cite class="title italic">Proceedings of the Thirty-first International Conference on
                     Machine Learning Beijing</cite>, CN, June 2014.
               </div>
               <div class="bibl"><span class="ref" id="lee2018">
                     <!-- close -->Lee et al. 2018</span> Lee, A. S., Guldi, J. and Zsom, A. “Measuring Similarity: Computationally Reproducing the Scholar’s
                  Interests”, <a href="https://arxiv.org/abs/1812.05984" onclick="window.open('https://arxiv.org/abs/1812.05984'); return false" class="ref">arXiv:1812.05984</a> [cs.CL] (2018).
               </div>
               <div class="bibl"><span class="ref" id="llora2008">
                     <!-- close -->Llora et al. 2008</span> Llora, X., Acs, B., Auvil, L. S.,
                  Capitanu, B., Welge, M. E. and Goldberg, D. E. “Meandre:
                  Semantic-Driven Data-Intensive Flows in the Clouds”, In <cite class="title italic">IEEE Fourth International Conference on eScience</cite>,
                  Indianapolis, IN, December 2008. doi: <a href="https://doi.org/10.1109/eScience.2008.172" onclick="window.open('https://doi.org/10.1109/eScience.2008.172'); return false" class="ref">10.1109/eScience.2008.172</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="manning1999">
                     <!-- close -->Manning and Schutze 1999</span> Manning, C. D. and Schutze,
                  H. <cite class="title italic">Foundations of Statistical Natural Language
                     Processing</cite>. MIT Press, Cambridge, MA (1999).
               </div>
               <div class="bibl"><span class="ref" id="mattern2013">
                     <!-- close -->Mattern 2013</span> Mattern, S. “Infrastructural Tourism”, <cite class="title italic">Places Journal</cite>
                  (2013). doi: <a href="https://doi.org/10.22269/130701" onclick="window.open('https://doi.org/10.22269/130701'); return false" class="ref">https://doi.org/10.22269/130701</a>
                  
               </div>
               <div class="bibl"><span class="ref" id="mattern2015">
                     <!-- close -->Mattern 2015</span> Mattern, S. “Deep
                  Time of Media Infrastructure”, In Lisa Parks and Nicole Staroeislski
                  (eds.), <cite class="title italic">Signal Traffic: Critical Studies of Media
                     Infrastructures</cite>. University of Illinois Press, Champaign, IL
                  (2015).
               </div>
               <div class="bibl"><span class="ref" id="mattern2016">
                     <!-- close -->Mattern 2016</span> Mattern, S. “Scaffolding, Hard and Soft - Infrastructures as Critical and Generative
                  Structures”, <cite class="title italic">Spheres Journal for Digital
                     Cultures</cite>, 3 (2016).
               </div>
               <div class="bibl"><span class="ref" id="mccallum2002">
                     <!-- close -->McCallum 2002</span> McCallum, A. K. “MALLET: A Machine Learning for Language Toolkit”, <cite class="title italic">Scientific Research</cite>, (2002). <a href="http://mallet.cs.umass.edu" onclick="window.open('http://mallet.cs.umass.edu'); return false" class="ref">http://mallet.cs.umass.edu</a>.
               </div>
               <div class="bibl"><span class="ref" id="mikolov2013">
                     <!-- close -->Mikolov et al. 2013</span> Mikolov, T., Sutskever, I., Chen,
                  K., Corrado, G. S. and Dean, J. “Distributed Representations of
                  Words and Phrases and Their Compositionality”, <cite class="title italic">Advances in Neural Information Processing Systems</cite>, (2013). 
               </div>
               <div class="bibl"><span class="ref" id="murdock2017">
                     <!-- close -->Murdock et al. 2017</span> Murdock, J., Allen, C., Borner,
                  K., Light, R., McAlister, S., Ravenscroft, A., Rose, R., Rose, D., Otsuka, J.,
                  Bourget, D., Lawrence, J., and Reed, C. “Multi-level Computation
                  Methods for Interdisciplinary Research in the HathiTrust Digital Library”,
                  <cite class="title italic">PLoS One</cite>, 12.0 (2017).
               </div>
               <div class="bibl"><span class="ref" id="nissenbaum2010">
                     <!-- close -->Nissenbaum 2010</span> Nissenbaum, H. <cite class="title italic">Privacy in Context: Technology, Policy, and the Integrity of Social
                     Life</cite>. Stanford University Press, Stanford, CA (2010).
               </div>
               <div class="bibl"><span class="ref" id="noble2018">
                     <!-- close -->Noble 2018</span> Noble, S.U. <cite class="title italic">Algorithms
                     of Oppression: How Search Engines Reinforce Racism</cite>. NYU Press, New York,
                  NY (2018).
               </div>
               <div class="bibl"><span class="ref" id="oneil2016">
                     <!-- close -->O’Neill 2016</span> O’Neil, C. <cite class="title italic">Weapons of
                     Math Destruction: How Big Data Increases Inequality and Threatens
                     Democracy</cite>. Broadway Books, New York, NY 2016. 
               </div>
               <div class="bibl"><span class="ref" id="readman2008">
                     <!-- close -->Readman 2008</span> Readman, P. <cite class="title italic">Land
                     and Nation in England: Patriotism, National Identity, and the Politics of Land,
                     1880-1914</cite>. Boydell Press, Woodbridge, UK (2008).
               </div>
               <div class="bibl"><span class="ref" id="rule2015">
                     <!-- close -->Rule et al. 2015</span> Rule, A., Cointet, J. and Bearman, P.
                  S. “Lexical shifts, substantive changes, and continuity in State
                  of the Union discourse, 1790–2014”, In <cite class="title italic">Proceedings of
                     the National Academy of Sciences</cite>, 112.35 (2015): 10837-44.
               </div>
               <div class="bibl"><span class="ref" id="sartori2014">
                     <!-- close -->Sartori 2014</span> Sartori, A. <cite class="title italic">Liberalism in Empire: An Alternative History</cite>. University of California
                  Press, Berkeley (2014).
               </div>
               <div class="bibl"><span class="ref" id="sinclair2016">
                     <!-- close -->Sinclair and Rockwell 2016</span> Sinclair, S. and
                  Rockwell, G. “Voyant Facts”, Hermeneuti.ca:
                  Computer-Assisted Interpretation in the Humanities (2016). <a href="http://hermeneuti.ca/VoyantFacts" onclick="window.open('http://hermeneuti.ca/VoyantFacts'); return false" class="ref">http://hermeneuti.ca/VoyantFacts</a>.
               </div>
               <div class="bibl"><span class="ref" id="steele1974">
                     <!-- close -->Steele 1974</span> Steele, D. <cite class="title italic">Irish Land
                     and British Politics: Tenant-Right and Nationality, 1865-1870</cite>. Cambridge
                  University Press, London (1974).
               </div>
               <div class="bibl"><span class="ref" id="svensson2011">
                     <!-- close -->Svensson 2011</span> Svensson, P. “From
                  Optical Fiber to Conceptual Cyberinfrastructure”, <cite class="title italic">Digital Humanities Quarterly</cite>, 5.1 (2011).
               </div>
               <div class="bibl"><span class="ref" id="tangherlini2013">
                     <!-- close -->Tangherlini and Leonard 2013</span> Tangherlini, T. and
                  Leonard, P. “Trawling in the Sea of the Great Unread: Sub-corpus
                  topic modeling and Humanities research”, <cite class="title italic">Poetics</cite>, 41.6 (2013): 725-749.
               </div>
               <div class="bibl"><span class="ref" id="tucker2004">
                     <!-- close -->Tucker 2004</span> Tucker, A. B. <cite class="title italic">Handbook of Computer Science</cite>. CRC Press, Boca Raton (2004).
               </div>
               <div class="bibl"><span class="ref" id="vaidhyanathan2012">
                     <!-- close -->Vaidhyanathan 2012</span> Vaidhyanathan, S. <cite class="title italic">The Googlization of Everything: (and why we should worry)</cite>.
                  University of California Press, Los Angeles, CA (2012).
               </div>
               <div class="bibl"><span class="ref" id="vaidhyanathan2018">
                     <!-- close -->Vaidhyanathan 2018</span> Vaidhyanathan, S. <cite class="title italic">Anti-Social Media: How Facebook Disconnects Us and Undermines
                     Democracy</cite>. Oxford University Press, New York, NY (2018).
               </div>
               <div class="bibl"><span class="ref" id="dallessandro2016">
                     <!-- close -->d’Alessandro et al. 2016</span> d’Alessandro, B.,
                  O’Neil, C. and LaGatta, T. “Conscientious Classification: A Data
                  Scientist’s Guide to Discrimination-Aware Classification”, <cite class="title italic">Big Data</cite>, 5.2 (2017). doi: <a href="http://doi.org/10.1089/big.2016.0048" onclick="window.open('http://doi.org/10.1089/big.2016.0048'); return false" class="ref">http://doi.org/10.1089/big.2016.0048</a></div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>