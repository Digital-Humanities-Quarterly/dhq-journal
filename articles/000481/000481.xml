<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">The Role of Critical Thinking in Humanities
               Infrastructure: The Pipeline Concept with a Study of HaToRI (Hansard Topic Relevance
               Identifier)</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Ashley S. <dhq:family>Lee</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Brown University</dhq:affiliation>
               <email>ashley_lee@brown.edu</email>
               <dhq:bio>
                  <p>Ashley Lee is a Senior Data Scientist at Brown University’s Center for
                     Computation and Visualization. She holds an academic background in biology and
                     data science and her professional experience has included work in machine
                     learning, data visualization, and software engineering.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Poom <dhq:family>Chiarawongse</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Brown University</dhq:affiliation>
               <email>t.chia@brown.edu</email>
               <dhq:bio>
                  <p>Poom Chiarawongse is a Software Engineer at Asana, located in the San Francisco
                     Bay Area. He holds a BA in Computer Science from Brown University and has
                     previously worked as a Data Science Intern at the Center for Computation and
                     Visualization at Brown University.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Jo <dhq:family>Guldi</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Southern Methodist University</dhq:affiliation>
               <email>jguldi@mail.smu.edu</email>
               <dhq:bio>
                  <p>Jo Guldi is an Associate Professor of History at Southern Methodist University.
                     Born in Dallas, Texas, She received her AB from Harvard University, and then
                     studied at Trinity College, Cambridge before completing her PhD in History at
                     the University of California, Berkeley, after which she continued on to
                     postdocs at the University of Chicago and the Harvard Society of Fellows. She
                     was also previously Hans Rothfels Assistant Professor of History at Brown. </p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Andras <dhq:family>Zsom</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Brown University</dhq:affiliation>
               <email>andras_zsom@brown.edu</email>
               <dhq:bio>
                  <p>Andras Zsom is a Lead Data Scientist and Adjunct Lecturer in Data Science at
                     Brown University. He completed his diploma at the Eotvos Lorand University in
                     Budapest, Hungary; then he completed his PhD at the Max Planck Institute for
                     Astronomy in Heidelberg, Germany. He was a postdoctoral associate at MIT before
                     joining Brown.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000481</idno>
            <idno type="volume">014</idno>
            <idno type="issue">3</idno>
                <date when="2020-09-25">25 September 2020</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                     >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change when="2020-07-01" who="murelj">Created file; began encoding</change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>This article proposes the concept of the pipeline as a category of tool that
               organizes a series of algorithms for users. The pipeline concept, adopted with
               limitations by the humanities, documents how a suite of algorithms produces a
               particular research result, with the goal of enabling interoperability, transparency,
               and iteration by future scholars who may switch out particular algorithms within the
               pipeline with different results. A pipeline-based application amplifies the concepts
               of interoperability and transparency for users by allowing the researcher to toggle
               on and off particular options, for example selecting and deselecting particular
               topics of interest from a program of visualizations based on a topic model of a large
               body of text. Pipelines support modular, interoperable, transparent, and documented
               processes of research that lend themselves to Prof. Guldi's Theory of Critical Search
               — the argument that critical thinking increasingly takes place at the design and
               research stage of digital processes. The article presents the case of how the
               pipeline concept influenced the development of <title rend="italic">HaToRI</title>
               (Hansard Topic Relevance Identifier), an open-source pipeline-based tool for
               identifying a cohort of thematically-linked passages in the nineteenth-century
               debates of Britain's parliament. In our pipeline, a series of algorithms move through
               the steps of cleaning a corpus, organizing them into topics, and selecting particular
               topics that are used to extract a sub-corpus that matches the user’s interests. Users
               have the option of searching based on multiple topics rather than merely keywords or
               a single topic at a time, allowing iterative searches to build upon each other. As an
               example of the Critical Search process in action, we follow an inquiry based on
               matching parliamentary reports with material from the Hansard British Parliamentary
               Debates. Using the pipeline, the user is able to identify multiple common topics of
               interest, and from these topics, extract a sub-corpus specific to land use and rent
               in the 19th century British Empire.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>The <soCalled>pipeline</soCalled>, a category of tool, and its influence on the
               development of the HaToRi is discussed in this article.</p>
         </dhq:teaser>
      </front>
      <body>
         <div>
            <head>Introduction</head>
            <p>In the era of web-enabled research, an enormous scholarly infrastructure of apps,
               software, and research portals structures how many scholars go about their research.
               This scholarly infrastructure comprises a significant portion of the <q>sites</q> (as
               this issue of <title rend="italic">Digital Humanities Quarterly</title> put it) that
               structure the modern meetings of minds. The design choices of that infrastructure
               have direct consequences on research. </p>
            <p>The developer’s choices, vision, and values shape the experience of knowledge, the
               movement of ideas, and the room for critical reflection on the interpretations
               offered by any scholar. Choices made in developing infrastructure govern how easy it
               is for students to adapt or build on the work of their professors, for scholars to
               make new discoveries about the same corpus, or members of the public to make new
               discoveries in an entirely new corpus. </p>
            <p>A world of interactive applications facilitating distant reading of texts already
               exists and many of these applications are organized in a way that allows a researcher
               to toggle endpoint options on and off, but choices upstream in the application
               development process are less available. Whether a piece of humanities infrastructure
               is pipeline-like plays a role in how far the results of one study can be extrapolated
               to another arena, for instance, how much effort is required to switch the British
               parliamentary debates out with the Swedish parliamentary debates, given the same
               infrastructure. This is not to say that pipelines are a panacea to interoperability
               challenges in digital humanities — many algorithms are specific to language or
               corpora. While pipelines allow scholars to apply methods to new data sets with little
               coding effort, the interpretation of results is a human task that still requires
               rigor.</p>
            <p>This article imports a concept (<q>the pipeline</q>) that has a specific meaning and
               use in the field of computer science. The pipeline becomes, in our argument, a metric
               for the kinds of online sites that digital humanists have built, which allows us to
               ask important questions about how our infrastructure facilitates transparency,
               exchange, and critical inspection of the results of research. Modularity,
               interoperability, and transparency are values in pipeline development and guide how
               we develop applications. </p>
            <p>We argue that the pinnacle of pipeline-building is when insights developed in one
               sphere can percolate to other spheres of knowledge with minimum translation — and
               that how we develop and code our scholarly infrastructure directly determines the
               options for collaboration, extrapolation, and interdisciplinary insight. We then
               present a case study of a web application that exemplifies some of the values
               inherent to a pipeline-based architecture. In a completely modular way, the
               application, called <title rend="quotes">Hansard Topic Relevance Identifier (<ref
                     target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref>)</title>,
               ingests the Hansard data set and presents the results of topic modeling and
               downstream analysis. </p>
            <p>Hansard is an archived and digitized resource of British Parliamentary speech going
               back to 1803. It contains full transcriptions of all spoken words as well as speaker
               and debate metadata. It is made up of hundreds of millions of words and the
               digitization has very few errors. As such, it is a well-studied and valuable resource
               for historical and socio-political research. </p>
            <p>Since its inception in 2003, Latent Dirichlet Allocation (LDA) has become a popular
               algorithm used to analyze large text corpora in Digital Humanities and beyond <ptr
                  target="#blei2009"/>. LDA is a particular flavor of topic models, which more
               broadly are probabilistic models for discovering the abstract concepts that occur in
               a large body of documents. The text is represented hierarchically, where a corpus is
               a collection of documents and in turn, a document is a collection of words. Topics
               are distributions over a fixed vocabulary of words and documents are made up of a
               mixture of topics. For example, an article in a newspaper could be about biology,
               neuroscience, computational techniques, and biochemistry and another article could be
               about politics, corruption, and finance. There might be other articles about the
               biological topic and more articles still about the political topic. The goal of topic
               modeling is to figure out what a large corpus is about and the results can be used
               for other text analysis tasks, like honing in on just the biological articles or
               tracing the prevalence of the political topic over time (given the corpus contains
               dates). </p>
         </div>
         <div>
            <head>The Pipeline Concept</head>
            <p>The concept of the <q>pipeline</q> is already prevalent in the literature of computer
               science, where pipelines organize suites of algorithms <ptr target="#butterfield2016"
               />. In code, a pipeline describes a series of algorithms that handle repeated
               processes — for example, data cleaning, stemming, topic modeling — in a series,
               handling completion of the previous task before moving on to the subsequent task. The
               omnipresence of pipelines in coding today is a relatively recent feature of the
               conditions of the available computational power over the last two decades, when
               pipelines, originally having been developed for use in supercomputers, came to be a
               feature of all high-power processes. </p>
            <p>The use of the term <q>pipeline</q> implied a series of steps, one leading on from
               the next, describing a purpose-built architecture for complex tasks, originally
               imagined after Henry Ford's assembly line <ptr target="#tucker2004"/>. For our
               purposes, the assembly line metaphor brings along with it the imagery of
                  <q>interchangeable parts</q>: that is, the ideal pipeline describes a process, and
               a product, where most of the parts can be swapped in and out, allowing for rapid
               fixes, upgrading, and comparison. When a pipeline is modular and open-source, if
               users disagree with our particular choices of algorithms, they can try other ones and
               assess how the change in algorithm affects the analysis. </p>
            <p>These qualities mark out a set of modern values appropriate to scholarly practice in
               a digital age, and they characterize coding practice around pipelines as a whole. The
               pipeline makes the process of going from raw text to topic modeling results modular,
               subject to dissent, and interchangeable with different corpora. It is worth looking
               at the three values in greater depth.</p>
            <list type="unordered">
               <item>By modular, we mean that an open-source pipeline insures that scholars can, at
                  all steps in the pipeline not just the endpoint, swap out choices or settings for
                  other ones. Modularity enables scholars to explore various results and arrive at
                  richer, more rigorous results. It also makes it possible to do this at little cost
                  to the researcher, in terms of time and resources needed to make and test the
                  changes.</item>
               <item>By subject to dissent, we mean that scholars may take issue with any individual
                  choice made in the construction of the pipeline. In order to intellectually engage
                  the implications of their disagreement on the research process, scholars must be
                  able to recreate alternative results that would have happened had one of the
                  algorithms in the pipeline been switched out — for instance, had a different
                  choice of stemmer or topic modeling algorithm been employed. In this process of
                  openness to dissent, the pipeline becomes a tool for exploring best practices in
                  digital research, where scholarly disagreements about tools and interpretation can
                  be pinpointed for discussion and turned into instructive examples. </item>
               <item>By interchangeable, we mean that once an open-source pipeline is deemed useful
                  for humanities and social science knowledge, other scholars may want to apply the
                  insights from a research process to other bodies of text. Where no scholarly
                  process exists for packaging tools into a pipeline, other scholars inspired by a
                  research process must recreate the process from scratch. Where research is
                  packaged as a pipeline, however, switching a few lines of code makes it possible
                  for the scholar to apply one pipeline from the British Hansard debates to the
                  Canadian or Australian Parliamentary debates, or any other text corpora.
                  Interchangeability is a scholarly value because it enables the extension of
                  insights from one domain of humanities or social science research into another
                  domain.</item>
            </list>
         </div>
         <div>
            <head>Previous Waves of Critical Thinking about Humanities Infrastructure</head>
            <p>A previous wave of efforts to bring structure to digital humanities research has
               foregrounded humanities infrastructure as a site of making. This literature has dealt
               with both the physical and organizational infrastructure of research <ptr
                  target="#svensson2011"/>, and data infrastructures <ptr target="#edmond2015"/>
               <ptr target="#brooke2015"/>. Brooke’s GutenTag, for example, is a software that
               samples sub-corpora from the Gutenberg corpus based on publication dates, gender of
               the author, etc., and adds user-selected tags to the selection. Some, but not all of
               this research has suggested a formal set of values for analysis. Mattern argues that
               whether infrastructure is hard (roads, railways, bridges, data centers, fiber-optic
               cables, e-waste handlers, etc.) or soft (measurement standards, technical protocols,
               naming conventions, etc.), digital humanists among other interfacers with
               infrastructure should have some literacy around them, including the values that
               underlie them including transparency and modularity. Matterns also argues that
               infrastructure is not a revolutionary concept (though it has evolved since its
               inception in the 1920’s), but allows people to <quote rend="inline">organize into
                  communities and share resources amongst themselves.</quote> It is this
               self-organizing quality that necessitates better interfaces that reflect how
               communities want to and should interact with structures <ptr target="#mattern2016"
               />.</p>
            <p>Discussions on the biases in the process of big data research can be found in the
               literature of the past decade. A primary example is the idea that racist data leads
               to racist algorithms. Noble challenges the idea that scientific research is
               impartial, that it levels the playing field for <quote rend="inline">all forms of
                  ideas, identities, and activities</quote>
               <ptr target="#noble2018"/>. Negative biases are embedded in search engine results for
                  <q>black girls,</q> where suggested searches are radically different from those
               for <q>white girls.</q> This results in biased search algorithms that privilege
               whiteness and oppress people of color. There is an increasing awareness of this issue
               - O’Neil et al. lay out situations where algorithms have the potential to amplify the
               biases and exacerbate the disparities present in society. Arenas such as going to
               college, online advertising, justice and law enforcement, getting a job and job
               performance, getting credit, and getting insurance can all be dangerously affected by
               algorithms’ increasing role in the decision making process <ptr
                  target="#dallessandro2016"/>. O’Neil calls for auditing in all steps of the data
               science development process <ptr target="#oneil2016"/>. Additional research in the
               field of bias, discrimination, and oppression in algorithms and what to do about them
               are abundant <ptr target="#eubanks2015"/>
               <ptr target="#introna2006"/>
               <ptr target="#nissenbaum2010"/>
               <ptr target="#vaidhyanathan2012"/>
               <ptr target="#vaidhyanathan2018"/>. Kaplan, in his map of big data research in
               digital humanities, briefly raises the question of bias and notes that choices need
               to be made in the process of digitally translating from primary sources into
               high-level human-processible insights and that the inevitable biases that result from
               these choices apply <ptr target="#kaplan2015"/>. We support a solution, not by
               codifying any particular set of standards, but by allowing these choices to be
               questioned in all steps in the pipeline, and changed directly by the questioner. </p>
            <p>As a result, the pipeline values of modularity, subjectness to dissent, and
               interchangeability is apparent in many of the web portals available today — but not
               in all. Perhaps the work that most closely mirrors our viewpoint is Edmond’s Data
               Soup, which describes an architecture and pipeline for text search and selection used
               in the <ptr target="#edmond2013"/> project with an emphasis on modularity of the
               processing algorithms and reproducibility. These works, however, deal mainly with the
               process of data preparation and selection, rather than the whole process of research,
               all the while treating it primarily as something to be done, rather than something to
               think about, discuss, and debate. Often times, in Digital Humanities, the description
               of the process is sidelined to the appendix or not discussed at all, which precludes
               any possibility of critical review of the pipeline. In fact, in a survey of close and
               distant reading techniques, <ptr target="#janicke2015"/> found that many papers using
               sophisticated data analysis techniques do not even provide sufficient information
               about the preprocessing steps to be reproducible, let alone facilitate discussions on
               these processes. We argue that it is not enough for the data analysis tool to be
               sophisticated and user-friendly, but that all steps in the data analysis pipeline be
               truly transparent and modular. Mattern argues for a collective consciousness of
                  <quote rend="inline">citizens/users</quote> of infrastructure around both
               awareness of and critical listening and thinking around infrastructure, implicitly
               making an argument for these values <ptr target="#mattern2013"/>. </p>
            <p>Mattern references <quote rend="inline">path dependency,</quote> a concept coined by
               Edwards et al., where past decisions limit future choices. While this is true of
               software, open source software gets around this dependency because the code is shared
               freely, with any infinite number of branches or forks possible from the original base
               code. If a scholar or user dislikes a past decision, they can change it so that
               future choices are not limited by a previous developer’s choices. Thus, the software
               not only is transparent and modular and subject to dissent, but allows scholars and
               users to go beyond dissent and towards action and agency to diverge from the
               path.</p>
            <p>It is advantageous to the community of researchers to discuss ideas about
               infrastructural choices built into a particular pipeline. If we do collectively adopt
               the convention of publishing articles wherein we describe the pipelines we have
               designed — and perhaps other articles wherein we critique recent conventions in
               building infrastructure — then the community of knowledge will benefit by
               collectively learning. The author can describe the work that went into designing a
               pipeline that maximizes critical thinking. Places in the research project where the
               best algorithm is uncertain — the places that lend themselves to critical review —
               can be described and highlighted, and the community of researchers can grow in its
               awareness of these uncertainties and the biases they create on the research project.
               Herein lies a major opportunity for collective critique and argumentation around the
               ideas that structure our infrastructure.</p>
         </div>
         <div>
            <head>The Pipeline-Based Web App</head>
            <p> In most web applications in the digital humanities, some pipeline exists in the
               background, but some of its functions are obscured from the user. The visualizations
               displayed are based on the results of cleaning and topic modeling run behind the
               scenes and then uploaded to the web app for the researcher to interact with. Simple
               navigation by date or keyword allows the user to expand or explore particular aspects
               of the results. Toggle options exist, but they are limited to later parts in the
               pipeline. For example, in the VoyantTools web application, we can upload any text we
               want and toggle visualization settings, but there are no options for how the text is
               cleaned and tokenized and furthermore, the way these normalization steps are done is
               obscured from the user <ptr target="#sinclair2016"/>. InPhO Topic Viewer, one of the
               HathiTrust Research Center algorithms, is a more modular pipeline than VoyantTools
               because it provides some options in the text pre-processing and normalization parts
               of the pipeline. However, the only two options are the choice of tokenizer (of which
               there are only two useful selections for English texts) and whether or not to decode
               unicode characters <ptr target="#murdock2017"/>. </p>
            <p>In web applications inspired by the pipeline concept, the designers seek to highlight
               and expand particular choices made in all steps of the design of the pipeline, giving
               the user options for how the data is handled and presented. In an ideal
               pipeline-based app, the user would have control over every process in the pipeline,
               from choosing different stemming and cleaning options to visualizations, allowing the
               community to upload different possible algorithms. </p>
            <p>By giving users the option of reviewing the choices behind any given analysis and
               visualization, the design principle of the pipeline-based app reinforces the
               possibility of scholarly dissent in analyzing and interpreting documents. Two
               scholars may have different instincts when it comes to how to clean the data, which
               topics are relevant to a query, or how to visualize the data, and these different
               instincts may lead them in different ways. To encourage a healthy climate of dissent
               and investigation, practitioners in the digital humanities need to incorporate design
               opportunities for dissent and counter-inspection of the evidence into the full
               pipeline of their software. </p>
            <p> In a sense, then, valuing scholarly dissent means that interface designers must
               aspire to web-apps that perform like <emph>code</emph> in terms of their flexibility
               and interoperability, making every choice of algorithm transparent and
               interchangeable. The ideal pipeline-based application would, in a sense, lead the
               user through the experience of coding, where the coder typically chooses various
               packages that are assembled into a process of cleaning, analyzing, and visualization.
               One example of a pipeline-based tool is Jean-Philippe Cointet’s Cortext,<note><ref
                     target="https://docs.cortext.net/">https://docs.cortext.net/</ref></note> a
               platform that allows users to upload textual documents and queue them for cleaning
               and various transformations including named entity recognition, topic modeling, and
               vector analysis <ptr target="#rule2015"/>.</p>
            <p>Pure pipeline-based tools have both advantages and disadvantages. The advantage, like
               the advantage of the pipeline itself, is the proliferation of the values of
               modularity, openness to dissent, and interoperability. Pipeline-based tools like
               Cortext recreate the experience of interactive, iterative coding with data for
               non-coders and mixed classrooms or readerships where not all who want to experience
               the data come from an equal background with respect to code. </p>
            <p>The major disadvantage of existing pipeline-based tools are, generally speaking,
               features of first-wave design. For instance, some web interfaces designed by computer
               specialists often lack an eye for design principles that would make their use
               transparent to users, which results in interfaces that are cumbersome to use, for
               instance in the case of Zotero. Zotero is an application for curating, storing, and
               organizing journal articles, but it is possible to build text mining and
               visualization on top of it. With Zotero, a user can add or remove journal articles
               from their hand-curated collection, topic model their corpus, and view some results.
               Changing the inputs is trivial, but changing the parameters in modeling and
               visualization are not. In Zotero, the emphasis is on interoperability, not
               user-friendliness, with the result that some users may be put off an artificially
               abstract interface. Other web interfaces command limited access to computational
               resources, which results in the case of Cortext in long waiting times while users
               queue for a server. These are issues that could be remedied by later design and
               improvement. However, for designers to rebuild existing infrastructure to align with
               new principles, they would need access to grants that emphasize extending access to
               computational resources and usability. </p>
            <p>Tool-builders who aspire to pipeline-level transparency and easy user interface may
               nonetheless adopt the principles of openness to dissent and interchangeability in
               certain <emph>parts</emph> of the research process, with the result of creating web
               interfaces that are more flexible, generative, and useful for scholarly debate than
               the first generation of web applications in the digital humanities. This article
               presents one such iteration, <ref
                  target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref>, which aspires
               to the pipeline concept. It realizes this promise by transparency — that is, by
               documenting the background pipeline of its code for users — and by limited
               modularity, that is, allowing the user to choose different seed topics and tweak the
               z-value of our sorting algorithm. Transparency and open-source software give users
               options: if they want to change other parts of the pipeline, like for example
               lemmatizing rather than stemming, the code should be run again but with that
               parameter changed. </p>
         </div>
         <div>
            <head>Introducing HaToRI, a Pipeline for modeling the British Parliamentary
               Debates</head>
            <p><ref target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> is the
               Hansard instance of the <q>ToRI</q> (Topic Relevance Identifier) tool. Its creation
               was motivated by the research questions asked of the Hansard British Parliamentary
               Debates data set by our scholarly collaborator, but it was built with a generic text
               analysis pipeline in mind. ToRI exemplifies the values of the pipeline concept in
               various ways. At the highest level, it is applicable to any text corpus with trivial
               customizations. For example, if a user likes all of the visualizations and features
               available in the <ref target="https://brown-ccv.github.io/hatori/page/home/"
                  >HaToRI</ref> app but wants to create an instance of ToRI analyzing Twitter data,
               we can spin up a <q>TweetToRI</q> web app with slight modifications to the underlying
               ToRI code pipeline. </p>
            <p>The code pipeline consists of three steps: we prepare and identify the topics of a
               corpus, a humanist selects topic(s) of interest, and we post-process the results.
               Post-processing consists of ranking the documents based on how prevalent the topic(s)
               of interest are, and using such a ranking for in-depth studies. We prepare a set of
               visualizations to illustrate how the topics cluster based on similarity, how their
               prevalence changed over time, etc. The choices that we made in each of the three
               steps were informed by the historical research aims and by the content and format of
               the Hansard data, but may not be the right choices for all data. They can be adjusted
               with ease for other data or research aims. For this reason, we believe that the
               pipeline driven web tool can greatly aid humanists in their research.</p>
            <p>The ToRI web app takes inspiration from many other useful Natural Language Processing
               (NLP) tools, most notably the Topic Explorer <ptr target="#goldstone2014"/>, but is
               distinguishable in its highly customizable end-to-end pipeline and novel document
               ranking algorithm. Many tools, like VoyantTools and OldBaileyVoices.org, are limited
               to basic keyword-based text analysis and corpus exploration <ptr
                  target="#sinclair2016"/>. And while VoyantTools can accommodate various corpora,
               it does not accept a tab-separated value format that is ideal for corpora like
               Hansard that contain structured fields. Other tools explore and extract semantic
               sub-corpora by ranking documents <ptr target="#tangherlini2013"/>
               <ptr target="#goldstone2014"/>, but use similarity to a single document or topic as
               the sorting value. Lengthy documents, in particular, can contain too much noise and a
               single topic does not capture the nuance that multiple topics can, resulting in a
               sub-corpus that potentially contains many false positives and leaves out harder to
               detect true positives. Meandre is a workflow tool that is modular and customizable to
               many corpora and offers a drag-and-drop interface for non-technical users to perform
               many NLP tasks including data cleaning, topic modeling, sentiment analysis, etc. <ptr
                  target="#llora2008"/>. While it is user-friendly and feature-rich, it does not
               offer sub-corpus extraction using multiple topics.</p>
            <p>The advantages of our pipeline over other corpus exploration tools are that they are
               self-contained and complete, modular, and open source. These qualities make the code
               generalizable to any corpus, in any format and highly adaptable to incorporate other
               algorithms or NLP decisions. Specifically, if a user does not agree with our choice
               of the stemming method (Snowball Stemmer), they can add a different algorithm (e.g.,
               the Porter Stemmer) by changing two lines of code and then assess how the change
               influences the results.</p>
            <p>Creating instances of the web app with other corpora requires the addition of a
               single, custom pre-processing script that converts the corpus to our generic data
               format. The generic format is a tab-separated file with three fields: <term>Document
                  ID</term>, <term>Text</term>, and <term>Year</term>. The <term>Document ID</term>
               can be any identifier such as a sequence of integers or document titles if available
               and it needs to uniquely identify the documents. The <term>Text</term> field contains
               the full, natural text of the documents, prior to cleaning and stemming. Then, the
               rest of the pipeline can run without any modifications. The pipeline performs the
               complete set of tasks needed to transform natural language to machine-readable
               vectors, a noisy collection of words to a hierarchically sorted set of topics, and an
               impossibly large volume of text to a manageable sub-corpus of semantically relevant
               documents. </p>
            <p>Additionally, the code is modular so the work required to make changes to the
               pipeline is trivial. The decisions we made at all points in the pipeline — such as
               our custom stopword list, our spell-checking dictionary, our decision to stem rather
               than lemmatize, topic modeling method and implementation, number of topics, document
               ranking algorithm, and z-value — can be customized to different corpora or user
               interests. The code is designed this way to make space for flexibility in
               decision-making; algorithms best suited for one corpus or domain may not be optimal
               for another corpus or domain. Furthermore, some of these decisions are built into the
               endpoint of the pipeline, the web app user interface, with no code changes required.
               For example, the z-value is a tune-able parameter that the user interacts with in a
               value entry bar on the detailed topic view pages of the web app. Future work will
               incorporate more user input at the endpoint. <note>The code is open-sourced (MIT
                  license), so it is free to use, modify, and share. The pipeline code can be found
                  at <ref
                     target="https://github.com/brown-data-science/inquiry-for-philologic-analysis"
                     >https://github.com/brown-data-science/inquiry-for-philologic-analysis</ref>,
                  and the web-app code, both front end and back end, at <ref
                     target="https://github.com/brown-data-science/hansard_api"
                     >https://github.com/brown-data-science/hansard_api</ref>. The visualizations
                  and front end are built using Javascript frameworks (<code>d3.js</code> and
                     <code>vue.js</code>) and the back end is deployed using Docker
                  containerization. The web app is hosted on github.io at <ref
                     target="https://eight1911.github.io/hansard/"
                     >https://eight1911.github.io/hansard</ref>.</note></p>
         </div>
         <div>
            <head>Comparing Applications: How Pipelines Broaden the Scholarly Implications of
               Research </head>
            <p>The Hansard parliamentary debates are important information mines for scholars of
               British history and attempts to digitize Hansard date back to at least the 1970s.
               Open source versions of the nineteenth-century Hansard corpus have been freely
               available in a digitized version for at least a decade, which has made them the
               subject of a great deal of infrastructure design already. At least two applications
               allow users access to the debates today — the Hansard Corpus site, designed by a team
               of linguists at Glasgow and Brigham Young, and the Millbank Hansard site,
               commissioned by parliament itself of a private developer. The two existing
               applications have already enabled a flood of new research about language use in
               parliament <ptr target="#blaxill2013"/>
               <ptr target="#alexander2017"/>. </p>
            <p>However, the first generation of applications made little use of the pipeline
               concept. In many cases, little documentation was available about how the corpus had
               been cleaned. Users were allowed to search by keyword, year, and (in some cases)
               speaker or other named entity, but few tools offered other ways of navigating the
               corpus or matching a user’s interests. Hansard, thus, offers an ideal test-case for
               building a web application that models the advantages of modularity, openness to
               dissent, and interoperability, due, in part, to the heavy scholarly interest in the
               subject matter; the material covered by Hansard is the subject of interest in
               history, literature, linguistics, and geography. Moreover, an interoperable pipeline
               or pipeline-based app for working with the British Hansards could be easily extended
               to the Australian, Irish, and Canadian Hansards, the debates of the EU parliament, or
               any other digital corpus that represents the debates of a democratic body.</p>
         </div>
         <div>
            <head>Interoperability: How to Pipe HaToRI to Other Corpora</head>
            <p>Unlike applications that readily ingest text data in the web browser without ever
               having to touch the underlying code, <ref
                  target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> requires some
               programming knowledge to adapt it to other corpora. The advantage of this approach is
               that the corpus can have associated structure and metadata beyond a single document
               of plain text. Additionally, minor code changes can be made in the process of
               spinning up another instance of ToRI, making the intermediary steps between data
               ingest and data visualization completely modular. To spin up an instance of ToRI for
               another corpus, clone the Inquiry for Philologic Analysis repository from Github and
               make modifications to the code in the <term>src</term> folder.</p>
            <p>The pipeline is set up in <code>main.py</code>. Each step in the pipeline can be
               toggled on or off by setting the switches at the top of the script. For example, to
               run the first step of the pipeline on the test data given, the user can toggle the
                  <code>to_tsv</code> switch to <q>True</q>, and the sample 10 Hansard data will be
               converted to a tabular, tab-separated value (TSV) file. To ingest another corpus, the
               user must first convert the raw text to a TSV file. The TSV must represent a corpus
               as one row per document, and each document must have a unique identifier associated
               with it. The identifier is the first column in the TSV file and the text of the
               documents should be the second column. Metadata is optional - for <ref
                  target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> the speaker
               name and debate year were added as metadata by appending them as columns three and
               four. This step is unique to each data set - for Hansard we wrote a code
                  (<code>raw_corpus2tsv.py</code>) to parse the data we needed from XML to TSV. For
               other corpora, the code will need to be custom written for the format it originates
               in.</p>
            <p>The next step, data cleaning, is a single script (<code>preprocess.py</code>) that
               can be run as is, changed with minor code edits to accommodate different scholarly
               choices or domain-specific corpora, or replaced by a custom preprocessing script
               entirely. Tokenization is by word, but can be changed to ngrams (n-length word
               phrases). Non-alphanumeric characters are removed, but perhaps punctuation and
               special characters are important, such as with a Twitter data set. All text is
               lower-cased, though with texts with many abbreviations like medical notes,
               capitalization could be preserved. Spell-checking can be turned from on to off, and
               word truncating can be changed from stemming to lemmatization, or turned off
               completely. Common as well as custom stopwords are removed, and either or both
               stopwords lists can be turned off or modified. Once cleaning choices have been made
               and run, the user must download the MALLET program and run topic modeling with the
               desired numbers of topics, k. The output of the topic models are then ready to be
               loaded into the website. </p>
            <p>There are two steps to creating the website: uploading the corpus and putting the
               website online. The user should clone the <term>hatori</term> repository from Github
               and change out the data in the <term>serv/data/</term> folder with their own corpus.
               Then, once the website works locally, they should host the website using one of a
               variety of web hosting services. We suggest doing this through Github Pages<note><ref
                     target="https://pages.github.com/">https://pages.github.com/</ref></note>
               because of its integration with Github.</p>
         </div>
         <div>
            <head>Methods: the Pipeline Outlined</head>
            <p>The remainder of this paper describes a full NLP pipeline for sub-corpus extraction
               using topic-based search and tests the hypothesis that topics are better than
               keywords for discovering a thematically coherent sub-corpus that includes
               under-studied documents. We will describe the pipeline’s machinery in detail, which
               includes custom data extraction and pre-processing, conversion of the text corpus to
               a numeric and machine-readable data structure, topic modeling the corpus, document
               ranking and sub-corpus extraction, and visualization in a web app. We give an example
               scholarly use case to show how features of the web app work, and how the pipeline
               enables reproducible and interoperable digital humanities research and openness to
               dissent as a way of collaboration.</p>
            <p>In characterizing the pipeline, we give a transparent description of the entire
               research project from raw text file (in this case; in others, it might be scanned
               documents) to visualizations. In emulating the value of transparency, this pipeline
               description highlights the opportunities for critical thinking along the way and how
               scholars can engage them. Indeed, we believe that this kind of description, which
               both illustrates the facts about the corpus and highlights the places for
               interpretive work by scholar-users, should become the standard for scholarly
               publications that document new scholarly infrastructures in the humanities. </p>
            <div>
               <head>Pre-processing</head>
               <p>Here, we describe the process by which we turned the raw Hansard parliamentary
                  debates into machine-readable material for downstream steps in the pipeline. The
                  process described contains choices specific to the Hansard dataset, but it is
                  modular and easily interchanged with a different suite of pre-processing
                  steps.</p>
               <p>We download Hansard from the web as a series of XML files<note><ref
                        target="http://www.hansard-archive.parliament.uk/"
                        >http://www.hansard-archive.parliament.uk/</ref></note> and parse full text
                  and metadata from the XML into a tabular, tab-separated value (TSV) format using
                  the Python programming language. The input data could be any collection of
                  documents (news articles, tweets, political speech, etc.) in any text format
                  (cannot be images or scans of documents), so long as it is converted into a TSV
                  structured such that each row is a document. In the Hansard TSV file, each row is
                  a debate and the columns are <term>debate ID</term>, <term>full text</term>, and
                     <term>metadata</term>. We additionally append four written reports crucial to
                  the historical analysis to our corpus. The result is a corpus made up of 45,585
                  speakers who uttered 294,203,233 words, 1,033,536 speech acts, and 111,689
                  debates. </p>
               <p>Figure 1 shows what parliamentary speech and the written reports look like over
                  time. The number of debates per year correlates closely with the number of
                  speakers per year, and they both increase over the course of the century with a
                  notable spike in the last quarter century. The average length of our documents
                  shows a weak inverse correlation. The number of speech acts and the number of
                  unigrams per debate are both somewhat reduced in the last quarter of the century.
                  In other words, as the number of debates and number of speakers increase, the
                  speech acts tend to get less verbose over time.</p>
               <figure>
                  <head>The number of debates and the number of speakers stayed relatively constant
                     until the last quarter of the century when there was a sharp increase in both.
                     While there are more debates and more speakers, the speakers were less verbose
                     during the last quarter century.</head>
                  <graphic url="resources/images/figure01.png"/>
               </figure>
               <p>As is standard in Natural Language Processing, we took steps to pre-process the
                  corpus to prepare the text for downstream analysis. These steps include
                  tokenization, removing non-alphanumeric characters, lowercasing, spell-checking,
                  replacing both common stopwords and custom stopwords with a substitute word, and
                  stemming. We walk through each step in detail during the rest of this subsection. </p>
               <list type="unordered">
                  <item>We lowercase the corpus and then tokenize the documents by splitting the
                     text on whitespace (e.g. spaces, tabs, returns). There are a variety of
                     tokenizers to choose from (e.g. Penn Treebank, Punkt, Multi-Word Expression,
                     Tweet, Regular Expressions, etc.) and we chose whitespace tokenization based on
                     the structure of Hansard. This reduces our unit of text from paragraphs to
                     words. Our corpus, Hansard, consists of 198,338 unique words.</item>
                  <item>We remove non-alphanumeric characters from each word, so they are free from
                     punctuation, white space, and symbols. This reduces the vocabulary to 186,060
                     words. Other text corpora may require the removal of different characters, like
                     emoji from tweets or html tags from data scraped from the web. Changes to the
                     removal list are trivial to implement in the code pipeline. </item>
                  <item>We spellcheck the words using a dictionary of British English words provided
                     by the Python library, <title rend="italic">enchant</title>. If the words in
                     our vocabulary are in the dictionary, we keep the word; if they are not, we
                     replace them with a substitute word that does not appear natively in our
                     corpus. This reduces the vocabulary to 49,789 words. We also replace common
                     stopwords (e.g. articles and conjunctions like the, and, or, it, etc. that have
                     no semantic meaning) and custom stopwords (proper nouns) with a substitute
                     word. Replacing stopwords reduces our vocabulary by less than one thousand
                     words. We can, with minimal effort, swap out the British English dictionary for
                     other language dictionaries or change the stopwords used.</item>
                  <item>We stem the words using Snowball Stemmer in the <title rend="italic"
                        >NLTK</title> Python library. Stemming further decreases the size of the
                     vocabulary by reducing inflected and derived words to their stem (base or root
                     form) to avoid duplicate counting words that may have slightly different
                     endings but the same semantic meaning. <q>Rent</q>, <q>rents</q>,
                     <q>rented</q>, and <q>renting</q> would all be reduced to the stem,
                     <q>rent</q>, and <q>property</q> and <q>properties</q> would both be reduced to
                     the stem, <q>properti</q>. We call the stemmed words unigrams because stems
                     like <q>properti</q> are not words. There are a number of different stemmers
                     and lemmatizers to choose from, should the Snowball Stemmer be called into
                     question.</item>
               </list>
               <p>By the end of the process, the original 294 million words are reduced to 97
                  million. Almost 200 million words are replaced by the substitute word because they
                  are common stopwords (for example: <q>a</q>, <q>an</q>, <q>the</q>, <q>I</q>,
                     <q>you</q>) or misspelled words. While the reduction seems significant,
                  substitute words would contribute very little to the topic modeling results and
                  thus minimal information is lost during the process. The final step in
                  pre-processing is to create a document-term matrix, which is a numerical
                  representation of the corpus that can be read and analyzed by a machine. Each
                  document is represented by a vector of word counts. Different weights and
                  normalizations can be applied to the document-term matrix, given the needs of the
                  data or research.</p>
               <table>
                  <head>We illustrate how the various pre-processing steps reduces the number of
                     unique unigrams in Hansard by a factor of 10. We get the greatest reduction in
                     unique unigrams by spell-checking and stemming words.</head>
                  <row role="label">
                     <cell>Pre-processing Step</cell>
                     <cell>Number of unigrams</cell>
                     <cell>Number of non-substitute unigrams</cell>
                  </row>
                  <row role="data">
                     <cell>All unigrams</cell>
                     <cell>198,338</cell>
                     <cell>196,642</cell>
                  </row>
                  <row role="data">
                     <cell>Alphanumeric characters only</cell>
                     <cell>186,060</cell>
                     <cell>184,447</cell>
                  </row>
                  <row role="data">
                     <cell>Correctly spelled unigrams</cell>
                     <cell>49,789</cell>
                     <cell>49,190</cell>
                  </row>
                  <row role="data">
                     <cell>Stems</cell>
                     <cell>21,828</cell>
                     <cell>21,444</cell>
                  </row>
               </table>

            </div>
            <div>
               <head>Topic Modeling</head>
               <p>The goal of applying NLP methods to Hansard is to answer historical questions
                  about the shifting concepts of land ownership and property in the
                  nineteenth-century British empire. Traditional historical scholarship involving
                  close reading and analysis is not suited to this task because the text corpus is
                  too large to be read by a single historian or even a team of researchers. Instead,
                  we offload the task to a computer.</p>
               <p>Topic models are probabilistic models for discovering the abstract topics that
                  occur in a large corpus of documents through a hierarchical analysis of the text
                     <ptr target="#blei2009"/>. The idea is that a corpus is made up of topics,
                  topics are distributions over a fixed vocabulary of terms, and documents are
                  mixtures of topics in different proportions. The vocabulary of terms can be words
                  or pre-processed tokens like unigrams or n-grams. Topic modeling can answer
                  questions that would otherwise be intractable with a large corpus: (1.) What is a
                  large, prohibitively long corpus of documents about? (2.) Can we extract a
                  human-readable sub-corpus of semantically-related documents? A random process is
                  assumed to have produced the observed data, which are the words that make up the
                  documents in the corpus. Given the observed data, the posterior distribution of
                  the hidden variables (word distributions for each topic, topic proportions for
                  each document, topic assignment per word for each document) determines the hidden
                  topical structure of the corpus which tells us what the corpus is about, answering
                  the first question we seek to solve with topic modeling; the posterior estimates
                  can be applied to tasks such as document browsing and information retrieval,
                  answering our second question. In fact, topic modeling has been used as the basis
                  for <q>trawling</q> for a smaller corpus <ptr target="#tangherlini2013"/>.</p>
               <p>The evaluation of topic modeling results is subjective, requiring manual
                  inspection by subject matter experts and cannot replace the traditional textual
                  analysis. The benefit, however, is that the subject matter experts need only to
                  read through a handful of terms per topic rather than a significant part of the
                  corpus. The drawback is that we lose vital information encoded in natural language
                  when we numericize it. For example, the bag-of-words representation of a corpus
                  loses context entirely by treating documents as unordered collections of words
                     <ptr target="#harris1954"/>; n-grams cannot preserve word relationships past
                  their immediate neighbors <ptr target="#manning1999"/>; and while vector models
                  like word2vec and doc2vec preserve far-apart word relationships, they are not
                  suitable for identifying a relevant sub-corpus within a large corpus <ptr
                     target="#mikolov2013"/>
                  <ptr target="#le2014"/>. </p>
               <p>We use a particular flavor of topic modeling called Latent Dirichlet Allocation
                  (LDA) <ptr target="#blei2003"/>. The model does not have a prior notion about the
                  existence of the topics; it is given a hyperparameter, k, which describes how many
                  topics are associated with the corpus, and discovers the k topics from the
                  observed data, the words in the documents. The algorithm is implemented in a
                  number of programming languages and is simple to use with few tunable parameters,
                  but requires hand labeling of topics by subject matter experts. We use a Java
                  implementation of LDA called MAchine Learning for LanguagE Toolkit (MALLET) <ptr
                     target="#mccallum2002"/>. In developing the pipeline, we made topic models with
                  varying k values (50, 100, 200, 500, 1000) and different topic modeling methods
                  like Non-negative Matrix Factorization <ptr target="#berry2005"/> and Dynamic
                  Topic Models <ptr target="#blei2006"/> available to the users.</p>
            </div>
            <div>
               <head>Extracting a Sub-corpus</head>
               <p>Within <title rend="italic"><ref
                        target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref></title>,
                  the process of extracting a subcorpus starts with LDA. LDA not only tells us the
                  topics that are discussed in a corpus, but also which ones each document contains.
                  Knowing what documents are about allows us to narrow our focus to a smaller
                  sub-corpus from the whole of Hansard, reading the debates and speech acts that are
                  about our topic of interest — land and property. Sub-corpus extraction using topic
                  modeling allows us to find a richer set of documents than keyword search alone
                  because the topics are discovered from the data, rather than imposing
                  domain-specific and subjective knowledge on the text. Keyword searches and term
                  frequencies work well when the topic of interest can be unambiguously identified
                  by one word or a short phrase. Documents in a corpus can be clustered based on
                  similarity to one another and subject matter experts need to analyze each cluster
                  to decide if the clusters make semantic sense and if any are about their topic of
                  interest. This can only be done if the corpus is not too large and the experts can
                  at the very least scan through each document. Sometimes it is already known that
                  one or a few documents in a corpus are about the topic of interest and the
                  question is whether there are other so far undiscovered relevant documents in the
                  corpus? The goal then is to find documents which are similar to the seed
                  documents. A search based on seed documents works if most words and terms in the
                  seeds are about the topic of interest and not other general topics. This is not
                  often the case. Our proposed method for sub-corpus extraction is a
                  topic-model-based document ranking process where users choose multiple topics of
                  interest, rank documents by how closely they match the topics of interest, explore
                  the results, and iterate by toggling on or off particular topics. We harness a
                  code pipeline to build a highly customizable web-app that makes available to the
                  ordinary user a process of Critical Search characterized by iterative interaction
                  with large corpora <ptr target="#guldi2018"/>. </p>
               <p>We use three steps in topic modeling: import, modeling, and post-processing. The
                  first two steps are fairly straightforward parts of the MALLET program. In
                  modeling, we tune the number of topics, keeping other hyperparameters (e.g. alpha
                  and beta (priors), number of iterations, sampling method, etc.) constant. We
                  created topic models with the number of topics equal to 100, 200, 500, and 1000.
                  In post-processing, we asked a group of humanists to read the ten most heavily
                  weighted words in each topic, give the topic labels based on those words, and
                  qualitatively evaluate which k number of topics is ideal for Hansard (see the
                  Results section for more details). </p>
               <p>Sub-corpus extraction is done by ranking the documents in a corpus using their
                  relevance to one or more topics of interest. The simplest ranking algorithm counts
                  the relative frequency of topic words, or word occurrences assigned to at least
                  one topic of interest, and ranks the documents with the highest frequencies as
                  most relevant. However, this algorithm can incorrectly down-rank longer documents.
                  We improve on this simple ranking algorithm by using the lower bound of Wilson
                  score confidence intervals for a Bernoulli parameter, z, as the sorting value <ptr
                     target="#agresti1998"/>. The parameter z is tuneable and a larger z
                  sporadically introduces longer documents into the top-ranked sub-corpus; in our
                  web app, the user can visually explore how increasing z changes the rankings by
                  reading the document titles and linking to the full text of the documents.
                  Different values of z may be optimal for sub-corpus extraction depending on the
                  topics of interest and the user’s goals. </p>
               <p>We argue that topic-based sub-corpus extraction is a highly effective way to
                  identify a relevant sub-corpus and highlight this point in the context of
                  exploring past land use and eviction in the British Parliamentary debates. </p>
            </div>
         </div>
         <div>
            <head>Use Case: Searching for Property in Hansard</head>
            <p>Parliamentary speech has been carefully archived since 1803 and is available as a
               digitized resource called the Parliamentary Debates; it contains full transcriptions
               of all of the spoken words in the British Parliament from 1803 to 1908 as well as
               metadata like speaker name, speaker constituency, date of speech, and debate
                     titles.<note><ref target="http://www.hansard-archive.parliament.uk/"
                     >http://www.hansard-archive.parliament.uk/</ref></note> The collection is
               commonly referred to as Hansard after Thomas Curson Hansard, a London publisher and
               first official printer to the parliament. We use the terms Parliamentary Debates and
               Hansard interchangeably throughout this paper. Hansard provides a rich corpus for
               text analysis using Natural Language Processing methods because its digitization is
               high quality (low errors in optical character recognition) and it is comprehensive,
               spanning over two centuries and hundreds of millions of words of speech.</p>
            <figure>
               <head>Scanned PDF images of the Parliamentary Debates. The left image shows some of
                  the metadata captured in the records, such as title, volume number, date, and
                  house of parliament. The right image shows some speech acts, separated by debate
                  topic and by speaker.</head>
               <graphic url="resources/images/figure02.jpg"/>
            </figure>

            <p>Because of the volume and breadth of topics in Hansard, it is a valuable resource for
               historical and socio-political analyses by students and scholars, alike. And while
               there is great value in being publicly available in its entirety, it is difficult to
               narrow down to smaller excerpts of interest in its existing online forms. We create a
               pipeline that ingests the Hansard data and presents it in a new digital space, the
                  <ref target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> web app,
               that broadens the horizons of possibility for digital humanities research and
               collaboration. We present a case study in using the web app for a historical analysis
               of how ideas about land and property changed in the nineteenth-century British empire
               in the following sections. However, the tool is modular and can be applied with some
               modification to any number of analyses by selecting a different focal point other
               than the land topics or swapping out the Hansard corpus for another collection of
               text.</p>
            <p>While historical scholarship on Hansard is rich and varied due to the completeness of
               the resource, there is a lack of research about the changing discourse of property in
               Hansard. In contrast, there is extensive research on four written reports specific to
               land and property <ptr target="#bull1996"/>
               <ptr target="#black1960"/>
               <ptr target="#campbell2005"/>
               <ptr target="#connelly2003"/>
               <ptr target="#donnelly1983"/>
               <ptr target="#grigor2000"/>
               <ptr target="#steele1974"/>. They were commissioned by the Queen and are commonly
               aliased by the name of the lords who wrote the report: Napier, Bessborough, Richmond,
               and Devon. A major shift occurred between the Devon and Bessborough Reports
               (1845-1881), from greater landlord protections to greater tenant protections, due in
               part to massive resistance in colonies of the Empire like Ireland, Scotland, and
               India <ptr target="#sartori2014"/>. The Encumbered Estates Act of 1849 disadvantaged
               tenants trying to improve their holdings by moving property with outstanding debts
               from Irish to English owners; Gladstone’s Irish Land Act of 1881 advantaged tenants
               by introducing the first rent control law in history, in addition to redistributing
               landlord property to Irish tenants <ptr target="#readman2008"/>
               <ptr target="#steele1974"/>. We might attribute the large amount of scholarship on
               the reports to both the specificity of the subject matter of the reports and also, to
               a citation feedback loop, where historians produce much research about widely cited
               sources because of their ubiquitousness in the literature. While studying these
               documents so extensively is valuable, it creates bias in the historical analysis by
               leaving out more uninvestigated primary sources. </p>
            <p>Contemporary ideas about property that favor tenant protections, a normalized
               cultural expectation that is often codified into law, have a long and bloody history
               that can in part be traced back to the revolts and reforms that occurred in the
               nineteenth-century British empire. A major shift from favoring landlords to tenants
               in areas like land distribution, eviction, fair rent, is theorized to have occurred
               from the first half to the second half of the century <ptr target="#sartori2014"/>.
               The evidence presented in the following section supports this theory.</p>
         </div>
         <div>
            <head>Results and Visualizations</head>
            <p> The historical analysis that follows is one of many possible threads of questioning
               into the Hansard body of text. We demonstrate how to explore the question of how the
               parliamentary discourse of property changed in the nineteenth-century, using the
               choices made in developing the code pipeline and the user- and visualization-driven
               search through the web app parameters. The threads of questioning, driven by scholars
               and research interests, can be approached in a new way that leans on digital tools
               for a deeper dive into large bodies of text. Historical claims are still made through
               close reading and analysis; but using topic-based search to guide close reading and
               analysis, we open a new and focused window into a previously under-studied primary
               source. In the next section, we both validate our hypothesis from the secondary
               literature and introduce new perspectives — of members of parliament — into the body
               of knowledge about British land reform.</p>
            <div>
               <head>Five Out of Five Hundred Topics are Relevant for Land and Property</head>
               <p>An early choice made in the pipeline is what the ideal number of topics (k) is for
                  the Hansard corpus. We pre-computed topic models at different increments of k (50,
                  100, 200, 500, 1000) for review. Humanists determined that the optimal number of
                  topics is 500 because of the uniqueness and specificity of the topics. Some topics
                  are difficult to label distinctly from other topics in the 1000 topic model, while
                  most topics could be given unique labels in the 500 topic model. Five distinct
                  land-related topics, their ten most heavily weighted words, and their designated
                  labels are shown in Table 2. They are sorted by the year(s) in which they peaked,
                  with early peaks on the left and late peaks on the right. The language used to
                  describe land in Hansard mirrors the historical shifts of the period, from an
                  agriculturally valued plot in the first half of the century to a rental
                  relationship between landlords and tenants in the middle of the century and to
                  legal protections that favored tenants in the latter half of the century —
                  protections that included rent restrictions, freedom from arbitrary evictions, and
                  land redistributions <ptr target="#readman2008"/>.</p>
               <table>
                  <head>Five land-related topics from our 500 topic model of Hansard. They are
                     sorted on when they peaked. The first half of the century does not talk about
                     land as much as a rental agreement but in terms of its productivity. The second
                     half of the century considers the question of land as rental agreement and the
                     legal protections and rights for both sides of the agreement.</head>
                  <row role="label">
                     <cell>17. Land value measured in agricultural output</cell>
                     <cell>46. Land value measured as a tax for the church and as rental income for
                        landlords</cell>
                     <cell>447. Property as a rental relationship between landlords and
                        tenants</cell>
                     <cell>35. Negotiating the interests of landowners vs. tenants who seek to
                        improve their rentals</cell>
                     <cell>180. Legal protections for tenants</cell>
                  </row>
                  <row role="data">
                     <cell>1820, 1846</cell>
                     <cell>1827-1840</cell>
                     <cell>1838, 1892</cell>
                     <cell>1847-1850, 1882</cell>
                     <cell>1882-1904</cell>
                  </row>
                  <row role="data">
                     <cell>farmer</cell>
                     <cell>tith</cell>
                     <cell>leas</cell>
                     <cell>properti</cell>
                     <cell>rent</cell>
                  </row>
                  <row role="data">
                     <cell>agricultur</cell>
                     <cell>rent</cell>
                     <cell>year</cell>
                     <cell>land</cell>
                     <cell>land</cell>
                  </row>
                  <row role="data">
                     <cell>land</cell>
                     <cell>clergi</cell>
                     <cell>rent</cell>
                     <cell>estat</cell>
                     <cell>case</cell>
                  </row>
                  <row role="data">
                     <cell>price</cell>
                     <cell>charg</cell>
                     <cell>leasehold</cell>
                     <cell>proprietor</cell>
                     <cell>commiss</cell>
                  </row>
                  <row role="data">
                     <cell>produc</cell>
                     <cell>land</cell>
                     <cell>lesse</cell>
                     <cell>owner</cell>
                     <cell>commission</cell>
                  </row>
                  <row role="data">
                     <cell>farm</cell>
                     <cell>commut</cell>
                     <cell>land</cell>
                     <cell>interest</cell>
                     <cell>fix</cell>
                  </row>
                  <row role="data">
                     <cell>cultiv</cell>
                     <cell>owner</cell>
                     <cell>term</cell>
                     <cell>possess</cell>
                     <cell>fair</cell>
                  </row>
                  <row role="data">
                     <cell>year</cell>
                     <cell>collect</cell>
                     <cell>tenant</cell>
                     <cell>person</cell>
                     <cell>court</cell>
                  </row>
                  <row role="data">
                     <cell>interest</cell>
                     <cell>landlord</cell>
                     <cell>properti</cell>
                     <cell>valu</cell>
                     <cell>chief</cell>
                  </row>
                  <row role="data">
                     <cell>crop</cell>
                     <cell>payment</cell>
                     <cell>case</cell>
                     <cell>improv</cell>
                     <cell>applic</cell>
                  </row>
               </table>

               <p> We build this process of preparing, reviewing, and labeling different k values
                  into the code pipeline. While the computational cost of preparing different models
                  is not reduced, the human effort of maintaining code and documentation on many
                  models is greatly reduced.</p>
            </div>
            <div>
               <head>Spikes in the discussion of land use and rent in 1850 and 1880</head>
               <p>Figure 3 shows topic 35, which we named <title rend="quotes">Negotiating the
                     interests of landowners vs. tenants who seek to improve their rentals</title>.
                  This topic peaks twice in the late 1840s and early 1850s and then peaks again in
                  1870 and 1880, mirroring the historical arc of this topic. The Devon and
                  Bessborough Commissions were ordered and written in 1845 and 1881 because of calls
                  for land reform from peasant farmers and tenants <ptr target="#sartori2014"/>.
                  This topic is one of several that point the scholar to the land question, and in
                  combination they can paint a fuller, richer picture of the topic than any single
                  excerpt of Hansard or written report.</p>
               <figure>
                  <head>The most frequently occurring words in the topic are ranked and displayed on
                     the left. The percentages are each word’s relative frequency in the topic and
                     add up to 100%. The line plot shows the frequency of the topic (per 10,000
                     words) as a function of time. Common and custom stopwords (such as articles and
                     conjunctions and proper nouns) are replaced with ‘substitute_word’ to
                     drastically reduce our vocabulary without much information loss.</head>
                  <graphic url="resources/images/figure03.png"/>
               </figure>
            </div>
            <div>
               <head>Topic modeling is better to extract a sub-corpus than word frequencies</head>
               <p> We present a land and property example that illustrates how topic modeling goes
                  beyond word frequencies for historical analysis. Figure 4 shows word frequencies
                  for the most commonly occurring words in topic 35. The word frequencies are
                  normalized by the total number of unigrams in each year and are shown per 10,000
                  words. Four peaks stand out: “person” in the early part of the period and “land”
                  in the late part of the period. “Land” and “properti” have a few smaller peaks
                  throughout the period. The pattern does have some historical significance, but it
                  is certainly noisier than the peaks in topic frequency seen in Figure 3. There is
                  no clear peak in the middle of the century where we would expect to see massive
                  activity about this topic, which could potentially lead us to miss some
                  interesting and important documents. Furthermore, if we were to search for
                  documents of interest in Hansard using these frequently occurring keywords, we
                  would likely incur many false negatives that are skewed towards the years in which
                  the word counts peaked.</p>
               <figure>
                  <head>Unigram Frequencies. This figure shows the relative frequencies of the most
                     commonly occurring unigrams in topic 35. The figure shows counts per 10,000
                     unigrams, normalized by the total number of unigrams in the year, on the y-axis
                     and year on the x-axis. The diagram demonstrates a transition from a language
                     of <q>person</q> and <q>interest</q>, visible in the dominance of yellow and
                     pink lines around 1815, to a language where spikes in 1882, 1896, and 1907 were
                     marked by the use of the terms <q>land</q>, <q>value</q>, and
                        <q>improvement</q>. </head>
                  <graphic url="resources/images/figure04.png"/>
               </figure>

            </div>
            <div>
               <head>D3 visualizations and web app</head>
               <p>The screenshots shown in Figures 3 and 4 are taken from the <ref
                     target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> web app.
                  The tool allows humanists to interact with their corpora in a user-friendly,
                  visual, iterative, and exploratory process and avoids the time and resource
                  penalties that traditional search and close reading of corpora incur. We will walk
                  through the various features of the tool with accompanying screenshots in this
                  next section with an emphasis on how the tool enhances digital humanities research
                  by incorporating some of the values of code pipelines discussed in the
                  introduction. </p>
               <p>The tool approaches visual topic exploration at different levels or views: word,
                  document, and topic. Taken together in a single interface, these levels give the
                  user a three-dimensional view of their corpus that is less possible when searching
                  for specific words, documents, or topics in silo. The n-gram plot in Figure 5
                  shows word frequencies per year for the top six words in a topic. Each line in the
                  plot shows frequencies for a given word and each word in the plot is also shown on
                  the right as a button; they can be removed from the plot by clicking on the words
                  and the plot will dynamically re-scale based on the frequencies present in the
                  current plot window. Words can be added by searching the vocabulary in the search
                  bar. The search results filter down as the user types a query because corpora are
                  often pre-processed differently, resulting in a vocabulary made up of stems or
                  lemmas rather than actual words. The frequencies can be shown in absolute terms or
                  relative, normalized to the total number of words in the year. The word view gives
                  the user a finer-grained view of the corpus, but leaves out the greater context
                  and interaction of the words. </p>
               <figure>
                  <head>Word level exploration of the corpus. The default is to display the top six
                     words in each topic. Users can add or remove words with the search bar and it
                     auto-fills with unigrams in the corpus vocabulary. The user can switch between
                     absolute and relative counts.</head>
                  <graphic url="resources/images/figure05.png"/>
               </figure>

               <p>Figures 6 and 7 show topic views of the corpus. The home page shows the first
                  view, where each topic is a word cloud showing the top words in the topic. The
                  size of the word indicates its probability within the topic; bigger words are more
                  likely to be sampled from the topic than smaller words. The page scrolls and loads
                  one chunk of word clouds at a time. The user can filter to a smaller subset of
                  topics by searching for unigrams in the search bar at the top right corner of the
                  page. The alternate view shows a table view of the topics with an additional sort
                  functionality that will sort the topics based on any of the table fields. They can
                  be sorted by their ID, by mean peak year, alphabetically by words, or by their
                  proportion in the corpus. The recommended navigational flow of the corpus
                  exploration part of the tool is to start at one of these two topic overviews and
                  drill down closer into a single topic. Figure 8 shows a detailed topic view. The
                  view includes more top words than the overviews, a plot of the topic over time, a
                  link to the n-gram plot, and a sorted table of documents that are most associated
                  with the topic. The detail view allows the user to explore the topic from various
                  different vantage points, but does not allow multi-topic exploration. All topics
                  are shown in the plot on the left in Figure 9, and they are arranged based on
                  their similarity to each other. Topics with very similar word distributions
                  cluster together and topics with very dissimilar word distributions are very far
                  apart. </p>
               <figure>
                  <head>The front page of the web app for topic level corpus exploration. Each topic
                     is a word cloud and starting point with which to move into a more detailed view
                     of the topic. The word clouds show topic distribution over time on hover and
                     they are searchable by word.</head>
                  <graphic url="resources/images/figure06.png"/>
               </figure>

               <figure>
                  <head>Alternate table view of home page. Sortable by topic ID, mean peak time,
                     alphabetically by words, and by proportion in corpus.</head>
                  <graphic url="resources/images/figure07.png"/>
               </figure>

               <figure>
                  <head>Detail view of topic. Ranks words and ranks documents, and for both shows
                     relevance to the topic. Topic relevance over time plot.</head>
                  <graphic url="resources/images/figure08.png"/>
               </figure>

               <figure>
                  <head>Topic Clustering view shows how similar topics are to each other.</head>
                  <graphic url="resources/images/figure09.png"/>
               </figure>

               <p>Figure 10 shows ranked documents relevant to the five land-related topics shown in
                  Table 2 for z = [0, 100, 500]. For land topics, larger z values produce a better
                  ranking than smaller z values. There are documents from the early and late periods
                  of the century, near the peaks of land-related activity that we expect. Also,
                  three out of four written reports, denoted by the prefix <q>seed-</q>, are in the
                  top-ranked documents list when z = 500; we expect the written reports to be ranked
                  highly because the historical literature confirms that they are key documents
                  summarizing land use in the nineteenth-century. The low proportion of topic tokens
                  out of total tokens in the written reports — 0.031, 0.066, and 0.156 for the
                  Bessborough, Richmond, and Devon reports, respectively — indicates that the seed
                  reports contain a lot of noise and shows the weakness of an earlier document-based
                  approach compared to our topic-based approach <ptr target="#lee2018"/>
                  <ptr target="#tangherlini2013"/>.</p>
               <figure>
                  <head>Top nine most relevant documents ranked using a Wilson-score based sorting
                     method with a single parameter, z. From top to bottom, the tables show results
                     for z = [0, 100, 500].</head>
                  <graphic url="resources/images/figure10.png"/>
               </figure>

            </div>
         </div>
         <div>
            <head>Summary</head>
            <p>In this paper, we introduce the pipeline concept as a means for scholars to conduct
               transparent, modular, and interoperable research and we create a web app that
               exemplifies these values. We make the <ref
                  target="https://brown-ccv.github.io/hatori/page/home/">HaToRI</ref> instance of
               the web app digitally available, and present a use case for doing enhanced historical
               analysis on the Hansard text corpus using the app. We explored how topic modeling can
               be used for sub-corpus extraction when other methods likely fail due to the sheer
               size of the corpus or nuances in the topics of interest that cannot be accurately
               captured by keywords or terms. To illustrate the advantages of topic modeling in
               sub-corpus extraction, we extracted a sub-corpus specific to land use, rent, and
               property from the Hansard British Parliamentary debates of the 19th century. There
               are two historically relevant periods when land use and property was exhaustively
               discussed in the Parliament: the 1850’s and 1880’s. We illustrate that topic-based
               sub-corpus extraction identifies these two eras based on the frequencies of the
               relevant topics, while keywords tend to wash out the signal and identify false peaks
               in the frequency plot. We described our pre-processing and post-processing steps in
               detail and provide a web app with a set of visualizations and functionalities to aid
               humanist research on the Hansard corpus. These functionalities include topic
               clustering based on similarity, document ranking based on topic prevalence, and
               frequency plots. The source codes for the pipeline and the web app are publicly
               available and can be modified to work with other corpora. We believe that topic
               modeling, in particular, as well as our novel document ranking algorithm, are well
               suited to extract a relevant sub-corpus for in-depth studies, especially if the
               corpus is prohibitively long to read by humans.</p>
         </div>
         <div type="appendix" xml:id="acknowledgments">
            <head>Acknowledgments</head>
            <p>Thanks to Mark Howison at Brown University for facilitating the collaboration with
               Prof. Jo Guldi and contributing early work to this project. This research was
               conducted using computational resources and services at the Center for Computation
               and Visualization, Brown University.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="agresti1998" label="Agresti and Coull 1998">Agresti, A. and Coull, B. A.
                  <title rend="italic">Approximate is Better than <q>Exact</q> for Interval
                  Estimation of Binomial Proportions</title>, <title rend="italic">The American
                  Statistician</title>, 52 (1998): 119-126.</bibl>
            <bibl xml:id="alexander2017" label="Alexander and Struan 2017">Alexander, M. and Struan,
               A. <title rend="quotes">Digital Hansard: Politics and the Uncivil</title>, <title
                  rend="italic">Digital Humanities</title>, (2017): 378-380</bibl>
            <bibl xml:id="berry2005" label="Berry and Browne 2005">Berry, M. W. and Browne, M.
                  <title rend="quotes">Email Surveillance Using Non-negative Matrix
                  Factorization</title>, <title rend="italic">Computational and Mathematical
                  Organization Theory</title>, 11.3 (2005): 249-264.</bibl>
            <bibl xml:id="black1960" label="Black 1960">Black, R. D. C. <title rend="italic"
                  >Economic Thought and the Irish Question, 1817-1870</title>. University Press,
               Cambridge, MA (1960).</bibl>
            <bibl xml:id="blaxill2013" label="Blaxill 2013">Blaxill, L. <title rend="quotes"
                  >Quantifying the Language of British Politics, 1880–1910</title>, <title
                  rend="italic">Historical Research</title>, 86 (2013): 313-41.</bibl>
            <bibl xml:id="blei2006" label="Blei and Lafferty 2006">Blei, D. M. and Lafferty, J. D.
                  <title rend="quotes">Dynamic Topic Models</title>, In <title rend="italic"
                  >Proceedings of the 23rd International Conference on Machine Learning</title>,
               Pittsburgh, PA, June 2006.</bibl>
            <bibl xml:id="blei2009" label="Blei and Lafferty 2009">Blei, D. M. and Lafferty, J. D.
                  <title rend="quotes">Topic Models</title>, In A. Srivastava and M. Sahami (eds),
                  <title rend="italic">Text Mining: Theory and Applications</title>, Taylor and
               Francis, London (2009).</bibl>
            <bibl xml:id="blei2003" label="Blei et al. 2003">Blei, D. M., Ng, A. Y. and Jordan, M.
               I. <title rend="quotes">Latent Dirichlet Allocation</title>, <title rend="italic"
                  >Journal of Machine Learning Research</title>, 3 (2003): 933-1022.</bibl>
            <bibl xml:id="brooke2015" label="Brooke et al. 2015">Brooke, J., Hammond, A. and Hirst,
               G. <title rend="quotes">GutenTag: An NLP-driven Tool for Digital Humanities Research
                  in the Project Gutenberg Corpus</title>, In <title rend="italic">Proceedings of
                  the Fourth Workshop on Computational Linguistics for Literature</title>, Denver,
               CO, June 2015.</bibl>
            <bibl xml:id="bull1996" label="Bull 1996">Bull, P. Land, <title rend="italic">Politics
                  and Nationalism: A Study of the Irish Land Question</title>. Gill and Macmillan,
               Dublin (1996).</bibl>
            <bibl xml:id="butterfield2016" label="Butterfield et al. 2016">Butterfield, A., Ngondi,
               G.E. and Kerr, A. A <title rend="italic">Dictionary of Computer Science</title>,
               Seventh Edition. Oxford University Press, Oxford (2016).</bibl>
            <bibl xml:id="campbell2005" label="Campbell 2005">Campbell, F. J. M. <title
                  rend="italic">Land and Revolution Nationalist Politics in the West of Ireland,
                  1891-1921</title>. Oxford University Press, Oxford, New York (2005).</bibl>
            <bibl xml:id="connelly2003" label="Connelly 2003">Connelly, S. J. <title rend="quotes"
                  >Jacobites, Whiteboys, and Republicans: Varieties of Disaffection in
                  Eighteenth-Century Ireland</title>, <title rend="italic">Eighteenth-Century
                  Ireland/Iris an Da Chultur</title>, 18 (2003): 63-79.</bibl>
            <bibl xml:id="donnelly1983" label="Donnelly 1983">Donnelly, J. S. 1983. <title
                  rend="quotes">Irish Agrarian Rebellion: The Whiteboys of 1769-76</title>, <title
                  rend="italic">Proceedings of the Royal Irish Academy, Section C: Archaeology,
                  Celtic Studies, History, Linguistics, Literature</title> (1983): 293-331.</bibl>
            <bibl xml:id="edmond2013" label="Edmond 2013">Edmond, J. <title rend="quotes">CENDARI’s
                  Grand Challenges: Building, Contextualising and Sustaining a New Knowledge
                  Infrastructure</title>, <title rend="italic">International Journal of Humanities
                  and Arts Computing</title>, 7.1-2 (2013): 58–69. doi:<ref
                  target="https://doi.org/10.3366/ijhac.2013.0081">10.3366/ijhac.2013.0081</ref>
            </bibl>
            <bibl xml:id="edmond2015" label="Edmond et al. 2015">Edmond, J., Bulatovic, N. and
               O’Connor, A. <title rend="quotes">The Taste of <q>Data Soup</q> and the Creation of a
                  Pipeline for Transnational Historical Research</title>, <title rend="italic"
                  >Journal of the Japanese Association for Digital Humanities</title> 1.1 (2015):
               107-122. doi:<ref target="https://doi.org/10.17928/jjadh.1.1_107"
                  >10.17928/jjadh.1.1_107</ref>
            </bibl>
            <bibl xml:id="edwards2007" label="Edwards et al. 2007">Edwards, P.N., Jackson, S.J.,
               Bowker, G.C., Knobel, C.P. <title rend="quotes">Understanding Infrastructure:
                  Dynamics, Tensions, and Design</title>, <title rend="italic">Human and Social
                  Dynamics</title> (2007). NSF Grant 0630263.</bibl>
            <bibl xml:id="eubanks2015" label="Eubanks 2015">Eubanks, V. <title rend="italic"
                  >Automating Inequality: How High-tech Tools Profile, Police, and Punish the
                  Poor</title>. St. Martin’s Press, New York, NY (2015).</bibl>
            <bibl xml:id="goldstone2014" label="Goldstone and Underwood 2014">Goldstone, A. and
               Underwood, T. <title rend="quotes">The Quiet Transformations of Literary Studies:
                  What Thirteen Thousand Scholars Could Tell Us</title>, <title rend="italic">New
                  Literary History</title>, 45.3 (2014): 359-384. doi: <ref
                  target="https://doi.org/10.1353/nlh.2014.0025">10.1353/nlh.2014.0025</ref>
            </bibl>
            <bibl xml:id="grigor2000" label="Grigor 2000">Grigor, I. F. <title rend="italic"
                  >Highland Resistance</title>. Mainstream Publishing, Edinburgh (2000).</bibl>
            <bibl xml:id="guldi2018" label="Guldi 2018">Guldi, J. <title rend="quotes">Critical
                  Search: A Procedure for Guided Reading in Large-Scale Textual Corpora</title>,
                  <title rend="italic">Journal of Cultural Analytics</title> (2018). doi: <ref
                  target="https://doi.org/10.31235/osf.io/g286e">10.31235/osf.io/g286e</ref></bibl>
            <bibl xml:id="harris1954" label="Harris 1954">Harris, Z. <title rend="quotes"
                  >Distributional Structure</title>, <title rend="italic">Word</title>, 10.2-3
               (1954): 146-62.</bibl>
            <bibl xml:id="introna2006" label="Introna and Nissenbaum 2006">Introna, L.D. and
               Nissenbaum, H. <title rend="quotes">Shaping the Web: Why the Politics of Search
                  Engines Matters</title>, <title rend="italic">The Information Society: An
                  International Journal</title> 16.3: 169-185 (2006).</bibl>
            <bibl xml:id="janicke2015" label="Janicke et al. 2015">Janicke, S., Franzini, G.,
               Cheema, M. F. and Scheuermann, G. <title rend="quotes">On Close and Distant Reading
                  in Digital Humanities: A Survey and Future Challenges</title>, <title
                  rend="italic">Eurographics Conference on Visualization State of the Art
                  Report</title> (2015): 21.</bibl>
            <bibl xml:id="kaplan2015" label="Kaplan 2015">Kaplan, F. <title rend="quotes">A Map for
                  Big Data Research in Digital Humanities</title>, <title rend="italic">Frontiers in
                  Digital Humanities</title>, 2 (2015). doi:<ref
                  target="https://doi.org/10.3389/fdigh.2015.00001">10.3389/fdigh.2015.00001</ref>
            </bibl>
            <bibl xml:id="le2014" label="Le and Mikolov 2014">Le, Q. V. and Mikolov, T. <title
                  rend="quotes">Distributed Representation of Sentences and Documents</title>, In
                  <title rend="italic">Proceedings of the Thirty-first International Conference on
                  Machine Learning Beijing</title>, CN, June 2014.</bibl>
            <bibl xml:id="lee2018" label="Lee et al. 2018">Lee, A. S., Guldi, J. and Zsom, A. <title
                  rend="quotes">Measuring Similarity: Computationally Reproducing the Scholar’s
                  Interests</title>, <ref target="https://arxiv.org/abs/1812.05984"
                  >arXiv:1812.05984</ref> [cs.CL] (2018).</bibl>
            <bibl xml:id="llora2008" label="Llora et al. 2008">Llora, X., Acs, B., Auvil, L. S.,
               Capitanu, B., Welge, M. E. and Goldberg, D. E. <title rend="quotes">Meandre:
                  Semantic-Driven Data-Intensive Flows in the Clouds</title>, In <title
                  rend="italic">IEEE Fourth International Conference on eScience</title>,
               Indianapolis, IN, December 2008. doi: <ref
                  target="https://doi.org/10.1109/eScience.2008.172">10.1109/eScience.2008.172</ref>
            </bibl>
            <bibl xml:id="manning1999" label="Manning and Schutze 1999">Manning, C. D. and Schutze,
               H. <title rend="italic">Foundations of Statistical Natural Language
                  Processing</title>. MIT Press, Cambridge, MA (1999).</bibl>
            <bibl xml:id="mattern2013" label="Mattern 2013">Mattern, S. <title rend="quotes"
                  >Infrastructural Tourism</title>, <title rend="italic">Places Journal</title>
               (2013). doi: <ref target="https://doi.org/10.22269/130701"
                  >https://doi.org/10.22269/130701</ref>
            </bibl>
            <bibl xml:id="mattern2015" label="Mattern 2015">Mattern, S. <title rend="quotes">Deep
                  Time of Media Infrastructure</title>, In Lisa Parks and Nicole Staroeislski
               (eds.), <title rend="italic">Signal Traffic: Critical Studies of Media
                  Infrastructures</title>. University of Illinois Press, Champaign, IL
               (2015).</bibl>
            <bibl xml:id="mattern2016" label="Mattern 2016">Mattern, S. <title rend="quotes"
                  >Scaffolding, Hard and Soft - Infrastructures as Critical and Generative
                  Structures</title>, <title rend="italic">Spheres Journal for Digital
                  Cultures</title>, 3 (2016).</bibl>
            <bibl xml:id="mccallum2002" label="McCallum 2002">McCallum, A. K. <title rend="quotes"
                  >MALLET: A Machine Learning for Language Toolkit</title>, <title rend="italic"
                  >Scientific Research</title>, (2002). <ref target="http://mallet.cs.umass.edu"
                  >http://mallet.cs.umass.edu</ref>.</bibl>
            <bibl xml:id="mikolov2013" label="Mikolov et al. 2013">Mikolov, T., Sutskever, I., Chen,
               K., Corrado, G. S. and Dean, J. <title rend="quotes">Distributed Representations of
                  Words and Phrases and Their Compositionality</title>, <title rend="italic"
                  >Advances in Neural Information Processing Systems</title>, (2013). </bibl>
            <bibl xml:id="murdock2017" label="Murdock et al. 2017">Murdock, J., Allen, C., Borner,
               K., Light, R., McAlister, S., Ravenscroft, A., Rose, R., Rose, D., Otsuka, J.,
               Bourget, D., Lawrence, J., and Reed, C. <title rend="quotes">Multi-level Computation
                  Methods for Interdisciplinary Research in the HathiTrust Digital Library</title>,
                  <title rend="italic">PLoS One</title>, 12.0 (2017).</bibl>
            <bibl xml:id="nissenbaum2010" label="Nissenbaum 2010">Nissenbaum, H. <title
                  rend="italic">Privacy in Context: Technology, Policy, and the Integrity of Social
                  Life</title>. Stanford University Press, Stanford, CA (2010).</bibl>
            <bibl xml:id="noble2018" label="Noble 2018">Noble, S.U. <title rend="italic">Algorithms
                  of Oppression: How Search Engines Reinforce Racism</title>. NYU Press, New York,
               NY (2018).</bibl>
            <bibl xml:id="dallessandro2016" label="d’Alessandro et al. 2016">d’Alessandro, B.,
               O’Neil, C. and LaGatta, T. <title rend="quotes">Conscientious Classification: A Data
                  Scientist’s Guide to Discrimination-Aware Classification</title>, <title
                  rend="italic">Big Data</title>, 5.2 (2017). doi: <ref
                  target="http://doi.org/10.1089/big.2016.0048"
                  >http://doi.org/10.1089/big.2016.0048</ref></bibl>
            <bibl xml:id="oneil2016" label="O’Neill 2016">O’Neil, C. <title rend="italic">Weapons of
                  Math Destruction: How Big Data Increases Inequality and Threatens
                  Democracy</title>. Broadway Books, New York, NY 2016. </bibl>
            <bibl xml:id="readman2008" label="Readman 2008">Readman, P. <title rend="italic">Land
                  and Nation in England: Patriotism, National Identity, and the Politics of Land,
                  1880-1914</title>. Boydell Press, Woodbridge, UK (2008).</bibl>
            <bibl xml:id="rule2015" label="Rule et al. 2015">Rule, A., Cointet, J. and Bearman, P.
               S. <title rend="quotes">Lexical shifts, substantive changes, and continuity in State
                  of the Union discourse, 1790–2014</title>, In <title rend="italic">Proceedings of
                  the National Academy of Sciences</title>, 112.35 (2015): 10837-44.</bibl>
            <bibl xml:id="sartori2014" label="Sartori 2014">Sartori, A. <title rend="italic"
                  >Liberalism in Empire: An Alternative History</title>. University of California
               Press, Berkeley (2014).</bibl>
            <bibl xml:id="sinclair2016" label="Sinclair and Rockwell 2016">Sinclair, S. and
               Rockwell, G. <title rend="quotes">Voyant Facts</title>, Hermeneuti.ca:
               Computer-Assisted Interpretation in the Humanities (2016). <ref
                  target="http://hermeneuti.ca/VoyantFacts"
               >http://hermeneuti.ca/VoyantFacts</ref>.</bibl>
            <bibl xml:id="steele1974" label="Steele 1974">Steele, D. <title rend="italic">Irish Land
                  and British Politics: Tenant-Right and Nationality, 1865-1870</title>. Cambridge
               University Press, London (1974).</bibl>
            <bibl xml:id="svensson2011" label="Svensson 2011">Svensson, P. <title rend="quotes">From
                  Optical Fiber to Conceptual Cyberinfrastructure</title>, <title rend="italic"
                  >Digital Humanities Quarterly</title>, 5.1 (2011).</bibl>
            <bibl xml:id="tangherlini2013" label="Tangherlini and Leonard 2013">Tangherlini, T. and
               Leonard, P. <title rend="quotes">Trawling in the Sea of the Great Unread: Sub-corpus
                  topic modeling and Humanities research</title>, <title rend="italic"
                  >Poetics</title>, 41.6 (2013): 725-749.</bibl>
            <bibl xml:id="tucker2004" label="Tucker 2004">Tucker, A. B. <title rend="italic"
                  >Handbook of Computer Science</title>. CRC Press, Boca Raton (2004).</bibl>
            <bibl xml:id="vaidhyanathan2012" label="Vaidhyanathan 2012">Vaidhyanathan, S. <title
                  rend="italic">The Googlization of Everything: (and why we should worry)</title>.
               University of California Press, Los Angeles, CA (2012).</bibl>
            <bibl xml:id="vaidhyanathan2018" label="Vaidhyanathan 2018">Vaidhyanathan, S. <title
                  rend="italic">Anti-Social Media: How Facebook Disconnects Us and Undermines
                  Democracy</title>. Oxford University Press, New York, NY (2018).</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
