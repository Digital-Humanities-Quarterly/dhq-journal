<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     xmlns:cc="http://web.resource.org/cc/"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
     xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
     xmlns:mml="http://www.w3.org/1998/Math/MathML">
   <teiHeader>
      <fileDesc>
         <titleStmt><!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!--article title in English-->Character Recognition Of Seventeenth-Century Spanish American Notary Records Using Deep Learning</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Nouf <dhq:family>Alrasheed</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Department of Computer Science. &amp; Electrical Engineering. University of Missouri-Kansas City</dhq:affiliation>
               <email>nalrasheed@mail.umkc.edu</email>
               <dhq:bio>
                  <p>Ms. Nouf Alrasheed is a doctoral student in computer science at UMKC’s School of Computing and Engineering. She received her M.S. in computer science with an emphasis on data science at UMKC and serves as lecturer at the University of Tabuk, Saudi Arabia. Her research focuses on artificial intelligence and deep learning. Under the supervision of Drs. Rao and Grieco, she is developing her doctoral research on employing robust deep learning methods to efficiently read and analyze historical handwritten documents. She is the recipient of the Grace Hopper Celebration (GHC) Scholarship (2019 and 2021) and the UMKC Women's Council Graduate Assistantship Fund GAF Award (2019, 2020, and 2021). She is an IEEE and ACM student member.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Praveen <dhq:family>Rao</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Department of Health Management &amp; Informatics, Department of Electrical Engineering &amp; Computer Science. University of Missouri-Columbia</dhq:affiliation>
               <email>praveen.rao@missouri.edu</email>
               <dhq:bio>
                  <p>Dr. Praveen Rao is an associate professor with joint appointment in the Department of Health Management &amp; Informatics and the Department of Electrical Engineering &amp; Computer Science at University of Missouri-Columbia. He is an expert in the areas of big data, knowledge management, and machine learning/deep learning. His research, teaching, and outreach activities have been supported by the National Science Foundation (NSF), National Endowment for the Humanities (NEH), National Institutes of Health (NIH), University of Missouri System, University of Missouri Research Board, Air Force Research Lab (AFRL), IBM, and local companies. He is a recipient of the IBM Smarter Planet Faculty Innovation Award (2010), the IBM Big Data and Analytics Faculty Award (2013), UMKC's Award for Excellence in Mentoring Undergraduate Researchers, Scholars, and Artists (2016), and UMKC’s N.T. Veatch Award for Distinguished Research and Creativity (2018). He is a recipient of the National Research Council (NRC) Research Associateship Senior Fellowship Award (2016-2017). He is a member of the Environmental Health Sciences Review Committee of the NIH. He serves on the editorial boards of several international journals. He is a Senior Member of ACM and IEEE.</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo><!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Viviana <dhq:family>Grieco</dhq:family>
               </dhq:author_name>
               <dhq:affiliation>Department of History. University of Missouri-Kansas City</dhq:affiliation>
               <email>griecov@umkc.edu</email>
               <dhq:bio>
                  <p>Dr. Viviana Grieco is a Professor in the Department of History at UMKC. As a colonialist, she specializes in the history of Spanish America between the sixteenth and early nineteenth centuries.  She received training in paleography and diplomatics in Argentina and in Spain and has conducted research in Spanish and Spanish American archives. Her work received funding from The National Endowment for the Humanities (NEH), The John Carter Brown Library, The University of Missouri Research Board, UMKC’s Women and Gender Studies Program, UMKC’s Funding for Excellence Program, UMKC’s Collaborative Data Science Program and the University of Missouri Strategic Investment Funds. Grieco is the author of <title rend="italic">The Politics of Giving in the Viceroyalty of Rio de la Plata. Donors, Lenders, Subjects and Citizens</title> (The University of New Mexico Press, 2014), which was translated and published in Spanish in 2018 <title rend="italic">La Política de Dar.  Donantes, Prestamistas, Súbditos y Ciudadanos</title> (Prometeo Libros, 2018). She is also the co-author (with Fabricio Prado and Alex Borucki) of <title rend="italic">The Río de la Plata from Colony to Nations: Commerce, Society, and Politics</title> (Palgrave McMillan, 2021). Grieco has been invited to present her research and gave keynote addresses at international research centers including the London School of Economics and Political Science in the U.K., the Instituto de Investigaciones Dr. Jose Maria Luis Mora in Mexico, and the Instituto de Historia Argentina y Americana <q>Dr. Emilio Ravignani</q> in Argentina.</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id"><!--including leading zeroes: e.g. 000110-->000581</idno>
            <idno type="volume"
               ><!--volume number, with leading zeroes as needed to make 3 digits: e.g. 006-->015</idno>
            <idno type="issue"><!--issue number, without leading zeroes: e.g. 2-->4</idno>
            <date when="2021-12-07">07 December 2021</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords"><!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
            <keywords scheme="#authorial_keywords"><!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc><!--Each change should include @who and @when as well as a brief note on what was done.-->
         <change when="2022-08-02" who="BRG">properly formatted div elements</change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract><!--Include a brief abstract of the article-->
            <p>Handwritten character recognition is a challenging
               pattern recognition problem due to the inconsistency of the handwritten scripts and
               the lack of accurate labeled data. Historical documents written in cursive are even
               more challenging as characters have unique and varying shapes. Frequently, words are
               linked by lines and ornamental doodles. When historical documents are digitized, the
               images contain various types of noise and degradation, which further complicates the
               recognition of characters. In this paper, we present an empirical study of how well
               state-of-the-art convolutional neural networks (CNNs) for image classification
               perform for the task of recognizing handwritten characters in seventeenth-century
               Spanish American notarial scripts. Professional historians, paleography experts and
               trained labelers were involved in preparing the labeled dataset of Spanish
               characters for training the CNNs. The
               labeled dataset used in this experiment was created from the manuscripts written by
               one of the multiple scribes that contributed to the collection of approximately
               220,000 digitized images of notary records housed at the
               <title rend="italic">Archivo General de la Nación Argentina </title>(National
               Archives). We removed the noise in these images by applying standard image
               processing techniques. After training different CNNs, we computed the classification
               accuracy for all the characters. We observed that ResNet-50 achieved a promising
               accuracy of 97.08% compared to InceptionResnet-V2, Inception-V3, and VGG-16, which
               achieved 96.66%, 96.33% and 70.91%, respectively.</p>
         </dhq:abstract>
         <dhq:teaser><!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>This article examines the handwriting of seventeenth-century Spanish American notary records</p>
         </dhq:teaser>
      </front>
      <body>
         <div>
            <head>I. INTRODUCTION <note> Viviana Grieco’s research focuses on Colonial Latin
               American history and has received extensive paleography training in
               Argentina and in Spain. She leads our labeling team which counts on the
               expertise of Martin Wasserman and David Freeman, historians who have
               employed in their research the collection of notary records used in this
               study. For more information about our research team visit <ref target="https://www.umkc.edu/mide/NEH-Project/">https://www.umkc.edu/mide/NEH-Project/</ref>
            </note></head>
         <p>Notary records contain a wealth of information for understanding different aspects of
                the human experience. For that reason, historians specialized in different regions
                and time periods employ them in writing social, economic, political, and cultural
                histories. The seventeenth-century Spanish American notarial scripts housed in the
                National Archives of Argentina are among the most challenging collections, as they
                were written by multiple hands, for an audience of experts, and at a time the
                written Spanish language underwent significant transformations <ptr target="#silvaprada2001"/> <ptr target="#wasserman2018"/>
                .<note> The speed and volume of the documentary production
                            forced the scribes to link words and use an increasing number of
                            abbreviations. The office of the public notary in seventeenth century
                            Buenos Aires had a high turnover rate, which explains the large number
                            of interim notaries as well as the variety of hands present in this
                            collection.</note> Consequently, it takes years of training
                and practice in seventeenth-century Spanish paleography to become proficient in
                reading and analyzing these notarial scripts (Fig.1). On an average, it takes expert
                Spanish speaking readers about one hour to read a four to five pages long notarized
                deed. The task is even more daunting for non-native Spanish speakers. </p> 
         
         
            <figure>
               <head>Samples of different handwriting styles present in the collection of
                  seventeenth-century notarial scripts used for this study</head>
               <graphic url="resources/images/Figure_1.png"/>
               <figDesc>Screenshot of two different styles of handwriting</figDesc>
            </figure>
         
         <p>Digitization significantly aided in the preservation of these records and made them
                relatively more accessible primarily by enabling their duplication without damaging
                the originals. However, despite the quantity and variety of documents this
                collection compiles, these records are still waiting to be fully utilized in
                scholarly endeavors. To this day, researchers and students rely on traditional, time
                consuming, and expensive methods of archival research to access these documents and
                archival discovery primarily depends on the skill, patience and luck of the scholar.
                The development of a system capable of storing, reading, querying, and analyzing
                this historical collection is crucial as it will make these manuscripts accessible
                to a broader community of researchers without requiring extensive and expensive
                paleography training. Character recognition is the first step in the development of
                such a system.</p>
         <p>Today, using optical character recognition (OCR), we can automatically convert
                printed or handwritten text into machine-readable, editable, and searchable text.
                For that reason, OCR is regarded as the heart of many document analysis systems.
                Unlike the recognition of printed text, historical handwritten text presents unique
                challenges. Written in cursive, historical scripts usually employ irregular
                characters and capitalization, abbreviations, archaic spelling, and linked words
                (Fig. 2).
                    <note> The Document Analysis Group at the
                            Universitat Autònoma de Barcelona has been developing a digital library
                            for the sixteenth-century <title rend="italic">Llibres d’Esposalles</title>
                            (marriage records). These handwritten marriage records are quite
                            challenging although each marriage license follows a regular formula and
                            the scripts are more consistent than those for the seventeenth-century
                            notary records, <ref target="http://dag.cvc.uab.es/the-esposalles-database/">http://dag.cvc.uab.es/the-esposalles-database/</ref>
            </note> Additionally, the scans contain different types of noise
                including discoloration, stains, as well as ink bleeds and smudges. In order to
                enable OCR tasks, researchers applied different methods including Support Vector
                Machine (SVM) <ptr target="#vellingiriraj2016"/>, K-NN <ptr target="#chammas2018"/>, and deep
            learning <ptr target="#granell2018"/>. </p> 
           
            
            <figure>
               <head>Example of some of the challenges
                  present in the manuscripts utilized for this study. On average, one
                  page has 24 lines. The variations showed in this short paragraph are
                  standard throughout the deeds.</head>
               <graphic url="resources/images/Figure2.png"/>
               <figDesc>Screenshot of two different styles of handwriting, annotated</figDesc>
            </figure>
              
         
         <p>The challenges of converting Spanish American historical texts into machine-readable
                text have been pointed out by Hannah Alpert-Abrams. She used <ref target="https://icebergnlp.github.io/">Ocular</ref>, the OCR tool developed by
                Taylor Berg-Kirkpatrick, to machine read the <title rend="italic">Primeros Libros</title>,
            a sixteenth-century, printed, bilingual (Spanish and Nahuatl) text <ptr target="#alpertabrams2016"/>. Her experiments
                delivered not only dirty OCR but also linguistic errors that could lead to
                inaccuracies in cultural translation. We experimented with <ref target="https://transkribus.eu/Transkribus/">Transkribus</ref>, another popular
                tool, which also yielded an inaccurate output for our collection of
                seventeenth-century, handwritten, Spanish American notarial records. However, in
                recent years, deep learning has achieved remarkable success for image understanding
                and classification, image segmentation, speech recognition, and natural language
                processing. In this work, we explore if deep learning, and more specifically CNNs,
                can enable accurate recognition of characters in Spanish American notarial scripts. </p>
         <p>Deep learning techniques perform better and achieve higher accuracy when large
            labeled datasets are available (e.g., ImageNet, MS COCO) <ptr target="#deng2009"/> <ptr target="#lin2014"/>.
            However, there is no readily available labeled dataset for
                seventeenth-century historical texts. Although data labeling is an expensive,
                error-prone, and time-consuming task, with the help of professional historians and
                paleography experts, we manually prepared a labeled dataset. Additionally,
                historical texts have various kinds of noises and degradation and the uneven
                scanning quality of the images pose additional challenges for image preprocessing
                and cleaning as it has to be performed without losing any of the essential features
                that define each of the characters.</p>
         <p>In this paper, we present an empirical study of how well state-of-the-art CNNs
                perform for the task of recognizing handwritten characters in seventeenth-century
                Spanish American notarial scripts. To the best of our knowledge, this is the first
                effort towards automatically recognizing characters in seventeenth-century Spanish
                American notarial scripts. </p>
            <p><hi rend="bold">The key contributions of our work are the following:</hi></p>
        <list type="unordered">
          <item>With the assistance of professional historians as well as labelers proficient in
                Spanish and trained in paleography, we prepared the training dataset in two steps.
                Firstly, we collected from our labelers 250 unique samples of each of the characters
                present on the manuscripts. Secondly, we augmented the dataset and generated
                additional characters by applying random distortions and rotations to the original
                ones. For quality control, before training CNNs on the generated dataset, our
                professional historians verified that the expanded characters resembled the original
                ones. There are certain characters that are rare in both, the Spanish language and
                the notarial scripts. For those characters we had fewer labels.</item>
         <item> We selected four state-of-the-art CNNs, namely, Inception-v3
            <ptr target="#szegedy2015"/> <ptr target="#szegedy2016"/>, ResNet-50 <ptr target="#he2016"/>, VGG-16 <ptr target="#simonyan2014"/> and
                InceptionResNet-v2
            <ptr target="#szegedy2017"/> to
                recognize high frequency characters as well as rare characters (i.e., x and z). We trained these networks by configuring the hyperparameters to
                achieve the best classification accuracy. Our experiments showed that ResNet-50
                achieved the best classification accuracy of 97.08% whereas other networks achieved
                lower accuracy with VGG-16 being the poorest.</item>
        <item>For broader use by the academic community and to foster new research in
                transcribing historical texts, our labeled dataset and software are publicly
                available on GitHub via <ref target="https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican">https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</ref>.</item></list>
         <p>This paper is organized as follows. Section II discusses the relevant related works
                and the motivation for this study. Section III presents the methodology we employed
                and discusses our evaluation results. Finally, we conclude in Section IV and outline
                our future work.</p> </div>
         <div>
            
         <head>II. RELATED WORK &amp; MOTIVATION</head>
         <p>Deep learning approaches have been widely used for handwritten text recognition of
                many modern languages. CNNs
            <ptr target="#krizhevsky2012"/> are
                among the most popular deep learning methods and have a proven record of outstanding
                performance when applied to image recognition tasks. Additionally, CNNs have shown
                an outstanding success when applied to the MNIST dataset <ptr target="#lechun1998"/>.
                Ashiquzzaman et al. proposed a CNN-based model using ReLU activation function and
                dropout as a regularization layer that has achieved 97.4% accuracy <ptr target="#ashiquzzaman2017"/>.
            Tsai investigated various convolutional neural network architectures
                for handwritten Japanese character recognition and created a model with a 96.1%
                recognition rate for character classification <ptr target="#tsai2016"/>. Another CNN-based model
                has been proposed by Rabby et al. to classify Bangla handwriting characters. A
                95.71% validation accuracy was achieved for the BanglaLekha-Isolated dataset <ptr target="#rabby2018"/>.</p>
         <p>However, applying these methods to historical documents present unique challenges due
                to the quality of the scanned images, writing style variations, and the lack of
                labeled data. Consequently, only a few studies have taken this path. For instance,
                Kölsch et al. used a Fully Convolutional Neural Network (FCNN)-based approach for
                historic German documents, which achieved 95.6% accuracy <ptr target="#kolsch2018"/>.
                Clanuwat et al. proposed a KuroNet model that jointly recognizes an entire page of
                text by using a residual U-Net architecture and predicts the location and identity
                of all characters on a given page. Additionally, their proposed system was able to
                successfully recognize a significant fraction of pre-modern Japanese documents
            <ptr target="#clanuwat2019"/>.</p>
         <p>Researchers tend to combine CNNs with recurrent neural networks (RNNs) to further
                improve accuracy. That was the case for Granell et al. who proposed a handwritten
                text recognition system to transcribe a corpus of Spanish medieval scrips based on a
                CNN and RNN <ptr target="#granell2018"/>. The authors showed that deep learning
                approaches outperform the traditional machine learning models such as Hidden Markov
                Model-based systems. Dona Valy et al. evaluated different deep learning approaches
                for character recognition that have been constructed from Khmer palm leaf
                manuscripts <ptr target="#valy2018"/>. The authors showed that the combination of CNN and
                RNN-based architectures achieves better results with a 5.01% error rate. Finally,
                Chammas et al. presented a CRNN system for text- line recognition of historical
                documents <ptr target="#chammas2018"/>. They showed how to train the system with only 10%
                manually labeled text-line data from the READ 2017 dataset.</p>
         <p>Next, we describe the salient features of four recent CNNs that we used in our
                study.</p>
         <list type="ordered">
            <item><p>VGG</p>
         <p>In 2014, Karen Simonyan and Andrew Zisserman (2014) proposed the VGGNet for the Large
                Scale Visual Recognition Challenge (ILSVRC2014). The key contribution from this
                model was to increase the depth of the architecture by using a 3x3 convolutional
                filters to achieve higher performance. The VGG model achieved 92.7% top-5 accuracy
                on the ImageNet dataset and won the ILSVRC2014 challenge. For our experimental
                study, we chose VGG-16 as a representative of the VGGNet due to its smaller number
                of parameters compared to VGG19.</p></item>
         
            <item><p>Inception</p>
         
         <p>Inception architecture was first proposed in 2014 by Szegedy et al. The authors
                claimed that deeper networks are more prone to overfitting and consume computational
                resources. They solved that challenge by moving from fully connected to sparsely
                connected architectures. They introduced the inception layer, which is a combination
                of three different convolutional layers (1x1 convolutional layer, 3x3 convolutional
                layer, and 5x5 convolutional layer) with a max pooling layer that operates at the
                same level. Their outputs are concatenated to be the input of the next layer. This
                architecture has been updated to increase the accuracy further and proved that any
                convolution with kernel size more substantial than 3x3 could be represented
                efficiently with a series of smaller convolutions. In our experimental study, we
                used Inception- V3 <ptr target="#szegedy2015"/> <ptr target="#szegedy2017"/>.</p></item>
         
            <item><p>ResNet</p>
         
         <p>He et al. introduced the deep residual neural network (ResNet) architecture and won
                the first place in the ILSVRC 2015 classification competition. ResNet introduces the
                idea of identity connections that skip one or more layers to train deeper neural
                networks. This resolved the vanishing gradient problem by allowing the gradients to
                flow directly through the skipped connections backward from later layers to the
                initial filter. For our experimental study, we used ResNet-50 as a representative
            <ptr target="#he2016"/>.</p></item>
        
            <item><p>InceptionResNet</p>
         
         <p>Inception-ResNet is a convolutional neural network proposed by Szegedy et al. in
                2016. It was trained on more than one million images from the ImageNet database and
                achieved a 3.08% top-5 error on the test set of the ImageNet classification (CLS)
                challenge <ptr target="#szegedy2017"/> The success of residual connections in training
                very deep architectures and the performance of the Inception-V3 inspired the authors
                to replace the Inception filter concatenation step with residual connections. This
                combination allows Inception to obtain all the advantages of the residual approach
                but with the preservation of its computational efficiency. We used
                InceptionResNet-v2 as a representative for our experimental study.</p></item></list>
            
            
         <p>Despite these advances, to this day, there is a lack of end-to-end systems capable of
                managing and analyzing historical documents in general and those in Spanish in
                particular. This gap, coupled with the professional training needs of 21st century humanities scholars, draw our attention and
                drives our experimentation efforts to make these manuscripts accessible to a broader
                community of researchers without requiring extensive and expensive paleography
                training. Our effort is the first step in this direction and will open up a wide
                range of research opportunities for others in the academic community.</p></div>
         
         <div>   
            
         <head>III. METHODOLOGY</head>
         <p>In this section, we present the methodology for conducting this empirical study. The
                overall steps are illustrated in Figure 3. There are four main stages: (a)
                pre-processing, (b) dataset preparation, (c) training and validation (to tune the
                hyperparameters), and (d) testing the accuracy of character recognition.</p> 
            
            
            <figure>
               <head>Overall
                  steps involved in character recognition of seventeenth-century
                  Spanish American notarial scripts</head>
               <graphic url="resources/images/Figure3.png"/>
               <figDesc>Image of a flowchart</figDesc>
            </figure>
            
            
        
         <div><head>a) Pre-processing:</head> <p>The manuscripts scans contained noise including spurious ink
                markings, ink smudges and bleeds. Such noise affects the feature extraction as well
                as the classification. Before constructing the character dataset, we reduced the
                noise on the manuscripts images (Figure 4). The following preprocessing techniques
                allowed us to clean the images without affecting the quality of the written content.
                Firstly, we converted the original manuscript images into grayscale. Secondly, we
                applied a median filter to soften the images backgrounds and remove background
                noise. Finally, we applied image binarization to convert the grayscale images into
                black and white ones as this technique significantly reduces the information
                contained in an image and increases the training speed <ptr target="#chamchong2009"/>. </p>
         
            <figure>
               <head>An example of an original scan and its cleaned version.</head>
               <graphic url="resources/images/Figure4.png"/>
               <figDesc>Two images of handwriting</figDesc>
            </figure></div>
            
           
         <div><head>b) Dataset preparation:</head> <p>We constructed the character dataset from clean images. <ref target="http://www.colabeler.com/">Colabeler</ref> tool was used to annotate and
                label the characters. The annotations were exported in JSON format along with each
                character label and its corresponding coordinates. We ran a Python script to crop
                and save every character in .png format. As it was difficult to keep each character
                within square dimensions while annotating them, each one of them was padded with
                white pixels and resized to fixed dimensions for training purposes. We considered 24
                characters that comprise most of the Spanish alphabet: a, b, c, c, d, e, f, g, h, i, j, l, m, n, o, p,
                    q, r, s, t, u/v, x, y, and z. Note
                that the "u" and the "v" were interchangeable in
                seventeenth-century Spanish, and thus, the alphabets provided by most paleography
                manuals list them as a single character. We followed this standard and treated them
                as single letter (u/v). Additionally,
                characters such as "k" and "w" are infrequently used in modern Spanish
                and were so uncommon in seventeenth-century notarial scripts that paleography
                textbooks do not even list them on their sample alphabets.<note> N.
                            Silva-Prada, Manual de paleografía y diplomática hispanoamericana,
                            siglos XVI, XVII y XVIII. Libros de texto, manuales de prácticas y
                            antologías. Universidad Autónoma Metropolitana, Unidad Iztapalapa, 2001.
                                <ref target="https://paleografi.hypotheses.org/el-manual-de-silva-prada">https://paleografi.hypotheses.org/el-manual-de-silva-prada</ref>
            </note>
                To build our sample dataset, we selected the hand of Nicolas de Valdibia y Brizuela
                who, by 1650, acted as an interim notary in Buenos Aires, Argentina. As opposed to
                those who held permanent positions, interim notaries did not receive extensive
                training nor were they skilled scribes. Thus, the experiments we report in this
                paper were based on very irregular and, therefore, hard to read scripts, as shown in
                Figure 2 &amp; Figure 4. </p>
         <p>Our labeling team labeled 250 unique samples for every character. This resulted in a
                total of 6,000 original images. As deep learning models perform well on large
                labeled datasets, the dataset was augmented by applying random distortion and
                rotation of +5 and -5 degrees without affecting the shape and/or the direction of
                each character. A few examples are shown in Figure 5. </p>
         
         
            <figure>
               <head>Example of the original characters <q>b</q>; <q>d</q> and <q>p</q>
               b) Example of
                     augmented characters after applying random distortion and
                     rotation</head>
               <graphic url="resources/images/Figure5.png"/>
               <figDesc>Two images of handwriting</figDesc>
            </figure>
         
         
      
         <p>For quality control, before training CNNs on the generated dataset, the professional
                historians and paleography experts in our team verified that the expanded characters
                resembled the original ones. Our dataset contained 1,000 samples for each character
                out of which 250 samples were manually labeled, and 750 samples were generated. This
                resulted in a total of 24,000 images.</p></div>
         <div><head>c) Training and validation:</head> <p>We conducted all our experiments on a GeForce RTX 2080 Ti
                GPU with 12GB GPU memory. Our software was implemented using Keras <ptr target="#chollet2015"/> with TensorFlow backend 
            <ptr target="#abadi2015"/>, TensorBoard,<note> Tensorflow,
                            <title rend="quotes">tensorflow/tensorboard,</title> Apr 2020. Available: <ref target="https://github.com/tensorflow/tensorboard">https://github.com/tensorflow/tensorboard</ref>
            </note> and
            OpenCV <ptr target="#bradski2000"/>.</p>
         <p>We trained the state-of-art CNNs using 24,000 images (1,000 images per character) in
                our labeled dataset prepared from our seventeenth-century Spanish American notary
                scripts. For each character, we split the training set into an 80-20 split. The
                samples not used for training were part of the testing set. The testing set
                contained 50 samples per character and were preprocessed in the same way as the
                training images. Data augmentation was applied to the training set to avoid
                overfitting. As we mentioned earlier, most of the paleography manuals list the <q>u</q> and the <q>v</q> as a single character as they were
                interchangeable in seventeenth-century Spanish. We followed the same standard and
                treated them as a single letter (u/v).</p></div>
         <div><head>d) Test of the accuracy of character recognition:</head> <p>In this research, the character
                recognition accuracy was used as the primary metric to evaluate the performance of
                different CNNs. An empirical tuning approach has been followed to tune the
                hyperparameters to obtain higher character recognition accuracy. To ensure a fair
                comparison, we set a total of 150 epochs to train the models, and used the Adam
                optimizer <ptr target="#kingma2015"/>. Adam is a gradient descent optimization algorithm
                that is popularly used in training deep learning models. (Using gradient descent, it
                is possible to find local minima of functions during optimization.) The performance
                of the models with different hyperparameters values is shown in Table I and Table
                II.</p>
         <p>Table I shows the recognition accuracy when we set the dropout value to 0.6.
                ResNet-50 achieved the highest accuracy of 97.02%, followed by InceptionResnet-v2,
                Inception-v3, and VGG-16 with a recognition accuracy of 96.33%, 93.83%, and 96.33%,
                respectively. The performance of most of the networks improved when the batch size
                was increased to 64 except for Inception-v3, which achieved a better recognition
                accuracy when the batch size was 32.</p>
         <p>The recognition accuracy results obtained from using 0.5 dropout rate are presented
                in Table II. The recognition accuracy decreased for most of the networks when we
                changed the dropout rate from 0.6 to 0.5. VGG-16 was the only model where it
                performed better on a 0.5 dropout rate.</p>
         <table>
            <head>Recognition accuracy obtained with 0.6 dropout rate; best accuracy is shown in blue
               and the worst in red</head>
            <row role="label">
               <cell role="label">Models</cell>
               <cell role="label">
                  Batch Size 32
               </cell>
               <cell role="label">
                  Batch Size 64
               </cell>
            </row>
            <row>
               <cell>VGG-16</cell>
               <cell>62.50%</cell>
                        <cell>69.33%</cell>
            </row>
            <row>
               <cell>Inception-V3</cell>
               <cell>94.91%</cell>
                        <cell>93.83%</cell>
            </row>
            <row>
               <cell>ResNet-50</cell>
               <cell>95.50%</cell>
                        <cell>97.08%</cell>
            </row>
            <row>
               <cell>InceptionResnet-V2</cell>
               <cell>96.00%</cell>
                        <cell>96.33%</cell>
            </row>
         </table>
        
            
         <table>
            <head>Recognition accuracy obtained with 0.5 dropout rate; best accuracy is shown in blue and the worst in red</head>
            <row role="label">
               <cell role="label">Models</cell>
               <cell role="label">
                  Batch Size 32
               </cell>
               <cell role="label">
                  Batch Size 64
               </cell>
            </row>
            <row>
               <cell>VGG-16</cell>
               <cell>70.33%</cell>
                       <cell> 70.91%</cell>
            </row>
            <row>
               <cell>Inception-V3</cell>
               <cell>93.83%</cell>
                        <cell>96.33%</cell>
            </row>
            <row>
               <cell>ResNet-50</cell>
               <cell>96.92%</cell>
               <cell>87.58%</cell>
            </row>
            <row>
               <cell>InceptionResnet-V2</cell>
               <cell>96.66%</cell>
                        <cell>94.08%</cell>
            </row>
         </table>
        
            
            
         <p>Table III shows the accuracy breakdown of each character obtained from two different
                experiments. We labeled the best results in bold and worse results in italics. As shown
                in the table, VGG-16 fails in recognizing non-confusing characters such as "o" and "u/v". However, it performs well on few characters such as <q>m</q> and <q>y</q>.</p>
         <table>
            <head>Recognition accuracy per character. Best results are highlighted in bold and the worst results are highlighted in italics</head>
           
            <row>
               <cell role="label"> </cell>
               <cell role="label">Batch Size
                  = 32 &amp; Dropout rate = 0.5</cell>
               <cell role="label"> </cell>
               <cell role="label"> </cell>
               <cell role="label"> </cell>
               <cell role="label">Batch Size
                  = 64 &amp; Dropout rate = 0.5</cell>
               <cell role="label"> </cell>
               <cell role="label"> </cell>
               <cell role="label"> </cell>
            </row>
            <row role="label">
               <cell/>
               <cell role="label">VGG</cell>
               <cell role="label">Inception</cell>
               <cell role="label">ResNet</cell>
               <cell role="label">Inception</cell>
               <cell role="label">VGG</cell>
               <cell role="label">Inception</cell>
               <cell role="label">ResNet</cell>
               <cell role="label">Inception</cell>
            </row>
            <row role="label">
               <cell role="label"/>
               <cell role="label">16</cell>
               <cell role="label">v3</cell>
               <cell role="label">50</cell>
               <cell role="label"> Resnet v2</cell>
               <cell role="label">16</cell>
               <cell role="label">v3</cell>
               <cell role="label">50</cell>
               <cell role="label"> Resnet v2</cell>
            </row>
            <row>
               <cell>A</cell>
               <cell><emph>82%</emph></cell>
               <cell>94%</cell>
               <cell>94%</cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell>92%</cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell><emph>90%</emph></cell>
               <cell>92%</cell>
            </row>
            <row>
               <cell>B</cell>
               <cell><emph>82%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>96%</cell>
               <cell><emph>86%</emph></cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell>92%</cell>
            </row>
            <row>
               <cell>C</cell>
               <cell><emph>62%</emph></cell>
               <cell>92%</cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell><emph>42%</emph></cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell>86%</cell>
               <cell>92%</cell>
            </row>
            <row>
               <cell>c¸</cell>
               <cell><emph>76%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>74%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell>98%</cell>
            </row>
            <row>
               <cell>D</cell>
               <cell><emph>74%</emph></cell>
               <cell>92%</cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell><emph>74%</emph></cell>
               <cell>94%</cell>
               <cell>76%</cell>
               <cell><hi rend="bold">96%</hi></cell>
            </row>
            <row>
               <cell>E</cell>
               <cell><emph>36%</emph></cell>
               <cell>84%</cell>
               <cell><hi rend="bold">90%</hi></cell>
               <cell>84%</cell>
               <cell><emph>46%</emph></cell>
               <cell><hi rend="bold">90%</hi></cell>
               <cell>76%</cell>
               <cell>86%</cell>
            </row>
            <row>
               <cell>F</cell>
               <cell><emph>38%</emph></cell>
               <cell>86%</cell>
               <cell>88%</cell>
               <cell><hi rend="bold">92%</hi></cell>
               <cell><emph>60%</emph></cell>
               <cell>92%</cell>
               <cell>92%</cell>
               <cell><hi rend="bold">94%</hi></cell>
            </row>
            <row>
               <cell>G</cell>
               <cell><emph>22%</emph></cell>
               <cell>94%</cell>
               <cell>96%</cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell><emph>32%</emph></cell>
               <cell><hi rend="bold">88%</hi></cell>
               <cell>68%</cell>
               <cell>68%</cell>
            </row>
            <row>
               <cell>H</cell>
               <cell><emph>74%</emph></cell>
               <cell>98%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell><emph>68%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>78%</cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>I</cell>
               <cell><emph>64%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>96%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>64%</cell>
               <cell>96%</cell>
               <cell><emph>44%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>J</cell>
               <cell><emph>88%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell>96%</cell>
               <cell><emph>86%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>96%</cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>L</cell>
               <cell><emph>76%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>70%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>80%</cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>M</cell>
               <cell>98%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>96%</emph></cell>
               <cell><emph>96%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>N</cell>
               <cell><emph>90%</emph></cell>
               <cell>94%</cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell>76%</cell>
               <cell><hi rend="bold">92%</hi></cell>
               <cell><emph>56%</emph></cell>
               <cell>78%</cell>
            </row>
            <row>
               <cell>O</cell>
               <cell><emph>82%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>82%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>P</cell>
               <cell><emph>88%</emph></cell>
               <cell>98%</cell>
               <cell>98%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>94%</emph></cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell><hi rend="bold">98%</hi></cell>
            </row>
            <row>
               <cell>Q</cell>
               <cell><emph>74%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>78%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
            </row>
            <row>
               <cell>R</cell>
               <cell><emph>58%</emph></cell>
               <cell>92%</cell>
               <cell>96%</cell>
               <cell><hi rend="bold">98%</hi></cell>
               <cell><emph>44%</emph></cell>
               <cell>94%</cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell>90%</cell>
            </row>
            <row>
               <cell>S</cell>
               <cell><emph>74%</emph></cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell><hi rend="bold">94%</hi></cell>
               <cell>92%</cell>
               <cell><emph>58%</emph></cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell>94%</cell>
               <cell>90%</cell>
            </row>
            <row>
               <cell>T</cell>
               <cell><emph>64%</emph></cell>
               <cell>96%</cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>82%</emph></cell>
               <cell><hi rend="bold">96%</hi></cell>
               <cell>94%</cell>
               <cell><hi rend="bold">96%</hi></cell>
            </row>
            <row>
               <cell>u/v</cell>
               <cell><emph>68%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>64%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>X</cell>
               <cell><emph>52%</emph></cell>
               <cell>62%</cell>
               <cell><hi rend="bold">92%</hi></cell>
               <cell>90%</cell>
               <cell><emph>66%</emph></cell>
               <cell><hi rend="bold">90%</hi></cell>
               <cell>84%</cell>
               <cell><hi rend="bold">90%</hi></cell>
            </row>
            <row>
               <cell>Y</cell>
               <cell><emph>94%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><emph>88%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
            <row>
               <cell>Z</cell>
               <cell><emph>72%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell>98%</cell>
               <cell><emph>80%</emph></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
               <cell><hi rend="bold">100%</hi></cell>
            </row>
         </table>
         
         
         <p>Overall, VGG-16 performed the worst for most of our character datasets. Figure 6
                gives the graphs of accuracy and loss values for the training set with respect to
                the number of epochs. It shows that VGG-16 accuracy could have been improved if it
                trained with more epochs.</p>
         
         <figure>
            <head>The accuracy (left) and loss (right) curves on the training set of the
               CNN models</head>
            <graphic url="resources/images/Figure6.png"/>
            <figDesc>Images of eight line charts</figDesc>
         </figure>
         
         
           
         <p>To further understand why some models achieved low accuracy, we generated confusion
                matrices for all the models. Confusion matrices help us study the miss-classified
                characters. As seen in Figure 7, confusion matrices confirm that most of the
                confusions occur between the characters that are written similarly. For instance,
                the character <q>n</q> is confused with <q>r</q> as shown in Figure 7(a), and <q>g</q> is confused with <q>q</q> as shown in Figure 7(b). The results
                are not surprising as these characters generally confuse non-expert human readers
                and, occasionally, trained paleographers. Figure 8 shows samples of these
                characters. However, as shown on Table III, the recognition accuracy remains overall
                strong.
         </p>
         
         <figure>
            <head>(a) ResNet-50 Confusion
               Matrix (b) InceptionReset-v2 Confusion
               Matrix
               Confusion
               matrices of selected models to show the miss-classified
               characters</head>
            <graphic url="resources/images/Figure7.png"/>
            <figDesc>Two images of matrices</figDesc>
         </figure>
         
         <figure>
            <head>a) Example of the shape similarities between the characters <q>r</q> and <q>n</q>. 
               b) Example of the shape similarities between the characters <q>g,</q> <q>q,</q> and <q>y</q></head>
            <graphic url="resources/images/Figure8.png"/>
            <figDesc>Two images of handwriting</figDesc>
         </figure></div>
         
       
         </div>
         <div>
         <head>IV. CONCLUSION &amp; FUTURE WORK</head>
         <p>Historical handwritten character recognition is a challenging pattern recognition
                problem due to the inconsistency of the handwritten scripts and the lack of accurate
                labeled data. In this paper, we presented an empirical study on how state-of-
                the-art CNNs (developed for image classification) perform for the task of
                recognizing handwritten characters in seventeenth-century Spanish American notarial
                scripts. The labeled dataset employed in this study was carefully curated with the
                help of paleography experts and professional historians. Data augmentation was
                employed to increase the number of training samples. We observed that ResNet-50
                achieved the promising accuracy of 97.08% compared to InceptionResnet- V2,
                Inception-V3, and VGG-16, which achieved 96.66%, 96.33%, and 70.91%, respectively. </p>
         <p>Our study demonstrates that recent CNNs are promising to detect characters in
                seventeenth-century Spanish notarial scripts. Our future work will test the
                performance of deep learning-based OCR models such as Keras-OCR, YOLO-OCR, Tesseract
                and Kraken for the detection and recognition of handwritten words on these
                manuscripts. Accurate word recognition will be a necessary step in the development
                of a tool for reading, querying, and analyzing this historical collection. We plan
                model the content of these manuscripts in a form that would make information
                retrieval faster and better.</p>
         <p>The labeled dataset and software used in this study are publicly available on GitHub
                via <ref target="https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican">https://github.com/UMKC-BigDataLab/DeepLearningSpanishAmerican</ref>.</p></div>
         <div type="appendix" xml:id="acknowledgements">
         <head>ACKNOWLEDGEMENTS</head>
         <p>This research was supported by the NEH Digital Humanities Advancement Grant
                (HAA-271747-20), UMKC’s Missouri Institute for Energy and Defense (MIDE), UMKC’s
                Funding for Excellence Program, and a Collaborative Data Science Grant from UMKC’s
                Institute for Data Education, Analytics and Science (IDEAS). The authors would like
                thank Ryan Rowland, Maha Alrasheed, and Vania Todorova for labeling data as well as
                the <title rend="italic">Archivo General de la República Argentina</title> for granting
                their permission to use in this study their digitized collection of notary records.
                Dr. Martin L. E. Wasserman contributed to this project with his expertise in Spanish
                paleography. The first author (N. A.) would like to thank UMKC’s Women’s Council
                Graduate Assistance Fund (GAF) as well as the University of Tabuk in Saudi Arabia
                for sponsoring her scholarship.</p>
         </div>
         
         
      </body>
      <back>
         <listBibl>
            <bibl xml:id="abadi2015" label="Abadi et al. 2015">Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G. S., Davis
               A., Dean J., Devin M., Ghemawat S., Goodfellow I., Harp A., Irving G., Isard M., Jia
               Y., Jozefowicz R., Kaiser L., Kudlur M., Levenberg J., Mané D., Monga R., Moore S.,
               Murray D., Olah C., Schuster M., Shlens J., Steiner B., Sutskever I., Talwar K.,
               Tucker P., Vanhoucke V., Vasudevan V., Viégas F., Vinyals O., Warden P., Wattenberg
               M., Wicke M., Yu Y., and Zheng, X. <title rend="quotes">TensorFlow: Large-scale machine learning on
               heterogeneous systems,</title> 2015, software available <ref target="https://www.tensorflow.org/">https://www.tensorflow.org/</ref>
            </bibl>
            <bibl xml:id="alpertabrams2016" label="Alpert-Abrams 2016">Alpert-Abrams, H., <title rend="quotes">Machine Reading the <hi rend="italic">Primeros Libros</hi>,</title> <title rend="italic">DHQ</title>
               10, no. 4, 2016.</bibl>
            <bibl xml:id="ashiquzzaman2017" label="Ashiquzzaman and Tushar 2017">Ashiquzzaman A. and Tushar A. K., <title rend="quotes">Handwritten Arabic numeral recognition using deep
               learning neural networks,</title> in 2017 IEEE International Conference on Imaging, Vision
               &amp; Pattern Recognition (icIVPR). IEEE, 2017, pp. 1–4.</bibl>
            <bibl xml:id="bradski2000" label="Bradski 2000">Bradski G., <title rend="quotes">The OpenCV Library,</title> <title rend="italic">Dr. Dobb’s Journal of Software Tools</title>, 2000.</bibl>
            <bibl xml:id="chamchong2009" label="Chamchong and Fung 2009">Chamchong R. and Fung C. C., <title rend="quotes">Comparing background elimination approaches for
               processing of ancient Thai manuscripts on palm leaves,</title> in 2009 International
               Conference on Machine Learning and Cybernetics, vol. 6. IEEE, 2009, pp.
               3436–3441.</bibl>
            <bibl xml:id="chammas2018" label="Chammas et al. 2018">Chammas E., Mokbel C., and Likforman-Sulem L., <title rend="quotes">Handwriting recognition of historical
               documents with few labeled data,</title> in 2018 13th IAPR International Workshop on
               Document Analysis Systems (DAS). IEEE, 2018, pp. 43–48.</bibl>
            <bibl xml:id="chollet2015" label="Chollet et al. 2015">Chollet F. et al., <title rend="quotes">Keras,</title> <ref target="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ref>, 2015.</bibl>
            <bibl xml:id="clanuwat2019" label="Clanuwat">Clanuwat T., Lamb A., and. Kitamoto A, <title rend="quotes">Kuronet: Pre-modern Japanese Kuzushiji
               character recognition with deep learning,</title> arXiv preprint arXiv:1910.09433,
               2019.</bibl>
            <bibl xml:id="deng2009" label="Deng et al. 2009">Deng J., Dong W., Socher R., Kai Li L. Li, and Li Fei-Fei, <title rend="quotes">Imagenet: A large-scale
               hierarchical image database,</title> in 2009 IEEE Conference on Computer Vision and Pattern
               Recognition, 2009, pp. 248–255.</bibl>
            <bibl xml:id="granell2018" label="Granell et al. 2018">Granell E., Chammas E., Likforman-Sulem L., Martíınez-Hinarejos C.-D., Mokbel C., and
               Cîrstea B.-I., <title rend="quotes">Transcription of Spanish historical handwritten documents with deep
               neural networks,</title> <title rend="italic">Journal of Imaging</title>, vol. 4, no. 1, p. 15, 2018.</bibl>
            <bibl xml:id="he2016" label="He et al. 2016">He K., Zhang X., Ren S., and Sun J., <title rend="quotes">Deep residual learning for image recognition,</title>
               in Proceedings of the IEEE conference on computer vision and pattern recognition,
               2016, pp. 770–778.</bibl>
            <bibl xml:id="krizhevsky2012" label="Krizhevsky et al. 2012">Krizhevsky A., Sutskever I., and Hinton G. E., <title rend="quotes">Imagenet classification with deep
               convolutional neural networks,</title> in <title rend="italic">Advances in neural infor- mation processing
               systems</title>, 2012, pp. 1097–1105.</bibl>
            <bibl xml:id="kolsch2018" label="Kolsch et al. 2018">Kölsch A., Mishra A., Varshneya S., Afzal M.Z., and Liwicki M., <title rend="quotes">Recognizing
               challenging handwritten annotations with fully convolutional networks,</title> in 2018 16th
               International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE,
               2018, pp. 25–31.</bibl>
            <bibl xml:id="lechun1998" label="LeCun et al. 1998">LeCun Y., Bottou L., Bengio Y., and Haffner P., <title rend="quotes">Gradient-based learning applied to
               document recognition,</title> Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324,
               1998.</bibl>
            <bibl xml:id="lin2014" label="Lin et al. 2014">Lin, T-Y., Maire M., S. Belongie, J. Hays, Perona P., Ramanan D., Dollár P., and.
               Zitnick C. L, <title rend="quotes">Microsoft coco: Common objects in con- text,</title> in Computer Vision –
               ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham: Springer
               International Publishing, 2014, pp. 740–755.</bibl>
            <bibl xml:id="silvaprada2001" label="Silva Prada 2001">Silva Prada, N. <title rend="quotes">Paleografías americanas,</title> <ref target="https://www.openedition.org/21549">https://www.openedition.org/21549</ref>, 2001.r
            </bibl>
            <bibl xml:id="simonyan2014" label="Simonyan and Zisserman 2014">Simonyan K. and Zisserman A., <title rend="quotes">Very deep convolutional networks for large-scale image
               recognition,</title> arXiv preprint arXiv:1409.1556, 2014.</bibl>
            <bibl xml:id="szegedy2015" label="Szegedy et al. 2015">Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke
               V., and Rabinovich A., <title rend="quotes">Going deeper with convolutions,</title> in Proceedings of the IEEE
               conference on computer vision and pattern recognition, 2015, pp. 1–9. </bibl>
            <bibl xml:id="szegedy2016" label="Szegedy et al. 2016">Szegedy C., Vanhoucke V., Ioffe S., Shlens J., and. Wojna Z, <title rend="quotes">Rethinking the
               inception architecture for computer vision,</title> in Proceedings of the IEEE conference
               on computer vision and pattern recognition, 2016, pp. 2818–2826.</bibl>
            <bibl xml:id="szegedy2017" label="Szegedy et al. 2017">Szegedy C., Ioffe S., Vanhoucke V., and Alemi A. A., <title rend="quotes">Inception-v4, inception-resnet
               and the impact of residual connections on learning,</title> in Thirty-first AAAI conference
               on artificial intelligence, 2017.</bibl>
            <bibl xml:id="rabby2018" label="Rabby et al. 2018">Rabby A. S. A., Haque S., Islam S., Abujar S., and Hossain S. A., <title rend="quotes">Bornonet: Bangla
               handwritten characters recognition using convolutional neural network,</title> Procedia
               computer science, vol. 143, pp. 528– 535, 2018.</bibl>
            <bibl xml:id="tsai2016" label="Tsai 2016">Tsai C., <title rend="quotes">Recognizing handwritten Japanese characters using deep convolutional neural
               networks,</title> 2016.</bibl>
            <bibl xml:id="valy2018" label="Valy et al. 2018">Valy D., Verleysen M., Chhun S., and Burie J.-C., <title rend="quotes">Character and text recognition of
               Khmer historical palm leaf manuscripts,</title> in 2018 16th International Conference on
               Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018, pp. 13–18.</bibl>
            <bibl xml:id="vellingiriraj2016" label="Vellingiriraj et al. 2016">Vellingiriraj E., Balamurugan M., and Balasubramanie P., <title rend="quotes">Information extraction and
               text mining of ancient vattezhuthu characters in historical documents using image
               zoning,</title> in 2016 International Conference on Asian Language Processing (IALP). IEEE,
               2016, pp. 37–40.</bibl>
            <bibl xml:id="wasserman2018" label="Wasserman 2018">Wasserman, M.L.E., <title rend="quotes">La escritura paleográfica Iberoamericana: letras procesales y
               encadenadas,</title> in <title rend="italic">Introducción a la Paleografía. Herramientas para la Lectura y
               Anáisis de Documentos Antiguos</title>, ed. Rosana Vassallo (La Plata: Facultad de
               Humanidades y Ciencias de la Educación, 2018)</bibl>
            <bibl xml:id="kingma2015" label="Kingma and Ba 2015">Kingma, Diederik P. and Ba, Jimmy, <title rend="quotes">Adam: A Method for
               Stochastic Optimization,</title> arxiv:1412.6980, published as a conference paper at
               the 3rd International Conference for Learning Representations, San Diego,
               2015.</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
