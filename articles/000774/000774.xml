<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en">Information theory unravels the subtext in Chekhov</title>

                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>J. Nathanael <dhq:family>Philipp</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0003-0577-7831</idno>
                    <dhq:affiliation>Sächsische Akademie der Wissenschaften zu Leipzig</dhq:affiliation>
                    <email>nathanael@philipp.land</email>
                    <dhq:bio>
                        <p>J. Nathanael Philipp is a computer scientist with a background in computer linguistic. Currently a researcher at the Sächsische Akademie der Wissenschaften zu Leipzig in the research project Forschungsportal BACH.</p>
                    </dhq:bio>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Michael <dhq:family>Richter</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0001-7460-4139</idno>
                    <dhq:affiliation>Universität Leipzig</dhq:affiliation>
                    <email>mprrichter@gmail.com</email>
                    <dhq:bio>
                        <p>Michael Richter is Linguist and Computational Linguist. He held a chair in computational linguistics at the Justus Liebig University of Giessen (Germany) and was head of a research group at the University of Leipzig (Germany). His recent research interests are Information Theory and information-theoretic models for natural language.</p>
                    </dhq:bio>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Olav <dhq:family>Mueller-Reichau</dhq:family></dhq:author_name>
                    <dhq:affiliation>Universität Leipzig</dhq:affiliation>
                    <email>reichau@uni-leipzig.de</email>
                    <dhq:bio>
                        <p>Olav Mueller-Reichau is professor for Slavic Linguistics at Leipzig University.</p>
                    </dhq:bio>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Matthias <dhq:family>Irmer</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0002-7719-9594</idno>
                    <dhq:affiliation>Digital Science</dhq:affiliation>
                    <email>irmer@conceptmining.de</email>
                    <dhq:bio>
                        <p>atthias Irmer has a background in Linguistic Semantics and Pragmatics, as well as in Computer Science. He obtained his PhD from University of Leipzig and collected professional experience at OntoChem GmbH in Halle (Saale) with a focus on extracting knowledge from scientific publications.</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>

            	<!-- This information should be added when the file is created -->
                <idno type="DHQarticle-id">000774</idno>


            	<!-- This information will be completed at publication -->
                <idno type="volume">019</idno>
                <idno type="issue">2</idno>
                <date><!-- include @when with ISO date and also content in the form 23 February 2024 --></date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
<!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            	<taxonomy xml:id="project_keywords">
            		<bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
            	</taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->

                	<!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp=""/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item></item>
                    </list>
                </keywords>
            	<keywords scheme="#project_keywords">
            		<list type="simple">
            			<item></item>
            		</list>
            	</keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
        	<!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
        	<change>The version history for this file can be found on <ref target=
        		"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000774/000774.xml">GitHub
        	</ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <p>The present paper reports a pilot study to approach the subtext in Chekhov’s short-story "Ward No. 6" by means of information theory.
                    The original text is enriched by glosses by which we intend to make explicit the implicit knowledge conveyed by the original text, i.e., the subtext.
                    We generated several text variants with meaningful enrichments and one fake variant that served as a baseline. We could not observe that
                    semantic surprisal as a feature of words and uniform information density have a subtext effect throughout all text variants.
                    However, it turned out that kurtosis and skewness are suitable classification criteria to distinguish meaningful enrichments from fake enrichments.</p>
            </dhq:abstract>
            <dhq:teaser>
                <p>xx</p>
            </dhq:teaser>
        </front>
         <body>
                <div>
                    <head>1 Introduction</head>
                    <p>This paper is a first attempt to approach the subtext by means of information theory <ptr target="#shannon1948"/>. The object of study is a text by the Russian writer <name>Anton P. Chekhov</name>, namely the short-story "Ward No. 6" ("Palata No. 6" in the original).</p>

                    <p>The message conveyed by any text is composed of two kinds of meaning. The first is the explicit meaning, i.e. the meaning that is explicitly coded by the linguistic material appearing black on white.
                        The second is the meaning that a cooperative interpreter will infer from the explicit text following Gricean or Neo-Gricean reasoning <ptr target="#grice1989"/>, making use of both linguistic knowledge and non-linguistic resources (world knowledge, conversational context). Even the most explicit text leaves open numerous informational gaps, which must be filled by the interpreter. These <quote rend="inline">bridging inferences</quote> <ptr target="#irmer2011"/> constitute the implicit meaning of the text, for which we utilise the term subtext here. Another metaphor which is sometimes employed is the visible text as opposed to <hi rend="italic">dark text</hi> (e.g. <ptr target="#hinrichs2014"/>).
                    </p>

                    <p>The discourse about the subtext is rich (see <ptr target="#lelis2011"/>, <ptr target="#lelis2013"/> for a survey), and the notion is controversial. According to the Oxford Dictionary of literary terms <ptr target="#baldick2015"/>, the subtext is <quote rend="inline">any meaning or set of meanings which is implied rather than explicitly stated in a literary work, especially in a play.</quote> For the Russian Literary encyclopedia of terms and concepts <ptr target="#nikoljukin2003"/>, the subtext (“podtekst” in Russian) represents the <quote rend="inline">hidden sense of an utterance, stemming from the interaction of the literary meanings, the context, and the speech situation.</quote>
                        As can be seen, there are basically two ways of understanding what subtext is (<ptr target="#ermatova2010"/>, p. 72.) Under the first reading, the one described above, advocated by <ptr target="#baldick2015"/>, the term is basically synonymous to “pragmatic inferences.” Besides that, the term subtext is used in the literature also to refer to the ultimate sense of a literary text, i.e. to the interpretation that is intended by the author and that the reader has to decipher (e.g. <ptr target="#nikoljukin2003"/>, <ptr target="#myrkin1976"/>). In that second respect, the meaning of subtext comes close to the "moral of the story.” In what follows, we use subtext only in the first of these two readings.
                    </p>

                    <p>To approach the implicit meanings of a text, we will investigate the Russian short-story "Palata No. 6", written by <name>Anton P. Chekhov</name>, published in 1892. <name>Chekhov</name> is considered to have originated the role of the subtext, in his plays but also in his prose (e.g. <ptr target="#gatrall2003"/>, <ptr target="#mcsweeny2004"/>). Using a narrative text for our purposes has the advantage that the informational impact of the conversational context is reduced because the addressee (reader) does not have to calculate her own position as well as the position of the speaker (author).
                    </p>

                    <p>Related to the previous point, <name>Chekhov</name> is known for having developed a specific literary technique of <quote rend="inline">concentration and shortness</quote> (<ptr target="#klugečechov1995"/>, p. 48-49), manifesting itself in the language of the narrator, which usually shows simple syntax, parataxis, short sentences. This and the sparse use of adjectives, comparisons and metaphors is functional, for <name>Chekhov</name>’s intention is to challenge the reader intellectually and to encourage critical reading (<ptr target="#klugečechov1995"/>, p. 67) <ptr target="#whyman2010"/>. The reader is asked to actively interpret by filling the informational gaps of the texts.
                        Therefore, we may expect a <name>Chekhov</name> story to entail a drastic discrepancy between what is said explicitly and what is meant by the text. If the subtext is “a level of speech between the lines” <ptr target="#freise1997"/>, we may thus safely expect a lot of speech between the lines in the work of <name>Chekhov</name>.
                    </p>

                    <p>The general language of the Chekhovian short-story is the Russian literary language (<ptr target="#klugečechov1995"/>, p. 49), which is important for our methodology to be introduced below. The particular text "Ward No. 6" has been chosen, moreover, because of its considerable length compared to other short-stories of the author. For our methodology to work, we need a text of a certain cardinality of words.
                    </p>

                </div>
             <div>
                 <head>2. Outline</head>
                 <p>In this paper, we presuppose a probabilistic notion of linguistic meaning. The basic assumption is that the meaning of a word influences the probability with which other words accompany that word in a text. Given a sufficiently large number of text passages in which a word appears, it is in principal possible to obtain a co-occurrence profile that is characteristic of the word. We consider this profile, the <hi rend="italic">co-text</hi> of the word, to be a function of its meaning.

                     <note>This idea is taken from the Distributional Semantics. Its hypothesis is that words with similar meanings tend to appear in similar contexts which enables to measure their semantic relationships, see for instance <ptr target="#firth1957"/>, <ptr target="#harris1954"/>, <ptr target="#rubensteingoodenough1965"/>, <ptr target="#landauerdumais1997"/>, <ptr target="#turneypantel2010"/>, <ptr target="#mikolov_etal2013"/>.</note>
                 </p>

                 <p>Our point of departure is that information and the flow of information (FoI) are characterising features of texts. By <hi rend="italic">FoI</hi> we mean the distribution of information per word over time in sentences. Our approach is inspired by <ptr target="#dretske1981"/> who, in a Shannon information theoretic framework, was concerned with the relationship between knowledge and information in linguistic messages <ptr target="#dretske1981"/> states that Shannon information (SI) forms a framework for conveying meaning and its evolvement: in his view, information is a key component of knowledge, and knowledge acquisition is a result of the way information is processed.
                     In the present paper, we exploit this idea, likewise assuming that the flow of information is a classifying semantic feature of texts.
                 </p>

                 <p>We use contextualised information, i.e., <hi rend="italic">surprisal</hi> <ptr target="#tribus1961"/>, <ptr target="#hale2001"/>, <ptr target="#levy2008"/> as a lexical feature of a word <formula notation="tex" rend="inline">\(w\)</formula>.
                     </p>

                 <p>In order to approach the actual meaning of a word, in other words, we measure the effect that the meaning of the word has on its co-text.</p>

                 <p>This idea is at the heart of the Topic Context Model (TCM), see Section 3.2. TCM calculates the amount of <hi rend="italic">semantic surprisal</hi> of a word, i.e., the amount of SI, respectively, that can be derived from a semantic context.</p>

                 <p>In the due course, we use the terms <hi rend="italic">information</hi> and <hi rend="italic">information value</hi> as equivalents of <hi rend="italic">surprisal</hi> and <hi rend="italic">surprisal value</hi>, respectively, and we speak of the flow of surprisal (FoS) and the flow of information (FoI).</p>

                 <p>Since the surprisal of a word <formula notation="tex" rend="inline">\(w\)</formula> depends on the text in which it appears, it should make a difference whether the information carried by <formula notation="tex" rend="inline">\(w\)</formula> is calculated only on the basis of those words that literally constitute the novel (bare narration), or whether it is calculated on the basis of the narration enriched by additional word material. We take methodological advantage of this fact in the following way.</p>

                 <p>In addition to the bare narration, we set up a contextualised narration, which consists of the original text words plus additional texts that contextualise the original text words by adding encyclopedic knowledge about them. These additional texts, which we also refer to as glosses, serve as a (very simple) model of the subtext of the novel. Details on how the text is enriched by additional knowledge are given in Section 4. In order to check whether the expected change in lexical information through contextualisation, i.e. through the enrichment of the text through meaningful glossing,
                     is a random effect, the original text was also enriched with text fragments that have no relation to the Chekhovian story in terms of content, that is, a <hi rend="italic">fake glossing</hi>. After that, in a first work step, we carry out two measurements: (i) we determine the surprisal values of the words in the bare narration, (ii) we measure them again, but this time both in the meaningful and fake glossed narration.
                 </p>

                 <p>From FoS, we determine the information density in the text’s paragraphs. Information density is another notion that we need to introduce to explain our methodology. The <title rend="italic">Uniform Information Density</title> (UID) principle <ptr target="#fenk1980"/>, <ptr target="#jaegerlevy2006"/> says that the sender of a message prefers to distribute surprisal evenly and smoothly across a sequence of linguistic units in a sentence, utterance or in a text. FoS should not be slowed down or even brought to a standstill by large jumps in information.
                     This implies that the more the information in sentences or statements swings up or down, the higher the peaks and troughs of information, the more difficult it is for the recipient to process the information. Excessive information fluctuations can even prevent processing altogether.
                 </p>

                 <p>Information density characterises linguistic communication. Therefore, if we assume that the subtext plays an essential role in <name>Chekhov</name>’s prose (which we do, see above for motivation), we can expect that factoring in the subtext will lead to a more balanced FoI. In other words, we interpret density as the decisive discriminating feature in order to distinguish the narration enriched both by meaningful and fake subtexts from the bare narration.
                 </p>

                 <p>The measurement of meaningfully enriched text should show less peaks and troughs than the one of the bare narration. From that our first prediction follows: in meaningfully enriched texts, the density will be optimised compared to the original text. In contrast, with fake enrichments, the density is predicted to deteriorate drastically.
                 </p>

                 <p>H1 Adding meaningful glosses to the original text leads to a more balanced FoS.</p>

                 <p>H2 Adding fake glosses to the original text leads to a less balanced FoS.</p>

                 <p>What does that mean in terms of the principle of UID? In Section 3.3, we give the definition of UID that we follow in this paper: information density gets more <hi rend="italic">balanced</hi>, that is, better, when its value shifts towards zero. This represents on average small surprisal jumps from word to word and it represents uniformity. In contrast, when a UID value moves away from zero, it is a deterioration since this means that there are larger surprisal gaps between words. The dispersion of UID values should not be large.
                     If this were the case, it would indicate arbitrariness in the UID distribution. From that, our second prediction follows: a meaningful subtext will reduce the scatter of UID values compared to the original text. Consequently, a fake subtext will lead to stronger scattering.
                 </p>

                 <p>Models that employ information theory are rarely applied in digital humanities, and to the best of our knowledge, there are no studies that aim to disclose the subtext in the framework of information theory.
                 </p>

                 <p>In the next sections, we introduce some fundamentals of information and surprisal theory, and we present the Topic Context Model. Subsequently, we describe our methodology and the different enrichment strategies (glossings) that we employed. Finally, we report the experimental results and discuss the outcomes of our study.
                 </p>

             </div>
             <div>
                 <head>3. Information theoretic background</head>
             <div><head>3.1 Information theory and information as a lexical feature</head>
                 <p><name>Shannon</name>’s information theory <ptr target="#shannon1948"/>, <ptr target="#shannonweaver1949"/> models the transmission of information from a sender to a receiver. The information theoretic model aims to output a code in the form of consecutive 0 and 1 that compresses a message as much as possible without loss of information. <name>Shannon</name> developed his theory in an engineering context, and from the 1950s onwards the theory was applied to natural language.
             </p>

                 <p>Shannon Information (SI) is measured in bits. 1 bit is the information of two equally probable possibilities. For example, a toss with a fair coin carries 1 bit of information because there are two possible outcomes of a toss, i.e., <quote rend="inline">front of the coin,</quote> for instance represented by ‘1’, or <quote rend="inline">back of the coin,</quote> represented by ‘0’. The amount of bits equals the number of yes/no decisions needed to determine results of processes such as tossing a coin or determining a word in a chain, given a good algorithm.
                 </p>

                 <p>For the sake of illustration, imagine a toy language <formula notation="tex" rend="inline">\(L\)</formula> that comprises solely eight words, that is, <formula notation="tex" rend="inline">\(L= \{Anton, Chechov, was, a, very talented, Russian, writer\}\)</formula>. Lets agree that in <formula notation="tex" rend="inline">\(L\)</formula> any sentence must consist of eight words, and there are no rules of linear precedence in a sentence, so that for instance both <hi rend="italic">Anton a was very <name>Chekhov</name> talented writer Russian</hi> or a <hi rend="italic">was <name>Chekhov</name> Russian Anton talented writer very</hi> are possible sentences with the same probability. For a sentence in the toy language it needs three yes/no questions to determine a word w uniquely as shown in Figure 1.</p>

                 <p>As can easily be seen, the word Chekhov as terminal element in Figure 1 carries three bits of information. We get the same result when Formula 1 is applied. This expression is the standard definition of SI, i.e., the negative log-likelihood of a linguistic unit <formula notation="tex" rend="inline">\(w\)</formula>: </p>

                 <!-- formula 1 -->
                 <p><formula notation="tex" rend="block">$$SI(w) = −log_2P (w)$$</formula> (1)</p>



                 <p>Since the probability of a word <formula notation="tex" rend="inline">\(w\)</formula> in <formula notation="tex" rend="inline">\(L\)</formula> is <formula notation="tex" rend="inline">\(P(w) = \frac{1}{8}\)</formula>, we get for instance for the word Anton in Figure 1 below:</p>

                 <!-- formula 2 -->
                 <p><formula rend="block" notation="tex">$$SI(\text{Anton}) = -\log_2 P\left(\frac{1}{8}\right) = 3\ \text{bits}$$</formula> (2)</p>

                 <p>The information type Surprisal <ptr target="#hale2001"/>, <ptr target="#levy2008"/> is basically s a psycholinguistic concept
                     <note>Empirical evidence comes amongst others from <ptr target="#delong_etal2005"/>, <ptr target="#bentum2021"/>.</note>:



                     <figure xml:id="figure01">
                       <head>Information amount in bits of words in a sentence represented in a decision tree.</head>
                       <figDesc>A downloaded image of a decision tree including information bits of words in a sentence.</figDesc>
                       <graphic url="resources/images/figure01.png"></graphic>
                   </figure>

            surprisal of a linguistic unit is proportional to the mental processing effort that it causes <ptr target="#hale2001"/>. If the probability of a word is high in a given context, its surprisal and the effort for the processing of that word are low. For instance, given the word <hi rend="italic">Barack</hi> in a text, the probability is presumably quite high that <name><hi>Obama</hi></name> follows. In that case,<name>Obama</name> would carry low surprisal. An example of high surprisal because of the semantic unexpectedness of words is <name>Chomsky</name>’s famous sentence <hi rend="italic">colorless green ideas sleep furiously</hi>,
                     with which he wanted to make clear that semantically uninterpretable sentences can nevertheless be syntactically well-formed <ptr target="#chomsky1957"/>. To give another example: the sentence final <hi rend="italic">verb</hi> fell in the famous Garden Path-sentence <hi rend="italic">the horse raced past the barn fell</hi> is extremely unexpected and carries high surprisal <ptr target="#hale2001"/>.
                     Consequently, this requires a considerable processing. A language processor would probably assume that <hi rend="italic">barn</hi> is the sentence final word given the preceding context. The expression in Formula 3 defines surprisal, as a special case of SI as in Formula 1, as negative log-likelihood of a linguistic unit. The basis is its conditional probability which is in Formula 3 represented by the vertical bar.</p>

                 <!-- FORMULA 3 -->
                 <p><formula rend="block" notation="tex">$$surprisal(wi) = −log_2P (w_i|w_1,\dots,w_{i−1},CONTEXT)$$</formula> (3)</p>

                 <p>The variables <formula rend="inline" notation="tex">\(w_1,\dots,w_{i−1}\)</formula> represent co-occurrences of any kind of the targetwordwi withinasentence. The variable <hi rend="italic">CONTEXT</hi> represents extra-sentential contexts <ptr target="#levy2008"/>.</p>

                 <p>Technically, contexts for the calculation of surprisal can be co-occurrences of target linguistic units, such as n-grams of any type (e.g. terminal symbols and of part-of-speech tags) <ptr target="#horchreich2016"/>, syntactic structures <ptr target="#celano_etal2018"/>,
                     <ptr target="#rubino_etal2016"/>, <ptr target="#levshina2017"/>, <ptr target="#richter_etal2022"/>, <ptr target="#richter_etal2019"/> and also semantic contexts <ptr target="#kölbl_etal2020"/>, <ptr target="#kölbl_etal2021"/>, <ptr target="#philipp_etal2022"/>,
                     <ptr target="#philipp_etal2023"/>, <ptr target="#venhuizen_etal2019"/>.
                 </p>

                 </div>
                 <div>
                     <head>3.2 Topic Context Model</head>
                     <p>The Topic Context Model (TCM) <ptr target="#kölbl_etal2020"/>, <ptr target="#kölbl_etal2021"/>, <ptr target="#philipp_etal2022"/>, <ptr target="#philipp_etal2023"/>, <ptr target="#philipp_kölbletal2023"/>

                         <note>Available at: <ref target="https://github.com/jnphilipp/tcm">https://github.com/jnphilipp/tcm</ref></note>.

                              calculates the semantic surprisal of words relative to an extra-sentential context in a given corpus and predicts that a word carries high surprisal when its mean probability, that is, the expected value, in the topics of a document is low. The prerequisite for calculating the information content of a word in a text is the knowledge of the topics that the text contains. In general terms, a <hi rend="italic">topic</hi> is what a text document is about.
                         Depending on how coarse or fine-grained we look at a text, the text can deal with more or less topics. In this study we calculate the surprisal on a paragraph basis, meaning that we use the different paragraphs as the basis for calculating topics.</p>

                     <p>The topics that a paragraph covers are established by the process of topic detection. In this study, TCM relies on the topic detection model <title rend="italic">Latent Dirichlet Allocation</title> (LDA) <ptr target="#blei_etal2003"/> which is a widely used, successful model, but other models can also be used in TCM such as <title rend="italic">Latent Semantic Analysis</title>.
                     </p>

                     <p>The assumption behind LDA is that (i) each text document contains a statistical mixture of topics, that (ii) similar topics correlate with similar probability distributions of words, and that consequently (iii) each topic is characterised by a specific distribution of the words. How many topics LDA should detect can be chosen before calculation by means of the topic detection starts.
                     </p>

                     <p>Assume that LDA has disclosed two topics in a document, as illustrated in Figure 2:

                     <figure xml:id="figure02">
                             <head>Topics as probability distributions of words.
                             </head>
                            <figDesc>Downloaded image depicting Topic 1 and Topic 2</figDesc>
                             <graphic url="resources/images/figure02.png"></graphic>
                         </figure>

                    </p>

                     <p>In the yellow-coloured topic, high probabilities are assigned to words in the semantic field of, say, human created buildings.
                         The pink-coloured topic contains highly probable words from the automobile sector. It is hardly surprising that a word from the nutrition such as chocolate has a low probability in both topics.
                         TCM calculates the surprisal of chocolate per topic according to the Formulae 1 and 3 and then takes the mean value. In our example, in <formula notation="tex" rend="inline">\(P_{t1}(\text{chocolate}) = 0.00003\)</formula> and <formula notation="tex" rend="inline">\(\text{surprisal}(\text{chocolate}) = -\log_2 0.00003 = 15.02\ \text{bits}\)</formula> and <formula notation="tex" rend="inline">\(P_{t2}(\text{chocolate}) = 0.000003\)</formula> and <formula notation="tex" rend="inline">\(\text{surprisal}(\text{chocolate}) = -\log_2 0.000003 = 18.35\ \text{bits}\)</formula>,and the mean is <formula notation="tex" rend="inline">16.69</formula>, see
			Formula 4.
		    </p>

                         <note>The above explanations and the calculation 4 are, of course, a simplified description TCM. For more details, see <ptr target="#kölbl_etal2020"/>, <ptr target="#kölbl_etal2021"/>, <ptr target="#philipp_etal2022"/>, <ptr target="#philipp_etal2023"/>, <ptr target="#philipp_kölbletal2023"/>.</note>

                         <!-- formula 4 -->
                         <p><formula rend="block" notation="tex">$$surprisal(chocolate) = \frac{-\log_2(0.00003) + (-\log_2(0.000003))}{2} = \frac{15.02 + 18.35}{2} = 16.69\ \text{bits}$$</formula> (4)</p>


                     <p>The surprisal for a language processor who encounters the word <hi rend="italic">chocolate</hi> in the context of the two topics <hi rend="italic">buildings</hi> and <hi rend="italic">automobiles</hi> is therefore quite high.
                     </p>
                     <p>Figure 3 illustrates the working of the TCM.

                        <figure xml:id="figure03">
                            <head>Illustration of the Topic Context Model.</head>
                            <figDesc>A screenshot of an image depicting TCM text to surprisal.</figDesc>
                            <graphic url="resources/images/figure03.png"></graphic>
                        </figure>

                     </p>
                 </div>
                 <div>
                     <head>3.3 Uniform information density</head>

                     <p>Uniform Information Density (UID) is an important principle in linguistic communication of humans <ptr target="#levyjaeger2007"/>, <ptr target="#jaeger2010"/> but it is also important in artificial generation of language: in any linguistic product, information peaks and troughs must not be too extreme, so as not to make it too difficult for the recipient of a message to process it. In this study, we utilise the operationalisation of UID in <ptr target="#collins2014"/>.

                         <note>Available at: <ref target="https://github.com/jnphilipp/uid">https://github.com/jnphilipp/uid</ref></note>.
                     </p>

                     <p><formula notation="tex" rend="inline">\(UID_{LOCAL}\)</formula> is the measure of the average (squared) information change from word to word in a sentence. Formula 5 gives its definition: <formula notation="tex" rend="inline">\(id\)</formula> is the information / surprisal of a word, <formula notation="tex" rend="inline">\(n\)</formula> represents the total number of words in a sentence. Instead of <formula notation="tex" rend="inline">\(UID_{LOCAL}\)</formula> we use the term <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> <ptr target="#scheffler_etal2023"/>, <ptr target="#philipp_richter_etal2023"/>.</p>

                     <!-- formula 5 -->
                     <p><formula rend="block" notation="tex">$$UID_{wordwise} = -\frac{1}{n-1} \sum_{i=2}^{n} (id_i - id_{i-1})^2$$</formula> (5)</p>

                     <p><formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> is negative by definition, and therefore a <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> value close to zero indicates a high uniformity of the information density distribution, that is, on average smaller information jumps from word to word.
                     </p>

                     <p>In the Garden Path-example from above information does not flow very smoothly as becomes clear in Figure 4 from <ptr target="#hale2001"/>. The information is distributed fairly evenly over the first five words. However, the jump in information in the sentence-final word <hi rend="italic">fell</hi> is enormous:

                         <figure xml:id="figure04">
                             <head>Flow of information in a Garden Path-sentence.</head>
                             <figDesc>A screenshot of a bargraph titled Flow of Information, including the axia Information Values and Garden Path sentence</figDesc>
                             <graphic url="resources/images/figure04.png"></graphic>
                         </figure>

                     </p>

                     <p>The steep sentence final information peak threatens the successful processing of the sentence.
                     </p>

                     <p>The comparison of <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> in the "expectable" sequence and in the garden path clearly shows that in the latter, the value of <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> is further away from 0 (see (b) below) than in the expected sequence (see (a) below). This represents the lower uniformity of the information density in the garden path sentence. We use the information values from Figure 4 as input of expression 5:</p>

                     <!-- formula a -->
                     <p>(a) <formula rend="block" notation="tex">$$UID(\text{the horse raced past the barn}) = \frac{-(0 - 1)^2 + (1 - .191)^2 + (.191 - .064)^2 + (.064 - 0)^2 + (0 - 1)^2}{5} = -0.53$$</formula></p>

                     <!-- formula b -->
                     <p>(b) <formula rend="block" notation="tex">$$UID(\text{the horse raced past the barn fell}) = \frac{-(0 - 1)^2 + (1 - .191)^2 + (.191 - .064)^2 + (.064 - 0)^2 + (0 - 1)^2 + (1 - 5.91)^2}{6} = -4.46$$</formula></p>

                     <p>In the present study, we use UID as a measure of the <quote rend="inline">communicative naturalness</quote> of the Chekhovian original text and its manipulated versions. As should have become clear, we hypothesise that the original text alone, i.e. without its subtext, performs worse with respect to this measure than the original text enriched by its subtext. In the next section we explain how we set up the enrichments.</p>
                 </div>
                 <div>
                     <head>4 Enrichment</head>

                     <p>As noted above, the general methodology is to perform three different measurements of surprisal of the text words in Ward No. 6. First, we measure the surprisal of the words based on the word material of the narration alone. In a second measurement (or set of measurements, see below), we enrich the narration by a text addendum which is semantically related to the original text words. Finally, we use an additional text which is not content-related to the original text in any obvious way.
                         In the present section, we describe how we set up the meaningful text addendum, i.e. the meaningful glosses.</p>

                     <p>Recall that we conceive of the meaningful glosses as a toy model of the subtext of the narration. Accordingly, the enrichment shall make explicit the implicit knowledge conveyed by the text. To determine appropriate subtext explications, we make use of the idea brought forward by <title rend="italic">Frame Semantics</title> <ptr target="#fillmore1976"/> that lexical material in the form of content words evokes, when used in a text, conceptual frames that represent generalised conceptual knowledge associated with the respective content word <ptr target="#busse2012"/>.</p>

                     <p>Specifically, we use <title rend="italic">BabelNet</title> <ptr target="#ferrucci_etal2009"/>, a multilingual semantic network compiled from a variety of sources such as <title rend="italic">WordNet</title> and <title rend="italic">Wikipedia</title>, as an encyclopedic resource providing the conceptual knowledge associated with lexical material. <title rend="italic">BabelNet</title> provides a network of conceptual nodes (“senses”) interconnected by semantic relations of multiple types, e.g. hypernyms, hyperonyms, meronyms etc. The nodes contain translations to multiple languages and also glosses providing definitions or descriptions of the concepts.
                     </p>

                     <p>We set up a pipeline that automatically enriches a given text by adding information associated with content words available from BabelNet. The pipeline is based on Apache UIMA <ptr target="#ferrucci_etal2009"/>, uses open source software components and comprises the following steps:
                     <list type="simple">
                         <item><hi rend="bold">1. Part-of-speech (POS) Tagging</hi>: In this step, POS-tags are assigned to all word forms using <title rend="italic">DKPro HunPosTagger</title> <ptr target="#halácsy_etal2007"/>.
                         </item>
                         <item><hi rend="bold">2. Lemmatisation</hi>: using <title rend="italic">DK Pro Language Tool Lemmatizer</title> <ptr target="#dkpro"/>, we lemmatised the word forms. Further, stopwords, punctuation and duplicate words within paragraphs are removed.</item>
                         <item ><hi rend="bold">3. Word Sense Disambiguation (WSD)</hi>: we employed <title rend="italic">DKPro WSD</title> <ptr target="#dkpro"/> in combination with an extension that enables the use of <title rend="italic">BabelNet</title> <ptr target="#codina2018"/> to resolve ambiguous lemmas to their most probable sense. A number of disambiguation methods have been tried, with varying results.</item>
                     </list>
                         <list type="unordered">
                             <item><hi rend="bold">Lesk algorithm</hi>: this method looks up <title rend="italic">BabelNet</title> glosses and decides upon textual overlap of those in order to disambiguate.</item>

                             <item><hi rend="bold">Graph connectivity</hi>: this method creates a graph from <title rend="italic">BabelNet</title> neighbours; achieves poor results with all <title rend="italic">BabelNet</title>
                                 sense types, and better results with only hyper/hypo/mero/holonyms as neighbours and only <title rend="italic">WordNet senses</title>.</item>
                         </list>

                         <list type="simple">
                         <item><hi rend="bold">4. Enrichment Generation</hi>: we used the following method to enrich the text:</item>
                         </list>

                         <list type="unordered">
                             <item><hi rend="bold">Glosses</hi>: by adding <title rend="italic">BabelNet</title> glosses (i.e. descriptions of concepts) to the original text. Addition of glosses is done in parentheses right after the content words triggering them.
                             </item>
                         </list>

                         Overall, we created a number of enrichment variations resulting from varying the following parameters:
                         <list type="unordered">
                             <item>BabelNet lexicon</item>
                         </list>
                         <list type="simple">
                             <item>– ALL: all available lexicons</item>
                             <item>– WNTR: only WordNet translations</item>
                         </list>

                         <list type="unordered">
                             <item>WSD algorithm</item>
                         </list>

                         <list type="simple">
                             <item>– LESK: overlap of glosses</item>
                             <item>– GRAPH: graph connectivity</item>
                         </list>

                         <list type="unordered">
                             <item>Enrichment type</item>
                         </list>

                         <list type="simple">
                             <item>– GLOSSES: BabelNet glosses in parentheses after triggering lemma</item>
                         </list>

                         The data resource for the “fake” glosses was the Wikipedia based ”rus_news_2020_1M” corpus (1M sentences) from the "Wortschatz Leipzig" corpora collection.

                         <note>Available at: <ref target="https://wortschatz.uni-leipzig.de/de">https://wortschatz.uni-leipzig.de/de</ref></note>
                     </p>

                     <p>The core concept of our methodology is to observe the difference in the surprisal for each text word before and after meaningful or fake enrichments. Therefore, we employed TCM to calculate the surprisal for every token in the original text and for the four enrichments of each type. Table 1 gives an overview of the used enrichments and the size of the corpora.</p>

                     <p>When running LDA, we use the lemmatised tokens and ignored punctuations, stopwords and tokens shorter than three letters. Defining the number of topics is essential for LDA and has been determined experimentally for TCM calculation so far. We therefore calculated the surprisal values three times using three different numbers of topics, see Section 3.2 for the calculations.
                         We then averaged the three resulting surprisal values for the final surprisal value. Further, we normalised the surprisal values to have a consistent range of values among different experiments. Then, we calculated the information density distribution for every enrichment and the original text to observe how enrichments change the information density.</p>
                 </div>
                 <div>
                     <head>5 Results</head>

                     <p>Firstly, we tested whether the different text variants differ measurably in their information density.


                         <table>
                             <head>Overview of the different enrichments used in the experiment.</head>
                             <row role="label">
                                 <cell></cell>
                                 <cell>#Paragraphs</cell>
                                 <cell>#Tokens</cell>
                                 <cell>#Unique lemmas</cell>
                             </row>
                             <row role="data">
                                 <cell>Original Text</cell>
                                 <cell>186</cell>
                                 <cell>8398</cell>
                                 <cell>3336</cell>
                             </row>
                             <row role="data">
                                 <cell>ALL LESK</cell>
                                 <cell>186</cell>
                                 <cell>41914</cell>
                                 <cell>7417</cell>
                             </row>
                             <row role="data">
                                 <cell>WNTR LESK</cell>
                                 <cell>187</cell>
                                 <cell>31094</cell>
                                 <cell>5874</cell>
                             </row>
                             <row role="data">
                                 <cell>ALL GRAPH</cell>
                                 <cell>186</cell>
                                 <cell>25853</cell>
                                 <cell>5569</cell>
                             </row>
                             <row role="data">
                                 <cell>WINTR GRAPH</cell>
                                 <cell>187</cell>
                                 <cell>25531</cell>
                                 <cell>5272</cell>
                             </row>
                             <row role="data">
                                 <cell>ALL LESK FAKE</cell>
                                 <cell>187</cell>
                                 <cell>41601</cell>
                                 <cell>7594</cell>
                             </row>
                             <row role="data">
                                 <cell>ALL GRAPH FAKE</cell>
                                 <cell>187</cell>
                                 <cell>40868</cell>
                                 <cell>7491</cell>
                             </row>
                             <row role="data">
                                 <cell>WNTR LESK FAKE</cell>
                                 <cell>188</cell>
                                 <cell>36146</cell>
                                 <cell>7506</cell>
                             </row>
                             <row role="data">
                                 <cell>WNTR GRAPH FAKE</cell>
                                 <cell>188</cell>
                                 <cell>39253</cell>
                                 <cell>7528</cell>
                             </row>
                         </table>

                         To do this, we compared the meaningful values in the sets and used the non-parametric <title rend="italic">Mann-Whitney</title>-test.

                         <note><hi rend="italic">Non-parametric</hi> means that the test makes no or just minimal assumptions about the distribution within the data. An example may illustrate the question posed by statistical tests of mean-differences: the Dutch and the Danes are among the tallest populations in the world. On average, there is hardly any difference between them.
                             In terms of height, the Dutch and Danes belong to the same group, that is, the group of the tallest people. In Malta, for example, the average height of the population is much lower. The difference in average height between the Dutch and the Maltese is likely to be highly significant. This means that the Dutch and the Maltese do not belong to the same height group.</note>

                         The results of the <title rend="italic">Mann-Whitney</title>-tests are given in Table 2. A significance level of $$p \leq 0.05$$ means that there are strong, non-random differences between the sets.
                     </p>

                     <p>All mean differences of information density between the original text and the texts enriched with fake glossings are highly significant. In each case $$p \approx 0$$, the hypothesis of equality of means has thus to be rejected. That is to say, the information density in the original texts differs strongly from the density in fake glossed texts. Statistically speaking, original texts and fake glossed texts are therefore different sets.</p>

                     <p>Weaker pronounced is the difference between the information density of the original text and the meaningfully glossed text. Approximately half of the comparisons lack significance, and the comparison of the original text with meaningful glossing sourced from all lexicons and graph connectivity is highly non-significant ($$p = 0.30$$).
                     </p>


                         <table>
                             <head>Mann–Whitney U test for mean differences in UIDs and p-values.</head>
                             <row role="label">
                                 <cell>Date</cell>
                                 <cell>Mann-Whitney U</cell>
                                 <cell>p-value</cell>
                             </row>
                             <row role="label">
                                 <cell>Meaningful glosses</cell>
                             </row>
                             <row role="data">
                                 <cell>original - ALL GRAPH</cell>
                                 <cell>16229.0</cell>
                                 <cell>0.3028</cell>
                             </row>
                             <row role="data">
                                 <cell>original - ALL LESK</cell>
                                 <cell>27039.0</cell>
                                 <cell>≈ 0</cell>
                             </row>
                             <row role="data">
                                 <cell>original - WNTR GRAPH</cell>
                                 <cell>19302.0</cell>
                                 <cell>0.0665</cell>
                             </row>
                             <row role="data">
                                 <cell>original - WNTR LESK</cell>
                                 <cell>21425.0</cell>
                                 <cell>0.0001</cell>
                             </row>
                             <row role="label">
                                 <cell>Fake glosses</cell>
                             </row>
                             <row role="data">
                                 <cell>original - WNTR LESK</cell>
                                 <cell>34560.0</cell>
                                 <cell>≈0</cell>
                             </row>
                             <row role="data">
                                 <cell>original - WNTR GRAPH</cell>
                                 <cell>34545.0</cell>
                                 <cell>≈0</cell>
                             </row>
                             <row role="data">
                                 <cell>original - ALL GRAPH</cell>
                                 <cell>34395.0</cell>
                                 <cell>≈0</cell>
                             </row>
                             <row role="data">
                                 <cell>original - ALL LESK</cell>
                                 <cell>34284.0</cell>
                                 <cell>≈0</cell>
                             </row>
                         </table>



                         <p>The following figures compare the mean values of the <formula rend="inline" notation="tex">\(UID_{wordwise}\)</formula>-values and their dispersion in original, meaningful and fake glossed texts. A mean tends towards 0 means that many of the sentences in a data set show little wordwise surprisal-changes which means that the information density is tendentially uniform. Recall that <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> is negative by definition.
                     </p>

                     <p>Figure 5 displays that for the All-GRAPH-method employing word sense disambiguation (WSD) via graph connectivity, meaningful glossing make the density of surprisal more uniform compared to the original text, albeit only a little bit (5b). Conversely, in fake glosses (5b) information density in sentences is much less uniform. Meaningful glossing reduces the dispersion of UID-values compared to the original text (5a), whereas fake glossing results in a substantially stronger dispersion of the UID-values (5b).</p>

                     <p>Considering only LESK disambiguation, the scenario alters clearly: meaningful glosses fail to make the information density more uniform, even worsening it while, however, decreasing the dispersion (5c). Fake glosses make information density less uniform, accompanied by heightened dispersion (5d).</p>

                     <p>For the WNTR-glossed method with GRAPH connectivity, Figure 6 shows as well that meaningful glossing does not manage to make information distribution more uniform, rather the contrary is the case. However the dispersion is reduced (6a) which is an improvement. The same is the case with LESK disambiguation (6c). With the fake glosses, the information density gets less uniform while, in contrast to meaningful glossing, the dispersion of the density values increases (6b/6d).

                        <figure xml:id="figure05">
                            <head>Comparison of means of original and meaningful / fake glossed texts based on UIDs for ALL-glossed method.</head>
                             <figDesc>A screenshot of four graphs, comparisons of means of original and meaninful/fake glossed texts.</figDesc>
                             <graphic url="resources/images/figure05.png"></graphic>
                         </figure>

                     </p>
                     <p>The plots in the Figures 7 and 8 depict probability distributions of UID values for the glossing types ALL GRAPH (ALL GRAPH method with WSD via graph connectivity), ALL LESK, WNTR GRAPH and WNTR LESK data.

                         <note>The area under the curves must be approximately 1 in each case.</note>

                             The x-axis gives the <formula notation="tex" rend="inline">\(UID_{wordwise}\)</formula> values, the y-axis gives the probabilities.</p>

                     <p>Firstly, all plots show very clearly that fake glossed texts have distributions of UID-values reminiscent of a flattened normal distribution halfway, so to speak, to a uniform distribution: we see flattened peaks,
                         and the edges of the distributions run wide to the left and right meaning the probability of extreme values is high. The distribution of the fake glossed text exhibits low kurtosis, i.e., peakiness.

                         <figure xml:id="figure06">
                             <head>Comparison of means of original and meaningful / fake glossed texts based on UIDs for WNTR-glossed method.</head>
                             <figDesc></figDesc>
                             <graphic url="resources/images/figure06.png"></graphic>
                         </figure>

                         <note>Available at: <ref target="https://en.wikipedia.org/wiki/Kurtosis">https://en.wikipedia.org/wiki/Kurtosis</ref></note>
                     </p>
                     <p>Secondly, the plots in the Figures 7a and 8 visualise that meaningfully enriched texts exhibit higher <hi rend="italic">kurtosis</hi> compared to the original text relative to fake enriched texts, that is to say, they have a higher, steeper peak.
                         However, this does not seem to hold when LESK algorithm is employed as becomes visible in Figure 7b: here, the density distribution in the original text seems to be more peaky than in the meaningful enriched text,
                         and the distribution in the latter runs out far to the left and right. Figure 7b seems to show that the generalisation about kurtosis, i.e., that a meaningful enriched texts has a more peaky density distribution than the original text, does not hold.
                         We aimed to confirm this suspicion and chose an alternative graphical representation of the density distribution in the LESK text with a <hi rend="italic">Cullen and Frey-Graph</hi> in Figure 9.

                     <figure xml:id="figure07">
                         <head>Information density of meaningfully/fake enriched texts and original texts.</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure07.png"></graphic>
                     </figure>

                     It visualises the proximity and distance of LESK distributions with a set of theoretical distributions. The x-axis gives the degree of skewness of a distribution, and the y-axis gives the <hi rend="italic">kurtosis</hi>.

                         <note>Available at: <ref target="https://en.wikipedia.org/wiki/Skewness">https://en.wikipedia.org/wiki/Skewness</ref></note>

                         Figure 9 discloses that the ALL LESK density-distribution

                         <figure xml:id="figure08">
                             <head>Information density of meaningfully/fake enriched texts and original texts.</head>
                             <figDesc></figDesc>
                             <graphic url="resources/images/figure08.png"></graphic>
                         </figure>

                         has higher kurtosis than the distribution in the original text, and the plot thus confirms the generalisation from above that the density distribution in meaningfully enriched texts is more peaky than the distribution in the original text. In addition, Figure 7b shows very clearly the proximity of the fake distribution to the uniform distribution that we observed already in the Figures 7a and 8. So, can we infer a subtext effect from these results? We will return to this research question posed above in the Section 6.

                     <figure xml:id="figure09">
                         <head>Cullen and Frey graph for ALL LESK GLOSSES with fake and meaningful enrichment’s and original text.</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure09.png"></graphic>
                     </figure>

                     </p>
                 </div>
                 <div>
                     <head>6 Discussion and conclusion</head>
                     <p>The study confirms H2, and it confirms H1 solely for the enrichment combination ALL GRAPH (i.e. all available lexicons and the graphtheoretical disambiguation model), albeit the improvement is only marginal. Just for this case we observe that meaningful glossing improves the FoI, and that, in addition, the dispersion of density values decreases.
                         Thus, with ALL GRAPH enrichments, the expected effect, albeit weak, occurs. In contrast, no such effect occurs with the rest of the meaningful enrichments: although the dispersion generally decreases (except for the LESK algorithm on all lexicons), the information density generally gets worse. So, we have to draw an overall negative conclusion: taking into calculation the otherwise invisible subtext, which we implemented by different versions of glosses semantically related to the original text, does not lead to a more uniform information density.</p>

                     <p>Our study also produced a very positive result, however, because we managed to determine a parameter that does correspond positively to the addition of meaningful glosses and negatively to the addition of fake glosses: peakiness/kurtosis. When fake enrichments are added to the original text, the distribution of UID-values flattens, which means that it approaches the contour of the uniform distribution
                         (the uniform distribution results from a maximally fake text in which every word is chosen arbitrarily, see Figure 10). When meaningfully related glosses of the same cardinality are added to the original text, by contrast, the resulting contour shows a higher peak compared to the one of the original text.</p>

                     <p>The described finding comes out clearly in Figures 7a, 8a, and 8b. It is only with respect to enrichment type ALL LESK that the difference cannot be read from the respective plot, see 7b. However, the alternative visual representation in Figure 9 shows that also in this case meaningful enrichments lead to a higher kurtosis value whereas fake enrichments lead to a lower kurtosis value compared to the kurtosis value of the original text.
                         This can be taken as a confirmation that the text features <hi rend="italic">surprisal</hi> and <hi rend="italic">information density</hi>, so to speak, measure semantic properties. We may confidently conclude that we attested a subtext effect.</p>

                     <p>Although we have shown that information theory may unravel the subtext in <name>Chekhov</name>, the observed subtext effects are subtle. This may be due to that the glosses that we used in the documented experiment are too primitive to reproduce any more clear subtext effect.
                         Recall the basic idea underlying this study: if the subtext is an inalienable part of the semantics of a meaningful text, measuring the surprisal values of original texts alone will produce values that reflect the actual content of the text only incompletely. To measure the true surprisal of a text, one would have to enrich the original text by the perfect explication of the subtext.

                     <figure xml:id="figure10">
                         <head>Uniform Distribution generated from 10,000 random numbers.</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure10.png"></graphic>
                     </figure>

                         The (sub)textual prostheses that we actually used in our experiments are obviously very primitive enrichments, far from being perfect.
                         Future research will therefore have to focus on the technique of enrichment.
                     </p>

                     <p>While in this study, the generation of the subtexts was based in principle on the Fillmore model, future experiments will be carried out statistically based by employing neural networks. Doing so while keeping the features <hi rend="italic">information density</hi> and <hi rend="italic">kurtosis/skewness</hi>, we hope to achieve and observe a more significant subtext effect than in the current study.

                     </p>
                 </div>
             </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="shannon1948" label="Shannon 1948">Shannon, C.E. (1948) <title rend="quotes">A mathematical theory of communication</title>, <title rend="italic">The Bell system technical journal</title> 27(3), pp. 379-423.
                </bibl>

                <bibl xml:id="grice1989" label="Grice 1989">Grice, P. (1989) <title rend="italic">Studies in the way of words</title>. Cambridge: Harvard University Press.
                </bibl>

                <bibl xml:id="irmer2011" label="Irmer 2011">Irmer, M. (2011) <title rend="italic">Bridging inferences</title>. Boston: de Gruyter.
                </bibl>

                <bibl xml:id="hinrichs2014" label="Hinrichs 2014">Hinrichs, U. (2014) <title rend="italic">Die dunkle materie des wissens: Über Leerstellen wissenschaftlicher Erkenntnis</title>. Giessen, Germany: Psychosozial-Verlag.
                </bibl>

                <bibl xml:id="lelis2011" label="Lelis 2011">Lelis, E.I. (2011) <title rend="quotes">Podtekst i smežnye javlenija</title>, <title rend="italic">Istorija i filologija</title>, 4, pp. 143–151.</bibl>

                <bibl xml:id="lelis2013" label="Lelis 2013">Lelis, E.I. (2013) <title rend="italic">Podtekst kak lingvopoėtičes kajakategorija v proze</title>. Iževsk, Russia: A.P. Čechova.</bibl>

                <bibl xml:id="baldick2015" label="Baldick 2015">Baldick, C. (2015) <title rend="italic">The Oxford dictionary of literary terms</title>. Oxford: Oxford University Press.</bibl>

                <bibl xml:id="nikoljukin2003" label="Nikoljukin 2003">Nikoljukin, A.N. (2003) <title rend="italic">Literaturnaja ėnciklopedija terminov iponjatij</title>. Moskva, Russia: NPK ”Intelvak".
            </bibl>

                <bibl xml:id="ermatova2010" label="Ermatova 2010">Ermatova, E.V. (2010) <title rend="quotes">Implicitnost’ v chudožestvennom tekste</title>, <title rend="italic">Izdatel’stvo Saratovskogo universiteta</title>.
                </bibl>

                <bibl xml:id="myrkin1976" label="Myrkin 1976">Myrkin, V.J. (1976) <title rend="quotes">Tekst, podtekst i kontekst</title>, <title rend="italic">Voprosy jazykoznanija</title>, 2.
                </bibl>

                <bibl xml:id="gatrall2003" label="Gatrall 2003">Gatrall, J.J. (2003) <title rend="quotes">The paradox of melancholy insight: Reading the medical subtext in chekhov’s “a boring story”</title>, <title rend="italic">Slavic Review</title>, 62(2), pp. 258-277.
                </bibl>

                <bibl xml:id="mcsweeny2004" label="McSweeny 2004">McSweeny, K. (2004) <title rend="quotes">Effects or subtexts?</title>, <title rend="italic">Modern Language Studies</title>, 34(1-2), pp. 42-51.
                </bibl>

                <bibl xml:id="klugečechov1995" label="Kluge and Čechov 1995">Kluge, R.D. and Čechov, A.P. (1995) <title rend="italic">Eine Einführung in Leben und Werk</title>. Darmstadt: Wissenschaftliche Buchgesellschaft.
                </bibl>

                <bibl xml:id="whyman2010" label="Whyman 2010">R. Whyman, R. (2010) <title rend="italic">Anton Chekhov</title>. London: Routledge.
                </bibl>

                <bibl xml:id="freise1997" label="Freise 1997">Freise, M. (1997) <title rend="italic">Die prosa Anton Čechovs: eine Untersuchung im Ausgang von Einzelanalysen</title>. Amsterdam: Rodopi.
                </bibl>

                <bibl xml:id="firth1957" label="Firth 1957">Firth, R. (1957) <title rend="italic">Two papers on linguistic theory: Studies in linguistic analysis (Special volume of the philological society)</title>. Oxford: Blackwell.
                </bibl>

                <bibl xml:id="harris1954" label="Harris 1954">Harris, Z.S. (1954) <title rend="quotes">Distributional structure</title>, <title rend="italic">Word</title>, 10(2-3), pp. 146-162.</bibl>

                <bibl xml:id="rubensteingoodenough1965" label="Rubenstein and Goodenough 1965">Rubenstein, H. and Goodenough, J.B. (1965) <title rend="quotes">Contextual correlates of synonymy</title>, <title rend="italic">Communications of the ACM</title>, 8(10), pp. 627-633.
                </bibl>

                <bibl xml:id="landauerdumais1997" label="Landauer and Dumais 1997">Landauer, T.K. and Dumais, S.T. (1997) <title rend="quotes">A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>, <title rend="italic"> Psychological Review</title>, 104(2), pp. 211-240.
                </bibl>

                <bibl xml:id="turneypantel2010" label="Turney and Pantel 2010">Turney, P.D. and Pantel, P. (2010) <title rend="quotes">From frequency to meaning: Vector space models of semantics</title>, <title rend="italic">Journal of Artificial Intelligence Research</title>, 37, pp. 141-188.
                </bibl>

                <bibl xml:id="mikolov_etal2013" label="Mikolov et al 2013">Mikolov, T., Chen, K., Corrado, J., and Dean, J. (2013) <title rend="quotes">Efficient estimation of word representations in vector space</title>, <title rend="italic">Proceedings of the International Conference on Learning Representations (ICLR)</title>.
                </bibl>

                <bibl xml:id="baroni_etal2014" label="Baroni et al 2014">Baroni, M., Dinu, G., and Kruszewski, G. (2014) <title rend="quotes">Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>, <title rend="italic">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>, 1, pp. 238-247.
                </bibl>

                <bibl xml:id="dretske1981" label="Dretske (1981)">Dretske, F. (1981) <title rend="italic">Knowledge and the Flow of Information</title>. Cambridge: MIT Press.
                </bibl>

                <bibl xml:id="tribus1961" label="Tribus 1961">Tribus, M. (1961) <title rend="quotes">Information theory as the basis for thermostatics and thermodynamics</title>, <title rend="italic">Journal of Applied Mechanics</title>, 28(1), pp. 1-8.
                </bibl>

                <bibl xml:id="hale2001" label="Hale 2001">Hale, J. (2001) <title rend="quotes">A probabilistic Earley parser as a psycholinguistic model</title>, in <title rend="italic">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, Association for Computational Linguistics</title>, pp. 1–8.
                </bibl>

                <bibl xml:id="levy2008" label="Levy 2008">Levy, R. (2008) <title rend="quotes">Expectation-based syntactic comprehension</title>, <title rend="italic">Cognition</title>, 106(3), pp. 1126-1177. <ref target="https://doi.org/10.1016/j.cognition.2007.05.06">https://doi.org/10.1016/j.cognition.2007.05.06</ref>.
                </bibl>

                <bibl xml:id="fenk1980" label="Fenk and Fenk 1980">Fenk, A. and Fenk, G. (1980) <title rend="quotes">Konstanz im Kurzzeitgedächtnis-Konstanz im sprachlichen Informationsfluß</title>, <title rend="italic">Zeitschrift für experimentelle und angewandte Psychologie</title>, 27(3), pp. 400-414.
                </bibl>

                <bibl xml:id="jaegerlevy2006" label="Jaeger and Levy 2006">Jaeger, T. and Levy, R. (2006) <title rend="quotes">Speakers optimize information density through syntactic reduction</title>, <title rend="italic">Advances in neural information processing systems</title>, 19.
                </bibl>

                <bibl xml:id="shannonweaver1949" label="Shannon and Weaver 1949">Shannon, C.E. and Weaver, W. (1949) <title rend="italic">The mathematical theory of communication</title>. Champaign: University of Illinois Press.
                </bibl>

                <bibl xml:id="delong_etal2005" label="Delong et al 2005">Delong, K.A., Urbach, T.P., and Kutas, M. (2005) <title rend="quotes">Probabilistic word preactivation during language comprehension inferred from electrical brain activity</title>, <title rend="italic">Nature neuroscience</title>, 8(8), pp. 1117-1121.
                </bibl>

                <bibl xml:id="bentum2021" label="Bentum 2021">Bentum, M. (2021) <title rend="quotes">Listening with great expectations: A study of predictive natural speech processing, Ph.D. thesis</title>, <title rend="italic">Radbound Repository</title>.
                </bibl>

                <bibl xml:id="chomsky1957" label="Chomsky 1957">Chomsky, N. (1957) <title rend="quotes">Syntactic structures</title>. The Hague: Mouton and Co.
                </bibl>

                <bibl xml:id="horchreich2016" label="Horch and Reich 2016">Horch, E. and Reich, I. (2016) <title rend="quotes">On “article omission” in German and the “uniform information density hypothesis”</title>, in <title rend="italic">Proceedings of the 13th Conference on Natural Language Processing (KONVENS 2016): Bochumer Linguistische Arbeitsberichte</title>, 16, pp. 125-127.
                </bibl>

                <bibl xml:id="celano_etal2018" label="Celano et al 2018">Celano, G.G., Richter, M., Voll, R., and Heyer, G. (2018) <title rend="quotes">Aspect coding asymmetries of verbs: The case of Russian</title>, in <title rend="italic">PROCEEDINGS of the 14th Conference on Natural Language Processing</title>, pp. 34-39.
                </bibl>

                <bibl xml:id="rubino_etal2016" label="Rubino et al 2016">Rubino, R., Lapshinova-Koltunski, E., and Van Genabith, J. (2016) <title rend="quotes"> Information density and quality estimation features as translationese in dicators for human translation classification</title>, in <title rend="italic">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies</title>, pp. 960-970.
                </bibl>

                <bibl xml:id="levshina2017" label="Levshina 2017">Levshina, N. (2017) <title rend="quotes">Communicative efficiency and syntactic predictability: A cross-linguistic study based on the universal dependencies corpora</title>, in <title rend="italic">Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies</title> Gothenburg, Sweden: Linköping University Electronic Press, pp. 72-78.
                </bibl>

                <bibl xml:id="richter_etal2022" label="Richter et al 2022">Richter, M., Farré, M.B., Kölbl, M., Kyogoku, Y., Philipp, J.N., Yousef, T., Heyer, G., and Himmelmann, N.P. (2022) <title rend="quotes">Uniform density in linguistic information derived from dependency structures</title>, in <title rend="italic">Proceedings of the 14th International Conference on Agents and Artificial Intelligence - Volume 1: NLPinAI INSTICC</title>. Setúbal, Portugal: SciTePress, pp. 496-503. Available at: <ref target=" https://doi.org/10.5220/0000155600003116"> https://doi.org/10.5220/0000155600003116</ref>.
                </bibl>

                <bibl xml:id="richter_etal2019" label="Richter et al 2019">Richter, M., Kyogoku, Y., and Kölbl, M. (2019) <title rend="quotes"> Estimation of average information content: Comparison of impact of contexts</title>, in <title rend="italic">Proceedings of SAI Intelligent Systems Conference</title>. London: Springer, pp. 1251-1257. Available at: <ref target="https://doi.org/10.1007/978-3-030-29513-4_91">https://doi.org/10.1007/978-3-030-29513-4_91</ref>.
                </bibl>

                <bibl xml:id="kölbl_etal2020" label="Kölbl et al 2020">Kölbl, M., Kyogoku, Y., Philipp, J., Richter, M., Rietdorf, C., and Yousef, T. (2020) <title rend="quotes">Keyword extraction in german: Information-theory vs. deep learning</title>, in <title rend="italic">Proceedings of the 12th International Conference on Agents and Artificial Intelligence- Volume 1: NLPinAI INSTICC.</title> Setúbal, Portugal: SciTePress, pp. 459-464. Available at: <ref target="https://doi.org/10.5220/0009374704590464">https://doi.org/10.5220/0009374704590464</ref>.
                </bibl>

                <bibl xml:id="kölbl_etal2021" label="Kölbl et al 2021">Kölbl, M., Kyogoku, Y., Philipp, J.N., Richter, M., Rietdorf, C., and Yousef, T. (2021) <title rend="quotes">The semantic level of shannon information: Are highly informative words good keywords? A study on German</title>, <title rend="italic">Vol. 939 of Studies in Computational Intelligence (SCI)</title>. London: Springer International Publishing, pp. 139-161. Available at: <ref target="https://doi.org/10.1007/978-3-030-63787-3_5">https://doi.org/10.1007/978-3-030-63787-3_5</ref>.
                </bibl>

                <bibl xml:id="philipp_etal2022" label="Philipp et al 2022">Philipp, J.N., Kölbl, M., Kyogoku, Y., Yousef, T., and Richter, M. (2022) <title rend="quotes">  One step beyond: Keyword extraction in german utilising surprisal from topic contexts</title>, in K. Arai (ed.) <title rend="italic">Intelligent Computing</title>. London: Springer International Publishing, pp. 774-786. Available at: <ref target="https://doi.org/10.1007/978-3-031-10464-0_53">https://doi.org/10.1007/978-3-031-10464-0_53</ref>.
                </bibl>

                <bibl xml:id="philipp_etal2023" label="Philipp et al 2023">Phillipp, J.N., Richter, M., Daas, E., and Kölbl, M. (2023) <title rend="quotes">Are idioms surprising?</title>, in  M. Georges, A. Herygers, A. Friedrich, B. Roth (eds.) <title rend="italic">Proceedings of the 19th Conference on Natural Language Processing (KONVENS 2023), Association for Computational Lingustics</title>, pp. 149-154. Available at: <ref target="https://aclanthology.org/2023.konvens-main.15">https://aclanthology.org/2023.konvens-main.15</ref>.
                </bibl>

                <bibl xml:id="venhuizen_etal2019" label="Venhuizen et al 2019">Venhuizen, N.J., Crocker, M.W., and Brouwer, H. (2019) <title rend="quotes">Expectation-based comprehension: Modeling the interaction of world knowledge and linguistic experience</title>, <title rend="italic">Discourse Processes</title>, 56(3), pp. 229-255.
                </bibl>

                <bibl xml:id="philipp_kölbletal2023" label="Philipp and Kölbl et al 2023">Philipp, J.N., Kölbl, M., Daas, E., Kyogoku, Y., and Richter, M. (2023) <title rend="quotes">Perplexed by idioms?</title>, in <title rend="italic">Knowledge Graphs: Semantics, Machine Learning, and Languages</title>. Amsterdam: IOS Press, pp. 70–76.
                </bibl>

                <bibl xml:id="blei_etal2003" label="Blei et al 2003">Bleu, D.M., Ng, A.Y., and Jordan, M.I. (2003) <title rend="quotes">Latent dirichlet allocation</title>, <title rend="italic">Journal of machine Learning research</title>, 3(Jan), pp. 993-1022.
                </bibl>

                <bibl xml:id="levyjaeger2007" label="Levy and Jaeger 2007">Levy, R. and Jaeger, T. (2007) <title rend="quotes"> Speakers optimize information density through syntactic reduction</title>, <title rend="italic">Advances in neural information processing systems</title>, 19, pp. 849-856.
                </bibl>

                <bibl xml:id="jaeger2010" label="Jaeger 2010">Jaeger, T.F. (2010) <title rend="quotes">Redundancy and reduction: Speakers manage syntactic information density</title>, <title rend="italic">Cognitive psychology</title>, 61(1), pp. 23-62. Available at: <ref target="https://doi.org/10.1016/j.cogpsych.2010.02.002">https://doi.org/10.1016/j.cogpsych.2010.02.002</ref>
                </bibl>

                <bibl xml:id="collins2014" label="Collins 2014">Collins, M.X. (2014) <title rend="quotes">Information density and dependency length as complementary cognitive models</title>, <title rend="italic">Journal of psycholinguistic research</title>, 43(5), pp. 651-681.
                </bibl>

                <bibl xml:id="scheffler_etal2023" label="Scheffler et al 2023">Scheffler, T., Richter, M., and van Hout, R. (2023) <title rend="quotes"> Tracing and classifying ger-man intensifiers via information theory</title>, <title rend="italic">Language Sciences</title>, 96.
                </bibl>

                <bibl xml:id="philipp_richter_etal2023" label="Philipp and Richter et al 2023">Philipp, J.N.P, Richter, M., Scheffler, T., and van Hout, R. (2023) <title rend="quotes">The role of information in modeling german intensifiers</title>, manuscript submitted for publication.
                </bibl>

                <bibl xml:id="fillmore1976" label="Fillmore 1976">Fillmore, C. (1976) <title rend="quotes"> Frame semantics and the nature of language</title>, in <title rend="italic">Annals of the NY Academy of Sciences: Conf. on the Origin and Development of Language and Speech</title>, 280, pp. 20-32.
                </bibl>

                <bibl xml:id="busse2012" label="Busse 2012">Busse, D. (2012) <title rend="italic">Frame-semantik: Ein kompendium</title>. Berlin: deGruyter.
                </bibl>

                <bibl xml:id="navigliponzetto2012" label="Navigli and Ponzetto 2012">Navigli, R. and Ponzetto, S.P. (2012) <title rend="quotes"> Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>, <title rend="italic">Artificial Intelligence</title>, 193(0), pp. 217-250. Available at: <ref target="http://www.sciencedirect.com/science/article/ pii/S0004370212000793">http://www.sciencedirect.com/science/article/ pii/S0004370212000793</ref>.
                </bibl>

                <bibl xml:id="ferrucci_etal2009" label="Ferrucci et al 2009">Ferrucci, D., Lally, A. Verspoor, K., and Nyberg, E. (2009) <title rend="quotes"> Unstructured information management architecture (UIMA) version 1.0</title>, <title rend="italic">OASIS Standard</title>, March 2009. Available at: <ref target="https://docs.oasis-open.org/uima/v1.0/uima-v1.0.html">https://docs.oasis-open.org/uima/v1.0/uima-v1.0.html</ref>.
                </bibl>

                <bibl xml:id="halácsy_etal2007" label="Halácsy et al 2007">Halácsy, P., Kornai, A., and Oravecz, C. (2007) <title rend="quotes"> HunPos– an open source trigram tagger</title>, in <title rend="italic"> Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>. Prague: Czech Republic, Association for Computational Linguistics, pp. 209-217. Available at: <ref target="https://aclanthology.org/P07-2053">https://aclanthology.org/P07-2053</ref>.
                </bibl>

                <bibl xml:id="dkpro" label="DKPro Core Component Reference">DKPro Core Component Reference (2017) Available at: <ref target="https://dkpro.github.io/">https://dkpro.github.io/</ref>. (Accessed 01 September 2022).
                </bibl>

                <bibl xml:id="codina2018" label="Codina 2018">Codina, J. (2018) OpenMinted BableNet (2018) Available at: <ref target="https://github.com/TalnUPF/OpenMinted_BabelNet">https://github.com/TalnUPF/OpenMinted_BabelNet</ref>. (Accessed 01 September 2022).
                </bibl>
            </listBibl>

        </back>
    </text>
</TEI>
