<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!-- article title in English --></title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>first name(s) <dhq:family>family name</dhq:family></dhq:author_name>
                    <idno type="ORCID"><!-- if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000 --></idno>
                    <dhq:affiliation></dhq:affiliation>
                    <email></email>
                    <dhq:bio><p></p></dhq:bio>  
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt><publisher>Alliance of Digital Humanities Organizations</publisher>
<publisher>Association for Computers and the Humanities</publisher>
            	
            	<!-- This information should be added when the file is created -->
                <idno type="DHQarticle-id">000774</idno>

            	
            	<!-- This information will be completed at publication -->
                <idno type="volume"><!-- volume number, with leading zeroes as needed to make 3 digits: e.g. 006 --></idno>
                <idno type="issue"><!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date><!-- include @when with ISO date and also content in the form 23 February 2024 --></date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
<!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->                    
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            	<taxonomy xml:id="project_keywords">
            		<bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
            	</taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->
                	
                	<!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp=""/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item></item>
                    </list>
                </keywords>
            	<keywords scheme="#project_keywords">
            		<list type="simple">
            			<item></item>
            		</list>
            	</keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
        	<!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
        	<change>The version history for this file can be found on <ref target=
        		"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000774/000774.xml">GitHub
        	</ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                
                <p>The present paper reports a pilot study to approach the sub- text in Chekhov’s short-story ”Ward No. 6” by means of informa- tion theory. 
                    The original text is enriched by glosses by which we intend to make explicit the implicit knowledge conveyed by the original text, i.e., the subtext. 
                    We generated several text variants with meaningful enrichments and one fake variant that served as a baseline. We could not observe that 
                    semantic surprisal as a feature of words and uniform information density have a sub- text effect throughout all text variants. 
                    However, it turned out that kurtosis and skewness are suitable classification criteria to distinguish meaningful enrichments from fake enrichments.
                </p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p></p>
            </dhq:teaser>
        </front>
         <body>
                <div>
                    <head>1. Introduction</head>
                    <p>This paper is a first attempt to approach the subtext by means of infor- mation theory [1]. The object of study is a text by the Russian writer Anton P. Chekhov, 
                        namely the short-story ”Ward No. 6” (”Palata No. 6” in the original).
                    </p>
                    <p>The message conveyed by any text is composed of two kinds of meaning. The first is the explicit meaning, i.e. the meaning that is explicitly coded by the linguistic material appearing black on white. 
                        The second is the meaning that a cooperative interpreter will infer from the explicit text following Gricean or Neo-Gricean reasoning [2], 
                        making use of both linguistic knowledge and non-linguistic resources (world knowledge, conversational context). Even the most explicit text leaves open numerous informational gaps, 
                        which must be filled by the interpreter. These ”bridging inferences” [3] constitute the im- plicit meaning of the text, for which we utilise the term subtext here. 
                        Another metaphor which is sometimes employed is the visible text as opposed to dark text (e.g. [4]).
                    </p>
                    <p>The discourse about the subtext is rich (see [5, 6] for a survey), and the notion is controversial. According to the Oxford Dictionary of lit- erary terms [7], 
                        the subtext is “any meaning or set of meanings which is implied rather than explicitly stated in a literary work, especially in a play”. 
                        For the Russian Literary encyclopedia of terms and concepts [8], the subtext (“podtekst” in Russian) represents the “hidden sense of an utterance, 
                        stemming from the interaction of the literary mean- ings, the context, and the speech situation”. 
                        As can be seen, there are basically two ways of understanding what subtext is [9, p. 72]. 
                        Un- der the first reading, the one described above, advocated by [7], the term is basically synonymous to “pragmatic inferences”. 
                        Besides that, the term subtext is used in the literature also to refer to the ultimate sense of a literary text, i.e. 
                        to the interpretation that is intended by the author and that the reader has to decipher (e.g. [8, 10]). 
                        In that second respect, the meaning of subtext comes close to the ”moral of the story”. In what follows, we use subtext only in the first of these two readings.                        
                    </p>
                    <p>To approach the implicit meanings of a text, we will investigate the Russian short-story ”Palata No. 6”, 
                        written by Anton P. Chekhov, published in 1892. Chekhov is considered to have originated the role of the subtext, 
                        in his plays but also in his prose (e.g. [11, 12]). Us- ing a narrative text for our purposes has the advantage that the in- formational 
                        impact of the conversational context is reduced because the addressee (reader) 
                        does not have to calculate her own position as well as the position of the speaker (author).                    
                    </p>
                    <p>Related to the previous point, Chekhov is known for having devel- oped a specific literary technique of “concentration and shortness” [13, p. 48-49], 
                        manifesting itself in the language of the narrator, which usually shows simple syntax, parataxis, short sentences. 
                        This and the sparse use of adjectives, comparisons and metaphors is functional, 
                        for Chekhov’s intention is to challenge the reader intellectually and to encourage critical reading[13, p. 67][14]. 
                        The reader is asked to ac- tively interpret by filling the informational gaps of the texts. 
                        There- fore, we may expect a Chekhov story to entail a drastic discrepancy between what is said explicitly and what is meant by the text. 
                        If the subtext is “a level of speech between the lines” ([15]), we may thus safely expect a lot of speech between the lines in the work of Chekhov.
                    </p>
                    <p>The general language of the Chekhovian short-story is the Russian literary language [13, p. 49], 
                        which is important for our methodol- ogy to be introduced below. 
                        The particular text ”Ward No. 6” has been chosen, moreover, 
                        because of its considerable length compared to other short-stories of the author. 
                        For our methodology to work, we need a text of a certain cardinality of words.
                    </p>
                   </div>
             <div>
                 <head>2. Outline</head>
                 <p>In this paper, we presuppose a probabilistic notion of linguistic mean- ing. The basic assumption is that the meaning of a word influences 
                     the probability with which other words accompany that word in a text. Given a sufficiently large number of text passages in which a word appears, 
                     it is in principal possible to obtain a co-occurrence profile that is characteristic of the word. 
                     We consider this profile, the co-text of the word, to be a function of its meaning 1
                 </p>
                 <p>Our point of departure is that information and the flow of infor- mation (FoI) are characterising features of texts. 
                     By FoI we mean the distribution of information per word over time in sentences. Our ap- proach is inspired by [23] who, 
                     in a Shannon information theoretic framework, was concerned with the relationship between knowledge and information in linguistic messages. 
                     [23] states that Shannon in- formation (SI) forms a framework for conveying meaning and its evolve- ment: in his view, information is a key component of knowledge, 
                     and knowledge acquisition is a result of the way information is processed. In the present paper, we exploit this idea, 
                     likewise assuming that the flow of information is a classifying semantic feature of texts.
                 </p>
                 <p>We use contextualised information, i.e., surprisal [24, 25, 26] as a lexical feature of a word w.
                     </p>
                 <p>In order to approach the actual meaning of a word, in other words, we measure the effect that the meaning of the word has on its co-text.
                 </p>
                 <p>This idea is at the heart of the Topic Context Model (TCM), see Sec- tion 3.2. TCM calculates the amount of semantic surprisal of a word, i.e., 
                     the amount of SI, respectively, that can be derived from a semantic context.      
                    </p>
                 <p>In the due course, we use the terms information and information value as equivalents of surprisal and surprisal value, 
                     respectively, and we speak of the flow of surprisal (FoS) and the flow of information (FoI).
                 </p>
                 <p>Since the surprisal of a word w depends on the text in which it ap- pears, 
                     it should make a difference whether the information carried by w is calculated only on the basis of those words 
                     that literally con- stitute the novel (bare narration), or whether it is calculated on the basis of the narration 
                     enriched by additional word material. We take methodological advantage of this fact in the following way.
                 </p>
                 <p>In addition to the bare narration, we set up a contextualised nar- ration, which consists of the original text words plus additional texts that 
                     contextualise the original text words by adding encyclopedic knowl- edge about them. These additional texts, which we also refer to as glosses, serve as a (very simple) model of the subtext of the novel. 
                     De- tails on how the text is enriched by additional knowledge are given in Section 4. In order to check whether the expected change in lexical information through contextualisation, i.e. through the enrichment
                     of the text through meaningful glossing, is a random effect, the orig- inal text was also enriched with text fragments that have no relation
                     to the Chekhovian story in terms of content, that is, a fake glossing. After that, in a first work step, we carry out two measurements: (i) 
                     we determine the surprisal values of the words in the bare narration, (ii) we measure them again, but this time both in the meaningful and fake glossed narration.
                 </p>
                 <p>From FoS, we determine the information density in the text’s para- graphs. Information density is another notion that we need to intro- duce to explain our methodology. 
                     The Uniform Information Density (UID) principle [27, 28] says that the sender of a message prefers to distribute surprisal evenly and smoothly across a 
                     sequence of linguis- tic units in a sentence, utterance or in a text. FoS should not be slowed down or even brought to a standstill by large jumps in information. 
                     This implies that the more the information in sentences or statements swings up or down, the higher the peaks and troughs of information, 
                     the more difficult it is for the recipient to process the information. Excessive information fluctuations can even prevent processing alto- gether.
                 </p>
                 <p>Information density characterises linguistic communication. There- fore, if we assume that the subtext plays an essential role in Chekhov’s prose 
                     (which we do, see above for motivation), we can expect that fac- toring in the subtext will lead to a more balanced FoI. 
                     In other words, we interpret density as the decisive discriminating feature in order to distinguish the narration enriched both by meaningful 
                     and fake subtexts from the bare narration.
                 </p>
                 <p>The measurement of meaningfully enriched text should show less peaks and troughs than the one of the bare narration. 
                     From that our first prediction follows: in meaningfully enriched texts, the density will be optimised compared to the original text. 
                     In contrast, with fake enrichments, the density is predicted to deteriorate drastically.
                 </p>
                 <p>H1 Adding meaningful glosses to the original text leads to a more balanced FoS.
                     H2 Adding fake glosses to the original text leads to a less balanced FoS.
                 </p>
                 <p>What does that mean in terms of the principle of UID? In Section 3.3, we give the definition of UID that we follow in this paper: 
                     in- formation density gets more balanced, that is, better, when its value shifts towards zero. 
                     This represents on average small surprisal jumps from word to word and it represents uniformity. 
                     In contrast, when a UID value moves away from zero, it is a deterioration since this means that 
                     there are larger surprisal gaps between words. The dis- persion of UID values should not be large. 
                     If this were the case, it would indicate arbitrariness in the UID distribution. From that, 
                     our second prediction follows: a meaningful subtext will reduce the scat- ter of UID values compared to the original text. 
                     Consequently, a fake subtext will lead to stronger scattering.
                 </p>
                 <p>Models that employ information theory are rarely applied in digi- tal humanities, and to the best of our knowledge, 
                     there are no studies that aim to disclose the subtext in the framework of information the- ory.
                 </p>
                 <p>In the next sections, we introduce some fundamentals of informa- tion and surprisal theory, and we present the Topic Context Model. 
                     Subsequently, we describe our methodology and the different enrich- ment strategies (glossings) that we employed. 
                     Finally, we report the experimental results and discuss the outcomes of our study.                
                 </p>
             </div>
             <div>
                 <head>3. Information theoretic background</head>
             <div><head>3.1 Information theory and information as a lexical feature</head>
                 <p>Shannon’s information theory [1, 29] models the transmission of in- formation from a sender to a receiver. 
                     The information theoretic model aims to output a code in the form of consecutive 0 and 1 that compresses a 
                     message as much as possible without loss of informa- tion. Shannon developed his theory in an engineering context, 
                     and from the 1950s onwards the theory was applied to natural language.
             </p>
                 <p>Shannon Information (SI) is measured in bits. 1 bit is the informa- tion of two equally probable possibilities. 
                     For example, a toss with a fair coin carries 1 bit of information because there are two possible outcomes of a toss, i.e., 
                     ”front of the coin”, for instance represented by ‘1’, or ”back of the coin”, represented by ‘0’. 
                     The amount of bits equals the number of yes/no decisions needed to determine results of processes 
                     such as tossing a coin or determining a word in a chain, given a good algorithm.
                 </p>
                 <p>For the sake of illustration, imagine a toy language L that com- prises solely eight words, 
                     that is, L = {Anton, Chechov, was, a, very talented, Russian, writer}. 
                     Lets agree that in L any sentence must con- sist of eight words, and there are no rules of linear precedence in a sentence, 
                     so that for instance both Anton a was very Chekhov talented writer Russian or a was Chekhov Russian Anton talented writer 
                     very are possible sentences with the same probability. For a sentence in the toy language it needs three yes/no questions to 
                     determine a word w uniquely as shown in Figure 1:
                 </p>
                 <p>As can easily be seen, the word Chekhov as terminal element in Figure 1 carries three bits of information. We get the same result when Formula 1 is applied. 
                     This expression is the standard definition of SI, i.e., the negative log-likelihood of a linguistic unit w: </p>
                     
                     <p>
                     <!-- equation 1 -->
                         <!-- come back and italicize -->
                         Since the probability of a word w in L is P(w)= 1/8, we get for instance for the word Anton in Figure 1 above:
                     </p>
                 <p>
                     <!-- equation 2 -->
                     The information type Surprisal [25, 26] is basically s a psycholinguistic concept: 
               
                   <figure xml:id="figure01">
                       <head>Information amount in bits of words in a sentence repre- sented in a decision tree.</head>
                       <figDesc>A downloaded image of a decision tree including information bits of words in a sentence.</figDesc>
                       <graphic url="resources/images/figure01.png"></graphic>
                   </figure>
                   
                     
                     <note>Empirical evidence comes amongst others from [30, 31]</note>
                     surprisal of a linguistic unit is proportional to the mental processing effort that it causes [25]. If the probability of a word is high in 
                     a given context, its surprisal and the effort for the processing of that word are low. For instance, given the word Barack in a text, 
                     the probability is presumably quite high that Obama fol- lows. In that case, Obama would carry low surprisal. 
                     An example of high surprisal because of the semantic unexpectedness of words is Chomsky’s
                     famous sentence colorless green ideas sleep furiously, with which he wanted to make clear that 
                     semantically uninterpretable sentences can nevertheless be syntactically well-formed [32]. 
                     To give another example: the sentence final verb fell in the famous Garden Path-sentence the horse 
                     raced past the barn fell is extremely unex- pected and carries high surprisal [25]. 
                     Consequently, this requires a considerable processing. A language processor would 
                     probably as- sume that barn is the sentence final word given the preceding context. The expression in 
                     Formula 3 defines surprisal, as a special case of SI as in Formula 1, as negative log-likelihood of a linguistic unit. 
                     The basis is its conditional probability which is in Formula3 represented by the vertical bar.
                 <!-- equation 3 -->
                     The variables w1 . . . wi−1 represent co-occurrences of any kind of the targetwordwi withinasentence. 
                     ThevariableCONTEXT represents extra-sentential contexts [26].
                 </p>     
                 <p>Technically, contexts for the calculation of surprisal can be co- occurrences of target linguistic units, such as n-grams of any type (e.g. terminal symbols and of part-of-speech tags) [33], 
                     syntactic structures [34, 35, 36, 37, 38] and also semantic contexts [39, 40, 41, 42, 43].
                 </p>
                                  
                 </div>
                 <div>
                     <head>3.2 Topic Context Model</head>
                     <p>The Topic Context Model (TCM)[39, 40, 41, 44, 42] 
                         
                         <note>https://github.com/jnphilipp/tcm</note>
                              
                         calculates the se- mantic surprisal of words relative to an extra-sentential context in a given corpus and predicts that a word carries 
                         high surprisal when its mean probability, that is, the expected value, in the topics of a docu- ment is low. 
                         The prerequisite for calculating the information content of a word in a text is the knowledge of the topics that the text contains. 
                         In general terms, a topic is what a text document is about. Depend- ing on how coarse- or fine-grained we look at a text, 
                         the text can deal with more or less topics. In this study we calculate the surprisal on a paragraph basis, meaning that we 
                         use the different paragraphs as the basis for calculating topics.
                     </p>
                     <p>The topics that a paragraph covers are established by the process of topic detection. In this study, TCM relies on the topic detection model Latent Dirichlet Allocation (LDA) 
                         [45] which is a widely used, successful model, but other models can also be used in TCM such as Latent Semantic Analysis.
                     </p>
                     <p>The assumption behind LDA is that (i) each text document con- tains a statistical mixture of topics, that (ii) similar topics correlate with similar probability distributions of words, 
                         and that consequently (iii) each topic is characterised by a specific distribution of the words. How many topics LDA should detect can be 
                         chosen before calculation by means of the topic detection starts.
                     </p>
                     <p>Assume that LDA has disclosed two topics in a document, as illus- trated in Figure 2:
                         
                         <figure>
                             <head>Topics as probability distributions of words.
                             </head>
                            <figDesc>Downloaded image depicting Topic 1 and Topic 2</figDesc>
                             <graphic url="resources/images/figure02.png"></graphic>
                         </figure>
                         
                    </p>
                     <p>In the yellow-coloured topic, high probabilities are assigned to words in the semantic field of, say, human created buildings. The pink-coloured topic contains highly probable words 
                         from the auto- mobile sector. It is hardly surprising that a word from the nutrition such as chocolate has a low probability in both topics. TCM calculates the surprisal of chocolate per topic according to the Formulae 1 and
                         3 and then takes the mean value. In our example, in Pt1 (chocolate) = 0.00003 and surprisal(chocolate) = −log20.00003 = 15.02 bits and Pt2 (chocolate) = 0.000003 and surprisal(chocolate) = −log20.000003 = 18.35 bits, and the
                         mean is 16.69, see Formula 4.
                         
                         <note>The above explanations and the calculation 4 are, of course, a simplified de- scription TCM. For more details, see [39, 40, 41, 42, 44].</note>
                         <!-- equation 4 -->
                     </p>
                     <p>The surprisal for a language processor who encounters the word chocolate in the context of the two topics buildings and automobiles is therefore quite high. 
                     </p>
                     <p>Figure 3 illustrates the working of the TCM.
                        
                        <figure>
                            <head>Illustration of the Topic Context Model.</head>
                            <figDesc>A screenshot of an image depicting TCM text to surprisal.</figDesc>
                            <graphic url="resources/images/figure03.png"></graphic>
                        </figure>
                        
                     </p>
                 </div>
                 <div>
                     <head>3.3 Uniform information density</head>
                     <p>Uniform Information Density (UID) is an important principle in lin- guistic communication of humans [46, 47] but it is also important in artificial generation of language: in any linguistic product, 
                         informaion peaks and troughs must not be too extreme, so as not to make it too difficult for the recipient of a message to process it. In this study, we utilise the operationalisation of UID in [48].
                         
                         <note>https://github.com/jnphilipp/uid</note>
                     </p>
                     <p>UIDLOCAL is the measure of the average (squared) information change from word to word in a sentence. Formula 5 gives its defi- nition: id is the information / surprisal of a word, n represents the total number of words in a sentence. 
                         Instead of UIDLOCAL we use the term UIDwordwise [49, 50].
                         
                         <!-- equation 5 -->
                     </p>
                     <p>UIDwordwise is negative by definition, and therefore a UIDwordwise value close to zero indicates a high uniformity of the information den- sity distribution, that is, on average smaller information jumps 
                         from word to word.
                     </p>
                     <p>In the Garden Path-example from above information does not flow very smoothly as becomes clear in Figure 4 from [25]. The informa- tion is distributed fairly evenly over the first five words. 
                         However, the jump in information in the sentence-final word fell is enormous:
                        
                         <figure>
                             <head>Flow of information in a Garden Path-sentence.</head>
                             <figDesc>A screenshot of a bargraph titled Flow of Information, including the axia Information Values and Garden Path sentence</figDesc>
                             <graphic url="resources/images/figure04.png"></graphic>
                         </figure>
                    
                     </p>
                     <p>The steep sentence final information peak threatens the successful processing of the sentence.
                     </p>
                     <p>The comparison of UIDwordwise in the ”expectable” sequence and in the garden path clearly shows that in the latter, the value of UIDwordwise is further away from 0 (see (b) below) than in the expected sequence (see (a) below). 
                         This represents the lower uniformity of the infor- mation density in the garden path sentence. We use the information values from Figure 4 as input of expression 5:</p>
                     <!-- equation a -->
                     <!-- equation b -->
                     <p>In the present study, we use UID as a measure of the ”communica- tive naturalness” of the Chekhovian original text and its manipulated versions. As should have become clear, we hypothesise that the orig- inal text alone, i.e. 
                         without its subtext, performs worse with respect to this measure than the original text enriched by its subtext. In the next section we explain how we set up the enrichments.</p>
                 </div>
                 <div>
                     <head>4 Enrichment</head>
                     <p>As noted above, the general methodology is to perform three differ- ent measurements of surprisal of the text words in Ward No. 6. First, we measure the surprisal of the words based on the word material of the narration alone. In a second measurement (or set of measure- ments, see below), 
                         we enrich the narration by a text addendum which is semantically related to the original text words. Finally, we use an additional text which is not content-related to the original text in any obvious way. In the present section, we describe how we set up the meaningful text addendum, i.e. 
                         the meaningful glosses.</p>
                     <p>Recall that we conceive of the meaningful glosses as a toy model of the subtext of the narration. Accordingly, the enrichment shall make explicit the implicit knowledge conveyed by the text. To determine appropriate subtext explications, we make use of the idea brought forward by Frame Semantics [51] that lexical material 
                         in the form of content words evokes, when used in a text, conceptual frames that represent generalised conceptual knowledge associated with the re- spective content word [52].</p>
                     <p>Specifically, we use BabelNet [53], a multilingual semantic network compiled from a variety of sources such as WordNet and Wikipedia, as an encyclopedic resource providing the conceptual knowledge as- sociated with lexical material. BabelNet provides a network of con- ceptual nodes (“senses”) 
                         interconnected by semantic relations of mul- tiple types, e.g. hypernyms, hyperonyms, meronyms etc. The nodes contain translations to multiple languages and also glosses providing definitions or descriptions of the concepts.
                     </p>
                     <p>We set up a pipeline that automatically enriches a given text by adding information associated with content words available from Ba- belNet. The pipeline is based on Apache UIMA [54], uses open source software components and comprises the following steps:
                     <list>
                         <item>Part-of-speech(POS)Tagging:Inthisstep,POS-tagsareassigned to all word forms using DKPro HunPosTagger [55].
                         </item>
                         <item>Lemmatisation:usingDKProLanguageToolLemmatizer[56],we lemmatised the word forms. Further, stopwords, 
                             punctuation and duplicate words within paragraphs are removed.</item>
                         <item >WordSenseDisambiguation(WSD):weemployedDKProWSD [56] in combination with an extension that enables the use of BabelNet [57] to resolve ambiguous lemmas to their most prob- able sense. 
                             A number of disambiguation methods have been tried, with varying results.</item> <!-- Lesk, Graph -->
                         <item>Enrichment Generation: we used the following method to en- rich the text:</item> <!-- glosses -->
                     </list>
                         Overall, we created a number of enrichment variations resulting from varying the following parameters:
                         <!-- list breakdown -->
                         The data resource for the “fake” glosses was the Wikipedia based ”rus_news_2020_1M” corpus (1M sentences) from the ”Wortschatz Leipzig” corpora collection.
                         
                         <note>https://wortschatz.uni-leipzig.de/de</note>
                     </p>
                     <p>The core concept of our methodology is to observe the difference in the surprisal for each text word before and after meaningful or fake enrichments. 
                         Therefore, we employed TCM to calculate the sur- prisal for every token in the original text and for the four enrichments of each type. Table 1 gives an overview of the used enrichments and the size of the corpora.</p>
                     
                     <p>When running LDA, we use the lemmatised tokens and ignored punctuations, stopwords and tokens shorter than three letters. 
                         Defining the number of topics is essential for LDA and has been determined experimentally for TCM calculation so far. 
                         We therefore calculated the surprisal values three times using three different numbers of top- ics, see Section 3.2 for the calculations. 
                         We then averaged the three resulting surprisal values for the final surprisal value. Further, we normalised the surprisal values to 
                         have a consistent range of values among different experiments. Then, we calculated the information density distribution for 
                         every enrichment and the original text to ob- serve how enrichments change the information density.</p>
                 </div>
                 <div>
                     <head>5 Results</head>
                     <p>Firstly, we tested whether the different text variants differ measurably in their information density. To do this, we compared the meaningfuk
                         
                        
                             
                         
                         values in the sets and used the non-parametric Mann-Whitney-test.
                         
                         <note>Non-parametric means that the test makes no or just minimal assumptions about the distribution within the data. An example may illustrate the question posed by statistical tests of mean-differences: the Dutch and the Danes are 
                             among the tallest populations in the world. On average, there is hardly any difference between them. Differences in height are therefore probably not characteristic to group, i.e., the group of the tallest populations. 
                             Rather, we could say that the differ- ences are not important, they are due to chance. In terms of height, the Dutch and Danes belong to the same group, that is, the group of the tallest people. 
                             In Malta, for example, the average height of the population is much lower. The difference in average height between the Dutch and the Maltese is likely to be highly significant. 
                             This means that the Dutch and the Maltese do not belong to the same height group.</note>
                         
                         The results of the Mann-Whitney-tests are given in Table 2. A significance level of p <!-- less than symbol --> .05 means 
                         that there are strong, non-random dif- ferences between the sets.
                     </p>
                     <p>All mean differences of information density between the original text and the texts enriched with fake glossings are highly significant. In each case p ≈ 0. The hypothesis of equality of means has thus to be rejected. 
                         That is to say, the information density in the original texts differs strongly from the density in fake glossed texts. Statistically speaking, original texts and fake glossed texts are therefore different sets.</p>
                     <p>Weaker pronounced is the difference between the information den- sity of the original text and the meaningfully glossed text. Approx- imately half of the comparisons lack significance, and the comparison of the original text 
                         with meaningful glossing sourced from all lexicons and graph connectivity is highly insignificant, with a signifance
                     <!-- Table 2 -->
                         level even of p = .30.
                     </p> 
                     <p>The following figures compare the mean values of the UIDwordwise-
                         values and their dispersion in original, meaningful and fake glossed texts. 
                         A mean tends towards 0 means that many of the sentences in a data set show little wordwise surprisal- changes which means that the information 
                         density is tendentialy uniform. Recall that UIDwordwise is negative by definition.
                     </p>
                     <p>Figure 5 displays that for the All-GRAPH-method employing word sense disambiguation (WSD) via graph connectivity, meaningful gloss- ing make the density of surprisal more uniform compared to the original text, 
                         albeit only a little bit (5b). Conversely, in fake glosses (5b) information density in sentences is much less uniform. Meaningful glossing reduces the dispersion of 
                         UID-values compared to the origi- nal text (5a), whereas fake glossing results in a substantially stronger dispersion of the UID-values (5b).</p>
                     <p>Considering only LESK disambiguation, the scenario alters clearly: meaningful glosses fail to make the information density more uni- form, even worsening it while, however, decreasing the dispersion (5c). 
                         Fake glosses make information density less uniform, accompa- nied by heightened dispersion (5d).</p>
                     <p>For the WNTR-glossed method with GRAPH connectivity, Figure 6 shows as well that meaningful glossing does not manage to make in- formation distribution more uniform, rather the contrary is the case. However the dispersion is reduced (6a) which is an improvement. 
                         The same is the case with LESK disambiguation (6c). With the fake glosses, the information density gets less uniform while, in contrast to meaningful glossing, the dispersion of the density values increases (6b / 6d).
                     
                        <figure>
                            <head>Comparison of means of original and meaningful / fake glossed texts based on UIDs for ALL-glossed method.</head>
                             <figDesc>A screenshot of four graphs, comparisons of means of original and meaninful/fake glossed texts.</figDesc>
                             <graphic url="resources/images/figure05.png"></graphic>
                         </figure>
                                           
                     </p>
                     <p>The plots in the Figures 7 and 8 depict probability distributions of UID values for the glossing types ALL GRAPH (ALL GRAPH method with WSD via graph connectivity), ALL LESK, WNTR GRAPH and WNTR LESK data. 
                         
                         <note>The area under the curves must be approximately 1 in each case.</note>
                         
                         The x-axis gives the UIDwordwise values, the y-axis gives the probabilities.</p>
                     
                     <p>Firstly, all plots show very clearly that fake glossed texts have distributions of UID-values reminiscent of a flattened normal distribu- tion halfway, so to speak, to a uniform distribution: we see flattened peaks, 
                         and the edges of the distributions run wide to the left and right meaning the probability of extreme values is high. The distribution of the fake glossed text exhibits low kurtosis, i.e., peakiness.
                         
                         <figure>
                             <head>Comparison of means of original and meaningful / fake glossed texts based on UIDs for WNTR-glossed method.</head>
                             <figDesc></figDesc>
                             <graphic url="resources/images/figure06.png"></graphic>
                         </figure>
                         
                         <note>https://en.wikipedia.org/wiki/Kurtosis</note>
                     </p>
                     <p>Secondly, the plots in the Figures 7a and 8 visualise that meaning- fully enriched texts exhibit higher kurtosis compared to the original text relative to fake enriched texts, that is to say, they have a higher, steeper peak. 
                         However, this does not seem to hold when LESK al- gorithm is employed as becomes visible in Figure 7b: here, the density distribution in the original text seems to be more peaky than in the meaningful enriched text, 
                         and the distribution in the latter runs out far to the left and right. Figure 7b seems to show that the gener- alisation about kurtosis, i.e., that a meaningful enriched texts has a more peaky density distribution than the original text, does not hold. 
                         We aimed to confirm this suspicion and chose an alternative graphical representation of the density distribution in the LESK text with a Cullen and Frey-Graph in Figure 9.
                     
                     <figure>
                         <head>Information density of meaningfully / fake enriched texts and original texts.</head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure07.png"></graphic>
                     </figure>
                     
                         It visualises the proximity and distance of LESK distributions with a set of theoretical distributions. The x-axis gives the degree of skewness of a distribution, and the y- axis gives the kurtosis. 
                         
                         <note>0 https://en.wikipedia.org/wiki/Skewness</note>
                         
                         Figure 9 discloses that the ALL LESK density-distribution
                         
                         <figure>
                             <head></head>
                             <figDesc></figDesc>
                             <graphic url="resources/images/figure08.png"></graphic>
                         </figure>
                         
                         has higher kurtosis than the distribution in the original text, and the plot thus confirms the generalisation from above that the density distribution in meaningfully enriched texts is more peaky than the distribution in the original text. In addition, Figure 7b shows very clearly the proximity of the fake distribution to the uniform distribution 
                         that we observed already in the Figures 7a and 8. So, can we infer a subtext effect from these results? We will return to this research question posed above in the Section 6.
                     
                     <figure>
                         <head></head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure09.png"></graphic>
                     </figure>
                     
                     </p>
                 </div>
                 <div>
                     <head>6 Discussion and conclusion</head>
                     <p>The study confirms H2, and it confirms H1 solely for the enrichment combination ALL GRAPH (i.e. all available lexicons and the graphtheoretical disambiguation model), 
                         albeit the improvement is only marginal. Just for this case we observe that meaningful glossing im- proves the FoI, and that, in addition, the dispersion of density values decreases. Thus, with ALL GRAPH enrichments, the expected effect, albeit weak, occurs. 
                         In contrast, no such effect occurs with the rest of the meaningful enrichments: although the dispersion generally decreases (except for the LESK algorithm on all lexicons), the information density generally gets worse. 
                         So, we have to draw an overall negative conclusion: taking into calculation the otherwise invisible subtext, which we implemented by different versions of glosses semantically related to the original text, 
                         does not lead to a more uni- form information density.</p>
                     <p>Our study also produced a very positive result, however, because we managed to determine a parameter that does correspond posi- tively to the addition of meaningful glosses and negatively to the ad- dition of fake glosses: peakiness/kurtosis. When fake enrichments are added to the original text, the distribution of UID-values flattens, 
                         which means that it approaches the contour of the uniform distribu- tion (the uniform distribution results from a maximally fake text in which every word is chosen arbitrarily, see Figure 10). 
                         When meaningfully related glosses of the same cardinality are added to the original text, by contrast, 
                         the resulting contour shows a higher peak compared to the one of the original text.</p>
                     <p>The described finding comes out clearly in Figures 7a, 8a, and 8b. It is only with respect to enrichment type ALL LESK that the differ- ence cannot be read from the respective plot, see 7b. However, the alternative visual representation in Figure 9 shows that also in this case meaningful enrichments lead to a higher kurtosis value whereas fake enrichments 
                         lead to a lower kurtosis value compared to the kurtosis value of the original text. This can be taken as a confirmation that the text features surprisal and information density, so to speak,
                         measure semantic properties. We may confidently conclude that we attested a subtext effect.</p>
                     <p>Although we have shown that information theory may unravel the subtext in Chekhov, the observed subtext effects are subtle. This may be due to that the glosses that we used in the documented experi- ment are too primitive to reproduce any more clear subtext effect. Recall the basic idea underlying this study: if the subtext is an in- alienable part of the semantics of a meaningful text, measuring the surprisal values of original texts alone will produce values that re- flect the actual content of the text only incompletely. 
                         To measure the true surprisal of a text, one would have to enrich the original text by the perfect explication of the subtext.
                     
                     <figure>
                         <head></head>
                         <figDesc></figDesc>
                         <graphic url="resources/images/figure10.png"></graphic>
                     </figure>
                     
                         The (sub)textual prostheses that we actually used in our experiments are obviously very primitive enrichments, far from being perfect. 
                         Future research will therefore have to focus on the technique of enrichment. 
                     </p>
                     <p>While in this study, the generation of the subtexts was based in principle on the Fillmore model, future experiments will be carried out statistically based by employing neural networks. Doing so while keeping the features information density and kurtosis / skewness, 
                         we hope to achieve and observe a more significant subtext effect than in the current study.
                     </p>
                 </div>
             </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="shannon1948" label="Shannon 1948">Shannon, C.E. (1948) <title rend="quotes">A mathematical theory of communication</title>, <title rend="italic">The Bell system technical journal</title> 27(3), pp. 379-423. 
                </bibl>
                
                <bibl xml:id="grice1989" label="Grice 1989">Grice, P. (1989) <title rend="italic">Studies in the way of words</title>. Cambridge: Harvard University Press.
                </bibl>
                
                <bibl xml:id="irmer2011" label="Irmer 2011">Irmer, M. (2011) <title rend="italic">Bridging inferences</title>. Boston: de Gruyter.            
                </bibl>
                
                <bibl xml:id="hinrichs2014" label="Hinrichs 2014">Hinrichs, U. (2014) <title rend="italic">Die dunkle materie des wissens: Über leerstellen wissenschaftlicher erkenntnis</title>. Giessen, Germany: Psychosozial-Verlag. 
                </bibl>
                
                <bibl xml:id="lelis2011" label="Lelis 2011">Lelis, E.I. (2011) <title rend="quotes">Podtekst i smežnye javlenija</title>, <title rend="italic">Istorija i filologija</title>, 4, pp. 143–151.</bibl>
                
                <bibl xml:id="lelis2013" label="Lelis 2013">Lelis, E.I. (2013) <title rend="italic">Podtekst kak lingvopoėtičes kajakategorija v proze</title>. Iževsk, Russia: A.P. Čechova.</bibl>
                
                <bibl xml:id="baldick2015" label="Baldick 2015">Baldick, C. (2015) <title rend="italic">The Oxford dictionary of literary terms</title>. Oxford: Oxford University Press.</bibl>
            
                <bibl xml:id="nikoljukin2003" label="Nikoljukin 2003">Nikoljukin, A.N. (2003) <title rend="italic">Literaturnaja ėnciklopedija terminov iponjatij</title>. Moskva, Russia: NPK ”Intelvak".    
            </bibl>
                
                <bibl xml:id="ermatova2010" label="Ermatova 2010">Ermatova, E.V. (2010) <title rend="quotes">Implicitnost’ v chudožestvennom tekste</title>, <title rend="italic">Izda- tel’stvo Saratovskogo universiteta</title>. 
                </bibl>
                
                <bibl xml:id="myrkin1976" label="Myrkin 1976">Myrkin, V.J. (1976) <title rend="quotes">Tekst, podtekst i kontekst</title>, <title rend="italic">Voprosy jazykoznanija</title>, 2. 
                </bibl>
                
                <bibl xml:id="gatrall2003" label="Gatrall 2003">Gatrall, J.J. (2003) <title rend="quotes">The paradox of melancholy insight: Reading the medical subtext in chekhov’s “a boring story”</title>, <title rend="italic">Slavic Review</title>, 62(2), pp. 258-277. 
                </bibl>
                
                <bibl xml:id="mcsweeny2004" label="McSweeny 2004">McSweeny, K. (2004) <title rend="quotes">Effects or subtexts?</title>, <title rend="italic">Modern Language Studies</title>, 34(1-2), pp. 42-51. 
                </bibl>
               
                <bibl xml:id="klugečechov1995" label="Kluge and Čechov 1995">Kluge, R.D. and Čechov, A.P. (1995) <title rend="italic">Eine einführung in leben und werk</title>. Darmstadt: Wissenschaftliche Buchgesellschaft.
                </bibl>
                
                <bibl xml:id="whyman2010" label="Whyman 2010">R. Whyman, R. (2010) <title rend="italic">Anton Chekhov</title>. London: Routledge. 
                </bibl>
                
                <bibl xml:id="freise1997" label="Freise 1997">Freise, M. (1997) <title rend="italic">Die prosa Anton Čechovs: eine untersuchung im ausgang von einzelanalysen</title>. Amsterdam: Rodopi.
                </bibl>
                
                <bibl xml:id="firth1957" label="Firth 1957">Firth, R. (1957) <title rend="italic">Two papers on linguistic theory: Studies in linguistic analysis (Special volume of the philological society)</title>. Oxford: Blackwell. 
                </bibl>
                
                <bibl xml:id="harris1954" label="Harris 1954">Harris, Z.S. (1954) <title rend="quotes">Distributional structure</title>, <title rend="italic">Word</title>, 10(2-3), pp. 146-162.</bibl>
                
                <bibl xml:id="rubensteingoodenough1965" label="Rubenstein and Goodenough 1965">Rubenstein, H. and Goodenough, J.B. (1965) <title rend="quotes">Contextual correlates of synonymy</title>, <title rend="italic">Communications of the ACM</title>, 8(10), pp. 627-633. 
                </bibl>
                
                <bibl xml:id="landauerdumais1997" label="Landauer and Dumais 1997">Landauer, T.K. and Dumais, S.T. (1997) <title rend="quotes">A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>, <title rend="italic"> Psychological Review</title>, 104(2), pp. 211-240. 
                </bibl>
                
                <bibl xml:id="turneypantel2010" label="Turney and Pantel 2010">Turney, P.D. and Pantel, P. (2010) <title rend="quotes">From frequency to meaning: Vector space models of semantics</title>, <title rend="italic">Journal of Artificial Intelligence Research</title>, 37, pp. 141-188. 
                </bibl>
                
                <bibl xml:id="mikolov_etal2013" label="Mikolov et al 2013">Mikolov, T., Chen, K., Corrado, J., and Dean, J. (2013) <title rend="quotes">Efficient estimation of word representations in vector space</title>, <title rend="italic">Proceedings of the International Conference on Learning Representations (ICLR)</title>.
                </bibl>
                
                <bibl xml:id="baroni_etal2014" label="Baroni et al 2014">Baroni, M., Dinu, G., and Kruszewski, G. (2014) <title rend="quotes">Don’t count, predict! a sys- tematic comparison of context-counting vs. context-predicting semantic vectors</title>, <title rend="italic">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>, 1, pp. 238-247.
                </bibl>
                
                <bibl xml:id="dretske1981" label="Dretske (1981)">Dretske, F. (1981) <title rend="italic">Knowledge and the Flow of Information</title>. Cambridge: MIT Press.
                </bibl>
                
                <bibl xml:id="tribus1961" label="Tribus 1961">Tribus, M. (1961) <title rend="quotes">Information theory as thebasis for thermostatics and thermodynamics</title>, <title rend="italic">Journal of Applied Mechanics</title>, 28(1), pp. 1-8. 
                </bibl>
                
                <bibl xml:id="hale2001" label="Hale 2001">Hale, J. (2001) <title rend="quotes"></title>
                    
            A probabilistic earley parser as a psycholinguistic model, in: Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Lan- guage technologies, Association for Computational Linguistics, 2001, pp. 1–8.
                </bibl>
            </listBibl>
           
        </back>
    </text>
</TEI>
