<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
            <title type="article" xml:lang="en"><!-- article title in English --></title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>first name(s) <dhq:family>family name</dhq:family></dhq:author_name>
                    <idno type="ORCID"><!-- if the author has an ORCID ID, include the full URI, e.g. https://orcid.org/0000-0000-0000-0000 --></idno>
                    <dhq:affiliation></dhq:affiliation>
                    <email></email>
                    <dhq:bio><p></p></dhq:bio>  
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt><publisher>Alliance of Digital Humanities Organizations</publisher>
<publisher>Association for Computers and the Humanities</publisher>
            	
            	<!-- This information should be added when the file is created -->
                <idno type="DHQarticle-id">000774</idno>

            	
            	<!-- This information will be completed at publication -->
                <idno type="volume"><!-- volume number, with leading zeroes as needed to make 3 digits: e.g. 006 --></idno>
                <idno type="issue"><!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date><!-- include @when with ISO date and also content in the form 23 February 2024 --></date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
<!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->                    
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            	<taxonomy xml:id="project_keywords">
            		<bibl>DHQ project registry; full list available at <ref target="http://www.digitalhumanities.org/dhq/projects.xml">http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
            	</taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->
                	
                	<!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp=""/>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item></item>
                    </list>
                </keywords>
            	<keywords scheme="#project_keywords">
            		<list type="simple">
            			<item></item>
            		</list>
            	</keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
        	<!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
        	<change>The version history for this file can be found on <ref target=
        		"https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000774/000774.xml">GitHub
        	</ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                
                <p>The present paper reports a pilot study to approach the sub- text in Chekhov’s short-story ”Ward No. 6” by means of informa- tion theory. 
                    The original text is enriched by glosses by which we intend to make explicit the implicit knowledge conveyed by the original text, i.e., the subtext. 
                    We generated several text variants with meaningful enrichments and one fake variant that served as a baseline. We could not observe that 
                    semantic surprisal as a feature of words and uniform information density have a sub- text effect throughout all text variants. 
                    However, it turned out that kurtosis and skewness are suitable classification criteria to distinguish meaningful enrichments from fake enrichments.
                </p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p></p>
            </dhq:teaser>
        </front>
         <body>
                <div>
                    <head>1. Introduction</head>
                    <p>This paper is a first attempt to approach the subtext by means of infor- mation theory [1]. The object of study is a text by the Russian writer Anton P. Chekhov, 
                        namely the short-story ”Ward No. 6” (”Palata No. 6” in the original).
                    </p>
                    <p>The message conveyed by any text is composed of two kinds of meaning. The first is the explicit meaning, i.e. the meaning that is explicitly coded by the linguistic material appearing black on white. 
                        The second is the meaning that a cooperative interpreter will infer from the explicit text following Gricean or Neo-Gricean reasoning [2], 
                        making use of both linguistic knowledge and non-linguistic resources (world knowledge, conversational context). Even the most explicit text leaves open numerous informational gaps, 
                        which must be filled by the interpreter. These ”bridging inferences” [3] constitute the im- plicit meaning of the text, for which we utilise the term subtext here. 
                        Another metaphor which is sometimes employed is the visible text as opposed to dark text (e.g. [4]).
                    </p>
                    <p>The discourse about the subtext is rich (see [5, 6] for a survey), and the notion is controversial. According to the Oxford Dictionary of lit- erary terms [7], 
                        the subtext is “any meaning or set of meanings which is implied rather than explicitly stated in a literary work, especially in a play”. 
                        For the Russian Literary encyclopedia of terms and concepts [8], the subtext (“podtekst” in Russian) represents the “hidden sense of an utterance, 
                        stemming from the interaction of the literary mean- ings, the context, and the speech situation”. 
                        As can be seen, there are basically two ways of understanding what subtext is [9, p. 72]. 
                        Un- der the first reading, the one described above, advocated by [7], the term is basically synonymous to “pragmatic inferences”. 
                        Besides that, the term subtext is used in the literature also to refer to the ultimate sense of a literary text, i.e. 
                        to the interpretation that is intended by the author and that the reader has to decipher (e.g. [8, 10]). 
                        In that second respect, the meaning of subtext comes close to the ”moral of the story”. In what follows, we use subtext only in the first of these two readings.                        
                    </p>
                    <p>To approach the implicit meanings of a text, we will investigate the Russian short-story ”Palata No. 6”, 
                        written by Anton P. Chekhov, published in 1892. Chekhov is considered to have originated the role of the subtext, 
                        in his plays but also in his prose (e.g. [11, 12]). Us- ing a narrative text for our purposes has the advantage that the in- formational 
                        impact of the conversational context is reduced because the addressee (reader) 
                        does not have to calculate her own position as well as the position of the speaker (author).                    
                    </p>
                    <p>Related to the previous point, Chekhov is known for having devel- oped a specific literary technique of “concentration and shortness” [13, p. 48-49], 
                        manifesting itself in the language of the narrator, which usually shows simple syntax, parataxis, short sentences. 
                        This and the sparse use of adjectives, comparisons and metaphors is functional, 
                        for Chekhov’s intention is to challenge the reader intellectually and to encourage critical reading[13, p. 67][14]. 
                        The reader is asked to ac- tively interpret by filling the informational gaps of the texts. 
                        There- fore, we may expect a Chekhov story to entail a drastic discrepancy between what is said explicitly and what is meant by the text. 
                        If the subtext is “a level of speech between the lines” ([15]), we may thus safely expect a lot of speech between the lines in the work of Chekhov.
                    </p>
                    <p>The general language of the Chekhovian short-story is the Russian literary language [13, p. 49], 
                        which is important for our methodol- ogy to be introduced below. 
                        The particular text ”Ward No. 6” has been chosen, moreover, 
                        because of its considerable length compared to other short-stories of the author. 
                        For our methodology to work, we need a text of a certain cardinality of words.
                    </p>
                   </div>
             <div>
                 <head>2. Outline</head>
                 <p>In this paper, we presuppose a probabilistic notion of linguistic mean- ing. The basic assumption is that the meaning of a word influences 
                     the probability with which other words accompany that word in a text. Given a sufficiently large number of text passages in which a word appears, 
                     it is in principal possible to obtain a co-occurrence profile that is characteristic of the word. 
                     We consider this profile, the co-text of the word, to be a function of its meaning 1
                 </p>
                 <p>Our point of departure is that information and the flow of infor- mation (FoI) are characterising features of texts. 
                     By FoI we mean the distribution of information per word over time in sentences. Our ap- proach is inspired by [23] who, 
                     in a Shannon information theoretic framework, was concerned with the relationship between knowledge and information in linguistic messages. 
                     [23] states that Shannon in- formation (SI) forms a framework for conveying meaning and its evolve- ment: in his view, information is a key component of knowledge, 
                     and knowledge acquisition is a result of the way information is processed. In the present paper, we exploit this idea, 
                     likewise assuming that the flow of information is a classifying semantic feature of texts.
                 </p>
                 <p>We use contextualised information, i.e., surprisal [24, 25, 26] as a lexical feature of a word w.
                     </p>
                 <p>In order to approach the actual meaning of a word, in other words, we measure the effect that the meaning of the word has on its co-text.
                 </p>
                 <p>This idea is at the heart of the Topic Context Model (TCM), see Sec- tion 3.2. TCM calculates the amount of semantic surprisal of a word, i.e., 
                     the amount of SI, respectively, that can be derived from a semantic context.      
                    </p>
                 <p>In the due course, we use the terms information and information value as equivalents of surprisal and surprisal value, 
                     respectively, and we speak of the flow of surprisal (FoS) and the flow of information (FoI).
                 </p>
                 <p>Since the surprisal of a word w depends on the text in which it ap- pears, 
                     it should make a difference whether the information carried by w is calculated only on the basis of those words 
                     that literally con- stitute the novel (bare narration), or whether it is calculated on the basis of the narration 
                     enriched by additional word material. We take methodological advantage of this fact in the following way.
                 </p>
                 <p>In addition to the bare narration, we set up a contextualised nar- ration, which consists of the original text words plus additional texts that 
                     contextualise the original text words by adding encyclopedic knowl- edge about them. These additional texts, which we also refer to as glosses, serve as a (very simple) model of the subtext of the novel. 
                     De- tails on how the text is enriched by additional knowledge are given in Section 4. In order to check whether the expected change in lexical information through contextualisation, i.e. through the enrichment
                     of the text through meaningful glossing, is a random effect, the orig- inal text was also enriched with text fragments that have no relation
                     to the Chekhovian story in terms of content, that is, a fake glossing. After that, in a first work step, we carry out two measurements: (i) 
                     we determine the surprisal values of the words in the bare narration, (ii) we measure them again, but this time both in the meaningful and fake glossed narration.
                 </p>
                 <p>From FoS, we determine the information density in the text’s para- graphs. Information density is another notion that we need to intro- duce to explain our methodology. 
                     The Uniform Information Density (UID) principle [27, 28] says that the sender of a message prefers to distribute surprisal evenly and smoothly across a 
                     sequence of linguis- tic units in a sentence, utterance or in a text. FoS should not be slowed down or even brought to a standstill by large jumps in information. 
                     This implies that the more the information in sentences or statements swings up or down, the higher the peaks and troughs of information, 
                     the more difficult it is for the recipient to process the information. Excessive information fluctuations can even prevent processing alto- gether.
                 </p>
                 <p>Information density characterises linguistic communication. There- fore, if we assume that the subtext plays an essential role in Chekhov’s prose 
                     (which we do, see above for motivation), we can expect that fac- toring in the subtext will lead to a more balanced FoI. 
                     In other words, we interpret density as the decisive discriminating feature in order to distinguish the narration enriched both by meaningful 
                     and fake subtexts from the bare narration.
                 </p>
                 <p>The measurement of meaningfully enriched text should show less peaks and troughs than the one of the bare narration. 
                     From that our first prediction follows: in meaningfully enriched texts, the density will be optimised compared to the original text. 
                     In contrast, with fake enrichments, the density is predicted to deteriorate drastically.
                 </p>
                 <p>H1 Adding meaningful glosses to the original text leads to a more balanced FoS.
                     H2 Adding fake glosses to the original text leads to a less balanced FoS.
                 </p>
                 <p>What does that mean in terms of the principle of UID? In Section 3.3, we give the definition of UID that we follow in this paper: 
                     in- formation density gets more balanced, that is, better, when its value shifts towards zero. 
                     This represents on average small surprisal jumps from word to word and it represents uniformity. 
                     In contrast, when a UID value moves away from zero, it is a deterioration since this means that 
                     there are larger surprisal gaps between words. The dis- persion of UID values should not be large. 
                     If this were the case, it would indicate arbitrariness in the UID distribution. From that, 
                     our second prediction follows: a meaningful subtext will reduce the scat- ter of UID values compared to the original text. 
                     Consequently, a fake subtext will lead to stronger scattering.
                 </p>
                 <p>Models that employ information theory are rarely applied in digi- tal humanities, and to the best of our knowledge, 
                     there are no studies that aim to disclose the subtext in the framework of information the- ory.
                 </p>
                 <p>In the next sections, we introduce some fundamentals of informa- tion and surprisal theory, and we present the Topic Context Model. 
                     Subsequently, we describe our methodology and the different enrich- ment strategies (glossings) that we employed. 
                     Finally, we report the experimental results and discuss the outcomes of our study.                
                 </p>
             </div>
             <div>
                 <head>3. Information theoretic background</head>
             <div><head>3.1 Information theory and information as a lexical feature</head>
                 <p>Shannon’s information theory [1, 29] models the transmission of in- formation from a sender to a receiver. 
                     The information theoretic model aims to output a code in the form of consecutive 0 and 1 that compresses a 
                     message as much as possible without loss of informa- tion. Shannon developed his theory in an engineering context, 
                     and from the 1950s onwards the theory was applied to natural language.
             </p>
                 <p>Shannon Information (SI) is measured in bits. 1 bit is the informa- tion of two equally probable possibilities. 
                     For example, a toss with a fair coin carries 1 bit of information because there are two possible outcomes of a toss, i.e., 
                     ”front of the coin”, for instance represented by ‘1’, or ”back of the coin”, represented by ‘0’. 
                     The amount of bits equals the number of yes/no decisions needed to determine results of processes 
                     such as tossing a coin or determining a word in a chain, given a good algorithm.
                 </p>
                 <p>For the sake of illustration, imagine a toy language L that com- prises solely eight words, 
                     that is, L = {Anton, Chechov, was, a, very talented, Russian, writer}. 
                     Lets agree that in L any sentence must con- sist of eight words, and there are no rules of linear precedence in a sentence, 
                     so that for instance both Anton a was very Chekhov talented writer Russian or a was Chekhov Russian Anton talented writer 
                     very are possible sentences with the same probability. For a sentence in the toy language it needs three yes/no questions to 
                     determine a word w uniquely as shown in Figure 1:
                 </p>
                 <p>As can easily be seen, the word Chekhov as terminal element in Figure 1 carries three bits of information. We get the same result when Formula 1 is applied. 
                     This expression is the standard definition of SI, i.e., the negative log-likelihood of a linguistic unit w:
                     
                     
                     
                     
                     
                 
                <graphic url="resources/images/figure01.png" style="width: 700px"></graphic>
                 
                 
                 </p>
                 <p>
                    
                 </p>
                 </div>
             </div>
        </body>
        <back>
            <listBibl>
                <bibl></bibl>
            </listBibl>
           
        </back>
    </text>
</TEI>
