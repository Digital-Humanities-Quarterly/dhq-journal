<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en"></h1>
               
               
               <div class="author"><span style="color: grey">first name(s) family name</span></div>
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=family name&amp;rft.aufirst=first name(s)&amp;rft.au=first name(s)%20family name"> </span></div>
            
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>The present paper reports a pilot study to approach the sub- text in Chekhov’s short-story
                     ”Ward No. 6” by means of informa- tion theory. 
                     The original text is enriched by glosses by which we intend to make explicit the implicit
                     knowledge conveyed by the original text, i.e., the subtext. 
                     We generated several text variants with meaningful enrichments and one fake variant
                     that served as a baseline. We could not observe that 
                     semantic surprisal as a feature of words and uniform information density have a sub-
                     text effect throughout all text variants. 
                     However, it turned out that kurtosis and skewness are suitable classification criteria
                     to distinguish meaningful enrichments from fake enrichments.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1. Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">This paper is a first attempt to approach the subtext by means of infor- mation theory
                     [1]. The object of study is a text by the Russian writer Anton P. Chekhov, 
                     namely the short-story ”Ward No. 6” (”Palata No. 6” in the original).
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">The message conveyed by any text is composed of two kinds of meaning. The first is
                     the explicit meaning, i.e. the meaning that is explicitly coded by the linguistic
                     material appearing black on white. 
                     The second is the meaning that a cooperative interpreter will infer from the explicit
                     text following Gricean or Neo-Gricean reasoning [2], 
                     making use of both linguistic knowledge and non-linguistic resources (world knowledge,
                     conversational context). Even the most explicit text leaves open numerous informational
                     gaps, 
                     which must be filled by the interpreter. These ”bridging inferences” [3] constitute
                     the im- plicit meaning of the text, for which we utilise the term subtext here. 
                     Another metaphor which is sometimes employed is the visible text as opposed to dark
                     text (e.g. [4]).
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The discourse about the subtext is rich (see [5, 6] for a survey), and the notion
                     is controversial. According to the Oxford Dictionary of lit- erary terms [7], 
                     the subtext is “any meaning or set of meanings which is implied rather than explicitly
                     stated in a literary work, especially in a play”. 
                     For the Russian Literary encyclopedia of terms and concepts [8], the subtext (“podtekst”
                     in Russian) represents the “hidden sense of an utterance, 
                     stemming from the interaction of the literary mean- ings, the context, and the speech
                     situation”. 
                     As can be seen, there are basically two ways of understanding what subtext is [9,
                     p. 72]. 
                     Un- der the first reading, the one described above, advocated by [7], the term is
                     basically synonymous to “pragmatic inferences”. 
                     Besides that, the term subtext is used in the literature also to refer to the ultimate
                     sense of a literary text, i.e. 
                     to the interpretation that is intended by the author and that the reader has to decipher
                     (e.g. [8, 10]). 
                     In that second respect, the meaning of subtext comes close to the ”moral of the story”.
                     In what follows, we use subtext only in the first of these two readings.         
                     
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">To approach the implicit meanings of a text, we will investigate the Russian short-story
                     ”Palata No. 6”, 
                     written by Anton P. Chekhov, published in 1892. Chekhov is considered to have originated
                     the role of the subtext, 
                     in his plays but also in his prose (e.g. [11, 12]). Us- ing a narrative text for our
                     purposes has the advantage that the in- formational 
                     impact of the conversational context is reduced because the addressee (reader) 
                     does not have to calculate her own position as well as the position of the speaker
                     (author).                    
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">Related to the previous point, Chekhov is known for having devel- oped a specific
                     literary technique of “concentration and shortness” [13, p. 48-49], 
                     manifesting itself in the language of the narrator, which usually shows simple syntax,
                     parataxis, short sentences. 
                     This and the sparse use of adjectives, comparisons and metaphors is functional, 
                     for Chekhov’s intention is to challenge the reader intellectually and to encourage
                     critical reading[13, p. 67][14]. 
                     The reader is asked to ac- tively interpret by filling the informational gaps of the
                     texts. 
                     There- fore, we may expect a Chekhov story to entail a drastic discrepancy between
                     what is said explicitly and what is meant by the text. 
                     If the subtext is “a level of speech between the lines” ([15]), we may thus safely
                     expect a lot of speech between the lines in the work of Chekhov.
                     </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">The general language of the Chekhovian short-story is the Russian literary language
                     [13, p. 49], 
                     which is important for our methodol- ogy to be introduced below. 
                     The particular text ”Ward No. 6” has been chosen, moreover, 
                     because of its considerable length compared to other short-stories of the author.
                     
                     For our methodology to work, we need a text of a certain cardinality of words.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2. Outline</h1>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">In this paper, we presuppose a probabilistic notion of linguistic mean- ing. The basic
                     assumption is that the meaning of a word influences 
                     the probability with which other words accompany that word in a text. Given a sufficiently
                     large number of text passages in which a word appears, 
                     it is in principal possible to obtain a co-occurrence profile that is characteristic
                     of the word. 
                     We consider this profile, the co-text of the word, to be a function of its meaning
                     1
                     </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Our point of departure is that information and the flow of infor- mation (FoI) are
                     characterising features of texts. 
                     By FoI we mean the distribution of information per word over time in sentences. Our
                     ap- proach is inspired by [23] who, 
                     in a Shannon information theoretic framework, was concerned with the relationship
                     between knowledge and information in linguistic messages. 
                     [23] states that Shannon in- formation (SI) forms a framework for conveying meaning
                     and its evolve- ment: in his view, information is a key component of knowledge, 
                     and knowledge acquisition is a result of the way information is processed. In the
                     present paper, we exploit this idea, 
                     likewise assuming that the flow of information is a classifying semantic feature of
                     texts.
                     </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">We use contextualised information, i.e., surprisal [24, 25, 26] as a lexical feature
                     of a word w.
                     </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">In order to approach the actual meaning of a word, in other words, we measure the
                     effect that the meaning of the word has on its co-text.
                     </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">This idea is at the heart of the Topic Context Model (TCM), see Sec- tion 3.2. TCM
                     calculates the amount of semantic surprisal of a word, i.e., 
                     the amount of SI, respectively, that can be derived from a semantic context.     
                     
                     </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">In the due course, we use the terms information and information value as equivalents
                     of surprisal and surprisal value, 
                     respectively, and we speak of the flow of surprisal (FoS) and the flow of information
                     (FoI).
                     </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">Since the surprisal of a word w depends on the text in which it ap- pears, 
                     it should make a difference whether the information carried by w is calculated only
                     on the basis of those words 
                     that literally con- stitute the novel (bare narration), or whether it is calculated
                     on the basis of the narration 
                     enriched by additional word material. We take methodological advantage of this fact
                     in the following way.
                     </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">In addition to the bare narration, we set up a contextualised nar- ration, which consists
                     of the original text words plus additional texts that 
                     contextualise the original text words by adding encyclopedic knowl- edge about them.
                     These additional texts, which we also refer to as glosses, serve as a (very simple)
                     model of the subtext of the novel. 
                     De- tails on how the text is enriched by additional knowledge are given in Section
                     4. In order to check whether the expected change in lexical information through contextualisation,
                     i.e. through the enrichment
                     of the text through meaningful glossing, is a random effect, the orig- inal text was
                     also enriched with text fragments that have no relation
                     to the Chekhovian story in terms of content, that is, a fake glossing. After that,
                     in a first work step, we carry out two measurements: (i) 
                     we determine the surprisal values of the words in the bare narration, (ii) we measure
                     them again, but this time both in the meaningful and fake glossed narration.
                     </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">From FoS, we determine the information density in the text’s para- graphs. Information
                     density is another notion that we need to intro- duce to explain our methodology.
                     
                     The Uniform Information Density (UID) principle [27, 28] says that the sender of a
                     message prefers to distribute surprisal evenly and smoothly across a 
                     sequence of linguis- tic units in a sentence, utterance or in a text. FoS should not
                     be slowed down or even brought to a standstill by large jumps in information. 
                     This implies that the more the information in sentences or statements swings up or
                     down, the higher the peaks and troughs of information, 
                     the more difficult it is for the recipient to process the information. Excessive information
                     fluctuations can even prevent processing alto- gether.
                     </div>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">Information density characterises linguistic communication. There- fore, if we assume
                     that the subtext plays an essential role in Chekhov’s prose 
                     (which we do, see above for motivation), we can expect that fac- toring in the subtext
                     will lead to a more balanced FoI. 
                     In other words, we interpret density as the decisive discriminating feature in order
                     to distinguish the narration enriched both by meaningful 
                     and fake subtexts from the bare narration.
                     </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">The measurement of meaningfully enriched text should show less peaks and troughs than
                     the one of the bare narration. 
                     From that our first prediction follows: in meaningfully enriched texts, the density
                     will be optimised compared to the original text. 
                     In contrast, with fake enrichments, the density is predicted to deteriorate drastically.
                     </div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">H1 Adding meaningful glosses to the original text leads to a more balanced FoS.
                     H2 Adding fake glosses to the original text leads to a less balanced FoS.
                     </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">What does that mean in terms of the principle of UID? In Section 3.3, we give the
                     definition of UID that we follow in this paper: 
                     in- formation density gets more balanced, that is, better, when its value shifts towards
                     zero. 
                     This represents on average small surprisal jumps from word to word and it represents
                     uniformity. 
                     In contrast, when a UID value moves away from zero, it is a deterioration since this
                     means that 
                     there are larger surprisal gaps between words. The dis- persion of UID values should
                     not be large. 
                     If this were the case, it would indicate arbitrariness in the UID distribution. From
                     that, 
                     our second prediction follows: a meaningful subtext will reduce the scat- ter of UID
                     values compared to the original text. 
                     Consequently, a fake subtext will lead to stronger scattering.
                     </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">Models that employ information theory are rarely applied in digi- tal humanities,
                     and to the best of our knowledge, 
                     there are no studies that aim to disclose the subtext in the framework of information
                     the- ory.
                     </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">In the next sections, we introduce some fundamentals of informa- tion and surprisal
                     theory, and we present the Topic Context Model. 
                     Subsequently, we describe our methodology and the different enrich- ment strategies
                     (glossings) that we employed. 
                     Finally, we report the experimental results and discuss the outcomes of our study.
                     
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3. Information theoretic background</h1>
                  
                  <div class="div div1">
                     <h2 class="head">3.1 Information theory and information as a lexical feature</h2>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">Shannon’s information theory [1, 29] models the transmission of in- formation from
                        a sender to a receiver. 
                        The information theoretic model aims to output a code in the form of consecutive 0
                        and 1 that compresses a 
                        message as much as possible without loss of informa- tion. Shannon developed his theory
                        in an engineering context, 
                        and from the 1950s onwards the theory was applied to natural language.
                        </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">Shannon Information (SI) is measured in bits. 1 bit is the informa- tion of two equally
                        probable possibilities. 
                        For example, a toss with a fair coin carries 1 bit of information because there are
                        two possible outcomes of a toss, i.e., 
                        ”front of the coin”, for instance represented by ‘1’, or ”back of the coin”, represented
                        by ‘0’. 
                        The amount of bits equals the number of yes/no decisions needed to determine results
                        of processes 
                        such as tossing a coin or determining a word in a chain, given a good algorithm.
                        </div>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">For the sake of illustration, imagine a toy language L that com- prises solely eight
                        words, 
                        that is, L = {Anton, Chechov, was, a, very talented, Russian, writer}. 
                        Lets agree that in L any sentence must con- sist of eight words, and there are no
                        rules of linear precedence in a sentence, 
                        so that for instance both Anton a was very Chekhov talented writer Russian or a was
                        Chekhov Russian Anton talented writer 
                        very are possible sentences with the same probability. For a sentence in the toy language
                        it needs three yes/no questions to 
                        determine a word w uniquely as shown in Figure 1:
                        </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">As can easily be seen, the word Chekhov as terminal element in Figure 1 carries three
                        bits of information. We get the same result when Formula 1 is applied. 
                        This expression is the standard definition of SI, i.e., the negative log-likelihood
                        of a linguistic unit w: </div>
                     
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">
                        
                        
                        Since the probability of a word w in L is P(w)= 1/8, we get for instance for the word
                        Anton in Figure 1 above:
                        </div>
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">
                        
                        The information type Surprisal [25, 26] is basically s a psycholinguistic concept:
                        
                        
                        
                        <div id="figure01" class="figure">
                           
                           
                           <a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="A downloaded image of a decision tree including information bits of words in a sentence." /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 1. </div>Information amount in bits of words in a sentence repre- sented in a decision tree.</div>
                        </div>
                        
                        
                        <a class="noteRef" href="#d4e267">[1]</a>
                        surprisal of a linguistic unit is proportional to the mental processing effort that
                        it causes [25]. If the probability of a word is high in 
                        a given context, its surprisal and the effort for the processing of that word are
                        low. For instance, given the word Barack in a text, 
                        the probability is presumably quite high that Obama fol- lows. In that case, Obama
                        would carry low surprisal. 
                        An example of high surprisal because of the semantic unexpectedness of words is Chomsky’s
                        famous sentence colorless green ideas sleep furiously, with which he wanted to make
                        clear that 
                        semantically uninterpretable sentences can nevertheless be syntactically well-formed
                        [32]. 
                        To give another example: the sentence final verb fell in the famous Garden Path-sentence
                        the horse 
                        raced past the barn fell is extremely unex- pected and carries high surprisal [25].
                        
                        Consequently, this requires a considerable processing. A language processor would
                        
                        probably as- sume that barn is the sentence final word given the preceding context.
                        The expression in 
                        Formula 3 defines surprisal, as a special case of SI as in Formula 1, as negative
                        log-likelihood of a linguistic unit. 
                        The basis is its conditional probability which is in Formula3 represented by the vertical
                        bar.
                        
                        The variables w1 . . . wi−1 represent co-occurrences of any kind of the targetwordwi
                        withinasentence. 
                        ThevariableCONTEXT represents extra-sentential contexts [26].
                        </div>     
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">Technically, contexts for the calculation of surprisal can be co- occurrences of target
                        linguistic units, such as n-grams of any type (e.g. terminal symbols and of part-of-speech
                        tags) [33], 
                        syntactic structures [34, 35, 36, 37, 38] and also semantic contexts [39, 40, 41,
                        42, 43].
                        </div>
                     
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.2 Topic Context Model</h2>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">The Topic Context Model (TCM)[39, 40, 41, 44, 42] 
                        
                        <a class="noteRef" href="#d4e281">[2]</a>
                        
                        calculates the se- mantic surprisal of words relative to an extra-sentential context
                        in a given corpus and predicts that a word carries 
                        high surprisal when its mean probability, that is, the expected value, in the topics
                        of a docu- ment is low. 
                        The prerequisite for calculating the information content of a word in a text is the
                        knowledge of the topics that the text contains. 
                        In general terms, a topic is what a text document is about. Depend- ing on how coarse-
                        or fine-grained we look at a text, 
                        the text can deal with more or less topics. In this study we calculate the surprisal
                        on a paragraph basis, meaning that we 
                        use the different paragraphs as the basis for calculating topics.
                        </div>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">The topics that a paragraph covers are established by the process of topic detection.
                        In this study, TCM relies on the topic detection model Latent Dirichlet Allocation
                        (LDA) 
                        [45] which is a widely used, successful model, but other models can also be used in
                        TCM such as Latent Semantic Analysis.
                        </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">The assumption behind LDA is that (i) each text document con- tains a statistical
                        mixture of topics, that (ii) similar topics correlate with similar probability distributions
                        of words, 
                        and that consequently (iii) each topic is characterised by a specific distribution
                        of the words. How many topics LDA should detect can be 
                        chosen before calculation by means of the topic detection starts.
                        </div>
                     
                     <div class="counter"><a href="#p32">32</a></div>
                     <div class="ptext" id="p32">Assume that LDA has disclosed two topics in a document, as illus- trated in Figure
                        2:
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" style="" alt="Downloaded image depicting Topic 1 and Topic 2" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 2. </div>Topics as probability distributions of words.
                              </div>
                        </div>
                        
                        </div>
                     
                     <div class="counter"><a href="#p33">33</a></div>
                     <div class="ptext" id="p33">In the yellow-coloured topic, high probabilities are assigned to words in the semantic
                        field of, say, human created buildings. The pink-coloured topic contains highly probable
                        words 
                        from the auto- mobile sector. It is hardly surprising that a word from the nutrition
                        such as chocolate has a low probability in both topics. TCM calculates the surprisal
                        of chocolate per topic according to the Formulae 1 and
                        3 and then takes the mean value. In our example, in Pt1 (chocolate) = 0.00003 and
                        surprisal(chocolate) = −log20.00003 = 15.02 bits and Pt2 (chocolate) = 0.000003 and
                        surprisal(chocolate) = −log20.000003 = 18.35 bits, and the
                        mean is 16.69, see Formula 4.
                        
                        <a class="noteRef" href="#d4e303">[3]</a>
                        
                        </div>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">The surprisal for a language processor who encounters the word chocolate in the context
                        of the two topics buildings and automobiles is therefore quite high. 
                        </div>
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">Figure 3 illustrates the working of the TCM.
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" style="" alt="A screenshot of an image depicting TCM text to surprisal." /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 3. </div>Illustration of the Topic Context Model.</div>
                        </div>
                        
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">3.3 Uniform information density</h2>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">Uniform Information Density (UID) is an important principle in lin- guistic communication
                        of humans [46, 47] but it is also important in artificial generation of language:
                        in any linguistic product, 
                        informaion peaks and troughs must not be too extreme, so as not to make it too difficult
                        for the recipient of a message to process it. In this study, we utilise the operationalisation
                        of UID in [48].
                        
                        <a class="noteRef" href="#d4e329">[4]</a>
                        </div>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">UIDLOCAL is the measure of the average (squared) information change from word to word
                        in a sentence. Formula 5 gives its defi- nition: id is the information / surprisal
                        of a word, n represents the total number of words in a sentence. 
                        Instead of UIDLOCAL we use the term UIDwordwise [49, 50].
                        
                        
                        </div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">UIDwordwise is negative by definition, and therefore a UIDwordwise value close to
                        zero indicates a high uniformity of the information den- sity distribution, that is,
                        on average smaller information jumps 
                        from word to word.
                        </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">In the Garden Path-example from above information does not flow very smoothly as becomes
                        clear in Figure 4 from [25]. The informa- tion is distributed fairly evenly over the
                        first five words. 
                        However, the jump in information in the sentence-final word fell is enormous:
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" style="" alt="A screenshot of a bargraph titled Flow of Information, including the axia Information Values and Garden Path sentence" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 4. </div>Flow of information in a Garden Path-sentence.</div>
                        </div>
                        
                        </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">The steep sentence final information peak threatens the successful processing of the
                        sentence.
                        </div>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">The comparison of UIDwordwise in the ”expectable” sequence and in the garden path
                        clearly shows that in the latter, the value of UIDwordwise is further away from 0
                        (see (b) below) than in the expected sequence (see (a) below). 
                        This represents the lower uniformity of the infor- mation density in the garden path
                        sentence. We use the information values from Figure 4 as input of expression 5:</div>
                     
                     
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">In the present study, we use UID as a measure of the ”communica- tive naturalness”
                        of the Chekhovian original text and its manipulated versions. As should have become
                        clear, we hypothesise that the orig- inal text alone, i.e. 
                        without its subtext, performs worse with respect to this measure than the original
                        text enriched by its subtext. In the next section we explain how we set up the enrichments.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">4 Enrichment</h2>
                     
                     <div class="counter"><a href="#p43">43</a></div>
                     <div class="ptext" id="p43">As noted above, the general methodology is to perform three differ- ent measurements
                        of surprisal of the text words in Ward No. 6. First, we measure the surprisal of the
                        words based on the word material of the narration alone. In a second measurement (or
                        set of measure- ments, see below), 
                        we enrich the narration by a text addendum which is semantically related to the original
                        text words. Finally, we use an additional text which is not content-related to the
                        original text in any obvious way. In the present section, we describe how we set up
                        the meaningful text addendum, i.e. 
                        the meaningful glosses.</div>
                     
                     <div class="counter"><a href="#p44">44</a></div>
                     <div class="ptext" id="p44">Recall that we conceive of the meaningful glosses as a toy model of the subtext of
                        the narration. Accordingly, the enrichment shall make explicit the implicit knowledge
                        conveyed by the text. To determine appropriate subtext explications, we make use of
                        the idea brought forward by Frame Semantics [51] that lexical material 
                        in the form of content words evokes, when used in a text, conceptual frames that represent
                        generalised conceptual knowledge associated with the re- spective content word [52].</div>
                     
                     <div class="counter"><a href="#p45">45</a></div>
                     <div class="ptext" id="p45">Specifically, we use BabelNet [53], a multilingual semantic network compiled from
                        a variety of sources such as WordNet and Wikipedia, as an encyclopedic resource providing
                        the conceptual knowledge as- sociated with lexical material. BabelNet provides a network
                        of con- ceptual nodes (“senses”) 
                        interconnected by semantic relations of mul- tiple types, e.g. hypernyms, hyperonyms,
                        meronyms etc. The nodes contain translations to multiple languages and also glosses
                        providing definitions or descriptions of the concepts.
                        </div>
                     
                     <div class="counter"><a href="#p46">46</a></div>
                     <div class="ptext" id="p46">We set up a pipeline that automatically enriches a given text by adding information
                        associated with content words available from Ba- belNet. The pipeline is based on
                        Apache UIMA [54], uses open source software components and comprises the following
                        steps:
                        
                        <ul class="list">
                           <li class="item">Part-of-speech(POS)Tagging:Inthisstep,POS-tagsareassigned to all word forms using
                              DKPro HunPosTagger [55].
                              </li>
                           <li class="item">Lemmatisation:usingDKProLanguageToolLemmatizer[56],we lemmatised the word forms. Further,
                              stopwords, 
                              punctuation and duplicate words within paragraphs are removed.</li>
                           <li class="item">WordSenseDisambiguation(WSD):weemployedDKProWSD [56] in combination with an extension
                              that enables the use of BabelNet [57] to resolve ambiguous lemmas to their most prob-
                              able sense. 
                              A number of disambiguation methods have been tried, with varying results.</li>
                           <li class="item">Enrichment Generation: we used the following method to en- rich the text:</li>
                        </ul>
                        Overall, we created a number of enrichment variations resulting from varying the following
                        parameters:
                        
                        The data resource for the “fake” glosses was the Wikipedia based ”rus_news_2020_1M”
                        corpus (1M sentences) from the ”Wortschatz Leipzig” corpora collection.
                        
                        <a class="noteRef" href="#d4e392">[5]</a>
                        </div>
                     
                     <div class="counter"><a href="#p47">47</a></div>
                     <div class="ptext" id="p47">The core concept of our methodology is to observe the difference in the surprisal
                        for each text word before and after meaningful or fake enrichments. 
                        Therefore, we employed TCM to calculate the sur- prisal for every token in the original
                        text and for the four enrichments of each type. Table 1 gives an overview of the used
                        enrichments and the size of the corpora.</div>
                     
                     
                     <div class="counter"><a href="#p48">48</a></div>
                     <div class="ptext" id="p48">When running LDA, we use the lemmatised tokens and ignored punctuations, stopwords
                        and tokens shorter than three letters. 
                        Defining the number of topics is essential for LDA and has been determined experimentally
                        for TCM calculation so far. 
                        We therefore calculated the surprisal values three times using three different numbers
                        of top- ics, see Section 3.2 for the calculations. 
                        We then averaged the three resulting surprisal values for the final surprisal value.
                        Further, we normalised the surprisal values to 
                        have a consistent range of values among different experiments. Then, we calculated
                        the information density distribution for 
                        every enrichment and the original text to ob- serve how enrichments change the information
                        density.</div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">5 Results</h2>
                     
                     <div class="counter"><a href="#p49">49</a></div>
                     <div class="ptext" id="p49">Firstly, we tested whether the different text variants differ measurably in their
                        information density. To do this, we compared the meaningfuk
                        
                        
                        
                        
                        values in the sets and used the non-parametric Mann-Whitney-test.
                        
                        <a class="noteRef" href="#d4e410">[6]</a>
                        
                        The results of the Mann-Whitney-tests are given in Table 2. A significance level of
                        p  .05 means 
                        that there are strong, non-random dif- ferences between the sets.
                        </div>
                     
                     <div class="counter"><a href="#p50">50</a></div>
                     <div class="ptext" id="p50">All mean differences of information density between the original text and the texts
                        enriched with fake glossings are highly significant. In each case p ≈ 0. The hypothesis
                        of equality of means has thus to be rejected. 
                        That is to say, the information density in the original texts differs strongly from
                        the density in fake glossed texts. Statistically speaking, original texts and fake
                        glossed texts are therefore different sets.</div>
                     
                     <div class="counter"><a href="#p51">51</a></div>
                     <div class="ptext" id="p51">Weaker pronounced is the difference between the information den- sity of the original
                        text and the meaningfully glossed text. Approx- imately half of the comparisons lack
                        significance, and the comparison of the original text 
                        with meaningful glossing sourced from all lexicons and graph connectivity is highly
                        insignificant, with a signifance
                        
                        level even of p = .30.
                        </div> 
                     
                     <div class="counter"><a href="#p52">52</a></div>
                     <div class="ptext" id="p52">The following figures compare the mean values of the UIDwordwise-
                        values and their dispersion in original, meaningful and fake glossed texts. 
                        A mean tends towards 0 means that many of the sentences in a data set show little
                        wordwise surprisal- changes which means that the information 
                        density is tendentialy uniform. Recall that UIDwordwise is negative by definition.
                        </div>
                     
                     <div class="counter"><a href="#p53">53</a></div>
                     <div class="ptext" id="p53">Figure 5 displays that for the All-GRAPH-method employing word sense disambiguation
                        (WSD) via graph connectivity, meaningful gloss- ing make the density of surprisal
                        more uniform compared to the original text, 
                        albeit only a little bit (5b). Conversely, in fake glosses (5b) information density
                        in sentences is much less uniform. Meaningful glossing reduces the dispersion of 
                        UID-values compared to the origi- nal text (5a), whereas fake glossing results in
                        a substantially stronger dispersion of the UID-values (5b).</div>
                     
                     <div class="counter"><a href="#p54">54</a></div>
                     <div class="ptext" id="p54">Considering only LESK disambiguation, the scenario alters clearly: meaningful glosses
                        fail to make the information density more uni- form, even worsening it while, however,
                        decreasing the dispersion (5c). 
                        Fake glosses make information density less uniform, accompa- nied by heightened dispersion
                        (5d).</div>
                     
                     <div class="counter"><a href="#p55">55</a></div>
                     <div class="ptext" id="p55">For the WNTR-glossed method with GRAPH connectivity, Figure 6 shows as well that meaningful
                        glossing does not manage to make in- formation distribution more uniform, rather the
                        contrary is the case. However the dispersion is reduced (6a) which is an improvement.
                        
                        The same is the case with LESK disambiguation (6c). With the fake glosses, the information
                        density gets less uniform while, in contrast to meaningful glossing, the dispersion
                        of the density values increases (6b / 6d).
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" style="" alt="A screenshot of four graphs, comparisons of means of original and meaninful/fake glossed texts." /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 5. </div>Comparison of means of original and meaningful / fake glossed texts based on UIDs
                              for ALL-glossed method.</div>
                        </div>
                        
                        </div>
                     
                     <div class="counter"><a href="#p56">56</a></div>
                     <div class="ptext" id="p56">The plots in the Figures 7 and 8 depict probability distributions of UID values for
                        the glossing types ALL GRAPH (ALL GRAPH method with WSD via graph connectivity), ALL
                        LESK, WNTR GRAPH and WNTR LESK data. 
                        
                        <a class="noteRef" href="#d4e443">[7]</a>
                        
                        The x-axis gives the UIDwordwise values, the y-axis gives the probabilities.</div>
                     
                     
                     <div class="counter"><a href="#p57">57</a></div>
                     <div class="ptext" id="p57">Firstly, all plots show very clearly that fake glossed texts have distributions of
                        UID-values reminiscent of a flattened normal distribu- tion halfway, so to speak,
                        to a uniform distribution: we see flattened peaks, 
                        and the edges of the distributions run wide to the left and right meaning the probability
                        of extreme values is high. The distribution of the fake glossed text exhibits low
                        kurtosis, i.e., peakiness.
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure06.png" rel="external"><img src="resources/images/figure06.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 6. </div>Comparison of means of original and meaningful / fake glossed texts based on UIDs
                              for WNTR-glossed method.</div>
                        </div>
                        
                        <a class="noteRef" href="#d4e457">[8]</a>
                        </div>
                     
                     <div class="counter"><a href="#p58">58</a></div>
                     <div class="ptext" id="p58">Secondly, the plots in the Figures 7a and 8 visualise that meaning- fully enriched
                        texts exhibit higher kurtosis compared to the original text relative to fake enriched
                        texts, that is to say, they have a higher, steeper peak. 
                        However, this does not seem to hold when LESK al- gorithm is employed as becomes visible
                        in Figure 7b: here, the density distribution in the original text seems to be more
                        peaky than in the meaningful enriched text, 
                        and the distribution in the latter runs out far to the left and right. Figure 7b seems
                        to show that the gener- alisation about kurtosis, i.e., that a meaningful enriched
                        texts has a more peaky density distribution than the original text, does not hold.
                        
                        We aimed to confirm this suspicion and chose an alternative graphical representation
                        of the density distribution in the LESK text with a Cullen and Frey-Graph in Figure
                        9.
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure07.png" rel="external"><img src="resources/images/figure07.png" style="" alt="" /></a>
                           
                           <div class="caption">
                              <div class="label">Figure 7. </div>Information density of meaningfully / fake enriched texts and original texts.</div>
                        </div>
                        
                        It visualises the proximity and distance of LESK distributions with a set of theoretical
                        distributions. The x-axis gives the degree of skewness of a distribution, and the
                        y- axis gives the kurtosis. 
                        
                        <a class="noteRef" href="#d4e471">[9]</a>
                        
                        Figure 9 discloses that the ALL LESK density-distribution
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure08.png" rel="external"><img src="resources/images/figure08.png" style="" alt="" /></a>
                           </div>
                        
                        has higher kurtosis than the distribution in the original text, and the plot thus
                        confirms the generalisation from above that the density distribution in meaningfully
                        enriched texts is more peaky than the distribution in the original text. In addition,
                        Figure 7b shows very clearly the proximity of the fake distribution to the uniform
                        distribution 
                        that we observed already in the Figures 7a and 8. So, can we infer a subtext effect
                        from these results? We will return to this research question posed above in the Section
                        6.
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure09.png" rel="external"><img src="resources/images/figure09.png" style="" alt="" /></a>
                           </div>
                        
                        </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">6 Discussion and conclusion</h2>
                     
                     <div class="counter"><a href="#p59">59</a></div>
                     <div class="ptext" id="p59">The study confirms H2, and it confirms H1 solely for the enrichment combination ALL
                        GRAPH (i.e. all available lexicons and the graphtheoretical disambiguation model),
                        
                        albeit the improvement is only marginal. Just for this case we observe that meaningful
                        glossing im- proves the FoI, and that, in addition, the dispersion of density values
                        decreases. Thus, with ALL GRAPH enrichments, the expected effect, albeit weak, occurs.
                        
                        In contrast, no such effect occurs with the rest of the meaningful enrichments: although
                        the dispersion generally decreases (except for the LESK algorithm on all lexicons),
                        the information density generally gets worse. 
                        So, we have to draw an overall negative conclusion: taking into calculation the otherwise
                        invisible subtext, which we implemented by different versions of glosses semantically
                        related to the original text, 
                        does not lead to a more uni- form information density.</div>
                     
                     <div class="counter"><a href="#p60">60</a></div>
                     <div class="ptext" id="p60">Our study also produced a very positive result, however, because we managed to determine
                        a parameter that does correspond posi- tively to the addition of meaningful glosses
                        and negatively to the ad- dition of fake glosses: peakiness/kurtosis. When fake enrichments
                        are added to the original text, the distribution of UID-values flattens, 
                        which means that it approaches the contour of the uniform distribu- tion (the uniform
                        distribution results from a maximally fake text in which every word is chosen arbitrarily,
                        see Figure 10). 
                        When meaningfully related glosses of the same cardinality are added to the original
                        text, by contrast, 
                        the resulting contour shows a higher peak compared to the one of the original text.</div>
                     
                     <div class="counter"><a href="#p61">61</a></div>
                     <div class="ptext" id="p61">The described finding comes out clearly in Figures 7a, 8a, and 8b. It is only with
                        respect to enrichment type ALL LESK that the differ- ence cannot be read from the
                        respective plot, see 7b. However, the alternative visual representation in Figure
                        9 shows that also in this case meaningful enrichments lead to a higher kurtosis value
                        whereas fake enrichments 
                        lead to a lower kurtosis value compared to the kurtosis value of the original text.
                        This can be taken as a confirmation that the text features surprisal and information
                        density, so to speak,
                        measure semantic properties. We may confidently conclude that we attested a subtext
                        effect.</div>
                     
                     <div class="counter"><a href="#p62">62</a></div>
                     <div class="ptext" id="p62">Although we have shown that information theory may unravel the subtext in Chekhov,
                        the observed subtext effects are subtle. This may be due to that the glosses that
                        we used in the documented experi- ment are too primitive to reproduce any more clear
                        subtext effect. Recall the basic idea underlying this study: if the subtext is an
                        in- alienable part of the semantics of a meaningful text, measuring the surprisal
                        values of original texts alone will produce values that re- flect the actual content
                        of the text only incompletely. 
                        To measure the true surprisal of a text, one would have to enrich the original text
                        by the perfect explication of the subtext.
                        
                        
                        <div class="figure">
                           
                           
                           <a href="resources/images/figure10.png" rel="external"><img src="resources/images/figure10.png" style="" alt="" /></a>
                           </div>
                        
                        The (sub)textual prostheses that we actually used in our experiments are obviously
                        very primitive enrichments, far from being perfect. 
                        Future research will therefore have to focus on the technique of enrichment. 
                        </div>
                     
                     <div class="counter"><a href="#p63">63</a></div>
                     <div class="ptext" id="p63">While in this study, the generation of the subtexts was based in principle on the
                        Fillmore model, future experiments will be carried out statistically based by employing
                        neural networks. Doing so while keeping the features information density and kurtosis
                        / skewness, 
                        we hope to achieve and observe a more significant subtext effect than in the current
                        study.
                        </div>
                     </div>
                  </div>
               
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e267"><span class="noteRef lang en">[1] Empirical evidence comes amongst others from [30, 31]</span></div>
               <div class="endnote" id="d4e281"><span class="noteRef lang en">[2] https://github.com/jnphilipp/tcm</span></div>
               <div class="endnote" id="d4e303"><span class="noteRef lang en">[3] The above explanations and the calculation 4 are, of course, a simplified de- scription
                     TCM. For more details, see [39, 40, 41, 42, 44].</span></div>
               <div class="endnote" id="d4e329"><span class="noteRef lang en">[4] https://github.com/jnphilipp/uid</span></div>
               <div class="endnote" id="d4e392"><span class="noteRef lang en">[5] https://wortschatz.uni-leipzig.de/de</span></div>
               <div class="endnote" id="d4e410"><span class="noteRef lang en">[6] Non-parametric means that the test makes no or just minimal assumptions about the
                     distribution within the data. An example may illustrate the question posed by statistical
                     tests of mean-differences: the Dutch and the Danes are 
                     among the tallest populations in the world. On average, there is hardly any difference
                     between them. Differences in height are therefore probably not characteristic to group,
                     i.e., the group of the tallest populations. 
                     Rather, we could say that the differ- ences are not important, they are due to chance.
                     In terms of height, the Dutch and Danes belong to the same group, that is, the group
                     of the tallest people. 
                     In Malta, for example, the average height of the population is much lower. The difference
                     in average height between the Dutch and the Maltese is likely to be highly significant.
                     
                     This means that the Dutch and the Maltese do not belong to the same height group.</span></div>
               <div class="endnote" id="d4e443"><span class="noteRef lang en">[7] The area under the curves must be approximately 1 in each case.</span></div>
               <div class="endnote" id="d4e457"><span class="noteRef lang en">[8] https://en.wikipedia.org/wiki/Kurtosis</span></div>
               <div class="endnote" id="d4e471"><span class="noteRef lang en">[9] 0 https://en.wikipedia.org/wiki/Skewness</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="d4e608"><!-- close --></span></div>
               <div class="bibl"><span class="ref" id="baldick2015"><!-- close -->Baldick 2015</span> Baldick, C. (2015) <cite class="title italic">The Oxford dictionary of literary terms</cite>. Oxford: Oxford University Press.</div>
               <div class="bibl"><span class="ref" id="ermatova2010"><!-- close -->Ermatova 2010</span> Ermatova, E.V. (2010) “Implicitnost’ v chudožestvennom tekste”, <cite class="title italic">Izda- tel’stvo Saratovskogo universiteta</cite>. 
                  </div>
               <div class="bibl"><span class="ref" id="gatrall2003"><!-- close -->Gatrall 2003</span> Gatrall, J.J. (2003) “The paradox of melancholy insight: Reading the medical subtext in chekhov’s “a boring
                  story””, <cite class="title italic">Slavic Review</cite>, 62(2): 258-277. 
                  </div>
               <div class="bibl"><span class="ref" id="grice1989"><!-- close -->Grice 1989</span> Grice, P. (1989) <cite class="title italic">Studies in the way of words</cite>. Cambridge: Harvard University Press.
                  </div>
               <div class="bibl"><span class="ref" id="hinrichs2014"><!-- close -->Hinrichs 2014</span> Hinrichs, U. (2014) <cite class="title italic">Die dunkle materie des wissens: Über leerstellen wissenschaftlicher erkenntnis</cite>. Giessen, Germany: Psychosozial-Verlag. 
                  </div>
               <div class="bibl"><span class="ref" id="irmer2011"><!-- close -->Irmer 2011</span> Irmer, M. (2011) <cite class="title italic">Bridging inferences</cite>. Boston: de Gruyter.            
                  </div>
               <div class="bibl"><span class="ref" id="lelis2011"><!-- close -->Lelis 2011</span> Lelis, E.I. (2011) “Podtekst i smežnye javlenija”, <cite class="title italic">Istorija i filologija</cite>, 4: 143–151.</div>
               <div class="bibl"><span class="ref" id="lelis2013"><!-- close -->Lelis 2013</span> Lelis, E.I. (2013) <cite class="title italic">Podtekst kak lingvopoėtičes kajakategorija v proze</cite>. Iževsk, Russia: A.P. Čechova.</div>
               <div class="bibl"><span class="ref" id="myrkin1976"><!-- close -->Myrkin 1976</span> Myrkin, V.J. (1976) “Tekst, podtekst i kontekst”, <cite class="title italic">Voprosy jazykoznanija</cite>, 2. 
                  </div>
               <div class="bibl"><span class="ref" id="nikoljukin2003"><!-- close -->Nikoljukin 2003</span> Nikoljukin, A.N. (2003) <cite class="title italic">Literaturnaja ėnciklopedija terminov iponjatij</cite>. Moskva, Russia: NPK ”Intelvak".    
                  </div>
               <div class="bibl"><span class="ref" id="shannon1948"><!-- close -->Shannon 1948</span> Shannon, C.E. (1948) “A mathematical theory of communication”, <cite class="title italic">The Bell system technical journal</cite> 27(3), 379-423. 
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>