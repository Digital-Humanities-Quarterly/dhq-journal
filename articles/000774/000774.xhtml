<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en"></h1>
               
               
               <div class="author"><span style="color: grey">first name(s) family name</span></div>
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=family name&amp;rft.aufirst=first name(s)&amp;rft.au=first name(s)%20family name"> </span></div>
            
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>The present paper reports a pilot study to approach the sub- text in Chekhov’s short-story
                     ”Ward No. 6” by means of informa- tion theory. 
                     The original text is enriched by glosses by which we intend to make explicit the implicit
                     knowledge conveyed by the original text, i.e., the subtext. 
                     We generated several text variants with meaningful enrichments and one fake variant
                     that served as a baseline. We could not observe that 
                     semantic surprisal as a feature of words and uniform information density have a sub-
                     text effect throughout all text variants. 
                     However, it turned out that kurtosis and skewness are suitable classification criteria
                     to distinguish meaningful enrichments from fake enrichments.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1. Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">This paper is a first attempt to approach the subtext by means of infor- mation theory
                     [1]. The object of study is a text by the Russian writer Anton P. Chekhov, 
                     namely the short-story ”Ward No. 6” (”Palata No. 6” in the original).
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">The message conveyed by any text is composed of two kinds of meaning. The first is
                     the explicit meaning, i.e. the meaning that is explicitly coded by the linguistic
                     material appearing black on white. 
                     The second is the meaning that a cooperative interpreter will infer from the explicit
                     text following Gricean or Neo-Gricean reasoning [2], 
                     making use of both linguistic knowledge and non-linguistic resources (world knowledge,
                     conversational context). Even the most explicit text leaves open numerous informational
                     gaps, 
                     which must be filled by the interpreter. These ”bridging inferences” [3] constitute
                     the im- plicit meaning of the text, for which we utilise the term subtext here. 
                     Another metaphor which is sometimes employed is the visible text as opposed to dark
                     text (e.g. [4]).
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The discourse about the subtext is rich (see [5, 6] for a survey), and the notion
                     is controversial. According to the Oxford Dictionary of lit- erary terms [7], 
                     the subtext is “any meaning or set of meanings which is implied rather than explicitly
                     stated in a literary work, especially in a play”. 
                     For the Russian Literary encyclopedia of terms and concepts [8], the subtext (“podtekst”
                     in Russian) represents the “hidden sense of an utterance, 
                     stemming from the interaction of the literary mean- ings, the context, and the speech
                     situation”. 
                     As can be seen, there are basically two ways of understanding what subtext is [9,
                     p. 72]. 
                     Un- der the first reading, the one described above, advocated by [7], the term is
                     basically synonymous to “pragmatic inferences”. 
                     Besides that, the term subtext is used in the literature also to refer to the ultimate
                     sense of a literary text, i.e. 
                     to the interpretation that is intended by the author and that the reader has to decipher
                     (e.g. [8, 10]). 
                     In that second respect, the meaning of subtext comes close to the ”moral of the story”.
                     In what follows, we use subtext only in the first of these two readings.         
                     
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">To approach the implicit meanings of a text, we will investigate the Russian short-story
                     ”Palata No. 6”, 
                     written by Anton P. Chekhov, published in 1892. Chekhov is considered to have originated
                     the role of the subtext, 
                     in his plays but also in his prose (e.g. [11, 12]). Us- ing a narrative text for our
                     purposes has the advantage that the in- formational 
                     impact of the conversational context is reduced because the addressee (reader) 
                     does not have to calculate her own position as well as the position of the speaker
                     (author).                    
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">Related to the previous point, Chekhov is known for having devel- oped a specific
                     literary technique of “concentration and shortness” [13, p. 48-49], 
                     manifesting itself in the language of the narrator, which usually shows simple syntax,
                     parataxis, short sentences. 
                     This and the sparse use of adjectives, comparisons and metaphors is functional, 
                     for Chekhov’s intention is to challenge the reader intellectually and to encourage
                     critical reading[13, p. 67][14]. 
                     The reader is asked to ac- tively interpret by filling the informational gaps of the
                     texts. 
                     There- fore, we may expect a Chekhov story to entail a drastic discrepancy between
                     what is said explicitly and what is meant by the text. 
                     If the subtext is “a level of speech between the lines” ([15]), we may thus safely
                     expect a lot of speech between the lines in the work of Chekhov.
                     </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">The general language of the Chekhovian short-story is the Russian literary language
                     [13, p. 49], 
                     which is important for our methodol- ogy to be introduced below. 
                     The particular text ”Ward No. 6” has been chosen, moreover, 
                     because of its considerable length compared to other short-stories of the author.
                     
                     For our methodology to work, we need a text of a certain cardinality of words.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2. Outline</h1>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">In this paper, we presuppose a probabilistic notion of linguistic mean- ing. The basic
                     assumption is that the meaning of a word influences 
                     the probability with which other words accompany that word in a text. Given a sufficiently
                     large number of text passages in which a word appears, 
                     it is in principal possible to obtain a co-occurrence profile that is characteristic
                     of the word. 
                     We consider this profile, the co-text of the word, to be a function of its meaning
                     1
                     </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Our point of departure is that information and the flow of infor- mation (FoI) are
                     characterising features of texts. 
                     By FoI we mean the distribution of information per word over time in sentences. Our
                     ap- proach is inspired by [23] who, 
                     in a Shannon information theoretic framework, was concerned with the relationship
                     between knowledge and information in linguistic messages. 
                     [23] states that Shannon in- formation (SI) forms a framework for conveying meaning
                     and its evolve- ment: in his view, information is a key component of knowledge, 
                     and knowledge acquisition is a result of the way information is processed. In the
                     present paper, we exploit this idea, 
                     likewise assuming that the flow of information is a classifying semantic feature of
                     texts.
                     </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">We use contextualised information, i.e., surprisal [24, 25, 26] as a lexical feature
                     of a word w.
                     </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">In order to approach the actual meaning of a word, in other words, we measure the
                     effect that the meaning of the word has on its co-text.
                     </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">This idea is at the heart of the Topic Context Model (TCM), see Sec- tion 3.2. TCM
                     calculates the amount of semantic surprisal of a word, i.e., 
                     the amount of SI, respectively, that can be derived from a semantic context.     
                     
                     </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">In the due course, we use the terms information and information value as equivalents
                     of surprisal and surprisal value, 
                     respectively, and we speak of the flow of surprisal (FoS) and the flow of information
                     (FoI).
                     </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">Since the surprisal of a word w depends on the text in which it ap- pears, 
                     it should make a difference whether the information carried by w is calculated only
                     on the basis of those words 
                     that literally con- stitute the novel (bare narration), or whether it is calculated
                     on the basis of the narration 
                     enriched by additional word material. We take methodological advantage of this fact
                     in the following way.
                     </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">In addition to the bare narration, we set up a contextualised nar- ration, which consists
                     of the original text words plus additional texts that 
                     contextualise the original text words by adding encyclopedic knowl- edge about them.
                     These additional texts, which we also refer to as glosses, serve as a (very simple)
                     model of the subtext of the novel. 
                     De- tails on how the text is enriched by additional knowledge are given in Section
                     4. In order to check whether the expected change in lexical information through contextualisation,
                     i.e. through the enrichment
                     of the text through meaningful glossing, is a random effect, the orig- inal text was
                     also enriched with text fragments that have no relation
                     to the Chekhovian story in terms of content, that is, a fake glossing. After that,
                     in a first work step, we carry out two measurements: (i) 
                     we determine the surprisal values of the words in the bare narration, (ii) we measure
                     them again, but this time both in the meaningful and fake glossed narration.
                     </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">From FoS, we determine the information density in the text’s para- graphs. Information
                     density is another notion that we need to intro- duce to explain our methodology.
                     
                     The Uniform Information Density (UID) principle [27, 28] says that the sender of a
                     message prefers to distribute surprisal evenly and smoothly across a 
                     sequence of linguis- tic units in a sentence, utterance or in a text. FoS should not
                     be slowed down or even brought to a standstill by large jumps in information. 
                     This implies that the more the information in sentences or statements swings up or
                     down, the higher the peaks and troughs of information, 
                     the more difficult it is for the recipient to process the information. Excessive information
                     fluctuations can even prevent processing alto- gether.
                     </div>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">Information density characterises linguistic communication. There- fore, if we assume
                     that the subtext plays an essential role in Chekhov’s prose 
                     (which we do, see above for motivation), we can expect that fac- toring in the subtext
                     will lead to a more balanced FoI. 
                     In other words, we interpret density as the decisive discriminating feature in order
                     to distinguish the narration enriched both by meaningful 
                     and fake subtexts from the bare narration.
                     </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">The measurement of meaningfully enriched text should show less peaks and troughs than
                     the one of the bare narration. 
                     From that our first prediction follows: in meaningfully enriched texts, the density
                     will be optimised compared to the original text. 
                     In contrast, with fake enrichments, the density is predicted to deteriorate drastically.
                     </div>
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">H1 Adding meaningful glosses to the original text leads to a more balanced FoS.
                     H2 Adding fake glosses to the original text leads to a less balanced FoS.
                     </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">What does that mean in terms of the principle of UID? In Section 3.3, we give the
                     definition of UID that we follow in this paper: 
                     in- formation density gets more balanced, that is, better, when its value shifts towards
                     zero. 
                     This represents on average small surprisal jumps from word to word and it represents
                     uniformity. 
                     In contrast, when a UID value moves away from zero, it is a deterioration since this
                     means that 
                     there are larger surprisal gaps between words. The dis- persion of UID values should
                     not be large. 
                     If this were the case, it would indicate arbitrariness in the UID distribution. From
                     that, 
                     our second prediction follows: a meaningful subtext will reduce the scat- ter of UID
                     values compared to the original text. 
                     Consequently, a fake subtext will lead to stronger scattering.
                     </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">Models that employ information theory are rarely applied in digi- tal humanities,
                     and to the best of our knowledge, 
                     there are no studies that aim to disclose the subtext in the framework of information
                     the- ory.
                     </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">In the next sections, we introduce some fundamentals of informa- tion and surprisal
                     theory, and we present the Topic Context Model. 
                     Subsequently, we describe our methodology and the different enrich- ment strategies
                     (glossings) that we employed. 
                     Finally, we report the experimental results and discuss the outcomes of our study.
                     
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3. Information theoretic background</h1>
                  
                  <div class="div div1">
                     <h2 class="head">3.1 Information theory and information as a lexical feature</h2>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">Shannon’s information theory [1, 29] models the transmission of in- formation from
                        a sender to a receiver. 
                        The information theoretic model aims to output a code in the form of consecutive 0
                        and 1 that compresses a 
                        message as much as possible without loss of informa- tion. Shannon developed his theory
                        in an engineering context, 
                        and from the 1950s onwards the theory was applied to natural language.
                        </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">Shannon Information (SI) is measured in bits. 1 bit is the informa- tion of two equally
                        probable possibilities. 
                        For example, a toss with a fair coin carries 1 bit of information because there are
                        two possible outcomes of a toss, i.e., 
                        ”front of the coin”, for instance represented by ‘1’, or ”back of the coin”, represented
                        by ‘0’. 
                        The amount of bits equals the number of yes/no decisions needed to determine results
                        of processes 
                        such as tossing a coin or determining a word in a chain, given a good algorithm.
                        </div>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">For the sake of illustration, imagine a toy language L that com- prises solely eight
                        words, 
                        that is, L = {Anton, Chechov, was, a, very talented, Russian, writer}. 
                        Lets agree that in L any sentence must con- sist of eight words, and there are no
                        rules of linear precedence in a sentence, 
                        so that for instance both Anton a was very Chekhov talented writer Russian or a was
                        Chekhov Russian Anton talented writer 
                        very are possible sentences with the same probability. For a sentence in the toy language
                        it needs three yes/no questions to 
                        determine a word w uniquely as shown in Figure 1:
                        </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">As can easily be seen, the word Chekhov as terminal element in Figure 1 carries three
                        bits of information. We get the same result when Formula 1 is applied. 
                        This expression is the standard definition of SI, i.e., the negative log-likelihood
                        of a linguistic unit w:
                        
                        <a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="width: 700px" alt="" /></a>
                        
                        
                        </div>
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">
                        
                        </div>
                     </div>
                  </div>
               
               
               
               
               
               </div>
            
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="d4e258"><!-- close --></span></div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>