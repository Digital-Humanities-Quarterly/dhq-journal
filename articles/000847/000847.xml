<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="../../common/schema/DHQauthor-TEI.rng"    type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0" ?>
<?xml-model href="../../common/schema/DHQauthor-TEI.isosch" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<?xml-model href="../../common/schema/dhqTEI-ready.sch"     type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq" xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information. Change the value of xml:lang if not English.-->
                <title type="article" xml:lang="en">Facets of Friction: Investigating
                    epistemological friction between computing and the humanities to support Digital
                    Humanities computing education</title>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Anna <dhq:family>Sollazzo</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0009-0008-1085-1999</idno>
                    <dhq:affiliation>University of Glasgow</dhq:affiliation>
                    <email>anna.sollazzo@glasgow.ac.uk</email>
                    <dhq:bio>
                        <p> Anna Sollazzo is a PhD student at the University of Glasgow with the
                            Centre for Computing Science Education. She obtained her BSc in Computer
                            Science from the University of Victoria and MA in Digital Humanities
                            from the University of Alberta. Her current research blends the two,
                            looking at DH-specific approaches to computing education. Anna is also a
                            member of the <title rend="italic">Comédie-Française Registers
                                Project</title> team.</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id">000847</idno>
                <idno type="volume">020</idno>
                <idno type="issue">1</idno>
                <date when="2026-02-01">1 February 2026</date>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
                    <!--         If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->
                    <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="https://dhq.digitalhumanities.org/taxonomy.xml"
                            >https://dhq.digitalhumanities.org/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
                <taxonomy xml:id="project_keywords">
                    <bibl>DHQ project registry; full list available at <ref
                            target="https://dhq.digitalhumanities.org/projects.xml"
                            >https://dhq.digitalhumanities.org/projects.xml</ref></bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at https://github.com/Digital-Humanities-Quarterly/dhq-journal/wiki/DHQ-Topic-Keywords; these may be supplemented or modified by DHQ editors -->

                    <!-- Enter keywords below preceeded by a "#". Create a new <term> element for each -->
                    <term corresp="#dh"/>
                    <term corresp="#cs"/>
                    <term corresp="#pedagogy"/>

                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->

                    <term>epistemology</term>
                </keywords>
                <keywords scheme="#project_keywords">
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
            <change>The version history for this file can be found on <ref type="gitHist"
                    target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000847/000847.xml"
                    >GitHub</ref>.</change>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->

        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>Learning computing can often be challenging to Digital Humanities (DH) students
                    because its knowledge-production practices and values do not necessarily align
                    with those of humanistic inquiry. Further, the breadth of the technologies used
                    in the field can make it difficult to identify which computational ideas and
                    skills are most essential to teach in DH. This paper argues that, in order to
                    teach students to enact a paradigm of 'humanistic computing' which is reflective
                    of humanistic interests and ways of knowing, it is necessary to centre the
                    epistemological friction that exists between computing and the humanities. It
                    proposes a four-faceted framework to aid in the identification and exploration
                    of that friction, with the intention that it be used by DH educators to
                    structure their computing curriculum – regardless of the technologies or level
                    of abstraction it engages with – and by students or even practitioners to
                    scaffold a critical approach to computational DH work.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>PLACEHOLDER</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>Introduction</head>
                <p>Computing can be understood as the mapping of human conceptions of structure and
                    process to machine interpretable ones. Computing education pioneer Alan Perlis
                    asserted that the aim of any computing course should be to <quote rend="inline">reveal [...] how
                    analysis of some intuitively performed human tasks leads to mechanical
                    algorithms accomplishable by a machine</quote> (Greenberger and others 1962). Learning
                    computing is a question of interrogating this process of translation. </p>
                <p>When translating between natural languages, the best translation is not one which
                    preserves exact vocabulary, but rather one which preserves tone and intention.
                    Rendering such a translation depends on understanding the structures that govern
                    the languages involved, which are a reflection of the way in which the cultures
                    from which they emerged view and makes sense of the world <ptr target="#kramsch_2014"/>. This
                    remains true with respect to the translation of structure and process.</p>
                <p>As C.P Snow argued, academic disciplines <emph>are</emph> cultures <ptr target="#snow_1959"/>. Karin Knorr
                    Cetina (1999) adds a level of precision to Snow’s definition in describing
                    academic fields specifically as <term>epistemic cultures</term>, naming epistemology as the
                    defining factor <ptr target="#cetina_1999"/>. Epistemology is concerned with how we know what
                    we know and why we believe it. Domain cultures are characterised by what types
                    of knowledge and ways of knowing a domain considers important and legitimate. A
                    domain’s <term>epistemic machinery</term> is the specific set of methodologies and tools used
                    for knowledge creation and validation; they are a concrete manifestation of its
                    epistemology.</p>
                <p>Natural language translation becomes more complex as a function of the cultural
                    distance between languages. Translating between French and Italian, for
                    instance, is fairly straightforward. They emerged from cultures with similar
                    values and ways of understanding the world. This is reflected in the similar
                    structure of their languages, making it relatively simple to find equivalencies.
                    Now consider a translation between French and languages such as Diné (Navajo),
                    in which the level of animacy of agents — understood according to a continuum
                    which reflects indigenous beliefs — impacts many structural elements (young,
                    1987); or Yolmo, a Peruvian language with evidentiality, meaning that the way a
                    speaker knows things must be encoded in the grammar <ptr target="#gawne_2014"/>: in these
                    cases, ideas and information are much more likely to be lost or altered in
                    translation. Similarly, computing and humanistic disciplines are more culturally
                    — epistemologically — different than, say, computing and mathematics. This
                    distance is a central challenge of computing in the Digital Humanities (DH).</p>
                <p>DH comprises the <quote rend="inline">the thoughtful use of computing in humanistic inquiry and the
                    thinking through of computing from the perspective of the traditions of the
                        humanities</quote><note> Definition from Geoffrey Rockwell, selected from a list of
                        crowdsourced definitions collected as part of annual <title rend="quotes">Day of DH</title> events:
                            <ref
                            target="https://github.com/hepplerj/whatisdigitalhumanities/blob/master/dayofquotes_full.csv"
                            >https://github.com/hepplerj/whatisdigitalhumanities/blob/master/dayofquotes_full.csv</ref>
                    </note> Some might describe it as trying to use computers to tackle
                    <soCalled>non-computational</soCalled> problems. However, there is nothing inherent to the problems
                    that makes them <soCalled>non-computational</soCalled>; rather, it is that the priorities and
                    interests of humanities inquiry are different from those of the scientific
                    culture from which computing emerged and which its associated tools and
                    procedures correspondingly embody <ptr target="#drucker_2012"/>. Consequently, doing computing
                    in a way that preserves humanistic outlooks and values requires attention to be
                    explicitly paid to points of epistemological friction.</p>
                <p>If the broad aim of computing education is to gain an understanding of the
                    translation process, then DH-specific computing education should centre the
                    interrogation of epistemological friction. In support of this aim, this paper
                    proposes a framework for describing and modelling friction between computing and
                    the humanities. This work is subject to two simplifying assumptions. First,
                    though it is evidently not the case, the humanities are treated as a homogeneous
                    entity, and one which includes disciplines such as history which could also be
                    classified as social sciences. Second, c<term>omputing</term>, in the abstract, is associated
                    with <term>computer science</term> and its corresponding knowledge culture, as this is how it
                    is situated both in the majority of educational institutions as well as, to draw
                    a broad generalisation, in the consciousness of the lay public.</p>
                <p>The framework comprises four facets of epistemological friction:</p>
                <list type="unordered">
                    <item><hi rend="bold">Negative Knowledge</hi>: This facet looks at what
                        disciplines choose to know, can not know, and the errors they make in
                        knowing, exploring how these elements are viewed and incorporated into
                        knowledge production practices.</item>
                    <item><hi rend="bold">Positionality and Context</hi>: A domain’s epistemology is
                        inextricably linked to its conception of the agency of knowledge creator(s)
                        and the influence of perspective. This second facet grapples with the
                        dissonance between detached scientific knowledge and situated humanistic
                        knowledge, and interrogates how computational techniques both resist and can
                        be made compatible with uniqueness, plurality, ambiguity, and nuance.</item>
                    <item><hi rend="bold">Relationship to Tools</hi>: Technology can be understood
                        as impartial and a means of circumventing human bias or as a product and
                        reflection of human culture and values. The third facet explores these
                        different conceptions and how they translate to the use of technology in
                        knowledge production, with particular attention to notions of objectivity
                        and independence, and a tool’s capacity to (intentionally or
                        unintentionally) influence and alter work.</item>
                    <item><hi rend="bold">Hermeneutic Attention</hi>: The final facet considers the
                        tension between a scholarly tradition whose procedures aim to eliminate the
                        need for interpretation and one to which it is central.</item>
                </list>
                <p>The framework is principally intended for use by DH computing instructors and
                    students — to guide the development of curriculum and scaffold critical
                    practice. An instructor looking to introduce databases might choose to structure
                    their lesson around <title rend="italic">Positionality and Context</title> and <title rend="italic">Negative Knowledge</title>, having
                    their students create schemata grounded in specific perspectives and analyse
                    what is highlighted and lost in each. A student applying a text analysis tool
                    might be encouraged, through use of the facets as framework for analysis, to
                    question the extent to which their intentions match up with the structural and
                    procedural logics imposed by the computational medium (<title rend="italic">Relationship to Tools</title>);
                    the use of the facets as a reflection tool when creating a visualisation might
                    also lead a student to more explicitly incorporate a representation of their
                    process of interpretation into the digital product (<title rend="italic">Hermeneutic Attention</title>).</p>
                <p>The facets are not expected to be comprehensive but highlight common sites of
                    friction which may play into a variety of DH projects. More importantly, they
                    illustrate the concept of epistemological friction in the abstract, such that
                    individuals might extend and adapt the framework to their own contexts.</p>
            </div>
            <div>
                <head>Negative Knowledge</head>
                <p>Artist Mimi Onuoha has a filing cabinet. Several, in fact — an oddity in the age
                    of increasingly digital paperwork. But the increasing digital-ness of society is
                    precisely why they exist. Onuoha’s cabinets are full of folders with labels such
                    as <title rend="quotes">People excluded from public housing because of criminal records,</title> <title rend="quotes">White
                    children adopted by POC</title> and <title rend="quotes">Quantifiable effect of corruption in lean
                    economies.</title> Every single one is empty. Onuoha calls her collection the <title rend="italic">Library
                    of Missing Datasets</title>; the folders represent collections of data that do not exist
                    — because those with the resources to create them lack the incentive, because
                    the subjects do not lend themselves to favoured modes of data collection,
                    because the work collection would require outweighs the perceived benefit, or
                    because there are advantages to non-existence.<note>
                        <ref target="https://github.com/MimiOnuoha/missing-datasets"
                            >https://github.com/MimiOnuoha/missing-datasets</ref>
                    </note> Ohunua’s artistic practice turns on what is forgotten when <quote rend="inline">globalised,
                    quantified societies require the fluid messiness of people to be made into
                    legible forms of data</quote>; she asserts that <quote rend="inline">What is missing is still there. This
                    is where my work begins.</quote><note>
                        <ref target="https://mimionuoha.com/about"
                            >https://mimionuoha.com/about</ref> (PLACEHOLDER)</note>
                </p>
                <p>Karin Knorr Cetina coined the term <term>Negative Knowledge</term> to designate not
                    <quote rend="inline">nonknowledge</quote> but rather <quote rend="inline">knowledge of the limits of knowing, of the mistakes
                    we make in trying to know, of the things that interfere with our knowing, of
                    what we are not interested in and do not want to know</quote> <ptr target="#cetina_1999" loc="64"/> We
                    might consider the core question of epistemology to be <quote rend="inline">How and why do we know
                    what we know?</quote> but consideration of the inverse — the photo negative — equally
                    stands to outline the characteristics of a given epistemic approach. Different
                    domains opt, to varying degrees, to disregard specific knowledge so as to enable
                    the creation of strong knowledge claims and ontologies of what is knowable, or
                    to leverage missingness and ambiguity to generate avenues of exploration.
                    Onuoha’s work specifically points to the negative as a notable friction point
                    when it comes to the intersection of computers and humanity.</p>
                <p> </p>
                <div>
                    <head>What we choose not to know</head>
                    <p>A folder labelled <title rend="quotes">Nuanced racial descriptions of American residents — 1890</title>
                        would have fit right in in the <title rend="italic">Library of Missing Datasets</title>. The 1890
                        American census is noteworthy in that it saw the introduction of Herman
                        Hollerith’s tabulating machine — considered a direct ancestor of the modern
                        computer. After being collected by surveyors, data was transferred to punch
                        cards to be tabulated automatically. This new scheme required that all
                        answers be discrete, or made to be so. Race<note> <soCalled>Ethnicity</soCalled> would be the
                            term used today to describe many of the options, but <soCalled>race</soCalled> is the term
                            that was used in the census.</note> was one of the characteristics
                        encoded on the punch cards, and the 1890 census introduced several new
                        options. The questionnaire was the first to include <title rend="quotes">Japanese,</title> (previously
                        there had been no distinction between East-Asian countries) and <title rend="quotes">Indian</title>
                        (first nations). Where, since 1850, only the categories <title rend="quotes">Black</title> and
                        <title rend="quotes">Mulatto</title> (any person of mixed race) had been included, the rise of the
                        eugenics movement drove congress to mandate the introduction of additional
                        categories based on blood quantum. The instructions given to the enumerators
                        explained that an individual having 3/4 or more <soCalled>black blood</soCalled> should be
                        labelled as <soCalled>Black,</soCalled> 3/8-5/8 as <soCalled>Mulatto,</soCalled> 1/4 as <soCalled>Quadroon</soCalled> and 1/8 or less
                        as <soCalled>Octoroon</soCalled> <ptr target="#lee_1993"/>. It is not clear the extent to which the precision
                        of the categories was government dictated or a product of the need for clean
                        delineations, but we can be more sure of two things: First, that the
                        transfer of data to punchcards served to erase the presence of any
                        uncertainty or ambiguity that might have been noted by surveyors; and,
                        second, that the tabulating machine facilitated the imposition and
                        propagation of seemingly strict racial categories — highly contextual
                        political constructions reified by the machine — creating data that would be
                        the foundation of government policies that would continually re-inscribe
                        them.</p>
                    <p><quote rend="inline">What we choose not to know</quote> in computing is principally a question of
                        encoding. The power of computing lies in the ability to manipulate
                        information at a scale and speed beyond human capability. Prerequisite to
                        this is an underlying model which operates at some level of abstraction —
                        necessary to accommodate volume. The process of creating a
                        reduced-complexity rendering of a real-world phenomenon is inherently lossy,
                        a noted limitation to the <quote rend="inline">correctness</quote> of computers <ptr target="#smith_1985"/>. The
                        challenge this presents is exacerbated in a humanities context as <quote rend="inline">the
                        humanities historically have adopted methodologies that are less amenable to
                        abstract generalization than those used by the sciences</quote>; they belong to a
                        tradition of <quote rend="inline">telling detail</quote> as opposed to <quote rend="inline">seeking generalizations</quote>
                        <ptr target="#quamen_2012"/>. Further, the computational form <quote rend="inline">accepts only that which can
                        be told explicitly and precisely</quote> <ptr target="#mccarty_2005"/>, a requirement which
                        drives the choice of what to know. Correspondingly, the knowledge
                        prioritised by the disciplines can be antipodal, the very specificity or
                        variation which would make something an interesting object of study in the
                        humanistic tradition <ptr target="#ginzburg_1979"/> incompatible with normalisation. The
                        strict encoding required by Hollerith’s machine — and, indeed, by modern
                        computers — allowed for faster and larger-scale processing; the 1890 census
                        received 25% more responses than the one in 1880 but was tabulated three and
                        a half times faster. At the same time, it made negative context and
                        ambiguity, and, correspondingly, not all the changes it encouraged, if not
                        imposed, proved productive. The fine grained African-American racial
                        classifications introduced in 1890 were removed before the 1900 census,
                        <quote rend="inline">after census officials judged the data <quote rend="inline">of little value and misleading</quote></quote>
                        <ptr target="#parker_2015" loc="25"/>. Echoes of similar encodings, however, continue to
                        persist — independently of the knowledge they made negative. In their
                        investigation of contemporary datasets used for image recognition, for
                        instance, Scheuerman et al. found that the racial categories used in the
                        Microsoft Celeb database were <title rend="quotes">Caucasian,</title> <title rend="quotes">Mongoloid,</title> and <title rend="quotes">Negroid</title>
                        <ptr target="#scheuerman_2020"/>. They wonder at the contrast of the use of these terms,
                        which are evidently sociohistorical, and the fact that, in modern practice,
                        <quote rend="inline">classifications of race [...] are portrayed as insignificant, indisputable,
                        and apolitical.</quote></p>
                    <p>Faults identified in the use of such large Machine Learning (ML) datasets are
                        often also a product of Negative Knowledge. ML models rely on the ample,
                        working under the assumption that with enough volume comes generalisability.
                        This is amusing when a model ends up relying on human fingers to identify
                        fish because they are principally represented in the training data being
                        held up by triumphant fishermen <ptr target="#brendel-bethge_2019"/>, but distinctly more
                        troubling when the widespread absence of racially diverse facial data
                        <ptr target="#buolamwini-gebru_2018"/> means that proctoring software exhibits significant bias
                        against students with darker skin tones <ptr target="#yoder-himes_2022"/>. danah boyd and
                        Michael Golebiewski’s notion of <soCalled>data voids</soCalled> further illustrates, on a
                        structural level, how computational infrastructure is not designed to
                        account for absence — as demonstrated by the fact that its negative spaces
                        are its weakness. Data voids, as they define them, are places where search
                        queries yielding few results leave space for exploitation by <quote rend="inline">media
                        manipulators with ideological, economic, or political agenda</quote>
                        <ptr target="#golebiewski-boyd_2019" loc="1"/>. These vulnerabilities <quote rend="inline">exist because of an
                        assumption baked into the design of search engines: that for any given
                        query, there exists some relevant content</quote> <ptr target="#golebiewski-boyd_2019" loc="16"/>.
                        With ample data but no true understanding of content; these systems can only
                        ever return what has been said, glossing over the nuance of novel queries.
                        It is a question of volume, never of interpretation. This foundation in
                        generalisation and macro-level aggregation characteristic of computing is
                        not in and of itself problematic or even undesirable, but it can nonetheless
                        create vulnerabilities whose severity is exacerbated by the fact that they
                        may be less expected and accounted for than attacks such as DOS, which are
                        also grounded in the computational facility for — and prioritisation of —
                        scale. Somewhat ironically, the very principles which created the data voids
                        are those that allow them to do damage. As attackers fill in identified gaps
                        with targeted material, the increased volume of content causes it to
                        proliferate widely and quickly. boyd and Golebiewski highlight that there is
                        no fix for data voids; rather, it is a question of awareness — of their
                        existence, of the circumstances of their creation, and of the potential
                        consequences and how they might be mitigated.</p>
                    <p>A Humanistic data practice, as described by Ryan Cordell (2019), should be
                        one that <quote rend="inline">seeks primarily to point our attention to the gaps in our
                        digitization and critical practices</quote> <ptr target="#cordell_2019"/>. Cordell comes from the
                        world of literary history; in this context, in addition to a more pronounced
                        materiality, which stands to reify absence, there is a methodological
                        tradition of archiving which foregrounds questions of provenance and
                        politics of preservation undergirding phenomena such as canon formation. He
                        insists that humanists must not lose sight of these perspectives when it
                        comes to computational work, asserting that a humanistic approach <quote rend="inline">must
                        begin with discussions of data construction rather than data analysis.</quote>
                        Attention to Negative Knowledge in DH relates not only to the critical
                        interrogation of knowledge made negative by the use of computers, but also
                        the ways in which the use of computers can bring to light knowledge made
                        negative by the same human factors Onuoha describes with respect to
                        digitisation — which, for all it presents unique challenges, is also simply
                        another way of creating and preserving information that traces a specific
                        narrative.</p>
                    <p>Cultural critic Walter Benjamin’s conception of <soCalled>aura</soCalled> could be considered a
                        type of Negative Knowledge. He argues that the aura of a piece of art, <quote rend="inline">its
                        presence in time and space, its unique existence at the place where it
                        happens to be,</quote> could never be captured by mechanical reproduction
                        <ptr target="#benjamin_2006" loc="114"/> — mechanical reproduction notably requiring similar
                        decontextualisation and normalisation to computational transformation. Bruno
                        Latour and Adam Lowe, to the contrary, argue that the aura of a work is not
                        eroded but rather extended and enriched by its facsimiles <ptr target="#latour-lowe_2011"/>.
                        The myriad reproductions together form an <term>assemblage</term>: a collection of
                        sources that, though each analysable in isolation, when considered in
                        conjunction with one another <quote rend="inline">have irreducible properties [...] that emerge
                        from the interactions between parts</quote> (delanda 2019, 10) Each component part
                        of the assemblage has its own trajectory — story of how it moved through the
                        world — and, though together they <quote rend="inline">do not form a seamless whole,</quote> their
                        conjunction and interaction is generative, continually adding to the aura.
                        What distinguishes digital texts and, more broadly, digital collections,
                        from their analog equivalents is their <term>massive addressability</term>. They are
                        flexibly interrogable from myriad angles and levels of granularity, which
                        facilitates linking <ptr target="#witmore_2010"/>. Further, there is a flattening of
                        hierarchies born of the equal accessibility of higher- and lower- level
                        components; Lev Manovich (2013) argues, for instance, that a defining
                        characteristics of databases as a New Media form is the way in which they
                        invert the relationship between syntagm and the paradigm <ptr target="#manovich_2013"/>.
                        They <quote rend="inline">materialize</quote> the myriad possibilities (paradigms) represented by an
                        individual row, diminishing the dominance of an isolated linear syntagm. A
                        digital collection, with its emphasis on individual trajectories and
                        linking, could correspondingly be understood as an assemblage; for all that
                        computational tools can make negative some knowledge — just as mechanical
                        reproduction did, especially with respect to uniqueness — the kinds of
                        multi-level and combinatoric interrogations they permit can help expose and
                        explore different sites of missingess.</p>
                    <p>In a name-based search of the <title rend="italic">Papers of Thomas Jefferson Digital Edition</title> — a
                        digital collection of a significant portion of the man’s correspondence —
                        Lauren Klein found no results for James Hemings, despite the central role
                        the enslaved man played in Jefferson’s diplomatic dealings in his capacity
                        as household chef <ptr target="#klein_2013"/>. A global search turns up a single
                        first-name mention, identifiable as Hemings only through a researcher’s
                        metadata note. This archive and its silences are a product of historical
                        power dynamics; the documents preserved may in rare cases be written about
                        marginalised populations, but are even more seldom written by them. Any
                        effort to recover these silences using its documents or others subject to
                        the same politics of creation and conservation (e.g. ledger books, sales
                        receipts) risks <quote rend="inline">reinforcing the damaging notion that African American
                        voices from before emancipation — not just in the archival record, but the
                        voices themselves — are silent, and irretrievably lost</quote> <ptr target="#klein_2013" loc="665"/>.
                        Modern humanities scholars have moved away from trying to fill archival
                        silences to instead explicitly highlighting them in a way that invites
                        critical speculation about the past; the shift is to a conceptual model that
                        <quote rend="inline">works by illumination rather than demystification</quote> and, <quote rend="inline">through
                        explication rather than appropriation or empowerment</quote> <ptr target="#klein_2013" loc="668"/>.
                        Klein demonstrates how digital techniques can be turned to this task. Using
                        named entity recognition and co-reference extraction to derive relationships
                        from letter <emph>contents</emph>, Klein created a <quote rend="inline">deformation</quote> of the archive that
                        disrupts the structures of power it inscribes: <quote rend="inline">Rather than privilege the
                        relationships between letter writers, I sought to dismantle the letter as
                        the unit of the archive, examining each word of content on equal plane</quote>
                        <ptr target="#klein_2013" loc="672"/>. Unlike the links drawn in a visualisation of the
                        correspondence network of the starting archive, <quote rend="inline">the arcs that link
                        Jefferson to the men and women he enslaved</quote> in an interaction network
                        visualisation Klein created from her new version of the data <quote rend="inline">are much more
                        prominent than those that link him to his family members and friends.</quote> This,
                        she asserts, <quote rend="inline">conjures a sense of the dependence, on the part of Jefferson,
                        on the men and women he enslaved, even as it cannot recreate what these
                        people said in their conversations, where they went in order to conduct
                        their transactions, and how they truly lived their everyday lives</quote> (klein,
                        2013, 674)</p>
                    <p>Closing the circle, the massive addressability of digital objects can be used
                        in service of inquiry which explicitly focuses on absence. For Deb
                        Verhoeven, the absence in question is that of women in creative roles in the
                        film industry. Verhoeven and her team use speculative network analysis to
                        re-imagine film industry DEI policies, the premise being that network
                        analysis puts <quote rend="inline">emphasis on relationships instead of individuals</quote> (verhoeven,
                        2020, 1) and is correspondingly suited to targeting systemic inequalities.
                        They do not look purely at what <emph>is</emph> there, but at what <emph>is not</emph> but <emph>could be</emph>,
                        adding nodes and vertices representing hypothetical diverse creatives and
                        their relationships to existing collaboration networks and analysing how
                        these changes impact the <soCalled>health</soCalled> of the network by comparing its overall
                        metrics to those of notably more equitable regional networks.</p>
                    <p>Foundationally humanistic work like Klein and Verhoeven’s is not possible
                        without the high-level manipulation and rapid calculation made possible by
                        computation. Its use, however, requires a specific type of problem modelling
                        — one whose focus on abstraction and generalisation can butt up against the
                        humanities interest in nuance and specificity. This is a tension which must
                        be recognised and navigated in DH work.</p>
                </div>
                <div>
                    <head>What can not be known</head>
                    <p>Every area of knowledge production deals with uncertainty. Potter et al.
                        divide it into two high-level types. The first is <term>Aleatoric</term> uncertainty,
                        which they define as <quote rend="inline">random uncertainty inherent to the problem.</quote> Negative
                        Knowledge is more closely tied to the second type, <term>Epistemic</term> uncertainty,
                        which designates things <quote rend="inline">which could, in principle, be known, but in
                        practice are not</quote>; this type of uncertainty is <quote rend="inline">introduced through deficient
                        measurements, poor models, or missing data</quote> <ptr target="#potter_2012"/>. Disciplines
                        develop their own distinct ways of representing and accounting for
                        uncertainty, which, being derived from domain scenarios, are a product of
                        their own epistemic standpoints. Given the differences in how knowledge
                        creation and validation functions between disciplines, these techniques and
                        standards can not necessarily be ported from one context to another.</p>
                    <p>The scholars of <title rend="italic">Project Cornelia</title> encountered this challenge with respect to
                        their data modelling relationship networks of seventeenth century Flemish
                        painters <ptr target="#brosens_2019"/>. Art history has a record of using <soCalled>best-guess</soCalled>
                        imputation to fill data gaps, with individual scholars proposing possible
                        values based on available related sources. Founded as they are in <quote rend="inline">insights
                        gained from literature, expertise, gut feeling, and <soCalled>common sense,</soCalled></quote> such
                        imputations are <quote rend="inline">often imprecise and divergent</quote> <ptr target="#brosens_2019" loc="117"/>; they
                        do not possess the necessary conditions for replication, which is frequently
                        associated with knowledge validation in computing. The <title rend="italic">Cornelia</title> team assert
                        that <quote rend="inline">while art history’s traditional ad hoc strategies and methods to
                        tackle missing data can certainly be used to a certain extent to address
                        qualitative and micro-historical questions, they fall short of meeting our
                        data-driven research agenda</quote> <ptr target="#brosens_2019" loc="118"/>; they instead turned to
                        statistical imputation. Single imputation, they discovered, even when run on
                        processed data subsets intended to be favourable to the technique through
                        constructed homogeneity, <quote rend="inline">increased rather than reduced the level of
                        uncertainty in the data set</quote> <ptr target="#brosens_2019" loc="119"/>. The difficulty of
                        imputation in cases like <title rend="italic">Project Cornelia</title>’s is that even small variances can
                        drastically alter the meaning of the data. Brosens et al. give the example
                        of the missing birthdate of artist Antoon Salert; depending on whether he
                        entered the network in 1579 or in 1590 — both values generated by variations
                        of single imputation underwritten by different mathematical functions — he
                        might have been considered a contemporary of renown painter Peter Paul
                        Rubens or as belonging to the first generation of painters following in
                        Ruben’s footsteps. The researchers’ next recourse was to attempt the more
                        complex technique of multiple imputation, teaming up with biostatisticians
                        in whose work the technique is commonly used. Multiple imputation, as the
                        name suggests, utilises single imputation across multiple datasets to
                        produce a mean value that is statistically valid. They found that <quote rend="inline">the
                        idiosyncratic nature of the Cornelia data and the high levels of
                        <soCalled>missingness</soCalled></quote> made even the results of this more robust technique varyingly
                        useful.</p>
                    <p>Rebecca Sutton Koeser and Zoe LeBlanc describe this complicating factor of
                        prevalent <soCalled>missingness</soCalled> as a <quote rend="inline">phenomenon that is endemic to humanistic
                        scholarship</quote> <ptr target="#koeser-leblanc_2024"/>. In light of this inescapable characteristic of
                        humanistic contexts, and in the knowledge that, as Enjoli alludes to,
                        <quote rend="inline">archival silences are magnified in digitization projects,</quote> they propose
                        that DH could adopt a practice of speculative reading that moves <quote rend="inline">the
                        margins of our analyses to the center,</quote> framing <quote rend="inline">missingness as something
                        that we can consider as an object of study outright.</quote> The analyses they
                        undertook in this line of inquiry did not look dramatically different from
                        those of the Cornelia team; Koeser and LeBlanc used statistical methods,
                        emerged from scientific and mathematical disciplines to make guesses about
                        lacunae in humanistic datasets — in their specific case, the membership and
                        borrowing records of Paris bookshop and library Shakespeare and Company.
                        They are conscious, in their work, to maintain methodological ties to the
                        humanistic, always contextualising as a means of understanding the efficacy
                        and limits of their work. In their attempt to use forecasting methods
                        typically employed in finance to estimate missing borrowing activity, they
                        identified that the predicted bottom line, which seems high relative to
                        subscriptions, could potentially be explained by the bookstore owner’s
                        propensity — as ascertained through additional documents — for granting but
                        not documenting free subscriptions. Koeser and LeBlanc also experimented
                        with the application of techniques used by ecologists to correct for bias
                        originating from the exclusion of species that are difficult to observe to
                        the problem of missing books, but acknowledge that, for all there have been
                        parallels drawn between cultural and ecological diversity, there are
                        assumptions in the unseen species model that are not analogous to their
                        problem — notably the rate of change in the system and the probable
                        frequency difference between a book, which they point out could quite
                        reasonably be borrowed only once, and a species for which multiple sightings
                        are expected. Another of their experiments, a recommender-system inspired
                        model aiming to predict possible borrowing history during a period of
                        missing records, turned out to be quite accurate, the author’s personal
                        correspondence confirming a number of its suggestions for Ernest
                        Hemingway.</p>
                    <p>Even when, as is the case with the Hemingway predictions, their results were
                        proven correct, Koeser and LeBlanc emphasise that correctness, or even the
                        results, are not the point. In the practice of speculative reading, the
                        models are intended to <quote rend="inline">provide a means to experiment in knowledge
                        creation.</quote> They propose that missingness should not be considered merely a
                        <quote rend="inline">technical category for measurement</quote> but rather positioned as <quote rend="inline">a conceptual
                        framework</quote> for considering data. They align themselves with the Uncertain
                        Archive Project’s aim of <quote rend="inline">carv[ing] out an epistemic space for modes of
                        inquiry that are motivated less by providing answers than by posing
                        questions, exploring uncertainties, and offering material and speculative
                        approaches</quote> <ptr target="#agostinho_2019" loc="432"/>. Under this paradigm, missingness is not
                        something bothersome to be accounted for, but something to be used
                        generatively.</p>
                </div>
                <div>
                    <head>Errors in knowing</head>
                    <p>Scientific knowledge production and validation are strongly rooted in a logic
                        of trial and error. There is an expectation of potential sources of error
                        being reported and, through replication, discounted. Some argue that, by
                        virtue of its disciplinary heritage, replication ought to be incorporated
                        into the scholarship of DH <ptr target="#tiefenbach-keller_2024"/>; however, as with
                        uncertainty, the portability of epistemic practices merits
                        consideration.</p>
                    <p>In his 2022 study intended as a replication of Langer et al.’s finding that
                        <quote rend="inline">the frequency and diversity of biological taxa [in fiction] have been
                        declining steadily since the first half of the nineteenth century,</quote> DH
                        scholar Andrew Piper’s primary conclusion was that he disagreed with the way
                        in which the authors of the initial paper defined fiction in the first
                        place <ptr target="#piper_2022"/>. One part of Piper’s study comprised what we might call
                        a <soCalled>traditional</soCalled> replication — testing the initial methods on the same data.
                        In this particular case, however, though also drawn from Project Gutenberg,
                        Piper’s data differed somewhat from Langer et al.’s. Though he reports
                        applying the filtering criteria specified in the original paper, Piper’s
                        search yielded only 13,075 texts, as compared with the original 15,238. In
                        running his approximation of their dictionary-based biological term instance
                        identification model on this corpus, Piper found routine discrepancies of
                        ten to fifteen years with regard to inflection points and, when he ran the
                        experiment on a subset of the data constructed using a different means of
                        identifying <soCalled>fiction,</soCalled> he observed the opposite of Langer’s reported trend.
                        Piper also undertook a conceptual replication <ptr target="#tiefenbach-keller_2024"/>,
                        introducing an additional ML-based model and two additional datasets to
                        investigate the result in question. These produced further contradictory
                        trends. For Piper, these results cannot be understood as disproving the
                        initial result; he asserts that they are, rather, a starting point of
                        alternative interpretations of the subject matter and, more abstractly, a
                        case to drive discussion of methodological questions in cultural
                        analytics.</p>
                    <p>In her historical examination of the notion of learning from errors in
                        science, Jutta Schickore describes two distinct types of error-related
                        practices: arguments from error and enquiries into error. The former
                        consists of <quote rend="inline">those methodological arguments that the practitioners put
                        forward to indicate that their research took the potential sources of error
                        into account</quote> <ptr target="#schickore_2005" loc="554"/>; conventions such as error bars and the
                        practice of direct replication fall into this category. Piper’s exploration,
                        in particular the conceptual replication, aligns more closely with the
                        latter type, where <quote rend="inline">errors are not merely uncovered to be cast aside</quote> but,
                        rather, <quote rend="inline">become the object of systematic exploration and discourse</quote>
                        <ptr target="#schickore_2005" loc="555"/>. This outlook is echoed by DH literary scholars Hoyt
                        Long and Richard So with respect to their work using machine leaning to
                        explore the evolution of the Haiku as a distinct poetic form <ptr target="#long-so_2016"/>.
                        They explicitly leveraged errors, in the understanding that they occurred at
                        points of class overlap, asserting that <quote rend="inline">what the machine learning
                        literature treats as misclassifications [...] we treat as opportunities for
                        interpretation</quote> <ptr target="#long-so_2016" loc="261"/>.</p>
                    <p>Negative knowledge, when it comes to computing, is often a product of
                        formalisation — whether it be the strict encoding of datafication or the
                        normalisation of messy and lacunar data required by model building. An
                        epistemologically-aware approach allows for this formalisation to move
                        beyond being <quote rend="inline">simply a necessary and straightforward condition for
                        computation</quote> to become <quote rend="inline">a rich and productive concept to think about
                        computation, the humanities, and their encounters</quote> <ptr target="#van-zundert_2012" loc="280"/>.
                        Negative knowledge, can be a tool to generate discourse <ptr target="#cetina_1999"/> — an
                        awareness of what is not there providing a new lens for investigating what
                        is. In their theorization of Humanistic HCI, Shaowen and Jeffrey Bardzell,
                        propose speculative <quote rend="inline">design futuring</quote> as an emblematic practice; they
                        specifically tie this exercise of <quote rend="inline">interpretively explor[ing] alternate
                        worlds, to discover the possible and the preferable in them, and to
                        construct both pathways and the collective will to pursue them</quote> (bardzell,
                        2016, 26) to the notion of emancipation, which they assert is a fundamental
                        aim of humanistic knowledge production. For all that computational tools and
                        methodologies can be the instigators of oppression, they can — with an
                        awareness of Negative Knowledge — equally be used to foster and facilitate
                        imagining otherwise.</p>
                </div>
            </div>
            <div>
                <head>Positionality and Context</head>
                <p>On July 11th 2021, Maxime Bernier, the leader of the People’s Party of Canada tweeted<note>
                        <ref target="https://x.com/MaximeBernier/status/1414302864672006146"
                            >https://x.com/MaximeBernier/status/1414302864672006146</ref>
                    </note>:</p>
                <p>There are no feminist math, indigenous math, black math, or any other kind of
                    math.</p>
                <p>There is only mathematics. </p>
                <p>Only physics. </p>
                <p>Only astronomy. </p>
                <p>He was quickly met with a rebuttal from mathematician, educator, and researcher
                    Michole Enjoli, in a thread<note>
                        <ref target="https://x.com/mathchole/status/1414653170232532999"
                            >https://x.com/mathchole/status/1414653170232532999</ref>
                    </note> beginning:</p>
                <p>Whew. I've been waiting for this one. </p>
                <p>*cracks knuckles*</p>
                <p>WHERE and WHO we get mathematical knowledge from, shapes the content of the math
                    being taught.</p>
                <p>Enjoli asserts that <quote rend="inline">The belief system of a society motivates which math is
                    taught,</quote> pointing to the work of Education scholar Gloria Ladson-Billings who
                    argues that the values being promoted in the U.S. system echo those of the
                    dominant White culture which <quote rend="inline">demands efficiency, consensus, abstraction, and
                    rationality</quote> <ptr target="#ladson-billings_1997" loc="699"/>. She further highlights the work of
                    mathematics education scholar Danny Martin (2013), who argues that the kinds of
                    mathematics emphasised by the current U.S. system are a reflection of the value
                    it places on economics: <quote rend="inline">…If we lived in a society with a different set of
                    values,</quote> concludes Enjoli, <quote rend="inline">then our mathematics and the education system along
                    with it would again look different and be used differently.</quote></p>
                <p>The reaction to Enjoli’s response testified to the fact that it was a complete
                    novelty to many who, though they disagree with Bernier’s far-right politics,
                    initially found themselves nodding along in recognition of his statement.</p>
                <p>Knowledge validation in science rests on uniformity and replicability. This
                    implies a certain separation of the methods and procedures from the people who
                    perform them and the circumstances in which they are undertaken. Though there is
                    interpretation of results that pushes knowledge forward, the knowledge is <soCalled>held</soCalled>
                    in the resulting detached axioms. This severing of knowledge from creator and
                    context is what makes it easier for a specific cultural construction of Math to
                    be viewed as <soCalled>just Math.</soCalled></p>
                <p>Humanistic knowledge production, to the contrary, is not looking to achieve
                    consensus in findings, but to create a dialogue between them. This entails a
                    focus on individual nuance and specificity, which is tied up in context of
                    creation. Further, critical interpretation is the core of its knowledge
                    creation; correspondingly, validity is born of acknowledging and incorporating
                    the influence of the creator and their positionality into its epistemic
                    practices rather than designing them to remove it. As DH scholar Harvey Quamen
                    describes, <quote rend="inline">charges of anthropocentrism or ethnocentrism can often be read,
                    especially by scientists, as a need for greater objectivity, but in humanistic
                    circles they are more often assertions that the anthropos and the ethnos remain
                    central to scholarly inquiry</quote>(quamen 2012, 10). These contrasting approaches to
                    positionality and context are an inevitable source of friction in DH work.</p>
                <div>
                    <head>Nowhere and Somewhere</head>
                    <p>The lab is a central space of scientific knowledge production. Objects
                        studied in the lab are necessarily removed from their spatial and temporal
                        contexts, and it is possible to manipulate the usual restrictions that those
                        impose. For example, scientists might simulate conditions to accelerate the
                        growth cycles of bacteria or alter the circadian rhythms of mice. It is also
                        permissible and expected to <quote rend="inline">substitute transformed and partial versions</quote>
                        <ptr target="#cetina_1999" loc="27"/> of an object for the whole. Though computing could be
                        said to lack the same flavour of lab culture, the computer itself stands to
                        act as a lab and encourage a parallel detachment; data, as seen, is a
                        partial and decontextualised representation, and processes such as
                        simulations similarly perform artificial and normalised manipulations. Lab
                        procedures are stringently codified in an effort to minimise space for
                        individual influence. As historian of science Theodore Porter describes, [a]
                        highly disciplined discourse helps to produce knowledge independent of the
                        particular people who make it” <ptr target="#porter_1995" loc="ix"/>. Computing, again,
                        emphasises these properties. It enforces a strict encoding and affords
                        quantification, which is <quote rend="inline">a technology of distance</quote> (porter 1995, xxi).
                        Further, in facilitating high-scale operation, it minimises the impact of
                        individual deviation. Uniformity is rigorous; human bodies and minds are not
                        uniform. They become more so, however, when viewed from a distance, which is
                        to say their specificity, as either the observer or the observed, is
                        abstracted away.</p>
                    <p>These kinds of <soCalled>realist</soCalled> models of knowledge, asserts Johanna Drucker,
                        <quote rend="inline">depend above all upon an idea that phenomena are observer-independent.</quote>The
                        aim is to take a <quote rend="inline">view from nowhere,</quote> the implication being that the act of
                        observation — its context and the particular standpoint of the observer —
                        should not have an impact on the phenomenon. This detachment is constructed
                        through standards and procedures. However, <quote rend="inline">[r]endering observation [...] as
                        if it were the same as the phenomena observed collapses the critical
                        distance between the phenomenal world and its interpretation,</quote> which
                        <quote rend="inline">und[oes] the basis of interpretation on which humanistic knowledge
                        production is based</quote> <ptr target="#drucker_2011"/>. Feminist standpoint theory contends
                        that all knowledge is inherently positional, dependent on individual
                        identities and structural power dynamics. As a counterpoint to scientific
                        objectivity, Donna Harraway offers Feminist objectivity, which <quote rend="inline">means quite
                        simply situated knowledges</quote> <ptr target="#haraway_1988" loc="581"/>. This outlook is
                        emphatically not an effort to account for every possible perspective — in
                        the way that its counterpart looks to eliminate them all — but a recognition
                        of the existence of the knowledge creator and the incorporation of their
                        positionality and context into praxis. This notion of a view from somewhere
                        is congruent with interpretive — and, correspondingly, humanistic —
                        knowledge production practices.</p>
                    <p>Roopika Risam’s comparative analysis of two visualisations of global
                        migration illustrates how these contrasting understandings can manifest in
                        digital artifacts <ptr target="#risam_2019"/>. The first, The Flow Towards Europe (FTE),
                        was produced by a Finnish startup using a dataset from the United Nations
                        Refugee Agency. It depicts migration towards Europe from African, Middle
                        Eastern, and Asian countries with waves of dots moving in straight lines
                        from source to destination. It is built around a map projection centred on
                        the global North, with a black and turquoise colour scheme that serves to
                        clearly highlight country borders; its authority is drawn from
                        quantification at scale and the use of established conventions. Crossing the
                        Mediterranean Sea by Boat (CMSB), her second example, conversely looks to
                        convince through a focus on individual experience. CMSB is a collection of
                        story maps built around interviews with over 250 migrants. Representations
                        of migration are accompanied by motivations (e.g. <title rend="quotes">escaping sexual
                        violence,</title> <title rend="quotes">escaping civil war</title>) and navigation between migrant stories is
                        based on familial and relational bonds (e.g. <title rend="quotes">brother,</title> <title rend="quotes">young mother</title>),
                        <quote rend="inline">offer[ing] humanizing representations of migrants by placing them within
                        social relations</quote> <ptr target="#risam_2019" loc="573"/>.</p>
                    <p>Each story map includes stops made along the way and integrates the voices of
                        migrants through quotations describing their journeys. Risam highlights that
                        the visualisation does not shy away from assuming a specific position;
                        alongside the visualisations, it directly puts questions to the user such as
                        <quote rend="inline">Unauthorised journeys through Europe are often frayed with dangers, and
                        many have died trying to reach their destinations. Does this change your
                        views on the need to open legal ways for people to travel?</quote> and <quote rend="inline">On arrival
                        to their destination, many wait long periods during the asylum process while
                        trying to learn about the rules and regulations in a new country. Can you
                        imagine what it feels like to be waiting in this situation? </quote>The crux of her
                        argument, however, is that the choices underlying FTE relating to
                        positionality and context — of the researchers and of the subjects — are no
                        less of an assumed stance than those more plainly legible in CMSB. She
                        argues that the <quote rend="inline">[u]se of a political map absent of topographical features
                        emphasizes national borders and depicts cohesion of the nation state
                        (however fictive)</quote> <ptr target="#risam_2019" loc="572"/>. The choice of the map also creates a
                        focus on the impact of migration on the destinations — some origin points
                        are even out of frame. Risam further points to the fact that the movement of
                        the dots in <quote rend="inline">unimpeded waves</quote> serves to <quote rend="inline">emphasiz[e] the magnitude of
                        migration</quote> but does not reflect how migration <quote rend="inline">take[s] many more forms than
                        travel from point A to point B [and] does not occur uniformly over time in
                        the flows depicted in the visualisation</quote> <ptr target="#risam_2019" loc="572–73"/>. In order to
                        produce the uniformity needed to apply the chosen methods, the team behind
                        FTE created departure dates not included in the original dataset and
                        calculated using arrival dates and average human walking speed. They also
                        chose to aggregate the migrants; each dot actually represents a group of 25
                        individuals who were not necessarily travelling together. This decision was
                        notably <quote rend="inline">determined by the limitations of the visualization platform and the
                        creators’ sense that if each migrant were represented by a dot, the
                        visualization would be too crowded and create performance issues</quote> (risam,
                        2019, 572).</p>
                    <p>The choices made by the FTE team are a reflection of an epistemic tradition
                        which prioritises generalisability and associates validity with detachment.
                        Computational mechanisms and conventions are instantiations of this
                        position, and therefore in tension with the values of humanistic inquiry,
                        for all that they can be used in service of it when the humanities, too, are
                        interested in large numbers and high-level patterns. Historian of Slavery
                        Jessica Marie Johnson, for instance, argues that while digital resources
                        such as the Trans-Atlantic Slave Database <quote rend="inline">immediately reshaped debates
                        about numbers of women and children exported from the continent,</quote> they
                        <quote rend="inline">could not function as a window into the everyday lives of Africans, who
                        remained faceless, anonymous, disembodied</quote> <ptr target="#johnson_2018" loc="64"/>. The database
                        reifies the logics of the documents used to construct it — slave ship
                        manifests, colonial censuses, and other European bureaucratic documents —
                        which were the very mechanisms that permitted the slave trade and, as such,
                        reflect colonizer attitudes and values antithetical to those of the
                        researchers looking to use it as a scholarly resource. </p>
                    <p>Cate Alexander (2020) encountered a similar mismatch of epistemic priorities
                        in the context of her work on the digital archiving of the Federal Writers’
                        Project Slave Narratives. She highlights how choosing to tag the interview
                        files exclusively by the state in which they were conducted and the surname
                        of the interviewee serves to situate them on a macro-level within the
                        Library of Congress collections but does not at all facilitate more
                        micro-level explorations. These recitations, which give voice to former
                        enslaved persons, have the potential to act as a counterpoint to the
                        coloniser-generated data described by Johnson. However, the fact that even
                        non-unique details of their lives (e.g. family separation, emancipation) and
                        especially the violence committed against them (e.g. corporal punishment,
                        sexual assault) remain untagged severely limits the scope and possibility of
                        digital discoverability and exploitation along these axes. Alexander
                        suggests that the archiving choices are a product of computational
                        conventions, which favour normalisation for abstraction, and provides eight
                        recommendations for the creation of a digital archiving system which
                        <quote rend="inline">consciously choos[es] to center the humanity of [the narratives]</quote>
                        <ptr target="#alexander_2020" loc="58"/>; these include in-depth content tagging anchored in
                        prosopography and the deliberate inclusion of contextual information —
                        additional documents and scholarship relating to cultural and historical
                        context but also information relating to how the data was created (the
                        conduct of the interviews and the project itself). They follow the ethos of
                        <soCalled>contextual digitisation</soCalled> which aims to <quote rend="inline">provide an apparatus by which users
                        can understand the provenance, history, curation and technologies that have
                        served to produce the new intellectual output</quote> <ptr target="#gooding_2023" loc="597"/>.
                        Alexander’s recommendations represent a kind of useful mid-way point for DH
                        work. </p>
                    <p>The techniques employed by the Library of Congress and equally by the FTE
                        team are, in part, a means of adapting to scale; they are standard
                        convention in many areas of computing precisely because the power of
                        computation lies in its facility with scale. While a thought-provoking use
                        of affect, the deep context and relationality underpinning the CMSB approach
                        would be infeasible and, arguably, less impactful, using comparable number
                        of records. Alexander’s recommendations strike a balance; they emphasise the
                        inclusion of context and specificity, while also accounting for the
                        higher-level more abstract structures — such as tagging system — whose
                        analytical affordances are the reason that humanists were drawn to the use
                        of computation in the first place.</p>
                </div>
                <div>
                    <head>Reshaping Convention</head>
                    <p>An essential part of the work of DH, as in the case of Alexander’s
                        speculative guidelines, is to (re)imagine and adapt conceptions and
                        practices born of one of its constituent disciplines’ epistemic traditions
                        to align with the other. Drucker takes this back to the most base level with
                        her reframing of <soCalled>data</soCalled> to <soCalled>capta.</soCalled> <soCalled>Data,</soCalled> with its Latin root of <soCalled>given,</soCalled>
                        implies passivity; data is a <quote rend="inline">natural representation of pre-existing fact</quote>
                        <soCalled>Capta,</soCalled> signifying <soCalled>taken,</soCalled> conveys the <soCalled>constructedness</soCalled> of a
                        representation, <quote rend="inline">acknowledg[ing] the situated, partial, and constitutive
                        character of knowledge production</quote> (drucker ,2011). </p>
                    <p>The notion of <soCalled>capta</soCalled> is at the heart of sociologist Maggie Walter’s work on
                        data-driven government policy and Australian indigenous communities. Walter
                        describes the data about indigenous communities resulting from the
                        prevailing paradigm as BADDR — Blaming, Aggregate, Decontextualised,
                        Deficit, and Restrictive <ptr target="#walter_2018"/>. A theoretically neutral and
                        realistically dominant standpoint engenders <soCalled>othering,</soCalled> measuring in terms
                        of deviation from a norm. Aggregation creates and reifies an imaginary
                        homogeneity. Removing context devalues lived experience. Metrics designed in
                        external contexts which do not align with a population’s worldviews, values,
                        and priorities will inevitably find them wanting. <quote rend="inline">[S]tatistics that
                        perpetually describe <soCalled>the problem</soCalled> are themselves part of the problem,
                        reproducing on a seemingly endless repeat the trope of Indigenous deficit,</quote>
                        writes Walter; these patterns are elevated and perpetuated by the insistence
                        that <quote rend="inline">data are just data [...] immutable and indisputable.</quote> As a
                        counterpoint to typical quantitative models, Walter introduces the idea of
                        nayri kati — <soCalled>good numbers,</soCalled> in the palawa Tasmanian Aboriginal
                        language <ptr target="#walter-andersen_2013"/>. Broadly, nayri kati refer to metrics
                        emerging from standpoint-informed methodologies. As one example, she
                        contrasts standard public health data with that collected by the Albuquerque
                        Area Southwest Tribal Epidemiology Centre; the latter’s metrics account for
                        the way in which </p>
                    <quote rend="block">or Indigenous peoples, health is not just about maintaining physical
                        health, such as through exercise or taking medications to prevent and manage
                        diseases, it is connected to their ways of being and doing that are unique
                        to their identity and understanding of the world [...] for example, a way of
                        life that supports wellness and spirituality and ceremonies, traditional
                        medicine, heritage languages, family and community connectedness,
                        agricultural way of life, and physical wellness” (walter-suina, 2019,
                        238)</quote>
                    <p>She notes that the introduction of new metrics is not the most important
                        aspect of this kind of intervention, emphasising rather the importance of
                        making visible the contrast and shift in methodology. Walter associates
                        western quantitative methodologies, especially scientific ones, with a lack
                        of <quote rend="inline">epistemological reflexivity</quote> stemming from the choice not to engage with
                        standpoint; <quote rend="inline">they have been the norm, and <soCalled>normal</soCalled> gets conflated with
                        <soCalled>natural.</soCalled> These methodologies contain within them the basic tenets of
                        societally dominant ways of knowing, but like a fish in water, most
                        researchers don’t subject these ways of knowing to scrutiny</quote>
                        <ptr target="#walter-andersen_2013" loc="43"/>. Computation does rely on the quantitative,
                        which may appear less amenable to situatedness than the qualitative, but
                        Walter demonstrates how quantitative work can be made compatible with
                        positionality.</p>
                    <p>Drucker herself illustrates the data-capta shift — focusing less on
                        collective values and more on questions of perspective and embodied
                        experience — through a series of experimental <soCalled>capta</soCalled> visualisations. In one
                        example she augments a traditional bar chart representing the publication of
                        nineteenth century novels with layered horizontal artifacts so as to
                        communicate the context of their creation — the whole timeline of steps
                        (e.g. writing, acquisition, editing, pre-press work) leading up to the event
                        of publication that had previously been the sole focus of the chart. In
                        another series of visualisations, she explores different ways of warping
                        linear time scales to reflect how time is perceived by the subject whose
                        data is being represented. The latter plays into some of the principles of
                        the emergent movement of affective data visualisation — or visceralisation —
                        which attempts to disrupt and extend the conventions of scientific computing
                        by challenging the binary and hierarchy between reason and emotion,
                        positioning the latter as a component of, rather than a risk to, valid
                        knowledge production. Lauren Klein and Catherine D’Ignazio give the example
                        of two visualisations of United States gun-related violence statistics. The
                        more <soCalled>typical</soCalled> example is a bar chart of the aggregated number of active
                        shooter events per year <ptr target="#dignazio-klein_2020"/>. The affective approach is an
                        animated chart where arcs are drawn along a time-marked x-axis, each
                        corresponding to a single gun-related fatality. Each arc is divided into two
                        colour coded segments: An initial orange section encodes the age to which a
                        victim of gun-violence lived, and the rest of the arc, drawn in white,
                        stretches to the point of their projected life expectancy, according to
                        demographic-specific health data. As each line is drawn, a name and age are
                        shown, and a global counter displays the increasing total number of <title rend="quotes">stolen
                        years.</title> Like FTE, this visualisation manufactures attributes not in the
                        original dataset (projected life expectancies); however, it does so in a
                        much more contextual way — using individualised demographic data — and also
                        makes it clear that this data is not sure. Further, it deliberately embraces
                        what FTE sought to avoid through the clustering of migrants into groups of
                        25: the illegible <soCalled>hairball</soCalled> created by including a visual representation of
                        each person. Visualisation is argument; rather than using detachment and
                        aggregation in the aim of persuading through logos, this affective
                        visualisation leverages pathos in the form of the emotional potential of
                        overwhelming individuality.</p>
                </div>
                <div>
                    <head>Humanistically Data-Driven?</head>
                    <p>The modern data economy, asserts Kate Crawford, is premised on <quote rend="inline">the dual
                        operations of abstraction and extraction</quote> <ptr target="#crawford_2021" loc="217"/>. Data itself
                        is an extraction of the perceived most salient parts from a phenomenon, and
                        the aim of this processing is to create representations which all conform to
                        an abstract model. The labour of the creators is similarly extracted, while
                        their presence is abstracted away. As an example, she points to data
                        training sets which are at the foundation of our increasingly AI-driven
                        world. Echoing aspects of the BADDR data issues Walter identified, Crawford
                        highlights the fact that one of the most well established datasets for
                        facial recognition (MEDS) is composed of prison mugshots collected over a
                        subject’s multiple arrests, used without their consent and often featuring
                        severe injuries, and that a similarly well-regarded dataset for emotion
                        classification (CK+) is principally composed of images of feigned emotion.
                        Yet, when ingested into a dataset, <quote rend="inline">the personal, social, and political
                        meanings</quote> of images representing individual people and the power exerted by
                        societal structures <quote rend="inline">are all imagined to be neutralized</quote> (crawford ,2021,
                        93). When integrated into a dataset, argues Crawford, there is a shift <quote rend="inline">from
                        image to infrastructure, where the meaning and care that might be given to
                        an individual person, or the context behind a scene is presumed to be erased
                        at the moment it becomes part of an aggregate mass that will drive a broader
                        system</quote> <ptr target="#crawford_2021" loc="93"/></p>
                    <p>Mechanisms such as Ghebru et al’s <title rend="quotes">Datasheets for Datasets</title> <ptr target="#gebru_2021"/>
                        make preliminary strides toward attending to the issues raised by Crawford.
                        This exercise, which emerged out of the critical computing movement, asks
                        developers to reflect on questions regarding a dataset’s:</p>
                    <list type="ordered">
                        <item>Motivation: Who funded and undertook the creation of the dataset, and
                            what was their intended purpose?</item>
                        <item>Composition and Processing: What does each instance represent, how is
                            it labelled or broken down, what is the coverage of the dataset as a
                            whole, and where might there be gaps?</item>
                        <item>Uses, Maintenance, and Distribution: How will the data be used and how
                            could it be used poorly; how will data sharing and updating be
                            controlled; and who will be responsible for these actions?</item>
                    </list>
                    <p>Mitchell et. al’s <title rend="quotes">Model Cards</title> look to fulfil a similar purpose with respect
                        to the models built from and for these datasets. In recognition of the fact
                        that these models are often used in <quote rend="inline">high-impact tasks in areas such as law
                        enforcement, medicine, education, and employment</quote> <ptr target="#mitchell_2019" loc="220"/>, the
                        authors propose that models be published with an accompanying model card,
                        which details its intended use (and contexts where it should not be used);
                        provide benchmarked evaluations across conditions involving different groups
                        (cultural, demographic, phenotypic) of subjects, including intersectional
                        groups; describe the agents and procedures involved in its creation; and
                        provide ethical considerations.</p>
                    <p>Both datasheets and model cards are working toward transparency in the aim of
                        democratising of Machine Learning (ML) and, specifically, Artificial
                        Intelligence (AI) technologies. However, whatever checks and balances these
                        provide, there remains the challenge in DH that these technologies embody
                        epistemic principles which, in many cases, run counter to those of
                        humanistic inquiry; they are, as Crawford describes, frequently fed
                        decontextualised data and are operating under the premise that consensus is
                        success, leaving no room for positionality.</p>
                    <p>For Lucy Havens, there needs to be <quote rend="inline">a shift away from removing social biases
                        and towards acknowledging them,</quote> so as to <quote rend="inline">creat[e] data and models that
                        surface the uncertainty and multiplicity characteristic of human societies</quote>
                        <ptr target="#havens_2024" loc="iv, emphasis mine"/>. As a step toward the creation of a
                        <soCalled>Bias-Aware</soCalled> Methodology for the use of AI/ML in DH, she highlights four
                        major priorities of AI/ML and describes corresponding humanistic
                        recalibrations — all of which relate back to questions of context and
                        positionality. The first priority Havens highlights is Quantity. AI/ML
                        practice and, in particular, performance evaluation has a noted focus on the
                        data set size and volume of model parameters, which <quote rend="inline">communicates an
                        underlying assumption that bigger is better</quote> <ptr target="#havens_2024" loc="24"/>. The
                        corresponding recalibration is Quality; Havens suggests using qualitative
                        methods to critically analyse dataset and model composition. To counter the
                        priority of Efficiency, which often manifests as the creation of
                        <soCalled>good-enough</soCalled> metrics which are over-simplified, vaguely defined, or <quote rend="inline">reduce
                        subjective tasks to a single interpretation</quote> <ptr target="#havens_2024" loc="25"/>, she offers
                        Accuracy — not in the sense of the success of a classification model on a
                        dataset contrived to be amenable to its logics, but regarding the creation
                        of metrics which are meaningful in real world contexts. These importantly
                        can not remain fixed but must be continually adapted as society evolves. The
                        third priority is Convenience, referring to the way in which AI/ML culture
                        will reach for easily-obtained data, often created through online crowd work
                        or — as in the case of MEDS — by exploiting marginalised populations. The
                        corresponding recalibration is Representativeness — a focus on the curation
                        of datasets which are representative of the population to which the models
                        will be applied, so as to avoid the widespread existing issue of models
                        which demonstrate obvious bias and prejudice. The lack of consideration of
                        representativeness is in part a consequence of the final priority: Universal
                        Thinking. This is a product of a valuation of the generalisable and neutral.
                        Situated Thinking is the evident humanistic counter.</p>
                    <p>Maxime Bernier insisted that there is no such thing as <soCalled>Indigenous math</soCalled>;
                        from this we might infer that he would similarly not believe in <soCalled>Indigenous
                        computing.</soCalled> That, however, is precisely what nehiyaw-Métis artist and
                        programmer Jon Corbett has worked to develop. Corbett describes how, in his
                        journey learning computing, he continually fought to avoid <quote rend="inline">conforming to
                        the design and language of the computer,</quote> instead <quote rend="inline">breaking and rewriting as
                        many computational rules as possible</quote> (e.g. altering to be less efficient,
                        but more reflective of the processes he was modelling), to instead have <quote rend="inline">the
                        computer understand how I wanted to [process/create information]</quote> (corbett,
                        2023, 91–92) — in a way that was reflective of an Indigenous worldview. This
                        experience culminated in his creation of Cree#, a programming language that
                        is Indigenous in its structure, symbology, syntax, and purpose. It is built
                        around the metaphor of <soCalled>code as story</soCalled> and written entirely in Aboriginal
                        Syllabics. The declaration of a function translates to this is a collected
                        story called [function name], and the keywords for looping and conditionals
                        are drawn for natural metaphors. The act of <soCalled>smudging,</soCalled> which is an
                        indigenous cleansing ritual, has been computationally transcribed as a
                        command which prepares the program for running by clearing the screen and
                        any cached memory. An intended possible output of a Cree# program is a
                        rendering of beaded artwork that is a traditional means of encoding and
                        transmitting stories.</p>
                    <p>Posing a challenge of sorts to the DH community, Drucker questions whether it
                        is possible to undertake meaningful humanistic scholarship using tools that
                        <quote rend="inline">preclude humanistic methods from their operations because of the very
                        assumptions on which they are designed: that objects of knowledge can be
                        understood as self-identical, self-evident, ahistorical, and autonomous</quote>
                        <ptr target="#drucker_2012" loc="86"/>. No matter where one stands on this question, the
                        reality is that, despite these mismatches — which are specifically a
                        reflection of differing epistemological stances relating to context and
                        positionality — digital humanists are actively choosing to use computational
                        tools and methods because they are interested in the analytic possibilities
                        and perspectives these represent. Doing this <soCalled>humanistically,</soCalled> whether it is
                        a question, as Corbett has done, of reimagining tools to align with
                        humanistic intentions or, more simply (and accessibly), of accounting for
                        and adapting to the epistemological underpinnings of existing ones, an
                        awareness of this friction point is essential to the knowledge production
                        work of DH.</p>
                </div>
            </div>
            <div>
                <head>Relationship to Tools</head>
                <p>Born scarcely twelve years apart — in 1911 and 1923 respectively — Marshall
                    McLuhan and Peter Elias both lived through the passage of computers from
                    room-sized systems limited to small academic and governmental spheres toward
                    much more ubiquitous and accessible technology. Being witness to this
                    development seemingly gave them foresight to predict where computing was headed:
                    toward independence and invisibility.</p>
                <p>Elias, a computer scientist and information theory pioneer, believed that the
                    ultimate goal for computing was to become <soCalled>frictionless.</soCalled> He imagined a future
                    where the machine would be able to carry out complex processes with minimal
                    instruction and intervention, insisting that <quote rend="inline">if it continues to demand as much
                    effort to learn how to speak to machines as it costs us to teach students a
                    course for a couple of semesters, then we have failed</quote> (greenberger,1962, 203).
                    The interest, for Elias, lay not in dialoguing with the computer, but in the
                    development of tools which could <soCalled>understand</soCalled> problems and solve them
                    independently. Implicit to this proposal is the equivalence — at least
                    perceived, if not actual — of human and machine logics.</p>
                <p>McLuhan foresaw a similar convergence or at least confusion of logics, not as a
                    technologist but as a media theorist. He placed the computer in a long history
                    of technologies — media — that have altered the way in which we view and
                    interact with the world. In a pattern that has occurred again and again, when it
                    first emerges, a new technology is strange and new, and we are correspondingly
                    highly aware of the way in which it is mediating our experience; as time passes,
                    the technology and the processes surrounding its use become more formalised,
                    normalised, then internalised such that we are less likely to notice its
                    presence. Unlike Elias, McLuhan was wary of this eventuality. McLuhan describes
                    media in terms of extensions and amputations of the body. A shovel, for
                    instance, is an extension of the arm, enhancing its capability; similarly, a
                    microscope is an extension of the eye <ptr target="#mcluhan_1994"/>. When pushed to an
                    extreme, these extensions become replacements for specific capabilities and
                    processes. Media amputate what they render obsolete, altering our behaviour; the
                    telephone, for instance, extends the voice but amputates skills in penmanship
                    and composition. He had a particular anxiety around digital media because he
                    understood them as extensions of the nervous system and, more importantly, the
                    brain. McLuhan worried about the <quote rend="inline">utter human docility</quote> <ptr target="#mcluhan_1994" loc="57"/> that
                    the extreme of amputating core cognition could represent.</p>
                <p>In our modern context, as advancements in computing continually work to blur the
                    boundary between user and interface intention <ptr target="#cellard-masure_2018"/> and
                    McLuhan’s fears have come to bear in the extreme in the form of generative
                    AI <ptr target="#lee_2025"/>, the Digital Humanities are left to grapple with the place of the
                    constantly elided human in computational work. What does interpretive and
                    embodied knowledge production look like when it is, in part, the product of a
                    machine? How do we go about analysing and critiquing its influence? The
                    relationship between human and machine is central to the core methodological
                    discussions of DH <ptr target="#van-es_2018"/>.</p>
                <div>
                    <head>Separation and Transplantation</head>
                    <p>The prevalent use of computers in scientific disciplines is consistent with
                        the way in which they have historically used other tools: in service of
                        mechanical objectivity. This notion of using machines to augment and
                        automate procedures in order to circumvent the faults of the human senses
                        which might obscure the objective truth predates computers. The introduction
                        of photography is a seminal example, the mechanical image as amputated eye
                        becoming <quote rend="inline">the emblem for all aspects of non-interventionist objectivity</quote>
                        <ptr target="#cordell_2019"/>. The realities of the use of photography in its earliest
                        days give the strongest insight into the motivation behind its inclusion: It
                        <quote rend="inline">was not because the photograph was more obviously faithful to nature than
                        handmade images — many paintings bore a closer resemblance to their subject
                        matter than early photographs, if only because they used color — but because
                        the camera apparently eliminated human agency</quote> (PLACEHOLDER).</p>
                    <p>Since it extends the brain, it is the <soCalled>faults</soCalled> of cognition that are
                        addressed by computation. In the early days of computing, <soCalled>faults</soCalled>
                        essentially referred to procedural errors that might occur in mathematical
                        calculation — of Bernoulli numbers, of missile trajectories, etc. What is
                        being extended is not the reasoning behind the problem solving, but the
                        speed, scale, and precision with which it can be done. In cases such as
                        these, the delegation of cognition is arguably largely unproblematic; the
                        problems are solved with strictly defined procedures that could be used by
                        any person to arrive at an identical answer — the assumptions subsumed by
                        the amputation are minimal and well-understood. Scientific instruments,
                        explains Lisa Gitelman, are <quote rend="inline">matters of consensus</quote> within a given community.
                        When a new instrument is invented or introduced, there is a period where its
                        proponents must <quote rend="inline">demonstrate persuasively that it does or means what they
                        say.</quote> After this point, as its use becomes more common, <quote rend="inline">its general
                        acceptance soon helps make it a transparent fact of scientific practice</quote>
                        (PLACEHOLDER). The tool and its surrounding protocols are rendered
                        self-evident such that scientists <quote rend="inline">look through the instrument the way one
                        looks through a telescope, without getting caught up in battles already won
                        over whether and how it does the job</quote> (gitelman ,2008, 5). </p>
                    <p>In computing, these battles were won at a time where the capabilities of the
                        technology were more limited and the user base more constrained, but the
                        established transparency means that the impact of the mediation continues to
                        remain unquestioned. However, using a limited set of machine instructions to
                        automate the calculation of mathematical tables and using a high-level
                        language to sketch a model to suggest insurance premiums or parole
                        sentences, for instance, represent two very different models of working with
                        machines. In the latter case, what is being extended and, eventually,
                        amputated, is no longer well defined and delimited procedures, but reasoning
                        and intuition. This usage is understood in the continuity of mechanical
                        objectivity — circumventing cognitive bias as opposed to simple faults of
                        the senses — and is attributed the same tacit trust despite not having been
                        subjected to the same requirements of general agreement and
                        understandability that underpinned the invisibilisation of the former.</p>
                    <p>Media are culturally defined. Our interactions with technology drive its
                        evolution which in turn influences future interaction. This circular
                        influence means that media come to reflect the beliefs and values of the
                        culture from which they emerge <ptr target="#latour_1990"/>, which is what allows them to
                        become invisible. In light of this, the invisibility of a medium should
                        theoretically be disrupted when transposed to a different context. However,
                        the mechanistic nature of early DH allowed norms to be ported without
                        interrogation or adjustment. In the now infamous case of Father Busa’s Index
                        Thomisticus <ptr target="#nyhan-passarotti_2019"/>, for instance, the problem of creating
                        a large concordance resembles the problems of early computing in that the
                        procedure is well defined and associated assumptions minimal, explainable,
                        and generally well understood by the relevant community. This does not hold
                        in the case of the interpretive and ambiguous problems of more modern DH,
                        where the more complex extension of reasoning is at play (Presner 2010).
                        Here, scientific epistemic notions embedded in the technology stand to alter
                        and shape humanistic inquiry in a way that might contradict the values of
                        the latter tradition — an influence that often passes unnoticed because of
                        the pre-established invisibility of the medium.</p>
                    <p>The choice of the fruit fly as an <soCalled>instrument</soCalled> has played a major role in
                        defining genetics as a discipline, argues Gitelman. If a different organism
                        were used <quote rend="inline">genetics itself would be a substantively different field</quote> —
                        insofar as, for instance, the pacing of research and framing of questions
                        are concerned. She recognizes that <quote rend="inline">[t]he particular authority of science
                        makes this an uncomfortable claim,</quote> but one that in that in other contexts,
                        however, would seem to be evident: </p>
                    <quote rend="block">[J]ust as it makes no sense to appreciate an artwork without attending to
                        its medium (painted in watercolors or oils? sculpted in granite or
                        Styrofoam?) it makes no sense to think about <soCalled>content</soCalled> without attending to
                        the medium that both communicates that content and represents or helps to
                        set the limits of what that content can consist of. Even when the content in
                        question is what has for the last century or so been termed <soCalled>information,</soCalled>
                        it cannot be considered <soCalled>free of</soCalled> or apart from the media that help to
                        define it. <ptr target="#gitelman_2008" loc="7"/></quote>
                    <p>Many humanistic disciplines have a history of considering the impact of
                        mediation — any scholar of early modern literature, for instance, would
                        argue the necessity of considering the nature of the novel as a medium when
                        analysing literary trends. In DH, however, there is a certain blind spot
                        when it comes to computer mediation.</p>
                    <p>Media scholar Anthony Masure insists that DH digital tools rarely reflect the
                        epistemological tensions between computing and the humanities because they
                        <quote rend="inline">hide the primary characteristic of computers: their facility for processing
                        symbols</quote> <ptr target="#masure_2018"/>; layers of abstraction obscure the base mechanisms
                        to create the <soCalled>natural</soCalled> interactions of Elias’ imagination. In order to be
                        able to actually see and analyse the impact of the computational medium, it
                        is necessary to understand its specificity. Media specificity is defined by
                        essentially what Masure is pointing to: how the medium inscribes
                        information. Computing is somewhat unique in that it inscribes both
                        procedure and artefact. <soCalled>Doing</soCalled> computation is mapping human understandings
                        of structure and process to a specific format that is manipulable by a
                        machine. The mediation is the adjustment made to the original conception to
                        conform to a paradigm that is defined and bounded by the way in which
                        information is represented. <quote rend="inline">We cannot continue to ignore how machine
                        algorithms <soCalled>read</soCalled> literary information,</quote> insist literary DH scholars Long
                        and So, <quote rend="inline">while blindly relying on them to enhance our own reading and
                        interpretative practices.</quote> <ptr target="#long-so_2016" loc="236–37"/>. Understanding what
                        machine <soCalled>reading</soCalled> represents requires understanding its <soCalled>underlying
                        computationality</soCalled> — which is to say the structured reasoning and symbolic
                        encoding that define the medium <ptr target="#berry_2011"/>.</p>
                </div>
                <div>
                    <head>A Complex Relationship Dynamic</head>
                    <p>The implications of underlying computationality are precisely the subject of
                        D. Sculley and Bradley Pasanek’s treatise on the use of Machine Learning in
                        DH. Their core argument is that computational learnability theory, which
                        guarantees the learnability of arbitrary concepts, makes assumptions that
                        are at odds with those of humanistic knowledge production. For instance, it
                        first assumes that <quote rend="inline">data is produced by some process with constant
                        probabilistic qualities</quote> <ptr target="#sculley-pasanek_2008" loc="411"/> — which crucially
                        means it does not change over time — and that examples can be drawn from it
                        identically and independently. This assumption is what enables formal
                        generalisation and creates the conditions for the proof that a hypothesis
                        learned on early data can be used to make predictions about later data. It
                        is, however, an assumption that rarely holds with respect to humanities
                        data. Historians and scholars of culture, for instance, are frequently
                        interested in change over time; they fully expect the dataset they are
                        working to exhibit some non-constant behaviour. The required existence of a
                        fixed distribution also implies the existence of some level of fixed truth
                        which, as seen, is a troubling notion in a humanistic context. The further
                        learnability assumptions of a fixed size hypothesis space and representative
                        data similarly butt up against the notions of situated knowledge and data
                        constructedness. Consequently, Sculley and Pasanek assert that ML tools and
                        techniques, which are built on the foundation of this theory of
                        learnability, can not be used to <soCalled>prove</soCalled> claims in the humanities in the
                        same way they do in the natural sciences, where <quote rend="inline">the validity of the
                        evidence lies inherent in the technology</quote> <ptr target="#sculley-pasanek_2008" loc="421"/>. They
                        argue that ML models in DH are only made relevant through critical
                        interpretation — not only on the level of the content, but also of the
                        experimental method. The latter demands engaging with the ways in which the
                        specific demands of the computational tools influenced the procedure and
                        results <ptr target="#underwood_2014"/>, something that is notably particularly difficult
                        in the realm of ML/AI, where the explainability of deep learning models
                        evolved through training remains an open challenge <ptr target="#minh_2022"/>.</p>
                    <p>In his work on sentiment analysis of the poetry of the Black Arts Movement
                        (BAM), Ethan Reed offers at once a critique and defence of the use of
                        computational methods in humanities contexts — both of which rely on
                        <quote rend="inline">look[ing] carefully at the nitty-gritty details [of] what these tools are,
                        how they work, as well as the biases and assumptions they bring to any
                        analyses they might perform</quote> <ptr target="#reed_2019" loc="111"/>. It is Reed’s awareness of
                        the demands and limitations of the computational medium which led him to
                        ascertain from the start that his corpus was rife with what he terms
                        <quote rend="inline">computational resistance.</quote> The <quote rend="inline">formal features</quote> that create meaning in the
                        poetic form (e.g. use of vernacular, irregular white space, deliberate word
                        misspellings or transformations) would <quote rend="inline">defy, bewilder, or otherwise
                        frustrate computational reading practices at its various stages</quote> because of
                        their connotations which do not align with those of dominant <soCalled>canonical</soCalled>
                        forms; however, facilitating analysis by manipulating these texts such that
                        they are <quote rend="inline">more amenable to computational forms of reading [...] often
                        involves manipulating the voice of the poet by making changes to the
                        original text</quote> <ptr target="#reed_2019" loc="99–100"/>. Reed insists that computational
                        techniques can nonetheless be useful in the study of these poetic texts if
                        the analysis places results in dialogue with these challenges. Using
                        PatternAnalyzer, a pattern-based classifier, Reed was surprised to find that
                        it classified what he perceived to be one of the most positive poems in the
                        corpus — <title rend="quotes">Come Sing a Song</title> — as the single most negative text. He
                        discovered that this came down to the model routinely classifying the word
                        <q>Black</q> as negative, despite the racial sense being designated as neutral in
                        the model lexicon. Reed hypothesised — calculations appearing to confirm —
                        that, failing to resolve the term’s polysemy, the model simply averaged the
                        valence values of the three possible senses of <soCalled>black,</soCalled> resulting in an
                        overall negative. This course of action, though guessable because of the
                        rule-based nature of the model, was completely black-boxed and not even
                        featured in documentation. The classifier similarly struggled with terms
                        whose sentiment was made opposite in the context of the poem (e.g. <soCalled>bad</soCalled> in
                        the line <quote rend="inline">sing a <soCalled>bad</soCalled> freedom song</quote>). Studying the classifier’s struggles,
                        Reed insists, contributed to a deeper consideration of the question of
                        audience and, correspondingly, tone, which can be conceptualised in terms of
                        an author’s perception of their audience. A positive reading of <title rend="quotes">Come Sing a
                        Song</title> would have relied on a specific interpretation of terms that is born
                        of belonging to the community for which the poem was intended. One of the
                        most terminologically violent poems — <title rend="quotes">The True Import of the Present
                        Dialogue, Black vs. Negro</title> — was determined by PatternAnalyzer to be
                        relatively neutral; this occurred through a combination of the fact that the
                        classifier simply marked vernacular terms for which it had no lexicon entry
                        as neutral and the way in which <soCalled>negative</soCalled> terms were often accompanied by
                        <soCalled>positive</soCalled> ones (e.g. <quote rend="inline">shoot straight</quote>), the scores essentially cancelling
                        out. The latter phenomenon, though born of misunderstanding (in human
                        terms), is an interesting computational reflection of the concept of
                        <soCalled>measured protest,</soCalled> which scholars had previously identified in BAM poems. </p>
                    <p>A different classifier, Vader, which was trained in a way that sees it
                        perform better on vernacular text and demonstrate more sensitivity to
                        sentiment intensity, while coming closer to the mark with <title rend="quotes">Come Sing a
                        Song,</title> performed a surface-level reading of <title rend="quotes">The True Import,</title> marking it as
                        extremely negative. It failed to capture the <soCalled>positive negativity</soCalled> —
                        strategically employed anger — that Reed and critics see in the poem by
                        virtue of knowledge of <quote rend="inline">vision of a broader social purpose</quote> (reed, 2019,
                        126). That said, Reed asserts that <quote rend="inline">the light shed by VADER’s peculiar
                        evaluative logic as to why this poem seems so negative allowed me to reflect
                        more seriously on my own understanding of the poem, as well as that of the
                        many other critics who have noted its particular affective charge</quote> (reed,
                        2019, 131). The value Reed finds in digital tools is in the way in which
                        they <quote rend="inline">[do] <emph>not</emph> transform the text the way that a
                        human reader does</quote> <ptr target="#reed_2019" loc="96"/>; the challenge lies in not conflating
                        the two when the computational mechanics are often black-boxed away for
                        <soCalled>ease of use,</soCalled> which, as described by Stephen Ramsay and Geoffrey Rockwell
                        <quote rend="inline">install[s] the user at a level of abstraction far above whatever
                        theoretical claims might lie beneath.</quote> <ptr target="#ramsay-rockwell_2012" loc="80"/></p>
                    <p>While Reed’s explorations focused on the use of tools in DH, there is also
                        the question of the construction of computational artefacts. The act of
                        <soCalled>building</soCalled> has been central to DH since the field’s inception; however,
                        there is complex debate over whether it (in isolation) <soCalled>counts</soCalled> as
                        scholarship under the epistemologies of both constituent disciplines. Ramsay
                        and Rockwell (2012) offer three interrelated framings of DH tools and three
                        corresponding arguments for <soCalled>building</soCalled> as a valid form of humanistic, and
                        therefore DH, scholarship. They first argue that DH tools can be considered
                        theories (in the humanistic more so than scientific sense — something that
                        explains rather than predicts) because they embody a specific discourse.
                        Digital artifacts have a procedural rhetoric; the information they choose to
                        display and the way they afford specific user interactions is an argument.
                        Because of this inherent orientation, Ramsey and Rockwell assert that
                        computational tools can also act as theoretical lenses, in that they help us
                        see things differently; as <soCalled>hermeneutical instruments</soCalled> they are undoubtedly
                        valid under a knowledge-culture built around interpretation. Finally, they
                        propose the notion of the digital as a theoretical model. Computing is a
                        modelling discipline because <quote rend="inline">any computing system is an explicit, delimited
                        conception of the world or <soCalled>model</soCalled> of it</quote> <ptr target="#mccarty_2005" loc="21"/>. Ramsey and
                        Rockwell draw a parallel between the coding used to create such a model and
                        the writing used to express specific conceptions of the world in more
                        traditional humanistic scholarship. They argue that the understanding that
                        comes of <quote rend="inline">the manipulation of features, objects, and states of interest</quote> in
                        programming is equivalent to that born of having to put things into writing.
                        They notably highlight that the words (or code) by themselves are not the
                        scholarship, but the modelling process behind them, which is highly
                        dependent on the medium of expression.</p>
                    <p>What unites the three framings Ramsey &amp; Rockwell propose — and indeed,
                        Sculley &amp; Pasanek and Reed’s work — is that they are all a question of
                        thinking with computers. Each relationship described relies on an
                        understanding of computational mediation, on seeing the ways in which the
                        machine’s inscription and processing of information transform phenomena and
                        treating it as another voice in an ongoing discourse, to be accounted for in
                        analysis and used as part of idea generation. There is a fine line between
                        this thinking with and the alternative of thinking like computers — which is
                        to say deliberately, if often unconsciously, trying to structure our
                        conceptions to be amenable to computation <ptr target="#doueihi_2011"/>. David Berry
                        argues that we are witnessing an increasing operationalisation or
                        mechanisation of thought; when we cede intention to computational
                        suggestions we are confounding human reasoning with <quote rend="inline">algorithmic
                        conceptualizations</quote> of their ideas, the latter of which are increasingly
                        constructed from information collected from millions of users and are
                        therefore inescapably anchored in hegemonic frameworks such as capitalism
                        and white supremacy. As Berry describes,
                    <quote rend="block">[...] Rather than acting as bicycles for the mind<note> A phrase initially
                            coined by Steve Jobs, meant to describe, according to Berry, the way in
                            which computers stood to <quote rend="inline">augmen[t] the cognitive capacities of the
                            user, making them faster and more capable.</quote> (emphasis mine)</note>,
                        these technologies replace certain cognitive functions of the mind. Being
                        owned and controlled by corporate organisations they tend to weaken
                        explanatory and critical thinking and instead nudge and influence human
                        behaviour in directions that are profitable</quote> <ptr target="#berry_2023"/>.
                    The sort of replacement Berry outlines could more moderately be understood as
                        the normalisation and invisibilisation that, as seen, is part of the life
                        cycle of any representational medium. In this light, we might see the core
                        challenge of DH’s relationship to machines as walking the line between like
                        and with — analysing the nature and impact of mediation so as to be able to
                        decide when and to what extent delegation is acceptable, and when and how to
                        undertake generative co-creation.</p>
                    <p>Edsger Dijkstra is famously known to have argued that <quote rend="inline">computer science is no
                        more about computers than astronomy is about telescopes.</quote><note> Attribution
                            is contested</note> In this, he implies that there can exist a complete
                        separation between logical process and the machine being used to express it.
                        Dijkstra was strongly against what he saw as the de-mathematisation of
                        computing, which opened it up to broader use. He spoke out, for instance,
                        against the development of early high-level programming language, COBOL,
                        insisting that it <quote rend="inline">sought to replace intellectual discipline by management
                        discipline</quote> <ptr target="#dijkstra_1984"/>. Perhaps in Dijkstra’s idealised framing, it
                        might have been feasible to separate human logic from machine expression; if
                        the applications remain limited and carried out by experts, there can be a
                        clear and well-understood mapping between human and machine problem
                        representations. The impact of mediation is minimal because the problem
                        domain and the context from which the medium emerged are one and the same.
                        As — contrary to Dijkstra’s wishes — the reach of computing has broadened,
                        the mapping between problem and representation has become more complex and
                        less evident, but the assumptions baked into the acceptance and
                        invisibilisation of the medium remain those established in that limited
                        context and expert community. The introduction of the digital to humanistic
                        scholarship has provoked a major change in how information can be viewed and
                        processed, and birthed new methodologies which, for all they may mimic the
                        old ones, inescapably operate differently. <quote rend="inline">Computation forces us to rethink
                        our current disciplinary practices in the humanities from the ground up,</quote>
                        asserts Andrew Piper — <quote rend="inline">What counts as evidence? What is the relationship
                        between theory and practice? How do we account for the technological
                        mediations of our critique?</quote> <ptr target="#piper_2016" loc="2"/>. These questions all come back
                        to the influence of the way the machine <soCalled>thinks</soCalled> on humanistic work. The
                        machine, of course, does not <soCalled>think</soCalled>; the way it structures and processes
                        information is a product of a specific epistemic perspective. Now that we
                        are pointing it at so much more than the stars, it is essential to consider
                        the telescope.</p>
                </div>
            </div>
            <div>
                <head>Hermeneutic Attention</head>
                <p>When working with computer scientists on digital visualisation tools, art
                    historians on Project Cornelia were left feeling that <quote rend="inline">the tech kids are running
                    too fast</quote> <ptr target="#lamqaddam_2018"/>. Houda Lamqaddam — one of the <soCalled>tech kids</soCalled> —
                    explains that the art historians found speed of development and release was at
                    odds with the <quote rend="inline">contemplative nature of their research.</quote> Artworks, through the
                    methodological lens of art historians, are <quote rend="inline">elements that trigger reflection
                    rather than contain answers,</quote> an outlook that contrasts digital interfaces which
                    <quote rend="inline">present the user with synthesized views of data and answers to research
                    questions.</quote> Contributions in art history, due to the inherent subjectivity of
                    the field, <quote rend="inline">often consist of making a claim denoting a unique perspective</quote> — one
                    that is based in <quote rend="inline">in-depth artwork analysis</quote> that <quote rend="inline">put[s] into evidence
                    previously unexplored nuances.</quote> Though, as seen, it is arguably embodied in the
                    tools, <quote rend="inline">the use of aggregates, statistics and data-crunching algorithms by
                    computer scientists is viewed as an approach where the reflection and the nuance
                    disappear.</quote> For art historians, Lamqaddam asserts, the intervention of the
                    digital <quote rend="inline">threatens their operational paradigm.</quote> In investigating their reticence
                    to engage with the digital, she focuses on the perceived speed difference, but
                    this mismatch could be viewed as an element of a broader question of divergent
                    approaches to interpretation.</p>
                <p>Interpretation is how information becomes knowledge; this sense-making is
                    inherently a part of any discipline. There is a common process of pattern
                    matching and formalisation — a collection of examples will be interpreted to
                    propose a more or less abstract theory demonstrating or explaining a phenomenon
                    <ptr target="#dixon_2012"/>. Where domains differ is in the emphasis they place on aspects of
                    this process. The heart of the scholarship for the art historians is the process
                    by which they arrive at their conclusions. They were discomforted by the work of
                    the developers, as the interpretation that went into the choice of encoding
                    entities and relationships to illustrate specific theories of connection was, if
                    not hidden, certainly not easily legible to the historians presented with the
                    finished interface — an object which, for the developers, was understood to be
                    the primary contribution. Underwood proposes that <quote rend="inline">humanists tend to think of
                    computer science as an instrumental rather than philosophical discourse</quote>
                    (PLACEHOLDER). This is untrue, he argues, highlighting how, for instance, the
                    Bayesian statistics driving many machine learning algorithms encode a specific
                    way of reasoning in their acknowledgement that we <quote rend="inline">approach every question with
                    some previous assumptions (called <soCalled>prior probabilities</soCalled>), as well as particular
                    kinds of uncertainty</quote> <ptr target="#underwood_2014" loc="66"/>. This interpretive cycle of existing
                    assumptions iteratively changing through the introduction of new evidence,
                    Underwood insists, should be familiar to humanists, and recognisably valid under
                    their paradigm of knowledge creation; however, it is seldom visible or legible.
                    Digital humanists are caught between — and left to reconcile — implicit and
                    explicit hermeneutic traditions.</p>
                <div>
                    <head>Divergent Traditions</head>
                    <p><quote rend="inline">There is no self-critical hermeneutic tradition in the sciences comparable
                        to that taken for granted by other scholars,</quote> argues Gyorgy Markus (1987),
                        as descried by <ptr target="#franklin_1995" loc="165"/> — the <soCalled>other</soCalled> gesturing to the
                        humanities and social sciences. Markus is not making a critique, but an
                        observation; he posits that it is a case of scientific domains constructing
                        themselves such that they do not need a strong hermeneutic tradition. Markus
                        points to scientific knowledge diffusion as a prime example. In interviews
                        with biologists, <ptr target="#cetina_1999"/> noted that many described an aspect of
                        intuition as essential to correctly carrying out experimental procedures,
                        but equally that this was not in any way integrated into methodology or
                        reporting. Since the late 19th century, scientific publications have been
                        characterised by their passivity; the figure of the author — the knowledge
                        interpreter — is effaced. This is not intended to imply that no
                        interpretation has taken place but rather that the reader’s interpretation,
                        specifically of the procedures and logic from which a conclusion was
                        derived, will never differ from that of the author. <quote rend="inline">The <soCalled>ideology</soCalled> [...] of
                        the natural sciences,</quote> Markus argues <quote rend="inline">regards any acceptable scientific text
                        as totally self-sufficient as to its meaning (and therefore as unambiguously
                        clear to any reader with adequate competence)</quote> <ptr target="#markus_1987" loc="9"/>. The
                        <soCalled>adequate competence</soCalled> to which he is referring is knowledge of rigidly
                        defined and strictly encoded axioms and procedures. Curtailing alternative
                        interpretations is one of the functions of the <soCalled>language</soCalled> of the natural
                            sciences<note> For all computing is arguably not one of the natural
                            science, it nonetheless largely mirrors their operation.</note>, which
                        is defined by these <soCalled>sign-systems.</soCalled> By virtue of their use, it is understood
                        that the writer and the reader have the same conception of what is being
                        described. When the writer and the reader are interchangeable, there is
                        nothing to be interpreted.</p>
                    <p>Elements of the sign-system are viewed as beyond interrogation — a framing
                        enabled by the decontextualized and ahistorical way in which they are
                        taught. Markus notes that, whereas humanities students are expected to
                        interrogate Aristotle’s works in light of the context of their creation and
                        with respect to different readings over time, the version of Newton’s laws
                        written in textbooks is <quote rend="inline">something Newton should have written had he used
                        modern mathematical notations, contemporary physical concepts, etc.</quote> For all
                        that, as Markus implies, the meaning of these and other theories and
                        expressions changes as science progresses, they are <quote rend="inline">posited as designating
                        well-identifiable, singular historical phenomena and events</quote> (markus, 1987,
                        32–33). The interpretation behind building blocks of scientific knowledge
                        production is subsumed by definitive <soCalled>signs.</soCalled> Mathematics education
                        researcher Stephen Brown relates the anecdote of a physics professor who
                        insisted that <quote rend="inline">the only things that count in proving anything are axioms,
                        definitions, rules of logic and previously established theorems</quote> (brown,
                        1996, 1292); having forgotten one of the axioms of vector space, the
                        professor, mumbling under his breath, turned his back to the class to derive
                        it on the blackboard, taking care to hide his work and hastily erasing it
                        before returning to his lecture in attempt to remain consistent with his
                        earlier assertion. Brown argues that this handing down of detached axioms in
                        <quote rend="inline">an authoritarian and non-controversial way</quote> leads us to <quote rend="inline">believe that they
                        were created in a way that involved no labor pains and in particular no
                        negotiation between what was considered worth proving and what was taken for
                        granted</quote> <ptr target="#brown_1996" loc="1292"/>. Interpretation is considered a key step of
                        theoretical mathematical modelling, however, the process of modelling is
                        rarely taught. Even in the rare cases that it is, student assessment has a
                        tendency to focus on the overall outcome rather than considering the
                        learner’s progress through modelling steps or sub-processes <ptr target="#berry_2002"/>.
                        Hermeneutics in the sciences, per Markus, is not synonymous with learning —
                        as might be said in the case of the humanities — but prerequisite to it.
                        Interpretation is a <quote rend="inline">tacit dimension</quote> <ptr target="#markus_1987" loc="7"/>.</p>
                    <p>Interpretation is anything but tacit in the humanities. As previously
                        discussed, the humanities lack a conception of absolute truth. In the
                        absence of a definitive answer, there is nothing left but interpretation. In
                        his book on literary criticism and computational text analysis, Reading
                        Machines, Ramsay writes:</p>
                    <quote rend="block">However far ranging a scientific debate might be, however varied the
                        interpretations being offered, the assumption remains that there is a
                        singular answer (or a singular set of answers) to the question at hand.
                        Literary criticism has no such assumption. In the humanities the fecundity
                        of any particular discussion is often judged precisely by the degree to
                        which it offers ramified solutions to the problem at hand. We are not trying
                        to solve Woolf. We are trying to ensure that discussion of The Waves
                        continues. <ptr target="#ramsay_2011" loc="15"/></quote>
                    <p>Ramsay’s assertion highlights a key distinction between the two hermeneutic
                        traditions. The humanities tend toward the dialogic, looking for the
                        interplay of multiple interpretations, whereas as the sciences are aiming
                        for consensus and, correspondingly, are more monologic in their practices.
                        Computer science is grounded in a problem solving paradigm and the
                        humanities in a problematising one <ptr target="#van-zundert_2015"/>.</p>
                    <p>Because of this difference in aim, technology use as it has been implemented
                        in one discipline may not be identically useful in the other. This mismatch
                        was at the forefront of the minds behind the Humanities Networked
                        Infrastructure (HuNI) project <ptr target="#burrows_2013"/>. HuNI, which looks to make
                        Australian cultural data from myriad institutions centrally accessible and
                        interrogable, was funded as part of a larger network of <soCalled>digital
                        laboratories</soCalled> to support data-centred research work, the rest of which were
                        anchored in scientific contexts. Correspondingly, the proposed parameters
                        very much followed in line with a <soCalled>big science</soCalled> model of data processes —
                        aiming to simplify and streamline established data workflows. The team
                        behind HuNI did not see these priorities as suitable for humanities research
                        needs and set out to create a modified paradigm deliberately constructed to
                        <quote rend="inline">enabl[e] researchers to express, share and discuss their differing
                        interpretations of the data</quote> <ptr target="#burrows-verhoeven_2016" loc="119"/>. HuNI is
                        structured around graphs generated by user-created connections between
                        cultural data entities (e.g. works, places, people) imported from different
                        organisations. All connections are public by default, are not required to
                        fit into any pre-defined ontology, and can be annotated with detailed
                        meaning such that <quote rend="inline">different perspectives between (and within) disciplines
                        are preserved and foregrounded, instead of being hidden behind a normalized,
                        authoritative framework.</quote> <ptr target="#burrows-verhoeven_2016" loc="119"/>. Its creators note
                        that HuNI is specifically designed to promote the kind of <soCalled>serendipitous
                        encounters</soCalled> that are fruitful in the humanities <ptr target="#ramsay_2010"/>, rather than
                        looking to scaffold any sort of <soCalled>optimal</soCalled> trajectory or set of operations.
                        An environment that affords serendipity is one in which there is enough
                        structure to maintain relevance but not so much as to <quote rend="inline">suppress the
                        unexpected or unpredicted</quote> and where there exists multiple perspectives and
                        the <quote rend="inline">ability to meander among these different understandings and
                        conceptualisations of the world and knowledge</quote> (burrows-verhoeven, 2023,
                        32). </p>
                    <p>Encountering the same friction as the HUNI team, Hinrichs, Forlini, and
                        Moynihan (2019) describe a pivotal scene for their work on visualisation in
                        DH where one member of an interdisciplinary project committee posed the
                        pointed question, with respect to project outcomes: <soCalled>Are we building tools
                        or just sandcastles?</soCalled> The source of the committee member’s frustration, they
                        argue, is a mismatch in how the term <soCalled>tool</soCalled> is understood in science and the
                        humanities. In the former context, it is related to <soCalled>prototype,</soCalled> and
                        understood as the stabilised and often generalised endpoint of iteration.
                        The successive prototypes are a means to an end rather than <quote rend="inline">object[s] of
                        inquiry</quote> <ptr target="#hinrichs_2019" loc="i82"/> in their own right. Humanistic understandings
                        tend instead to frame tools as narrative and generative — an element of a
                        process rather than a product. DH visualisations are, correspondingly, often
                        small scale and highly specific. Hinrichs et al. defend the metaphor of the
                        sandcastle as it does not permit a focus product. A sandcastle is always
                        transient, will always be taken by the tide at the end of the day. The
                        <soCalled>point</soCalled> is to build and rebuild, learning and changing without the
                        constraints of a defined endpoint or a need to reach it. The emphasis is
                        therefore on the ludic act of <soCalled>sandcastling,</soCalled> which they describe as a
                        <quote rend="inline">dynamic process wherein speculation and re-interpretation advance
                        knowledge</quote> <ptr target="#hinrichs_2019" loc="i80"/>. The success of humanistic visualisations,
                        argue Sinclair et al., should be evaluated as a function of <quote rend="inline">how well they
                        support interpretive activity,</quote> asserting that <quote rend="inline">the humanities approach
                        consists not of converging toward a single interpretation that cannot be
                        challenged but rather of examining the objects of study from as many
                        reasonable and original perspectives as possible to develop convincing
                        interpretations</quote> <ptr target="#sinclair_2013"/>.</p>
                </div>
                <div>
                    <head>Developing Digital Hermeneutics</head>
                    <p><quote rend="inline">For as long as humanities scholars have made use of digital tools,</quote> asserts
                        Eef Masson, <quote rend="inline">they have met with critique from fellow practitioners [...]
                        stem[ming] from the perception that the projects conducted do not do justice
                        to the critical-interpretive legacy of much humanities research</quote> (masson,
                        2017, 31). With its departure from traditional close reading and its use of
                        computational tools and methods in ways which do not necessarily align with
                        the conventions of hypothesis-driven scientific research, detractors of DH
                        accuse it of being both methodologically bereft and interpretation-poor, and
                        correspondingly lacking in validity under the either disciplinary
                        epistemology <ptr target="#fish_2012"/> <ptr target="#allington_2016"/>. Out of this critique has
                        emerged a nascent <soCalled>digital hermeneutics,</soCalled> with a two-pronged goal of making
                        explicit the interpretation implicit to digital methods and tools, so as to
                        situate their use in the humanistic hermeneutic tradition, and of
                        articulating the ways in which their design and use can be adapted to make
                        space for interpretation.</p>
                    <p>The coordinators of the University of Luxembourg’s <title rend="quotes">Digital History and
                        Hermeneutics</title> Doctoral Training Unit (DTU) insist that digital hermeneutics
                        must concern itself with how <quote rend="inline">digital tools and infrastructures are
                        transforming historical research practices in all stages of the iterative
                        research process</quote> and should operate as <quote rend="inline">a critical framework for making the
                        methodological and epistemological tensions in current history practices
                        explicit</quote> <ptr target="#fickers_2022" loc="7"/>. They correspondingly chose to structure the
                        DTU’s training and projects around the notion of hands-on <soCalled>Thinkering</soCalled>: a
                        <quote rend="inline">heuristic mode of doing</quote> that <quote rend="inline">combines playful tinkering with critical
                        thinking</quote> <ptr target="#fickers_2022" loc="7"/>. Emblematic of this approach, kiara and Lumy
                        are a paired set of the DTU’s research projects that together form a virtual
                        research environment which scaffolds <soCalled>Thinkering</soCalled> (crouy-chanel 2022). kiara
                        — which acts as a backend to Lumy but can also be used independently via
                        command line — is described by its creators as a <quote rend="inline">data orchestration
                        engine.</quote> It supports the creation and analysis of network graphs by breaking
                        workflow steps that would typically be subsumed into a single script down
                        into functional modules. These can be rearranged and chained to create any
                        number of bespoke pipelines. A user can also create custom modules; each is
                        specified by an expected input and output, process description, and
                        associated process python code. Though things are compartmentalised, nothing
                        is blackboxed. What makes kiara different from similar workflow management
                        tools emerged from a software development context is its focus on metadata
                        management and iterative user interaction. It keeps a tree-structured log of
                        every user interaction, allowing the user to reflect on their process and
                        easily trace the <soCalled>lineage</soCalled> of any data result. Lumy is similarly intended to
                        <quote rend="inline">encourage users to adopt recursive, self-reflexive and reproducible work
                        practices.</quote> It sits on top of kiara, giving GUI access to the network
                        manipulation functionality in one part of the interface, while also
                        providing space in another panel for users to take notes on their practice.
                        The aim of associating metadata snapshots with researcher reflections is to
                        <quote rend="inline">cultivat[e] the holistic practice of interweaving methodology, data, code,
                        and metadata with a narrative of researcher’s choices and actions</quote>
                        <ptr target="#crouy-chanel_2022" loc="37"/>. This deliberate pairing of the <quote rend="inline">the ability to
                        record what the software is doing with the ability to record what the
                        scholar is doing</quote> <ptr target="#crouy-chanel_2022" loc="37"/> brings the interpretation
                        underlying the use of computational tools to the forefront.</p>
                    <p>Emergent digital hermeneutics must honour and encompass established
                        disciplinary hermeneutic approaches. A significant proportion of DH work is
                        connected to literary studies; however, the kind of abstract data that is
                        generated from texts using <quote rend="inline">statistical and computational methods</quote> is not
                        <quote rend="inline">the kind of data that literary scholars practicing an [sic] hermeneutic
                        approach are concerned with</quote> <ptr target="#kleymann-stange_2021"/>. Literary hermeneutics
                        is an <quote rend="inline">approach to produce meaning through an iterative, non-deterministic,
                        and subjective procedure,</quote> which relies on three premises: a distinction
                        between the intention of the author and the text, in the acknowledgement of
                        meaning as something that is not inherent but constructed; a circular
                        process of building understanding, which holistically considers both the
                        whole and its parts; and a dependency between text and recipient, which
                        implies a contextual and subjective reasoning process. Though they
                        acknowledge that some digital tools successfully replicate the conditions of
                        analogue hermeneutic processes, Rabea Kleymann and Jan-Erik Stange argue
                        that these essential premises are largely disregarded by tools more anchored
                        in digital methods, especially in the realm of data visualisation. In
                        response, drawing heavily on the work of <ptr target="#meister_2017"/> and (drucker,
                        2018), they propose four <soCalled>postulates</soCalled> for hermeneutic data visualisation
                        which attend to the premises. The first, Two Way Screen, refers to the
                        quality of being manipulable rather than limited to rendering. A hermeneutic
                        visualisation should not only facilitate interpretive activities, such as
                        annotation and information structuring, but display them. This bidirectional
                        interaction encourages the circular and holistic approach. The second
                        postulate, Quality, asks that data be shown as constructed and subjective,
                        as understood from the text-recipient relationship premise. This does not
                        necessarily require novel conventions, but might simply mean using standard
                        visual variables to represent <quote rend="inline">interpretative dimensions like salience or
                        relatedness.</quote> Parallax, the third postulate, emphasises the importance of
                        polyvalence. Access to multiple views of an object of inquiry highlights its
                        ambiguities, and, correspondingly, interpretive possibilities. The aim of
                        Parallax is to deliberately engender uncertainty and doubt, which stands in
                        stark contrast to the Tuftian conception of the purpose of visualisation as
                        being to reveal the truth of data <ptr target="#tufte-graves-morris_1983"/> — the not a —
                        that dominates CS visualisation <ptr target="#munzner_2014"/>. The final postulate,
                        Discourse, is focused on the back and forth between visualisation and
                        interpreter in the construction of argument and is particularly interested
                        in forefronting the influence of the digital representation on the way in
                        which the argument is constructed. In practice, this may look like the kind
                        of visualisation and written interpretation pairing facilitated by
                        Luxembourg DTU’s Lumy interface.</p>
                    <p>While Kleymann and Stange focus on bidirectionality and polyvalence, others
                        insist that transparency is central to supporting interpretation <ptr target="#berry_2023"/> <ptr target="#dobson_2021"/>; inputs, procedures, and results must evidently be
                        interpretable in order to be interpreted. Transparency, in this case, is not
                        the transparency of eyeglasses, where the aim is for transparency to be
                        synonymous with invisibility — the kind of transparency which, as mentioned,
                        is prevalent in computational interfaces <ptr target="#cellard-masure_2018"/> — but the
                        transparency of a skeleton watch, where the point is to expose the inner
                        workings. Exposing the <quote rend="inline">guts of the process</quote> is the central premise of
                        CLAIRON, a natural language database interface (NLIDB) for the theatrical
                        data of the Comédie-Française Registers Project <ptr target="#sollazzo_2022"/>. Like other
                        tools in this class, CLAIRON does natural language question-to-query (SQL)
                        translation, displaying the result of the latter. However, unlike other
                        NLIDBs, it exposes each step of the translation process to the user in an
                        effort to get them to <quote rend="inline">engage with the human to machine translation whose
                        complexities and necessary compromises are at the heart of enacting a more
                        humanistic style of computing</quote> <ptr target="#sollazzo_2022" loc="20"/>. Question-query
                        highlight correspondence shows users how each of their questions is
                        reformulated for execution, which may not match their initial conception,
                        encouraging critical reflection on their intention. Users can directly
                        modify queries that are produced, which are accessible through an
                        interactive log, or even write queries from scratch, which facilitates the
                        circularity evoked by Kleymann and Stange and reflects the dialogic nature
                        of humanistic inquiry. The process of entity matching and disambiguation
                        (establishing a correspondence between question terms and database entitles
                        and selecting between entities of the same name) is explicated through a
                        series of visualisations, including notation for when foreign keys or
                        calculated fields are used, and a series of successive query and expression
                        trees show how linguistic dependencies and keywords are used to produce the
                        query (e.g. implied negation, aggregation, ordering etc.). Notably, the
                        design of CLAIRON makes no attempt to take on implicit interpretation. If
                        asked for the best of Molière’s plays, the interface returns an error
                        message saying that it does not understand the meaning of <soCalled>best</soCalled>; however,
                        questions about the most profitable or most frequently performed plays are
                        translated and executed without issue. This feature forces users to confront
                        their assumptions, which draws attention to the contextual and subjective
                        nature of interpretive knowledge construction.</p>
                    <p>There exists a prevalent assumption in DH that <quote rend="inline">if the computer is to be
                        useful to the humanist, its efficacy must necessarily lie in the aptness of
                        the scientific metaphor for humanistic study</quote> (PLACEHOLDER). However,
                        regardless of how closely a humanistic object of study can be made to
                        resemble a scientific problem, the ways in which disciplines seek to
                        interact with it remain different. The aim of digital hermeneutics, as
                        illustrated by tools such as <title rend="italic">kiara/Lumy</title> and CLAIRON,
                        is to establish new conventions of design and use which leverage the kind of
                        interrogations made possible by digital transformation but which also make
                        explicit sites of interpretation born of this transformation and include
                        mechanisms for manipulation and reflection to support interpretive
                        activity.</p>
                    <p>During her time as Artist in Residence at CERN, Su Wen-Chei observed that,
                        when faced with a complex problem, the scientists would split into two
                        groups, each of which would individually derive an answer; if the two
                        solutions matched, the solution was considered to be correct. When she later
                        related her experience in a workshop, one artist-participant pointed out
                        that, were the same procedure followed with two groups of artists, if they
                        arrived at the same answer, the conclusion would be not that the solution
                        was correct, but that the question was wrong <ptr target="#escobar-varela_2021"/>. These
                        interactions exemplify the different positions interpretation occupies in
                        knowledge production across disciplines and the assumptions underpinning
                        them. In the sciences, there is a notion of a single correct solution and,
                        correspondingly, and implicit aim of consensus. The primary interest lies in
                        arriving at a solution, at which point the process of interpretation leading
                        to it is considered tacit. In the arts, on the other hand — which are
                        epistemologically closer, in this respect, to the humanities — the interest
                        lies in the development and comparison of different interpretations. DH
                        exists at the crossroads of a strong hermeneutic tradition and one which is
                        doing its best to fade away. The explicit interpretation central to
                        humanistic scholarship is challenged by the implicit interpretation of
                        computational artefacts which presume to embody it. At the same time,
                        interpretive methodologies, based as they are in intuition and perspective
                        rather than formal shared language, are difficult to precisely specify,
                        making their use in concert with computation challenging. In order to
                        reconcile these traditions and undertake knowledge production that is valid
                        under both, it is essential to attend to their respective understandings of
                        the role of interpretation and the assumptions which permit these
                        framings.</p>
                </div>
            </div>
            <div>
                <head>Conclusion</head>
                <p>The first wave of DH introduced humanities scholars to the potential of
                    high-level quantitative computational techniques. As they transform their
                    objects of inquiry and methodologies to be amenable to distant rather than close
                    reading, there are aspects which are de-emphasised — made negative — as others
                    come to the forefront.</p>
                <p>Second wave DH saw a renewed focus on the qualitative, with a particular emphasis
                    on the experiential and affective, both of which are rooted in the notion of
                    positionality. This reorientation toward <quote rend="inline">the Humanities’ core methodological
                    strengths</quote> <ptr target="#presner_2010"/>, most notably its attention to complexity, depth, and
                    context, begs the question of how these might be preserved, integrated, and
                    re-imagined for computational work, given its often divergent priorities and
                    assumptions.</p>
                <p>The still speculative third wave <ptr target="#berry_2011"/> pushes for increased attention to
                    <quote rend="inline">the organizing logic that underlies our work</quote> <ptr target="#posner_2015"/> and the ways in
                    which it may stand in opposition to the values and interests of the field. This
                    demands interrogating our interactions with machines on multiple levels of
                    abstraction — notably considering the extent to which they are used in addition
                    to and instead of human intervention as well as how our ideas are transformed
                    through their use, as a function of the affordances and limitations of the
                    computational medium.</p>
                <p>Throughout it all, the validity of DH scholarship has been challenged from both
                    sides over questions of interpretation and its role in knowledge creation.</p>
                <p>Successive developments in the field have introduced new challenges and questions
                    brought about by the integration of humanistic and computational theories and
                    methodologies. What is clear is that DH is doing computing in a way that is
                    different to how it is understood in CS. It is not merely a question of content
                    but of epistemology; computing is used for theory generation and creativity more
                    so than confirmation and generalisation, for enhancing complexity rather than
                    reducing it, for exploring the interplay of — rather than sharply delineating
                    between — the mechanical and the embodied and the rational and emotional.</p>
                <p>DH, however, lacks a dedicated computing curriculum that attends to these
                    differences. That said, the development of one is not necessarily of interest to
                    the community. Given the number of topics and methodologies involved, the
                    difficulty of defining itself has been a constant of the field; this
                    ever-evolving nature coupled with the rapid pace of technological development
                    <quote rend="inline">should make us wary of efforts to encapsulate Humanities Computing into a list
                    of acceptable perspectives and methods that are supposed to have any
                    universality or permanence</quote> <ptr target="#sinclair-gouglas_2002" loc="168"/>. The breadth and
                    variation of DH is its strength, and the success of <quote rend="inline">shared teaching endeavours</quote>
                    in DH rests upon <quote rend="inline">accommodating th[e] wonderful variation and richness and
                    situatedness</quote> <ptr target="#gambell_2021" loc="9"/> inherent to the discipline and its
                    instructors.</p>
                <p>The facets are envisaged as a curricular middle ground. They do not prescribe any
                    specific set of technologies, topics, or competencies, but can serve as a
                    theoretical anchor for the development of a humanistic approach to computational
                    work — for instance as a scaffold to construction and analysis activities or a
                    framework for curriculum design.</p>
                <p>Finally, Computer Science, especially in higher education, is epistemologically
                    closed <ptr target="#turkle-papert_1990"/>, which has been linked to its lack of diversity and
                    argued to be a weakness of the discipline with respect to preparing its students
                    for the complex ways in which computing is embedded in society <ptr target="#ko_2020"/>.
                    Though designed with DH in mind, the facets framework could potentially also be
                    of use in CS — in particular to those working in the areas of CS4All, computing
                    for non-majors, or critically conscious computing — in helping to introduce and
                    explore the potential of alternative ways of viewing and understanding the act
                    of doing computing and its role in society <ptr target="#connolly_2020"/>.</p>
            </div>
            <div>
                <head>Acknowledgements</head>
                <p>Thank you to Professor Quintin Cutts for his thoughtful supervision and eagle
                    eyes!</p>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="agostinho_2019" label="Agostinho et al. 2019">Agostinho, D.,
                    D'Ignazio, C., Ring, A., Thylstrup, N.B., and Veel, K. (2019) <title
                        rend="quotes">Uncertain archives: Approaching the unknowns, errors, and
                        vulnerabilities of big data through cultural theories of the
                    archive</title>, <title rend="italic">Surveillance &amp; Society</title>
                    17(3/4), 422–41. <ref target="https://doi.org/10.24908/ss.v17i3/4.12330"
                        >https://doi.org/10.24908/ss.v17i3/4.12330</ref></bibl>

                <bibl xml:id="alexander_2020" label="Alexander 2020">Alexander, C. (2020) <title
                        rend="quotes">Enabling access to the Federal Writers' Project slave
                        narratives: A case study in digital archive design</title>. Master's thesis,
                    University of Alberta.</bibl>

                <bibl xml:id="allington_2016" label="Allington, Brouillette and Golumbia 2016"
                    >Allington, D., Brouillette, S., and Golumbia, D. (2016) <title rend="quotes"
                        >Neoliberal tools (and archives): A political history of digital
                        humanities</title>, <title rend="italic">Los Angeles Review of
                    Books</title>. Available at: <ref
                        target="https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/"
                        >https://lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/</ref></bibl>

                <bibl xml:id="bardzell-bardzell_2016" label="Bardzell and Bardzell 2016">Bardzell,
                    J., and Bardzell, S. (2016) <title rend="quotes">Humanistic HCI</title>, <title
                        rend="italic">Interactions</title> 23 (2), pp. 20–29. <ref
                        target="https://doi.org/10.1145/2888576"
                        >https://doi.org/10.1145/2888576</ref></bibl>

                <bibl xml:id="benjamin_2006" label="Benjamin 2006">Benjamin, W. (2006) <title
                        rend="quotes">The work of art in the age of mechanical reproduction</title>,
                        <title rend="italic">Visual Culture: Experiences in Visual Culture</title>
                    4, pp. 114–37.</bibl>

                <bibl xml:id="berry_2011" label="Berry 2011" sortKey="Berrya">Berry, D.M. (2011)
                        <title rend="quotes">The computational turn: Thinking about the digital
                        humanities</title>, <title rend="italic">Culture Machine</title> 12.
                    Available at: <ref
                        target="https://culturemachine.net/wp-content/uploads/2019/01/10-Computational-Turn-440-893-1-PB.pdf"
                        >https://culturemachine.net/wp-content/uploads/2019/01/10-Computational-Turn-440-893-1-PB.pdf</ref></bibl>

                <bibl xml:id="berry_2023" label="Berry 2023" sortKey="Berryb">Berry, D.M. (2023)
                        <title rend="quotes">The explainability turn</title>, <title rend="italic"
                        >DHQ: Digital Humanities Quarterly</title> 17(2). Available at:<ref
                        target="https://dhq.digitalhumanities.org/vol/17/2/000685/000685.html">
                        https://dhq.digitalhumanities.org/vol/17/2/000685/000685.html</ref></bibl>

                <bibl xml:id="berry_2002" label="Berry 2002" sortKey="Berryc">Berry, J. (2002)
                        <title rend="quotes">Developing mathematical modelling skills: The role of
                        CAS</title>, <title rend="italic">ZDM–Mathematics Education</title> 34(5),
                    pp. 212–20. <ref target="https://doi.org/10.1007/BF02655824"
                        >https://doi.org/10.1007/BF02655824</ref></bibl>

                <bibl xml:id="brendel-bethge_2019" label="Brendel and Bethge 2019">Brendel, W., and
                    Bethge, M. (2019) <title rend="quotes">Approximating CNNs with
                        bag-of-local-features models works surprisingly well on ImageNet</title>,
                        <title rend="italic">arXiv</title>. <ref
                        target="https://doi.org/10.48550/arXiv.1904.00760"
                        >https://doi.org/10.48550/arXiv.1904.00760</ref>.</bibl>

                <bibl xml:id="brosens_2019" label="Brosens et al. 2019">Brosens, K., Aerts, J.,
                    Alen, K., Beerens, R.J., Cardoso, B., De Prekel, I., Ivanova, A., et al. (2019)
                        <title rend="quotes">Slow digital art history in action: Project Cornelia's
                        computational approach to seventeenth-century Flemish creative
                        communities</title>, <title rend="italic">Visual Resources</title> 35(1-2),
                    pp. 105–24. <ref target="https://doi.org/10.1080/01973762.2019.1553444"
                        >https://doi.org/10.1080/01973762.2019.1553444</ref></bibl>

                <bibl xml:id="brown_1996" label="Brown 1996">Brown, S.I. (1996) <title rend="quotes"
                        >Towards humanistic mathematics education</title>. In <title rend="italic"
                        >International Handbook of Mathematics Education: Part 1</title>, pp.
                    1289–1321. Springer.</bibl>

                <bibl xml:id="buolamwini-gebru_2018" label="Buolamwini and Gebru 2018">Buolamwini,
                    J., and Gebru, T. (2018) <title rend="quotes">Gender shades: Intersectional
                        accuracy disparities in commercial gender classification</title>. In <title
                        rend="italic">Conference on Fairness, Accountability and
                        Transparency</title>, 77–91. PMLR. Available at: <ref
                        target="https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf"
                        >https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf</ref></bibl>

                <bibl xml:id="burrows_2013" label="Burrows 2013" sortKey="Burrowsa">Burrows, T.
                    (2013) <title rend="quotes">A data-centred <q>virtual laboratory</q> for the
                        humanities: Designing the Australian Humanities Networked Infrastructure
                        (HuNI) service</title>, <title rend="italic">Literary and Linguistic
                        Computing</title> 28(4), pp. 576–81. <ref
                        target="https://doi.org/10.1093/llc/fqt064"
                        >https://doi.org/10.1093/llc/fqt064</ref></bibl>

                <bibl xml:id="burrows-verhoeven_2016" label="Burrows and Verhoeven 2016"
                    sortKey="Burrowsb">Burrows, T., and Verhoeven, D. (2016) <title rend="quotes"
                        >Aggregating data for social linking in the humanities and creative arts:
                        The <title rend="quotes">Humanities Networked Infrastructure
                        (HuNI)</title></title>. <title rend="italic">Signa</title>, 25, pp.
                    109-119</bibl>

                <bibl xml:id="burrows-verhoeven_2023" label="Burrows and Verhoeven 2023"
                    sortKey="Burrowsc">Burrows, T., and Verhoeven, D. (2023) <title rend="quotes"
                        >Serendipity and knowledge organisation</title>. In <title rend="italic"
                        >Serendipity Science: An Emerging Field and Its Methods</title>, 31–48.
                    Springer.</bibl>

                <bibl xml:id="cellard-masure_2018" label="Cellard and Masure 2018">Cellard, L., and
                    Masure, A. (2018) <title rend="quotes" xml:lang="fr">Le design de la
                        transparence: Une rhétorique au cœur des interfaces numériques</title>,
                        <title rend="italic">Multitudes</title> 73(4), pp. 100–111.</bibl>

                <bibl xml:id="cetina_1999" label="Cetina 1999">Cetina, K.K. (1999) <title
                        rend="italic">Epistemic cultures: How the sciences make knowledge</title>.
                    Harvard University Press.</bibl>

                <bibl xml:id="connolly_2020" label="Connolly 2020">Connolly, R. (2020) <title
                        rend="quotes">Why computing belongs within the social sciences</title>,
                        <title rend="italic">Communications of the ACM</title> 63(8), pp. 54–59.
                        <ref target="https://doi.org/10.1145/3383444"
                        >https://doi.org/10.1145/3383444</ref></bibl>

                <bibl xml:id="corbett_2023" label="Corbett 2023">Corbett, J. (2023) <title
                        rend="quotes">ᒫᒥᑐᓀᔨᐦᒋᑲᓂᐦᑳᐣ ᓂᒦᑭᓯᐢᑕᐦᐃᑫᐏᐣ ᐁᑿ ᓂᒥᑐᓀᔨᐦᒋᑲᐣ/Mâmitoneyihcikanihkân
                        Nimı̂kisistahikêwin Ekwa Nimitonêyihcikan: My reflections of beading with a
                        computer</title>, <title rend="italic">Transmotion</title> 9(1&amp;2), pp.
                    86–109.</bibl>

                <bibl xml:id="cordell_2019" label="Cordell 2019">Cordell, R. (2019) <title
                        rend="quotes">Teaching humanistic data analysis</title>, <title
                        rend="italic">Digital Scholarship, Digital Classrooms: New International
                        Perspectives on Research and Teaching: Proceeding of the Gale Digital
                        Humanities Day at the British Library</title>.</bibl>

                <bibl xml:id="crawford_2021" label="Crawford 2021">Crawford, K. (2021) <title
                        rend="italic">The Atlas of AI: Power, Politics, and the Planetary Costs of
                        Artificial Intelligence</title>. Yale University Press.</bibl>

                <bibl xml:id="crouy-chanel_2022" label="Crouy-Chanel 2022">Crouy-Chanel, M. de
                    (2022) <title rend="quotes">Introducing the DHARPA project: An interdisciplinary
                        lab to enable critical DH practice</title>, <title rend="italic">DH Benelux
                        Journal</title>, 29–41. Available at: <ref
                        target="https://journal.dhbenelux.org/journal/issues/004/article-2-Cunningham.html"
                        >https://journal.dhbenelux.org/journal/issues/004/article-2-Cunningham.html</ref></bibl>

                <bibl xml:id="delanda_2019" label="DeLanda 2019">DeLanda, M. (2019) <title
                        rend="italic">A New Philosophy of Society</title>. Bloomsbury
                    Publishing.</bibl>

                <bibl xml:id="dignazio-klein_2020" label="D'Ignazio and Klein 2020">D'Ignazio, C.,
                    and Klein, L. (2020) <title rend="quotes">3. On rational, scientific, objective
                        viewpoints from mythical, imaginary, impossible standpoints</title>. In C.
                    D'Ignazio, C. and L. Klein (eds.) <title rend="italic">Data
                    Feminism</title>.</bibl>

                <bibl xml:id="dijkstra_1984" label="Dijkstra 1984">Dijkstra, E.W. (1984) <title
                        rend="quotes">The threats to computing science</title>.</bibl>

                <bibl xml:id="dixon_2012" label="Dixon 2012">Dixon, D. (2012) <title rend="quotes"
                        >Analysis tool or research methodology: Is there an epistemology for
                        patterns?</title> In D.M. Berry (ed.) <title rend="italic">Understanding
                        Digital Humanities</title>, pp. 191–209. Springer.</bibl>

                <bibl xml:id="dobson_2021" label="Dobson 2021">Dobson, J. (2021) <title
                        rend="quotes">Interpretable outputs: Criteria for machine learning in the
                        humanities</title>, <title rend="italic">DHQ: Digital Humanities
                        Quarterly</title> 15(2). Available at: <ref
                        target="https://dhq.digitalhumanities.org/vol/15/2/000555/000555.html"
                        >https://dhq.digitalhumanities.org/vol/15/2/000555/000555.html</ref></bibl>

                <bibl xml:id="doueihi_2011" label="Doueihi 2011">Doueihi, M. (2011) <title
                        rend="quotes">Un humanisme numérique</title>, <title rend="italic"
                        >Communication &amp; Langages</title> 167(1), pp. 3–15. <ref
                        target="https://doi.org/10.4074/S033615001101101X"
                        >https://doi.org/10.4074/S033615001101101X</ref></bibl>

                <bibl xml:id="drucker_2011" label="Drucker 2011" sortKey="Druckera">Drucker, J.
                    (2011) <title rend="quotes">Humanities approaches to graphical display</title>,
                        <title rend="italic">Digital Humanities Quarterly</title> 5(1), pp. 1–21.
                    Available at: <ref
                        target="https://dhq.digitalhumanities.org/vol/5/1/000091/000091.html"
                        >https://dhq.digitalhumanities.org/vol/5/1/000091/000091.html</ref></bibl>

                <bibl xml:id="drucker_2012" label="Drucker 2012" sortKey="Druckerb">Drucker, J.
                    (2012) <title rend="quotes">Humanistic theory and digital scholarship</title>,
                    in M.K. Gold (ed.) <title rend="italic">Debates in the Digital
                        Humanities</title> pp. 85–95.</bibl>

                <bibl xml:id="drucker_2018" label="Drucker 2018" sortKey="Druckerc">Drucker, J.
                    (2018) <title rend="quotes">Non-representational approaches to modeling
                        interpretation in a graphical environment</title>, <title rend="italic"
                        >Digital Scholarship in the Humanities</title> 33(2), pp. 248–63. <ref
                        target="https://doi.org/10.1093/llc/fqx034"
                        >https://doi.org/10.1093/llc/fqx034</ref></bibl>

                <bibl xml:id="escobar-varela_2021" label="Escobar Varela 2021">Escobar Varela, M.
                    (2021) <title rend="italic">Theater as data: Computational journeys into theater
                        research</title>. University of Michigan Press.</bibl>

                <bibl xml:id="fickers_2022" label="Fickers, Tatarinov and Van Der Heijden 2022"
                    >Fickers, A., Tatarinov, J., and Van Der Heijden, T. (2022) <title rend="quotes"
                        >Digital history and hermeneutics–between theory and practice: An
                        introduction</title>, <title rend="italic">Digital History and Hermeneutics
                        Between Theory and Practice</title>. Berlin, Boston: De Gruyter, pp.
                    1–22.</bibl>

                <bibl xml:id="fish_2012" label="Fish 2012">Fish, S. (2012) <title rend="quotes">Mind
                        your P's and B's: The digital humanities and interpretation</title>, <title
                        rend="italic">New York Times</title> 23(1). Available at: h<ref
                        target="https://archive.nytimes.com/opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/"
                        >ttps://archive.nytimes.com/opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/</ref></bibl>

                <bibl xml:id="franklin_1995" label="Franklin 1995">Franklin, S. (1995) <title
                        rend="quotes">Science as culture, cultures of science</title>, <title
                        rend="italic">Annual Review of Anthropology</title> 24(1), pp. 163–84.
                    Available at: <ref target="https://www.jstor.org/stable/2155934"
                        >https://www.jstor.org/stable/2155934</ref></bibl>

                <bibl xml:id="gambell_2021" label="Gambell et al. 2021">Gambell, S., Gooding, P.,
                    Hughes, L., Doran, M., Murphy, O., Tupman, C., Winters, J., et al. (2021) <title
                        rend="quotes">Communicating the value and impact of digital humanities in
                        teaching, research, and infrastructure development</title>, <title
                        rend="italic">Digital Humanities Research Network</title>. <ref
                        target="https://doi.org/10.31219/osf.io/z8v9c"
                        >https://doi.org/10.31219/osf.io/z8v9c</ref></bibl>

                <bibl xml:id="gawne_2014" label="Gawne 2014">Gawne, L. (2014) <title rend="quotes"
                        >Evidentiality in Lamjung Yolmo</title>, <title rend="italic">Journal of the
                        Southeast Asian Linguistics Society</title> 7, pp. 76–96.</bibl>

                <bibl xml:id="gebru_2021" label="Gebru et al. 2021">Gebru, T., Morgenstern, J.,
                    Vecchione, B., Vaughan, J.W., Wallach, H., Daumé, H., III, and Crawford, K.
                    (2021) <title rend="quotes">Datasheets for datasets</title>, <title
                        rend="italic">Communications of the ACM</title> 64(12), pp. 86–92. <ref
                        target="https://doi.org/10.1145/3458723"/></bibl>

                <bibl xml:id="ginzburg_1979" label="Ginzburg 1979">Ginzburg, C. (1979) <title
                        rend="quotes">Clues: Roots of a scientific paradigm</title>, <title
                        rend="italic">Theory and Society</title> 7(3), pp. 273–88. Available at:
                        <ref target="https://www.jstor.org/stable/656747"
                        >https://www.jstor.org/stable/656747</ref></bibl>

                <bibl xml:id="gitelman_2008" label="Gitelman 2008">Gitelman, L. (2008) <title
                        rend="italic">Always already new: Media, history, and the data of
                        culture</title>. MIT Press.</bibl>

                <bibl xml:id="golebiewski-boyd_2019" label="Golebiewski and boyd 2019">Golebiewski,
                    M., and boyd, d. (2019) <title rend="quotes">Data voids: Where missing data can
                        easily be exploited</title>, <title rend="italic">Data &amp; Society
                        Research Institute</title>. Available at: <ref
                        target="https://datasociety.net/library/data-voids-where-missing-data-can-easily-be-exploited/"
                        >https://datasociety.net/library/data-voids-where-missing-data-can-easily-be-exploited/</ref></bibl>

                <bibl xml:id="gooding_2023" label="Gooding 2023">Gooding, P. (2023) <title
                        rend="quotes">Informational abundance and material absence in the digitised
                        early modern press: The case for contextual digitisation</title>, <title
                        rend="italic">The Edinburgh History of the British and Irish Press</title>,
                    Edinburgh University Press, Edinburgh, Beginnings and Consolidation,
                    1640–1800.</bibl>

                <bibl xml:id="greenberger_1962" label="Greenberger and others 1962">Greenberger, M.,
                    and others (1962) <title rend="italic">Management and the computer of the
                        future</title>. M.I.T Press &amp; Wiley, New York.</bibl>

                <bibl xml:id="haraway_1988" label="Haraway 1988">Haraway, D. (1988) <title
                        rend="quotes">Situated knowledges: The science question in feminism and the
                        privilege of partial perspective</title>, <title rend="italic">Feminist
                        Studies</title> 14 (3): 575–99. <ref
                        target="http://www.jstor.org/stable/3178066"
                        >http://www.jstor.org/stable/3178066</ref>.</bibl>

                <bibl xml:id="havens_2024" label="Havens 2024">Havens, L.J. (2024) <title
                        rend="quotes">Recalibrating machine learning for social biases:
                        Demonstrating a new methodology through a case study classifying gender
                        biases in archival documentation</title>. PhD thesis, University of
                    Edinburgh.</bibl>

                <bibl xml:id="hinrichs_2019" label="Hinrichs, Forlini, and Moynihan 2019">Hinrichs,
                    U., Forlini, S., and Moynihan, B. (2019) <title rend="quotes">In defense of
                        sandcastles: Research thinking through visualization in digital
                        humanities</title>, <title rend="italic">Digital Scholarship in the
                        Humanities</title> 34(Supplement_1): pp. i80–i99. <ref
                        target="https://doi.org/10.1093/llc/fqy051"
                        >https://doi.org/10.1093/llc/fqy051</ref></bibl>

                <bibl xml:id="johnson_2018" label="Johnson 2018">Johnson, J.M. (2018) <title
                        rend="quotes">Markup bodies: Black [life] studies and slavery [death]
                        studies at the digital crossroads</title>, <title rend="italic">Social
                        Text</title> 36(4), pp. 57–79. <ref
                        target="https://doi.org/10.1215/01642472-7145658"
                        >https://doi.org/10.1215/01642472-7145658</ref></bibl>

                <bibl xml:id="klein_2013" label="Klein 2013">Klein, L.F. (2013) <title rend="quotes"
                        >The image of absence: Archival silence, data visualization, and James
                        Hemings</title>, <title rend="italic">American Literature</title> 85(4), pp.
                    661–88. <ref target="https://doi.org/10.1215/00029831-2367310">https://doi.org/10.1215/00029831-2367310</ref></bibl>

                <bibl xml:id="kleymann-stange_2021" label="Kleymann and Stange 2021">Kleymann, R.,
                    and Stange, J.-E. (2021) <title rend="quotes">Towards hermeneutic visualization
                        in digital literary studies</title>, <title rend="italic">Digital Humanities
                            Quarterly</title> 15(2). Available at: <ref target="https://dhq.digitalhumanities.org/vol/15/2/000547/000547.html">https://dhq.digitalhumanities.org/vol/15/2/000547/000547.html</ref></bibl>

                <bibl xml:id="ko_2020" label="Ko et al. 2020">Ko, A.J., Oleson, A., Ryan, N.,
                    Register, Y., Xie, B., Tari, M., Davidson, M., Druga, S., and Loksa, D. (2020)
                        <title rend="quotes">It is time for more critical CS education</title>,
                        <title rend="italic">Communications of the ACM</title> 63(11), pp.
                    31–33. <ref target="https://doi.org/10.1145/342400">https://doi.org/10.1145/342400</ref></bibl>

                <bibl xml:id="koeser-leblanc_2024" label="Koeser and LeBlanc 2024">Koeser, R.S., and
                    LeBlanc, Z. (2024) <title rend="quotes">Missing data, speculative
                        reading</title>, <title rend="italic">Journal of Cultural Analytics</title>
                    9(2). <ref target="https://doi.org/10.22148/001c.116926">https://doi.org/10.22148/001c.116926</ref></bibl>

                <bibl xml:id="kramsch_2014" label="Kramsch 2014">Kramsch, C. (2014) <title
                        rend="quotes">Language and culture</title>, <title rend="italic">AILA
                            Review</title> 27(1), pp. 30–55. <ref target="https://doi.org/10.1075/aila.27.02kra">https://doi.org/10.1075/aila.27.02kra</ref></bibl>

                <bibl xml:id="ladson-billings_1997" label="Ladson-Billings 1997">Ladson-Billings, G.
                    (1997) <title rend="quotes">It doesn't add up: African American students'
                        mathematics achievement</title>, <title rend="italic">Journal for Research
                            in Mathematics Education</title> 28(6), pp. 697–708. h<ref target="https://doi.org/10.2307/749638">ttps://doi.org/10.2307/749638</ref></bibl>

                <bibl xml:id="lamqaddam_2018" label="Lamqaddam et al. 2018">Lamqaddam, H., Brosens,
                    K., Truyen, F., Beerens, R.J., De Prekel, I., and Verbert, K. (2018) <title
                        rend="quotes">When the tech kids are running too fast: Data visualisation
                        through the lens of art history research</title>, <title rend="italic">IEEE
                        Transactions on Visualization and Computer Graphics</title>.</bibl>

                <bibl xml:id="latour_1990" label="Latour 1990" sortKey="Latoura">Latour, B. (1990)
                        <title rend="quotes">Technology is society made durable</title>, <title
                        rend="italic">The Sociological Review</title> 38(S1), pp. 103–31.</bibl>

                <bibl xml:id="latour-lowe_2011" label="Latour and Lowe 2011" sortKey="Latourb"
                    >Latour, B., and Lowe, A. (2011) <title rend="quotes">The migration of the aura
                        exploring the original through its fac similes</title>, <title rend="italic"
                        >Switching Codes Thinking Through Digital Technology in the Humanities and
                        the Arts</title>, pp 275–97.</bibl>

                <bibl xml:id="lee_2025" label="Lee et al. 2025" sortKey="Leea">Lee, H.-P., Sarkar,
                    A., Tankelevitch, L., Drosos, I., Rintel, S., Banks, R., and Wilson, N. (2025)
                        <title rend="quotes">The impact of generative AI on critical thinking:
                        Self-reported reductions in cognitive effort and confidence effects from a
                        survey of knowledge workers</title>. In <title rend="italic">Proceedings of
                        the 2025 CHI Conference on Human Factors in Computing Systems</title>. CHI
                    '25. New York, NY, USA: Association for Computing Machinery. <ref
                        target="https://doi.org/10.1145/3706598.3713778"
                        >https://doi.org/10.1145/3706598.3713778</ref>.</bibl>

                <bibl xml:id="lee_1993" label="Lee 1993" sortKey="Leeb">Lee, S.M. (1993) <title
                        rend="quotes">Racial classifications in the US census: 1890–1990</title>,
                    <title rend="italic">Ethnic and Racial Studies</title> 16(1), pp. 75–94. <ref target="https://doi.org/10.1080/01419870.1993.9993773">https://doi.org/10.1080/01419870.1993.9993773</ref></bibl>

                <bibl xml:id="long-so_2016" label="Long and So 2016">Long, H., and So, R.J. (2016)
                        <title rend="quotes">Literary pattern recognition: Modernism between close
                        reading and machine learning</title>, <title rend="italic">Critical
                            Inquiry</title> 42(2), pp. 235–67. <ref target="https://doi.org/10.1086/684353">https://doi.org/10.1086/684353</ref></bibl>

                <bibl xml:id="manovich_2013" label="Manovich 2013">Manovich, L. (2013) <title
                        rend="quotes">Database as symbolic form</title>, in R. Perry (ed.) <title rend="italic"
                        >Museums in a Digital Age</title>, pp. 64–71. Routledge.</bibl>

                <bibl xml:id="markus_1987" label="Markus 1987">Markus, G. (1987) <title
                        rend="quotes">Why is there no hermeneutics of natural sciences? Some
                        preliminary theses</title>, <title rend="italic">Science in Context</title>
                    1(1), pp. 5–51.</bibl>

                <bibl xml:id="martin_2013" label="Martin 2013">Martin, D.B. (2013) <title
                        rend="quotes">Race, racial projects, and mathematics education</title>,
                        <title rend="italic">Journal for Research in Mathematics Education</title>
                    44(1), pp. 316–33. <ref target="https://doi.org/10.5951/jresematheduc.44.1.0316">https://doi.org/10.5951/jresematheduc.44.1.0316</ref></bibl>

                <bibl xml:id="masson_2017" label="Masson 2017">Masson, E. (2017) <title
                        rend="quotes">Humanistic data research: An encounter between epistemic
                        traditions</title>. In <title rend="italic">The Datafied Society: Studying
                        Culture Through Data</title>, 25–38. Amsterdam University Press. <ref
                        target="http://www.jstor.org/stable/j.ctt1v2xsqn.6"
                        >http://www.jstor.org/stable/j.ctt1v2xsqn.6</ref>.</bibl>

                <bibl xml:id="masure_2018" label="Masure 2018">Masure, A. (2018) <title
                        rend="quotes" xml:lang="fr">Vers des humanités numériques critiques</title>. Repéré à
                    Dlis. Hypotheses. Org/2088.</bibl>

                <bibl xml:id="mccarty_2005" label="McCarty 2005" sortKey="Mccarty">McCarty, W.
                    (2005) <title rend="italic">Humanities computing</title>. Springer.</bibl>

                <bibl xml:id="mcluhan_1994" label="McLuhan 1994" sortKey="Mcluhan">McLuhan, M.
                    (1994) <title rend="italic">Understanding media: The extensions of man</title>.
                    MIT Press.</bibl>

                <bibl xml:id="meister_2017" label="Meister, Drucker and Rockwell 2017">Meister,
                    J.C., Drucker, J., and Rockwell, G. (2017) <title rend="quotes">Modelling
                        interpretation in 3DH: New dimensions of visualization</title>. In Digital
                    Humanities Conference 2017. Available at: <ref target="https://dh2017.adho.org/abstracts/058/058.pdf">https://dh2017.adho.org/abstracts/058/058.pdf</ref></bibl>

                <bibl xml:id="minh_2022" label="Minh et al. 2022">Minh, D., Wang, H.X., Li, Y.F.,
                    and Nguyen, T.N. (2022) <title rend="quotes">Explainable artificial
                        intelligence: A comprehensive review</title>, <title rend="italic"
                        >Artificial Intelligence Review</title>, pp. 1–66. <ref
                        target="https://doi.org/10.1007/s10462-021-10088-y"
                        >https://doi.org/10.1007/s10462-021-10088-y</ref></bibl>

                <bibl xml:id="mitchell_2019" label="Mitchell et al. 2019">Mitchell, M., Wu, S.,
                    Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji,
                    I.D., and Gebru, T. (2019) <title rend="quotes">Model cards for model
                        reporting</title>. In <title rend="italic">Proceedings of the Conference on
                        Fairness, Accountability, and Transparency</title>, pp. 220–29. <ref
                        target="https://doi.org/10.48550/arXiv.1810.03993"
                        >https://doi.org/10.48550/arXiv.1810.03993</ref></bibl>

                <bibl xml:id="munzner_2014" label="Munzner 2014">Munzner, T. (2014) <title
                        rend="italic">Visualization analysis and design</title>. CRC Press.</bibl>

                <bibl xml:id="nyhan-passarotti_2019" label="Nyhan and Passarotti 2019">Nyhan, J.,
                    and Passarotti, M. (2019) <title rend="italic">One origin of Digital Humanities:
                        Fr Roberto Busa in his own words</title>. Springer.</bibl>

                <bibl xml:id="parker_2015" label="Parker et al. 2016">Parker, K., Horowitz, J.M.,
                    Morin, R., and Lopez, M.H. (2015) <title rend="quotes">Race and multiracial
                        Americans in the US census</title>, <title rend="italic">Pew Research
                        Center</title>. Available at: <ref
                        target="https://www.pewresearch.org/social-trends/2015/06/11/chapter-1-race-and-multiracial-americans-in-the-u-s-census/"
                        >https://www.pewresearch.org/social-trends/2015/06/11/chapter-1-race-and-multiracial-americans-in-the-u-s-census/</ref></bibl>

                <bibl xml:id="piper_2016" label="Piper 2016" sortKey="Pipera">Piper, A. (2016)
                        <title rend="quotes">There will be numbers</title>, <title rend="italic"
                        >Journal of Cultural Analytics</title> 1 (1).</bibl>

                <bibl xml:id="piper_2022" label="Piper 2022" sortKey="Piperb">Piper, A. (2022)
                        <title rend="quotes">Biodiversity is not declining in fiction</title>,
                        <title rend="italic">Journal of Cultural Analytics</title> 7(3). <ref
                        target="https://doi.org/10.22148/001c.38739"
                        >https://doi.org/10.22148/001c.38739</ref></bibl>

                <bibl xml:id="porter_1995" label="Porter 1995">Porter, T.M. (1995) <title
                        rend="italic">Trust in numbers: The pursuit of objectivity in science and
                        public life</title>. Princeton University Press. Available at: <ref
                        target="http://www.jstor.org/stable/j.ctt7sp8x"
                        >http://www.jstor.org/stable/j.ctt7sp8x</ref></bibl>

                <bibl xml:id="posner_2015" label="Posner 2015">Posner, M. (2015) <title
                        rend="quotes">What's next: The radical, unrealized potential of digital
                        humanities</title>, <title rend="italic">Miriam Posner's Blog</title>.
                    Available at: <ref
                        target="https://miriamposner.com/blog/whats-next-the-radical-unrealized-potential-of-digital-humanities/"
                        >https://miriamposner.com/blog/whats-next-the-radical-unrealized-potential-of-digital-humanities/</ref></bibl>

                <bibl xml:id="potter_2012" label="Potter, Rosen, and Johnson 2012">Potter, K.,
                    Rosen, P., and Johnson, C.R. (2012) <title rend="quotes">From quantification to
                        visualization: A taxonomy of uncertainty visualization approaches</title>.
                    In <title rend="italic">Uncertainty Quantification in Scientific Computing: 10th
                        IFIP WG 2.5 Working Conference, WoCoUQ 2011, Boulder, CO, USA, August 1-4,
                        2011, Revised Selected Papers</title>, pp. 226–49. Springer. <ref
                        target="https://doi.org/10.1007/978-3-642-32677-6_15"
                        >https://doi.org/10.1007/978-3-642-32677-6_15</ref></bibl>

                <bibl xml:id="presner_2010" label="Presner 2010">Presner, T. (2010) <title
                        rend="quotes">Digital humanities 2.0: A report on knowledge</title>.
                    Citeseer. </bibl>

                <bibl xml:id="quamen_2012" label="Quamen 2012">Quamen, H. (2012) <title
                        rend="quotes">The limits of modelling: Data culture and the
                        humanities</title>, <title rend="italic">Scholarly and Research
                        Communication</title> 3(4). <ref
                        target="https://doi.org/10.22230/src.2012v3n4a69 "
                        >https://doi.org/10.22230/src.2012v3n4a69</ref></bibl>

                <bibl xml:id="ramsay_2010" label="Ramsay 2010" sortKey="Ramsaya">Ramsay, S. (2010)
                        <title rend="quotes">The hermeneutics of screwing around; or what you do
                        with a million books</title>, in K. Khee (ed.) <title rend="italic"
                        >Pastplay: Teaching and Learning History with Technology</title>, pp.
                    111–20.</bibl>

                <bibl xml:id="ramsay_2011" label="Ramsay 2011" sortKey="Ramsayb">Ramsay, S. (2011)
                        <title rend="italic">Reading machines: Toward and algorithmic
                        criticism</title>. University of Illinois Press.</bibl>

                <bibl xml:id="ramsay-rockwell_2012" label="Ramsay and Rockwell 2012"
                    sortKey="Ramsayc">Ramsay, S., and Rockwell, G. (2012) <title rend="quotes"
                        >Developing things: Notes toward an epistemology of building in the digital
                        humanities</title>, in M.K. Gold (ed) <title rend="italic">Debates in the
                        Digital Humanities</title>. Available at: <ref
                        target="https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/c733786e-5787-454e-8f12-e1b7a85cac72"
                        >https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/c733786e-5787-454e-8f12-e1b7a85cac72</ref></bibl>

                <bibl xml:id="reed_2019" label="Reed 2019">Reed, E. (2019) <title rend="quotes"
                        >Forms of frustration: Unrest and unfulfillment in American literature after
                        1934</title>. PhD thesis, University of Virginia.</bibl>

                <bibl xml:id="risam_2019" label="Risam 2019">Risam, R. (2019) <title rend="quotes"
                        >Beyond the migrant <q>problem</q>: Visualizing global migration</title>,
                        <title rend="italic">Television &amp; New Media</title> 20(6), pp. 566–80.
                        <ref target="https://doi.org/10.1177/1527476419857679"
                        >https://doi.org/10.1177/1527476419857679</ref></bibl>

                <bibl xml:id="scheuerman_2020" label="Scheuerman et al. 2020">Scheuerman, M.K.,
                    Wade, K., Lustig, C., and Brubaker, J.R. (2020) <title rend="quotes">How we've
                        taught algorithms to see identity: Constructing race and gender in image
                        databases for facial analysis</title>, <title rend="italic">Proceedings of
                        the ACM on Human-Computer Interaction</title> 4(CSCW1). pp. 1–35. <ref
                        target="https://doi.org/10.1145/3392866"
                        >https://doi.org/10.1145/3392866</ref></bibl>

                <bibl xml:id="schickore_2005" label="Schickore 2005">Schickore, J. (2005) <title
                        rend="quotes"><q>Through thousands of errors we reach the truth</q> — but
                        how? On the epistemic roles of error in scientific practice</title>, <title
                        rend="italic">Studies in History and Philosophy of Science Part A</title> 36
                    (3), pp. 539–56. <ref target="https://doi.org/10.1016/j.shpsa.2005.06.011"
                        >https://doi.org/10.1016/j.shpsa.2005.06.011</ref></bibl>

                <bibl xml:id="sculley-pasanek_2008" label="Sculley and Pasanek 2008">Sculley, D.,
                    and Pasanek, B.M. (2008) <title rend="quotes">Meaning and mining: The impact of
                        implicit assumptions in data mining for the humanities</title>, <title
                        rend="italic">Literary and Linguistic Computing</title> 23(4), pp. 409–24.
                        <ref target="https://doi.org/10.1093/llc/fqn019"
                        >https://doi.org/10.1093/llc/fqn019</ref></bibl>

                <bibl xml:id="sinclair-gouglas_2002" label="Sinclair and Gouglas 2002"
                    sortKey="Sinclaira">Sinclair, S., and Gouglas, S.W. (2002) <title rend="quotes"
                        >Theory into practice: A case study of the humanities computing master of
                        arts programme at the University of Alberta</title>, <title rend="italic"
                        >Arts and Humanities in Higher Education</title> 1(2), pp. 167–83. <ref
                        target="https://doi.org/10.1177/147402220200100200"
                        >https://doi.org/10.1177/147402220200100200</ref></bibl>

                <bibl xml:id="sinclair_2013" label="Sinclair et al. 2013" sortKey="Sinclairb"
                    >Sinclair, S., Ruecker, S., Radzikowska, M., and Inke, I. (2013) <title
                        rend="quotes">Information visualization for humanities scholars</title>,
                        <title rend="italic">Literary Studies in the Digital Age-an Evolving
                        Anthology</title>.</bibl>

                <bibl xml:id="smith_1985" label="Smith 1985">Smith, B.C. (1985) <title rend="quotes"
                        >The limits of correctness</title>, <title rend="italic">ACM SIGCAS
                        Computers and Society</title> 14(1, 2, 3, 4), pp. 18–26. <ref
                        target="https://doi.org/10.1145/379486.37951"
                        >https://doi.org/10.1145/379486.37951</ref></bibl>

                <bibl xml:id="snow_1959" label="Snow 1959">Snow, C.P. (1959) <title rend="quotes"
                        >Two cultures</title>, <title rend="italic">Science</title> 130(3373), pp.
                    419–19. <ref target="https://doi.org/10.1126/science.130.3373.419"
                        >https://doi.org/10.1126/science.130.3373.419</ref></bibl>

                <bibl xml:id="sollazzo_2022" label="Sollazzo 2022">Sollazzo, A. (2022) <title
                        rend="quotes">Embracing the friction: Towards a computationally aware
                        approach to humanistic data interfaces</title>. Master's thesis, University
                    of Alberta.</bibl>

                <bibl xml:id="tiefenbach-keller_2024" label="Tiefenbach Keller 2024">Tiefenbach
                    Keller, S. (2024) <title rend="quotes">Towards a culture of replication in the
                        digital humanities: An understanding of transparency and openness practices
                        in published DH papers</title>. PhD thesis, McGill University.</bibl>

                <bibl xml:id="tufte-graves-morris_1983" label="Tufte and Graves-Morris 1983">Tufte,
                    E.R., and Graves-Morris, P.R. (1983) <title rend="italic">The visual display of
                        quantitative information</title>. Graphics Press Cheshire, CT.</bibl>

                <bibl xml:id="turkle-papert_1990" label="Turkle and Papert 1990">Turkle, S., and
                    Papert, S. (1990) <title rend="quotes">Epistemological pluralism: Styles and
                        voices within the computer culture</title>, <title rend="italic">Signs:
                        Journal of Women in Culture and Society</title> 16(1), pp. 128–57. Available
                        at:<ref target="https://www.jstor.org/stable/3174610">
                        https://www.jstor.org/stable/3174610</ref></bibl>

                <bibl xml:id="underwood_2014" label="Underwood 2014">Underwood, T. (2014) <title
                        rend="quotes">Theorizing research practices we forgot to theorize twenty
                        years ago</title>, <title rend="italic">Representations</title> 127(1), pp.
                    64–72. <ref target="https://doi.org/10.1525/rep.2014.127.1.64"
                        >https://doi.org/10.1525/rep.2014.127.1.64</ref></bibl>

                <bibl xml:id="van-es_2018" label="Van Es, Wieringa, and Schäfer 2018"
                    sortKey="Vanes">Van Es, K., Wieringa, M., and Schäfer, M.T. (2018) <title
                        rend="quotes">Tool criticism: From digital methods to digital
                        methodology</title>. In <title rend="italic">Proceedings of the 2nd
                        International Conference on Web Studies</title>, pp. 24–27. <ref
                        target="https://doi.org/10.1145/3240431.3240436"
                        >https://doi.org/10.1145/3240431.3240436</ref></bibl>

                <bibl xml:id="van-zundert_2012" label="Van Zundert et al. 2012"
                    sortKey="Vanzunderta">Van Zundert, J., Antonijevic, S., Beaulieu, A., van
                    Dalen-Oskam, K., Zeldenrust, D., and Andrews, T.L. (2012) <title rend="quotes"
                        >Cultures of formalisation: Towards an encounter between humanities and
                        computing</title>, in D.M. Berry (ed.) <title rend="italic">Understanding
                        Digital Humanities</title>, pp. 279–94. Springer. <ref
                        target="https://doi.org/10.1057/9780230371934_15"
                        >https://doi.org/10.1057/9780230371934_15</ref></bibl>

                <bibl xml:id="van-zundert_2015" label="Van Zundert 2015" sortKey="Vanzundertb">Van
                    Zundert, J.J. (2015) <title rend="quotes">Screwmeneutics and hermenumericals:
                        The computationality of hermeneutics</title>, in S. Schreibman, R. Siemens,
                    and J. Unsworth (eds.) <title rend="italic">A New Companion to Digital
                        Humanities</title>, pp. 331–47. <ref
                        target="https://doi.org/10.1002/9781118680605"
                        >https://doi.org/10.1002/9781118680605</ref>
                </bibl>

                <bibl xml:id="verhoeven_2020" label="Verhoeven et al. 2020">Verhoeven, D., Musial,
                    K., Palmer, S., Taylor, S., Abidi, S., Zemaityte, V., and Simpson, L. (2020)
                        <title rend="quotes">Controlling for openness in the male-dominated
                        collaborative networks of the global film industry</title>, <title
                        rend="italic">PloS One</title> 15(6): e0234460. <ref
                        target="https://doi.org/10.1371/journal.pone.0234460"
                        >https://doi.org/10.1371/journal.pone.0234460</ref></bibl>

                <bibl xml:id="walter_2018" label="Walter 2018" sortKey="Waltera">Walter, M. (2018)
                        <title rend="quotes">The voice of indigenous data: Beyond the markers of
                        disadvantage</title>, <title rend="italic">Griffith Review</title>, 60,
                    256–63. Available at: <ref
                        target="https://www.griffithreview.com/articles/voice-indigenous-data-beyond-disadvantage/"
                        >https://www.griffithreview.com/articles/voice-indigenous-data-beyond-disadvantage/</ref></bibl>

                <bibl xml:id="walter-andersen_2013" label="Walter and Andersen 2013"
                    sortKey="Walterb">Walter, M., and Andersen, C. (2013) <title rend="italic"
                        >Indigenous Statistics: A Quantitative Research Methodology</title>. Taylor
                    &amp; Francis.</bibl>

                <bibl xml:id="walter-suina_2019" label="Walter and Suina 2019" sortKey="Walterc"
                    >Walter, M., and Suina, M. (2019) <title rend="quotes">Indigenous data,
                        indigenous methodologies and indigenous data sovereignty</title>, <title
                        rend="italic">International Journal of Social Research Methodology</title>
                    22(3), pp. 233–43. <ref target="https://doi.org/10.1080/13645579.2018.1531228"
                        >https://doi.org/10.1080/13645579.2018.1531228</ref></bibl>

                <bibl xml:id="witmore_2010" label="Witmore 2010">Witmore, M. (2010) <title
                        rend="quotes">Text: A massively addressable object</title>, <title
                        rend="italic">Wine Dark Sea</title> 31. Available at: <ref
                        target="https://winedarksea.org/?p=926"
                    >https://winedarksea.org/?p=926</ref></bibl>

                <bibl xml:id="yoder-himes_2022" label="Yoder-Himes et al. 2022">Yoder-Himes, D.R.,
                    Asif, A., Kinney, K., Brandt, T.J., Cecil, R.E., Himes, P.R., Cashon, C., Hopp,
                    R.M.P., and Ross, E. (2022) <title rend="quotes">Racial, skin tone, and sex
                        disparities in automated proctoring software</title>. In <title
                        rend="italic">Frontiers in Education</title>, 7:881449. <ref
                        target="https://doi.org/10.3389/feduc.2022.881449"
                        >https://doi.org/10.3389/feduc.2022.881449</ref></bibl>

                <bibl xml:id="young_1987" label="Young, Morgan, and Young 1987">Young, R.W., Morgan,
                    W., and Young, R.W. (1987) <title rend="italic">The Navajo Language: A Grammar
                        and Colloquial Dictionary</title>. University of New Mexico Press
                    Albuquerque.</bibl>
            </listBibl>

        </back>
    </text>
</TEI>
