<?xml version="1.0" encoding="UTF-8"?><?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?><?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en">Continuous Integration and Unit Testing of
                    Digital Editions</title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Bridget <dhq:family>Almas</dhq:family></dhq:author_name>
                    <dhq:affiliation>The Alpheios Project, Ltd.</dhq:affiliation>
                    <email>balmas@gmail.com</email>
                    <dhq:bio>
                        <p><ref target="https://orcid.org/0000-0001-7556-1572">Bridget Almas</ref>
                            is currently the lead software developer and architect for The Alpheios
                            Project, developing open source tools for the study and enjoyment of
                            classical languages. In her prior role at Tufts University, Bridget was
                            the technical lead on the Perseids Project and before that the Perseus
                            Digital Library. She has also acted in several leadership roles in the
                            Research Data Allliance, serving as an elected member of the Technical
                            Advisory Board from 2013-2015, and as co-chair of the Research Data
                            Collections Working Group, the Data Fabric Interest Group, and as a
                            liaison between the Alliance of Digital Humanities Organizations (ADHO)
                            and RDA. Bridget also has a background in the study of foreign
                            languages, including French and Mandarin Chinese.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Thibault <dhq:family>Clérice</dhq:family></dhq:author_name>
                    <dhq:affiliation>Centre Jean-Mabillon (École des chartes) -
                        PSL</dhq:affiliation>
                    <email>tthibault.clerice@enc-sorbonne.fr</email>
                    <dhq:bio>
                        <p><ref target="http://orcid.org/0000-0003-1852-9204">Thibault Clérice</ref>
                            is the head of the MA <title rend="quotes">Digital Technologies Applied
                                to History</title> (Technologies Numériques Appliquées à l’Histoire)
                            at the École Nationale des Chartes (Paris, France). He is a classicist
                            who served as an engineer both at the Centre for eResearch (Kings
                            College London, UK) and the Humboldt Chair for Digital Humanities
                            (Leipzig, Germany) where he developed the data backbone of the future
                            Perseus 5 (under the CapiTainS.org project). His main interests lie in
                            data and software sustainability and Latin data mining.</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>

                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id">000350</idno>
                <idno type="volume">011</idno>
                <idno type="issue">4</idno>
                <date when="2018-01-22">22 January 2018</date>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref target="http://www.digitalhumanities.org/dhq/taxonomy.xml">http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>continuous integration, unit test, test, edition, repository, open source,
                        object oriented programming</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item>continuous integration</item>
                        <item>unit test</item>
                        <item>test</item>
                        <item>edition</item>
                        <item>repository</item>
                        <item>open source</item>
                        <item>object oriented programing</item>
                        <item>curation</item>
                        <item>tei</item>
                        <item>epidoc</item>
                        <item>Open Philology</item>
                        <item>Capitains</item>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change when="2018-07-03" who="DD">Validation Check</change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <p>Over the last few years, the Perseus Digital Library (PDL) and the Open Philology
                    Project (OPP) have been moving towards enabling better interoperability and
                    citability of their texts by implementing the Canonical Text Services URN
                    standard and the Epidoc subset of the TEI P5 guidelines. This is a
                    resource-intensive effort necessitating a scalable workflow centered on
                    continuous curation of these texts, from both within and outside the PDL/OPP
                    ecosystem. Key requirements for such a workflow are ease of maintenance and
                    speed of deployment of texts for use by a wide variety of analytical services
                    and user interfaces. Drawing on software engineering best practices, we have
                    designed an architecture meant for continuous integration with customizable
                    services that test individual files upon each contribution made to our public
                    git repositories. The services can be configured to test and report status on a
                    variety of checkpoints from schema compliance to CTS-ready markup designed for
                    flexibility and interoperability.</p>
            </dhq:abstract>
            <dhq:teaser>
                <p>Describes a suite of tools, services and guidelines to support a continous
                    integration workflow for curation and publication of CTS-compliant digital
                    editions.</p>
            </dhq:teaser>
        </front>
        <body>
            <head>Continuous Integration and Unit Testing of Digital Editions</head>
            <div>
                <head>Introduction</head>
                <p>In 2012, the <title>Perseus Digital Library (PDL)</title>
                    <ptr target="#perseuscts"/> decided to apply a nascent norm in the digital
                    classics world, the <term>Canonical Text Services</term> (CTS) protocol <ptr target="#cts"/>, to its corpus of primary source Greek and Latin texts (<ref target="#figure01">see Figure 1</ref>). This effort coincided with a rather
                    aggressive <term>Optical Character Recognition</term> (OCR) campaign by its
                    sister project, the <title>Open Philology Project</title> (OPP) in Leipzig,
                    aimed <quote rend="inline" source="#opp">at providing at least one version for all Greek and
                        Latin sources produced during antiquity</quote>. Through this effort OPP is
                    adding thousands of new Greek and Latin texts to open access repositories, with
                    a focus on post-classical corpora available online <ptr target="#opp"/>. With
                    hundreds of pre-existing <title>PDL</title> texts needing to be made
                        <term>CTS</term> compliant as well as upgraded from the <term>Text Encoding
                        Initiative (TEI) P4</term> Guidelines <ptr target="#teip4"/> to the
                        <term>Epidoc</term>
                    <ptr target="#epidoc"/> subset of <term>TEI P5</term>
                    <ptr target="#teip5"/>, together with the incoming hundreds or thousands of
                    texts coming out of the <title>OPP</title> pipeline, the work of a curator would
                    require much tedious checking of technical details.</p>
                <p>In addition, management of resources needs to be scaled within the context of a
                    non-uniform corpus. Both the conversion process of pre-existing <term>TEI
                        XML</term> files and the integration of new files needs to be validated
                    against the agreed upon norms. While the <term>TEI</term> norm and any of its
                    subsets are a good first step towards unification of resources, norms like
                        <term>CTS</term> and digital libraries like <title>Perseus</title> require
                    some specific technical solutions that can be both scalable and
                    cost-efficient.</p>
                <figure xml:id="figure01">
                    <head>CTS API Requests Explanation Diagram</head>
                    <graphic url="./resources/images/figure01.png"/>
                </figure>
            </div>
            <div>
                <head>Motivating Factors, Decisions and History</head>
                <p>Lessons learned from the long history of managing the Perseus corpus and its
                    supporting applications drove some of the technical decisions of this project.
                    Ingesting new and updated texts in the legacy <title>Perseus 4.0</title>
                    application is a tedious process. <title>Perseus 4.0</title> is a traditional
                    3-tier Java web application which is deployed under <term>Tomcat</term>. The
                    views it presents the users combine the results of relational database queries
                    of a <term>MySQL</term> database <ptr target="#mysql"/> with static data served
                    directly from the filesystem. Much of the runtime analytical functionality
                    (frequency calculations, word lookups, entity identification) relies on textual
                    data being parsed and pre-loaded into tables in the supporting
                        <term>MySQL</term> database. The binary offset location of text within the
                        <term>XML</term> source files is used to synchronize the relational data
                    with the <term>XML</term> source. This tight coupling between application code,
                    database and raw data means that any time a text is touched, the entire database
                    needs to be reloaded. A more scalable solution was needed that would enable
                    Perseus to serve new and updated texts in real time as they became available,
                    with the confidence that they would work correctly and not break other parts of
                    the application. This requires a distributed architecture. Implementation of
                        <term>CTS</term> is one step in this direction, as it allows us to identify
                    and serve text passages by their canonical identifiers, using persistent stable
                    identifiers and a technology-independent <term>API</term>.</p>
                <p>Another primary objective for the <title>Perseus</title> and <title>OPP</title>
                    projects is to provide a fully open-access and self-describing corpus of texts
                    which can stand on its own and support a wide variety of scholarly needs. Any
                    solution which embeds knowledge of text content or structure in software
                    application or database code is antithetical to this goal.</p>
            </div>
            <div>
                <head>Structural Markup Guidelines</head>
                <p>As previously noted, the <term>CTS</term> service protocol allows us to identify
                    and serve text passages by their canonical identifiers, using persistent stable
                    identifiers and a technology-independent API. The <term>CTS URN</term> notation
                    is based on a strict hierarchical concept of the text, where its passages are
                    sub-ordered down to the word level with no limitations applied to the depth of
                    the passages tree. In this context, <term>XML</term> fits the technical
                    requirements. But to implement <term>CTS</term> we must decide upon a single
                        <soCalled>canonical</soCalled> hierarchical <term>XML</term> markup
                    structure for each text. External indices and transformations can be used to
                    present alternative schemes or visualizations, in addition to or instead of
                    relying upon embedded milestones to deal with issues of overlapping citation
                    hierarchies.</p>
                <div>
                    <head>From scholarly tradition to <term>XML</term> encoding</head>
                    <p>Most scholarly tradition is easily transferred from text to tree <ptr target="#ohco"/>: hierarchical models of lines, verses, books or
                        chapters are easily expressed using traditional <term>TEI</term><note> Tree,
                            or as put by <ptr target="#ohco"/>
                            <title rend="quotes">Ordered hierarchy of content objects
                            (OHCO)</title>, is a model that many texts of western classical
                            literature can fit. This modelization is the same that supports the real
                            bases of <term>TEI</term>. See <title rend="quotes">Complicating the
                                Issue</title> in the <title rend="italic">TEI Guidelines</title>
                            <ptr target="#complicating"/></note>. Verses (in the context of
                            <date>Antiquity</date>, poetry and theater) and paragraph-based citation
                        schemes translate perfectly to a tree system. Use of the <code>tei:n</code>
                        attribute to denote the identifier of a passage allows for a fast, real-time
                        traversing of the tree, with technologies such as <term>XPath</term> and
                            <term>XQuery</term>, to reconstitute passages such as <ref target="http://data.perseus.org/citations/urn:cts:greekLit:tlg0012.tlg001:1.1">Homer's <title rend="italic">Iliad</title> 1.1</ref>. Identification of
                        passages becomes scalable and encoder-friendly and respects both
                            <term>TEI</term> guidelines and the scholarly tradition.</p>
                    <p>However, a complex situation emerges from another tradition: page-based
                        citation schemes. Most of <name>Perseus’</name> prose resources, whose
                        citation schemes are inherited from scholarly traditions, are quoted by
                        semantical unit (book, chapter, section, etc.) whereas some systems have
                        preferred topological ones (mostly pages). <name>Cicero</name>'s and
                            <name>Plato</name>'s work, two of the most studied authors in Greek and
                        Latin, follow a page based scheme <ptr target="#readinglist"/>. In this
                        context, we find ourselves with two concurrent trees: one that reflects
                        paragraphs and divisions through markup; a second one that embodies the
                        topographical citation scheme. This leads to the use of the fairly common
                            <gi>tei:pb</gi> or <gi>tei:milestone</gi>, identifying the name and the
                        identifier of the canonical citation scheme if required. With the constraint
                        of an <term>XML</term> based delivery of passages, however, this structure
                        fails and collides with the tree oriented query system of XML, namely
                            <term>XPath</term>.</p>
                    <p>The <title>Perseus Digital Library</title> needs not only to be scalable in
                        terms of speed but also in terms of code efficiency. Ideally a single
                        technical implementation of the <term>CTS protocol</term> should be able to
                        support the entire corpus. And to deliver a rather fast response to the
                            <term>GetValidReff</term> request for passages in the <title rend="italic">Iliad</title> - which, without refinement, can necessitate
                        the transfer and the identification of the 15,693 <term>URNs</term>
                        corresponding to the complete set of line identifiers available in the text <note>
                            <term>GetValidReff</term> for Homer's <title>Iliad</title>, with a level
                            parameter set to 2, should return identifiers for all 15,693 lines to
                            the <term>API</term> client. See <ref target="http://www.perseus.tufts.edu/hopper/CTS?request=GetValidReff&amp;urn=urn:cts:greekLit:tlg0012.tlg001.perseus-grc1">http://www.perseus.tufts.edu/hopper/CTS?request=GetValidReff&amp;urn=urn:cts:greekLit:tlg0012.tlg001.perseus-grc1</ref></note>-
                        the XPATH for passage retrieval needs to be cost-efficient. The first
                        solution to this problem is a shift from the traditional citation scheme to
                        a more logical one, with the publication of an equivalences registry between
                        one scheme and another. A second one is the manipulation of the markup
                        rules, with attributes which would indicate that one paragraph and its
                        sibling actually belong to a common unit.</p>
                </div>
                <div>
                    <head>Self-containing text vs. outer metadata: <title rend="italic"><name>CapiTainS</name> Guidelines</title></head>
                    <p><term>CTS</term> is built around three major sets of information which are
                        covered by its guidelines and which come from three different sources:
                        metadata from the library, with authorship and edition information, metadata
                        from the data repository, including the object identifier, and metadata from
                        scholarship, as embodied by the citation scheme. The <title rend="italic"><name>CapiTainS</name> Guidelines</title>
                        <ptr target="#capitains"/> are designed specifically for a <term>XML</term>
                        based implementation of the <term>CTS protocol</term>. They supplement the
                        core <term>CTS specification</term> and provide a solution to the challenges
                        of enabling reuse and scalability. The <title rend="italic"><name>CapiTainS</name> Guidelines</title> include : <list>
                            <item>a directory and file naming convention (<ref target="#figure02">see Figure 2</ref>),</item>
                            <item>expression of the <term>CTS citation scheme</term> and edition
                                specific metadata inside the edition <term>XML</term> file,</item>
                            <item>shared metadata files at the textgroup and notional work
                                level</item>
                        </list></p>
                    <figure xml:id="figure02">
                        <head>Directory structure</head>
                        <graphic url="./resources/images/figure02.png"/>
                    </figure>
                    <p>The rationale behind this approach is to avoid unnecessary duplication of
                        information while still allowing for a completely self-describing corpus
                        structure. Texts adhering to the guidelines can then be integrated into the
                        corpus with a much lighter dependency on the current implementation of the
                        services and tools built to support it, while shared metadata can be of use
                        separately from the text itself.</p>
                    <p>To facilitate text identification, the identifier of the text should be
                        accessible from both inside and outside the markup. While the naming
                        convention of files does cover external identification using the work
                        identifier, a simple query on the text should also be able to return its
                        full <term>URN</term>. In <term>TEI P5</term>, the deepest required common
                        node is the <gi>tei:body</gi>. In the subset commonly used in
                            <term>Epidoc</term>, the deepest required node is one level deeper, the
                        first <gi>tei:div</gi> inside one text, which identifies the text as being
                        of <att>type</att>
                        <val>translation</val> or <val>edition</val>. The <title rend="italic"><name>CapiTainS</name> Guidelines</title> add to this a required
                            <att>n</att> attribute containing the <term>CTS URN</term> of the text.
                        This is enough for the <term>CTS API</term> to identify the author, the work
                        and the edition or translation specific metadata from internal markup or
                        external databases. We use the <att>n</att> attribute on the div enclosing
                        the text or translation, rather than metadata in the <term>TEI</term>
                        <gi>header</gi>, because the <term>TEI P5</term> (and the
                            <term>Epidoc</term> subset) allow for multiple editions or translations
                        to be included in one file, and we want the <term>URN</term> to be
                        unambiguously associated with the text it identifies.</p>
                    <p>In addition to the individual file naming convention, applying a similar
                        approach to the hierarchical directory structure allows us to easily support
                        human browsing of the resources in the source repository <ptr target="#pdlLatin"/>. Our guidelines call for the first level of the
                        directory structure to be named for the <term>CTS textgroup</term> and to
                        include a file containing the <term>CTS</term> metadata for the textgroup,
                        named as <q>__cts__.xml</q>. The second level of the directory structure is
                        named after the identifier of the notional work and itself contains a
                        metadata file which contains the <term>CTS</term> metadata for the work,
                        edition and translation. These metadata files can be used by the service
                        application to dynamically construct a complete <term>CTS
                            TextInventory</term>, a required output of the applications implementing
                        the <term>CTS API</term>.</p>
                    <p>As for the citation scheme, the <term>TEI P5</term> specifically already
                        defines a set of nodes, the <gi>tei:cRefPattern</gi>, as children of
                            <gi>tei:refsDecl</gi>, that are built for this specific task:
                        identifying references through the traversal of the tree using <term>regular
                            expressions</term> and <term>XPath</term>. The <title rend="italic"><term>CapiTainS</term> Guidelines</title> call for implementation
                        of this <gi>tei:refsDecl</gi> structure, using the <att>n</att> attribute to
                        identify it as the <val>CTS</val> reference declaration and the definition
                        of <gi>cRefPattern</gi> for each level of citation to allow for the internal
                        description to perform information retrieval (<ref target="#code01">see Code
                            Sample 1</ref>). Applications which serve the corpus and which want to
                        implement the <term>CTS API</term> can aggregate this information with that
                        provided by the external <term>CTS metadata</term> files to dynamically
                        report the citation scheme as part of the <term>TextInventory</term>. </p>
                    <dhq:example xml:id="code01">
                        <head>Implementation of <term>CTS</term>
                            <gi>refsDecl</gi> for an edition of the Iliad</head>
                        <eg>
                            
&lt;refsDecl n="CTS"&gt;
 &lt;cRefPattern n="line" matchPattern="(.+).(.+)"
   replacementPattern="#xpath(/tei:TEI/tei:text/tei:body/tei:div[&lt;att&gt;@n&lt;/att&gt;='$1']//tei:l[&lt;att&gt;@n&lt;/att&gt;='$2'])"&gt;
  &lt;p&gt;This pointer pattern extracts book and line&lt;/p&gt;
 &lt;/cRefPattern&gt;
 &lt;cRefPattern n="book" matchPattern="(.+)"
   replacementPattern="#xpath(/tei:TEI/tei:text/tei:body/tei:div[&lt;att&gt;@n&lt;/att&gt;='$1'])"&gt;
  &lt;p&gt;This pointer pattern extracts book.&lt;/p&gt;
 &lt;/cRefPattern&gt;
&lt;/refsDecl&gt;
                            
                        </eg>
                    </dhq:example>
                </div>
            </div>
            <div>
                <head>
                    <term>Unit Tests</term>
                </head>
                <div>
                    <head>From text to software : defining properties and functions</head>
                    <p><term>Unit testing</term> is a software engineering practice which focuses on
                        ensuring the functional capacity of software following changes to it by
                        running tests on the smallest unit in a non-deployment environment to
                        prevent propagation of errors in the software base <ptr target="#unittest" loc="75"/>. Test results can be expressed in many different ways :
                        through percentage relative to the last test, or absolutely, or in a simple
                        binary fashion with a passed/not passed information. Tests can generally be
                        developed automatically but might be expanded once specific bugs needing
                        testing surface. Unit tests are intended to check the valid output and/or
                        the consistency of resources, whether they are compute-free or not.
                        (Constants and properties are examples of compute-free resources, whereas
                        functions and objects are examples of the opposite, because a specific input
                        should give a specific output.) Unit tests on <term>XML</term> documents are
                        focused on testing properties of the document against a schema such as
                            <term>TEI</term> using <term>RelaxNG</term>
                        <ptr target="#relaxng"/>. <term>RelaxNG</term> is a description language for
                            <term>XML</term> that specifies how an <term>XML</term> document should
                        be structured, such as what values are acceptable for attributes and what
                        nodes allow or require as their descendants. The scope of what we can test
                        with a <term>RelaxNG</term> schema is limited to these tests and the content
                        and structure of a given document. It has no external data access and is not
                        designed for computing variable document structures.</p>
                    <p>The first step to properly apply <term>unit testing</term> in this context is
                        to define, for an encoded text, the parts which are <q>properties</q> and
                        the parts which are <q>functions</q>. Identifying these parts helps design
                        the general test scenario by grouping resources which are less compute
                        intensive. In a CTS corpus, we can think of metadata such as the <term>CTS
                            URN</term> identifier and the text markup as properties, i.e. they
                        should be present and respected but they are not to be computed upon.
                        Additional testable properties, given the <title rend="italic"><name>CapiTainS</name> Guidelines</title>, include information from
                        the outer metadata files about the work and author, along with their
                        translations.</p>
                    <p>Adherence to and application of a specific text encoding scheme falls in
                        between function and property. In the context of <term>Object Oriented
                            Programming (OOP)</term>
                        <ptr target="#oop" loc="225"/>, the <term>TEI</term> Encoding, and its
                        subsets, represents the architecture of the <term>proto-object</term> or the
                            <term>parent class</term>. Objects derived from this class should
                        respect the parent structure. In this context, <term>XML</term> compliance,
                        and moreover, schema and <term>DTD</term> compliance, can be thought of as
                        required properties of those objects.</p>
                    <p>Passage retrieval is the only specific function that one encounters in
                            <term>CTS</term>. The presence of the <gi>refsDecl</gi> in the
                            <term>XML</term> file of a text is a property, but the accuracy of the
                            <gi>refsDecl</gi> and the presence within the text of at least one
                        element for each level of citation is a requirement for the text to be
                        functional. In addition, for any text, the <att>replacementPattern</att>
                        given for any level of citation should not, when completed, resolve to more
                        than one passage for any given identifier at any level of the hierarchy</p>
                    <p>These then are the base cases for our tests (<ref target="#figure03">see
                            Figure 3</ref>), but experience tells us that additional properties and
                        tests will likely be discovered to be necessary, and need to be added to the
                        existing texts. For example, with the expansion to semitic languages, the
                        existence of right-to-left markers should be checked against language
                        rules.</p>
                    <figure xml:id="figure03">
                        <head>Base Test Diagram</head>
                        <graphic url="./resources/images/figure03.png"/>
                    </figure>
                </div>
                <div>
                    <head>Reuse, present and future development</head>
                    <p>Taking the software engineering paradigm further, we can treat the corpora as
                        a whole as a set of software packages, where each text is a unit
                        representing an individual code base. The test should happen in three
                        different steps: <term>object discovery</term>, <term>test
                            attribution</term>, and <term>unit tests</term>. Within this context,
                        test discovery means detection of <term>XML</term> files. Then in the test
                        attribution step, objects are dispatched by a type detector: here, metadata
                        files adhering to the __cts__.xml name are automatically sent to a specific
                        metadata test class while others are sent to a text test class. Finally,
                        objects are dealt with in a test object whose output is sent back to the
                        main test process. In case the results are needed for further tests, such as
                        the presence of metadata about author and <term>notional work</term>, those
                        are made available in this process.</p>
                    <p>Tests rely on different technical resources, and some do not require custom
                        coding: for example, schemes are tested against <term>TEI</term> or
                            <term>Epidoc</term> using jingtrang <ptr target="#jingtrang"/> and the
                        respective <term>RelaxNG</term> resources. Other tests, such as those which
                        check the naming conventions, are implemented simply as regular expressions.
                        And finally, the <title><name>CapiTainS</name> Guidelines</title> for the
                        definition and resolution of <term>CTS</term> passages are exercised through
                        tests written in <term>Python</term>.</p>
                    <p>The open source software for this test framework is designed to enable
                        extensibility and reuse. An entirely different type of document, for
                        example, a repository of Treebank data (<ref target="#code02">see Code
                            Sample 2</ref>), could be tested through reuse of the archetypal test
                        class objects and coding of new rules for the the file resolver. The
                            <term>archetypal unit test class</term> takes a path, a <q>parsable</q>
                        method for testing ingestion, a <q>logs</q> property and a <q>test</q>
                        method for starting the tests. This class also has two constants which need
                        to be supplied: <q>test</q>, which contains the list of method names to be
                        used for tests, and <q>readable</q>, which should provide human readable
                        explanation of the tests.</p>
                    <dhq:example xml:id="code02">
                        <head>Code sample, Pseudo-python sample integration of Treebank Unit Test
                            class</head>
                        <eg>
class TreebankUnit(HookTest.units.TESTUnit):
    tests = ["parsable", "has_root"]
    readable = {
        "parsable": "File parsing",
        "has_root": "Root declared"
    }
    
    def __init__(self, path):
        super(HookTest.units.TESTUnit, self).__init__(path)
    
    def has_root(self):
        # Process
        self.log("If something needs to be verbose")
        has_root = True  # Assign result as a boolean
        yield has_root
    
    def test(self, scheme):
        tests = [] + CTSUnit.tests
        tests.append(scheme)
        
        for testname in tests:
            # Show the logs and return the status
            for status in getattr(self, testname)():
                yield (
                   TreebankUnit.readable[testname],
                   status,
                   self.logs
                )
        self.flush()</eg>
                    </dhq:example>
                </div>
            </div>
            <div>
                <head>Continuous Integration</head>
                <div>
                    <head>Context and architecture</head>
                    <p><term>Continuous Integration</term> is a software development practice in
                        which programmers sharing the same project commit different changes to a
                        code base. These commits lead to the running of a series of tests to check
                        on compatibility of the new code and finally to the delivery of the
                        community accepted changes to a production or a stage environment <ptr target="#ci"/>.</p>
                    <p><name>Perseus</name> data has been hosted on <name>GitHub</name> since July,
                        26th 2013. Before this, <name>Perseus</name> resources were hosted
                        internally and distributed at release points only on
                            <name>SourceForge</name>. This made incorporating contributions of
                        corrections from external sources difficult. Opening the data of Perseus had
                        two goals. The first one is simply openness. Hosting resources and giving
                        access to them in a raw fashion not dependent on any application or
                            <term>API</term> has been a best practice espoused by numerous projects
                        in the Humanities, such as the Pleiades project <ptr target="#pleiades"/>.
                        The second point of giving access to the data on these collaboration
                        platforms is to allow for citizen scientists, fellow researchers and
                        classical studies enthusiasts, to participate in the correction of
                            <name>Perseus</name> resources the same way.</p>
                    <p>In this context, the library curator finds themselves in a situation where
                        they should ensure that changes proposed, made in the form of <term>pull
                            requests</term>, are correct from both the technical and the
                        philological perspective ( <ref target="#figure04">see Figure 4</ref>).
                        Developing a <term>webhook</term> to check on technical validity, built on
                        the capacity of <name>GitHub</name> to ping services when changes are
                        proposed, has allowed us to significantly lighten the work required of the
                        curator. It also allows us to measure and report on progress, from the
                        highest level (the percentage of the entire repository which is fully CTS
                        CapiTainS Compliant) to the individual object test result (percentage of
                        tests passed). Results of these tests can then also be checked automatically
                        by deployment scripts for the CTS-enabled applications serving the
                        texts.</p>
                    <figure xml:id="figure04">
                        <head>Continuous integration workflow</head>
                        <graphic url="./resources/images/figure04.png"/>
                    </figure>
                </div>
                <div>
                    <head>Scalability and deployment</head>
                    <p>The tool suite used for this continuous integration environment makes use of
                        free online services and is divided into two separate code bases, each
                        presenting its own set of challenges. The user interface, <title rend="italic">Hook</title> (<ptr target="#hook"/>), needs to offer an
                            <term>API</term> endpoint for the test results and user management for
                        registering <term>API</term> access to the <name>GitHub</name> repositories.
                            <title rend="italic">Hook</title> acts as the archival service,
                        listening for test results and annotating pull requests or commits on the
                        source repositories with a summary. On each transaction between
                            <name>Github</name> and <title rend="italic">Hook</title>,
                        identification tokens are exchanged along the required data via the <title rend="italic">oAuth protocol</title>
                        <ptr target="#oauth"/>. The user interface is itself a lightweight Python
                            <title rend="italic">Flask</title> web application <ptr target="#flask"/>.</p>
                    <figure xml:id="figure05">
                        <head>Hook Testing Architecture</head>
                        <graphic url="./resources/images/figure05.png"/>
                    </figure>
                    <p>The second application, <title rend="italic">HookTest</title>
                        <ptr target="#hooktest"/>, is the testing software that actually runs the
                        tests. <title rend="italic">HookTest</title> has been designed for its
                        stable release 1.0.0 as a tool that can be both run on local machines or on
                        free services for <term>Continuous Integration</term> such as
                            <name>Travis-CI</name>. Depending on the size of the corpus, different
                        types of verbosity of the results are made available so text status messages
                        are manageable even on really large corpora. <title rend="italic">HookTest</title> also provides a second set of optional services to
                        package the corpus into a set of only valid files (i.e. files passing tests)
                        and push this package back as a release to <name>Github</name>.</p>
                    <figure xml:id="figure06">
                        <head>Continuous Integration Workflow Sequence</head>
                        <graphic url="./resources/images/figure06.png"/>
                    </figure>
                    <p>In a configuration which leverages both <title rend="italic">Hook</title> and
                            <title rend="italic">HookTest</title> together with the
                            <name>Travis-CI</name> service there are two steps to the feedback
                        process. At the end of the test, <title rend="italic">HookTest</title>
                        displays on <name>Travis</name> the results of the tests in a table (<ref target="#figure06">see Figure 6</ref>) and dispatches the results to
                            <title rend="italic">Hook</title><note> In a local-only configuration,
                                <title rend="italic">HookTest</title> displays results of tests on
                            the console or a local log file.</note>. The <title rend="italic">Hook</title> application adds a comment to the resource on
                            <name>Github</name> (i.e. the <term>Pull Request</term> or the
                            <term>Commit</term> which triggered the test) with a score, a binary
                        result (passed/failed) and a difference status (New text passings, number of
                        new nodes, etc.). In addition to the code comments, <title rend="italic">Hook</title> creates and serves icons, in the form of badges which can
                        be referenced from the <term>README</term> of the repository, for the users
                        of the repository and the application to be able to quickly access
                        information and status from the <name>GitHub</name> repository home page
                            (<ref target="#figure07">see Figure 7</ref>)</p>
                    <figure xml:id="figure07">
                        <head>PerseusDL/canonical-latinLit GitHub homepage</head>
                        <graphic url="./resources/images/figure07.png"/>
                    </figure>
                    <p>If the release packaging service is enabled, each new version of the corpus
                        that has been released can then automatically be deployed into production
                        and test environments, in the same manner as a software update (<ref target="#figure08">see Figure 8</ref>). </p>
                    <figure xml:id="figure08">
                        <head>Hook Update/Integration Architecture</head>
                        <graphic url="./resources/images/figure08.png"/>
                    </figure>
                    <p>The comments added by <title rend="italic">Hook</title> to <term>Pull
                            Requests</term> and <term>Commits</term> on the <name>GitHub</name>
                        repository enables the curator to easily assess changes made by other
                        contributors. The test results can be found on the <name>GitHub</name>
                        resources, and also activate <name>GitHub</name>-managed notifications (mail
                        or web) that states the summary results of the tests with links back to the
                        detailed results in <name>Travis</name>. These notices are sent to the
                        curator owners of the <name>GitHub</name> repository and issuers of the Pull
                        Request or Commit, and also can be subscribed to by other interested
                        parties.</p>
                    <p>Whether working on a new corpus or converting an existing corpus to comply
                        with the <title rend="italic">CapiTainS</title> specification, the tests
                        allow for detection of errors that could not be easily caught by schema
                        validation with <name>RelaxNG</name> or <name>Schematron</name>. One common
                        example of an error of this sort is the duplication of passage identifiers.
                        Because passage identifiers are built by combining identifiers of elements
                        at different levels of the hierarchy, this cannot be done without a
                        programmatic test. These errors are identified in the <title rend="italic">HookTest</title> results. When a text conversion is done and the push
                        request made, <title rend="italic">Hook</title> provides a list of duplicate
                        passages and writes in the summary on the <term>Pull Request</term>. If
                        there is no new text passing, the curator and the contributor can check the
                        output and could find the report written by <title>HookTest</title> on
                            <name>Travis</name> (<ref target="#figure09">see Figure 9</ref>).</p>
                    <figure xml:id="figure09">
                        <head>Logs example for PerseusDL/canonical-latinLit</head>
                        <graphic url="./resources/images/figure09.png"/>
                    </figure>
                </div>
            </div>
            <div>
                <head>Conclusion</head>
                <p>With around 100 million words available on <name>PDL</name>, and millions more
                    words still to come through <name>OPP</name>, in a context of opening
                    contributions up to wide ranging communities of users, dealing with ingestion of
                    new texts scalably is a matter of security, flexibility and efficiency.
                    Developing stronger and more flexible guidelines has helped the project move
                    towards generalization of its norms and reduced the cost to encode, develop and
                    curate.</p>
                <p>With a strong continuous integration service in place, we can now support not
                    only a wider range of genres and languages, but also a wider diversity of
                    contributors. We can delegate the tedious tasks of checking markup to the
                    machine, leaving curators free to focus on the scholarship. We also expect that
                    automating checks on the integrity and the adaptability of textual objects for
                    specific frameworks can reduce the error rate and allow for shorter feedback
                    loops to contributors and users of our corpora.</p>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="perseuscts" label="Almas 2013">
                    <date>2013-05-01</date>
                    <title ref="http://sites.tufts.edu/perseusupdates/beta-features/perseus-cts-api/">Perseus CTS API</title>
                    <author>Bridget Almas</author>
                </bibl>
                <bibl xml:id="cts" label="Smith and Blackwell 2012">
                    <author>Neel Smith</author>
                    <author>Christopher Blackwell</author>
                    <date>2012</date>
                    <title>An overview of the CTS URN notation</title>
                    <publisher>Homer Multitext project</publisher>
                </bibl>
                <bibl xml:id="opp" label="Crane et al 2013">
                    <author>Gregory R. Crane</author>
                    <title ref="http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/">Open Greek and Latin Project</title>
                    <sponsor>Humboldt Chair of Digital Humanities</sponsor>
                    <date>2013-12-13</date>
                </bibl>
                <bibl xml:id="teip4" label="TEI-Consortium 2002">
                    <author>TEI Consortium</author>
                    <title ref="http://www.tei-c.org/Guidelines/P5/">TEI: P5 Guidelines</title>
                    <date>2002</date>
                </bibl>
                <bibl xml:id="epidoc" label="Elliott, Bodard, Cayless et al. 2006">
                    <date>2006-2016</date>
                    <author>Tom Elliott</author>
                    <author>Gabriel Bodard</author>
                    <author>Hugh Cayless</author>
                    <title ref="http://epidoc.sf.net">EpiDoc: Epigraphic Documents in TEI
                        XML</title>
                </bibl>
                <bibl xml:id="teip5" label="TEI-Consortium 2007">
                    <author>TEI Consortium</author>
                    <title ref="http://www.tei-c.org/Vault/P4/">TEI: P4 Guidelines</title>
                    <publisher>University of Virginia Press</publisher>
                    <date>2002</date>
                </bibl>
                <bibl xml:id="mysql" label="MySQL 2004">
                    <date>2004</date>
                    <author>MySQL AB</author>
                    <title ref="http://www.mysql.com">MySQL database server</title>
                </bibl>
                <bibl xml:id="ohco" label="Renear, Mylonas and Durang 1993">
                    <title ref="http://cds.library.brown.edu/resources/stg/monographs/ohco.html">Refining our notion of what text really is: The problem of overlapping
                        hierarchies.</title>
                    <date>1993-01-06</date>
                    <author>Allen Renear</author>
                    <author>Elli Mylonas</author>
                    <author>David Durand</author>
                </bibl>
                <bibl xml:id="complicating" label="TEI-C 2007">
                    <author>TEI Consortium</author>
                    <title ref="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/SG.html#SG152">Complicating the Issue</title>
                    <date>2007</date>
                </bibl>
                <bibl xml:id="readinglist" label="Franzini and Foradi 2014">
                    <author>Elena Franzini</author>
                    <author>Maryam Foradi</author>
                    <title ref="http://www.dh.uni-leipzig.de/wo/latin-and-greek-texts-what-are-we-reading-in-schools-and-universities/">Latin and Greek Texts: What Are We Reading in Schools and
                        Universities?</title>
                    <sponsor>Humboldt Chair of Digital Humanities</sponsor>
                    <date>2014-09-10</date>
                </bibl>
                <bibl xml:id="capitains" label="Almas, Clérice and Munson 2017" ref="http://capitains.github.io">
                    <author>Bridget Almas</author>
                    <author>Thibault Clérice</author>
                    <author>Matthew Munson</author>
                    <title ref="http://doi.org/10.5281/zenodo.570516">CapiTainS Guidelines
                        2.0.0</title>
                    <date>2017-05-02</date>
                </bibl>
                <bibl xml:id="pdlLatin" label="Crane et al. 2015">
                    <author>Gregory R. Crane</author>
                    <title ref="http://github.com/PerseusDL/canonical-latinLit">Perseus Digital
                        Library Canonical Latin Literature Repository</title>
                    <date>2015</date>
                </bibl>
                <bibl xml:id="unittest" label="Huizinga and Kolawa 2007">
                    <title>Automated defect prevention: best practices in software
                        management</title>
                    <author>Dorota Huizinga</author>
                    <author>Adam Kolawa</author>
                    <date>2007-01-22</date>
                    <publisher>Wiley-IEEE Computer Society Pr</publisher>
                </bibl>
                <bibl xml:id="relaxng" label="Clark 2001">
                    <author>Clark James</author>
                    <title ref="http://www. thaiopensource.com/relaxng/design.html">The Design of
                        RelaxNG</title>
                    <date>2001</date>
                </bibl>
                <bibl xml:id="oop" label="Pierce 2002">
                    <author>Benjamin C. Pierce</author>
                    <title>Types and Programming Languages</title>
                    <date>2002</date>
                    <publisher>MIT Press</publisher>
                </bibl>
                <bibl xml:id="jingtrang" label="Clark 2001 (2)">
                    <author>Clark James</author>
                    <title ref="http://www.thaiopensource.com/relaxng/trang.html">JingTrang</title>
                    <date>2001</date>
                </bibl>
                <bibl xml:id="pleiades" label="Ragnall, Talbert, Horne and Elliott 2008">
                    <author>Roger Bagnall</author>
                    <date>2008</date>
                    <author>Richard Talbert</author>
                    <author>Ryan Horne</author>
                    <author>Tom Elliott</author>
                    <title ref="http://pleiades.stoa.org">PLEIADES, A community-built gazetteer and
                        graph of ancient places</title>
                </bibl>
                <bibl xml:id="ci" label="Fowler 2006">
                    <name>Martin Fowler</name>
                    <title ref="http://www.martinfowler.com/articles/continuousIntegration.html">Continuous Integration</title>
                    <date>2006-05-01</date>
                </bibl>
                <bibl xml:id="flask" label="Ronachter 2010">
                    <title>Flask (A Python Microframework)</title>
                    <author>Armin Ronachter</author>
                </bibl>
                <bibl xml:id="oauth" label="Hardt 2012">
                    <date>2012</date>
                    <author>Dick Hardt</author>
                    <title>The OAuth 2.0 authorization framework</title>
                </bibl>
                <bibl xml:id="hook" label="Almas and Clérice 2017" ref="http://ci.perseids.org">
                    <author>Bridget Almas</author>
                    <author>Thibault Clérice</author>
                    <title>Hook</title>
                    <title ref="http://doi.org/10.5281/zenodo.820883">Hook</title>
                    <date>2017-06-19</date>
                    <publisher>Zenodo</publisher>
                </bibl>
                <bibl xml:id="hooktest" label="Almas, Clérice and Munson 2017 b" ref="https://github.com/Capitains/HookTest/tree/1.1.2">
                    <author>Bridget Almas</author>
                    <author>Thibault Clérice</author>
                    <author>Matthew Munson</author>
                    <title ref="http://doi.org/10.5281/zenodo.817503">HookTest: 1.1.2</title>
                    <date>2017-06-23</date>
                    <publisher>Zenodo</publisher></bibl>
            </listBibl>

        </back>
    </text>
</TEI>