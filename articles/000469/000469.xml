<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
    xmlns:mml="http://www.w3.org/1998/Math/MathML">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!--Author should supply the title and personal information-->
                <title type="article" xml:lang="en">Crowdsourcing Image Extraction and Annotation:
                    Software Development and Case Study</title>
                <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>Ana <dhq:family>Jofre</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>SUNY Polytechnic</dhq:affiliation>
                    <email>jofrea@sunypoly.edu</email>
                    <dhq:bio>
                        <p>Dr. Ana Jofre is an Assistant Professor in Creative Arts and Technology
                            at SUNY Polytechnic in Utica NY. She has a PhD in Physics from the
                            University of Toronto and an MFA in Interdisciplinary Arts Media and
                            Design from OCAD University. Her publications and conference
                            presentations cover a wide range of intellectual interests, from physics
                            to critical theory, and she has exhibited her artwork internationally.
                            Her creative and research interests include figurative sculpture,
                            interactive new media, internet art, human-computer interaction, and
                            data visualization.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>Vincent <dhq:family>Berardi</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>Chapman University</dhq:affiliation>
                    <email>berardi@chapman.edu</email>
                    <dhq:bio>
                        <p>Dr. Vincent Berardi is an Assistant Professor of Computational Psychology
                            at Chapman University (Orange, CA) and is the director of the
                            Computational Analysis of Health Behavior Laboratory (CAHB Lab). His
                            works focuses on identifying trends in intensive longitudinal data, in
                            both digital humanities studies and within health behavior
                            interventions.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>Kathleen P.J. <dhq:family>Brennan</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation>University of Queensland</dhq:affiliation>
                    <email>KPJBRENNAN@GMAIL.COM</email>
                    <dhq:bio>
                        <p>Dr. Kathleen P.J. Brennan is a Postdoctoral Research Fellow in the School
                            of Political Science and International Studies (POLSIS) at the
                            University of Queensland (Brisbane, Australia). She completed her PhD in
                            Political Science at the University of Hawai’i at Mānoa in 2016 and her
                            MSc in International Relations Theory at the London School of Economics
                            in 2009. Her work draws on the intersections of political theory, IR,
                            popular culture, and media studies.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>Aisha <dhq:family>Cornejo</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation/>
                    <email>corne129@mail.chapman.edu</email>
                    <dhq:bio>
                        <p>Aisha Cornejo is a recent graduate of Chapman University, with a double
                            major in psychology and philosophy.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>Carl <dhq:family>Bennett</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation/>
                    <email>bennetca@sunypoly.edu</email>
                    <dhq:bio>
                        <p>Carl Bennett is a recent graduate of SUNY Polytechnic, with a BS in
                            Computer Science, and is currently a software developer at General
                            Motors.</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!--Include a separate <dhq:authorInfo> element for each author-->
                    <dhq:author_name>John <dhq:family>Harlan</dhq:family>
                    </dhq:author_name>
                    <dhq:affiliation/>
                    <email>harlanj@sunypoly.edu</email>
                    <dhq:bio>
                        <p>John Harlan is a recent graduate of SUNY Polytechnic, with a BS in
                            Interactive Media and Game Design. He is computer programmer, currently
                            at Shiprite Software, specializing in procedural design and user
                            interfaces.</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>
                <publisher>Association for Computers and the Humanities</publisher>
                <!--This information will be completed at publication-->
                <idno type="DHQarticle-id">000469</idno>
                <idno type="volume"
                    >014</idno>
                <idno type="issue">2</idno>
                <date when="2020-06-19">19 June 2020</date>
                <dhq:articleType>Case Study</dhq:articleType>
                <availability>
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
                    </bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!--add <language> with appropriate @ident for any additional languages-->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!--Authors may include one or more keywords of their choice-->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!--Each change should include @who and @when as well as a brief note on what was done.-->
            <change/>
        </revisionDesc>
    </teiHeader>
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!--Include a brief abstract of the article-->
                <p>We describe the development of web-based software that facilitates large-scale,
                    crowdsourced image extraction and annotation within image-heavy corpora that are
                    of interest to the digital humanities. An application of this software is then
                    detailed and evaluated through a case study where it was deployed within Amazon
                    Mechanical Turk to extract and annotate faces from the archives of <title
                        rend="italic">Time</title> magazine. Annotation labels included categories
                    such as age, gender, and race that were subsequently used to train machine
                    learning models. The systemization of our crowdsourced data collection and
                    worker quality verification procedures are detailed within this case study. We
                    outline a data verification methodology that used validation images and required
                    only two annotations per image to produce high-fidelity data that has comparable
                    results to methods using five annotations per image. Finally, we provide
                    instructions for customizing our software to meet the needs for other studies,
                    with the goal of offering this resource to researchers undertaking the analysis
                    of objects within other image-heavy archives.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!--Include a brief teaser, no more than a phrase or a single sentence-->
                <p>This is a case study that demonstrates how the authors' software can be used to
                    extract and annotate faces from a magazine archive.</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>1. Introduction</head>
                <p>The amount of multimedia data available is steadily increasing <ptr
                        target="#james2014"/>, which has led to many instances where it is desirable
                    to identify and annotate objects located within an image. Examples include the
                    detection of features from outdoor cameras <ptr target="#hipp2013"/>
                    <ptr target="#hipp2015"/> and the classification of animal species <ptr
                        target="#welinder2010"/>
                    <ptr target="#caltech2018"/>. Machine learning and other quantitative
                    methodologies can be used to identify objects within images (see <ptr
                        target="#lecun2015"/> for an example), but their complexity and the
                    requirement for an optimized training set often limit the use of these
                    approaches. A viable alternative is crowdsourcing, the process of enlisting
                    untrained individuals to perform computationally intensive tasks, which has been
                    extensively used in a variety of projects <ptr target="#kuang2015"/>
                    <ptr target="#manovich2016"/>
                    <ptr target="#yu2013"/>
                    <ptr target="#tomnod2018"/>
                    <ptr target="#clickworkers2018"/>. Amazon’s Mechanical Turk (AMT) service for
                    crowdsourcing work is popular with many researchers across disciplines and
                    allows requesters to post tasks, and matches these tasks with anonymous workers
                    who complete them. </p>
                <p>Our specific interest is in identifying and labeling images of faces from the
                        <title rend="italic">Time</title> magazine archive to gain historical
                    insight on American cultural trends. Collecting such a data set requires a
                    two-step process: 1) identify all faces within the corpus and 2) annotate each
                    face according to standardized protocols for feature designation. In this paper,
                    we detail the development of a web-based image-cropping and annotation software
                    for performing these tasks, and we describe our rigorous verification methods
                    for both the cropping and the annotation. Notably, we developed a verification
                    procedure that required only two annotations per image to produce high-fidelity
                    data. The data collected using the methods described here was then used to train
                    an object detector and image classifier machine learning models. </p>
                <p> Our methods are illustrated through a case study where the software was used to
                    crop and label human faces from an archive of <title rend="italic">Time</title>
                    magazine. While our web-based interface is platform-independent, it was
                    administered as an external survey link on AMT. The design process, details of
                    our data collection methods, and instructions for others to customize the
                    software to crop alternative objects from other archives are all described. The
                    software and methodology described here has been used for our own digital
                    humanities project <ptr target="#jofre2020a"/>
                    <ptr target="#jofre2020b"/>, and we believe it may be useful to other
                    researchers. </p>
            </div>
            <div>
                <head>2. Motivation and Background</head>
                <p>This work was motivated by an interest in using large, image-heavy corpora, in
                    particular periodical archives, to gain insights into cultural history.
                    Interpreting large cultural corpora requires both quantitative methods drawn
                    from data science and qualitative methods drawn from technology, cultural, and
                    social studies. From this perspective, we are interested in questions concerning
                    what the faces in a magazine archive could reveal about the larger, historical
                    context of a publication, questions such as how gender/race/age representation
                    have changed over time, and how these correlate with the magazine’s text and
                    with broader cultural trends.</p>
                <p>The archive under consideration for our case study consists of approximately
                    4,500 issues from <title rend="italic">Time</title> magazine, ranging from 1923
                    through 2014. The corpus comprises approximately 500,000 .jpg files, with each
                    page of each issue, including the cover, representing one file. We selected
                        <title rend="italic">Time</title> magazine for a number of reasons. First,
                    while there are a few existing studies of this corpus (see <ptr
                        target="#desouza2014"/> and <ptr target="#manovich2009"/>), there is
                    certainly more work to be done on the visual aspects of the archive by moving
                    beyond the cover images and text. Second, <title rend="italic">Time</title> has
                    been a mainstay publication in the United States for nearly a century, and in
                    that period has witnessed vast cultural, political, and technological changes.
                    Third, it has a relatively well documented corporate history (see <ptr
                        target="#prendergast1986"/>), which allows us to examine the internal
                    context of the production of the magazine vis-à-vis its external context like
                    wars, political movements, changes in fashion, and so on. Finally, the <title
                        rend="italic">Time</title> corpus is widely held in library collections in
                    the United States, and available online through <title rend="italic">The
                        Vault</title> at <ref target="https://time.com/vault/"
                        >https://time.com/vault/</ref>. </p>
                <p>The data we collected using the crowdsourcing methods described in this paper has
                    been published as a dataset in the Journal of Cultural Analytics <ptr
                        target="#jofre2020a"/>, available for use to all researchers in the digital
                    humanities. We used the crowdsourced data to train an algorithm to extract all
                    the images of faces from our <title rend="italic">Time</title> magazine archive
                    and classify their gender. The high-granularity of the automatically generated
                    data allowed us to undertake a detailed study on gender representation in <title
                        rend="italic">Time</title> magazine <ptr target="#jofre2020b"/>.</p>
                <p>Previous studies have successfully used crowdsourcing to achieve goals similar to
                    ours. For instance, when examining features of traffic intersections, the
                    correlation between crowdsourced results and experts was 0.86 for vehicles, 0.90
                    for pedestrians, and 0.49 for cyclists <ptr target="#hipp2013"/>. Similarly,
                    when assessing 99 Flickr images for the presence of 53 features, the correlation
                    between crowdsourced results and expert raters was 0.92 <ptr target="#nowak2010"
                    />. In both of these studies, crowdsourced labels were derived by averaging the
                    labels produced by multiple individuals. While these correlations are
                    encouraging, there are known challenges associated with crowdsourcing.
                    Occasionally, crowdsource workers have been shown to arbitrarily select answers
                    or give vague responses in an effort to complete jobs more quickly <ptr
                        target="#downs2010"/>. This behavior can be reduced by adding verifiable
                    qualification questions, often called honeypots, to crowdsourcing procedures
                        <ptr target="#kittur2008"/>. Furthermore, the demographics of crowdsource
                    workers are typically skewed towards low income workers from India and the
                    United States, who tend to be young and female <ptr target="#casey2017"/>. Our
                    data collection crowdsourcing methods are mindful of concerns about the
                    potential of inadvertently exploiting low-visibility and/or vulnerable
                    populations and intentionally aim to provide reasonable compensation (for
                    further discussion of these issues, see <ptr target="#irani2015"/>). </p>
                <p>While there are many other solutions for researchers seeking to perform image
                    extraction and annotation via crowdsourcing, we believe that our software fills
                    a unique niche for humanities researchers who want to have full control of the
                    data collection and quality controls. Most solutions are geared towards machine
                    learning researchers and provide these services as a bundle, where the client
                    receives the requested clean data. These include LabelBox (<ref
                        target="https://labelbox.com/product/platform"
                        >https://labelbox.com/product/platform</ref>), LionBridge (<ref
                        target="https://lionbridge.ai/services/image-annotation/"
                        >https://lionbridge.ai/services/image-annotation/</ref>), Hive (<ref
                        target="https://thehive.ai/">https://thehive.ai/</ref>), Figure Eight (<ref
                        target="https://www.figure-eight.com/">https://www.figure-eight.com/</ref>),
                    and Appen (<ref target="https://appen.com/">https://appen.com/</ref>). Such
                    black-box solutions are not suitable for the humanities, where we must be
                    mindful of who is doing the tagging. Our software allows the researcher to track
                    individual workers to examine their effect on the data. Furthermore, it is
                    platform-independent, allowing it to be deployed on any crowdsourcing site. We
                    are aware of one other standalone image cropping and tagging software package,
                    labelImg (<ref target="https://github.com/tzutalin/labelImg"
                        >https://github.com/tzutalin/labelImg</ref>), but it is not web-based, which
                    limits its deployment. </p>
                <p>The software package and methodology we developed are intentionally flexible,
                    both in the corpora they can analyze and in the crowdsourcing platform on which
                    they can be deployed. For the former, our motivation was to allow our tools to
                    be used with a variety of sources, such as the Look Magazine archive, hosted by
                    the Library of Congress <ptr target="#lookmagazine2012"/>. For the latter, we
                    did not want to exclusively link the project to AMT because we want the option
                    of using other crowdsourcing platforms.</p>
            </div>
            <div>
                <head>3. Development and Deployment of Interface</head>
                <div>
                    <head>3.1 Determination of Image Features to Be Assessed</head>
                    <p>In preliminary work, project leaders identified the following nine facial
                        features of interest: 1.) Gender, classified as Male, Female or Unknown; 2.)
                        Race, classified according to current U.S. census categories as American
                        Indian, Asian, Black, Pacific Islander, White, or Unknown; 3.) Emotion,
                        classified according to Ekman’s six basic emotions as Anger, Disgust, Fear,
                        Happy, Sad, or Surprise (Ekman and Friesen 1986); 4.) Racial Stereotype,
                        classified as Yes or No; 5.) Magazine Context, classified as Advertisement,
                        Cover, or Feature Story; 6.) Image Type, classified as Photograph versus
                        Illustration; 7.) Image Color, classified as Color or Black &amp; White; 8.)
                        Multiple Faces in the Image, classified as Yes or No; and 9.) Image Quality,
                        classified as Good, Fair, or Poor. </p>
                    <p>One issue from each of the ten decades spanned by the data (1920s-2010s) was
                        selected at random and analyzed by student research assistants. The student
                        coders proceeded through all pages in an issue (range: 50-160), identified
                        faces, and annotated the features according to the above categories.
                        Throughout this process, coders were asked to keep track of anomalous faces
                        that were not easily classified, a process that was extremely valuable in
                        refining our procedures. For example, due to the presence of animal faces
                        and masks, the operational definition of a classifiable face was changed to
                        human faces where at least one eye and clear facial features are present.
                        Single color images required the Image Color classification levels to be
                        changed to Color versus Monochrome and an <soCalled>Author</soCalled>
                        category was added to Magazine Context. There was little agreement among
                        raters concerning the presence of stereotypes and facial emotions, so these
                        categories were eliminated. The emotion variable was replaced by a binary
                        Smile variable and a Face Angle variable (whether the face is in profile or
                        facing the viewer). Furthermore, most of the Unknown labels for the Race and
                        Gender categories were assigned to babies or young children, so a binary
                        Adult variable was also added. The final list of facial features is provided
                        in Table 1. </p>
                    <p>With the updated feature list established, three coders reviewed a single
                        issue and annotated the 185 faces that were identified by all three
                        individuals when reviewing the issue. To assess interrater reliability
                        (IRR), Cohen’s kappa (<mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                        </mml:math>) was calculated for each facial category. <mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                        </mml:math> values between 0.60 and 0.75 are typically interpreted as
                        representing good agreement while <mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                        </mml:math> &gt; 0.75 characterizes excellent agreement. The average <mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                        </mml:math> was 0.809 and all values were above 0.721 with the exception of
                        image quality, with <mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                            <mo xmlns="http://www.w3.org/1998/Math/MathML">=</mo>
                        </mml:math> 0.363. These values are summarized in Table 1. This IRR exercise
                        revealed that, when reviewing the coder data for pages with multiple faces,
                        it was challenging to make interrater comparisons since it was often
                        difficult to determine which exact face corresponded with a given set of
                        labels. This led to a major revision in our protocol where, rather than
                        having individuals annotate faces and store the results as they reviewed
                        pages, they would first crop each face, so that each set of assigned labels
                        could be associated with a specific cropped face. </p>
                    <table>
                        <head>Classification features and categories used for annotating facial
                            images along with κ, quantifying the interrater reliability among three
                            raters over 185 faces from the same magazine issue. * Denotes that this
                            category was classified according to the current U.S. Census <ptr
                                target="#aboutrace2018"/></head>
                        <row role="data">
                            <cell>Variable Name</cell>
                            <cell>Classification Options</cell>
                            <cell>Cohen’s <mml:math>
                                    <mi xmlns="http://www.w3.org/1998/Math/MathML">κ</mi>
                                </mml:math>
                            </cell>
                        </row>
                        <row role="data">
                            <cell>Adult</cell>
                            <cell>Yes or No</cell>
                            <cell>0.771</cell>
                        </row>
                        <row role="data">
                            <cell>Face Angle</cell>
                            <cell>Profile or Straight</cell>
                            <cell>0.819</cell>
                        </row>
                        <row role="data">
                            <cell>Gender</cell>
                            <cell>Female, Male, or Unknown</cell>
                            <cell>0.932</cell>
                        </row>
                        <row role="data">
                            <cell>Image Color</cell>
                            <cell>Color or Monochrome</cell>
                            <cell>0.985</cell>
                        </row>
                        <row role="data">
                            <cell>Image Quality</cell>
                            <cell>Good, Fair, or Poor</cell>
                            <cell>0.363</cell>
                        </row>
                        <row role="data">
                            <cell>Image Type</cell>
                            <cell>Photo or Illustration</cell>
                            <cell>0.928</cell>
                        </row>
                        <row role="data">
                            <cell>Context</cell>
                            <cell>Advertisement, Cover Page, or Feature Story </cell>
                            <cell>0.974</cell>
                        </row>
                        <row role="data">
                            <cell>Multiface</cell>
                            <cell>Yes or No</cell>
                            <cell>0.869</cell>
                        </row>
                        <row role="data">
                            <cell>Race</cell>
                            <cell>American Indian, Asian, Black, Pacific Islander, White, or
                                Unknown*</cell>
                            <cell>0.721</cell>
                        </row>
                        <row role="data">
                            <cell>Smile</cell>
                            <cell>Yes or No</cell>
                            <cell>0.731</cell>
                        </row>
                    </table>
                </div>
                <div>
                    <head>3.2 Deployment of Web-Based Application</head>
                    <p>To scale up data collection, we created a web-based form in PHP, coupled to
                        an SQL database, that could be deployed within crowdsourcing platforms to
                        perform the two tasks required to obtain the data of interest. In Task 1, a
                        magazine page was presented, and participants were instructed to crop any
                        faces that are present; in Task 2, participants were instructed to
                        categorize the faces identified in Task 1 according to the specifications in
                        Table 1. The data collection protocol was to first complete Task 1
                        (cropping) on all our selected pages before moving on to the annotation
                        phase, which allowed cropping errors to be eliminated before sending the
                        extracted images for annotation. Task 1 was separated from Task 2 so that
                        crowdsource workers would only have to be trained for and perform one scope
                        of work. </p>
                    <p>While the data-collection interface is platform-independent and can be used
                        to directly collect data, we found it beneficial to use AMT to recruit
                        participants and manage payments. <q>Jobs</q> (or human interface tasks
                        (HITs) in AMT vernacular) were deployed in AMT as a survey link. For Task 1,
                        each job consisted of reviewing 50 pages and cropping all of the observed
                        faces within each page. AMT workers were paid $5 USD (all payment rates
                        cited here are in USD) for each completed job, which was based on the time
                        it took student coders to complete similarly-sized jobs (30-40 minutes) with
                        a goal of paying between $8-$10/hour, above U.S. federal minimum wage <ptr
                            target="#silberman2018"/>. For Task 2 jobs, AMT workers were required to
                        categorize 25 to 50 faces, each of which was previously cropped from a page
                        in Task 1. Student coders spent 10-15 minutes to complete jobs consisting of
                        50 faces on our context-free interface (discussed in section 3.4.1) and
                        15-20 minutes on jobs consisting of 25 faces on our default interface;
                        therefore, AMT workers were paid $2.25 for these jobs. Once an assigned job
                        was completed, the software generated a completion code that workers entered
                        into AMT to receive payment. Using this code as an identifier, we were able
                        to verify the quality of the work (see sections 3.3 and 3.4 for details) and
                        process payments. For borderline or questionable work quality, we
                        intentionally erred towards payment and only withheld payment for the most
                        extreme circumstances. Each job also included an optional demographic
                        survey, which will inform future studies exploring relationships between
                        demographics and face annotation outcomes. All procedures were approved by
                        the SUNY Polytechnic Internal Review Board. </p>
                </div>
                <div>
                    <head>3.3 Description of Task 1 (Cropping) Interface</head>
                    <p>In this task, workers were presented with a job consisting of 50 images, 47
                        of which were randomly-selected magazine pages and three of which were
                        validation pages. On each assigned page, AMT workers were asked to crop a
                        rectangle around individual faces by clicking and dragging from one corner
                        of a rectangle to the opposite corner. (See Figure 1). If there was more
                        than one face on the page, workers selected an option to remain on the page
                        and continue cropping. Once all the faces were cropped, or if there were no
                        faces on the page, workers selected an option to move onto the next page in
                        their job. We observed that workers often abandoned an assigned job after
                        the first few pages, resulting incomplete jobs within our system. To
                        eliminate these jobs, a script was created that ran in the background to
                        look for pages that had been assigned within a job that had been inactive
                        (i.e. no faces cropped) for more than 2 hours. Any data collected from these
                        jobs was deleted and the pages within them were made available for a new job
                        assignment.</p>
                    <figure>
                        <head>The cropping interface. Left: page as first presented to worker.
                            Middle: worker selects face to crop. Right: Faces that have already been
                            selected and submitted are covered up to help workers keep track of
                            cropped faces.</head>
                        <graphic url="resources/images/figure01.png"/>
                        <figDesc>This image shows three stages of the process of the cropping
                            interface. A face is selected and cropped, leaving a red box covering
                            the face in the final stage.</figDesc>
                    </figure>
                    <p>Within each job, 3 of the 50 pages that the workers analyzed were validation
                        pages, whose inclusion was meant to help detect workers that attempted to
                        quickly receive payment by repeatedly indicating that there were no faces on
                        each page, regardless of content. These pages were selected randomly from a
                        database which contains a list of magazine pages and the known number of
                        faces on each page, as determined by trained project personnel. These are
                        our <q>ground-truth</q> faces. Worker quality was assessed by comparing the
                        number of cropped faces on these pages to the known number of faces.
                        Workers’ validation page was flagged if they cropped more than one face on a
                        validation page with only 1 face or cropped <mml:math>
                            <mo xmlns="http://www.w3.org/1998/Math/MathML">±</mo>
                        </mml:math> 1 face beyond the known number of faces on pages with &gt; 1
                        face. When determining if payment should be provided, workers with 2 or 3
                        flags were subject to additional review while payment was immediately
                        processed for all others.</p>
                    <figure>
                        <head>Screenshot of a page within our in-house review interface. Selections
                            cropped by workers are outlined with a red rectangle.</head>
                        <graphic url="resources/images/figure02.png"/>
                        <figDesc>A screenshot of a <title rend="italic">Time</title> magazine page
                            with faces identified with red boxes around them.</figDesc>
                    </figure>
                    <p>To facilitate the further inspection of AMT workers with a high number of
                        flags, an easy-to-use, in-house review interface was built (Figure 2). On a
                        single webpage, this interface displayed all of the magazine pages assigned
                        to any worker, along with frames around the image areas that the worker
                        selected for cropping. Using this interface, project personnel were able to
                        rapidly scroll through the pages, inspect the work, and make note of pages
                        with mistaken crops or faces left uncropped. If a worker had errors on more
                        than half of their pages, then payment was not provided and all pages in
                        their job were re-analyzed. We paid all other workers but used our revision
                        process to identify pages with egregious errors, which were returned to the
                        pool to have their analysis redone. </p>
                </div>
                <div>
                    <head>3.4 Description of Task 2 (Annotating) </head>
                    <p>In this task, workers were presented with a job consisting of either 25 or 50
                        images of faces, and were asked to enter appropriate tags for each face. The
                        faces were randomly selected from the images that were cropped in Task 1.
                        Procedures similar to those outlined in Task 1 were used to simultaneously
                        manage multiple jobs, ensure that a sufficient number of images are
                        available to populate each job, and cancel jobs that have timed out. For
                        each face in a job, workers classified facial features according to the
                        categories in Table 1 with an additional <quote rend="inline">not a
                            face</quote> option that served as a quality check for the collection of
                        cropped faces. To maximize task efficiency, the options for each
                        classification were presented as clickable radio buttons, rather than as
                        drop-down menus. As in Task 1, once the job was completed, the workers were
                        given a randomly-generated completion code that was used to secure payment
                        through the AMT platform. </p>
                    <p>In a similar process to Task 1, each job contained 3 validation faces, also
                        known as ground-truth faces, each of which was consistently labeled the same
                        by three student coders over all categories. To create a flagging system, we
                        focused on the three categories that had the highest rates of agreement in
                        our preliminary data collection: gender, image color, and image type.
                        Magazine context had the second-highest interrater reliability, but as will
                        be discussed in section 3.4.1, our software was configured to assess this
                        feature in two different ways so it could not be used for validation. When
                        the classifications matched the known values for a given validation image,
                        the flag value was set to zero. Each mismatch contributed a value of 1 to
                        the flag, with a maximum of 3. Images with large flag values were subject to
                        further scrutiny. For the cases where an AMT worker had mismatches with the
                        validation images, it was not possible to build a succinct visual inspection
                        tool for all images as was done in Task 1 since category selections cannot
                        easily be represented visually. Furthermore, there is a degree of
                        subjectivity and ambiguity in certain categories, such as the presence of a
                        smile, so we chose not to develop explicit criteria for processing AMT
                        payments and all workers were paid. To navigate the potential for erroneous
                        data and/or ambiguous categories, we obtained multiple annotations for each
                        face, which were aggregated to obtain a crowdsourced label. As will be
                        described in a subsequent section, we had each face annotated twice and
                        resolved inconsistencies by choosing the label associated with the worker
                        who was most consistent with other workers over all annotated faces, and who
                        had the lowest number of flags. </p>
                </div>
                <div>
                    <head>3.4.1 Examination of variations in the interface</head>
                    <p>We also took this opportunity to examine how variations in the interface
                        affected annotation results (see Figure 3). In particular, we were curious
                        about whether faces taken out of context were more likely to be erroneously
                        labeled. For example, a closely cropped face may not include gender cues,
                        such as hair and clothing. To address this question, we developed two
                        different annotation interfaces. In the context-free version, we show only a
                        cropped face to workers, who then determine the characteristics. Because
                        there is no context around the face, the magazine context (ad, feature,
                        cover, etc.) and multi-face (whether the face being tagged is accompanied by
                        other faces) categories were required to be determined in Task 1 while
                        workers did the cropping. In the second (default) version of the task,
                        workers see the full page with a rectangle around the face of interest when
                        labeling the face and workers answer questions about the face as well as
                        about the context around it. We default to this later version of the
                        interface since we were able to automate Task 1 (see section 5.2), requiring
                        the context annotations to be assigned in Task 2. We found that, despite
                        there being only two additional questions in the default version of the
                        interface compared to the context-free version, it took almost twice as long
                        to complete the labeling tasks, which is why AMT jobs consisted of 25 rather
                        than 50 faces with the default version. </p>
                    <figure>
                        <head>Left: The context-free version of the interface shows workers only the
                            face to be annotated. Right: The default version of the interface shows
                            the full page with the face in question outlined in green.</head>
                        <graphic url="resources/images/figure03.png"/>
                        <figDesc>This image depicts two options in the interface. On the left is a
                            context-free face, with a picture of only the selected face on a black
                            background. On the right is a face highlighted in green on the page it
                            is from which is the default option.</figDesc>
                    </figure>
                </div>
            </div>
            <div>
                <head>4. Software Evaluation: Case Study with Magazine Archive</head>
                <p>A case study was performed using a subset of our magazine archive consisting of
                    one July issue selected from every year between 1961 and 1991, which corresponds
                    with our historic period of interest. Additionally, each of the one-per-decade
                    issues that the student coders manually labeled during our preliminary studies
                    were used as a second data set. The first data set was denoted as 30YR (it spans
                    30 years) while the second was called OPD (as we selected One issue Per Decade).
                    After being cropped, both the 30YR and OPD data were each labeled by two
                    distinct AMT workers. </p>
                <div>
                    <head>4.1 Summary of AMT Accuracy</head>
                    <p>A total of 87 AMT workers cropped 3,722 total pages in Task 1. Due to various
                        glitches that were discovered during deployment and eventually rectified,
                        certain jobs contained less than 50 pages with the average being 47.18 pages
                        per job. The average time to complete a job was 47 minutes. Three validation
                        pages were randomly included within each job to address concerns about
                        individuals incorrectly indicating there were no faces on a given page.
                        However, this behavior was not widely observed, as less than 5% of all
                        validation pages were characterized as having no faces. More common errors
                        appear to have been cropping only a fraction of the faces present on a given
                        page or including many faces within a single crop. For example, 20.0% of
                        validation pages with 3 or more ground truth faces were characterized as
                        having only 1 face. The cropping error rate was significantly reduced when
                        workers were required to acknowledge that they read our instructions before
                        beginning the job. Overall, for 72.8% of validation pages, the number of
                        faces identified by the AMT workers agreed with known number of faces. For
                        an additional 7.6% of validation pages, AMT workers cropped more faces than
                        the known number. It is likely that these cases represent genuine attempts
                        at completing the task, where the known faces along with additional small,
                        poor quality faces were cropped. Processes were implemented to eliminate
                        poor quality faces (see section 4.3). Therefore, the cropping accuracy
                        should consider true positives to be those validation pages where the number
                        of cropped faces either matched or exceeded the ground truth, which led to
                        an effective accuracy of 80.4%. Each page was verified with our inspection
                        interface described above and crop errors were corrected before proceeding
                        to Task 2. </p>
                    <p>In Task 2, a total of 342 workers annotated 9,369 faces. One AMT assignment
                        consisted of either 25 or 50 faces, depending on whether the default or
                        context-free interface was being used. Technical glitches, which were later
                        corrected, occasionally caused the number of faces in a job to slightly
                        vary. The average time to complete a job was 30 minutes using the
                        context-free interface, with a job consisting of 50 faces, and 25 minutes
                        using the default interface with a job consisting of 25 faces. Table 2
                        illustrates the consistency of image annotations with the known labels of
                        the validation images. With the exception of image quality, the accuracy for
                        each category was above 87%.</p>
                    <table>
                        <head>Proportion of images where AMT worker’s label matched the known
                            validation image label in Task 2. Results are not provided for Magazine
                            Context or Multi face since these categories were assessed in Task 1
                            when the context-free interface was used.</head>
                        <row role="data">
                            <cell>Photo</cell>
                            <cell>Color</cell>
                            <cell>Angle</cell>
                            <cell>Quality</cell>
                            <cell>Gender</cell>
                            <cell>Race</cell>
                            <cell>Smile</cell>
                            <cell>Adult</cell>
                        </row>
                        <row role="data">
                            <cell>0.96</cell>
                            <cell>0.93</cell>
                            <cell>0.88</cell>
                            <cell>0.48</cell>
                            <cell>0.94</cell>
                            <cell>0.88</cell>
                            <cell>0.89</cell>
                            <cell>0.99</cell>
                        </row>
                    </table>
                </div>
                <div>
                    <head>4.2 Comparison of Default versus Context-Free Interface for Task 2</head>
                    <p>As described in section 3.4.1, Task 2 was deployed with two different
                        interfaces. In the default case, faces were presented in the context of the
                        original page they were cropped from, while in the context-free case, the
                        face alone was presented. To investigate whether the interfaces affected the
                        labeling task, we used the default interface for both rounds of OPD
                        labeling, but varied the interface for the 30YR data, as shown in Table 3.
                        We then examined the consistency of labels over these two cases. </p>
                    <table>
                        <head>Deployment of Task 2 over various interfaces.</head>
                        <row role="data">
                            <cell/>
                            <cell>Round 1</cell>
                            <cell>Round 2</cell>
                        </row>
                        <row role="data">
                            <cell>OPD</cell>
                            <cell>Default Interface</cell>
                            <cell>Default Interface</cell>
                        </row>
                        <row role="data">
                            <cell>30YR</cell>
                            <cell>Context-free interface</cell>
                            <cell>Default interface</cell>
                        </row>
                    </table>
                    <p>For each of the 10 labeled features, the proportion of images where the
                        ratings agreed was calculated for both the 30YR and OPD data sets. The
                        results are illustrated in Table 4. According to <mml:math>
                            <msup xmlns="http://www.w3.org/1998/Math/MathML">
                                <mrow>
                                    <mi>χ</mi>
                                </mrow>
                                <mrow>
                                    <mn>2</mn>
                                </mrow>
                            </msup>
                        </mml:math> analyses, the differences between the proportion of matches was
                        significant for 5 of the 10 features with the largest differences being
                        between magazine context and image quality. This is to be expected since the
                        two different interfaces used for the 30YR data primarily differed in ways
                        that can be expected to affect these features. There were relatively large
                        differences in image quality based on the presence of context, with 54.1%
                        and 9.6% of faces labeled as good and poor quality, respectively, compared
                        to 3.4% and 20.5% of images labeled good and poor, respectively, for the
                        context-free design. It is possible that the presence of context increased
                        the readability of the face. </p>
                    <p>Interestingly, the correspondence in magazine context was larger across
                        different interfaces in the 30YR data than across the consistent interfaces
                        in the OPD data. The observed statistically significant differences may be
                        due to the large sample size, which is bolstered by effect sizes (Cohen’s
                            <hi rend="italic">f</hi>) that are well below 0.1 in every case;
                        typically, a moderate effect is considered 0.3. As a result, we conclude
                        that the differences in annotation quality according to the interface design
                        are relatively small.</p>
                    <table>
                        <head>Proportion of ratings agreeing for both 30YR and OPD data with <mml:math>
                                <msup xmlns="http://www.w3.org/1998/Math/MathML">
                                    <mrow>
                                        <mi mathvariant="bold-italic">χ</mi>
                                    </mrow>
                                    <mrow>
                                        <mn>2</mn>
                                    </mrow>
                                </msup>
                            </mml:math> analysis <hi rend="italic">p</hi>-values and effect sizes
                            (Cohen’s <hi rend="italic">f</hi>) for the differences in proportions
                            provided. * indicates a p-value &lt; 0.05.</head>
                        <row role="data">
                            <cell/>
                            <cell>Multiface</cell>
                            <cell>Color</cell>
                            <cell>Context</cell>
                            <cell>Photo</cell>
                            <cell>Angle</cell>
                        </row>
                        <row role="data">
                            <cell>30YR</cell>
                            <cell>0.68</cell>
                            <cell>0.90</cell>
                            <cell>0.69</cell>
                            <cell>0.92</cell>
                            <cell>0.79</cell>
                        </row>
                        <row role="data">
                            <cell>OPD</cell>
                            <cell>0.74</cell>
                            <cell>0.88</cell>
                            <cell>0.60</cell>
                            <cell>0.91</cell>
                            <cell>0.78</cell>
                        </row>
                        <row role="data">
                            <cell>p</cell>
                            <cell>0.004*</cell>
                            <cell>0.11</cell>
                            <cell>&lt;0.001*</cell>
                            <cell>0.48</cell>
                            <cell>0.47</cell>
                        </row>
                        <row role="data">
                            <cell>effect</cell>
                            <cell>0.04</cell>
                            <cell>0.02</cell>
                            <cell>0.06</cell>
                            <cell>0.01</cell>
                            <cell>0.01</cell>
                        </row>
                        <row role="data">
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell/>
                            <cell>Gender</cell>
                            <cell>Race</cell>
                            <cell>Adult</cell>
                            <cell>Smile</cell>
                            <cell>Quality</cell>
                        </row>
                        <row role="data">
                            <cell>30YR</cell>
                            <cell>0.90</cell>
                            <cell>0.71</cell>
                            <cell>0.93</cell>
                            <cell>0.81</cell>
                            <cell>0.45</cell>
                        </row>
                        <row role="data">
                            <cell>OPD</cell>
                            <cell>0.89</cell>
                            <cell>0.77</cell>
                            <cell>0.97</cell>
                            <cell>0.82</cell>
                            <cell>0.53</cell>
                        </row>
                        <row role="data">
                            <cell>p</cell>
                            <cell>0.18</cell>
                            <cell>0.002*</cell>
                            <cell>0.001*</cell>
                            <cell>0.72</cell>
                            <cell>&lt;0.001*</cell>
                        </row>
                        <row role="data">
                            <cell>effect</cell>
                            <cell>0.02</cell>
                            <cell>0.05</cell>
                            <cell>0.05</cell>
                            <cell>0.005</cell>
                            <cell>0.05</cell>
                        </row>
                    </table>
                </div>
                <div>
                    <head>4.3 Effect of Image Quality</head>
                    <p>We next explored the effect of image quality on the consistency between
                        raters. Each image was classified as having <hi rend="italic">Satisfactory
                            Quality</hi> (SQ) if both raters scored its quality as either good or
                        fair, or <hi rend="italic">Non-Satisfactory Quality </hi>(NSQ) otherwise.
                        Approximately 27% of the observations were classified as NSQ. The proportion
                        of matches for each feature was then calculated separately for both the SQ
                        and NSQ cases. The results are illustrated in Table 5. For 6 of the 10
                        features, <mml:math>
                            <msup xmlns="http://www.w3.org/1998/Math/MathML">
                                <mrow>
                                    <mi>χ</mi>
                                </mrow>
                                <mrow>
                                    <mn>2</mn>
                                </mrow>
                            </msup>
                        </mml:math> analyses indicated that the concordance between raters was
                        significantly different for SQ and NSQ images. The effect sizes (Cohen’s <hi
                            rend="italic">f</hi>) were larger than when comparing 30YR to OPD images
                        with the adult and image quality features approaching a moderate effect.</p>
                    <table>
                        <head>Proportion of ratings agreeing for both SQ and NSQ data <mml:math>
                                <msup xmlns="http://www.w3.org/1998/Math/MathML">
                                    <mrow>
                                        <mi mathvariant="bold-italic">χ</mi>
                                    </mrow>
                                    <mrow>
                                        <mn>2</mn>
                                    </mrow>
                                </msup>
                            </mml:math> analysis <hi rend="italic">p</hi>-values and effect sizes
                            (Cohen’s <hi rend="italic">f</hi>) for the differences in proportions
                            provided. * indicates a p-value &lt; 0.05.</head>
                        <row role="data">
                            <cell/>
                            <cell>Multi-face</cell>
                            <cell>Color</cell>
                            <cell>Context</cell>
                            <cell>Photo</cell>
                            <cell>Angle</cell>
                        </row>
                        <row role="data">
                            <cell>SQ</cell>
                            <cell>0.68</cell>
                            <cell>0.90</cell>
                            <cell>0.68</cell>
                            <cell>0.94</cell>
                            <cell>0.81</cell>
                        </row>
                        <row role="data">
                            <cell>NSQ</cell>
                            <cell>0.71</cell>
                            <cell>0.89</cell>
                            <cell>0.68</cell>
                            <cell>0.85</cell>
                            <cell>0.75</cell>
                        </row>
                        <row role="data">
                            <cell>p</cell>
                            <cell>0.11</cell>
                            <cell>0.11</cell>
                            <cell>0.60</cell>
                            <cell>&lt;0.001*</cell>
                            <cell>&lt;0.0001*</cell>
                        </row>
                        <row role="data">
                            <cell>effect</cell>
                            <cell>0.02</cell>
                            <cell>0.02</cell>
                            <cell>0.008</cell>
                            <cell>0.14</cell>
                            <cell>0.06</cell>
                        </row>
                        <row role="data">
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell/>
                            <cell>Gender</cell>
                            <cell>Race</cell>
                            <cell>Adult</cell>
                            <cell>Smile</cell>
                            <cell>Quality</cell>
                        </row>
                        <row role="data">
                            <cell>SQ</cell>
                            <cell>0.94</cell>
                            <cell>0.75</cell>
                            <cell>0.97</cell>
                            <cell>0.82</cell>
                            <cell>0.56</cell>
                        </row>
                        <row role="data">
                            <cell>NSQ</cell>
                            <cell>0.81</cell>
                            <cell>0.64</cell>
                            <cell>0.85</cell>
                            <cell>0.81</cell>
                            <cell>0.21</cell>
                        </row>
                        <row role="data">
                            <cell>p</cell>
                            <cell>&lt;0.001*</cell>
                            <cell>&lt;0.001*</cell>
                            <cell>&lt;0.001*</cell>
                            <cell>0.52</cell>
                            <cell>&lt;0.001*</cell>
                        </row>
                        <row role="data">
                            <cell>effect</cell>
                            <cell>0.18</cell>
                            <cell>0.11</cell>
                            <cell>0.21</cell>
                            <cell>0.01</cell>
                            <cell>0.31</cell>
                        </row>
                    </table>
                    <p> The results in Table 5 indicate that it may be advantageous to eliminate NSQ
                        data from subsequent analyses. Before doing so, it is important to determine
                        if this will introduce a bias. Due to changes in printing technology and
                        subject matter over the 90+ years spanned by the data, there is the
                        potential for image quality to differ by time. This possibility was assessed
                        by separately calculating the frequency of SQ and NSQ images in each issue.
                        A <mml:math>
                            <msup xmlns="http://www.w3.org/1998/Math/MathML">
                                <mrow>
                                    <mi>χ</mi>
                                </mrow>
                                <mrow>
                                    <mn>2</mn>
                                </mrow>
                            </msup>
                        </mml:math> analysis was then performed, which indicated that there was no
                        significant difference between the SQ and NSQ frequency distributions.
                        Therefore, eliminating the NSQ images will not introduce temporal bias. </p>
                </div>
                <div>
                    <head>4.4 Aggregation of Multiple Image Labels</head>
                    <p>Each face was annotated twice, each time by distinct AMT workers. While the
                        majority of labels (~ 80%) were in agreement, we required a methodology to
                        resolve disagreements between labels in order to have a definitive value for
                        each annotation. When crowdsourcing data, this is often achieved by having
                        multiple individuals rate a given image and then using a majority rules
                        approach for each feature <ptr target="#hipp2013"/>
                        <ptr target="#hipp2015"/>
                        <ptr target="#nowak2010"/>. However, this approach can be resource
                        intensive. More targeted approaches have been developed that implement an
                        expectation-maximization algorithm to determine the most likely label for a
                        given object in order to ultimately determine a score for the quality of
                        each work <ptr target="#dawid1979"/>
                        <ptr target="#wang2011"/>
                        <ptr target="#organisciak2012"/>
                        <ptr target="#welinder2010"/>
                        <ptr target="#whitehill2009"/>. Lower-performing workers can then be
                        filtered out of the rating system. We aimed to emulate such approaches, but
                        with a simplified procedure that functions over only two coders per image.
                        Our strategy was to calculate a proficiency score for each of the raters and
                        to resolve inconsistencies by selecting the response recorded by the
                        individual with the better proficiency score. Proficiency scores were
                        determined for each worker by examining their validation images and
                        calculating the fraction of annotations matched between the worker’s input
                        and the ground truth. A proficiency score of 1 is a perfect score. The
                        average proficiency score (<mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">μ</mi>
                        </mml:math>) was 0.87 with a standard deviation (SD) of 0.09. An alternate
                        way to calculate the proficiency score was by considering all of the images
                        tagged by a given rater and computing the average fraction of image features
                        that matched the images’ other raters. The average proficiency score with
                        this convention was <mml:math>
                            <mi xmlns="http://www.w3.org/1998/Math/MathML">μ</mi>
                        </mml:math> = 0.81 with SD = 0.06. </p>
                    <table>
                        <head>Mean proficiency score stratified by the sum of flag values over all
                            validation images.</head>
                        <row role="data">
                            <cell>Flag Sum</cell>
                            <cell>0</cell>
                            <cell>1</cell>
                            <cell>2</cell>
                            <cell>3</cell>
                        </row>
                        <row role="data">
                            <cell> Mean Proficiency (All Rated Image) </cell>
                            <cell>0.82</cell>
                            <cell>0.80</cell>
                            <cell>0.78</cell>
                            <cell>0.64</cell>
                        </row>
                        <row role="data">
                            <cell>Mean Proficiency (Validation Images)</cell>
                            <cell>0.90</cell>
                            <cell>0.85</cell>
                            <cell>0.77</cell>
                            <cell>0.76</cell>
                        </row>
                    </table>
                    <p>Table 6 compares our two methods of calculating the proficiency score with
                        the flagging system for image annotations. The sum of the flags for each
                        participant was calculated and proficiency scores were stratified by these
                        values. As shown in Table 6, lower proficiency scores were associated with
                        larger flag values, which indicates that our flagging system provides a
                        reasonably good indicator of worker proficiency. An ANOVA test indicated
                        that the differences in proficiency score values among the flag values were
                        significant (p&lt;0.001) for both varieties of the proficiency score.</p>
                </div>
                <div>
                    <head>4.5 Validation of Proficiency Score</head>
                    <p>Prior to deploying the proficiency score methodology to resolve annotation
                        inconsistencies throughout the entire corpus, it was necessary to determine
                        the consistency of this methodology with the more established majority-rules
                        procedure. To assess this, a subset of 1,000 SQ images were selected from
                        the corpus at random and then submitted to AMT for three additional
                        annotations (i.e., five total annotations). The annotation label selected
                        most frequently was selected for this image with ties between annotation
                        labels (&lt; 1% of all annotations) chosen at random. Table 7 summarizes the
                        proportion of faces for which the annotation labels in the five-rater
                        consensus and proficiency score (using the all rated images option) matched.
                        These results indicate that the proficiency scoring procedure is
                        sufficiently accurate to allow future iterations of this system to proceed
                        with only two raters per image, which will allow for a more
                        resource-efficient project. </p>
                    <table>
                        <head>Proportion of images where the five-rater consensus and proficiency
                            score labels matched, stratified by annotation category.</head>
                        <row role="data">
                            <cell>Photo</cell>
                            <cell>Color</cell>
                            <cell>Angle</cell>
                            <cell>Quality</cell>
                            <cell>Gender</cell>
                            <cell>Race</cell>
                            <cell>Smile</cell>
                            <cell>Adult</cell>
                            <cell>Context</cell>
                            <cell>Multiface</cell>
                        </row>
                        <row role="data">
                            <cell>0.97</cell>
                            <cell>0.97</cell>
                            <cell>0.92</cell>
                            <cell>0.74</cell>
                            <cell>0.97</cell>
                            <cell>0.93</cell>
                            <cell>0.91</cell>
                            <cell>0.99</cell>
                            <cell>0.90</cell>
                            <cell>0.85</cell>
                        </row>
                    </table>
                </div>
            </div>
            <div>
                <head>5. Software Applications</head>
                <div>
                    <head>5.1 Applying Software to Other Data Sets</head>
                    <p>While this software was built for our specific purpose of cropping and
                        annotating faces from <title rend="italic">Time</title> magazine, we were
                        mindful about its generalizability and developed it with the hope that it
                        could serve as a useful tool for other researchers with other corpora. To
                        this end, the code is hosted on GitHub (<ref
                            target="https://github.com/Culture-Analytics-Research-Group/Data-Collection"
                            >https://github.com/Culture-Analytics-Research-Group/Data-Collection</ref>)
                        and is written so that both tasks (image cropping and annotation) are easily
                        generalized, and the annotation variables are straightforward to modify.
                        Instructions for modifying this software to a different archive, along with
                        detailed instructions on how to use the software, are provided in the
                        Appendix.</p>
                    <p>As a demonstration of this flexibility, we hosted a proof-of-concept workshop
                        in December 2018 demonstrating the use of our tool on selected pages from
                        the GQ Magazine corpus <ptr target="#jofre2018"/>. Prior to the workshop, we
                        used our trained face detector to identify and crop faces from these pages,
                        and the workshop demonstrated Task 2 (annotating the selected images) to
                        explore trends in facial hair.</p>
                    <p>The cropping part of the software (Task 1) is particularly easy to adapt for
                        cropping other objects. In our own research, we are currently using the
                        cropping part of the software to extract the advertisements from the corpus.
                        The software is also being used to identify measures of neighborhood
                        distress (graffiti, abandoned vehicles, etc.) in a study that examines the
                        role of environmental factors in promoting physical activity. </p>
                </div>
                <div>
                    <head>5.2 Task Automation </head>
                    <p>Our case-study data has provided us with a corpus-specific training set that
                        we have used to train a RetinaNet detector <ptr target="#lin2017"/>
                        <ptr target="#lin2018"/> to automatically identify and extract the rest of
                        the faces from the archive <ptr target="#jofre2020a"/>
                        <ptr target="#jofre2020b"/>. Our case-study data set of 1,958 pages with
                        4739 face annotations and 1708 pages containing zero faces was used to train
                        the detector. The detector was trained for twenty epochs, since training for
                        more resulted in overfitting and poor generalization in face detection
                        across different historical eras. After running the detector on every page
                        from the archive over 400 thousand facial images were extracted, using a
                        threshold of 50% certainty. When we increased the accuracy threshold to 90%,
                        we were able to extract over 327 thousand faces with very high accuracy. In
                        comparison, our first attempts at automated extraction with OpenCV yielded
                        only 117 thousand facial images from the entire corpus, and 5% of these were
                        false positive (i.e. not actually faces). Compared to OpenCV, the trained
                        RetinaNet detector was able to extract more faces, particularly those with a
                        profile orientation, and those that were illustrated instead of
                        photographed. </p>
                    <p>We have also trained classifiers to automatically label the gender of the
                        face by fine-tuning a pre-trained VGG Face CNN Descriptor network <ptr
                            target="#parkhi2015"/>
                        <ptr target="#malli2018"/> with our crowd-sourced data. From the initial set
                        of data described here, 3,274 faces were male, and only 1,131 were female,
                        which skewed our results on the first run. To expand the training set, we
                        employed a bootstrapping technique to acquire additional, more balanced,
                        training data and thus improve our classifier. The model trained on the AMT
                        data was used to classify all 327,322 faces from the archive. From these
                        faces, we randomly selected images and manually verified the classification
                        results. These new images plus the AMT data yielded a new dataset of 17,698
                        faces for the second round of training, with roughly equal male/female
                        representation. This yielded a 95% accuracy <ptr target="#jofre2020a"/>
                        <ptr target="#jofre2020b"/>. </p>
                </div>
                <div>
                    <head>5.3 Visualizing Annotation Results</head>
                    <p>We created an additional piece of software, also available on our Github page
                            (<ref
                            target="https://github.com/Culture-Analytics-Research-Group/Metadata-Analysis"
                            >https://github.com/Culture-Analytics-Research-Group/Metadata-Analysis</ref>
                        ), that pulls the data directly from the database where the crowdsourced
                        annotations are stored and creates visual summaries of image annotations
                        versus time. The user can select any annotation category and easily generate
                        a chart of the selection as a function of time, aggregated by year or by
                        month. In addition, the tool allows users to select subsets of categories.
                        The example in Figure 4 shows the percentage of women’s faces out of the
                        subset of faces identified in the context of advertisements. This tool is
                        intended for preliminary analysis that allows researchers to quickly
                        identify temporal trends and patterns.</p>
                    <figure>
                        <head>Screenshot showing the percentage of faces that are tagged female out
                            of faces that are tagged as being within advertisements.</head>
                        <graphic url="resources/images/figure04.png"/>
                        <figDesc>This image depicts a graph of faces being tagged female out of all
                            faces tagged in advertisements between 1930 and 2010. There is a sharp
                            spike in female faces being tagged in 1960.</figDesc>
                    </figure>
                </div>
                <div>
                    <head>5.4 Digital humanities studies</head>
                    <p> The data we collected with these methods have allowed us to generate more
                        data via machine learning, and has allowed us to ask the following questions
                            <ptr target="#jofre2020a"/>. How has the importance of the image of the
                        face changed over time? How has gender representation changed over time? How
                        does gender representation correlate with the magazine’s text and with the
                        historical context? How has race representation changed over time? How has
                        the representation of children changed over time? How does race and/or age
                        correlate with the magazine’s text and with the historical context? What
                        types of faces are more likely to be smiling? In what context (ads or news)
                        do certain types of faces tend to appear, and how does this change over
                        time? What types of faces are more likely to be presented as individualized
                        portraits?</p>
                    <p> In our own work, we used the data collected through this method (as well as
                        the automatically-extracted data that this work made possible) to examine
                        how the percentage of female faces found in <title rend="italic"
                            >Time</title> magazine between the 1940s and 1990s correlates with
                        changing attitudes towards women. We found that the percentage of images of
                        women’s faces peaks during eras when women have been more active in public
                        life, and wanes in eras of backlash against women’s rights. The changes in
                        the representation of women in the magazine over time tracked closely not
                        only with the overall historical context, but also with the internal
                        policies of the publication, and with a close reading of the magazine’s
                        content. We believe that this finding is particularly relevant in our
                        contemporary post-literate world in which people absorb culture primarily
                        through images <ptr target="#jofre2020b"/>.</p>
                </div>
            </div>
            <div>
                <head>6. Discussion and Future work</head>
                <div>
                    <head>6.1 Observations</head>
                    <p>We were successful in building and deploying software to manage the
                        crowdsourced extraction and labeling of features from an image-heavy corpus.
                        While the software is generalizable, we focused on an application where
                        faces were required to be extracted and labeled from <title rend="italic"
                            >Time</title> magazine. The accuracy for both Task 1 and Task 2 were in
                        line with those seen for other studies that have used crowdsourcing for
                        similar tasks <ptr target="#hipp2013"/>
                        <ptr target="#hipp2015"/>
                        <ptr target="#nowak2010"/>. In contrast to these other studies that required
                        multiple workers for each image, our method only requires two individuals to
                        annotate each image to gain results with a similar accuracy. </p>
                    <p>Our case-study results show that the differences between labeling performed
                        on context-free versus context-rich interfaces were small. However, there
                        was a notable difference when we instead compared images that were tagged as
                            <soCalled>good</soCalled> quality with images tagged as
                            <soCalled>poor</soCalled> quality, an effect likely due to challenges in
                        reading poor quality figures. This indicates that there is value in
                        requiring workers to evaluate image quality, as it allows us to flag
                        potentially ambiguous annotations. Interestingly, faces that were viewed in
                        the context of a full image were less likely to be labeled as having poor
                        quality compared to faces that were viewed in the context-free interface. It
                        seems that context increases the readability of the face in question, which
                        makes our default interface advantageous. On the other hand, a disadvantage
                        of the default interface is that it takes nearly twice as long to label a
                        single face compared to the context-free interface. While the default
                        interface contains two additional features to be assessed, we speculate that
                        providing a full image rather than a cropped image adds a significant
                        cognitive load to the task. We anecdotally note that personnel who tested
                        both interfaces observed that the default interface felt <quote
                            rend="inline">less tedious</quote> than the context-free interface:
                        viewing pages from vintage magazines was <quote rend="inline">more
                            entertaining</quote> than viewing decontextualized images of faces. In
                        the end, we likely will opt for the default interface in our future studies.
                        This is in part because we have been able to fully automate image
                        extraction, but also because the context-rich environment seems to increase
                        the readability of the selected face. An image of a face alone loses the
                        rich contextual information of the complete page in which it appeared.</p>
                    <p>Using the methods described in this case study, we successfully collected
                        data that was 1) used to train an object detector and an image classifier,
                        2) published and made accessible to other digital humanities researchers
                            <ptr target="#jofre2020a"/>, and 3) used to undertake a study on gender
                        representation in <title rend="italic">Time</title> magazine <ptr
                            target="#jofre2020b"/>.</p>
                </div>
                <div>
                    <head>6.2 Advantages of a Standalone Application</head>
                    <p>While AMT offers multiple options, including developer tools and a sandbox,
                        for creating image cropping and tagging interfaces, we chose to build our
                        own web-based application for several reasons. For one, this allows complete
                        customizability, which was beneficial as we tweaked our approach in response
                        to preliminary data. Also, this web-form enables us to collect data in a
                        manner that is independent of any service providers, which allows us to use
                        different services without compromising our methods. In this work, we used
                        AMT to provide a proof-of-principle, but we plan to deploy this system on
                        other crowdsourcing platforms. The stand-alone interface also opens the
                        possibility of collecting data with volunteer crowdsourcing, as has been
                        done in projects from the New York City Public Library <ptr
                            target="#nyplmap2018"/>
                        <ptr target="#nypllabs2018"/>
                        <ptr target="#allhands2018"/>. The biggest challenge in using volunteers is
                        generating sufficient interest to collect a significant amount of data. We
                        may have to consider methods of gamifying the tasks to make them more
                        appealing, and our hope is that once our results are presented publicly,
                        people may become interested in participating in the project. Lastly, a
                        standalone application can be shared with other researchers and adapted to
                        different types of projects in a way that is not possible with
                        platform-specific approaches.</p>
                </div>
                <div>
                    <head>6.3 Limitations</head>
                    <p> From a humanistic perspective, there is a limitation in using only visual
                        data to classify race and gender. In the case of gender, our data doesn’t
                        distinguish between someone who identifies as a woman (or man) and someone
                        who presents as female (or male), and the automatic classification trained
                        on this data assumes that gender is binary, which is problematic. Human
                        coders, who see the context of the page can mitigate this problem by
                        labeling the gender as ‘unknown’, which accounted for 6% of the faces.
                        However, upon closer inspection, we found that none of these were actually
                        gender non-binary adult individuals: many were not faces at all (errors in
                        the face extraction), many were very small low resolution images that were
                        hard to read, some were non-gendered cartoon illustrations (a face drawn
                        onto an object, for example), and some were infants or small children. So,
                        while problematic, the assumption of a binary gender may be suitable for
                        examining certain mainstream 20th century publications such as <title
                            rend="italic">Time</title> magazine. In the case of race, we found its
                        classification was difficult because race categories are somewhat arbitrary,
                        and because the concept of race is highly context-dependent. Census
                        categories have changed significantly over the past century and they
                        continue to be contentious. In our experience with human coders, we found
                        that the race of a face is often not recognized unless it is embedded within
                        a stereotyped setting, and that when the face was not white, coders tended
                        to disagree on race more than with other categories.</p>
                    <p> A second, more practical, limitation is that this software requires that the
                        user have some familiarity with PHP and with managing SQL databases. Our
                        goal was to make a useful tool for researchers, rather than a polished
                        commercial product. Researchers using this software need to have someone on
                        their team with basic programming experience. The tradeoff, however, is that
                        this software allows researchers to have full control of the data collection
                        and quality controls. </p>
                </div>
                <div>
                    <head>6.4 Long term project goals</head>
                    <p>Our next steps are to continue using this crowdsourced data we collected to
                        automate the classification of other categories, and to undertake a close
                        examination of the context in which faces appear, particularly
                        advertisements. To this end, we are using our software to crowdsource the
                        extraction of all advertisements from selected issues of the corpus. These
                        will be used to train an algorithm that will extract all the advertisements
                        from the corpus. Using this advertising data in conjunction with our face
                        data will allow us to undertake a study on trends in advertising in this
                        particular media outlet.</p>
                    <p>The ultimate goal of this project is to create web-based interactive
                        visualizations of the data we extract from our <title rend="italic">Time
                        </title>magazine archive, and of the results of our analysis. We hope to
                        provide insights into how depictions of faces have changed over time and
                        what such changes in visual representation can tell us about the
                        intersection of politics, culture, race, gender, and class over time. We
                        hope that the online resource we create will be of interest to researchers
                        and students of media and cultural history, as well as to the general
                        public. Our visualization approach is inspired by Manovich’s Selfie-city and
                        Photo-trails work <ptr target="#manovich2016"/>
                        <ptr target="#douglass2011"/>
                        <ptr target="#hochman2016"/>, and by his team’s use of direct visualization
                            <ptr target="#crockett2016"/>, which is an effective way to engage broad
                        audiences into complex corpuses. We also draw inspiration from Robots
                        Reading Vogue <ptr target="#king2016"/> and Neural Neighbors <ptr
                            target="#leonard2018"/>, which are projects based in the Yale University
                        library system. Most recently, we have been using and modifying software
                        from Yale’s DH lab, <title rend="italic">PixPlot</title>
                        <ptr target="#duhaime2018"/>, to sort the images with unsupervised
                        clustering. </p>
                    <p>In addition to gaining insights from our corpus and making these publicly
                        accessible, we also aim to develop novel methodologies for the visual
                        analytics of large, image-based data sets that can be applied to a variety
                        of projects and shared with other researchers. </p>
                </div>
            </div>
            <div>
                <head>Acknowledgements</head>
                <p>We would like to acknowledge Michael Reale for his help with automating image
                    extraction and tagging. We would also like to acknowledge generous research
                    support from our institutions, SUNY Polytechnic and Chapman University, for the
                    start-up funding that made this research possible. Finally, we acknowledge IPAM
                    at UCLA for bringing this collaboration together at the Culture Analytics Long
                    Program and for equipping us with the tools to undertake this research. </p>
            </div>
            <div>
                <head>Appendix: Using and modifying the software </head>
                <div>
                    <head>Part 1: Details About the Code</head>
                    <p>This is a web interface for gathering data from images on a large scale.
                        Users should serve it with accompanying writable SQL databases. We provide
                        the accompanying database structures here and on Github, along with the
                        code. </p>
                    <p>This web-based interface facilitates gathering data from images: it allows
                        users to crop a selection from a larger image and to input information about
                        the crop. In our case, we are selecting faces out of images from a magazine
                        archive, but with some minor edits this code can be used to select anything
                        else from an image archive (cars, trains, signs, etc.). </p>
                    <p>This web interface is platform independent. Users only need a link to access
                        it. </p>
                    <p>The code itself has three different data gathering surveys that are part of
                        it.</p>
                    <p>The first survey allows participants to select and save a cropped portion of
                        an image. The survey contains multiple pages (in our case 50), and the
                        participant has to select and submit all the faces from each page. To access
                        the cropping survey use the link <q>survey.php?load=crop</q>.</p>
                    <p>For the crop, we used <ref target="https://github.com/odyniec/imgareaselect"
                            >https://github.com/odyniec/imgareaselect</ref>
                        <q>imgareaselect</q> by Michal Wojciechowski. </p>
                    <p>The second survey allows users to classify the already cropped images from a
                        selection of categories. To access the cropping survey use the link
                            <q>survey.php?load=tag</q>.</p>
                    <p>The third survey is simply a demographics survey that allows users to enter
                        their demographic information, and is presented at the end of each of the
                        previous two surveys. </p>
                    <p>The code of this survey is split into 4 different files <title rend="italic"
                            >instructions.php</title>, <title rend="italic">survey.php</title>,
                            <title rend="italic">post.php</title>, and <title rend="italic"
                            >functions.php</title>. </p>
                    <p><title rend="italic">instructions.php</title> is a landing page that presents
                        the user with instructions for the current survey either the cropping survey
                        or the classify survey. The survey and instructions that will be presented
                        are determined by the GET variable load in the URL. If load=crop the crop
                        instructions are presented if load=tag then the classifying survey is
                        presented. Users must select that they have read the instructions in order
                        to move onto the survey. </p>
                    <p><title rend="italic">survey.php</title> is the main interface of the survey
                        that the user interacts with. </p>
                    <p>If the job is to crop images, the url <q>survey.php?load=crop</q> should be
                        used. The image to be cropped is presented and users are asked if the object
                        to be cropped is present (faces in the case of the original purpose) in the
                        image. If the object is present users can crop it be clicking and dragging
                        over the object in the image. If multiple objects are present users may
                        select that there are more objects (faces) on the page. Any previous cropped
                        objects will be covered when cropping another object. If it is not present
                        users may simply select that the object is not there and move to the next
                        image. </p>
                    <p>If the job is classifying images that were previously cropped, the url
                            <q>survey.php?load=tag</q> should be used. The user is presented the
                        image from which an object of interest was cropped, with the cropped portion
                        highlighted along with questions about the classification of the object. </p>
                    <p>Each job within the survey has a total number of images to be done at one
                        time that can be set along with three check points that can be set (in
                            <title rend="italic">functions.php</title>). The check points present
                        the user with ground truth pages where the classification or number of
                        objects cropped is already known in order to check whether a user has
                        properly completed the survey. These variables can be set in <title
                            rend="italic">functions.php</title>.</p>
                    <p><title rend="italic">post.php</title> handles all submission of data to the
                        data base after a user has hit the submit button. If the job was cropping,
                        data is submitted to the database and the selected portion is cropped and
                        saved to a folder on the server. If the job was classifying, data is just
                        submitted to the database. If a user has completed a check page then
                        information on the page is placed in an array to later be checked and
                        entered at the end of the survey. If the user has reached the end of the
                        survey and filled out the demographics information then the demographics
                        data and check data is submitted and a completion code is generated. If a
                        user has no activity for 2 hours and then tries to submit data <title
                            rend="italic">post.php</title> will cause the session to timeout.</p>
                    <p><title rend="italic">functions.php</title> contains all the functions that
                        are used in the survey and is included in both <title rend="italic"
                            >survey.php</title> and <title rend="italic">post.php</title></p>
                    <div>
                        <head>functions.php Overview</head>
                        <p>$job — php $_GET variable that indicates whether the job I for cropping
                            or tagging so that the proper page is loaded. Obtained from the url, for
                            example, in the url <q>survey.php?load=crop</q> $job=crop. </p>
                        <p>$batch_size — variable controlling the number of images per job</p>
                        <p>$check — array variable that contains when ground truth images will be
                            shown in the job</p>
                        <p>$face_total — variable for cropping that keeps track of the number of
                            objects cropped from a specific image</p>
                        <p>$file_array — holds image file names to have a group number added at the
                            end of each job</p>
                        <p>$check_data1, $check_data2, $check_data3 — holds data submitted by users
                            on each of the three ground truth images</p>
                        <p>db_connect() — returns a mysqli_connection object for connecting to the
                            database, set $servername, $username, $password, and $database you wish
                            to connect to</p>
                        <p>select($job, $batch_size, $connection) — selects images one at a time as
                            long as there is enough images available for another job, otherwise
                            users are presented with a message that requests are currently at
                            capacity. This function also marks pages as being worked on in the
                            database and adds a timestamp for clearing data on a job that was never
                            finished. The file name of the image is returned</p>
                        <p>check_select($job, $connection) — similar to select, except it selects
                            ground truth images from their tables.</p>
                        <p>parse_filename($job, $filename) — parses information from the file name
                            of the image. If the job is cropping, then this information is used to
                            create the path that cropped images will be stored in. If the job is
                            classifying, then this information is used to determine the path of the
                            original image. The parsed data is stored in the $file_data array to
                            later be displayed and submitted to the database. This function is based
                            on the file name scheme of the images originally used with this code. </p>
                        <p>display($job, $file_data) — handles what is displayed for the user
                            depending what the job is. Inputs for the survey questions are printed
                            out as radio buttons</p>
                        <p>hidden($job, $batch_current, $filename, $file_data, $file_array,
                            $check_data1, $check_data2, $check_data3) — prints out the hidden inputs
                            for each job mainly the data parsed from the filename. If the job is
                            cropping the hidden inputs containing information for cropping the data
                            is printed out.</p>
                        <p>post_hidden() — prints out hidden inputs for <title rend="italic"
                                >post.php</title> that need to be sent back to <title rend="italic"
                                >survey.php</title></p>
                        <p>crop_image() — handles the cropping of images for the crop job and
                            accounts for offset of different window resolutions and sizes.</p>
                        <p>post_variables($job) — sets the variables in post that will be submitted
                            to the database for each job along with variables needed for post
                            functions</p>
                        <p>submit($job, $connection) — submits data to the database for each job and
                            marks images as no longer being worked on. If the job is cropping and no
                            object was cropped then no data is submitted. If the job was cropping
                            and the page was a ground truth page a temporary entry is mad in a table
                            so that covering previously cropped objects on pages with multiple
                            objects will work properly.</p>
                        <p>final_submit($job, $connection) — submits the demographics information to
                            the database. A group number is generated by selecting the highest group
                            number from the database group tables for each job and adding one.</p>
                        <p>This group number is assigned to each image that was part of the job. It
                            is also inserted into the check table for each job along with possible
                            flags raised from the information in the check arrays and a randomly
                            generated code that will be presented to the user. This code is for
                            admins to manage payment via Amazon Mechanical Turk.</p>
                        <p>demographic($job, $file_array, $check_data1, $check_data2, $check_data3)-
                            displays the form and the inputs for users to enter their demographic
                            information</p>
                        <p>coverfaces($job, $connection, $filename, $file_data) — </p>
                        <p>If the job is set to <q>crop</q>, covers previously cropped objects
                            (faces) on images where multiple objects need to be cropped, by
                            selecting previously submitted x and y coordinates from the database. If
                            the image is a ground truth image then it selects from the temporary
                            entry in the table for crop checks. If the job is set to <q>tag</q>,
                            this function is used to find the coordinates and draw the rectangle
                            around the object to be classified. </p>
                    </div>
                </div>
                <div>
                    <head>Part 2: The Data Tables</head>
                    <p>Below is the <q>pages</q> table structure — Used for the cropping task. </p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>page_file</q></cell>
                            <cell>File name of magazine page image</cell>
                        </row>
                        <row role="data">
                            <cell><q>faces</q></cell>
                            <cell>Total number of faces on that page (starts out as null until page
                                is analyzed)</cell>
                        </row>
                        <row role="data">
                            <cell><q>group_num</q></cell>
                            <cell>Identifies a completed job. This cell is null until a job is
                                completed, when the job is completed, all the pages that belonged to
                                that job are marked with this group number. This number is unique
                                and increments each time a job is completed. </cell>
                        </row>
                        <row role="data">
                            <cell><q>working</q></cell>
                            <cell> flags whether that particular page is being worked on by another
                                worker. </cell>
                        </row>
                        <row role="data">
                            <cell><q>timestamp</q></cell>
                            <cell>which marks the date/time a page is displayed. If a page was
                                displayed more than 2 hours ago and does not have an associated
                                    <q>group number</q>, then any data collected on that page is
                                cleared, timestamp is marked null, and the page is made available
                                again for selection.</cell>
                        </row>
                    </table>
                    <p>Below is the <q>crop_groups</q> table structure — Used to track workers in
                        cropping task.</p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>group_num</q></cell>
                            <cell>Job identifier</cell>
                        </row>
                        <row role="data">
                            <cell><q>flag1</q></cell>
                            <cell>Results from <q>ground-truth</q> comparisons.</cell>
                        </row>
                        <row role="data">
                            <cell><q>flag2</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>flag3</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>code</q></cell>
                            <cell>Unique completion code. Randomly generated by our software, to be
                                entered into mechanical turk.</cell>
                        </row>
                    </table>
                    <p>Below is the <q>ground_truth_crops</q> table structure. This is the ground
                        truth table that is used for the cropping task.</p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>file</q></cell>
                            <cell>File name of the image</cell>
                        </row>
                        <row role="data">
                            <cell><q>nfaces</q></cell>
                            <cell>Number of faces on this image</cell>
                        </row>
                        <row role="data">
                            <cell><q>working</q></cell>
                            <cell>Marks whether the file is currently being used</cell>
                        </row>
                        <row role="data">
                            <cell><q>timestamp</q></cell>
                            <cell>Marks time that file was displayed. Resets after 1 hour. </cell>
                        </row>
                    </table>
                    <p>Below is the <q>tag_groups</q> table structure – Used to track workers in
                        tagging task.</p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>tag_group</q></cell>
                            <cell>Job identifier</cell>
                        </row>
                        <row role="data">
                            <cell><q>flag1</q></cell>
                            <cell>Results from <q>ground-truth</q> comparisons.</cell>
                        </row>
                        <row role="data">
                            <cell><q>flag2</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>flag3</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>code</q></cell>
                            <cell>Unique completion code. Randomly generated by our software, to be
                                entered into mechanical turk.</cell>
                        </row>
                    </table>
                    <p>Below is the <q>data</q> table structure — this is the table that contains
                        the collected data. Year, month, day, page, image, and coordinates are
                        populated during the cropping task. The rest of the columns are populated in
                        the tagging task. </p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>year</q></cell>
                            <cell>These identify the source image, which is labeled by issue date
                                and page number.</cell>
                        </row>
                        <row role="data">
                            <cell><q>month</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>day</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>page</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>multiface</q></cell>
                            <cell> Is there more than one person in the image (yes/no)? </cell>
                        </row>
                        <row role="data">
                            <cell><q>category</q></cell>
                            <cell> Is the image part of a feature story, an ad, or the cover page?
                            </cell>
                        </row>
                        <row role="data">
                            <cell><q>color</q></cell>
                            <cell> Is the image in color or monochrome? </cell>
                        </row>
                        <row role="data">
                            <cell><q>photo</q></cell>
                            <cell> Is the face a photograph or an illustration? </cell>
                        </row>
                        <row role="data">
                            <cell><q>angle</q></cell>
                            <cell> Is the face in profile or looking straight ahead? </cell>
                        </row>
                        <row role="data">
                            <cell><q>gender</q></cell>
                            <cell> Is the face male or female (or other)? </cell>
                        </row>
                        <row role="data">
                            <cell><q>race</q></cell>
                            <cell> What is the race of the face? (select from 5 census categories:
                                White, Black, Asian, American Indian, Pacific Islander) </cell>
                        </row>
                        <row role="data">
                            <cell><q>adult</q></cell>
                            <cell> Is it an adult or a child? </cell>
                        </row>
                        <row role="data">
                            <cell><q>smile</q></cell>
                            <cell> Is the face smiling? </cell>
                        </row>
                        <row role="data">
                            <cell><q>quality</q></cell>
                            <cell> What is the image quality like? (Good — face is clearly visible,
                                Fair — face is small or slightly blurry, Poor — face is barely
                                visible, Discard — this is not a human face)</cell>
                        </row>
                        <row role="data">
                            <cell><q>image</q></cell>
                            <cell>the name of the cropped image that is saved in the data folder on
                                the back end.</cell>
                        </row>
                        <row role="data">
                            <cell><q>x1</q></cell>
                            <cell>These are the diagonal corner coordinates of the cropped
                                selection.</cell>
                        </row>
                        <row role="data">
                            <cell><q>y1</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>x2</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>y2</q></cell>
                            <cell/>
                        </row>
                        <row role="data">
                            <cell><q>tag_group</q></cell>
                            <cell>tracks completed tagging jobs. This cell is null until a job is
                                completed, when the job is completed, all the crops that belonged to
                                that job are marked with this group number. This number is unique
                                and increments each time a job is completed.</cell>
                        </row>
                        <row role="data">
                            <cell><q>working</q></cell>
                            <cell>, <q>working</q> flags whether that crop is currently being tagged
                                by another worker</cell>
                        </row>
                        <row role="data">
                            <cell><q>timestamp</q></cell>
                            <cell> , and <q>timestamp</q> marks the date/time an object is displayed
                                for tagging. If an image was displayed more than 2 hours ago and
                                does not have an associated <q>tag_group</q>, then any data
                                collected on that crop is cleared and the crop is made available
                                again for selection. </cell>
                        </row>
                    </table>
                    <p>The <q>ground_truth</q> table has the same structure as the data table — This
                        is the ground truth table for the tagging task.</p>
                    <p>The <q>crop_check</q> table stores the year, month, day, page, and
                        coordinates of the ground truth pages that the user crops. This keeps track
                        of the objects cropped out of the <q>ground truth</q> pages. It is used to
                        cover objects that a user has already cropped from a single page when
                        multiple objects are present, and it is used to calculate the flags in the
                            <q>crop_groups</q> table. Once the job is finished and the flags are
                        calculated, the entries in this table are deleted. </p>
                    <p><q>tag_check</q> table structure (this table records workers’ entries on the
                        validation pages)</p>
                    <table>
                        <row role="data">
                            <cell>Column</cell>
                            <cell>Description</cell>
                        </row>
                        <row role="data">
                            <cell><q>tag_group</q></cell>
                            <cell> Job identifier </cell>
                        </row>
                        <row role="data">
                            <cell><q>multiface</q></cell>
                            <cell> Is there more than one person in the image (yes/no)? </cell>
                        </row>
                        <row role="data">
                            <cell><q>category</q></cell>
                            <cell> Is the image part of a feature story, an ad, or the cover page?
                            </cell>
                        </row>
                        <row role="data">
                            <cell><q>color</q></cell>
                            <cell> Is the image in color or monochrome? </cell>
                        </row>
                        <row role="data">
                            <cell><q>photo</q></cell>
                            <cell> Is the face a photograph or an illustration? </cell>
                        </row>
                        <row role="data">
                            <cell><q>angle</q></cell>
                            <cell> Is the face in profile or looking straight ahead? </cell>
                        </row>
                        <row role="data">
                            <cell><q>gender</q></cell>
                            <cell> Is the face male or female (or other)? </cell>
                        </row>
                        <row role="data">
                            <cell><q>race</q></cell>
                            <cell> What is the race of the face? (select from 5 census categories:
                                White, Black, Asian, American Indian, Pacific Islander) </cell>
                        </row>
                        <row role="data">
                            <cell><q>adult</q></cell>
                            <cell> Is it an adult or a child? </cell>
                        </row>
                        <row role="data">
                            <cell><q>smile</q></cell>
                            <cell> Is the face smiling? </cell>
                        </row>
                        <row role="data">
                            <cell><q>image</q></cell>
                            <cell> The validation image used </cell>
                        </row>
                    </table>
                </div>
                <div>
                    <head>Part 3: Instructions for Modifying the Software for Use in Other
                        Studies</head>
                    <p>While this software was built for our specific purpose of cropping and
                        annotating faces from a specific periodical archive, we were mindful about
                        its generalizability and developed it with the hope that it could serve as a
                        useful tool for other researchers. We share our code and database structure
                        on GitHub with this intent. The code is written so that the cropping job is
                        easily generalized and the annotation variables are easy to modify. </p>
                    <p>The most straightforward application of this software is for researchers
                        interested in cropping and annotating objects from other magazine archives.
                        To use our application, the archive needs to be stored as a collection of
                        .jpg images named using the following convention: YYYY-MM-DD page X.jpg
                        (where YYYY is the year, MM is the month, DD is the day, X is the page
                        number). We share the database structure so that users can easily configure
                        it from their server. Users can change column names (and corresponding
                        variable names in the code) as needed. </p>
                    <p>The key part of the code consists of four php files: <title rend="italic"
                            >instructions.php</title> is a landing page in case users want to
                        present workers with instructions at the beginning of a task, <title
                            rend="italic">survey.php</title> contains the interface the worker
                        interacts with, <hi rend="italic">post.php</hi> handles all the submission
                        of data to the database, and <title rend="italic">functions.php</title>
                        contains all the functions used in <title rend="italic">survey.php</title>
                        and <title rend="italic">post.php</title>. The user will have to modify
                        these files, depending on the application. At a minimum, the user will need
                        to edit the <title rend="italic">db_connect()</title> function in the <title
                            rend="italic">functions.php</title> file with their own server
                        configurations.</p>
                    <p>To use the cropping task, users should list the images they want analyzed in
                        the <title rend="italic">page_file</title> column in the <title
                            rend="italic">pages</title> data table and serve the
                        ‘survey.php?load=crop’ URL to display the cropping task. (A link to the demo
                        will be included here if this paper is accepted, after anonymity is lifted.)
                        In the <title rend="italic">function.php</title> file, users can adjust the
                        number of pages that comprise a job, the number of validation images per
                        job, and the location of the validation images (2nd image seen, 5th image
                        seen, etc.). The validation images are drawn from the <title rend="italic"
                            >ground_truth_crop</title> table, which the user must populate. </p>
                    <p>When a worker crops a face with this interface, a copy of the cropped image
                        is stored on the backend and the <title rend="italic">data</title> table is
                        populated with information about this face. The user must specify the name
                        and path of the folder where the cropped images will be stored: this is done
                        in the <title rend="italic">crop_image</title> function in <title
                            rend="italic">functions.php</title>. The information stored in the data
                        table is the year, month, day, and page number, parsed from the source image
                        name; the coordinates of the crop; and the name of the cropped image. If
                        users need to have a different file naming convention and need their source
                        image names parsed differently, they can modify the <title rend="italic"
                            >parse_filename()</title> function in the <title rend="italic"
                            >functions.php</title> file. The total number of crops made per page is
                        stored in the <title rend="italic">faces</title> column of the <title
                            rend="italic">pages</title> table. If the user is cropping an object
                        other than a face, the names of variables, data columns, and the descriptors
                        on the frontend can be changed to more appropriate terms. </p>
                    <p>To display the annotation task, users should serve the
                            <q>survey.php?load=tag</q> URL. (a demo page can be viewed here: <ref
                            target="https://magazineproject.org/TIMEvault/survey.php?load=tag"
                            >https://magazineproject.org/TIMEvault/survey.php?load=tag</ref> .) To
                        use the tagging task, the <title rend="italic">data</title> table should be
                        populated with the source image identifiers (year, month, day, and page) and
                        with the coordinates of the crop. If the user wants to use the context-free
                        version of the interface, they will only need to provide the name of the
                        cropped image in the <title rend="italic">data</title> table and modify the
                        source image in the <title rend="quotes">content-div</title> html element in
                        the <title rend="italic">survey.php</title> file.</p>
                    <p>If users want to annotate features that are different from the ones we
                        listed, the names of the data columns can be changed, as well as the
                        corresponding variable names in the functions <title rend="italic"
                            >post_variables()</title>, <title rend="italic">submit()</title>, and
                            <title rend="italic">display()</title>, which are in the <title
                            rend="italic">functions.php</title> file. Data columns and corresponding
                        variables can be added or removed as needed. </p>
                </div>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="aboutrace2018" label="About Race 2018">About Race, United States
                    Census Bureau. URL <ref
                        target="https://www.census.gov/topics/population/race/about.html"
                        >https://www.census.gov/topics/population/race/about.html</ref> (accessed
                    9.20.18).</bibl>
                <bibl xml:id="allhands2018" label="All Hands on Deck 2018">
                    <title rend="quotes">All Hands on Deck: NYPL Turns to the Crowd to Develop
                        Digital Collections</title>. <title rend="italic">The New York Public
                        Library</title>. URL <ref
                        target="https://www.nypl.org/blog/2011/09/15/all-hands-deck-nypl-turns-crowd-develop-digital-collections"
                        >https://www.nypl.org/blog/2011/09/15/all-hands-deck-nypl-turns-crowd-develop-digital-collections</ref>
                    (accessed 9.6.18).</bibl>
                <bibl xml:id="bradski2000" label="Bradski 2000">Bradski, G. <title rend="quotes">The
                        OpenCV Library</title>, <title rend="italic">Dr. Dobb’s Journal of Software
                        Tools</title> (2000).</bibl>
                <bibl xml:id="caltech2018" label="Caltech-UCSD Birds 200 2018">Caltech-UCSD Birds
                    200. URL <ref target="http://www.vision.caltech.edu/visipedia/CUB-200.html"
                        >http://www.vision.caltech.edu/visipedia/CUB-200.html</ref> (accessed
                    9.18.18).</bibl>
                <bibl xml:id="casey2017" label="Casey et al. 2017">Casey, L. S., Chandler, J.,
                    Levine, A. S., Proctor, A., Strolovitch, D.Z. <title rend="quotes">Intertemporal
                        Differences Among MTurk Workers: Time-Based Sample Variations and
                        Implications for Online Data Collection</title>, <title rend="italic">SAGE
                        Open</title> 7, 215824401771277. <ref
                        target="https://doi.org/10.1177/2158244017712774"
                        >https://doi.org/10.1177/2158244017712774</ref> (2017).</bibl>
                <bibl xml:id="clickworkers2018" label="Clickworkers 2018">CLICKWORKERS: Home. URL
                        <ref target="http://www.nasaclickworkers.com/"
                        >http://www.nasaclickworkers.com/</ref> (accessed 9.18.18).</bibl>
                <bibl xml:id="crockett2016" label="Crockett 2016">Crockett, D. <title rend="quotes"
                        >Direct visualization techniques for the analysis of image data: the slice
                        histogram and the growing entourage plot</title>, <title rend="italic"
                        >International Journal for Digital Art History</title> 2. <ref
                        target="https://doi.org/10.11588/dah.2016.2.33529"
                        >https://doi.org/10.11588/dah.2016.2.33529</ref> (2016).</bibl>
                <bibl xml:id="dawid1979" label="Dawid and Skene 1979">Dawid, A. P., Skene, A. M.
                        <title rend="quotes">Maximum likelihood estimation of observer error‐rates
                        using the EM algorithm</title>, <title rend="italic">Journal of the Royal
                        Statistical Society: Series C (Applied Statistics)</title> 28(1): 20-28.
                    doi: 10.2307/2346806 (1979).</bibl>
                <bibl xml:id="desouza2014" label="de Souza 2014">de Souza, R.C. <title rend="quotes"
                        >Chapter 2.3 dimensions of variation in <title rend="italic">TIME</title>
                        magazine.</title> In T. Berber Sardinha, and M. Veirano Pinto (eds), <title
                        rend="italic">Multi-Dimensional Analysis, 25 Years on: A Tribute to Douglas
                        Bieber</title>, Amsterdam: 177-194. <ref
                        target="https://doi.org/10.1075/scl.60.06sou"
                        >https://doi.org/10.1075/scl.60.06sou</ref> (2014).</bibl>
                <bibl xml:id="douglass2011" label="Douglass et al. 2011">Douglass, J., Huber, W.,
                    Manovich, M. <title rend="quotes">Understanding scanlation: how to read one
                        million fan-translated manga pages</title>, <title rend="italic">Image &amp;
                        Narrative</title> 12: 190–227 (2011).</bibl>
                <bibl xml:id="downs2010" label="Downs et al. 2010">Downs, J. S., Holbrook, M. B.,
                    Sheng, S., Cranor, L. F. <title rend="quotes">Are your participants gaming the
                        system?: screening mechanical turk workers</title>, <title rend="italic">CHI
                        2010 Proceedings of the SIGCHI Conference on Human Factors in Computing
                        Systems</title>, Atlanta, Georgia, April 2010: 2399–2402. <ref
                        target="https://doi.org/10.1145/1753326.1753688"
                        >https://doi.org/10.1145/1753326.1753688</ref> (2010).</bibl>
                <bibl xml:id="duhaime2018" label="Duhaime 2018">Duhaime, D. PixPlot. Yale Digital
                    Humanities Lab (2018).</bibl>
                <bibl xml:id="ekman1986" label="Ekhman and Friesen 1986">Ekman, P., Friesen, W. V.
                        <title rend="quotes">A new pan-cultural facial expression of
                    emotion</title>, <title rend="italic">Motivation and Emotion</title> 10:
                    159–168. <ref target="https://doi.org/10.1007/BF00992253"
                        >https://doi.org/10.1007/BF00992253</ref> (1986).</bibl>
                <bibl xml:id="han2014" label="Han et al. 2014">Han, H., Jain, A. K. <title
                        rend="quotes">Age, gender and race estimation from unconstrained face
                        images</title>, Dept. Comput. Sci. Eng., Michigan State Univ., East Lansing,
                    MI, USA, MSU Tech. Rep.(MSU-CSE-14-5). (2014).</bibl>
                <bibl xml:id="han2015" label="Han et al. 2015">Han, H., Otto, C., Liu, X., Jain, A.
                    K. <title rend="quotes">Demographic estimation from face images: Human vs.
                        machine performance</title>
                    <title rend="italic">IEEE Transactions on Pattern Analysis &amp; Machine
                        Intelligence</title>: 1148–1161. (2015).</bibl>
                <bibl xml:id="hipp2013" label="Hipp et al. 2013">Hipp, J. A., Adlakha, D., Gernes,
                    R., Kargol, A., Pless, R. <title rend="quotes">Do you see what I see:
                        crowdsource annotation of captured scenes</title>
                    <title rend="italic">SenseCam 2013 Proceedings the 4th International SenseCam
                        &amp; Pervasive Imaging Conference</title>, San Diego, California, November
                    2013: 24–25. <ref target="https://doi.org/10.1145/2526667.2526671"
                        >https://doi.org/10.1145/2526667.2526671</ref> (2013).</bibl>
                <bibl xml:id="hipp2015" label="Hipp et al. 2015">Hipp, J. A., Manteiga, A., Burgess,
                    A., Stylianou, A., Pless, R. <title rend="quotes">Cameras and crowds in
                        transportation tracking</title>
                    <title rend="italic">WH 2015 Proceedings of the conference on Wireless
                        Health</title>, Bethesda, Maryland October 2015: 1–8. <ref
                        target="https://doi.org/10.1145/2811780.2811941"
                        >https://doi.org/10.1145/2811780.2811941</ref> (2015).</bibl>
                <bibl xml:id="hochman2016" label="Hochman et al. 2016">Hochman, N., Manovich, L.,
                    Chow, J. <title rend="italic">Phototrails: Visualizing 2.3 M Instagram photos
                        from 13 global cities</title>. URL <ref
                        target="http://lab.culturalanalytics.info/2016/04/phototrails-visualizing-23-m-instagram.html"
                        >http://lab.culturalanalytics.info/2016/04/phototrails-visualizing-23-m-instagram.html</ref>
                    (accessed 12.30.16).</bibl>
                <bibl xml:id="irani2015" label="Irani 2015">Irani, L. <title rend="quotes">The
                        cultural work of microwork</title>, New Media &amp; Society 17: 720–739.
                        <ref target="https://doi.org/10.1177/1461444813511926"
                        >https://doi.org/10.1177/1461444813511926</ref> (2015).</bibl>
                <bibl xml:id="james2014" label="James 2014">James, J. Data Never Sleeps 2.0 | Domo.
                    URL <ref target="https://www.domo.com/blog/data-never-sleeps-2-0/"
                        >https://www.domo.com/blog/data-never-sleeps-2-0/</ref> (accessed 9.20.18)
                    (2014).</bibl>
                <bibl xml:id="jofre2018" label="Jofre et al. 2018">Jofre, A., Berardi, V., and
                    Brennan, K.. <title rend="quotes">Time magazine archive: Annotating Faces,
                        Visualizations, and Alternative Applications</title> (Workshop), <title
                        rend="italic">IPAM Culture Analytics Reunion Conference II</title>, Lake
                    Arrowhead, California, December. (2018).</bibl>
                <bibl xml:id="jofre2020a" label="Jofre et al. 2020a">Jofre, A., Berardi, V.,
                    Bennett, C., Reale, M., Cole, J.. <title rend="quotes">Dataset: Faces extracted
                        from <title rend="italic">Time</title> Magazine 1923-2014</title>, <title
                        rend="italic">Journal of Cultural Analytics</title>. March 16, 2020, <ref
                        target="https://doi.org/10.22148/001c.12265"
                        >https://doi.org/10.22148/001c.12265</ref> (2020)</bibl>
                <bibl xml:id="jofre2020b" label="Jofre et al. 2020b">Jofre, A., Cole, J., Berardi,
                    V., Bennett, C., Reale, M. <title rend="quotes">What’s in a Face? Gender
                        representation of faces in <title rend="italic">Time</title>,
                        1940s-1990s</title>, <title rend="italic">Journal of Cultural
                        Analytics</title>. March 16, 2020 <ref
                        target="https://doi.org/10.22148/001c.12266"
                        >https://doi.org/10.22148/001c.12266</ref> (2020)</bibl>
                <bibl xml:id="king2016" label="King and Leonard 2016">King, L., Leonard, P. <title
                        rend="italic">Robots Reading Vogue</title>. Yale DHLab. URL <ref
                        target="http://dh.library.yale.edu/projects/vogue/"
                        >http://dh.library.yale.edu/projects/vogue/</ref> (accessed 11.8.16).</bibl>
                <bibl xml:id="kittur2008" label="Kittur et al. 2008">Kittur, A., Chi, E. H., Suh, B.
                        <title rend="quotes">Crowdsourcing user studies with Mechanical
                    Turk</title>. In: <title rend="italic">CHI 2008 Proceedings of the SIGCHI
                        Conference on Human Factors in Computing Systems</title>, Florence, Italy,
                    April 2008: 453-456. <ref target="https://doi.org/10.1145/1357054.1357127"
                        >https://doi.org/10.1145/1357054.1357127</ref> (2008).</bibl>
                <bibl xml:id="kuang2015" label="Kuang et al. 2015">Kuang, J., Argo, L., Stoddard,
                    G., Bray, B. E., Zeng-Treitler, Q. <title rend="quotes">Assessing Pictograph
                        Recognition: A Comparison of Crowdsourcing and Traditional Survey
                        Approaches</title>. <title rend="italic">J Med Internet Res</title> 17. <ref
                        target="https://doi.org/10.2196/jmir.4582"
                        >https://doi.org/10.2196/jmir.4582</ref> (2015).</bibl>
                <bibl xml:id="lecun2015" label="LeCun et al. 2015">LeCun, Y., Bengio, Y., Hinton, G.
                        <title rend="quotes">Deep learning</title>, <title rend="italic"
                        >Nature</title> 521: 436–444. <ref
                        target="https://doi.org/10.1038/nature14539"
                        >https://doi.org/10.1038/nature14539</ref> (2015).</bibl>
                <bibl xml:id="lin2017" label="Lin et al. 2017">Lin, T., Goyal, P, Girshick, R., He,
                    K., &amp; Dollar, P. <title rend="quotes">Focal Loss for Dense Object
                        Detection</title>, <title rend="italic">The IEEE International Conference on
                        Computer Vision (ICCV)</title>, October. <ref
                        target="https://arxiv.org/abs/1708.02002"
                        >https://arxiv.org/abs/1708.02002</ref> (2017).</bibl>
                <bibl xml:id="lin2018" label="Lin et al. 2018">Lin, T., Goyal, P, Girshick, R., He,
                    K., &amp; Dollar, P. GitHub Repository. <ref
                        target="https://github.com/fizyr/keras-retinanet"
                        >https://github.com/fizyr/keras-retinanet</ref> (accessed 2018).</bibl>
                <bibl xml:id="leonard2018" label="Leonard and Duhaime 2018">Leonard, P., Duhaime, D.
                        <title rend="italic">Yale DHLab - Neural Neighbors: Capturing Image
                        Similarity</title>. Yale DHLab. URL <ref
                        target="http://dhlab.yale.edu/projects/neural_neighbors.html"
                        >http://dhlab.yale.edu/projects/neural_neighbors.html</ref> (accessed
                    9.14.18).</bibl>
                <bibl xml:id="lookmagazine2012" label="Look Magazine 2012">Look Magazine Photograph
                    Collection, Library of Congress, Prints &amp; Photographs Division. Library of
                    Congress, Washington, D.C. 20540 USA. URL <ref
                        target="https://www.loc.gov/collections/look-magazine/about-this-collection/"
                        >https://www.loc.gov/collections/look-magazine/about-this-collection/</ref>
                    (accessed 9.20.18).</bibl>
                <bibl xml:id="malli2018" label="Malli et al. 2018">Malli, R.C., Suri A., &amp;
                    Ramírez S. Github Repository <ref
                        target="https://github.com/rcmalli/keras-vggface"
                        >https://github.com/rcmalli/keras-vggface</ref> (accessed 2018).</bibl>
                <bibl xml:id="manovich2009" label="Manovich and Douglass 2009">Manovich, L., and
                    Douglass, J. Timeline: 4535 <title rend="italic">Time</title> magazine covers,
                    1923-2009. <ref target="https://www.flickr.com/photos/culturevis/3951496507/"
                        >https://www.flickr.com/photos/culturevis/3951496507/</ref> (2009).</bibl>
                <bibl xml:id="manovich2016" label="Manovich et al. 2016">Manovich, L., Stefaner, M.,
                    Yazdani, M., Baur, D., Goddemeyer, D., Tifentale, A., Chow, J. selfiecity,
                    selfiecity. URL <ref target="http://selfiecity.net/"
                        >http://selfiecity.net/</ref> (accessed 12.30.16).</bibl>
                <bibl xml:id="nowak2010" label="Nowak and Rüger 2010">Nowak, S., Rüger, S. <title
                        rend="quotes">How reliable are annotations via crowdsourcing: a study about
                        inter-annotator agreement for multi-label image annotation</title>
                    <title rend="italic">MIR 2010 Proceedings of the international conference on
                        Multimedia information retrieval</title>, Philadelphia, Pennsylvania, March
                    2010: 557-566. <ref target="https://doi.org/10.1145/1743384.1743478"
                        >https://doi.org/10.1145/1743384.1743478</ref> (2010).</bibl>
                <bibl xml:id="nypllabs2018" label="NYPL Labs 2018">NYPL Labs. The New York Public
                    Library. URL <ref target="https://www.nypl.org/collections/labs"
                        >https://www.nypl.org/collections/labs</ref> (accessed 9.14.18).</bibl>
                <bibl xml:id="nyplmap2018" label="NYPL Map Warper 2018">NYPL Map Warper: Home. URL
                        <ref target="http://maps.nypl.org/warper">http://maps.nypl.org/warper</ref>
                    (accessed 9.14.18).</bibl>
                <bibl xml:id="opencv" label="OpenCV – CiteOpenCV 2017">OpenCV - CiteOpenCV - OpenCV
                    DevZone. URL <ref
                        target="http://code.opencv.org/projects/opencv/wiki/CiteOpenCV"
                        >http://code.opencv.org/projects/opencv/wiki/CiteOpenCV</ref> (accessed
                    1.5.17).</bibl>
                <bibl xml:id="organisciak2012" label="Organisciak et al. 2012">Organisciak, P.,
                    Efron, M., Fenlon, K., Senseney, M. <title rend="quotes">Evaluating rater
                        quality and rating difficulty in online annotation activities</title>,
                        <title rend="italic">Proceedings of the American Society for Information
                        Science and Technology</title> 49(1): 1-10. <ref
                        target="https://doi.org/10.1002/meet.14504901166"
                        >https://doi.org/10.1002/meet.14504901166</ref> (2012).</bibl>
                <bibl xml:id="parkhi2015" label="Parkhi et al. 2015">Parkhi, O. M., Vedaldi, A.,
                    Zisserman, A. <title rend="quotes">Deep Face Recognition</title>
                    <title rend="italic">Proceedings of the British Machine Vision Conference
                        2015</title>, Swansea, UK, September 2015: 41.1-41.12. <ref
                        target="https://doi.org/10.5244/C.29.41"
                        >https://doi.org/10.5244/C.29.41</ref> (2015).</bibl>
                <bibl xml:id="prendergast1986" label="Prendergast and Colvin 1986">Prendergast, C.,
                    and Colvin, G. <title rend="italic">The world of Time Inc.: The intimate history
                        of a changing enterprise</title>, Volume 3: 1960-1980. New York
                    (1986).</bibl>
                <bibl xml:id="silberman2018" label="Silberman et al. 2018">Silberman, M. S.,
                    Tomlinson, B., LaPlante, R., Ross, J., Irani, L., Zaldivar, A. <title
                        rend="quotes">Responsible research with crowds: pay crowdworkers at least
                        minimum wage</title>, <title rend="italic">Communications of the ACM</title>
                    61: 39–41. <ref target="https://doi.org/10.1145/3180492"
                        >https://doi.org/10.1145/3180492</ref> (2018).</bibl>
                <bibl xml:id="tomnod2018" label="Tomnod 2018">Tomnod, Tomnod. URL <ref
                        target="https://www.tomnod.com">https://www.tomnod.com</ref> (accessed
                    9.17.18).</bibl>
                <bibl xml:id="wang2011" label="Wang et al. 2011">Wang, J., Ipeirotis, P.G., Provost,
                    F. <title rend="quotes">Managing Crowdsourcing Workers</title>
                    <title rend="italic">The 2011 Winter Conference on Business
                    Intelligence</title>, Salt Lake City, Utah: 10-12. (2011).</bibl>
                <bibl xml:id="welinder2010" label="Welinder and Perona 2010">Welinder, P., Perona,
                    P. <title rend="quotes">Online crowdsourcing: Rating annotators and obtaining
                        cost-effective labels</title>
                    <title rend="italic">2010 IEEE Computer Society Conference on Computer Vision
                        and Pattern Recognition – Workshops</title>, San Francisco, California, June
                    2010: 25–32. <ref target="https://doi.org/10.1109/CVPRW.2010.5543189"
                        >https://doi.org/10.1109/CVPRW.2010.5543189</ref> (2010).</bibl>
                <bibl xml:id="whitehill2009" label="Whitehill et al. 2009">Whitehill, J., Ruvolo,
                    P., Wu, T., Bergsma, J., Movellan, J. <title rend="quotes">Whose vote should
                        count more: Optimal integration of labels from labelers of unknown
                        expertise</title>, <title rend="italic">Advances in Neural Information
                        Processing Systems</title> 22: 2035-2043. (2009).</bibl>
                <bibl xml:id="yu2013" label="Yu et al. 2013">Yu, B., Willis, M., Sun, P., Wang, J.
                        <title rend="quotes">Crowdsourcing Participatory Evaluation of Medical
                        Pictograms Using Amazon Mechanical Turk</title>, <title rend="italic">J Med
                        Internet Res</title> 15. <ref target="https://doi.org/10.2196/jmir.2513"
                        >https://doi.org/10.2196/jmir.2513</ref> (2013).</bibl>
            </listBibl>
        </back>
    </text>
</TEI>
