<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" />
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
   </head>
   <body>
      <div id="mainContent">
         <div class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Experiments in Distant Reading: Using Topic Modeling on Chinese Buddhist Texts from
                  500-800 CE</h1>
               
               
               <div class="author"><span style="color: grey">Marcus Bingenheimer</span> &lt;<a href="mailto:m_dot_bingenheimer_at_gmail_dot_com" onclick="javascript:window.location.href='mailto:'+deobfuscate('m_dot_bingenheimer_at_gmail_dot_com'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('m_dot_bingenheimer_at_gmail_dot_com'); return false;">m_dot_bingenheimer_at_gmail_dot_com</a>&gt;, Temple University</div>
               
               
               <div class="author"><span style="color: grey">Justin  Brody</span> &lt;<a href="mailto:jdbrody_at_gmail_dot_com" onclick="javascript:window.location.href='mailto:'+deobfuscate('jdbrody_at_gmail_dot_com'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('jdbrody_at_gmail_dot_com'); return false;">jdbrody_at_gmail_dot_com</a>&gt;, Franklin and Marshall College</div>
               
               
               <div class="author"><span style="color: grey">Ryan  Nichols</span> &lt;<a href="mailto:rnichols_at_fullerton_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('rnichols_at_fullerton_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('rnichols_at_fullerton_dot_edu'); return false;">rnichols_at_fullerton_dot_edu</a>&gt;, Califoria State University, Fullerton</div>
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Experiments%20in%20Distant%20Reading%3A%20Using%20Topic%20Modeling%20on%20Chinese%20Buddhist%20Texts%20from%20500-800%20CE&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=Bingenheimer&amp;rft.aufirst=Marcus&amp;rft.au=Marcus%20Bingenheimer&amp;rft.au=Justin%20Brody&amp;rft.au=Ryan%20Nichols"> </span></div>
            
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  <p>The article tries to answer whether the BERTopic topic modeling framework can be used
                     to obtain topics that meaningfully distinguish two corpora of Buddhist Chinese texts
                     from 500 to 800 CE. 
                     The first corpus consists of translated “Indian-Chinese” Buddhist texts, the second of “Chinese-Chinese” texts, i.e. texts directly authored
                     in Buddhist Chinese. 
                     Does the application of topic modeling reveal aspects that are typical for these corpora
                     and do these topics suggest avenues for 
                     future research into the sinicization of Buddhism that took place during that time?</p>
                  
                  <p>For our implementation of BERTopic, we used the customized GuwenBERT, a language model
                     trained on classical Chinese. 
                     To reduce the dimensionality of the embeddings we used the UMAP algorithm. Next, the
                     HDBSCAN takes care of hierarchical clustering. 
                     The most relevant words of each cluster are identified with c-tf-idf. As a last step,
                     we score each cluster by its monochromaticity – 
                     this is a measure of how likely the documents in the cluster are to be derived from
                     either just the “Chinese-Chinese” or just the “Indian-Chinese” documents.
                     </p>
                  
                  <p>In order to communicate the topics we create virtual paragraphs that combine most
                     of the top twenty terms that represent a sample of ten highly monochromatic topics.
                     
                     Discussing these topics from a Buddhist Studies point of view, we find that our modified
                     BERTopic workflow does indeed return topics that are characteristic of their 
                     corpus and highlights facets that help to understand the process of how Buddhism became
                     sinicized in the three centuries between 500 and 800 CE. 
                     Thus distant reading of latent topics in the corpus is possible. While some topics
                     are in themselves unsurprising, others highlight new promising areas for research.
                     </p>
                  </div>
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">1. Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">In an article on the “Genealogy of Distant Reading” Ted Underwood made the valid point that distant reading as a form of 
                     “macroscopic literary inquiry” did not originate with digital text [<a class="ref" href="#underwood2017">Underwood 2017</a>]. Still, all DH methods, by definition, process digital data, and most can only be
                     applied effectively only within a computational setting. 
                     While some methods (e.g. markup or GIS) extend analog practice (such as editing or
                     cartography) into the digital, others are digital natives.
                     Topic modeling is one of the computational methods that over the last twenty years
                     have made their way from Natural Language Processing [<a class="ref" href="#blei_etal2003">Blei et al2003</a>] into the Digital Humanities. 
                     It has been consistently associated with distant reading. In the introduction to a
                     special issue on topic modeling in the <cite class="title italic">Journal of the Digital Humanities</cite> the editors even called it 
                     “distant reading in the most pure sense” [<a class="ref" href="#meeks_weingart2012">Meeks and Weingart 2012</a>]. 
                     
                     <a class="noteRef" href="#d4e249">[1]</a>  
                     
                     As digital corpora grow ever larger, so does the need for distant reading. Thus topic
                     modeling has survived early hype and critique [<a class="ref" href="#schmidt2012">Schmidt 2012</a>]
                     and is still going strong in DH [<a class="ref" href="#du2019">Du 2019</a>] and in information retrieval in general. Originally dominated by the application
                     of Latent Dirichlet Allocation [<a class="ref" href="#blei_etal2003">Blei et al2003</a>], by now there are several different approaches to topic modeling. 
                     Comparing their respective strengths and weaknesses has resulted in a veritable sub-genre
                     of comparative studies [<a class="ref" href="#kherwa_bansal2019">Kherwa and Bansal 2019</a>], [<a class="ref" href="#vayansky_kumar2020">Vayansky and Kumar 2020</a>], [<a class="ref" href="#fu_etal2020">Fu et al 2020</a>], [<a class="ref" href="#ma_etal2021">Ma et al 2021</a>], [<a class="ref" href="#eggar_yu2022">Egger and Yu 2022</a>], [<a class="ref" href="#r%C3%BCdiger_etal2022">Rüdiger et al 2022</a>], [<a class="ref" href="#chen_etal2023">Chen et al 2023</a>]. 
                     In this paper we will use a modified version of BERTopic [<a class="ref" href="#grootendorst2022">Grootendorst 2022</a>], a new, neural network based approach, to distant read two related text corpora in
                     Buddhist Chinese, a low-resource idiom. 
                     Although an attempt has been made to use BERTopic on Chinese poetry [<a class="ref" href="#fang2023">Fang 2023</a>], to our knowledge this is the first time BERTopic has been used on canonical Buddhist
                     texts in any language. 
                     </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Our first question is whether BERTopic, which relies on BERT for word embeddings,
                     does return coherent topics at all when confronted with a low-resource idiom such
                     as Buddhist Chinese, which was not represented in the training data for BERT, 
                     and for which tokenization algorithms are available but not widely implemented [<a class="ref" href="#wang2020">Wang 2020</a>]. Secondly, we would like to know whether the application of the BERTopic workflow
                     can yield meaningful topics that 
                     can be taken to express concerns in Chinese Buddhist texts created between 500 and
                     800 CE and might in particular be used to distinguish two different corpora, Indian-Chinese
                     and Chinese-Chinese texts, and how they might be used in furthering our understanding
                     of the texts.
                     </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">Why this particular time-frame? In the centuries between 500 and 800 CE, Buddhism
                     in China turned into Chinese Buddhism. The first Indian Buddhist texts were translated
                     into Chinese in the second century. Starting from the late third century 
                     we are able to reconstruct an unbroken social network that connects generations of
                     Chinese Buddhists until today [<a class="ref" href="#bingenheimer2021">Bigenheimer 2021</a>]. 
                     
                     <a class="noteRef" href="#d4e287">[2]</a>
                     
                     Until the 6th century this network was mainly informed by Indian-Chinese texts, i.e.
                     Indian texts 
                     translated into Chinese. Chinese-Chinese texts, i.e. texts authored by Chinese (or
                     Korean or Japanese) writers, began as paratext (prefaces, catalogs etc.) to these
                     translations, then developed via short essays, apocrypha and commentaries, 
                     eventually blossoming into long historiographical and doctrinal works in the 6th century.
                     The time between 500 and 800 is special in the sense that in spite of the continuing
                     reception and translation of Indian texts, it were the Chinese-Chinese texts 
                     that were produced during that period that became distinctive for later Chinese Buddhism.
                     The philosophical works of Zhiyi 智顗 (538–597) and Fazang 法藏 (643–712), 
                     the devotional treatises of Tanluan曇鸞 (476-542) and Shandao善導 (613–681), and the recorded sayings of early Chan masters like Huineng 慧能 (638–713) and Mazu Daoyi 馬祖道一 (709–788), 
                     to give but a few examples, became the textual foundation for the dominant traditions
                     within Chinese, and indeed East Asian, Buddhism of the second millennium: Chan, Pure
                     Land, and Tiantai Buddhism.                     
                     </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">While in the year 500 CE Buddhism in China was still very much dominated by the influx
                     of Indian Buddhist ideas, by 800 CE Chinese Buddhists had started to put their own
                     spin on Buddhism. Most features that would come to characterize Chinese Buddhism were
                     in place, 
                     even if it took another few centuries for them to fully form: the dichotomy, both
                     stable and dynamic, between meditative Chan and devotional Pure Land practice, an
                     interest in historiography and lineage, a commitment to the lay-monastic distinction,
                     
                     and the necessity of apologetics in the framework of the Chinese state. While Buddhism
                     declined in India after 800 CE and became virtually extinct there after the 12th century,
                     Chinese Buddhism blossomed throughout the Song Dynasty (960–1279) 
                     and today remains the largest world religion in China with more than 200 million adherents
                     [<a class="ref" href="#pewresearchcenter2023">Pew Research Center 2023</a>].
                     </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">The topics returned by topic modeling methods are notoriously open to interpretation.
                     Whether or not they cohere with respect to a particular domain can only be evaluated
                     by domain experts. In order to communicate how we read a topic to a wider audience
                     we combine the lists of terms that represent topics into “virtual paragraphs” (Appendix
                     A). In that way we hope to make our reading transparent for non-specialists who can
                     easily grasp what the topic is about. 
                     Creating virtual paragraphs from topics could also be used in teaching. When practiced
                     in conjunction with close, linear reading of classical texts, students stand to gain
                     familiarity with concepts and semantic fields that are latent in larger corpora.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">2. Data</h1>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">Based on the largest digital archive of Chinese Buddhist texts [<a class="ref" href="#cbetaver2021">CBETA ver 2021</a>] we have assembled two corpora of comparable size. 
                     
                     <a class="noteRef" href="#d4e344">[3]</a> 
                     
                     
                     <ol class="list">
                        <li class="item">Indian-Chinese: This consists of all 661 originally Indian texts in the CBETA corpus
                           that were translated into Buddhist Chinese between 500 and 800 CE. Total number of
                           characters: 17,959,037.</li>
                        <li class="item">Chinese-Chinese: This corpus consists of all 293 texts in the CBETA corpus that were
                           composed directly in Buddhist Chinese between 500 and 800 CE. Total number of characters:
                           21,136,841.</li>
                     </ol>
                     Both corpora include material from before 500 CE. Obviously, the Indian-Chinese texts
                     are translations and might have been authored before 500 CE. 
                     But the Chinese-Chinese corpus too, although authored between 500 and 800 CE, contains
                     commentaries on older texts and thus includes material (and topics) from earlier periods.
                     
                     This caveat notwithstanding the corpora represent two different modes of textual production
                     in Chinese Buddhism during that period and an experiment in modeling is justified
                     and might yield new insights. 
                     In Section 4 we will try to interpret select topics guided by the hypothesis that
                     the topics in these two corpora differ meaningfully. It should be understood that
                     the topic modeling workflow we implemented does not process the two corpora separately.
                     
                     Whether a topic is aligned with the Indian-Chinese or the Chinese-Chinese corpus is
                     determined by its monochromaticity (s. Sec. 3). For the topic modeling pipeline described
                     here we use these texts in the tokenized, segmented version produced by Yu-chun Wang (making use of the Conditional Random Field (CRF) model he describes in [<a class="ref" href="#wang2020">Wang 2020</a>]).  
                     
                     <a class="noteRef" href="#d4e362">[4]</a>
                     
                     This is currently the most consistently segmented available corpus for Buddhist Chinese.
                     </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">We use the term “Buddhist Chinese” here for the particular idiom (or set of idioms) in which pre-modern Buddhist texts
                     were composed. Needless to say, across 1700 years there was a lot of variation. 
                     The translation of Indian Buddhist texts into Chinese was never standardized and there
                     exists a many-to-many relationship in the translation of terms: One Indian word could
                     be rendered by different Chinese characters, 
                     and the same Chinese character was used for different Indian terms. Chinese-Chinese
                     Buddhist texts were generally closer to the classical idiom of literary Chinese than
                     translated texts, but they share features with translated texts 
                     that sets them apart from the usual “classical Chinese” written in that period. Among these features are the morphology of Buddhist vocabulary
                     which makes words more likely to be compounds of two more more characters, 
                     instead of the preferred one-character = one-semantic-unit equivalence of “<em class="emph">classical</em> classical Chinese.”
                     
                     <a class="noteRef" href="#d4e379">[5]</a>
                     </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">The reason why there are more than twice as many Indian-Chinese than Chinese-Chinese
                     texts is because in the eight century a large number of short dhāraṇī text were translated,
                     or in fact rather transcribed. The main component of such texts is the dhāraṇī itself: 
                     a Sanskrit invocation or spell, which was used in the context of esoteric ritual (see
                     also Sec. 4). Although longer than a mantra, dhāraṇīs could not be too long as many
                     of them were supposed to be remembered by heart, but within esoteric Buddhism they
                     were popular. 
                     Thus the Indian-Chinese corpus consists of a larger number of short texts, in spite
                     of the corpus being overall smaller than the Chinese-Chinese corpus in terms of characters.
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">3. Method</h1>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">Our method follows BERTopic [<a class="ref" href="#grootendorst2022">Grootendorst 2022</a>], a recent topic modeling framework that has quickly attracted attention. 
                     
                     <a class="noteRef" href="#d4e397">[6]</a> 
                     
                     We use our own variation of the BERTopic pipeline, which we make available at <a href="https://github.com/mbingenheimer/cbetaCorpusSorted" onclick="window.open('https://github.com/mbingenheimer/cbetaCorpusSorted'); return false" class="ref">https://github.com/mbingenheimer/cbetaCorpusSorted</a>.
                     
                     <a class="noteRef" href="#d4e402">[7]</a>
                     
                     
                     Specifically, our implementation involves the following steps:
                     
                     <ol class="list">
                        <li class="item">Using a large language model trained on classical Chinese (Koichi Yasuoka’s variant of GuwenBERT), we embed every sentence in our corpus into a high dimensional
                           vector space.</li>
                        <li class="item">We perform UMAP to reduce the sentence embeddings to a 3-dimensional set.</li>
                        <li class="item">We use HDBSCAN to determine clusters which we interpret as topics.</li>
                        <li class="item">The words in the sentences of each topic are collected and ranked according to c-tf-idf
                           to measure the degree to which they represent the entire topic.</li>
                        <li class="item">The individual sentences are tagged as to their provenance (whether they come from
                           a Chinese-Chinese or a Chinese-Indian source). This allows us to compute the monochromaticity
                           of each cluster and determine which clusters are mixed versus 
                           which ones are representative of one or the other of our corpora.</li>
                     </ol>
                     </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">The main idea of BERTopic is to take advantage of the capacity of modern neural networks
                     to create sentence embeddings which respect <em class="emph">semantic similarity</em>, meaning that linguistic elements with similar meaning should be embedded close to
                     each other in the target vector space. 
                     While many previous techniques are based on mapping language elements to vectors (with
                     word2vec probably being the best-known), encoder-only models like BERT have the advantage
                     of embedding language in a way that is sensitive to semantics. In particular the embedding
                     of a multivalent word <em class="emph">bank</em> is dependent on the surrounding sentence. 
                     Thus homonyms can be disambiguated in the embedding. At the same time synonyms like
                     <em class="emph">happy</em> and <em class="emph">joyful</em> that have a similar probability of occurring at a particular position in a sentence
                     are defined with similar embeddings. As a result, any BERT model can be expected to
                     display some degree of identifying semantic similarity. 
                     Specifically, these models operate by learning how to predict missing tokens (e.g.
                     words) from a natural language sentence. To be successful, the model has to learn
                     the probabilities associated with various possible words at a specific point in the
                     sentence. 
                     This will necessarily entail disambiguating homonyms – if it would represent all instances
                     of the word bank in the same way then it could not correctly learn the context-dependent
                     likelihoods of the words <em class="emph">money</em> and <em class="emph">river</em> occurring elsewhere in the sentence. 
                     Similarly, it would be helpful (if not strictly necessary) for synonyms to have close
                     embeddings; this would allow them to automatically treat their influence on the rest
                     of the sentence similarly.
                     
                     </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">Moreover, BERT models can be explicitly trained to respect semantic similarity – this
                     requires a custom dataset designed for this purpose. In our case we chose Koichi Yasuoka’s
                     variant of GuwenBERT as our base model.
                     
                     <a class="noteRef" href="#d4e443">[8]</a>
                     
                     GuwenBERT is a BERT model which is trained on the Daizhige 殆知阁 dataset, which according to the creators of GuwenBERT contains “15,694 books in Classical Chinese, 
                     covering Buddhism, Confucianism, Medicine, History, ...Taoism, [and others]”. 
                     
                     <a class="noteRef" href="#d4e460">[9]</a>
                     
                     Whereas the original GuwenBERT was trained on the corpus in simplified Chinese, Yasuoka’s variant allows for traditional Chinese characters. To our knowledge, there is no
                     semantic similarity training set for Classical Chinese; 
                     thus we rely on the level of semantic similarity native to our base model. 
                     
                     <a class="noteRef" href="#d4e474">[10]</a>
                     
                     This results in the danger of outputting collocations instead of topics: For example,
                     our pipeline sometimes picks up on ngrams that contain “十一eleven (-somethings)” or
                     “四十forty (-somethings)”, resulting in clusters that are based on one or two characters
                     rather than genuine, 
                     domain-specific semantic clustering. By grouping for instance, “ 四十 (forty),四十二 (Forty-two),
                     四十九 (forty nine), 四十八 (forty-eight), 四十四 (Forty-four), 四十一 (forty one), 四十六 (Forty
                     -six)” in one cluster the pipeline does its job 
                     (it picks up on the signal of “forty”), but because we are (not yet) able to fine
                     tune it to produce more semantically relevant clusters. The more a cluster looks like
                     a snippet from a KWIC index, 
                     the less interesting it is for topic modeling, because we are looking for semantically
                     related terms not the same term in different contexts.
                     </div>
                  
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">We pass each sentence of each document into our base model, thereby obtaining a high-dimensional
                     vector representation of each sentence. These sentences are tagged according to the
                     corpus they originated from. It is understood within data science that high dimensional
                     datasets are difficult to work with; 
                     many mathematical algorithms designed for low dimensional datasets break down in high
                     dimensions – this is known as the <em class="emph">curse of dimensionality</em>. Thus our set of high dimensional sentence representation is then passed through
                     the UMAP algorithm [<a class="ref" href="#mcinnes_etal2018">McInnes et al 2018</a>]. 
                     which reduces the representations to being 3-dimensional. UMAP is designed so that
                     representations which are close (in some sense) in the high dimensional space will
                     get transformed into representations which are still close in 3-dimensional space.
                     
                     This ensures that clusters that are observed in the 3 dimensional space do actually
                     correspond to representations which are close in the high-dimensional space. If our
                     embeddings display a high degree of semantic similarity, 
                     this should mean that sentences whose representations are clustered close together
                     actually represent sentences with related meanings.
                     </div>
                  
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">Having reduced the sentences representations to three dimensions, we use HDBSCAN [<a class="ref" href="#mcinnes_etal2017">McInnes et al 2017</a>], a clustering algorithm, to identify and extract clusters. Each cluster is comprised
                     of a set of sentences. To uncover what topic each cluster represents, the sentences
                     are broken into words. 
                     Each word in the cluster is given a score determined by the frequency of the word
                     in the cluster, multiplied by the information content (or rarity) of the word in the
                     corpus. This metric is known as c-tf-idf. We take the top 20 scoring words as indicative
                     of the cluster's content. 
                     As usual with topic modeling, the total number of topics varies with parameterization.
                     </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">Finally we score each cluster by how <em class="emph">monochromatic</em> (in our case: purely derived from the “Chinese-Chinese” corpus) and how large it is (i.e. how many sentences are represented in the cluster).
                     Our intuition is that large clusters are more representative of the corpus. 
                     Monochromaticity (MC) is expressed by a score between 0 and 1, with values converging
                     to 0 indicating a cluster largely derived from the Indian-Chinese corpus, convergence
                     to 1 indicates a cluster mostly derived from the Chinese-Chinese corpus, and clusters
                     with MC around 0.5 are derived equally from both corpora.
                     </div>
                  
                  <div class="counter"><a href="#p15">15</a></div>
                  <div class="ptext" id="p15">Below an attempt to visualize the results of this process:
                     
                     
                     <div id="figure01" class="figure">
                        
                        
                        <a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" style="" alt="A screenshot showing the visualizations of the topics and their chromaticity, in color. &#xA;                         " /></a>      
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>Visualizations of the topics (left) and their chromaticity (right). Both images are
                           projected from 3-dimensional space, which itself is a reduction from a 1024-dimensional
                           vector space. 
                           On the left, individual topics are shown with different clusters in (randomly) different
                           colors. 
                           On the right, each point in the embedding space is colored red if it corresponds to
                           a sentence from a Chinese-Chinese text 
                           and blue if it corresponds to an Indian-Chinese sentence. The images are taken from
                           different viewing angles, 
                           thus the displayed clusters do not align.
                           </div>
                     </div>
                     
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">4. Discussion</h1>
                  
                  <div class="counter"><a href="#p16">16</a></div>
                  <div class="ptext" id="p16">Orienting ourselves by monochromaticity and cluster size allows us to focus on the
                     “<span class="hi bold">coherent</span>” topics that have the best chance at being “<span class="hi bold">meaningful</span>” for distinguishing between Indian-Chinese and Chinese-Chinese corpora. We found
                     it useful to distinguish between these two orders of sense-making: 
                     We define “coherent” here not by one of the several computed coherence metrics for
                     topic modeling (such as Umass, C-V etc.). 
                     In the case of BERTopic, that is already taken care of during the dimensionality reduction
                     via HDBSCAN. 
                     Rather, we use human evaluation to decide whether the terms in a topic semantically
                     relate to each other in a way that allows a reader with domain knowledge 
                     to string them together in virtual paragraphs (Appendix A) without forcing associations.
                     Domain knowledge remains indispensable for such exercises. 
                     On trying out the classic tf-idf formula on Woolf’s The Waves, Stephen Ramsay [<a class="ref" href="#ramsay2011">Ramsay 2011</a>, 12] wrote: “Few readers of The Waves would fail to see some emergence of pattern in this list”. 
                     It should be added that, conversely, for someone who has not read the text “patterns”
                     are highly unlikely to emerge. A modified form of tf-idf remains part of BERTopic
                     and other topic modeling frameworks, 
                     and although the topic coherence as assessed by computable metrics arguably has improved,
                     the need for domain knowledge for “the ‘lighting up’ of an aspect (das ‘Aufleuchten’ eines Aspekts) remains. 
                     
                     <a class="noteRef" href="#d4e536">[11]</a>
                     </div>
                  
                  <div class="counter"><a href="#p17">17</a></div>
                  <div class="ptext" id="p17">Beyond seeing a coherent pattern in the word lists that represent topics, a second-order
                     of sense making is relevant. “Meaningful” topics must be meaningful not only in themselves
                     (i.e. coherent) but also relevant in the wider context of a research question. 
                     Do the topics make heuristic sense for a researcher in Buddhist studies? Are they
                     insightful to the distant reader? In our case, do they actually distinguish the Indian-Chinese
                     from the Chinese-Chinese texts or are they just different? 
                     Obviously, coherence here is a condition of meaningfulness. If the top-twenty words
                     that represent a topic do not cohere, there is no topic to discuss in a wider context.
                     
                     </div> 
                  
                  <div class="counter"><a href="#p18">18</a></div>
                  <div class="ptext" id="p18">The particular iteration from which the ten topics below are taken yielded c. 750
                     clusters, but there is no natural upper or lower limit for the amount of clusters
                     produced in any given pipeline. 
                     How did we select the ten clusters we discuss below, that we have identified as coherent
                     and meaningful? Like with the overall amount of clusters, 
                     “ten” too is an arbitrary limit that takes into account what can be comfortable presented
                     to DHQ readers. 
                     Arguments could be made for longer discussions of five topics, or a more concise presentation
                     of twenty. 
                     Our selection “algorithm” was: Descending from the most monochromatic cluster 
                     (which are most likely to distinguish Indian-Chinese from Chinese-Chinese texts) 
                     select the first ten clusters that are both “coherent” and “meaningful” in the sense sketched above. 
                     Different researchers of Chinese Buddhism might come up with slightly different sets
                     but that is the not yet the point. 
                     Distant reading allows for hermeneutic difference just as close reading. Are there
                     many more clusters that are coherent and meaningful? 
                     This is hard to quantify, because “meaningful” implies a concrete research questions. In our case, reading through these lists of
                     topics, 
                     our intuition is that about one in ten clusters seems coherent, but of these, only
                     those with high monochromaticity should be expected 
                     to <em class="emph">meaningfully</em> distinguish the Indian-Chinese from the Chinese-Chinese corpus.
                     </div>
                  
                  <div class="counter"><a href="#p19">19</a></div>
                  <div class="ptext" id="p19">Below, we will discuss the heuristic value of ten topics suggested by BERTopic. These
                     are presented as ‘virtual paragraphs’ in Appendix A. 
                     What signals does a distant reading of our two corpora discern? Among the topics that
                     align strongly with the <span class="hi bold">Indian-Chinese</span> corpus the least surprising ones 
                     are two that relate to the introduction of tantric, esoteric Buddhism to China: <span class="hi italic">maṇḍala</span> <a href="#a1">(A1)</a> and <span class="hi italic">mantra</span> <a href="#a2">(A2)</a>. 
                     <em class="emph">Maṇḍalas</em> were used in Indian esoteric Buddhism in visualization practice, as part of rituals,
                     and as an art form. 
                     These three aspects are related as sophisticated <em class="emph">maṇḍala</em> paintings are not only used in rituals, but also as models for meditators who learn
                     to visualize them as part of their meditative practice. 
                     In India, the earliest Buddhist uses of <span class="hi italic">maṇḍala</span> images are attested for the 6th century, well within our time-frame. 
                     In China, a host of texts on how to use <span class="hi italic">maṇḍalas</span> in rituals were translated in the 8th century (e.g. in the Taishō edition (T): 
                     T0850, T0852a, T0852b, T0862, T0911, T0912, T0959, T1001, T1004, T1040, T1067, T1167,
                     T1168B, T1184). 
                     Texts containing <span class="hi italic">mantras</span> or the longer <span class="hi italic">dhāraṇī</span> spells (e.g. T0402, T0899, T0901, T0902, T0903, T0905, T0907, T0918, T0933, T0944A,
                     T0952, T0956, T0962, T0963, T0964, T0967, T0968) 
                     were already mentioned above as the reason why the Indian-Chinese corpus has twice
                     as many texts but an overall lower character count than the Chinese-Chinese corpus.
                     
                     Neither poetry nor prose, <span class="hi italic">mantra</span> and <span class="hi italic">dhāraṇī</span> texts are a distinctive genre in themselves. That distinctiveness, not surprisingly,
                     appears in the BERTopic output. 
                     Topic A2 illustrates a ‘mantra’ topic; the terms <span class="hi italic">mantra</span> and <span class="hi italic">dhāraṇī</span> echo through several other topics and indeed also appear together in A5. 
                     <span class="hi italic">Maṇḍalas</span> and <span class="hi italic">mantras/dhāraṇīs</span> are the visual and aural elements of esoteric ritual and meditation practice that
                     was introduced to China in the 8th century (and from there to Japan in the early 9th
                     century). 
                     This was the last great transmission of a distinct Buddhist tradition from India to
                     China and topic modeling picks up a clear signal of this process. 
                     This proves at the very least that BERTopic works for our corpus, and can identify
                     “typical” Indian-Chinese topics.
                     </div>
                  
                  <div class="counter"><a href="#p20">20</a></div>
                  <div class="ptext" id="p20">But there are other, more subtle topics, such as <a href="#a3">A3</a>, “Yama Heaven,” where things get more interesting to think with. 
                     Indian cosmography posits a number of heavens above (and hells below) our “middle
                     earth” (madhyadeśa).  
                     
                     <a class="noteRef" href="#d4e621">[12]</a> 
                     
                     In Chinese Buddhism between 500 and 800 CE, the writings of Tanluan, Daochuo and Shandao laid the foundation for the Pure Land school in which practitioners aim to be reborn
                     in the heavenly paradise of Amitābha Buddha. 
                     Their writings in turn are based on Mahāyāna Indian sūtra texts translated before
                     500 CE. The “Yama Heaven” topic signals that in the Indian texts translated 500-800 CE the traditional view
                     of “layered” heavens still persisted, 
                     and was not yet subsumed into the otherworldly Pure Land of the West that became so
                     extraordinarily influential in East Asian Buddhism in the second millennium. 
                     This generates new avenues for research: The texts connected to the topic (e.g. T21n1340,
                     T13n0416, T16n0675, T18n0892, T14n0455, T17n0721) 
                     can now be further explored e.g. to see how the heavenly realms in Mahāyāna Indian-Chinese texts differed from Amitābha’s Pure Land extolled in the Chinese-Chinese works in our period.
                     </div>
                  
                  <div class="counter"><a href="#p21">21</a></div>
                  <div class="ptext" id="p21">Another Indian topic related to place is <a href="#a4">A4</a> “The palace 宮 of Śākyamuni.” Like early Buddhist iconography the topic is in a way aniconic: it describes the
                     city where Prince Siddhartha, 
                     the Sage (muni) of the Śākya clan grew up, but names only his father, Śuddhodana, not Śākyamuni Buddha himself. 
                     That the monochromaticity of the topic converges to 0 suggests the Buddha legend features
                     highly in Indian-Chinese texts. 
                     And indeed, the topic appears not only in a dedicated Buddha legend epic (T0190),
                     but also in (Mūlasārvastivādin) Vinaya texts 
                     (T1442, T1443, T1450), and encyclopedic collections (T2121, T2122). The latter are
                     compiled in China, but contain extensive quotations from translated Indian texts.
                     
                     The topic marks an influx of Indian texts which speak of the historical Buddha in
                     a Chinese Mahāyāna environment, 
                     where many other texts propagate the existence of a multitude of Buddhas “innumerable as grains of sands in the Ganges.” 
                     It is a reminder that the proliferation of Buddhas did not overwrite interest in the
                     story of Śākyamuni, the historical Buddha.
                     </div>
                  
                  <div class="counter"><a href="#p22">22</a></div>
                  <div class="ptext" id="p22">Topic <a href="#a5">A5</a> reflects an ongoing concern with the “propagation of the Dharma.” That this topic lights up in Indian texts translated 500-800 CE is perhaps an indication
                     of the pressures Buddhism faced in India. 
                     While in China the Sui (581–618), Tang (618–907), and Song (960–1279) dynasties were
                     (mostly) a long golden summer for Buddhism, in India autumn had set in by the 7th
                     century. 
                     The invasion and rule of North India by the Alchon Huns in the sixth and early fifth
                     centuries (c. 460-530 CE) was adversarial to Buddhism, 
                     and Xuanzang who traveled to India some hundred years later (629-645 CE) found many
                     pilgrimage sites in decline. 
                     Besides the last blossoming of Buddhist philosophy in the works of Dharamakīrti and Candrakīrti (7th century), 
                     which were not translated into Chinese, the development and transmission of tantric
                     esoteric Buddhism was the last major doctrinal development in Indian Buddhism. 
                     As Indian Buddhism slowly lost ground to powerful Hindu movements (Vaishnavism, Shaivaism,
                     Bhakti etc.) 
                     the need to teach and propagate the Buddhist teachings to laypeople remained a concern
                     in Buddhist literature.
                     </div>
                  
                  <div class="counter"><a href="#p23">23</a></div>
                  <div class="ptext" id="p23">Two highly monochromatic topics are associated with monastic life and its rules, one
                     <a href="#a6">(A6)</a> tending to the Indian-Chinese corpus, 
                     the other <a href="#b1">(B1)</a> to the Chinese-Chinese corpus. Whereas <a href="#a6">A6</a> draws on the more technical discourse of Buddhist canon law, the Vinaya, and its
                     history, 
                     <a href="#b1">B1</a> is about precepts, the rules that Buddhists ought to follow. Returning to the texts
                     we realize that topic <a href="#a6">A6</a> is connected to the translations of Yijing 義淨 (635–713), 
                     who went to India and returned with the Mūlasarvastivāda Vinaya and commentaries.
                     Although in the end the authoritative version of the monastic rules in East Asian
                     Buddhism relied on a different Vinaya tradition, 
                     <a href="#a6">A6</a> can be said to reflect an ongoing exchange between India and China in terms of canon
                     law. 
                     Although associated with translated Indian texts the topic does not so much reflect
                     a new concern within Indian Buddhism, 
                     but rather an ongoing Chinese interest in the Vinaya. Indeed, later historiographers
                     have asserted the development of a “Vinaya School”
                     律宗 in seventh and eighth century China. In contrast, <a href="#b1">B1</a> is marked as a Chinese-Chinese topic by the peculiar term jieti 戒體, the “essence” of the precepts, 
                     which had great traction in the Chinese Vinaya tradition, but for which there is no
                     ready Indian equivalent. Distant reading <a href="#a6">A6</a> against <a href="#b1">B1</a>, 
                     one can discern an important polarity which reflects two competing Vinaya traditions
                     in East Asia. 
                     
                     Next to the mainstream Indian Vinaya, there were the “Bodhisattva ”precepts of an Eastern Mahāyāna Vinaya that formed around the,
                     probably apocryphal [<a class="ref" href="#funayama1996">Funayama 1996</a>], <span class="hi italic">Fanwang jing</span>梵網經. Both traditions were present in China during and beyond our timeframe in China as
                     well as in Japan and Korea, 
                     but are rarely addressed as distinct. The two topics thus do not merely reflect historical
                     reality, but highlight a fundamental distinction in the development of Buddhist norms.
                     
                     In addition, the association of the Fanwang jing vocabulary of <a href="#b1">B1</a> with Chinese-Chinese texts supports the claim that this sūtra was 
                     indeed apocryphal and topically related to Chinese, not Indian concerns.
                     </div>
                  
                  <div class="counter"><a href="#p24">24</a></div>
                  <div class="ptext" id="p24">Regarding topic <a href="#b2">B2</a>, “Early Translators,” one might at first be tempted to think that any “translation” topic (there are other, less monochromatic ones), 
                     might be due to paratext, such as the translator byline that precedes most fascicles
                     of an Indian-Chinese text, but obviously this topic is aligned with the Chinese-Chinese
                     corpus. 
                     Moreover, the topic is specifically about “early” translators, active from the second
                     to the fourth centuries, before our timeframe. 
                     It therefore cannot reflect paratext, but rather a specific Chinese concern with Buddhist
                     historiography, a fundamental difference between Chinese and Indian Buddhism in any
                     period. 
                     Chinese Buddhists made use of Chinese historiographic genres to create catalogs, biographies,
                     annals and more. Compared with India, 
                     the historical record for Chinese Buddhism of the first millennium is extremely rich
                     and detailed. 
                     Although this might be better understood as a general cultural trait, not specific
                     to Buddhism, topic B2 points to the role Buddhist historiography played in the formation
                     of a distinct Chinese Buddhism.
                     </div>
                  
                  <div class="counter"><a href="#p25">25</a></div>
                  <div class="ptext" id="p25">Another “typical” Chinese topic is “Pillars of the state”<a href="#b3">(B3)</a> which clusters the titles of Chinese government officials, none of which would appear
                     in an Indian text, 
                     where the administrative hierarchy was much less sophisticated. Which texts are responsible
                     for the topic? The first two texts associated with it are anthologies of apologetic
                     writing (T2110, T2103) 
                     where Buddhists are in debate with officials, friendly or adversarial. Next is the
                     encyclopedic Fayuan zhulin 法苑珠林 (T2122), which, like T2110, too was compiled by Falin 法琳 (571-639). 
                     Then there are scriptural catalogs (T2156, T2157) and biographies (T2051 (again Falin), T2060). Thus a closer look at the sources of the terms that represent Topic B3
                     encourages us to reevaluate Falin’s role in the sinicization of Buddhism <a href="#j%C3%BClch2014"></a>.
                     </div>
                  
                  <div class="counter"><a href="#p26">26</a></div>
                  <div class="ptext" id="p26">A final monochromatic topic associated with the Chinese-Chinese corpus is <a href="#b4">B4</a> which seems to built around collocations of sheng 乘 ‘vehicle’ (Sk. yāna). 
                     The topic is meaningful in that it reflects a major concern in medieval Chinese Buddhism,
                     namely the categorization of Buddhism in different traditions. 
                     After c. 400 CE Chinese Buddhists increasingly understood themselves as followers
                     of Mahāyāna, a distinctive new development within Indian Buddhism. 
                     While in India Mahāyāna Buddhism developed further into esoteric or tantric Buddhism
                     (Vajrayāna, Tantrayāna), Chinese Buddhism continue to define itself as Mahāyāna, 
                     although, as we saw above, was exposed to esoteric Buddhist doctrines in our time
                     frame. In the sheng 乘 imaginaire earlier Indian mainstream Buddhism was cast as the 
                     ‘smaller (or ‘lesser’) vehicle’ Hīnayāna, in contrast to the ‘larger (or ‘greater’)
                     vehicle’, the Mahāyāna. Topic B4 reflects this coming to terms with different counts
                     of yānas, 
                     such as one (the single transcendent truth applicable to all), two (Hīnayāna and Mahāyāna),
                     or five yānas (the teachings of non-Buddhist humans, deities, śrāvaka Buddhists, pratyekabuddhas,
                     and bodhisattva Buddhists). 
                     Although not used in academic narratives of Buddhist history, these distinctions are
                     still of interest to modern Chinese Buddhists and BERTopic is able to identify this
                     concern in the texts produced between 500 and 800 CE.
                     </div>      
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">5. Conclusion</h1>
                  
                  <div class="counter"><a href="#p27">27</a></div>
                  <div class="ptext" id="p27">Topic modeling is a real upgrade on divination. Traditional divination systems set
                     up a randomized procedure that selects symbolic tokens from a given set. 
                     It is then left to the diviner to interpret the tokens for the question at hand. Whereas
                     with divination systems the number of elements in the set is usually 
                     fixed (78 cards in the Marseille Tarot, 64 Yijing hexagrams, 12 zodiac signs etc.),
                     topic modeling builds its topics as it goes along and can return any number of them.
                     
                     However, like divination, it relies heavily on the interpreter to perceive the internal
                     coherence as well as the overall meaning of a topic. The output of topic modeling
                     is certainly grounded in more sophisticated probabilistic methods than the randomness
                     of lot drawing, but its susceptibility to parametrization and the fact that different
                     approaches yield somewhat different topics, lend the procedure a vagueness and ambivalence
                     not unlike the hermeneutic play of divination. In the BERTopic pipeline a degree of
                     randomness, specifically in the UMAP dimensionality reduction, means that even with
                     the same data and parameters, the top twenty words in a topic may be slightly different.
                     As Benjamin Schmidt (2012) warned at the beginning of the DH topic modeling boom,
                     topics are neither necessarily coherent nor stable. There is no surefire method to
                     separate a “true” topic from a “fata morgana topic”. And whether a topic is meaningful
                     in the context of a research question can (for now) only be decided by humans.
                     This being said, our experiments with different BERTopic workflows, give us confidence
                     that the latent topics are stable at least within this framework. 
                     Moreover, as argued in Section 4, our experiments indeed turned up coherent and meaningful
                     topics. These topics mark differences between the Indian-Chinese 
                     and the Chinese-Chinese corpus, in spite of BERTopic working on a low-resource idiom
                     (Buddhist Chinese). The caveats are: First, there is no guarantee that we 
                     have identified all relevant topics – the rate of recall is unknown and perhaps unknowable.
                     Second, there is no guarantee to precision either, as other 
                     approaches might turn up different, but equally plausible topics.</div>
                  
                  
                  <div class="counter"><a href="#p28">28</a></div>
                  <div class="ptext" id="p28">To counter these limitations to a degree, we are working on a follow-up study in which
                     we use machine translated versions of the texts instead of the Chinese originals.
                     The approach might 
                     confirm or undermine the stability of topics. Furthermore automating the production
                     of “virtual paragraphs” might lead to greater involvement by the wider 
                     community of scholars interested in the corpus. In the end, to us the glass is half
                     full: Our modified BERTopic workflow yields coherent topics that meaningfully distinguish
                     Indian-Chinese and 
                     Chinese-Chinese texts in our time frame. Some of the topics are more surprising than
                     others, but even those that at first glance seem obvious, such as the “translation” topic <a href="#b2">(B2)</a> associated with the Chinese-Chinese corpus, 
                     turned out to be more complex than expected. Since all topics can be traced back to
                     texts, it is possible to follow up and study them further. Thus the distant reading
                     via topic 
                     modeling can nudge our close reading and research in directions it would not have
                     taken otherwise.
                     
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Acknowledgments:</h1>
                  
                  <div class="counter"><a href="#p29">29</a></div>
                  <div class="ptext" id="p29">Research on this paper was funded by the <cite class="title italic">Anatomy of Agency</cite> project (Nichols, PI; 1003) via the “Launching Experimental Philosophy of Religion” grant 
                     (Ian Church, PI; 61886), itself supported by the John Templeton Foundation. We give
                     special thanks to 
                     Robert Buswell, Nicholaos Jones, Song Wang, and Michael Radich for comments and correspondence leading to the improvement of this project.
                     </div>
                  </div>
               
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix A: Topics as virtual paragraphs</h1>
                  
                  <div class="counter"><a href="#p30">30</a></div>
                  <div class="ptext" id="p30">In order to communicate our findings to those who have neglected their study of medieval
                     Chinese Buddhist texts in recent years, we have in the following created “virtual
                     paragraphs” for ten of the more monochromatic and coherent topics that were stable
                     across different iterations of our BERTopic pipeline. 
                     The current workflow outputs more than 750 topics, so this is only a small sample.
                     All the Chinese terms are as they appear in the topic and follow their English translation.
                     Their syntactic relationship is invented freely, but informed by our domain knowledge,
                     i.e. in the context of medieval Buddhism. 
                     The idea is to use such artificial paragraphs to demonstrate to a non-specialist readership
                     that these topics indeed constitute intelligible semantic fields. The numbering does
                     not imply any ranked order.
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">A. Indian-Chinese Corpus</h2>
                     
                     <div class="ptext">
                        <dl>
                           <dd id="a1" class="item"><span class="label bold">A1 Maṇḍala曼拏羅</span> Maṇḍalas 曼拏羅 depict a variety of Buddhist deities from various tantric families 種族 
                              such as Vajrapāṇi 金剛手 or Vajrasattva 金剛薩埵, and Amoghapāśa 不空羂索 or Amogharāja 不空王/㫊暮伽王 Bodhisattva. 
                              By drawing maṇḍalas, together with the secret 祕密 transmission of the esoteric teachings 密迹/密跡主 and the help of dhāraṇī 陀羅尼 spells and mantras 真言 
                              practioners can achieve adamantine 金剛 samādhi 三昧(耶). Maṇḍalas are often used in rituals on or around altars 壇.
                              </dd>
                           <dd id="a2" class="item"><span class="label bold">A2 Chanting 誦 Mantras 真言</span> Three times circulambulating 三匝 altars, stupas, or images one chants (e.g.) the Mantra 真言 of the King of Fury 奮怒王. 
                              One recites rapidly 緊捷誦, sometimes using the 108 一百八 beads of the rosary 念珠. 
                              One recites and memorizes 誦持 a thousand times 一千 遍, imprinting the mind-seal 心印, until one can recite the secret 誦密 spells in one’s mind 誦心 / 祕密心, thereby creating and gaining blessings 加持.
                              </dd>
                           <dd id="a3" class="item"><span class="label bold">A3 In Yama Heaven 夜摩天</span> The King of Yama Heaven 夜摩天 is *Musulundha 牟修樓陀.
                              
                              <a class="noteRef" href="#d4e963">[13]</a> 
                              
                              There the goddesses 天女 play 遊戲 in the parks 園林, singing and dancing 歌舞, 
                              between ponds 池 and groves 林. The mountains 山峯 are full of many-colored 雜色 birds 鳥. 
                              The gods in Yama Heaven sport 娛樂 in an lovely 可愛 environment filled with the seven precious materials 七寶 such as silver 白銀, beryl 毘琉璃, crystal 頗梨. There also are bees 蜂.
                              </dd>
                           <dd id="a4" class="item"><span class="label bold">A4 The palace 宮 of Śākyamuni</span> A palace 宮 in a walled city 城 with high ministers 大臣. Where the great king 大王 Śuddhodana 淨飯王 lives with his wifes 妻/婦 and crown prince 太子. 
                              In the city lived merchants 商人/商主, householder women 女, and elders 長者. That time 是時 a seer 仙人 came to the palace to tell 告 the king father 父王 Śuddhodana about the fate of the prince 王子.
                              </dd>
                           <dd id="a5" class="item"><span class="label bold">A5 Propagating 演說 the Dharma</span> Teachings are explained 演說 by a good friend 善知識, a guiding teacher 導師, endowed with rhetorical skill 辯才. It is done at Buddhist temples 佛剎 to an audience of sentient beings 眾生, 
                              including good women 善女人, who have approached 親近 the teacher. It is done in detail 廣大, reaching everywhere 十方/普入, for all possessed with wisdom 智慧, 
                              in order to tame the minds 調伏 of sentient beings, benefit 利益 them, and make them to rid 捨離 themselves of suffering. The teaching of a omniscient 一切智 Buddha or Bodhisattva demonstrates 示現 even that which cannot be explained 不可說.
                              </dd>
                           <dd id="a6" class="item"><span class="label bold">A6 Sangha 僧伽  life</span> The members of the Sangha 僧伽, monks 苾芻, nuns 苾芻尼, and others seeking nirvāṇa 求寂 begin their life of training 學處 under a preceptor 鄔波馱耶. Different from lay people 俗人, 
                              they have left their secular family 俗家, and are allowed only few possessions such as their alms bowl 鉢 and some bedding 臥具. If they commit a pārājika offense 波羅市迦, however, they are expelled.
                              Some famous monks had known the Buddha even before he went forth, such as Ānanda 阿難陀 and Chandaka 闡陀. 
                              There also were trouble makers such as the monk Upananda 鄔波難陀 or the nun Sthūlanandā 吐羅難陀, who were without shame 嫌恥 or remorse 惡作.
                              </dd>
                        </dl>
                     </div>
                     </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">B. Chinese-Chinese corpus</h2>
                     
                     <div class="ptext">
                        <dl>
                           <dd id="b1" class="item"><span class="label bold">B1 Keeping the precepts 戒/禁戒/戒法</span> All Buddhists take refuge in the three 三歸 jewels. Lay-people take five, eight or ten precepts 五戒, 八戒, 十戒. To become a monastic one must take the precepts 受戒, the complete set of precepts 具戒, 
                              as found in the prātimokṣa 戒本, 
                              the list of precepts, as they have been explained 說戒  by the Buddha. The ordination takes place in a marked ordination site 戒場. 
                              Once one has taken the pure precepts 淨戒 one must keep the precepts 持戒 is bound by the precepts 結戒. One may not break the precepts 犯戒/破戒. Otherwise one cannot experience their essence 戒體 that develops in one as one practices. 
                              One should never give them up 捨戒 again.
                              </dd>
                           <dd id="b2" class="item"><span class="label bold">B2 Translators and translation 譯</span> At the time when the Indo-Scythians 月支 ruled Northern India, the early translations 譯 of Buddhist texts began in China in the final decades of the Latter Han 後漢 Dynasty with An Shigao 安世高/安公. It continued later in the Kingdom of Wu 吳 
                              (222-80) with Zhi Qian 支謙 (fl.222–252 CE) 
                              and under the Western and Eastern Jin 西晉/東晉 (266–316 / 317–420) with Dharmarakṣa 竺法護 (c. 233-310). They wrote their translations on sheets of paper 紙, wrapped in bundles 帙, or glued together into scrolls 卷. 
                              Later catalogs 錄 like that of Sengyou 僧祐 (445–518) recorded not only first translations but also alternative translations
                              異譯 of the the same text and translation of which the translator is unknown 失譯.
                              </dd>
                           <dd id="b3" class="item"><span class="label bold">B3 Officials as “pillars of the state”</span> Officials are the “pillars of the state” 柱國: Next to the civil administration with its Grand Counselors 侍中, the Director of the Department of State Affairs 尚書令, the Vice Directors 僕射 and ministers 尚書 
                              and members of the Secretariat 中書, the Minister of Education 司徒, the vice ministers 侍郎侍郎, the provincial governors 太守, the secretaries 內史 and aides 長史, and the inspectors 刺史 of the censorate, there are the military 軍事 officials such as the military provincial governors 都督, 
                              the field marshals 大將軍 and generals 將軍, 
                              the 總管 regional commanders and commanders 司馬 and adjutants 參軍.
                              </dd>
                           <dd id="b4" class="item"><span class="label bold">B4 How many vehicles/yānas 乘?</span> There is the Hīnayāna 小乘 and the Mahāyāna 大乘 both 大小乘. There are Hīnayāna scriptures 小乘經 and Mahāyāna scriptures 大乘經. 
                              But is that all? Aren’t there perhaps five vehicles 五乘 or three vehicles 三乘, instead of these two vehicles 二乘? 
                              Or is there perhaps only the teaching of one single vehicle 一乘教/一乘法, a single Buddha vehicle 一佛乘.
                              </dd>
                        </dl>
                     </div>
                     </div>
                  </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Appendix B: Workflow - Technical Details</h1>
                  
                  <div class="counter"><a href="#p31">31</a></div>
                  <div class="ptext" id="p31">Below the specific parameters that we used in our instantiation of the BERTopic pipeline.
                     All computational work was performed on a Gentoo Linux workstation with a Xeon W-2295
                     CPU, 
                     an Nvidia A6000 GPU and 512GB of memory.
                     </div>
                  
                  <div class="counter"><a href="#p32">32</a></div>
                  <div class="ptext" id="p32">Our corpus was already pre-segmented, we therefore did not use automatic segmentation
                     tools like jieba.  As explained above, the segmentation was performed by researchers
                     at the Dharma Drum Institute of Liberal Arts as part of a project on using conditional
                     random fields for automatic segmentation (for details see Wang 2020). 
                     The sentence embeddings are then produced using Yasuoka’s version of GuwenBERT.  This
                     itself was wrapped in a SentenceTransformers instance to extract the embedding of
                     the [CLS] token to get an embedding that represents the sentence as a whole [<a class="ref" href="#reimers_etal2019">Reimers et al 2019</a>]. This produces 2037769 embedded sentences, each of which in 1024-dimensional space.
                     </div>
                  
                  <div class="counter"><a href="#p33">33</a></div>
                  <div class="ptext" id="p33">From here, we apply UMAP to create 3-dimensional embeddings which respect relative
                     closeness of embedded sentences by projecting the 1024-dimensional points onto a 3-dimensional
                     manifold.  For this, we use the CUDA-enabled version of UMAP provided by the NVIDIA
                     cuML library [<a class="ref" href="#raschka_etal2020">Raschka et al 2020</a>].  
                     We used the default parameters for the reduction, only specifying that the output
                     should be 3-dimensional.
                     </div>
                  
                  <div class="counter"><a href="#p34">34</a></div>
                  <div class="ptext" id="p34">For the clustering, we used hdscan, again using the CUDA-enabled version from cuML.
                     We set three parameters for the clustering algorithm, using 750 for the minimal number
                     of clusters 100 for the minimal number of samples.  The first parameter indicates
                     that we want the algorithm to produce at least 750 clusters, 
                     while the second indicates that very small clusters (greater than 100 sentences) should
                     not be counted.
                     </div>
                  </div>
               
               
               
               
               
               </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e249"><span class="noteRef lang en">[1] The fact that the method has survived the now defunct journal, is an indication of
                     the fluid, evolving nature of DH as a field.</span></div>
               <div class="endnote" id="d4e287"><span class="noteRef lang en">[2] Data for this network is available as the <cite class="title italic">Historical Social Network of Chinese Buddhism</cite> <a href="https://github.com/mbingenheimer/ChineseBuddhism_SNA" onclick="window.open('https://github.com/mbingenheimer/ChineseBuddhism_SNA'); return false" class="ref">https://github.com/mbingenheimer/ChineseBuddhism_SNA</a>.</span></div>
               <div class="endnote" id="d4e344"><span class="noteRef lang en">[3] Our corpora are available here: <a href="https://github.com/mbingenheimer/cbetaCorpusSorted" onclick="window.open('https://github.com/mbingenheimer/cbetaCorpusSorted'); return false" class="ref">https://github.com/mbingenheimer/cbetaCorpusSorted</a>.</span></div>
               <div class="endnote" id="d4e362"><span class="noteRef lang en">[4] A large part of the CBETA corpus of Buddhist texts is available in this tokenized
                     form here: <a href="https://github.com/DILA-edu/word-segment" onclick="window.open('https://github.com/DILA-edu/word-segment'); return false" class="ref">https://github.com/DILA-edu/word-segment</a> (accessed 2024-09).</span></div>
               <div class="endnote" id="d4e379"><span class="noteRef lang en">[5] The high-brow, civil examination style of classical Chinese that was modeled on the
                     thirteen Confucian classics and other works of the Zhou, Qin, and Han dynasties. 
                     While this style of writing was used until the early 20th century in certain genres,
                     other, more vernacular, styles existed that were closer to spoken Chinese where many
                     words are multimorphemic.</span></div>
               <div class="endnote" id="d4e397"><span class="noteRef lang en">[6] As of 2023-11 a web of science search returned 49 articles and 13 proceeding papers
                     with “BERTopic” in the title or abstract.</span></div>
               <div class="endnote" id="d4e402"><span class="noteRef lang en">[7] The pre-segmented version was originally produced by Yu-chun Wang and forked from
                     the larger repository (which includes more texts from the Taishō canon) at https://github.com/DILA-edu/word-segment.
                     
                     The version we used for the experiment is available (together with our code) at <a href="https://github.com/mbingenheimer/cbetaCorpusSorted/tree/main/bertopic/word-segment-main" onclick="window.open('https://github.com/mbingenheimer/cbetaCorpusSorted/tree/main/bertopic/word-segment-main'); return false" class="ref">https://github.com/mbingenheimer/cbetaCorpusSorted/tree/main/bertopic/word-segment-main</a>.</span></div>
               <div class="endnote" id="d4e443"><span class="noteRef lang en">[8] At: <a href="https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-large-char" onclick="window.open('https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-large-char'); return false" class="ref">https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-large-char</a> (accessed 2023-12-10). GuwenBERT at: <a href="https://huggingface.co/ethanyt/guwenbert-base" onclick="window.open('https://huggingface.co/ethanyt/guwenbert-base'); return false" class="ref">https://huggingface.co/ethanyt/guwenbert-base</a> (accessed 2023-12-10).</span></div>
               <div class="endnote" id="d4e460"><span class="noteRef lang en">[9] <a href="https://huggingface.co/ethanyt/guwenbert-base" onclick="window.open('https://huggingface.co/ethanyt/guwenbert-base'); return false" class="ref">https://huggingface.co/ethanyt/guwenbert-base</a> (accessed 2023-12-10). Slightly different figures are given at: <a href="https://www.kaggle.com/datasets/wptouxx/daizhige" onclick="window.open('https://www.kaggle.com/datasets/wptouxx/daizhige'); return false" class="ref">https://www.kaggle.com/datasets/wptouxx/daizhige</a> (accessed 2023-12-10). The Daizhige dataset is floating around on the web, slightly
                     unmoored. One accessible version 
                     is on Github: <a href="https://github.com/garychowcmu/daizhigev20" onclick="window.open('https://github.com/garychowcmu/daizhigev20'); return false" class="ref">https://github.com/garychowcmu/daizhigev20</a> (accessed 2023-12-10). As one of the largest datasets of classical Chinese it deserves
                     to be better curated.</span></div>
               <div class="endnote" id="d4e474"><span class="noteRef lang en">[10] We did experiment with fine-tuning the model based on a Korean model which had been
                     fine-tuned for semantic similarity, but the results were not an improvement. This
                     will be a topic of future research.</span></div>
               <div class="endnote" id="d4e536"><span class="noteRef lang en">[11] This is how Wittgenstein describes the moment when we recognize a hitherto unseen pattern (Wittgenstein Philosophical
                     Investigations ¶118, ¶140).</span></div>
               <div class="endnote" id="d4e621"><span class="noteRef lang en">[12] See [<a class="ref" href="#kirfel1920">Kirfel 1920</a>] for an overview. For a distant corpus based reading of the role of gods and deities
                     in the classical Chinese tradition see <a href="#nichols_etal2020">Nichols et. al (2020)</a>.</span></div>
               <div class="endnote" id="d4e963"><span class="noteRef lang en">[13] Musulundha as Sanskrit for 牟修樓陀 is suggested by [<a class="ref" href="#lin1949">Lin 1949</a>, 61].</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="bingenheimer2021"><!-- close -->Bigenheimer 2021</span> Bingenheimer, M. (2021) “The historical social network of Chinese Buddhism”, <cite class="title italic">Journal of Historical Network Review</cite>, 5: 233-257. Available at: <a href="https://doi.org/10.25517/jhnr.v5i1.119" onclick="window.open('https://doi.org/10.25517/jhnr.v5i1.119'); return false" class="ref">https://doi.org/10.25517/jhnr.v5i1.119</a>
                  </div>
               <div class="bibl"><span class="ref" id="blei2012"><!-- close -->Blei 2012</span> Blei, D. M. (2021) “Topic modeling and digital humanities”, <cite class="title italic">Journal of Digital Humanities </cite>, 2.1. Available at: <a href="https://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/" onclick="window.open('https://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/'); return false" class="ref">https://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/</a>.
                  (Accessed November 2023.)</div>
               <div class="bibl"><span class="ref" id="blei_etal2003"><!-- close -->Blei et al2003</span> Blei, D. M, Ng, A. and Jordan, M. (2003) “Latent dirichlet allocation”, <cite class="title italic">Journal of Machine Learning Research </cite>, 3: 993-1022. 
                  </div>
               <div class="bibl"><span class="ref" id="cbetaver2021"><!-- close -->CBETA ver 2021</span>  CBETA v. 2021: Chinese Buddhist Electronic Text Association (2021) “There are different, evolving versions of this digital corpus. We were using the texts
                  as made available here”: Available at <a href="https://github.com/DILA-edu/CBETA_TAFxml" onclick="window.open('https://github.com/DILA-edu/CBETA_TAFxml'); return false" class="ref">https://github.com/DILA-edu/CBETA_TAFxml</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="chen_etal2023"><!-- close -->Chen et al 2023</span> Chen, Y., Zhao Peng, S.H.K., and Chang, W. C. (2023) “What we can do and cannot do with topic modeling: A systematic eeview”, <cite class="title italic">Communication Methods and Measures</cite>, 17-2: 111-130.
                  </div>
               <div class="bibl"><span class="ref" id="du2019"><!-- close -->Du 2019</span> Du, k. (2019) “A survey on lda topic modeling in digital humanities”in: Abstract for <cite class="title italic">Digital Humanities </cite> (Utrecht). Available at: <a href="https://doi.org/10.34894/H9UYPI" onclick="window.open('https://doi.org/10.34894/H9UYPI'); return false" class="ref">https://doi.org/10.34894/H9UYPI</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="eggar_yu2022"><!-- close -->Egger and Yu 2022</span> Egger. R. and Yu, J. (2022) “A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify Twitter
                  posts”, <cite class="title italic">Frontiers in Socieology</cite>. Available at: <a href="https://www.frontiersin.org/journals/sociology/articles/10.3389/fsoc.2022.886498/full" onclick="window.open('https://www.frontiersin.org/journals/sociology/articles/10.3389/fsoc.2022.886498/full'); return false" class="ref">https://www.frontiersin.org/journals/sociology/articles/10.3389/fsoc.2022.886498/full</a>. 
                  
                  </div>
               <div class="bibl"><span class="ref" id="fang2023"><!-- close -->Fang 2023</span> Fang, Y. (2023) “Theme classification of the complete Song Ci from the perspective of the digital humanities”, <cite class="title italic">Lecture Notes on Language and Literature</cite>, 13-24. Available at: <a href="https://clausiuspress.com/assets/default/article/2023/08/17/article_1692280542.pdf" onclick="window.open('https://clausiuspress.com/assets/default/article/2023/08/17/article_1692280542.pdf'); return false" class="ref">https://clausiuspress.com/assets/default/article/2023/08/17/article_1692280542.pdf</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="fu_etal2020"><!-- close -->Fu et al 2020</span> Fu, Q., Yufan, Z., Jiaxin, G., Yushu, Z., and Xin, G. (2020) “Agreeing to disagree: Choosing among eight topic-modeling methods”, <cite class="title italic">Big Data Research</cite>. Available at: <a href="https://doi.org/10.1016/j.bdr.2020.100173" onclick="window.open('https://doi.org/10.1016/j.bdr.2020.100173'); return false" class="ref">https://doi.org/10.1016/j.bdr.2020.100173</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="funayama1996"><!-- close -->Funayama 1996</span> Funayama, T. 船山徹 (1996) “Gikyō Bonmōkyō seiritsu no shomondai 疑経『梵網経』成立の諸問題”, <cite class="title italic">Bukkyō shigaku kenkyū 佛教史學研究</cite>, 39-1: 54-78. 
                  </div>
               <div class="bibl"><span class="ref" id="grootendorst2022"><!-- close -->Grootendorst 2022</span> Grootendorst, M. (2022) “BERTopic: Neural topic modeling with a class-based TF-IDF procedure”. Available at: <a href="https://arxiv.org/abs/2203.05794" onclick="window.open('https://arxiv.org/abs/2203.05794'); return false" class="ref">https://arxiv.org/abs/2203.05794</a>. (Accessed 11 March 2022). 
                  </div>
               <div class="bibl"><span class="ref" id="jülch2014"><!-- close -->Jülch 2014</span> Jülch, T. (2014) <cite class="title italic">Bodhisattva der apologetik - die mission des buddhistischen Tang-Mönches Falin</cite>. München: Herbert Utz Verlag.
                  </div>
               <div class="bibl"><span class="ref" id="kherwa_bansal2019"><!-- close -->Kherwa and Bansal 2019</span> Kherwa, P. and Bansal, P. (2019) “Topic modeling: A comprehensive review”, <cite class="title italic">ICST Transactions on Scalable Information Systems</cite>, 7. Available at: <a href="https://eudl.eu/doi/10.4108/eai.13-7-2018.159623" onclick="window.open('https://eudl.eu/doi/10.4108/eai.13-7-2018.159623'); return false" class="ref">https://eudl.eu/doi/10.4108/eai.13-7-2018.159623</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="kirfel1920"><!-- close -->Kirfel 1920</span> Kirfek, W. (1920) <cite class="title italic">Die Kosmographie der Inder nach den Quellen dargestellt</cite>. Bonn und Leipzig: K Schroader.   
                  </div>
               <div class="bibl"><span class="ref" id="lin1949"><!-- close -->Lin 1949</span> Lin, Li-kouang 林藜光 (1949). <cite class="title italic">L’aide-mémoire de la Vraie Loi (Saddharma-smrtyupasthâna-sùtra). Recherches sur un
                     Sûtra développé du Petit Véhicule</cite>. Paris: Adrien-Maisonneuve. 
                  </div>
               <div class="bibl"><span class="ref" id="ma_etal2021"><!-- close -->Ma et al 2021</span> Ma, P., Qing Zeng-Treitler, Nelson, Stuart J. (2021) “Use of two topic modeling methods to investigate covid vaccine hesitancy”, <cite class="title italic">Proceedings 14th International Conference on ICT, Society and Human Beings (ICT 2021),
                     the 18th International Conference Web Based Communities and Social Media (WBC 2021)</cite>: 221-226. 
                  </div>
               <div class="bibl"><span class="ref" id="mcinnes_etal2017"><!-- close -->McInnes et al 2017</span> McInnes, L., Healy, J., and Astels S. (2017) “HDBSCAN: Hierarchical density based clustering”, <cite class="title italic">Journal of Open Source Software </cite>, vol. 2.11: 205. 
                  </div>
               <div class="bibl"><span class="ref" id="mcinnes_etal2018"><!-- close -->McInnes et al 2018</span> McInnes, L., Healy, J., and Melville, J. (2018) <cite class="title italic">Umap: Uniform manifold approximation and projection for dimension reduction</cite>. Available at: <a href="https://arxiv.org/abs/1802.03426" onclick="window.open('https://arxiv.org/abs/1802.03426'); return false" class="ref">https://arxiv.org/abs/1802.03426</a>.  
                  </div>
               <div class="bibl"><span class="ref" id="meeks_weingart2012"><!-- close -->Meeks and Weingart 2012</span> Meeks, E. and Weingart, S.B. (2012) “The digital humanities contribution to topic modeling”, <cite class="title italic">” Journal of Digital Humanities </cite>, 2.1. Available at: <a href="http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling" onclick="window.open('http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling'); return false" class="ref">http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling</a>. (Accessed November 2023).   
                  </div>
               <div class="bibl"><span class="ref" id="nichols_etal2020"><!-- close -->Nichols et al 2020</span> Nichols, R., Slingerland, E., Nielbo, K. L., Kirby, P., and Logan, C. (2020) “Supernatural agents and prosociality in historical China: Micro-modeling the cultural
                  evolution of gods and morality in textual corpora”, <cite class="title italic">Religion, Brain &amp; Behavior</cite>, 1-19. Available at: <a href="https://doi.org/10.1080/2153599X.2020.1742778" onclick="window.open('https://doi.org/10.1080/2153599X.2020.1742778'); return false" class="ref">https://doi.org/10.1080/2153599X.2020.1742778</a>.       
                  </div>
               <div class="bibl"><span class="ref" id="pewresearchcenter2023"><!-- close -->Pew Research Center 2023</span> Pew Research Center (2023) “Measuring religion in China”. Available at: <a href="https://www.pewresearch.org/religion/2023/08/30/measuring-religion-in-china/" onclick="window.open('https://www.pewresearch.org/religion/2023/08/30/measuring-religion-in-china/'); return false" class="ref">https://www.pewresearch.org/religion/2023/08/30/measuring-religion-in-china/</a>. (Accessed November 2023). 
                  </div>
               <div class="bibl"><span class="ref" id="ramsay2011"><!-- close -->Ramsay 2011</span> Ramsay, S. (2011) <cite class="title italic">Reading machines: Toward an algorithmic criticism</cite>. Urbana: University of Illinois Press.                    
                  </div>
               <div class="bibl"><span class="ref" id="raschka_etal2020"><!-- close -->Raschka et al 2020</span> Raschka, M., Antons, D., Joshi, A.M., and Salge, T. (2020) “Machine learning in Python: Main developments and technology trends in data science,
                  machine learning, and artificial intelligence”. Available at: <a href="https://arxiv.org/abs/2002.04803" onclick="window.open('https://arxiv.org/abs/2002.04803'); return false" class="ref">https://arxiv.org/abs/2002.04803</a>.  
                  </div>
               <div class="bibl"><span class="ref" id="reimers_etal2019"><!-- close -->Reimers et al 2019</span> Reimers, N. and Girevich, I. (2019) “Sentence-BERT: Sentence embeddings using Siamese BERT-Networks”, <cite class="title italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                     (EMNLP 2019)</cite>. 
                  
                  </div>
               <div class="bibl"><span class="ref" id="rüdiger_etal2022"><!-- close -->Rüdiger et al 2022</span> Rüdiger, M., Antons, D., Joshi, A.M., and Salge, T.O. (2022) “Topic modeling revisited: New evidence on algorithm performance and quality metrics”, <cite class="title italic">PLoS ONE </cite>, 17(4): e0266325. Available at: <a href="https://doi.org/10.1371/journal.pone.0266325" onclick="window.open('https://doi.org/10.1371/journal.pone.0266325'); return false" class="ref">https://doi.org/10.1371/journal.pone.0266325</a>.    
                  </div>
               <div class="bibl"><span class="ref" id="schmidt2012"><!-- close -->Schmidt 2012</span> Schmidt, B. (2012) “Words alone: Dismantling topic models in the humanities”, <cite class="title italic">Journal of Digital Humanities </cite>, 2.1. Available at: <a href="http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt" onclick="window.open('http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt'); return false" class="ref">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt</a>. 
                  </div>
               <div class="bibl"><span class="ref" id="underwood2017"><!-- close -->Underwood 2017</span> Underwood, T. (2017) “A genealogy of distant reading”, <cite class="title italic">DHQ: Digital Humanities Quarterly</cite>, vol. 11-2. 
                  </div>
               <div class="bibl"><span class="ref" id="vayansky_kumar2020"><!-- close -->Vayansky and Kumar 2020</span> Vayansky, I. and Kumar, S.A.P. (2020) “A review of topic modeling methods”, <cite class="title italic">Information Systems</cite>, 94.  
                  </div>
               <div class="bibl"><span class="ref" id="wang2020"><!-- close -->Wang 2020</span> Wang, Y.D. (2020) “Word segmentation for classical chinese Buddhist literature”, <cite class="title italic">Journal of the Japanese Association for Digital Humanities</cite>, 5-2: 154-172. Available at: <a href="https://doi.org/10.17928/jjadh.5.2_154" onclick="window.open('https://doi.org/10.17928/jjadh.5.2_154'); return false" class="ref">https://doi.org/10.17928/jjadh.5.2_154</a>. 
                  </div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
            <div class="license"><a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</a>.
               </div>
         </div>
      </div>
   </body>
</html>