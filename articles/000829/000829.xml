<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
   xmlns:dhq="http://www.digitalhumanities.org/ns/dhq"
   xmlns:mml="http://www.w3.org/1998/Math/MathML"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <!--Author should supply the title and personal information-->
            <title type="article" xml:lang="en">Defactoring Pace of Change</title>
            <!--Add a <title> with appropriate @xml:lang for articles in languages other than English-->
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Matt <dhq:family>Burtpm</dhq:family>
               </dhq:author_name>
               <dhq:affiliation> University of Pittsburgh </dhq:affiliation>
               <email>mcburton@pitt.edu</email>
               <dhq:bio>
                  <p>PLACEHOLDER</p>
               </dhq:bio>
            </dhq:authorInfo>
            <dhq:authorInfo>
               <!--Include a separate <dhq:authorInfo> element for each author-->
               <dhq:author_name>Joris <dhq:family>Van Zundert</dhq:family>
               </dhq:author_name>
               <idno type="ORCID">https://orcid.org/0000-0003-3862-7602</idno>
               <dhq:affiliation>Huygens Institute of the Royal Netherlands Academy of Arts and
                  Sciences </dhq:affiliation>
               <email>joris.van.zundert@huygens.knaw.nl</email>
               <dhq:bio>
                  <p>PLACEHOLDER</p>
               </dhq:bio>
            </dhq:authorInfo>
         </titleStmt>
         <publicationStmt>
            <publisher>Alliance of Digital Humanities Organizations</publisher>
            <publisher>Association for Computers and the Humanities</publisher>
            <!--This information will be completed at publication-->
            <idno type="DHQarticle-id">000829</idno>
            <idno type="volume">020</idno>
            <idno type="issue">1</idno>
            <date when="2026-01-01">1 January 20206</date>
            <dhq:articleType>article</dhq:articleType>
            <availability status="CC-BY-ND">
               <!--If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default):        
                  CC-BY:    
                  CC0:  -->
               <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>This is the source</p>
         </sourceDesc>
      </fileDesc>
      <encodingDesc>
         <classDecl>
            <taxonomy xml:id="dhq_keywords">
               <bibl>DHQ classification scheme; full list available at <ref
                     target="https://dhq.digitalhumanities.org/taxonomy.xml"
                     >https://dhq.digitalhumanities.org/taxonomy.xml</ref>
               </bibl>
            </taxonomy>
            <taxonomy xml:id="authorial_keywords">
               <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
            </taxonomy>
            <taxonomy xml:id="project_keywords">
               <bibl>DHQ project registry; full list available at <ref
                     target="https://dhq.digitalhumanities.org/projects.xml"
                     >https://dhq.digitalhumanities.org/projects.xml</ref>
               </bibl>
            </taxonomy>
         </classDecl>
      </encodingDesc>
      <profileDesc>
         <langUsage>
            <language ident="en" extent="original"/>
            <!--add <language> with appropriate @ident for any additional languages-->
         </langUsage>
         <textClass>
            <keywords scheme="#dhq_keywords">
               <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at https://dhq.digitalhumanities.org/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
               <!--Enter keywords below preceeded by a "#". Create a new term element for each-->
               <term corresp="#code_studies"/>
            </keywords>
            <keywords scheme="#authorial_keywords">
               <!--Authors may include one or more keywords of their choice-->
               <list type="simple">
                  <item>defactoring</item>
                  <item>bespoke code</item>
                  <item>close reading</item>
               </list>
            </keywords>
            <keywords scheme="#project_keywords">
               <list type="simple">
                  <item/>
               </list>
            </keywords>
         </textClass>
      </profileDesc>
      <revisionDesc>
         <!-- Replace "NNNNNN" in the @target of ref below with the appropriate DHQarticle-id value. -->
         <change>The version history for this file can be found on <ref
               target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/main/articles/000829/000829.xml"
               >GitHub </ref>
         </change>
      </revisionDesc>
   </teiHeader>
   <text xml:lang="en" type="original">
      <front>
         <dhq:abstract>
            <!--Include a brief abstract of the article-->
            <p>Code, the symbolic representation of computer instructions driving software, has long
               been a part of research methods in literary scholarship. However, the bespoke code of
               data and computationally inflected Digital Humanities research is not always a part
               of the final publication. We emphasize the need to elevate code from its generally
               invisible status in scholarly publications and make it a visible research output. We
               highlight the lack of conventions and practices for theorizing, critiquing, and peer
               reviewing bespoke code in the humanities, as well as the insufficient support the
               dissemination and preservation of code in scholarly publishing. We introduce <quote
                  rend="inline">defactoring</quote> as a method for analyzing and reading code used
               in humanities research and present a case study of applying this technique to a
               publication from literary studies. We explore the implications of code as methodology
               made material, advocating for a more integrated and computationally informed mode of
               interacting with scholarship. We conclude by posing questions about the potential
               benefits and challenges of linking code and theoretical exposition to foster a more
               robust scholarly dialogue.</p>
         </dhq:abstract>
         <dhq:teaser>
            <!--Include a brief teaser, no more than a phrase or a single sentence-->
            <p>PLACEHOLDER</p>
         </dhq:teaser>
      </front>
      <body>
         <div>
            <head>Introduction</head>
            <p>Code, the symbolic representation of computer instructions driving software, has long
               been a part of research methods in literary scholarship. While it is a tired cliché
               to point to the work of Father Busa and his compatriots at IBM as foundational work
               in this respect, it was indeed a very early and important application of computation
               as a means of analyzing literature <ptr target="#jones_2016"/>
               <ptr target="#nyhan_2016"/>. In more recent examples we find researchers using
               computational code to calculate geo-references for a large corpus of folklore stories
                  <ptr target="#broadwell_2012"/>, or to seek linguistic identifiers that signal
                  <quote rend="inline">conversional reading</quote>
               <ptr target="#piper_2015"/>. However, we ask: Where is the code associated with these
               works? Currently retrieving the purpose built, one-off, bespoke codebases that enable
               such feats of computational literary analysis is in most scholarly domains more a
               stroke of luck than the guaranteed result of responsible scholarly behavior,
               scientific accountability, critical review, or academic credit. Sometimes we have
               codebases. Such as in the case of Scott Enderle (2016) who crucially contributed to a
               methodological discussion about perceived <quote rend="inline">fundamental narrative
                  arcs</quote> from sentiment data in works of fiction <ptr target="#enderle_2016"
               />. Or in the case of Kestemont et al.’s <quote rend="inline">Lemmatization for
                  variation-rich languages using deep learning</quote>
               <ptr target="#kestemont_2017"/>. However, much of the code used in the long history
               of humanities computing and recent digital humanities has not been adequately
               reviewed nor recognized for its importance in the production of knowledge.</p>
            <p>In this article, we argue that bespoke code cannot be simply regarded as the
               interim-stage detritus of research; it is an explicit codification of methodology and
               therefore must be treated as a fundamental part of scholarly output alongside figures
               and narrative prose. The increased application of code both as a means to create
               digital cultural artifacts and as an analytical instrument in humanities research
               warrants the necessity to elevate code out of its invisibility in the research
               process and into visible research outputs. The current system and practices of
               scholarly publishing do not adequately accommodate the potential of code in
               computational work — although it is a lynchpin of the discourse as both the
               explication and execution of method, the code itself is not presented as part of the
               discourse. We posit that its methodological and rhetorical role in the evolving
               epistemology of literary studies (and the humanities in general) warrants a more
               overt inclusion of <term>code-as-method</term> in the scholarly discourse.</p>
            <p>The current systems of scholarly communication are not tailored very well to such
               inclusion. In the humanities, there are no set conventions for theorizing,
               critiquing, interrogating, or peer reviewing bespoke code. Nor are there agreed upon
               methods on how to read and understand code as a techno-scholarly object. Finally,
               widespread conventions and infrastructure of scholarly publishing do not support the
               dissemination and preservation of code as part of the scholarly record. But given the
               increasing role of code and coding in computational and data-intensive humanities
               research, this absence is increasingly becoming a deficit in the scholarly process
               requiring urgent attention and action.</p>
            <p>This article is a first step towards investigating solutions to the problems laid out
               above. More specifically we develop one possible method for reading and interrogating
               the <term>bespoke code </term>of computational research. Modern computing is possible
               because of an enormous infrastructure of code, layers upon layers of accumulated
               operating systems, shared libraries, and software packages. Rather than following
               every codified thread and unravelling the many layered sweater of software, we focus
               on the <term>bespoke code</term> associated with a single scholarly publication. That
               is, the code written and executed in the pursuit of a specific and unique research
               output, not general-purpose libraries or software packages; the code that runs only
               once or maybe a few times in the context of a specific research question. </p>
            <p>The method we present for the critical study of bespoke code used in humanities
               research is called <term>defactoring</term>. Defactoring is a technique for analyzing
               and reading code used in computational and data-intensive research. </p>
            <p>In a supplement to this article on GitHub, we apply this technique to the code
               underlying a publication in the field of literary studies by Ted Underwood and Jordan
               Sellers named <title rend="quotes">The <foreign xml:lang="fr">Longue Duree of
                     Literary Prestige</foreign></title> (2016). This analysis of their work was
               made possible by the fact that — rather contrary to current convention — Underwood
               and Sellers released preprints and published their bespoke code on both Zenodo <ptr
                  target="#underwood_2015"/> and GitHub <ptr target="#underwood_2018"/>. Building
               upon their informally published work, we produced a computational narrative in the
               form of a Jupyter Notebook documenting our experience studying and interrogating the
               code. From the supplementary <quote rend="inline">close reading</quote> of Underwood
               and Sellers’s code, we discuss and reflect upon their work and on defactoring as an
               approach to the critical study of code.</p>
            <p>On the epistemological level, this article subsequently questions how useful it is
               that conventional scholarly literacy, means of critique, and publications conventions
               keep in place and even enforce a strong separation between the realm of the scholarly
               publication and the realm of code, if both are expressions of the same. We contend
               that these walls of separation can and should be broken down, and that this is one of
               the primary tasks and responsibilities of critical code studies. To this end, we need
               to examine the effects and affordances of disrupting and breaking these boundaries.
               This article and supplementary materials are an attempt to create an instance of such
               a disruption. What would it look like if we linked quotidian code to loftily
               theoretical exposition to create a complementary discursive space? How does this new
               computationally inflected mode of discourse afford a more robust scholarly dialogue?
               What may be gained by opening up bespoke code and having multiple discussions — the
               literary interpretative and, the computationally methodological — simultaneously?</p>
         </div>
         <div>
            <head>Background: Being Critical about Code in Literary Scholarship</head>
            <p>According to various scholars (e.g. <ptr target="#burgess_2011"/>
               <ptr target="#clement_2016"/>) there is a dichotomy between on the one hand a
                  <soCalled>pure</soCalled> intellectual realm associated with scholarly writing and
               academic print publication, and on the other hand the <soCalled>material
                  labour</soCalled> associated with the performance of the pure intellectual realm,
               for example, instrument making or programing. On closer inspection, such a dichotomy
               turns out to be largely artificial. For their argument, Burgess, Hamming, and Clement
               refer to the earlier work of Bruno Latour (1993) who casts the defining
               characteristic of modernity as a process of <soCalled>purification</soCalled> aiming
               to contrast the human culture of modernity to nature <ptr target="#latour_1993"/>.
               Burgess and Hamming observe a congruent process in academia: <quote rend="inline"
                  >Within the academy we see these processes of purification and mediation at work,
                  producing and maintaining the distinction between intellectual labor and material
                  labor, both of which are essential to multimedia production</quote> (burgess,
               2011, ¶11). This process serves to distinguish between scholarly and non-scholarly
               activities: <quote rend="block">The distinction between intellectual and material
                  labor is pervasive throughout scholarly criticism and evaluation of media forms.
                  […] In addition, any discussion of scholarly activities in multimedia formats are
                  usually elided in favor of literary texts, which can be safely analyzed using
                  traditional tools of critical analysis. <ptr target="#burgess_2011" loc="¶12"
                  />.</quote> However, as Burgess and Hamming note, this distinction is based upon a
               technological fallacy already pointed out by Richard Grusin in 1994. Grusin argued
               that Hypertext has not changed the essential nature of text, as writing has always
               already been hypertextual through the use of indices, notes, annotations, and
               intertextual references. To assume that the technology
               of Hypertext has revolutionarily unveiled or activated the associative nature of text
               amounts to the fallacy of ascribing the associative agency of cognition to the
               technology, which however is of course a <soCalled>mere</soCalled> expression of that
               agency.</p>
            <p>To assume an intellectual dichotomy between scholarly publication resulting from
               writing versus code resulting from programming, is a similar technological fallacy.
               To assert that scholarship is somehow bound to print publication exclusively is akin
               to the ascribing of agency to the technology of written text, because such an
               understanding of scholarship presupposes that something is scholarship because it is
               in writing, that writing makes it scholarship. But obviously, publication is a
               function of scholarship and scholarship is not a function of publication, because
               scholarship does not arise from publication but is <soCalled>merely</soCalled>
               expressed through it.</p>
            <p>If scholarship expresses anything through publication it is argument, which is — much
               more than writing — an essential property of scholarship. But in essence, it does not
               matter how, or by which form, the argument is made — whether it is made through
               numbers, pictures, symbols, words, or objects. Those are all technologies that enable
               us to shape and express an argument. This is not to say that technologies are mere
               inert and neutral epistemological tools; different technologies shape and affect
               argument in different ways. Technological choices do matter, and different
               technologies can enrich scholarly argument. Producing an argument requires some
               expressive technology, and the knowledge and ability to wield that technology
               effectively, which in the case of writing is called <soCalled>literacy</soCalled>. As
               Alan Kay observed, literacy is not just gaining a fluency in technical skills of
               reading and writing; it also requires a <quote rend="inline">fluency in higher level
                  ideas and concepts and how these can be combined</quote> (kay, 1993, 83). This
               fluency is both structural and semantic. In the case of writing as technology, it is
               for instance about sentence structure, semantic cohesion between sentences, and about
               how to express larger ideas by connecting paragraphs and documents. These elements of
               literacy translate to the realm of coding and computing <ptr target="#vee_2013"/>
               <ptr target="#vee_2017"/> where fluency is about the syntax of statements and how to
               express concepts, for instance, as object classes, methods and functions that call
               upon other programs and data structures to control the flow of computation. Text and
               writing may still be the most celebrated semiotic technologies to express an
               argument, but computer code understood as <soCalled>just another</soCalled> literacy
                  <ptr target="#knuth_1984"/>
               <ptr target="#kittler_1993"/>
               <ptr target="#vee_2013"/>
               <ptr target="#vee_2017"/> means it can thus equally be a medium of scholarly
               argument. We start from this assertion that coding and code — as the source code of
               computer programs that is readable to humans and which drives the performative nature
               of software <ptr target="#ford_2015"/>
               <ptr target="#hiller_2015"/> — can be inherent parts of scholarship or even
               scholarship by itself. That is: We assert that code can be scholarly, that coding can
               be scholarship, and that there is little difference between the authorship of code or
               text <ptr target="#van-zundert_2016"/>.</p>
            <p>There are two compelling reasons why code should be of interest to scholars. Much has
               been written about the dramatic increase of software, code, and digital objects in
               society and culture over the last decades often with a lamenting or dystopian view
                  <ptr target="#morozov_2013"/>
               <ptr target="#bostrom_2016"/>. But aside from doomsday prognostications, there is
               ample evidence that society and its artifacts are increasingly also made up of a
                  <soCalled>digital fabric</soCalled>
               <ptr target="#jones_2014"/>
               <ptr target="#berry_2014"/>
               <ptr target="#manovich_2013"/>. This means that the object of study of humanities
               scholars is also changing — literature, texts, movies, games, and music increasingly
               exist as digital data created through software (and thus code). This different fabric
               is also branching off cultural objects with different and new properties, for
               instance in the case of electronic literature and storytelling in general <ptr
                  target="#murray_2016"/>. It is thus crucial for scholars studying these new forms
               of humanistic artifacts to have an understanding of how to read code and the
               computational processes it represents. Furthermore, as code and software are
               increasingly part of the technologies that humanities scholars employ to examine
               their sources — examples of this abound (e.g. <ptr target="#van-dalen-oskam_2007"/>
               (broadwell, 2012), <ptr target="#piper_2015"/>
               <ptr target="#kestemont_2015"/>, etc.) — understanding the workings of code is
               therefore becoming a prerequisite for a solid methodological footing in the
               humanities.</p>
            <p>As an epistemological instrument, code has the interesting property of representing
               both the intellectual and material labor of scholarly argument in computational
               research. Code affords method not as a prosaic, descriptive abstraction, but as the
               actual, executable inscription of methodology. However, the code underpinning the
               methodological elements of the scholarly discourse are themselves not presented as
               elements in the discourse. Their status is akin to how data and facts are
               colloquially perceived, as givens, objective and neutral pieces of information or
               observable objects. But like data <ptr target="#gitelman_2013"/> and facts <ptr target="#betti_2015"/>, code is
               unlikely to be ever <soCalled>clean</soCalled>, <soCalled>unbiased</soCalled>, or
                  <soCalled>neutral</soCalled> (cf. also <ptr target="#berry_2014"/>). </p>
            <p>Code is the result of a particular literacy (cf. for instance <ptr
                  target="#knuth_1984"/>
               <ptr target="#kittler_1993"/>
               <ptr target="#vee_2013"/> (vee 2017) that encompasses the skills to read and write
               code, to create and interrelate code constructs, and to express concepts, ideas, and
               arguments in the various programming paradigms and dialects in existence. Like text,
               code has performative properties that can be exploited to cause certain intended
               effects. And also like text, code may have unintended side effects (cf. e.g. <ptr
                  target="#mcpherson_2012"/>. Thus, code is a symbolic system with its own rhetoric,
               cultural embeddedness <ptr target="#marino_2006"/>, and latent agency <ptr
                  target="#van-zundert_2016"/>. Therefore, rather than accepting code and its
               workings as an unproblematic expression of a mathematically neutral or abstract
               mechanism, it should be regarded as a first-order part of a critical discourse. </p>
            <p>However, the acceptance of code as another form of scholarly argument presents
               problems to the current scholarly process of evaluation because of a lack of
               well-developed methods for reading, reviewing, and critiquing bespoke code in the
               humanities domain. Digital humanities as a site of production of non-conventional
               research outputs — digital editions, web-based publications, new analytical methods,
               and computational tools for instance — has spurred the debate on evaluative practices
               in the humanities, exactly because practitioners of digital scholarship acknowledge
               that much of the relevant scholarship is not expressed in the form of traditional
               scholarly output. Yet the focus of critical engagement remains on <quote
                  rend="inline">the fiction of <soCalled>final outputs</soCalled> in digital
                  scholarship</quote>
               <ptr target="#nowviskie_2011"/>, on old form peer review <ptr
                  target="#antonijevic_2015"/>, and on approximating equivalencies of digital
               content and traditional print publication <ptr target="#presner_2012"/>. Discussions
               around the evaluation of digital scholarship have thus <quote rend="inline">tended to
                  focus primarily on establishing digital work as equivalent to print publications
                  to make it count instead of considering how digital scholarship might transform
                  knowledge practices</quote> <ptr target="#purdy_2010" loc="178"/> <ptr target="#anderson_2011"/>. As
               a reaction, digital scholars have stressed how peer review of digital scholarship
               should foremost consider how digital scholarship is different from conventional
               scholarship. They argue that review should be focussed on the process of developing,
               building, and knowledge creation <ptr target="#nowviskie_2011"/>, on the contrast and
               overlap between the representationality of conventional scholarship and the strong
               performative aspects of digital scholarship <ptr target="#burgess_2011"/>, and on the
               specific medium of digital scholarship <ptr target="#rockwell_2011"/>.</p>
            <p>The debate on peer review of digital output in digital scholarship might have
               propelled a discourse on the critical reading of code. However, the debate geared
               almost completely towards high level evaluation, concentrating for instance on the
               issue of how digital scholarship could be reviewed in the context of promotion and
               tenure track evaluations. Very little has been proposed as to concrete techniques and
               methods for the practical critical study of code as scholarship in scholarship.<note>
                  The situation is different in the sciences, where more concrete experiments with
                  code review are found. For instance the Journal of Open Source Software (<ref
                     target="http://joss.theoj.org/about">http://joss.theoj.org/about</ref>) is
                  attempting to alleviate these challenges by creating a platform for the
                  submission, review, and validation of scientific code and software. </note>
               Existing guidance pertains to digital objects such as digital editions <ptr
                  target="#sahle_2014"/> or to code as cultural artefact <ptr target="#marino_2006"
               />, but no substantial work has been put forward on how to read, critique, or
               critically study bespoke scholarly code. We are left with the rather general
               statement that <quote rend="inline">traditional humanities standards need to be part
                  of the mix, [but] the domain is too different for them to be applied without
                  considerable adaptation</quote>
               <ptr target="#smithies_2012"/> and the often echoed contention that digital artifacts
               should be evaluated <hi rend="italic">in silico</hi> as they are and not as to how
               they manifest in conventional publications <ptr target="#rockwell_2011"/></p>
            <p>In what went before, we hope to have shown and argued that bespoke code developed
               specifically in the context of scholarly research projects should be regarded as
               first class citizens of the academic process. As scholars, we need methods and
               techniques to critique and review such code. We posit that developing these should be
               an objective central to critical code studies. As a gambit for this development in
               what follows we present <term>defactoring</term> as one possible method.</p>
         </div>
         <div>
            <head>Towards a Practice of Critically Reading Bespoke Scholarly Code</head>
            <p>Beyond the theoretical and methodological challenges, reading and critically studying
               code introduces practical challenges. Foremost is the problem of determining what
               code is actually in scope for these practices. The rabbit hole runs deep as research
               code is built on top of standard libraries, which are built on top of programming
               languages, which are built on top of operating systems, and so on. Methodologically,
               a boundary must be drawn between the epistemologically salient code and the context
               within which it executes. <ptr target="#hinsen_2017"/> makes a useful distinction
               that divides scientific software into four layers.</p>
            <figure>
               <head>Layers of code according to their <soCalled>bespokeness</soCalled>, after
                  Hinsen 2017.</head>
               <graphic url="resources/images/figure01.png"/>
               <figDesc>PLACEHOLDER</figDesc>
            </figure>
            <p>First, there is a layer of <term>general software</term>. Generalized computational
               infrastructure like operating systems, compilers, and user interfaces fall into this
               category. Generic tools like Microsoft Word or Apache OpenOffice or general-purpose
               programming languages like Python or C, while heavily used in scientific domains,
               also have a rich life outside of science (and academia more broadly). The second
               layer comprises <term>scientific software</term>. Applications, libraries, or
               software packages that are not as general purpose as layer one, rather they are
               designed for use in scientific or academic activities. For example, Stata or SPSS for
               statistical analysis, Open-MPI for parallel computing in high performance computing
               applications, or Globus as a general tool for managing data transfers, AntConc for
               text corpus analytics, Classical Text Editor to create editions of ancient texts, or
               Zotero for bibliographic data management. A third layer comprises <term>disciplinary
                  software</term>, libraries for use in specific epistemological contexts for
               analysis of which the Natural Language Toolkit (NLTK) or the Syuzhet R package for
               literary analysis are excellent examples. The design of Syuzhet means it can be used
               in a variety of analyses, not just the analysis performed by Jockers <ptr
                  target="#jockers_2014"/>
               <ptr target="#jockers_2015"/>. Disciplinary software is distinct from lower layers as
               it embeds certain epistemological and methodological assumptions into the design of
               the software package that may not hold across disciplinary contexts. Lastly, there is
               a fourth layer of <term>bespoke software</term>. This layer comprises project
               specific code developed in pursuit of the very specific set of tasks associated with
               one particular analysis. This is the plumbing connecting other layers together to
               accomplish a desired outcome. Unlike the previous layers, this code is not meant to
               be generalized or reused in other contexts.</p>
            <p>As argued above: with increasing frequency project specific <term>bespoke code</term>
               is created and used in textual scholarship and literary studies (cf. for instance
                  <ptr target="#enderle_2016"/>
               <ptr target="#jockers_2013"/>
               <ptr target="#piper_2015"/>
               <ptr target="#rybicki_2014"/>
               <ptr target="#underwood_2014"/>
               <ptr target="#lahti_2020"/>. The algorithms, code, and software underpinning the
               analyses in these examples are not completely standardized <soCalled>off the
                  shelf</soCalled> software projects or tools. These codebases are not a software
               package such as AntConc that can be viewed as a generic distributable tool. Instead,
               these codebases are (in Hinsen’s model) the fourth layer of bespoke code: They are
               one-off highly specific and complex analytical engines, tailored to solving one
               highly specific research question based on one specific set of data. Reuse,
               scalability, and ease-of-use are — justifiably <ptr target="#baldridge_2015"/> — not
               specific aims of these code objects. This is code meant only to run a limited number
               of times in the context of a specific project or its evaluation.</p>
            <p>The words, prose, and narrative of a scholarly article are an expression of a
               rhetorical procedure. When critically engaging with a scholarly article, the focus is
               on the underlying argument and its attending evidence. Scholarly dialectic pertains
               not to the specifics of the prose, the style of writing, or the sentence structure.
               One could argue, paying attention to the details of code is equivalent to paying
               attention to wordsmithing and thus missing the forest for the trees, that is,
               fetishizing the material representation at the expense of the methodological
               abstraction. However, the significant difference is that the words are plainly
               visible to any and all readers, whereas the code is often hidden away in a GitHub
               repository (if we are lucky) or on a drive somewhere in a scholar’s office or
               personal cloud storage. We argue that the computational and data driven arguments are
               missing the material expression (the code) of their methodological procedures. The
               prosaic descriptions currently found in computational literary history and the
               digital humanities are not sufficient. Notwithstanding their admirable aims and
               objectives platforms where one would expect bespoke code bases to be in focus for
               primary attention and publication (such as, for instance, <title rend="italic"
                  >Cultural Analytics</title>
               <note>
                  <ref target="https://culturalanalytics.org/">https://culturalanalytics.org/</ref>
               </note> and <title rend="italic">Computational Humanities Research</title>)<note>
                  <ref target="https://computational-humanities-research.org/"
                     >https://computational-humanities-research.org/</ref>
               </note> do not provide standardized access to code or code repositories related to
               publications. Nor do they enforce the open publication of such research output. If
               the prose is an expression of the argument, with charts and numbers and models as
               evidence, then the code, which is the expression of the methodological procedures
               that produced the evidence, is just as important as the words. Our thinking is not
               unlike Kirschenbaum’s (2008) forensic approaches to writing in that just as digital
               text has a material trace, methodological analysis also has a materiality inscribed
               in code <ptr target="#kirschenbaum_2008"/>. However, we perhaps go farther to argue
               these inscriptions should be surfaced in publication. Why write an abstract prosaic
               description of methodological procedures divorced from the codified expression of
               those procedures? Why not interweave or link both types of expression more fully and
               more intrinsically?</p>
         </div>
         <div>
            <head>Defactoring</head>
            <p>We introduce an experimental technique called <term>defactoring</term> to address the
               challenges of critically reading and evaluating code. We expand on ideas of <ptr
                  target="#braithwaite_2013"/> who considered defactoring as the process of
               de-modularizing software to reduce its flexibility in the interest of improving
               design choices. This is counter intuitive to software engineering best practices,
               which prefer increased generalization, flexibility, and modularization. Refactoring
               and software engineering emphasize writing code that can be managed according to the
               organizational labor practices of software production and object-oriented design,
               breaking blocks of code into independent units that can be written and re-used by
               teams of developers (cf. for instance <ptr target="#metz_2016"/>. While these best
               practices make sense for code intended for Hinsen's first three layers of software,
               the fourth layer, bespoke code, might better have a different style with less
               emphasis on abstraction and modularity because such code becomes harder to read as a
               narrative.</p>
            <p>Here, we are interested in elucidating what a process of defactoring code looks like
               for the purposes of critically assessing code, which implies reading code. In our
               expanded notion of the concept, defactoring can be understood as a close reading of
               source code — and if necessary a reorganization of that code — to create a narrative
               around the function of the code. This technique serves multiple purposes: critically
               engaging the workings and meanings of code; peer reviewing code; understanding the
               epistemological and methodological implications of the inscribed computational
               process; and a mechanism for disseminating and teaching computational methods. We use
               defactoring to produce what might prospectively be called the first critical edition
               of source code in the digital humanities by unpacking Ted Underwood and Jordan
               Sellers’s code associated with their article <title rend="quotes">The <foreign
                     xml:lang="fr">Longue Duree</foreign> of Literary Prestige</title>
               (underwood-sellers, 2016a).<note> Interestingly, the article and the accompanying
                  code by Underwood and Sellers have been subject to scrutiny before. The
                  publication and code served as a use case for a workflow modeling approach aimed
                  at replication studies <ptr target="#senseney_2016"/>. We are not primarily
                  interested in replication per se, but the example serves to show how valuable the
                  publication of open source/open access code and data are for replication and peer
                  review.</note>
            </p>
            <p>The codebase that Underwood and Sellers produced and that underpins their argument in
                  <title rend="quotes">The <foreign xml:lang="fr">Longue Duree</foreign> of Literary
                  Prestige</title> is a typical example of multi-layered code. The code written by
               Underwood and Sellers is bespoke code, or fourth layer code in Hinsen’s model of
               scientific software layers. When dealing with a scholarly publication such as <title
                  rend="quotes">The <foreign xml:lang="fr">Longue Duree</foreign> of Literary
                  Prestige</title> , our reading should concentrate on layer four, the bespoke code.
               The code from the lower layers, while extremely important, should be evaluated in
               other processes. As Hinsen points out, layer four software is the least likely to be
               shared or preserved because it is bespoke code intended only for a specific use case;
               this means it most likely has not been seen by anyone except the original authors.
               Lower layer software, such as scikit-learn, has been used, abused, reviewed, and
               debugged by countless people. There is much less urgency therefore to focus the kind
               of intense critical attention that comes with scholarly scrutiny on this software
               because it already has undergone so much review and has been battle tested in actual
               use.</p>
            <p>There is no established definition of <term>defactoring </term>or its practice. We
               introduce defactoring as a process for <soCalled>close reading</soCalled> or possibly
               a tool for <soCalled>opening the black box</soCalled> of computational and data
               intensive scholarship. While it shares some similarity to the process of refactoring
               — in that we are <quote rend="inline">restructuring existing computing code without
                  changing its external behavior</quote> — refactoring restructures code into
               separate functions or modules to make it more reusable and recombinable. Defactoring
               does just the opposite. We have taken code that was broken up over several functions
               and files and combined it into a single, linear narrative. </p>
            <p>Our development of defactoring as a method of code analysis is deeply imbricated with
               a technical platform (just as all computational research is). But rather than pushing
               the code into a distant repository separate from the prosaic narrative, we compose a
                  <term>computational narrative</term>
               <ptr target="#perez_2015"/> — echoing Knuth’s literate programming (1984) — whereby
               Underwood and Sellers’s data and code are bundled with our expository descriptions
               and critical annotations. This method is intimately intertwined with the Jupyter
               Notebook platform which allows for the composition of scholarly and scientific
               inscriptions that are simultaneously human and machine readable. The particular
               affordances of the Notebook allow us to weave code, data, and prose together into a
               single narrative that is simultaneously readable and executable. Given our goals to
               develop a method for critically engaging computational scholarship, it is imperative
               we foreground Underwood and Sellers’s bespoke code, and the Jupyter Notebooks enables
               us to do so.</p>
         </div>
         <div>
            <head>Pace of Change</head>
            <p>The bespoke code we defactor is that which underlies an article that Underwood and
               Sellers published in <title rend="italic">Modern Language Quarterly</title> (MLQ)
               <ptr target="#underwood-sellers_2016"/>: <title rend="quotes">The <foreign xml:lang="fr">Longue
                     Durée</foreign> of Literary Prestige.</title> This article was the culmination
               of prior work in data preparation <ptr target="#underwood-sellers_2014"/>, coding
                  <ptr target="#underwood-sellers_2015"/>
               <ptr target="#underwood_2018"/>
               <ptr target="#underwood_2018"/> preparatory analysis <ptr
                  target="#underwood-sellers_2015"/>. The main thrust of the MLQ article seems to be
               one of method: <quote rend="block">Scholars more commonly study reception by
                  contrasting positive and negative reviews. That approach makes sense if you’re
                  interested in gradations of approval between well-known writers, but it leaves out
                  many works that were rarely reviewed at all in selective venues. We believe that
                  this blind spot matters: literary historians cannot understand the boundary of
                  literary distinction if they look only at works on one side of the boundary. <ptr target="#underwood-sellers_2016" loc="324"/>
                  </quote> To substantiate their claim, Underwood and
               Sellers begin their <quote rend="inline">inquiry with the hypothesis that a widely
                  discussed <soCalled>great divide</soCalled> between elite literary culture and the
                  rest of the literary field started to open in the late nineteenth century.</quote>
               To this end, they compare volumes of poetry that were reviewed in elite journals in
               the period 1820–1917 with randomly sampled volumes of poetry from the HathiTrust
               Digital Library from the same period. They filtered out volumes from the HathiTrust
               resource that were written by authors that were also present in the reviewed set,
               effectively ending up with non-reviewed volumes. In all, they compare 360 volumes of
                  <soCalled>elite</soCalled> poetry and 360 non-reviewed volumes. For each volume,
               the relative frequencies of the 3200 most common words are tallied and they apply
               linear regression to these frequency histograms. This linear regression model enables
               them finally to predict whether a sample that was not part of the regression data
               would have been reviewed or not. The accuracy of their predictions turn out to be
               between 77.5 and 79.2 percent. This by itself demonstrates that there is some
               relationship between some poetry volume’s vocabulary and that volume being reviewed.
               But more importantly, what they can glean from their results is that the traditional
               idea that literary fashions are pretty stable over some decades and are then
               revolutionized towards a new fashion deserves revisiting: The big 19th century divide
               turns out not to be a revolutionary change but a stable and slowly progressing trend
               since at least since 1840. Underwood and Sellers conclude: <quote rend="block">None
                  of our <quote rend="inline">models can explain reception perfectly, because
                     reception is shaped by all kinds of social factors, and accidents, that are not
                     legible in the text. But a significant chunk of poetic reception can be
                     explained by the text itself (the text supports predictions that are right
                     almost 80 percent of the time), and that aspect of poetic reception remained
                     mostly stable across a century</quote> (underwood-sellers, 2016b)</quote>
               Sudden changes also do not emerge if they try to predict other social categories like
               genre or authorial gender. They finally conclude that the question of why the general
               slow trend they see exists is too big to answer from these experiments alone, because
               of the many social factors that are involved.</p>
            <p>Underwood and Sellers purposely divided their code into logical and meaningful parts,
               modules, and functions stitched together into a data processing and analysis script.
               We found to better understand the code as readers (vs. authors) and therefore
               necessary to restructure, defactor, the code into what is usually understood as a
               poor software engineering practice, namely making a single long, strongly integrated,
               procedural process. This makes the code a linear narrative, which is easier for
               humans to read while the computer is, for the most part, indifferent. There is a
               tension between these two mutually exclusive representations of narratives with code
               divided and branched, emerging from the process of development by software engineers,
               and with prose as a linear narrative intended for a human reader. What we observed is
               that the processes of deconstructing literature and code are not symmetrical but
               mirrored. Where deconstructing literature usually involves breaking a text apart into
               its various components, meanings, and contexts, deconstructing software by
               defactoring means integrating the code’s disparate parts into a single, linear
               computational narrative. <quote rend="inline">Good code,</quote> in other words, is
               already deconstructed (or <soCalled>refactored</soCalled>) into modules and
               composable parts. For all practical purposes we effectively are turning <quote
                  rend="inline">well engineered</quote> code into sub-optimal code full of
                  <soCalled>hacks</soCalled> and terrible <soCalled>code smells</soCalled> by
               de-modularizing it. However, we argue, this <soCalled>bad</soCalled> code is easier
               to read and critique while still functioning as its authors intended.</p>
            <p>Defactoring injects the logical sections of the code, parts that execute steps in the
               workflow, with our own narrative reporting on our understanding of the code and its
               functioning at that moment of the execution. The Jupyter Notebook platform makes this
               kind of incremental exploration of the code possible and allows us to present a fully
               functioning and executable version of Underwood and Sellers’s code that we have
               annotated. Reading (and executing along the way) this notebook therefore gives the
               reader a close resemblance of the experience of how we as deconstructionists
                  <soCalled>closely read</soCalled> the code.<note> To support ourselves in the
                  reading process, we found it useful to keep track of the
                     <soCalled>state</soCalled> of the code as it was executing. We implemented this
                  by listing all the <soCalled>active</soCalled> variables and their values at each
                  step of the process. The explanation of each step is therefore also amended with a
                  listing of these variables.</note>
            </p>
         </div>
         <div>
            <head>Defactoring Pace of Change Case Study</head>
            <p>As a supplement to this article, we have included our example defactoring of
               Underwood and Sellers’ Pace of Change code. We have forked their Github repository
               and re-worked their code into a Jupyter notebook. Conceptually, we combined their
               Python code files with our narrative to create a computational narrative that can be
               read or incrementally executed to facilitate the exploration of their computational
               analysis (illustrated in Figure 2).</p>
            <figure>
               <head>Illustration of defactoring Pace of Change as a conceptual process. On the left
                  the original highly structured and modularized code. Right our interpretation and
                  narrative. In the middle code and narrative brought together in a single
                  computational story published as a Jupyter notebook. </head>
               <graphic url="resources/images/figure02.png"/>
               <figDesc>PLACEHOLDER</figDesc>
            </figure>
            <p>The defactored code is available in the following GitHub repository: <ref
                  target="https://github.com/interedition/paceofchange"
                  >https://github.com/interedition/paceofchange</ref>
            </p>
            <p>Readers are strongly encouraged to review the notebook on GitHub or download and
               execute it for an even richer, interactive experience. Here we want to highlight two
               specific examples within the code of Underwood and Sellers that in our reading of the
               code became rather significant.</p>
            <p>Binomal_select()</p>
            <p>Consider the following snippet of <hi rend="italic">commented</hi> code that gestures
               to a path not taken in Underwood and Sellers' data analysis.</p>
            <p># vocablist = binormal_select(vocablist, positivecounts, negativecounts,
               totalposvols, totalnegvols, 3000)</p>
            <p># Feature selection is deprecated. There are cool things</p>
            <p># we could do with feature selection,</p>
            <p># but they'd improve accuracy by 1% at the cost of complicating our explanatory
               task.</p>
            <p># The tradeoff isn't worth it. Explanation is more important.</p>
            <p># So we just take the most common words (by number of documents containing them)</p>
            <p># in the whole corpus. Technically, I suppose, we could crossvalidate that as
               well,</p>
            <p># but *eyeroll*.</p>
            <p>Underwood and Seller's code above does not actually perform any work as each line has
               been commented out; however, we include it because it points towards an execution
               path not taken and an interesting rationale for why it was not followed. In the
                  <title rend="quotes">production</title> code, the heuristic for feature selection
               is to simply select the 3200 most common words by their appearance in the 720
               documents. This is a simple and easy technique to implement and — more importantly —
               explain to a literary history and digital humanities audience. Selecting the top
               words is a well-established practice in text analysis, and it has a high degree of
               face validity. It is a good mechanism for removing features that have diminishing
               returns. However, the commented code above tells a different, and methodologically
               significant, story. The comment discusses an alternative technique for feature
               selection using binormal selection. Because this function is commented out and not
               used in the analysis, we have opted to not include it as part of the defactoring.
               Instead, we have decided to focus on the more interesting rationale about why
               binormal selection is not being used in the analysis as indicated in the comments:
                  <quote rend="block">There are cool things we could do with feature selection, but
                  they'd improve accuracy by 1% at the cost of complicating our explanatory task.
                  The tradeoff isn't worth it. Explanation is more important.</quote> This comment
               reveals much about the reasoning, the effort, and energy focused on the important,
               but in the humanities oft neglected, work of discussing methodology. As Underwood
               argued in <hi rend="italic">The literary uses of high-dimensional space</hi>
               <ptr target="#underwood_2015"/>, while there is enormous potential for the application of
               statistical methods in humanistic fields like literary history, there is resistance
               to these methods because there is a resistance to methodology. Underwood has
               described the humanities disciplines relationship to methodology as an <quote
                  rend="inline">insistence on staging methodology as ethical struggle</quote>
               <ptr target="#underwood_2013"/>. In this commented code , we can see the material
               manifestation of Underwood's methodological sentiment, in this case embodied by
               self-censorship in the decision to not use more statistically robust techniques for
               feature selection. We do not argue this choice compromises the analysis or final
               conclusions, rather we want to highlight the practical and material ways research
               methods are not a metaphysical abstraction, but rather have a tangible and observable
               reality. By focusing on a close reading of the code and execution environment, by
                  <term>defactoring</term>, we illuminate methodology and its relation to the
               omnipresent <hi rend="italic">explanatory</hi> task commensurate with the use of
               computational research methods in the humanities.</p>
            <div><head>The Croakers</head>
            <p>At this point, any prosaic resemblance left in the data is gone and now we are
               dealing entirely with textual data in a numeric form.</p>
            <figure>
               <head>
                  <title rend="italic">The Croakers</title> by Joseph Rodman Drake as a series of
                  word frequencies. </head>
               <graphic url="resources/images/figure03.png"/>
               <figDesc>PLACEHOLDER</figDesc>
            </figure>

            <p>The code output above shows us a single volume processed by the code, <hi
                  rend="italic">The Croakers</hi> by Joseph Rodman Drake. As we can see, the words
               are now represented as a list of numbers (representing word frequencies). However,
               this list of numbers still requires additional transformation in order to be
               consumable by a logistic regression. The word frequencies need to be normalized so
               they are comparable across volumes. To do this, Underwood and Sellers divide the
               frequency of each individual word by the total number of words in that volume. This
               makes volumes of different lengths comparable by turning absolute frequencies into
               relative frequencies. The code output below shows the normalized frequency values for
               10 columns (10 words) of the last 5 poetry volumes.</p>
            <p/>
            <figure>
               <head>
                  <title rend="italic">The Croakers</title> by Joseph Rodman Drake as a series of
                  relative word frequencies. </head>
               <graphic url="resources/images/figure04.png"/>
               <figDesc>PLACEHOLDER</figDesc>
            </figure>
            <figure>
               <head>One page of poetry from a print publication of <title rend="italic">The
                     Croakers</title> by Joseph Rodman Drake. (Source <ref
                     target="https://books.google.com/books?id=j1s4AAAAIAAJ&amp;source=gbs_navlinks_s"
                     >Google Books</ref>)</head>
               <graphic url="resources/images/figure05.png"/>
               <figDesc>PLACEHOLDER</figDesc>
            </figure>
            <figure>
               <head> The final representation of <title rend="italic">The Croakers </title>as a
                  cross on this chart at 1860 on the x-axis and around 0.4 on the y-axis.</head>
               <graphic url="resources/images/figure06.png"/>
            </figure>
            <p>The last row in code listing 2 (the row starting with the number 719) is the
               normalized representation of <title rend="italic">The Croakers</title> by Joseph
               Rodman Drake. It is one of 720 relatively indistinguishable rows of numbers in this
               representation of 19th century poetry. This is a radical transformation of the
               original, prosaic representation literary historians are probably used to seeing
               (shown in Figure 3) and that would be the subject of close reading. What we can see
               here is the contrast between representations for close and distant reading
               side-by-side. </p></div>
         </div>
         <div>
            <head>Discussion</head>
            <p>The story told by the <title rend="italic">Defactoring Pace of Change</title>
               case-study is that of methodological data transformation through a close reading <hi
                  rend="italic">and execution</hi> of bespoke code. <hi rend="italic">The code is an
                  engine of intermediate representations,</hi> meaning the computational narrative
               told by Defactoring Pace of Change is one of cleaning, shaping, and restructuring
               data; transformation of poetry into data (and metadata), of large data into small
               data, and finally of data into visualizations. The Code is a material record, a
               documentary residue, of Underwood and Sellers’ methodology. </p>
         </div>
         <div>
            <head>Representations of Data in and Through Code</head>
            <p>From the perspective of data, the code of Pace of Change is not the beginning. The
               project begins with a collection of prepared data and metadata from poetry volumes
               transformed into bags of words <ptr target="#underwood_2014"/>. There was a
               significant amount of <soCalled>data-work</soCalled> completed before Pace of Change
               began, but just as bespoke code is built on shared libraries, operating systems, and
               general purpose programming languages, the <term>bespoke data</term> is built on
               previous data and data work. Both the data and code included in Pace of Change are
               products derived from larger <soCalled>libraries</soCalled> (both in the sense of
               software libraries like scikit-learn and digital libraries like HathiTrust). The
               OCR’d texts in the HathiTrust digital library are akin to the general purpose
               programming languages or operating systems; digital collections have many uses.<note>
                  See the Collections as Data project for more explorations of these dynamics. <ref
                     target="https://collectionsasdata.github.io"
                     >https://collectionsasdata.github.io</ref>
               </note> The content and context of the poetry data before their use in the <hi
                  rend="italic">Pace of Change</hi> analysis are salient and important; data are the
               result of socio-technical processes <ptr target="#chalmers_2017"/>
               <ptr target="#gitelman_2013"/>. Defactoring focuses on the intimate relationship
               between the bespoke-data, bespoke-code, and the environment within which computations
               occur. </p>
            <p>The data in Pace of Change are not one thing, but rather a set of things undergoing a
               series of interconnected renderings and transformations. As we see in <hi
                  rend="italic">The Croakers</hi> example above, poetry starts as a tabular
               collection of metadata and bags of words. The code filters, cleans, identifies a
               subset of the poetry data relevant to the analysis. The selected metadata then drives
               the processing of the data files, the bags of words, to select the top 3,200 words
               and creates a standard vectorized representation of each poetry volume and,
               importantly for supervised learning, their associated labels (<quote rend="inline"
                  >reviewed</quote> or <quote rend="inline">not reviewed</quote> ). Much of the
               bespoke data/code-work of Pace of Change is in the service of producing the
               representation of the data we see in Figure 4; bespoke code for transforming bespoke
               data into standardized data conformant to standardized code. What comes out the other
               side of the mathematically intensive computation of Pace of Change, the logistic
               regression, is <hi rend="italic">more</hi> data. But these data are qualitatively and
               quantitatively different because they reveal new insights, patterns, and significance
               about poetry. The predictions of 720 individual statistical models for each poetry
               volume, as seen in Figure 6, and the coefficients of the final statistical model are,
               for Underwood and Sellers, the important representations — for it is through the
               interpretation of these representations that they can find new insights about
               literary history. The data story of the <hi rend="italic">Pace of Change</hi> code
               ends with a chart and two new CSV files. One could, theoretically, review and
               critique these data, but we would argue, focusing on just the data in absence of the
               code which documents their provenance would only be a small part of the story.</p>
            <p>Even though the data, its transformations, and renderings are central to Underwood’s
               and Sellers’ understanding of the pace of change of literary vocabulary, almost
               nothing of this scientific material makes it to the final transformation that is
               represented with the scholarly article eventually published in <hi rend="italic"
                  >Modern Language Quarterly</hi>. In fact, all code and data are jettisoned from
               the narrative and only a high-level prose description of the computational research
               and one picture congruent with Figure 6 in this publication remain. As a full and
               accountable description of the methodology, this seems rather underwhelming.</p>
            <div>
               <head>Method Made Material</head>
               <p>Defactoring Pace of Change reveals an important methodological dynamic about the
                  relationship between mundane data management and urbane methodology. As the
                     <soCalled>binormal_select()</soCalled> example above shows, there were
                  analytical paths implemented, but ultimately not pursued. Only one of the two
                  feature selection methods is discussed in the MLQ article. Underwood and Sellers
                  chose not to use a more robust method for feature selection because they foresaw
                  the improved accuracy would not balance the effort required to explain it to an
                  audience unfamiliar with more advanced statistical expertise. There is a clear
                  tension, and gap, between the techniques being used and the explanation that must
                  accompany them. Similarly in the MLQ publication and the Figshare contribution, we
                  find many allusions to the tweaking of the models that Underwood and Sellers used,
                  but for which we do not find in the code — for instance, where they refer to
                  including gender as a feature <ptr target="#underwood-sellers_2016" loc="338"/>
                  and to using multi period models <ptr target="#underwood-sellers_2016" loc="329"
                  />. Traces remain of these analyses in the code but supplanted by later code and
                  workflows these statistical wanderings are not explicitly documented
                     anymore.<note> This also does not address Andrew Goldstone’s efforts to <ref
                        target="https://andrewgoldstone.com/blog/2016/01/04/standards/">reproduce
                        Pace of Change in R</ref>, which implies a form of methodological
                     equivalence of materially dissimilar code bases. </note>
               </p>
               <p>In all, this means that there is a lot of analysis and code-work that remains
                  unrepresented. Even in a radically open project such as Pace of Change, there is
                  still going to be code, data, and interpretive prose that does not make the final
                  cut (i.e. the MLQ article). Moreover, much analytic and code effort remains
                  invisible because it does not appear in the final code repository, leaving only an
                  odd trace in the various narratives. We are not arguing that <hi rend="italic"
                     >all</hi> of the developmental work of scholarship be open and available, but
                  our defactoring of Pace of Change makes us wonder what a final, camera-ready
                  representation of the code produced by Underwood and Sellers would include. </p>
               <p>The invisibility of so many parts of the research narrative signals to us the very
                  need for the development of a scholarly literacy and infrastructure that engages
                  with the bespoke code of research not as banal drudgery, but as the actual,
                  material manifestation of methodology. What if the annotated code, such as that
                  which we produced in defactoring Pace of Change, was the <title rend="quotes"
                     >methods</title> section? Only by such deep and intimate understanding of code
                  can we award credit and merit to the full analytical effort that scholars
                  undertake in computational explorations. </p>
            </div>
            <div>
               <head>On Reading Code</head>
               <p>This experiment in defactoring highlights a gap between the narrative of the code
                  and that of the MLQ article. This gap is enlarged by the current conventions of
                  scholarly publishing and communication that discourages including code in the
                  publication itself. But who really wants to read code anyway? As an invited early
                  reader of this work pointed out, the code is not that interesting because scholars
                  are primarily interested in the <soCalled>underlying methodology;</soCalled> an
                  abstract theoretical construct. But where exactly does this <soCalled>underlying
                     methodology</soCalled> obtain a material reality? In the minds of authors,
                  reviewers, and readers? We argue computational research creates a new discursive
                  space: the code <hi rend="italic">articulates</hi> the underlying methodology.
                  There is not some metaphysical intellectual method whose material reality exists
                  in the noosphere. When we read code, we read methodology. </p>
               <p>This is a radical proposition that implies a disruptive intervention in the
                  conventions of scholarly publishing and communication where data and
                  computationally intensive works are concerned. Very rarely are the data and code
                  incorporated directly into the prosaic narrative, and why would they? Code is
                  difficult to read, filled with banal boilerplate that doesn’t directly contribute
                  to an argument and interpretation. Furthermore, code is challenging to express in
                  print-centric mediums like PDFs and books. But when the documentary medium itself
                  becomes a platform for the manipulation and execution of the code (i.e. a web
                  browser and computational notebooks) then it is possible to imbricate the material
                  expression of methodological procedures <hi rend="italic">in-line</hi> with the
                  prosaic representations of the rhetorical procedure. </p>
               <p>
                  <title rend="italic">Defactoring Pace of Change</title> is perhaps a first,
                  roughshod attempt at exploring a new discursive space. We have, given the tools at
                  our disposal, tried to create a publication that represents what could be, and
                  should be, possible. However, the platforms and the practices do not exist or have
                  not come together to truly represent this idyllic confluence of prose, data, and
                  code. <title rend="italic">Defactoring Pace of Change</title> leverages Jupyter
                  Notebooks as a platform that affords the ability for both humans and machines to
                  read the same document, but the platforms of publishing and reading such documents
                  are non-existent or are immature at best. Beyond platforms, there are research and
                  publication practices, a set of conventions that need to emerge where code is more
                  seamlessly integrated into the narrative.<note> One could even imagine the
                     creation of Domain Specific Languages (DSLs) designed specifically for both the
                     data cleaning and analysis work while also being easier on the eyes for human
                     readers. Paradigms like<ref
                        target="https://en.wikipedia.org/wiki/Language-oriented_programming">
                        Language-oriented programming</ref> and the <ref
                        target="https://racket-lang.org/">Racket programming language</ref> could
                     push the idea of literate programming even farther than what Knuth
                     envisioned.</note> Our reconfiguration of Underwood and Seller’s Python code
                  into a linear structure and the intermixing of prosaic descriptions is an
                  experiment in establishing new practices and conventions for computational
                  narratives.</p>
            </div>
         </div>
         <div>
            <head>Conclusion </head>
            <p>There is a tendency both in scholars and engineers to separate things <ptr
                  target="#bowker_1999"/>. We can see one such separation in the TEI-XML community.
               Inspired by early developments in electronic typesetting <ptr target="#goldfarb_1996"
               />, both textual scholars and engineers arrived upon the idea of separation of form
               and content <ptr target="#derose_1990"/>: There is the textual information (<quote
                  rend="inline">Nixon resigns</quote> ), and there is how that information looks
               (e.g. bold large caps in the case of a newspaper heading). Thus, in TEI-XML an
               unproblematic separation of information and layout is assumed. On closer inspection
               however, such a separation is not as unproblematic at all <ptr target="#welch_2010"/>
               <ptr target="#galey_2010"/>. Form demands to be part of meaning and interpretation as
               is dramatically clear from looking at just one poem by William Blake. Yet such
               separation has emerged in science and research: Data tends to be separated from
               research as an analytical process, and the creation of digital research objects (such
               as digital data and analytic code) goes often unrecognized as intellectual research
               work and is considered <soCalled>mere</soCalled> supportive material labor <ptr
                  target="#burgess_2011"/>. Data is mostly regarded as a neutral and research
               independent entity, indeed something <soCalled>given</soCalled> as the Latin root
               suggests. That the state of data are not quite as straightforward has been argued
               before <ptr target="#galey_2010"/>
               <ptr target="#drucker_2011"/>
               <ptr target="#gitelman_2013"/>. From our experience defactoring Pace of Change we
               derive the same finding: There are rich stories to tell about the interactions
               between code and data.</p>
            <p>Code can be read and examined independently of its context and purpose, as a static
               textual object. In such a case, one looks critically at the structure of the code —
               are separate steps of the process clearly delineated and pushed to individual
               subroutines to create a clearly articulated and maintainable process; are there
               considerable performance issues; have existing proven libraries been used? This kind
               of criticism — we could compare it to textual criticism — is informative but in a way
               that is wholly unconnected to the context of its execution. It is like picking apart
               every single cog in a mechanical clock to judge if it is well built, but without
               being interested in what context and for what purpose it will tell time. This would
               be code review as practiced in an industrial setting. Code review takes the
               structural and technical quality of code into consideration only insofar that obvious
               analytical errors should be pointed out, judged against measures of performance and
               industrial scalability and maintainability. However, this approach has little
               relevance for the bespoke code of scholarly productions; it is relatively <quote
                  rend="inline">okay for academic code to suck</quote> as compared to the best
               practices of industrial code-work <ptr target="#baldridge_2015"/>. But what about
               best practices for <hi rend="italic">understanding</hi> the bespoke code of scholarly
               research? What about critically understanding code that <soCalled>only runs
                  once</soCalled> and whose purpose is insight rather than staying around as
               reusable software? We put forth defactoring as a technique for unveiling the workings
               of such bespoke code to the scholarly reader (and potentially a larger audience). We
               cast it as a form of close reading that draws out the interaction between code and
               the content, the data, and the subject matter. Essentially, we think, defactoring is
               a technique to read and critique those moments of interaction. Data, analytic code,
               and subject matter co-exist in a meaningful dynamic and deserve inspection. It is at
               these points that defactoring affords a scholar to ask — not unlike she would while
               enacting literary criticism: What happens here and what does it mean? Whereas the
               original code is mere supportive materials, the defactored and critically read code
               morphs into a first-order computational narrative that elevates the documentary
               residue of analysis to a critical component of the scholarly contribution. </p>
            <p>In this sense, we found that defactoring is more than just a method to open up
               bespoke code to close reading in the digital humanities. It also shows how code
               literacy, just as <soCalled>conventional</soCalled> literacy, affords an
               interpretative intellectual engagement with the work of other scholars, which is
               interesting in itself. The code is an inscription of methodological choices and can
               bridge the gap between the work that was done and the accounts of the work. We think
               that the potential of defactoring reaches beyond the domain of digital humanities. As
               a critical method it intersects with the domains of Critical Code Studies and
               Cultural Analytics as well, and could as a matter of fact, prove viable and useful in
               Science and Technology Studies or any scientific/scholarly domain where bespoke code
               is used or studied.</p>
            <p>On an epistemological level, once again it appears that we cannot carve up research
               in neatly containerized independent activities of which the individual quality can be
               easily established and aggregated to a sum that is greater than the parts. The
                  <soCalled>greater</soCalled> is exactly in the relations that exist between the
               various activities and that become opaque if the focus is put on what is inside the
               different containers. This is why we would push even farther than saying that data
               and code are not unproblematically separable entities. Indeed, we would argue that
               they are both intrinsic parts of a grander story that Underwood and Sellers tell us
               and which consists of several intertwined narratives: There is a narrative that is
               told by the code, one that is told by the comments we found in that code, and there
               is a narrative of data transformations. These narratives together become the premises
               of an overarching narrative that results first as a Figshare contribution,<note>
                  <ref
                     target="https://figshare.com/articles/journal_contribution/How_Quickly_Do_Literary_Standards_Change_/1418394"
                     >https://figshare.com/articles/journal_contribution/How_Quickly_Do_Literary_Standards_Change_/1418394</ref>
               </note> and later as an MLQ publication. These narratives are all stacked turtles,
               and they all deserve proper telling. </p>
            <p>Quite logically, with each stacked narrative the contribution of each underlying
               narrative becomes a little more opaque. The MLQ article suggests an unproblematic,
               neat, and polished process of question-data-analysis-result. But it is only thanks to
               their openness that Underwood and Sellers grant us insight to peer into the gap and
               see the computational process of data analysis to a presentable result. Underwood and
               Sellers went through several iterations of refining their story. The Figshare
               contribution and the code give us much insight into what the real research looked
               like for which the MLQ article, in agreement with Jon Claerbout’s ideas (buckheit,
               1995), turns out to be a mere advertising of the underlying scholarship. In contrast
               to what we argue here — that data and code deserve more exposure and critical
               engagement as being integral parts of a research narrative — we observed in the
               succession of narrative transformations that the aspect and contribution of code
               became not only more opaque with every stacked narrative but vanished altogether from
               the MLQ article. This gap is not a fault of Underwood and Sellers, but rather deeply
               is embedded in the practices and expectations of scholarly publishing.</p>
            <p>We wish to close with a pivotal reflection on our own challenges publishing this
               article. Our original vision for <title rend="italic">Defactoring Pace of Change
               </title>was to publish it as a single, extensive computational narrative that
               includes both this theoretical discussion and interpretation along with the
               defactored narrative of Underwood and Sellers' Python code. We wanted all of the
               context, interpretation, and methodological code to be a single document; a combined
               narrative readable by humans and executable by computers. However, every single
               reviewer we have met has suggested that we should split up the scholarly argument and
               the defactored code, thus creating a <hi rend="italic">new</hi> gap between this
               theoretical discussion you are reading and the case-study/Notebook with the actual
                  <title rend="italic">Defactoring Pace of Change</title> living on another
               platform. Reading the notebook is difficult, possibly mind numbing. We have done our
               best to make the Pace of Change code readable and our close reading of the code
               revealed many interesting aspects about data, representation, process, and
               transformation. Our hope was to make an argument with the very structure and form of
                  <hi rend="italic">Defactoring Pace of Change </hi>by including the code as part of
               the narrative. However, as one of our earlier readers pointed out, this has been a
                  <quote rend="inline">brilliant, glorious, provocative failure,</quote> while we
               hope to have put forth an argument about bespoke code using the standard scholarly
               prosaic conventions, we have failed to challenge and change the deeply ingrained
               conventions and infrastructures of scholarly publishing.<note>
                  <quote rend="inline">We fought the law and the law won.</quote>
               </note>
            </p>
            <p>What if Underwood and Sellers' had written The <foreign xml:lang="fr">Longue
                  Durée</foreign> of Literary Prestige to include the code written in a defactored
               style, that is, as a linear narrative intermixed with human-readable expository
               annotations? They would have also faced the same structural challenges publishing
               their article as we faced in <hi rend="italic">Defactoring Pace of Change</hi>. The
               conventions of scholarly publishing and structuring a scholarly narrative are not
               congruent with the Computational Notebook paradigm. We do not yet have academic genre
               conventions for <hi rend="italic">publishing</hi> bespoke code. What would a
               notebook-centric scholarly publication, one with no gap that imbricated code and
               interpretation, look like? <hi rend="italic">Defactoring Pace of Change</hi> is our
               attempt at a provocation not only to consider the epistemological and methodological
               significance of bespoke code in computational and data intensive digital humanities
               scholarship, but also to consider the possibilities of computation in the expression
               of such scholarship.</p>
         </div>
      </body>
      <back>
         <listBibl>
            <bibl xml:id="anderson_2011" label="Anderson and McPherson 2011">Anderson, S., and
               McPherson, T. (2011) <title rend="quotes">Engaging digital scholarship: Thoughts on
                  evaluating multimedia scholarship</title>, <title rend="italic"
               >Profession</title>, pp. 136–51. Available at: <ref
                  target="https://doi.org/prof.2011.2011.1.136"
                  >https://doi.org/prof.2011.2011.1.136</ref>.</bibl>

            <bibl xml:id="antonijevic_2015" label="Antonijević 2015" sortKey="Antonijevic"
               >Antonijević, S. (2015) <title rend="italic">Amongst digital humanists: An
                  ethnographic study of digital knowledge production</title>. London, Houndmills,
               New York: Palgrave Macmillan.</bibl>

            <bibl xml:id="baldridge_2015" label="Baldridge 2015">Baldridge, J. (12 May, 2015) <title
                  rend="quotes">It's okay for academic software to suck</title>, <title
                  rend="italic">Java Code Geeks</title> (blog). Available at: <ref
                  target="https://www.javacodegeeks.com/2015/05/its-okay-for-academic-software-to-suck.html"
                  >https://www.javacodegeeks.com/2015/05/its-okay-for-academic-software-to-suck.html</ref>.</bibl>

            <bibl xml:id="berry_2014" label="Berry 2014">Berry, D.M. (2014) <title rend="italic"
                  >Critical theory and the digital. Critical theory and contemporary
               society</title>. New York: Bloomsbury Academic.</bibl>

            <bibl xml:id="betti_2015" label="Betti 2015">Betti, A. (2015) <title rend="italic"
                  >Against facts</title>. Cambridge Massachusetts, London England: MIT Press.</bibl>

            <bibl xml:id="bostrom_2016" label="Bostrom 2016">Bostrom, N. (2016) <title rend="italic"
                  >Superintelligence: Paths, dangers, strategies</title>. Oxford: Oxford University
               Press.</bibl>

            <bibl xml:id="bowker_1999" label="Bowker and Star 1999">Bowker, G.C., and Star, S.L.
               (1999) <title rend="italic">Sorting things out: Classification and its
                  consequences</title>. The MIT Press.</bibl>

            <bibl xml:id="braithwaite_2013" label="Braithwaite 2013">Braithwaite, R. (8 October,
               2013) <title rend="quotes">Defactoring</title>. Available at: <ref
                  target="http://raganwald.com/2013/10/08/defactoring.html"
                  >http://raganwald.com/2013/10/08/defactoring.html</ref>.</bibl>

            <bibl xml:id="broadwell_2012" label="Broadwell and Tangherlini 2012">Broadwell, P., and
               Tangherlini, T.R. (2012) <title rend="quotes">TrollFinder: Geo-semantic exploration
                  of a very large corpus of Danish folklore</title>, in <title rend="italic"
                  >Proceedings of LREC</title>. Istanbul, Turkey.</bibl>

            <bibl xml:id="buckheit_1995" label="Buckheit and Donoho 1995">Buckheit, J., and Donoho,
               D.L. (1995) <title rend="quotes">Wavelab and reproducible research</title>, in A.
               Antoniadis (ed.) <title rend="italic">Wavelets and Statistics</title>, pp. 55–81. New
               York, NY: Springer. Available at: <ref
                  target="https://statweb.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf"
                  >https://statweb.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf</ref>.</bibl>

            <bibl xml:id="burgess_2011" label="Burgess and Hamming 2011">Burgess, H.J., and Hamming,
               J. (2011) <title rend="quotes">New media in academy: Labor and the production of
                  knowledge in scholarly multimedia</title>, <title rend="italic">DHQ: Digital
                  Humanities Quarterly</title>, 5(3). Available at: <ref
                  target="http://digitalhumanities.org/dhq/vol/5/3/000102/000102.html"
                  >http://digitalhumanities.org/dhq/vol/5/3/000102/000102.html</ref>.</bibl>

            <bibl xml:id="carullo_2020" label="Carullo 2020">Carullo, G. (2020) <title rend="italic"
                  >Implementing effective code reviews: How to build and maintain clean
               code</title>. New York: Apress.</bibl>

            <bibl xml:id="cerquiglini_1999" label="Cerquiglini 1999">Cerquiglini, B. (1999) <title
                  rend="italic">In praise of the variant: A critical history of philology</title>.
               Baltimore: The Johns Hopkins University Press.</bibl>

            <bibl xml:id="chalmers_2017" label="Chalmers and Edwards 2017">Chalmers, M.K., and
               Edwards, P.N. (2017) <title rend="quotes">Producing 'one vast index': Google Book
                  Search as an algorithmic system</title>, <title rend="italic">Big Data &amp;
                  Society</title>, 4(2). Available at: <ref
                  target="https://doi.org/10.1177/2053951717716950"
                  >https://doi.org/10.1177/2053951717716950</ref>.</bibl>

            <bibl xml:id="clement_2016" label="Clement 2016">Clement, T.E. (2016) <title
                  rend="quotes">Where is methodology in digital humanities?</title>, in <title
                  rend="italic">Debates in the Digital Humanities 2016</title>, pp. 153–75.
               University of Minnesota Press. Available at: <ref
                  target="http://www.jstor.org/stable/10.5749/j.ctt1cn6thb.17"
                  >http://www.jstor.org/stable/10.5749/j.ctt1cn6thb.17</ref>.</bibl>

            <bibl xml:id="derose_1990" label="DeRose et al. 1990" sortKey="Derose">PLACEHOLDER</bibl>
            
            <bibl xml:id="drucker_2011" label="Drucker 2011">Drucker, J. (2011) <title rend="quotes"
                  >Humanities approaches to graphical display</title>, <title rend="italic">Digital
                  Humanities Quarterly</title>, 5(1). Available at: <ref
                  target="http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html"
                  >http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html</ref>.</bibl>

            <bibl xml:id="enderle_2016" label="Enderle 2016">Enderle, J.S. (2016) <title
                  rend="quotes">A plot of Brownian noise</title>, <title rend="italic">Jonathan
                  Scott Enderle</title> (blog). July 18. Available at: <ref
                  target="https://github.com/senderle/svd-noise/blob/main/Noise.ipynb"
                  >https://github.com/senderle/svd-noise/blob/main/Noise.ipynb</ref>.</bibl>

            <bibl xml:id="ford_2015" label="Ford 2015">Ford, P. (2015) <title rend="quotes">What is
                  code?</title>, <title rend="italic">Businessweek</title>, June 11. Available at:
                  <ref target="http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/"
                  >http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/</ref>.</bibl>

            <bibl xml:id="gabler_1995" label="Gabler 1995">Gabler, H.W. (1995) <title rend="quotes"
                  >Introduction: Textual criticism and theory in modern German editing</title>, in
               H.W. Gabler, G. Bornstein, and G.B. Pierce (eds.) <title rend="italic">Contemporary
                  German Editorial Theory</title>, pp. 1–16. Ann Arbor: University of Michigan
               Press. Available at: <ref
                  target="https://www.academia.edu/856505/Textual_Criticism_and_Theory_in_Modern_German_Editing"
                  >https://www.academia.edu/856505/Textual_Criticism_and_Theory_in_Modern_German_Editing</ref>.</bibl>

            <bibl xml:id="galey_2010" label="Galey 2010">Galey, A. (2010) <title rend="quotes">The
                  human presence in digital artifacts</title>, in W. McCarty (ed.) <title
                  rend="italic">Text and Genre in Reconstruction: Effects of Digitalization on
                  Ideas, Behaviors, Products and Institutions</title>, pp. 93–118. Cambridge (UK):
               Open Book Publishers. Available at: <ref
                  target="http://individual.utoronto.ca/alangaley/files/publications/Galey_Human.pdf"
                  >http://individual.utoronto.ca/alangaley/files/publications/Galey_Human.pdf</ref>.</bibl>

            <bibl xml:id="gitelman_2013" label="Gitelman 2013">Gitelman, L. (ed.) (2013) <title
                  rend="italic">"Raw Data" Is an Oxymoron</title>. Cambridge: The MIT Press.</bibl>

            <bibl xml:id="goldfarb_1996" label="Goldfarb 1996">Goldfarb, C.F. (1996) <title
               rend="quotes">The roots of SGML — A personal recollection</title>. Available at:
                  <ref target="http://www.sgmlsource.com/history/roots.htm"
                  >http://www.sgmlsource.com/history/roots.htm</ref>.</bibl>

            <bibl xml:id="hiller_2015" label="Hiller 2015">Hiller, M. (2015) <title rend="quotes"
                  >Signs o' the times: The software of philology and a philology of
               software</title>, <title rend="italic">Digital Culture and Society</title>, 1(1), pp.
               152–63. Available at: <ref target="https://doi.org/10.14361/dcs-2015-0110"
                  >https://doi.org/10.14361/dcs-2015-0110</ref>.</bibl>

            <bibl xml:id="hinsen_2017" label="Hinsen 2017">Hinsen, K. (2017) <title rend="quotes"
                  >Sustainable software and reproducible research: Dealing with software
                  collapse</title>, <title rend="italic">Konrad Hinsen's Blog</title> (blog).
               January 13. Available at: <ref
                  target="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"
                  >http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/</ref>.</bibl>

            <bibl xml:id="jockers_2013" label="Jockers 2013">Jockers, M.L. (2013) <title
                  rend="italic">Macroanalysis: Digital Methods and Literary History</title>.
               Urabana, Chicago, Springfield: UI Press.</bibl>

            <bibl xml:id="jockers_2014" label="Jockers 2014">Jockers, M.L. (2014) <title
                  rend="quotes">A novel method for detecting plot</title>, <title rend="italic"
                  >Matthew L. Jockers</title> (blog). June 5. Available at: <ref
                  target="http://www.matthewjockers.net/2014/06/05/a-novel-method-for-detecting-plot/#note1"
                  >http://www.matthewjockers.net/2014/06/05/a-novel-method-for-detecting-plot/#note1</ref>.</bibl>

            <bibl xml:id="jockers_2015" label="Jockers 2015">Jockers, M.L. (2015) <title
                  rend="quotes">Revealing sentiment and plot arcs with the Syuzhet package</title>,
                  <title rend="italic">Matthew L. Jockers</title> (blog). February 2. Available at:
                  <ref target="http://www.matthewjockers.net/2015/02/02/syuzhet/"
                  >http://www.matthewjockers.net/2015/02/02/syuzhet/</ref>.</bibl>

            <bibl xml:id="jones_2014" label="Jones 2014">Jones, S.E. (2014) <title rend="italic">The
                  Emergence of the Digital Humanities</title>. New York, London: Routledge.</bibl>

            <bibl xml:id="jones_2016" label="Jones 2016">Jones, S.E. (2016) <title rend="italic"
                  >Roberto Busa, S.J., and the Emergence of Humanities Computing: The Priest and the
                  Punched Cards</title>. New York, London: Routledge, Taylor &amp; Francis
               Group.</bibl>

            <bibl xml:id="kay_1993" label="Kay 1993">Kay, A.C. (1993) <title rend="quotes">The early
                  history of Smalltalk</title>, <title rend="italic">ACM SIGPLAN Notices</title>,
               28(3), pp. 69–95. Available at: <ref target="https://doi.org/10.1145/155360.155364"
                  >https://doi.org/10.1145/155360.155364</ref>.</bibl>

            <bibl xml:id="kestemont_2015" label="Kestemont et al. 2015">Kestemont, M., Moens, S.,
               and Deploige, J. (2015) <title rend="quotes">Collaborative authorship in the twelfth
                  century: A stylometric study of Hildegard of Bingen and Guibert of
                  Gembloux</title>, <title rend="italic">Literary and Linguistic Computing</title>,
               30(2), pp. 199–224. Available at: <ref target="https://doi.org/10.1093/LLC/FQT063"
                  >https://doi.org/10.1093/LLC/FQT063</ref>.</bibl>

            <bibl xml:id="kestemont_2017" label="Kestemont et al. 2017">Kestemont, M., de Pauw, G.,
               van Nie, R., and Daelemans, W. (2017) <title rend="quotes">Lemmatization for
                  variation-rich languages using deep learning</title>, <title rend="italic">Digital
                  Scholarship in the Humanities</title>, 32(4), pp. 797–815. Available at: <ref
                  target="https://doi.org/10.1093/llc/fqw034"
                  >https://doi.org/10.1093/llc/fqw034</ref>.</bibl>

            <bibl xml:id="kirschenbaum_2008" label="Kirschenbaum 2008">Kirschenbaum, M. (2008)
                  <title rend="italic">Mechanisms: New Media and the Forensic Imagination</title>.
               Cambridge (Massachusetts), London (England): The MIT Press.</bibl>

            <bibl xml:id="kittler_1993" label="Kittler 1993">Kittler, F. (1993) <title rend="quotes"
                  >Es gibt keine Software</title>, in <title rend="italic">Draculas
                  Vermächtmis</title>, pp. 225–42. Leipzig: Reclam Verlag.</bibl>

            <bibl xml:id="knuth_1984" label="Knuth 1984">Knuth, D.E. (1984) <title rend="quotes"
                  >Literate programming</title>, <title rend="italic">The Computer Journal</title>,
               27(1), pp. 97–111. Available at: <ref target="https://doi.org/10.1093/comjnl/27.2.97"
                  >https://doi.org/10.1093/comjnl/27.2.97</ref>.</bibl>

            <bibl xml:id="lahti_2020" label="Lahti et al. 2020">Lahti, L., Mäkelä, E., and Tolonen,
               M. (2020) <title rend="quotes">Quantifying bias and uncertainty in historical data
                  collections with probabilistic programming</title>, in <title rend="italic"
                  >Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)</title>, pp. 280-289. Amsterdam: CHR. Available at: <ref
                  target="http://ceur-ws.org/Vol-2723/short46.pdf"
                  >http://ceur-ws.org/Vol-2723/short46.pdf</ref>.</bibl>

            <bibl xml:id="latour_1993" label="Latour 1993">Latour, B. (1993) <title rend="italic">We
                  Have Never Been Modern</title>. Translated by C. Porter. Cambridge, Massachusetts:
               Harvard University Press.</bibl>

            <bibl xml:id="manovich_2013" label="Manovich 2013">Manovich, L. (2013) <title
                  rend="italic">Software Takes Command</title>. Vol. 5. International Texts in
               Critical Media Aesthestics. New York, London, New Delhi etc.: Bloomsbury
               Academic.</bibl>

            <bibl xml:id="marino_2006" label="Marino 2006">Marino, M.C. (2006) <title rend="quotes"
                  >Critical code studies</title>, <title rend="italic">Electronic Book
                  Review</title>, December. Available at: <ref
                  target="http://www.electronicbookreview.com/thread/electropoetics/codology"
                  >http://www.electronicbookreview.com/thread/electropoetics/codology</ref>.</bibl>

            <bibl xml:id="mcpherson_2012" label="McPherson 2012">McPherson, T. (2012) <title
                  rend="quotes">Why are the digital humanities so white? Or thinking the histories
                  of race and computation</title>, in M.K. Gold (ed.) <title rend="italic">Debates
                  in the Digital Humanities</title>, pp. 139–60. Minneapolis: University of
               Minnesota Press. Available at: <ref
                  target="http://dhdebates.gc.cuny.edu/debates/text/29"
                  >http://dhdebates.gc.cuny.edu/debates/text/29</ref>.</bibl>

            <bibl xml:id="metz_2016" label="Metz and Owen 2016">Metz, S., and Owen, K. (2016) <title
                  rend="italic">99 Bottles of OOP</title>. Potato Canyon Software, LLC.</bibl>

            <bibl xml:id="morozov_2013" label="Morozov 2013">Morozov, E. (2013) <title rend="italic"
                  >To Save Everything, Click Here: The Folly of Technological Solutionism</title>.
               New York: PublicAffairs.</bibl>

            <bibl xml:id="murray_2016" label="Murray 2016">Murray, J.H. (2016) <title rend="italic"
                  >Hamlet on the Holodeck: The Future of Narrative in Cyberspace</title>. Revised
               2016 edition, First published 1997. New York: The Free Press. Available at: <ref
                  target="https://lm03-17.digitalscenography.org/wp-content/uploads/2017/09/Hamlet-on-the-Holodeck.pdf"
                  >https://lm03-17.digitalscenography.org/wp-content/uploads/2017/09/Hamlet-on-the-Holodeck.pdf</ref>.</bibl>

            <bibl xml:id="nowviskie_2011" label="Nowviskie 2011">Nowviskie, B. (2011) <title
                  rend="quotes">Where credit is due: Preconditions for the evaluation of
                  collaborative digital scholarship</title>, <title rend="italic"
               >Profession</title>, 6, pp. 169–81. Available at: <ref
                  target="https://doi.org/10.1632/prof.2011.2011.1.169"
                  >https://doi.org/10.1632/prof.2011.2011.1.169</ref>.</bibl>

            <bibl xml:id="nyhan_2016" label="Nyhan and Flinn 2016">Nyhan, J., and Flinn, A. (2016)
                  <title rend="italic">Computation and the Humanities: Towards an Oral History of
                  Digital Humanities. Springer Series on Cultural Computing</title>. Cham (CH):
               Springer Open. Available at: <ref
                  target="http://www.springer.com/gp/book/9783319201696"
                  >http://www.springer.com/gp/book/9783319201696</ref>.</bibl>

            <bibl xml:id="perez_2015" label="Perez and Granger 2015">Perez, F., and Granger, B.E.
               (2015) <title rend="quotes">Project Jupyter: Computational narratives as the engine
                  of collaborative data science</title>, <title rend="italic">Project Jupyter:
                  Interactive Computing</title> (blog). July 7. Available at: <ref
                  target="http://blog.jupyter.org/2015/07/07/project-jupyter-computational-narratives-as-the-engine-of-collaborative-data-science/"
                  >http://blog.jupyter.org/2015/07/07/project-jupyter-computational-narratives-as-the-engine-of-collaborative-data-science/</ref>.</bibl>

            <bibl xml:id="piper_2015" label="Piper 2015">Piper, A. (2015) <title rend="quotes">Novel
                  devotions: Conversional reading, computational modeling, and the modern
                  novel</title>, <title rend="italic">New Literary History</title>, 46(1), pp.
               63–98. Available at: <ref target="https://doi.org/10.1353/nlh.2015.0008"
                  >https://doi.org/10.1353/nlh.2015.0008</ref>.</bibl>

            <bibl xml:id="presner_2012" label="Presner 2012">Presner, T. (2012) <title rend="quotes"
                  >How to evaluate digital scholarship</title>, <title rend="italic">Journal of
                  Digital Humanities</title>, 1(4). Available at: <ref
                  target="http://journalofdigitalhumanities.org/1-4/how-to-evaluate-digital-scholarship-by-todd-presner/"
                  >http://journalofdigitalhumanities.org/1-4/how-to-evaluate-digital-scholarship-by-todd-presner/</ref>.</bibl>

            <bibl xml:id="purdy_2010" label="Purdy and Walker 2010">Purdy, J.P., and Walker, J.R.
               (2010) <title rend="quotes">Valuing digital scholarship: Exploring the changing
                  realities of intellectual work</title>, <title rend="italic">Profession</title>,
               pp. 177–95.</bibl>

            <bibl xml:id="rockwell_2011" label="Rockwell 2011">Rockwell, G. (2011) <title
                  rend="quotes">On the evaluation of digital media as scholarship</title>, <title
                  rend="italic">Profession</title>, pp. 152–68. Available at: <ref
                  target="https://doi.org/prof.2011.2011.1.152"
                  >https://doi.org/prof.2011.2011.1.152</ref>.</bibl>

            <bibl xml:id="rybicki_2014" label="Rybicki et al. 2014">Rybicki, J., Hoover, D., and
               Kestemont, M. (2014) <title rend="quotes">Collaborative authorship: Conrad, Ford and
                  rolling delta</title>, <title rend="italic">Literary and Linguistic
                  Computing</title>, 29(3), pp. 422–31. Available at: <ref
                  target="https://doi.org/10.1093/llc/fqu016"
                  >https://doi.org/10.1093/llc/fqu016</ref>.</bibl>

            <bibl xml:id="sahle_2014" label="Sahle and Vogeler 2014">Sahle, P., and Vogeler, G.
               (2014) <title rend="quotes">Criteria for reviewing scholarly digital editions
                  (version 1.1)</title>, <title rend="italic">Institut Für Dokumentologie Und
                  Editorik</title>. June. Available at: <ref
                  target="http://www.i-d-e.de/publikationen/weitereschriften/criteria-version-1-1/"
                  >http://www.i-d-e.de/publikationen/weitereschriften/criteria-version-1-1/</ref>.</bibl>

            <bibl xml:id="senseney_2016" label="Senseney 2016">Senseney, M. (2016) <title
                  rend="quotes">Pace of change: A preliminary YesWorkflow case study</title>.
               Technical Report 201601–1. Urbana: University of Illinois at Urbana-Champaign.
               Available at: <ref
                  target="https://www.ideals.illinois.edu/bitstream/handle/2142/88856/Senseney_YW_PaceofChange_CaseStudy.pdf?sequence=2"
                  >https://www.ideals.illinois.edu/bitstream/handle/2142/88856/Senseney_YW_PaceofChange_CaseStudy.pdf?sequence=2</ref>.</bibl>

            <bibl xml:id="smithies_2012" label="Smithies 2012">Smithies, J. (2012) <title
                  rend="quotes">Evaluating scholarly digital outputs: The six layers
                  approach</title>, <title rend="italic">Journal of Digital Humanities</title>,
               1(4). Available at: <ref
                  target="http://journalofdigitalhumanities.org/1-4/evaluating-scholarly-digital-outputs-by-james-smithies/"
                  >http://journalofdigitalhumanities.org/1-4/evaluating-scholarly-digital-outputs-by-james-smithies/</ref>.</bibl>

            <bibl xml:id="underwood_2015" label="Underwood 2015">Underwood, T. (2015) <title
                  rend="quotes">Paceofchange: Initial release; Version used in article</title>.
               Zenodo, 31 December. Available at: <ref target="https://doi.org/10.5281/zenodo.44226"
                  >https://doi.org/10.5281/zenodo.44226</ref>.</bibl>

            <bibl xml:id="underwood_2018" label="Underwood 2018">Underwood, T. (2018) <title
                  rend="quotes">Paceofchange: Initial release; Version used in article</title>.
               Github Repository, last commit: 18 April. Available at: <ref
                  target="https://github.com/tedunderwood/paceofchange"
                  >https://github.com/tedunderwood/paceofchange</ref>.</bibl>

            <bibl xml:id="underwood_2014" label="Underwood 2014">Underwood, T. (2014) <title
                  rend="quotes">Understanding genre in a collection of a million volumes, interim
                  report</title>. INTERIM PERFORMANCE REPORT DIGITAL HUMANITIES START-UP GRANT,
               AWARD HD5178713. Urbana: University of Illinois, Urbana-Champaign. Available at: <ref
                  target="https://figshare.com/articles/Understanding_Genre_in_a_Collection_of_a_Million_Volumes_Interim_Report/1281251"
                  >https://figshare.com/articles/Understanding_Genre_in_a_Collection_of_a_Million_Volumes_Interim_Report/1281251</ref>.</bibl>
            
            <bibl xml:id="underwood_2013" label="Underwood 2013">Underwood, T. (2013)
               PLACEHOLDER</bibl>

            <bibl xml:id="underwood-sellers_2014" label="Underwood and Sellers 2014">Underwood, T.,
               and Sellers, J. (2014) <title rend="quotes">Page-level genre metadata for
                  English-language volumes in HathiTrust, 1700–1922</title>. Figshare. Available at:
                  <ref
                  target="https://figshare.com/articles/Page_Level_Genre_Metadata_for_English_Language_Volumes_in_HathiTrust_1700_1922/1279201"
                  >https://figshare.com/articles/Page_Level_Genre_Metadata_for_English_Language_Volumes_in_HathiTrust_1700_1922/1279201</ref>.</bibl>

            <bibl xml:id="underwood-sellers_2015" label="Underwood and Sellers 2015">Underwood, T.,
               and Sellers, J. (2015) <title rend="quotes">How quickly do literary standards
                  change?</title> Figshare. Available at: <ref
                  target="http://figshare.com/articles/How_Quickly_Do_Literary_Standards_Change_/1418394"
                  >http://figshare.com/articles/How_Quickly_Do_Literary_Standards_Change_/1418394</ref>.</bibl>

            <bibl xml:id="underwood-sellers_2016" label="Underwood and Sellers 2016a">Underwood,
               T., and Sellers, J. (2016) <title rend="quotes">The longue durée of literary
                  prestige</title>, <title rend="italic">Modern Language Quarterly</title>, 77(3),
               pp. 321–44. Available at: <ref target="https://doi.org/10.1215/00267929-3570634"
                  >https://doi.org/10.1215/00267929-3570634</ref>.</bibl>

            <bibl xml:id="van-dalen-oskam_2007" label="Van Dalen-Oskam and Van Zundert 2007">Van
               Dalen-Oskam, K., and Van Zundert, J.J. (2007) <title rend="quotes">Delta for Middle
                  Dutch: Author and copyist distinction in *Walewein*</title>, <title rend="italic"
                  >Literary and Linguistic Computing</title>, 22(3), pp. 345–62. Available at: <ref
                  target="https://doi.org/10.1093/llc/fqm012"
                  >https://doi.org/10.1093/llc/fqm012</ref>.</bibl>

            <bibl xml:id="van-zundert_2016" label="Van Zundert 2016">Van Zundert, J.J. (2016) <title
                  rend="quotes">Author, editor, engineer — Code &amp; the rewriting of authorship in
                  scholarly editing</title>, <title rend="italic">Interdisciplinary Science
                  Reviews</title>, 40(4), pp. 349–75. Available at: <ref
                  target="http://dx.doi.org/10.1080/03080188.2016.1165453"
                  >http://dx.doi.org/10.1080/03080188.2016.1165453</ref>.</bibl>

            <bibl xml:id="vee_2013" label="Vee 2013">Vee, A. (2013) <title rend="quotes"
                  >Understanding computer programming as a literacy</title>, <title rend="italic"
                  >Literacy in Composition Studies</title>, 1(2), pp. 42–64.</bibl>

            <bibl xml:id="vee_2017" label="Vee 2017">Vee, A. (2017) <title rend="italic">Coding
                  Literacy: How Computer Programming Is Changing Writing. Software Studies</title>.
               Cambridge, MA: The MIT Press.</bibl>

            <bibl xml:id="welch_2010" label="Welch 2010">Welch, A. (2010) <title rend="quotes">Text,
                  image, book: Reading William Blake — On reading Blake's 'Milton' in
                  particular</title>, <title rend="italic">Imagining William Blake</title> (blog).
               December 5. Available at: <ref
                  target="https://imaginingwilliamblake.wordpress.com/2010/12/05/blake-milton/"
                  >https://imaginingwilliamblake.wordpress.com/2010/12/05/blake-milton/</ref>.</bibl>
         </listBibl>
      </back>
   </text>
</TEI>
