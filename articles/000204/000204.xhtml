<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet"/>
      <link href="../../common/css/dhq_screen.css"
            media="screen"
            type="text/css"
            rel="stylesheet"/>
      <link href="../../common/css/dhq_print.css"
            media="print"
            type="text/css"
            rel="stylesheet"/>
      <style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style>
      <script src="../../common/js/javascriptLibrary.js" type="text/javascript"/>
   </head>
   <body>
      <div id="mainContent">
         <div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio"
              class="DHQarticle">
            <div id="pubInfo">Preview<br/>Volume 009 Number 1</div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div>
                     <a href="/dhq/preview/index.html">Preview</a>
                     | 
                    <a rel="external" href="/dhq/vol/9/1/000204.xml">XML</a>

| 
		   Discuss
			(<a href="/dhq/vol/9/1/000204/000204.html#disqus_thread"
                        data-disqus-identifier="000204">
				Comments
			</a>)
                </div>
               </form>
            </div>
            <div class="DHQheader">
               <h1 class="articleTitle">"By the People, For the People":
               Assessing the Value of Crowdsourced, User-Generated Metadata</h1>
               <div class="author">
                  <span style="color: grey">Christina Manzo</span> &lt;<a href="mailto:manzo_dot_christina_at_gmail_dot_com"
                     onclick="javascript:window.location.href='mailto:'+deobfuscate('manzo_dot_christina_at_gmail_dot_com'); return false;"
                     onkeypress="javascript:window.location.href='mailto:'+deobfuscate('manzo_dot_christina_at_gmail_dot_com'); return false;">manzo_dot_christina_at_gmail_dot_com</a>&gt;, Simmons College, USA</div>
               <div class="author">
                  <span style="color: grey">Geoff Kaufman</span> &lt;<a href="mailto:geoff_dot_kaufman_at_dartmouth_dot_edu"
                     onclick="javascript:window.location.href='mailto:'+deobfuscate('geoff_dot_kaufman_at_dartmouth_dot_edu'); return false;"
                     onkeypress="javascript:window.location.href='mailto:'+deobfuscate('geoff_dot_kaufman_at_dartmouth_dot_edu'); return false;">geoff_dot_kaufman_at_dartmouth_dot_edu</a>&gt;, Tiltfactor Laboratory, Dartmouth College, USA</div>
               <div class="author">
                  <span style="color: grey">Sukdith Punjasthitkul</span> &lt;<a href="mailto:sukdith_dot_punjasthitkul_at_dartmouth_dot_edu"
                     onclick="javascript:window.location.href='mailto:'+deobfuscate('sukdith_dot_punjasthitkul_at_dartmouth_dot_edu'); return false;"
                     onkeypress="javascript:window.location.href='mailto:'+deobfuscate('sukdith_dot_punjasthitkul_at_dartmouth_dot_edu'); return false;">sukdith_dot_punjasthitkul_at_dartmouth_dot_edu</a>&gt;, Tiltfactor Laboratory, Dartmouth College, USA</div>
               <div class="author">
                  <span style="color: grey">Mary Flanagan</span> &lt;<a href="mailto:mary_dot_flanagan_at_dartmouth_dot_edu"
                     onclick="javascript:window.location.href='mailto:'+deobfuscate('mary_dot_flanagan_at_dartmouth_dot_edu'); return false;"
                     onkeypress="javascript:window.location.href='mailto:'+deobfuscate('mary_dot_flanagan_at_dartmouth_dot_edu'); return false;">mary_dot_flanagan_at_dartmouth_dot_edu</a>&gt;, Tiltfactor Laboratory, Dartmouth College, USA</div>
               <span xmlns=""
                     class="Z3988"
                     title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=By%20the%20People,%20For%20the%20People%3A%20Assessing%20the%20Value%20of%20Crowdsourced,%20User-Generated%20Metadata&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=009&amp;rft.issue=1&amp;rft.aulast=Manzo&amp;rft.aufirst=Christina&amp;rft.au=Christina%20Manzo&amp;rft.au=Geoff%20Kaufman&amp;rft.au=Sukdith%20Punjasthitkul&amp;rft.au=Mary%20Flanagan"> </span>
            </div>
            <div id="DHQtext">
               <div id="abstract">
                  <h2>Abstract</h2>
                  <p>With the growing volume of user-generated classification systems arising from media
               tagging-based platforms (such as Flickr and Tumblr) and the advent of new
               crowdsourcing platforms for cultural heritage collections, determining the value and
               usability of crowdsourced, "folksonomic," or
               user-generated, "freely chosen keywords" [<a class="ref" href="#folksonomy">21st Century Lexicon</a>] for libraries, museums and other cultural heritage
               organizations becomes increasingly essential. The present study builds on prior work
               investigating the value and accuracy of folksonomies by: (1) demonstrating the
               benefit of user-generated "tags" - or unregulated keywords
               typically meant for personal organizational purposes - for facilitating item
               retrieval and (2) assessing the accuracy of descriptive metadata generated via a
               game-based crowdsourcing application. In this study, participants (N = 16) were first
               tasked with finding a set of five images using a search index containing either a
               combination of folksonomic and controlled vocabulary metadata or only controlled
               vocabulary metadata. Data analysis revealed that participants in the folksonomic and
               controlled vocabulary search condition were, on average, six times faster to search
               for each image (<em class="emph">M</em> = 25.08 secs) compared to participants searching with
               access only to controlled vocabulary metadata (<em class="emph">M</em> = 154.1 secs), and
               successfully retrieved significantly more items overall. Following this search task,
               all participants were asked to provide descriptive metadata for nine digital objects
               by playing three separate single-player tagging games. Analysis showed that 88% of
               participant-provided tags were judged to be accurate, and that both tagging patterns
               and accuracy levels did not significantly differ between groups of professional
               librarians and participants outside of the Library Science field. These findings
               illustrate the value of folksonomies for enhancing item
                  "findability," or the ease with which a patron can access
               materials, and the ability of librarians and general users alike to contribute valid,
               meaningful metadata. This could significantly impact the way libraries and other
               cultural heritage organizations conceptualize the tasks of searching and
               classification.</p>
               </div>
               <div class="div div0">
                  <h1 class="head">1. Introduction</h1>
                  <div class="counter">
                     <a href="#p1">1</a>
                  </div>
                  <div class="ptext" id="p1">Classification is a basic, integral and historically significant human function.
               Defined by Golder and Huberman as an act "through which meaning
                  emerges" [<a class="ref" href="#golder2006">Golder and Huberman 2006</a>, 200], the practice of classification represents one
               of the primary intellectual foundations of library and information sciences. Useful
               classification systems both accurately reflect the contents of a particular
               collection and allow for the effective and efficient retrieval of items or
               information. Given the subjective nature of human classification, examining meaning
               has no universal procedure. From the birth of the library science field until the
               late 1800s, during which the first edition of Dewey Decimal Classification was
               published in 1876, followed quickly by the Cutter Expansive Classification System and
               the introduction of Library of Congress Subject Headings [<a class="ref" href="#stone2000">Stone 2000</a>],
               all libraries and centers of information each used an independent, unstandardized
               systems of organization, otherwise know as "local
                  classification."</div>
                  <div class="counter">
                     <a href="#p2">2</a>
                  </div>
                  <div class="ptext" id="p2">With the rise of such classifications as Dewey, Cutter and LC Classification, local
               classification was mostly retired in the late 19th century in favor of a united
               system that would allow for understanding across all libraries. This was, of course,
               until the rise of Internet tagging-based platforms such as Flickr, Twitter, Tumblr,
               and Delicious presented a challenge to these standardized, centralized systems.
               Currently information professionals are facing an unprecedented amount of
               unstructured classification. Classification systems generated by such tagging-based
               platforms are referred to as "folksonomy," or "a type of classification system for online content, created by an
                  individual user who tags information with freely chosen keywords; also, the
                  cooperation of a group of people to create such a classification system" [<a class="ref" href="#folksonomy">21st Century Lexicon</a>].</div>
                  <div class="counter">
                     <a href="#p3">3</a>
                  </div>
                  <div class="ptext" id="p3">While folksonomies represented an increased diversity of classification, they were
               perceived mostly as sources of entertainment and documentations of casual
               colloquialisms rather than a formal system of documentation. However, in 2006 Golder
               and Huberman conducted a study of folksonomic data generated by users of the website
               Delicious demonstrating that user-provided tags not only formed in coherent groups,
               but also accurately described the basic elements of items that were tagged, such as
               the "Who,""What,""Where,""When,""Why," and "How" of items, proving their
               ability to supplement formal records. </div>
                  <div class="counter">
                     <a href="#p4">4</a>
                  </div>
                  <div class="ptext" id="p4">Golder and Huberman’s analysis also revealed two highly subjective categories that
               may diminish the potential value of crowdsourced metadata: "Self Reference"
               (i.e., any tag including the word "my," such as "mydocuments") and "Task
                  Organizing" (i.e., tags denoting actions such as "readlater" or
                  "sendtomom"). In examining the overall accuracy and reliability of tags,
               Golder and Huberman concluded: "Because stable patterns emerge in
                  tag proportions, minority opinions can coexist alongside extremely popular ones
                  without disrupting the nearly stable consensus choices made by many users" [<a class="ref" href="#golder2006">Golder and Huberman 2006</a>, 199]. </div>
                  <div class="counter">
                     <a href="#p5">5</a>
                  </div>
                  <div class="ptext" id="p5">This research was expanded upon in 2007, when Noll and Meinel compared tags from
               websites Delicious and Google against author-created metadata and found that the
               former provided a more accurate representation of items’ overall content. Bischoff et
               al. [2009] also examined folksonomic metadata within the context of the music
               industry and found that tags submitted by users at the website Last.fm were
               comparable with professional metadata being produced at AllMusic.com. Syn and Spring
               [2009] examined folksonomic classifications within the domain of academic publishing,
               and also found authoritative metadata to be lacking when compared with its
               user-generated counterparts. As stated by Noll and Mienel, "tags
                  provide additional information which is not directly contained within a document.
                  We therefore argue that integrating tagging information can help to improve or
                  augment document classification and retrieval techniques and is worth further
                  research." [<a class="ref" href="#noll2007">Noll and Meinel 2007</a>, 186]. Together, these studies indicate that
               folksonomies can easily and usefully be stored alongside classical, controlled
               vocabularies.</div>
                  <div class="counter">
                     <a href="#p6">6</a>
                  </div>
                  <div class="ptext" id="p6">While some may have reservations regarding the "mixing" of
               folksonomic and controlled vocabularies, these two classification systems need not be
               viewed as mutually exclusive. Both systems have inherent advantages as well as
               potential flaws. Controlled vocabularies are reliable and logically structured, but
               can be somewhat inaccessible to the casual user [<a class="ref" href="#maggio2009">Maggio et al. 2009</a>].
               Furthermore, they can be time-consuming and expensive to produce and maintain. For
               example, if one’s local library employs two catalogers at an average salary of
               $58,960/year [<a class="ref" href="#ALA2008">American Library Association - Allied Professional                Association 2008</a>], it sets aside $117,920 for implementing
               controlled vocabularies on a limited number of new collection items alone. In
               contrast, folksonomies represent a relatively quicker and more cost-effective
               alternative [<a class="ref" href="#syn2009">Syn and Spring 2009</a>], with greater public appeal and accessibility,
               as evidenced by their overwhelming usage on social media. Mai noted: "Folksonomies have come about in part in reaction to the perception
                  that many classificatory structures represent an outdated worldview, in part from
                  the belief that since there is no single purpose, goal or activity that unifies
                  the universe of knowledge" [<a class="ref" href="#mai2011">Mai 2011</a>, 115]. As many users have become accustomed to the level
               of service and interaction styles offered by current popular search engines,
               traditional searches are "unlikely to be very successful"
               and are becoming "less frequent as patrons’ behavior is shaped by
                  keyword search engines" [<a class="ref" href="#antell2008">Antell and Huang 2008</a>, 76]. As researcher Heather Hedden points out, if an
               artifact "[in today's culture] if it cannot be found, it may as
                  well not exist" [<a class="ref" href="#hedden2008">Hedden 2008</a>, 1]. However, their lack of centralization renders
               folksonomies prone to issues of potential data contamination, such as an unorganized,
               unstructured plurality of subjects and the likelihood of data duplication between
               users. Regardless, folksonomies and traditional systems of organizations may be used
               in tandem to address the shortcomings of their respective features, allowing for a
               more diverse and organized form of classification. </div>
                  <div class="counter">
                     <a href="#p7">7</a>
                  </div>
                  <div class="ptext" id="p7">Our research aimed to provide new empirical evidence supporting the value of
               folksonomies by: (1) directly testing the benefits of adding user-generated
               folksonomic metadata to a search index and (2) comparing the range and accuracy of
               tags produced by library and information science professionals and non-professional
               users. The three main questions guiding this work were:</div>
                  <div class="ptext">
                     <ul class="list">
                        <li class="item">RQ1: Will users exhibit reduced search times and greater hit rates when
                  retrieving images with a search index that includes folksonomic metadata
                  contributed by previous users? </li>
                        <li class="item">RQ2: Will general users and information science professionals differ in the
                  type and quality of metadata they provide in a free-form tagging game? </li>
                        <li class="item">RQ3: Will users only provide information useful to them (e.g., Self Reference
                  and Task Organization tags), or will they attempt to provide metadata that is
                  useful on a wider scale?</li>
                     </ul>
                  </div>
               </div>
               <div class="div div0">
                  <h1 class="head">2. Background and Overview of Present Research</h1>
                  <div class="counter">
                     <a href="#p8">8</a>
                  </div>
                  <div class="ptext" id="p8">The present research employed a hybrid form of usability testing utilizing the
               Metadata Games platform [<a class="ref"
                        href="http://www.metadatagames.org"
                        onclick="window.open('http://www.metadatagames.org'); return false">http://www.metadatagames.org</a>], an online, free and open-source suite of
               games supported by the National Endowment for the Humanities and developed by
               Dartmouth College’s Tiltfactor Laboratory. The Metadata Games Project, launched in
               January 2014, aims to use games to help augment digital records by collecting
               metadata on image, audio, and film/video artifacts through gameplay [<a class="ref" href="#flanagan2013">Flanagan et al. 2013</a>]. Current Metadata Games media content partners include
               Dartmouth College’s Rauner Special Collections Library, the British Library, the
               Boston Public Library, the Sterling and Francine Clark Art Institute Library, UCLA,
               and Clemson University’s Open Parks Network. Inspired by other successful
               crowdsourcing efforts, the designers endeavored to create a diverse suite of games
               that could enable the public to engage with cultural heritage institutions and their
               digital collections, invite them to contribute knowledge to those collections, and
               set the stage for users to create and discover new connections among material within
               and across collections. The Metadata Games platform currently includes a palette of
               games that cater to a variety of player interests, including both single-player and
               multi-player games, competitive and cooperative games, and real-time and turn-based
               games, available for browsers and/or mobile devices. Despite their variety, all the
               games in the suite are united by a common purpose: to allow players to access media
               items from a number of cultural heritage institutions’ collections and provide them
               with the opportunity to contribute new metadata, in the form of single-word or
               short-phrase tags, within the context of an immersive, enjoyable game experience. The
               end result is that institutions benefit from increased engagement from a variety of
               users and acquire a wider array of data about their media collections. </div>
                  <div class="counter">
                     <a href="#p9">9</a>
                  </div>
                  <div class="ptext" id="p9">This research represents a collaborative project between the first author, who chose
               to use the Metadata Games platform as the focus for an independent study project at
               the Simmons College of Library and Information Sciences, and the co-authors from
               Dartmouth College’s Tiltfactor Laboratory. To be clear, the goal of the reported
               study was not to provide a validation of the Metadata Games platform, but rather to
               study the value of folksonomic metadata more generally; that is, the focus of this
               research was on the data itself, and the tool employed was intended to be largely
               incidental and peripheral to the study’s aims. At the time, the Metadata Games
               project was one of the few open-source metadata gathering tools available for
               cultural heritage institutions.<a class="noteRef" href="#d4e270">[1]</a> Thus, while the reported study
               results <em class="emph">are</em> specific to datasets gathered by the Metadata Games
               platform, the conclusions drawn from this study are intended to be generalizable to
               any organization currently making use of services such as CrowdAsk, LibraryThing or
               Scripto or considering a crowdsourced metadata project.</div>
               </div>
               <div class="div div0">
                  <h1 class="head">3. Methods</h1>
                  <div class="counter">
                     <a href="#p10">10</a>
                  </div>
                  <div class="ptext" id="p10">According to Nielsen, the number of participants needed for a usability test to be
               considered statistically relevant is five [<a class="ref" href="#nielsen2012b">Nielsen 2012b</a>]; however,
               because of the additional collection of quantitative data in our hybrid study,
               sixteen individuals (eight men and eight women; six of whom were aged 18-24, eight
               aged 25-44, one aged 45-60, and one over 60 years of age) were recruited to
               participate individually in 30-40 minute sessions. In order to discern any
               differential patterns of results between librarians and non-librarians, and to
               separate out any potential advantage that users in the field of Library and
               Information Sciences might have with content search and metadata generation, a mixed
               sample (with nine participants recruited from LIS-related fields and seven from
               non-LIS-related fields) was used for the study. </div>
                  <div class="counter">
                     <a href="#p11">11</a>
                  </div>
                  <div class="ptext" id="p11">The study was divided into two main tasks. In the first task, participants were
               presented with physical facsimiles of five images from the Leslie Jones Collection of
               the Boston Public Library and instructed to retrieve these items using the Metadata
               Games search platform. The images presented to participants were divided into the
               following categories: Places, People (Recognizable), People (Unidentified or
               Unrecognizable), Events/Actions, Miscellaneous Formats (Posters, Drawings,
               Manuscripts etc.), as seen in <a href="#figure01">Figures 1-5</a> below. Upon
               being given each physical facsimile, participants were timed from the moment they
               entered their first search term until the moment they clicked on the correct digital
               item retrieved from the Metadata Games search platform. This practice was adapted to
               reflect the digital items that would most commonly exist in a typical
               humanities-based collection, (i.e., photographs, manuscripts, postcards, glass plate
               negatives and other miscellanea). This design mirrored the common everyday occurrence
               of patrons attempting to retrieve a specific media item that they have in mind when
               using a library search index. According to a 2013 PEW Research Study, 82% of people
               that used the library in the last 12 months did so looking for a specific book, DVD
               or other resource [<a class="ref" href="#pew2013">PEW Internet 2013</a>]. </div>
                  <div class="counter">
                     <a href="#p12">12</a>
                  </div>
                  <div class="ptext" id="p12">For this image search component of the study, participants were randomly assigned to
               one of two search index conditions: one with access to controlled vocabularies and
               folksonomic metadata (i.e., the "folksonomy condition") and the other with
               restricted access only to controlled vocabularies (i.e., the "controlled vocabulary
                  condition") [See <a href="#figure06">Figure 6</a> for a schematic
               representation of the study design]. Searches were conducted using two different,
               customized versions of the search index on the Metadata Games website [<a class="ref"
                        href="play.metadatagames.org/search"
                        onclick="window.open('play.metadatagames.org/search'); return false">play.metadatagames.org/search</a>]. The
               folksonomic metadata was generated by previous users of the Metadata Games platform,
               whereas the controlled vocabularies attached to the items were generated by Boston
               Public Library staff. The process of inputting the controlled vocabularies into both
               versions of the search index required some reformatting. For example, because the
               version of the search platform used in the study did not allow for special characters
               such as the dash "-" or the comma ",", terms such as "Boston Red Sox
                  (baseball team)" were imported as two individual tags: "Boston Red Sox"
               and "baseball team."</div>
                  <div id="figure01" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure01.png" rel="external">
                           <img src="resources/images/figure01.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 1. </div>Search Image 1; Category: Places.</div>
                  </div>
                  <div id="figure02" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure02.png" rel="external">
                           <img src="resources/images/figure02.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 2. </div>Search Image 2; Category: People (Recognizable).</div>
                  </div>
                  <div id="figure03" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure03.png" rel="external">
                           <img src="resources/images/figure03.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 3. </div>Search Image 3; Category: People (Unrecognizable).</div>
                  </div>
                  <div id="figure04" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure04.png" rel="external">
                           <img src="resources/images/figure04.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 4. </div>Search Image 4; Category: Events/Actions.</div>
                  </div>
                  <div id="figure05" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure05.png" rel="external">
                           <img src="resources/images/figure05.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 5. </div>Search Image 5; Category: Misc. Formats.</div>
                  </div>
                  <div id="figure06" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure06.png" rel="external">
                           <img src="resources/images/figure06.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 6. </div>Schematic representation of study design for Task 1 (Image Search).</div>
                  </div>
                  <div class="counter">
                     <a href="#p13">13</a>
                  </div>
                  <div class="ptext" id="p13">Additionally, the system returned exact matches only, which meant that if a
               participant searched for "sailboat" and the only term present in the system was
                  "sailboats," the search would be unsuccessful. This aspect of the study
               design was necessitated by the technical specifications and functionality of the
               version of the Metadata Games search index used in the study, rather than a strategic
               methodological choice. The frequency of preventable "exact match"
               retrieval failures is discussed below in Section 4.1.</div>
                  <div class="counter">
                     <a href="#p14">14</a>
                  </div>
                  <div class="ptext" id="p14">To further illustrate the differences between the two search index conditions,
               consider the case of a participant in each condition attempting to retrieve image 3
               (see <a href="#figure03">Figure 3</a> above). In the controlled vocabulary
               condition, the only search terms that would yield a successful retrieval were:
                  "harbors,""sailboats,""marblehead harbor" and "glass negatives." In contrast, in the folksonomy
               condition, a participant would successfully retrieve this item by entering any of the
               following search terms: "harbor,""sailboats,""water,""woman,""sail boats,""porch,""scenic,""view,""sail,""sun,""summer,""marblehead harbor,""boats,""dock,""harbors,""veranda,""balcony,""girl looking at boats,""marina,""glass negatives,""sailboats on water" and "yacht."</div>
                  <div class="counter">
                     <a href="#p15">15</a>
                  </div>
                  <div class="ptext" id="p15">Immediately following the image search task, participants were instructed to play
               three different single-player tagging games from the Metadata Games suite: <cite class="title italic">Zen Tag</cite>, <cite class="title italic">NexTag</cite>, and <cite class="title italic">Stupid Robot</cite>. In the "free-tagging" game
                  <cite class="title italic">Zen Tag</cite> (<a href="#figure07">Figure 7</a>),
               users are able to input as many tags as they wish for four separate images. <cite class="title italic">NexTag</cite> (<a href="#figure08">Figure 8</a>), uses the
               same game mechanic as <cite class="title italic">Zen Tag</cite>, but utilizes a more
               minimalist user interface and presents a more robust image to players. Finally, in
                  <cite class="title italic">Stupid Robot</cite> (<a href="#figure09">Figure
               9</a>), a novice robot asks users to help it learn new words by tagging images. The
               game presents one image to users and gives them two minutes to input tags, with the
               constraint that they may only enter one tag for each given word length (i.e., one
               four-letter word, one five-letter word, and so on). In playing a single session of
               each game, participants in the present study tagged the same nine images (four images
               each in <cite class="title italic">Zen Tag </cite>and <cite class="title italic">NexTag
               </cite>and one in <cite class="title italic">Stupid Robot</cite>). Tags from these three
               games were compiled and compared against the traditional metadata provided by staff
               from the Boston Public Library.</div>
                  <div id="figure07" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure07.jpeg" rel="external">
                           <img src="resources/images/figure07.jpeg" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 7. </div>Zen Tag.</div>
                  </div>
                  <div id="figure08" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure08.png" rel="external">
                           <img src="resources/images/figure08.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 8. </div>NexTag.</div>
                  </div>
                  <div id="figure09" class="figure">
                     <div class="ptext">
                        <a href="resources/images/figure09.png" rel="external">
                           <img src="resources/images/figure09.png" alt=""/>
                        </a>
                     </div>
                     <div class="caption">
                        <div class="label">Figure 9. </div>Stupid Robot.</div>
                  </div>
                  <div class="div div1">
                     <h2 class="head">3.1 Scoring</h2>
                     <div class="counter">
                        <a href="#p16">16</a>
                     </div>
                     <div class="ptext" id="p16">Tags were scored by the lead author using a revised version of the Voorbij and
                  Kipp scale used by Thomas et al. [<a class="ref" href="#thomas2009">Thomas et al. 2009</a>]. This scale, which
                  was chosen due to its overall similarity to the Library of Congress Subject
                  Headings hierarchy, includes the following categories for scoring the level of
                  correspondence between a folksonomic tag and a tag included in the controlled
                  vocabulary for the same item: <ol class="list">
                           <li class="item">Exact match to controlled vocabulary</li>
                           <li class="item">Synonyms</li>
                           <li class="item">Broader terms</li>
                           <li class="item">Narrower terms</li>
                           <li class="item">Related terms</li>
                           <li class="item">Terms with an undefined relationship</li>
                           <li class="item">Terms that were not related at all</li>
                        </ol>
                     </div>
                     <div class="counter">
                        <a href="#p17">17</a>
                     </div>
                     <div class="ptext" id="p17">A score of one was thus reserved for an exact match between a user-provided tag
                  and the professional metadata, including punctuation. To illustrate, consider the
                  sample image provided in <a href="#figure10">Figure 10</a> and the
                  corresponding professional and folksonomic metadata provided in <a href="#table01">Tables 1</a> and <a href="#table02">2</a> below. With
                  this example, the user-provided term "Hindenburg Airship" would not be deemed
                  an exact match because, as indicated in Table 1, the controlled vocabulary term
                  encloses "Airship" in parentheses. Scores three through five were based on
                  judgments made by the Library of Congress in their subject heading hierarchy. For
                  example, "dog" would represent a broader term of the controlled vocabulary
                  term "Golden retriever," whereas the tag "biology" would represent a
                  narrower term than the controlled vocabulary term "Science." We reserved
                     "related terms" (a score of 5) for tags referring to objects or concepts
                  that were represented in the image but not expressed in the professional metadata.
                  A score of six was only to be awarded if, after research, the conclusion was made
                  that the term was unrelated to the image or any of the terms included in the
                  controlled vocabulary. A score of seven, though rare, was reserved for useless
                     "junk" tags, such as any term that was profane, explicit,
                  nonsensical or anything that would not qualify as useful to libraries (e.g.,
                  anything under the "Self Reference" or "Task Organization" classes
                  mentioned previously). </div>
                     <div id="figure10" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure10.png" rel="external">
                              <img src="resources/images/figure10.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 10. </div>Hindenburg explodes. (Leslie Jones Collection; Boston Public Library)</div>
                     </div>
                     <div id="table01" class="table">
                        <table class="table">
                           <tr class="row">
                              <td valign="top" class="cell">Airships</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Explosions</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Aircraft Accidents</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Hindenburg (Airship)</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">1934-1956 (approx.)</td>
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>Professional Metadata for Figure 9.</div>
                     </div>
                     <div id="table02" class="table">
                        <table class="table">
                           <tr class="row">
                              <td valign="top" class="cell label">Folksonomic Metadata</td>
                              <td valign="top" class="cell label">Score (Voorbij and Kipp Scale)</td>
                              <td valign="top" class="cell label">Notes</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Hindenburg (Airship)</td>
                              <td valign="top" class="cell">1</td>
                              <td valign="top" class="cell">exact match to "Hindenburg (Airship)"</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Hindenburg</td>
                              <td valign="top" class="cell">2</td>
                              <td valign="top" class="cell">synonym for "Hindenburg (Airship)"</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Accidents</td>
                              <td valign="top" class="cell">3</td>
                              <td valign="top" class="cell">broader term of "Aircraft accidents"</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Zeppelin</td>
                              <td valign="top" class="cell">4</td>
                              <td valign="top" class="cell">narrower term of "Airships"</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Flames</td>
                              <td valign="top" class="cell">5</td>
                              <td valign="top" class="cell">Present in photograph; related to "Explosions"</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">Painting</td>
                              <td valign="top" class="cell">6</td>
                              <td valign="top" class="cell">this is a photograph</td>
                           </tr>
                           <tr class="row">
                              <td valign="top" class="cell">omgreadlater</td>
                              <td valign="top" class="cell">7</td>
                              <td valign="top" class="cell">junk tag</td>
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 2. </div>Folksonomic Metadata and Scores from Voorbij and Kipp Scale for Figure
                     9.</div>
                     </div>
                  </div>
               </div>
               <div class="div div0">
                  <h1 class="head">4. Findings and Analysis</h1>
                  <div class="div div1">
                     <h2 class="head">4.1 Findability and Searching</h2>
                     <div class="counter">
                        <a href="#p18">18</a>
                     </div>
                     <div class="ptext" id="p18">
                        <span class="hi bold">
                           <em class="emph">Search Time.</em>
                        </span> On average, participants in the
                  controlled vocabulary index condition took six times longer to search for each
                  image (<em class="emph">M </em>= 154.1 secs, <em class="emph">SD </em>= 98.84) compared to
                  participants in the combined index condition (<em class="emph">M </em>= 25.08 secs,
                     <em class="emph">SD </em>= 19.39) (see <a href="#figure11">Figure 11</a> below). A
                  one-way Analysis of Variance (ANOVA) confirmed that this difference was a
                  statistically significant one, <em class="emph">F </em>(3, 15) = 3.94, <em class="emph">p </em>&lt;
                  .04. However, it is important to note that participants were free to ask to move
                  on at their discretion if they no longer wished to continue searching for an item.
                  Focusing exclusively on the search times associated with <em class="emph">successfully</em>
                  retrieved items in both conditions, an ANOVA confirmed that participants in the
                  controlled vocabulary condition still exhibited a significantly longer search time
                  per item (M = 111.36, SD = 89.24) compared to participants in the folksonomy
                  condition (M = 19.94, SD = 9.04), F (1, 12) = 8.51, <em class="emph">p</em> &lt; .014.</div>
                     <div class="counter">
                        <a href="#p19">19</a>
                     </div>
                     <div class="ptext" id="p19">
                        <span class="hi bold">
                           <em class="emph">Items Found.</em>
                        </span>Because participants were allowed to "give up" on finding
                  any particular item, each participant was assigned a numeric score from 0 to 5 to
                  represent the number of items they <em class="emph">successfully</em> found. A one-way
                  ANOVA showed that the average number of "found items" was
                  significantly higher in the folksonomy index condition (<em class="emph">M </em>= 4.88,
                     <em class="emph">SD </em>= .35) than in the controlled vocabulary index condition
                     (<em class="emph">M </em>= 1.38, <em class="emph">SD </em>= 1.06), <em class="emph">F </em>(1, 15) =
                  78.40, <em class="emph">p </em>&lt; .001 (see <a href="#figure12">Figure 12</a>
                  below).</div>
                     <div id="figure11" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure11.png" rel="external">
                              <img src="resources/images/figure11.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 11. </div>Average Group Search Time (in secs).</div>
                     </div>
                     <div id="figure12" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure12.png" rel="external">
                              <img src="resources/images/figure12.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 12. </div>Average Items Found per group.</div>
                     </div>
                     <div class="counter">
                        <a href="#p20">20</a>
                     </div>
                     <div class="ptext" id="p20">When taking the failed searches of the controlled vocabulary condition into
                  account, there were a total of 131 completely "preventable"
                  failures overall. Importantly, a majority of these failures were due to the entry
                  of folksonomic synonyms for tags that were included in the controlled vocabulary
                  index, which clearly demonstrates the value of folksonomies for improving search
                  effectiveness and efficiency. Additionally, of the total 379 searches that
                  participants in the controlled vocabulary condition conducted, only a small
                  fraction of search failures (13 searches or 3.4%) were caused by the exact match
                  parameters set forth by the Metadata Games search system. Thus, the differential
                  levels of search time and item retrieval exhibited by participants in the two
                  search index conditions are primarily attributable to the specific tags that were
                  accessible in the two indexes – and not to the particular of the search system
                  used in the present study.</div>
                  </div>
                  <div class="div div1">
                     <h2 class="head">4.2 Tag Analysis</h2>
                     <div class="counter">
                        <a href="#p21">21</a>
                     </div>
                     <div class="ptext" id="p21">Recall that all tags generated by participants in the gameplay portion of the
                  study were coded using the Voorbij and Kipp scales; <a href="#figure13">Figure
                     13</a> (below) depicts the breakdown of scores assigned to the 811 tags
                  generated by the participants.</div>
                     <div id="figure13" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure13.png" rel="external">
                              <img src="resources/images/figure13.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 13. </div>Breakdown of overall Voorbij and Kipp scores.</div>
                     </div>
                     <div class="counter">
                        <a href="#p22">22</a>
                     </div>
                     <div class="ptext" id="p22">As shown above, a score of five ("related terms") accounts for the largest
                  segment of recorded tags, meaning that 50% of all of the tags entered were valid
                  classifications not included in traditional metadata. This implies a fundamental
                  semantic gap between traditional classification and folksonomies.</div>
                     <div id="figure14" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure14.png" rel="external">
                              <img src="resources/images/figure14.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 14. </div>Distribution of Voorbij and Kipp scores for tags generated by LIS
                     participants.</div>
                     </div>
                     <div id="figure15" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure15.png" rel="external">
                              <img src="resources/images/figure15.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 15. </div>Distribution of Voorbij and Kip scores for tags generated by non-LIS
                     participants.</div>
                     </div>
                     <div class="counter">
                        <a href="#p23">23</a>
                     </div>
                     <div class="ptext" id="p23">As illustrated in <a href="#figure14">Figures 14</a> and <a href="#figure15">15</a> (above), the distribution of Voorbij and Kipp
                  scores was constant and nearly identical between the LIS and non-LIS subsamples.
                  This suggests that, when given the same instructions, both librarians and
                  non-librarians can and do produce the same types of useful, accurate data.
                  Additionally, a score of seven, for a so-called "junk" tag, was
                  equally rare in both subsamples’ data. By comparing every participant’s percentage
                  of exact matches and synonyms versus undefined and unrelated terms (<a href="#figure16">Figure 16</a> below), it is clear that most participants
                  (88%) show an inclination towards folksonomic correctness. It is worth noting that
                  the two highest scoring participants were a horticulturalist between the ages of
                  18-25 (participant 16) and a librarian between the ages of 45-60 (participant 14). </div>
                     <div id="figure16" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure16.png" rel="external">
                              <img src="resources/images/figure16.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 16. </div>Score percentages for perfect matches and synonyms vs. undefined or
                     unrelated terms.</div>
                     </div>
                  </div>
                  <div class="div div1">
                     <h2 class="head">4.3 Best Subjects for Crowdsourcing</h2>
                     <div class="counter">
                        <a href="#p24">24</a>
                     </div>
                     <div class="ptext" id="p24">Another concern for cultural heritage institutions is determining what media
                  subjects might work best to collect new metadata through crowdsourcing. As
                  previously mentioned, the images that participants tagged in the present study
                  were divided into five key subject groups (see <a href="#figure17">Figure
                     17</a>). </div>
                     <div id="figure17" class="figure">
                        <div class="ptext">
                           <a href="resources/images/figure17.png" rel="external">
                              <img src="resources/images/figure17.png" alt=""/>
                           </a>
                        </div>
                        <div class="caption">
                           <div class="label">Figure 17. </div>Average Search Time per Image Type (in seconds)</div>
                     </div>
                     <div class="counter">
                        <a href="#p25">25</a>
                     </div>
                     <div class="ptext" id="p25">Results revealed that the images garnering the highest number of unique tags were
                  those that fell into the categories People (Unrecognizable) and Miscellaneous
                  Formats (in this case, a digitized newspaper). The image that generated the fewest
                  tags was Image 4, a picture of Thomas Edison, Harvey Firestone, and Henry Ford.
                  Few people recognized the inventors and many simply input tags such as "old
                     men," although it is important to note that several participants did express
                  some level of familiarity with the figures in the image (e.g., one participant
                  uttered, "I feel like I should know this."). These results suggest that the
                  best subjects for crowdsourced metadata might be media items that requires no
                  prior knowledge. For example, the unrecognizable person and the digitized
                  newspaper were some of the only instances in which the intent of the photograph
                  was either completely subjective (unrecognizable person) or objectively stated
                  (digitized newspaper). Many other images of famous historical figures and events
                  simply caused the participants to become frustrated with their own lack of
                  knowledge. In light of this fact, crowdsourcing platforms may be well-advised to
                  provide users with the tools to perform their own research about the content of
                  the media to fill in any gaps in knowledge or recollection that they experience.
                  This is a challenge that Metadata Games has begun to address, with the addition of
                  a Wikipedia search bar to encourage users to research what they do not know about
                  a particular media item. </div>
                  </div>
               </div>
               <div class="div div0">
                  <h1 class="head">5. Conclusions</h1>
                  <div class="counter">
                     <a href="#p26">26</a>
                  </div>
                  <div class="ptext" id="p26">As of now, there remains debate about the comparative value of traditional and
               folksonomic metadata as organizational systems for today’s information needs.
               Nonetheless, there is growing recognition of the fact that folksonomies offer
               libraries with an ideal return-on-investment scenario [<a class="ref" href="#syn2009">Syn and Spring 2009</a>] with
               minimum cost (much of which can be off-set by digital humanities grants), maximum
               output of data [<a class="ref" href="#bischoff2009">Bischoff et al. 2009</a>][<a class="ref" href="#noll2007">Noll and Meinel 2007</a>], as well as the chance to increase community engagement
               with their patrons. As the findings of the present study demonstrate, folksonomic
               metadata, when used in tandem with traditional metadata, increases findability,
               corrects preventable search failures, and is by and large accurate. Furthermore, the
               data suggest that given the same tagging conditions, librarians and non-librarians
               produce a surprisingly similar distribution of useful metadata. Collectively, these
               findings point to the potential to change the way we search for and organize our most
               treasured media. </div>
               </div>
               <div class="div div0">
                  <h1 class="head">Acknowledgements</h1>
                  <div class="counter">
                     <a href="#p27">27</a>
                  </div>
                  <div class="ptext" id="p27">The research team would like to thank all study participants, Tom Blake, Linda
               Gallant, the Boston Public Library, The Digital Public Library of America, Mary
               Wilkins Jordan, Jeremy Guillette, and the National Endowment for the Humanities. </div>
               </div>
            </div>
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e270">
                  <span class="noteRef">[1]</span> Other noteworthy examples include the
                  steve.museum, one of the first projects to crowdsource metadata in the cultural
                  heritage domain, which created an online web interface for registered users to
                  submit tags to a selection of works from participating museums [<a class="ref"
                     href="https://code.google.com/p/steve-museum/"
                     onclick="window.open('https://code.google.com/p/steve-museum/'); return false">https://code.google.com/p/steve-museum/</a>]. In addition, in 2008, the
                  Library of Congress (LoC) used the photo-sharing service Flickr to gather comments
                  and tags on 3,000 images [<a class="ref" href="#springer2008">Springer et al. 2008</a>]. From this project, the LoC
                  teamed with Flickr to form the Flickr Commons, a communal page for other cultural
                  heritage institutions with image collections [<a class="ref"
                     href="https://www.flickr.com/commons"
                     onclick="window.open('https://www.flickr.com/commons'); return false">https://www.flickr.com/commons</a>].</div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl">
                  <span class="ref" id="folksonomy"><!-- close -->21st Century Lexicon</span> folksonomy. (n.d.). <a class="ref"
                     href="dictionary.com"
                     onclick="window.open('dictionary.com'); return false">Dictionary.com</a>'s 21st Century Lexicon. Retrieved
               March 31, 2015, from <a class="ref"
                     href="dictionary.com"
                     onclick="window.open('dictionary.com'); return false">Dictionary.com</a> website: <a class="ref"
                     href="http://dictionary.reference.com/browse/folksonomy"
                     onclick="window.open('http://dictionary.reference.com/browse/folksonomy'); return false">http://dictionary.reference.com/browse/folksonomy</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="ALA2008"><!-- close -->American Library Association - Allied Professional                Association 2008</span> American Library Association - Allied Professional Association (2008). Salary Survey
               2008. Retrieved April 11, 2014 from <a class="ref"
                     href="http://www.ala.org/educationcareers/employment/salaries"
                     onclick="window.open('http://www.ala.org/educationcareers/employment/salaries'); return false">http://www.ala.org/educationcareers/employment/salaries</a>.</div>
               <div class="bibl">
                  <span class="ref" id="antell2008"><!-- close -->Antell and Huang 2008</span> Antell, K., and Huang, J.
               (2008). Subject Searching Success Transaction Logs, Patron Perceptions, and
               Implications for Library Instruction. <cite class="title italic">Reference and User
                  Services Quarterly 48, no. 1 (Fall 2008):</cite> 68-76. Academic Search Complete,
               EBSCOhost</div>
               <div class="bibl">
                  <span class="ref" id="bischoff2009"><!-- close -->Bischoff et al. 2009</span> Bischoff, K., Firan, C.S.,
               Paiu, R., Nejdl, W., Laurier, C. and Sordo, M. (2009). Music Mood and Theme
               Classification-a Hybrid Approach. In <cite class="title italic">Proceedings of the 10th
                  International Society for Music Information Retrieval Conference (ISMIR
                  2009),</cite> pages 657-662.</div>
               <div class="bibl">
                  <span class="ref" id="flanagan2013"><!-- close -->Flanagan et al. 2013</span> Flanagan M., Punjasthitkul, S.,
               Seidman, M., and Kaufman, G. (2013). Citizen Archivists at Play: Game Design for
               Gathering Metadata for Cultural Heritage Institutions. <cite class="title italic">Proceedings of DiGRA 2013, Atlanta, Georgia</cite>. Retrieved January 15, 2014
               from <a class="ref"
                     href="http://www.tiltfactor.org/wp-content/uploads2/tiltfactor_citizenArchivistsAtPlay_digra2013.pdf"
                     onclick="window.open('http://www.tiltfactor.org/wp-content/uploads2/tiltfactor_citizenArchivistsAtPlay_digra2013.pdf'); return false">http://www.tiltfactor.org/wp-content/uploads2/tiltfactor_citizenArchivistsAtPlay_digra2013.pdf</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="golder2006"><!-- close -->Golder and Huberman 2006</span> Golder, S., and Huberman, B.
               (2006). Usage patterns of collaborative tagging systems. <cite class="title italic">Journal
                  of Information Science, 32(2),</cite> 198–208.</div>
               <div class="bibl">
                  <span class="ref" id="grey2012"><!-- close -->Grey and Hurko 2012</span> Grey, A., and Hurko, C. (2012). So
               You Think You’re an Expert: Keyword Searching vs. Controlled Subject Headings. Codex:
                  <cite class="title italic">The Journal of the Louisiana Chapter of the ACRL,
                  1(4).</cite> Retrieved March 1, 2014 from <a class="ref"
                     href="http://journal.acrlla.org/index.php/codex/article/view/47"
                     onclick="window.open('http://journal.acrlla.org/index.php/codex/article/view/47'); return false">http://journal.acrlla.org/index.php/codex/article/view/47</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="gross2005"><!-- close -->Gross and Taylor 2005</span> Gross, T. and Taylor, A. (2005).
               What have we got to lose? The effect of controlled vocabulary on keyword searching
               results. <cite class="title italic">College &amp; Research Libraries, May 2005.</cite>
               Retrieved May 1, 2014 from <a class="ref"
                     href="http://crl.acrl.org/content/66/3/212.full.pdf+html"
                     onclick="window.open('http://crl.acrl.org/content/66/3/212.full.pdf+html'); return false">http://crl.acrl.org/content/66/3/212.full.pdf+html</a>. </div>
               <div class="bibl">
                  <span class="ref" id="hedden2008"><!-- close -->Hedden 2008</span> Hedden, H. (2008). How Semantic Tagging
               Increases Findability. <cite class="title italic">Econtent Magazine</cite>. Retrieved
               March 1, 2014 from <a class="ref"
                     href="http://www.hedden-information.com/SemanticTagging.pdf"
                     onclick="window.open('http://www.hedden-information.com/SemanticTagging.pdf'); return false">http://www.hedden-information.com/SemanticTagging.pdf</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="heintz2010"><!-- close -->Heintz and Taraborelli 2010</span> Heintz, C., and
               Taraborelli, D. (2010). Editorial: Folk Epistemology. The Cognitive Bases of
               Epistemic Evaluation. <cite class="title italic">Review of Philosophy and Psychology,
                  1(4)</cite>. doi:10.1007/s13164-010-0046-8</div>
               <div class="bibl">
                  <span class="ref" id="heo-lian2011"><!-- close -->Heo-Lian Goh et al. 2011</span> Heo-Lian Goh, D., Ang, R.,
               Sian Lee, C., and Y.K. Chua, A. (2011). Fight or unite: Investigating game genres for
               image tagging. <cite class="title italic">Journal of the American Society for Library
                  Science and Technology, 62(7),</cite> 1311–1324. doi:10.1002/asi.21478</div>
               <div class="bibl">
                  <span class="ref" id="kipp2006"><!-- close -->Kipp 2006</span> Kipp, M. E. I. (2006). Complementary or
               Discrete Contexts in Online Indexing: A Comparison of User, Creator and Intermediary
               Keywords. <cite class="title italic">Canadian Journal of Information and Library Science,
                  29(4),</cite> 419–36.</div>
               <div class="bibl">
                  <span class="ref" id="lu2010"><!-- close -->Lu et al. 2010</span> Lu, C., Park, J., Hu, X., and Song, I.-Y.
               (2010). Metadata Effectiveness: A Comparison between User-Created Social Tags and
               Author-Provided Metadata (pp. 1–10). <cite class="title italic">IEEE</cite>.
               doi:10.1109/HICSS.2010.273</div>
               <div class="bibl">
                  <span class="ref" id="maggio2009"><!-- close -->Maggio et al. 2009</span> Maggio, L., Breshnahan, M., Flynn,
               D., Harzbecker, J., Blanchard, M., and Ginn, D. (2009). A case study: using social
               tagging to engage students in learning Medical Subject Headings. <cite class="title italic">Journal of the Medical Library Association, 97(2),</cite> 77.</div>
               <div class="bibl">
                  <span class="ref" id="mai2011"><!-- close -->Mai 2011</span> Mai, J.-E. (2011). Folksonomies and the New
               Order: Authority in the Digital Disorder. <cite class="title italic"> Knowledge
                  Organization, 38(2). </cite>Retrieved from <a class="ref"
                     href="http://jenserikmai.info/Papers/2011_folksonomies.pdf"
                     onclick="window.open('http://jenserikmai.info/Papers/2011_folksonomies.pdf'); return false">http://jenserikmai.info/Papers/2011_folksonomies.pdf</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="moltedo2012"><!-- close -->Moltedo et al. 2012</span> Moltedo, C., Astudillo, H., and
               Mendoza, M. (2012). Tagging tagged images: on the impact of existing annotations on
               image tagging (p. 3). <cite class="title italic">ACM Press.</cite>
               doi:10.1145/2390803.2390807</div>
               <div class="bibl">
                  <span class="ref" id="nielsen2012a"><!-- close -->Nielsen 2012a</span> Nielsen, J. (2012). Usability 101:
               Introduction to Usability. <cite class="title italic">Nielsen Norman Group</cite>.
               Retrieved December 15, 2014 from <a class="ref"
                     href="http://www.nngroup.com/articles/usability-101-introduction-to-usability/"
                     onclick="window.open('http://www.nngroup.com/articles/usability-101-introduction-to-usability/'); return false">http://www.nngroup.com/articles/usability-101-introduction-to-usability/</a>.</div>
               <div class="bibl">
                  <span class="ref" id="nielsen2012b"><!-- close -->Nielsen 2012b</span> Nielsen, J. (2012). How many test
               users in a usability study? <cite class="title italic">Nielsen Norman Group.</cite>
               Retrieved April 30, 2014 from <a class="ref"
                     href="http://www.nngroup.com/articles/how-many-test-users/"
                     onclick="window.open('http://www.nngroup.com/articles/how-many-test-users/'); return false">http://www.nngroup.com/articles/how-many-test-users/</a>. </div>
               <div class="bibl">
                  <span class="ref" id="noll2007"><!-- close -->Noll and Meinel 2007</span> M. G. Noll, C. Meinel (2007).
               Authors vs. Readers: A Comparative Study of Document Metadata and Content in the
               World Wide Web. <cite class="title italic">Proceedings of 7th International ACM Symposium
                  on Document Engineering, Winnipeg, Canada, August 2007,</cite> pp. 177-186, ISBN
               978-1-59593-776-6</div>
               <div class="bibl">
                  <span class="ref" id="oomen2011"><!-- close -->Oomen 2011</span> Oomen, J., and Aroyo, L. (2011).
               Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges. In
                  <cite class="title italic">Proceedings of the 5th International Conference on
                  Communities and Technologies</cite> (pp. 138–149). New York, NY, USA: ACM.
               doi:10.1145/2103354.2103373</div>
               <div class="bibl">
                  <span class="ref" id="pew2013"><!-- close -->PEW Internet 2013</span> PEW Internet (2013). Library Services
               in the Digital Age Part 2: What people do at libraries and library websites. PEW
               Internet. Retrieved January 11, 2015 from <a class="ref"
                     href="http://libraries.pewinternet.org/2013/01/22/part-2-what-people-do-at-libraries-and-library-websites/"
                     onclick="window.open('http://libraries.pewinternet.org/2013/01/22/part-2-what-people-do-at-libraries-and-library-websites/'); return false">http://libraries.pewinternet.org/2013/01/22/part-2-what-people-do-at-libraries-and-library-websites/</a>.</div>
               <div class="bibl">
                  <span class="ref" id="peters2009"><!-- close -->Peters 2009</span> Peters, I. (2009). Folksonomies: indexing
               and retrieval in Web 2.0. Berlin: De Gruyter/Saur. Retrieved March 1, 2014 from <a class="ref"
                     href="http://books.google.com/books?isbn=3598441851"
                     onclick="window.open('http://books.google.com/books?isbn=3598441851'); return false">http://books.google.com/books?isbn=3598441851</a>
               </div>
               <div class="bibl">
                  <span class="ref" id="porter2011"><!-- close -->Porter 2011</span> Porter, J. (2011). Folksonomies in the
               library: their impact on user experience, and their implications for the work of
               librarians. <cite class="title italic">The Australian Library Journal.</cite> Retrieved
               March 1, 2014 from <a class="ref"
                     href="http://www.tandfonline.com/toc/ualj20/current#.UxTDvPRdVLo"
                     onclick="window.open('http://www.tandfonline.com/toc/ualj20/current#.UxTDvPRdVLo'); return false">http://www.tandfonline.com/toc/ualj20/current#.UxTDvPRdVLo</a>.</div>
               <div class="bibl">
                  <span class="ref" id="springer2008"><!-- close -->Springer et al. 2008</span> Springer et al. (2008). For the
               Common Good: The Library of Congress Flickr Pilot Project Washington. <cite class="title italic">Library of Congress</cite>. Retrieved on January 11, 2015 from <a class="ref"
                     href="http://www.loc.gov/rr/print/flickr_report_final.pdf"
                     onclick="window.open('http://www.loc.gov/rr/print/flickr_report_final.pdf'); return false">http://www.loc.gov/rr/print/flickr_report_final.pdf</a>.</div>
               <div class="bibl">
                  <span class="ref" id="stone2000"><!-- close -->Stone 2000</span> Stone, A. (2000). The LCSH Century: A brief
               history of the Library of Congress Subject Headings, and introduction to centennial
               essays. <cite class="title italic">Cataloging &amp; Classification Quarterly
                  29(1-2).</cite> Retrieved December 23, 2014 from <a class="ref"
                     href="http://www.catalogingandclassificationquarterly.com/ccq29nr1-2ed.htm"
                     onclick="window.open('http://www.catalogingandclassificationquarterly.com/ccq29nr1-2ed.htm'); return false">http://www.catalogingandclassificationquarterly.com/ccq29nr1-2ed.htm</a>.</div>
               <div class="bibl">
                  <span class="ref" id="syn2009"><!-- close -->Syn and Spring 2009</span> Syn, S.Y. and Spring, M.B. (2009).
               Tags as Keywords – Comparison of the Relative Quality of Tags and Keywords, In <cite class="title italic">Proceedings of ASIS&amp;T Annual Meeting, 46(1).</cite> November
               6-11, 2009, Vancouver, BC, Canada.</div>
               <div class="bibl">
                  <span class="ref" id="syn2013"><!-- close -->Syn and Spring 2013</span> Syn, S.Y., and Spring, M.B. (2013).
               Finding subject terms for classificatory metadata from user-generated social tags.
                  <cite class="title italic">Journal of the American Society for Library Science and
                  Technology, 64(5),</cite> 964–980. doi:10.1002/asi.22804</div>
               <div class="bibl">
                  <span class="ref" id="thomas2009"><!-- close -->Thomas et al. 2009</span> Thomas, M., Caudle, D. M., and
               Schmitz, C. M. (2009). To tag or not to tag? <cite class="title italic">Library Hi Tech,
                  27(3), </cite>411–434. doi:10.1108/07378830910988540</div>
               <div class="bibl">
                  <span class="ref" id="usc"><!-- close -->U.S. Constitution</span> U.S. Constitution, pmbl. </div>
               <div class="bibl">
                  <span class="ref" id="voorbij1998"><!-- close -->Voorbij 1998</span> Voorbij, H. J. (1998). Title keywords
               and subject descriptors: a comparison of subject search entries of books in the
               humanities and social sciences. <cite class="title italic">Journal of Documentation,
                  54(4),</cite> 466–476. doi:10.1108/EUM0000000007178</div>
            </div>
            <div class="toolbar">
               <a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#"
                  onclick="javascript:window.print();"
                  title="Click for print friendly version">Print Article</a>
            </div>
         </div>
      </div>
   </body>
</html>
