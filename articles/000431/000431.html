<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>[PREVIEW] DHQ: Digital Humanities Quarterly: </title>
      <link href="../../common/css/dhq.css" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_screen.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../common/css/dhq_print.css" media="print" type="text/css" rel="stylesheet" /><style type="text/css">
        #mainContent {
          float: none;
          padding-top: 2em;
          padding-left: 4em;
          padding-right: 4em;
          margin-left: 225px;
           
        }</style></head>
   <body>
      <div id="mainContent">
         <div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" xmlns:m="http://www.w3.org/1998/Math/MathML" class="DHQarticle">
            <div id="pubInfo">Preview<br />Volume  Number 
            </div>
            <div class="toolbar">
               <form id="taporware" action="get">
                  <div><a href="//preview/index.html">Preview</a>
                      | 
                     <a rel="external" href="//vol///000431.xml">XML</a>
                     
                     | 
                     		   Discuss
                     			(<a href="/dhq/vol///000431/000431.html#disqus_thread" data-disqus-identifier="000431">
                        				Comments
                        			</a>)
                     
                  </div>
               </form>
            </div>
            
            <div class="DHQheader">
               
               
               
               
               <h1 class="articleTitle lang en">Manual Annotation of Unsupervised Models: Close
                  and Distant Reading of Politics on Reddit
               </h1>
               
               
               <div class="author"><span style="color: grey">first name(s) family
                     name</span></div>
               
               
               
               
               
               
               
               
               <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Manual%20Annotation%20of%20Unsupervised%20Models%3A%20Close%20and%20Distant%20Reading%20of%20Politics%20on%20Reddit&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=&amp;rft.volume=&amp;rft.issue=&amp;rft.aulast=family name&amp;rft.aufirst=first name(s)&amp;rft.au=first name(s)%20family name"> </span></div>
            
            
            <div id="DHQtext">
               
               
               <div id="abstract">
                  <h2>Abstract</h2>
                  
                  
                  <p>This article offers a methodological contribution to manually-assisted topic
                     modeling. With the availability of vast amounts of (online) texts, performing
                     full scale literary analysis using a close reading approach is not practically
                     feasible. The set of alternatives proposed by Franco Moretti (2000) under the
                     umbrella term of “distant reading” aims to show broad patterns that can be
                     found throughout the entire text collection. After a survey of literary-critical
                     practices that combine close and distant reading methods, we use manual
                     annotations of a thread on Reddit, both to evaluate an LDA model, and to provide
                     information that topic modeling lacks. We also make a case for applying these
                     reading techniques that originate in literary reading more broadly to online,
                     non-literary contexts. Given a large collection of posts from a Reddit thread,
                     we compare a manual, close reading analysis against an automatic, computational
                     distant reading approach based on topic modeling using LDA. For each text in the
                     collection, we label the contents, effectively clustering related texts. Next,
                     we evaluate the similarity of the respective outcomes of the two approaches. Our
                     results show that the computational content/topic-based labeling partially
                     overlaps with the manual annotation. However, the close reading approach not
                     only identifies texts with similar content, but also those with similar
                     function. The differences in annotation approaches require rethinking the
                     purpose of computational techniques in reading analysis. Thus, we present a
                     model that could be valuable for scholars who have a small amount of manual
                     annotation that could be used to tune an unsupervised model of a larger
                     dataset.
                  </p>
                  
               </div>
               
               
               
               
               
               <div class="div div0">
                  
                  <h1 class="head">Introduction</h1>
                  
                  <div class="counter"><a href="#p1">1</a></div>
                  <div class="ptext" id="p1">The management of ever-vaster amounts of information that bombard us daily is one
                     of the most important challenges we have been facing since the broad
                     availability of the Internet. Technological developments during this time have
                     drastically changed our abilities to access, process, and transfer information.
                     In particular, the popularity of Web 2.0, which places human interaction and
                     collaboration at its center, has led to massive amounts of available data.
                     Additionally, other types of data are made available, including the contents of
                     individual books that are integrated into bigger and bigger networks of texts,
                     usurped by the continuously expanding structures of online databases. Google, as
                     well as non-profit organizations such as Project Gutenberg, the Million Book
                     Project and the Internet Archive, carry out large-scale projects to scan and
                     upload the contents of whole libraries at a time. Kevin Kelly, co-founder of
                     <cite class="title italic">Wired</cite> magazine rejoices: “[o]nce books are digital, books
                     seep out of their bindings and weave themselves together. The collective
                     intelligence of a library allows us to see things we can’t see in a
                     single, isolated book”
                      <span class="error"><a class="ref" href="#kelly2012">#kelly2012</a></span>. So far, Google has scanned and made available over 30 million books. It
                     would take a human an estimated twenty thousand years to read such a vast
                     collection at the reasonable pace of two hundred words per minute, without
                     interruptions for food or sleep <span class="error"><a class="ref" href="#aiden2013">#aiden2013</a></span>.
                  </div>
                  
                  <div class="counter"><a href="#p2">2</a></div>
                  <div class="ptext" id="p2">Projects that make available great amounts of data, however, pose what Matthew
                     Wilkens calls a “problem of
                     abundance”: 
                     <blockquote>
                        <p>
                           We don’t read any faster than we
                           ever did, even as the quantity of text produced grows larger by the
                           year. If we need to read books in order to extract information from them
                           and if we need to have read things in common in order to talk about
                           them, we’re going to spend most of our time dealing with a relatively
                           small set of texts. … each of us reads only a truly minuscule fraction
                           of contemporary fiction (on the order of 0.1 percent, often much less).
                           … we need to decide what to ignore.
                            <span class="error"><a class="ref" href="#wilkens2012">#wilkens2012</a></span>
                           
                        </p>
                     </blockquote>
                  </div>
                  
                  <div class="counter"><a href="#p3">3</a></div>
                  <div class="ptext" id="p3">The field of Digital Humanities (DH) aims to offer methodological innovations to
                     solve this problem of abundance. Since 2000, many have followed Franco Moretti's
                     provocative call for distant reading. Moretti deemed close reading “a theological exercise” and urged
                     humanists to “read less”.
                     According to some big data theorists, the act of sampling is “an artifact of a period
                     of information scarcity, a product of the natural constraints on
                     interacting with information in an analog era”
                      <span class="error"><a class="ref" href="#mayerschonberger2013">#mayerschonberger2013</a></span>. With rich metadata and computational pattern matching, difficult texts
                     like <cite class="title italic">The Making of Americans</cite>, are now
                     <em class="emph">not-readable</em> in important new ways <span class="error"><a class="ref" href="#don2007">#don2007</a></span>
                     (see also <span class="error"><a class="ref" href="#kirschenbaum2007">#kirschenbaum2007</a></span>).
                  </div>
                  
                  <div class="counter"><a href="#p4">4</a></div>
                  <div class="ptext" id="p4">Els Stronks has opined we should make the gap between close and distant as wide
                     as possible by further developing distant reading techniques, and at the same
                     time making our close readings more precise and skillful in order to interpret
                     the results gained by computational analysis <span class="error"><a class="ref" href="#stronks2013">#stronks2013</a></span>. Juxtaposition of the two, however, is mostly a polemical issue
                     or rhetorical strategy. According to Jeremy Rosen (2011), Wilkens makes it seem
                     like we have to choose between methods. Distant reading is not a replacement but
                     a supplement or alternative to traditional close reading practices. On the
                     contrary, “[t]he availability of voluminous
                     electronic data, one might conclude, makes it even more necessary to
                     cultivate the faculty of analyzing data closely and critically”
                      <span class="error"><a class="ref" href="#rosen2011">#rosen2011</a></span>. Indeed, in his study (2013) of changes in the geographic imagination of
                     American fiction around the Civil War, Wilkens exclusively makes use of distant
                     reading techniques. Yet, close and distant reading are by no means antithetical
                     <span class="error"><a class="ref" href="#moretti2009">#moretti2009</a></span>
                     <span class="error"><a class="ref" href="#earheart2015">#earheart2015</a></span>.
                  </div>
                  
                  <div class="counter"><a href="#p5">5</a></div>
                  <div class="ptext" id="p5">Moretti's argument in “Slaughterhouse of Literature”
                     (2000) for instance, as one of the first applications of his distant reading
                     methods, relies entirely on manual annotation of 108 detective stories, read by
                     human beings in a graduate seminar (if not exactly “close
                     reading,” reading for clues involves at least skimming the text).
                     Many other works that are usually taken as representative of distant reading
                     rely extensively on manual annotation or close readings of illustrative literary
                     passages. So contrary to Moretti’s somewhat provocative proposition of distant
                     reading as alternative to close reading, his work typifies the importance of
                     human reading and tabulation. Yohei Igarashi <span class="error"><a class="ref" href="#igarashi2015">#igarashi2015</a></span> has
                     shown how the interrelatedness between close and non-close reading predates the
                     digital humanities, as it occurs at least since educational word lists of the
                     early twentieth century.
                  </div>
                  
                  <div class="counter"><a href="#p6">6</a></div>
                  <div class="ptext" id="p6">A brief survey of how different scales of analyses are connected in actual
                     literary-critical practice will show us that indeed, close and distant reading
                     were never mutually exclusive. In DH, this work has mostly been carried out in
                     literary history — see, for instance, a recent study on race, religion, and the
                     US novel <span class="error"><a class="ref" href="#so2019">#so2019</a></span>. Stephen Ramsay, in his book <cite class="title italic">Reading Machines</cite> (2011), offers different methods to
                     engage in what he calls “algorithmic criticism,”
                      <span class="error"><a class="ref" href="#ramsay2011">#ramsay2011</a></span> criticism derived from algorithmic manipulation of text. Ramsay holds
                     that we should not take close reading to be diametrically opposed to
                     computational, data-driven approaches, since there are important similarities
                     between the two. Both methods are interpretive, in the sense that they transform
                     the original text into something else. “The critic who endeavors to put
                     forth a ‘reading,’ puts forth not the text, but a new text in which the
                     data has been paraphrased, elaborated, selected, truncated, and
                     transduced”
                      <span class="error"><a class="ref" href="#jockers2011">#jockers2011</a></span>. But the same is true for a computer-driven distant reading where the
                     original text is converted to information by an algorithm. In a recent issue of
                     <cite class="title italic">PMLA</cite>, moreover, several scholars reflect on
                     ways to nuance Moretti’s statements on “not reading,”
                     effectively reducing the distance between the two textual approaches <span class="error"><a class="ref" href="#booth2017">#booth2017</a></span>
                     <span class="error"><a class="ref" href="#drucker2017">#drucker2017</a></span>
                     <span class="error"><a class="ref" href="#piper2017">#piper2017</a></span>.
                  </div>
                  
                  <div class="counter"><a href="#p7">7</a></div>
                  <div class="ptext" id="p7">For a good overview of papers combining close and distant reading from 2005-2015,
                     see <span class="error"><a class="ref" href="#janicke2015">#janicke2015</a></span>. They argue that most papers that combine the
                     two usually follow the “Information Seeking Mantra”: “Overview first, zoom &amp;
                     filter, details on demand”
                      <span class="error"><a class="ref" href="#shneidermann1996">#shneidermann1996</a></span>. The output of a distant reading is then an overview of the data that
                     highlights potentially interesting patterns for close reading. They call this
                     most common form of connecting scales of analysis “top-down”: first, a
                     distant view on the textual data is shown, and later the details come into view.
                     One can think of the reading of Christina Rossetti in Ted Underwood’s “The Longue Durée of Literary Prestige” (2016).
                     Underwood collected two samples of English-language poetry from 1820–1919: one
                     from volumes reviewed in prominent periodicals, and one of an obscure author,
                     randomly chosen from a large digital library. Looking at 360 reviewed and 360
                     random volumes, his study assessed the strength of the relationship between
                     poetic language and reception. The approach taken was to first look at broad
                     patterns, and then zoom in and read a few revealing passages. Likewise, in a
                     recent study of novelty in modernist texts <span class="error"><a class="ref" href="#mcgrath2018">#mcgrath2018</a></span>,
                     scholars first measure intratextual novelty and then use close reading of a
                     sample to test the measurements, scores, and graphs. Another <span class="error"><a class="ref" href="#lee2018">#lee2018</a></span> analyzes textual scale as a structuring principle of
                     geographical, spatial scale in studying the pre-modern world, by not-reading
                     tens of thousands of Renaissance books.
                  </div>
                  
                  <div class="counter"><a href="#p8">8</a></div>
                  <div class="ptext" id="p8">Stanford Lit Lab pamphlet 4 <span class="error"><a class="ref" href="#heuser2012">#heuser2012</a></span> traces macroscopic
                     changes in the British novel during the nineteenth century. It signals two
                     interrelated transformations in novelistic language during this period: a
                     systemic concretization of language and a change in the social spaces of the
                     novel. The authors research quantifiable features such as word usage, adopting a “dialogic approach that oscillates
                     between the historical and the semantic, between empirical word
                     frequencies that reveal the historical trends of words and semantic
                     taxonomies that help us identify the meaning and content of those
                     trends”
                      <span class="error"><a class="ref" href="#heuser2012">#heuser2012</a></span>. Through their “hypothesis-testing mode of
                     interpretation”
                      <span class="error"><a class="ref" href="#heuser2012">#heuser2012</a></span>, they make sure their results are semantically and culturally
                     interpretable. This way, they offer a possible answer to what Alan Liu has
                     called the “meaning problem” at the
                     heart of the digital humanities: to determine the relation between “quantitative interpretation and
                     humanly meaningful qualitative interpretation”
                      <span class="error"><a class="ref" href="#liu2013">#liu2013</a></span>. As a certain measure of sampling when researching literary history is
                     unavoidable, the issue of canon vesus archive is of import here <span class="error"><a class="ref" href="#algeehewitt2016">#algeehewitt2016</a></span>. In digital research, a researcher often works
                     according to a process that Jo Guldi <span class="error"><a class="ref" href="#guldi2018">#guldi2018</a></span> calls winnowing:
                     careful culling and fine-tuning of the algorithm to get rid of false positives
                     or messy data, towards cleaner data and clearer results. In that respect, it is
                     in fact not so far removed from more traditional approaches such as close
                     reading the single text.
                  </div>
                  
                  <div class="counter"><a href="#p9">9</a></div>
                  <div class="ptext" id="p9">Therefore, we follow this trend in mixing qualitative and quantitative methods
                     and using them to analyze corpora that solicit readings that zoom in and out
                     between part and whole. But rather than following the top-down approach or the
                     “Information Seeking Mantra,” in our mixed
                     method, manual annotation is not of a sample that follows from the overview
                     produced by the distant reading, but a method that comes before the LDA
                     analyses, to evaluate its workings and to reflect on its strengths and
                     weaknesses. 
                  </div>
                  
                  <div class="counter"><a href="#p10">10</a></div>
                  <div class="ptext" id="p10">We believe that this mostly literary-historical body of work since Moretti,
                     especially concerning the “great unread”
                      <span class="error"><a class="ref" href="#moretti2000a">#moretti2000a</a></span> has value when it comes to the current “information
                     overload” that we exemplify here using a case study from web
                     platform Reddit. Here, we propose a transfer from the literary to non-literary
                     informational contexts. What is interesting for our purpose here is not some
                     presumed binary between close and distant reading. It is, rather, the collective
                     recognition, since Moretti, that literary history is not a clearly demarcated,
                     well-mapped, and exhaustible field, but an “uncharted expanse”
                      <span class="error"><a class="ref" href="#moretti2000a">#moretti2000a</a></span> whose macroscopic shape we cannot fully know. This is true, albeit in a
                     different, non-historical sense, for online web forums. By using a mixed method
                     of manual annotation and LDA, we propose a means to confront this other,
                     contemporary version of the “great unread”
                      <span class="error"><a class="ref" href="#cohen1999">#cohen1999</a></span>. Other studies <span class="error"><a class="ref" href="#jockers2016">#jockers2016</a></span> rely and build on the
                     readings and value judgments of other professional readers to select their
                     corpus. In “testing” and “correcting”
                     previous gender and genre classifications of their peers, such studies have a
                     strong sense of tradition that studies of a contemporary phenomenon such as
                     Reddit threads obviously lack. 
                  </div>
                  
                  <div class="counter"><a href="#p11">11</a></div>
                  <div class="ptext" id="p11">As announced, we propose a strategy for using manual annotation to evaluate and
                     supplement an LDA model. Our analysis incorporates local annotation in a distant
                     reading. The investigation requires the development of computational tools
                     (e.g., topic identification, summarization) that deal with large amounts of
                     documents. This should provide large patterns in the dataset that enable a
                     fine-grained analysis of interesting parts of the data. Additionally, in-depth,
                     qualitative inspection of the performance of the computational analyses is
                     essential, in order to evaluate the computational approach. 
                  </div>
                  
                  <div class="counter"><a href="#p12">12</a></div>
                  <div class="ptext" id="p12">In this article, we investigate whether we can analyze a large document
                     collection from a distant reading perspective, indicating the different
                     semantics of the texts within the entire collection. This analysis is compared
                     against a manual, close reading analysis, to be able to evaluate the performance
                     of the computational approach. If the computational technique behaves similarly
                     to the manual approach, we can effectively use a distant reading technique to
                     complement the close reading analysis. Summarizing, our approach entails a
                     comparative analysis of the two reading methodologies: a close reading with
                     manual annotation on the one hand, and a distant reading with topic modeling on
                     the other. In order to explore both close and distant reading methods, we begin
                     by annotating the data both manually and computationally. This way, we can
                     reflect on and distillate the most valuable properties from both approaches. At
                     the same time we can evaluate the suitability of generic computational
                     techniques to high-quality manual analysis.
                  </div>
                  
                  <div class="counter"><a href="#p13">13</a></div>
                  <div class="ptext" id="p13">In order to explore the possible strategies that are located between the extremes
                     of distant and close reading, we focus on a selected discussion thread of the
                     popular online forum Reddit. Applying reading strategies from a literary studies
                     context to this non-literary environment provides us a valuable insight into the
                     merits of these strategies in an age of digital information. Furthermore, the
                     Reddit thread is large enough to identify higher level patterns and manageable
                     enough for manual analysis. Additionally, the posts within the thread, which
                     pose suitable units of information, are expected to have different semantic
                     content.
                  </div>
                  
                  <div class="counter"><a href="#p14">14</a></div>
                  <div class="ptext" id="p14">We will first give an overview of the close and distant reading methods employed
                     on a conceptual level, and then outline how we operationalized them in this
                     research. Following this, results of the comparison between the results found
                     using the two methods are presented and discussed in the context of efficiently
                     combining close and distant reading. This shows in how far we can use
                     computational techniques as a pre-processing phase to relieve us from a large
                     part of the labor-intensive work, enabling close reading of the interesting
                     parts of huge data collections, an informed choice based on the results of the
                     computational analysis. Lastly, we summarise the results and propose next
                     steps.
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Background</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Close Reading</h2>
                     
                     <div class="counter"><a href="#p15">15</a></div>
                     <div class="ptext" id="p15">Close reading is an umbrella term for an assortment of reading strategies
                        characterized by devout and detailed attention to the meaning and
                        composition of art works. The approach was made famous by the New Critics, a
                        group of Anglo-American literary scholars including Cleanth Brooks, William
                        K. Wimsatt, and Monroe C. Beardsley. Inspired by I.A. Richards (author of
                        <cite class="title italic">Practical Criticism</cite>, 1929), Matthew Arnold,
                        and T.S. Eliot, these scholars experienced their heyday of academic fame in
                        the forties and fifties of the last century. Going against contemporary
                        practices that, in their view, overvalued historical context and
                        biographical information, the New Critics suggested that literary scholars
                        should investigate the text itself. They wrote extensively on certain
                        contemporary fallacies of literary analysis, for instance, letting your own
                        emotions factor into the interpretation (the “affective fallacy”
                         <span class="error"><a class="ref" href="#wimsatt1949">#wimsatt1949</a></span>) or writing about authorial intentions (the “intentional fallacy”
                         <span class="error"><a class="ref" href="#wimsatt1946">#wimsatt1946</a></span>). Another practice they attacked was the paraphrasing of the contents
                        or message of a work (the “heresy of paraphrase”
                         <span class="error"><a class="ref" href="#brooks1947">#brooks1947</a></span>).<a class="noteRef" href="#d4e460">[1]</a> Instead, this school propagated the careful examination
                        of evidence offered by the text itself: images, symbols, and metaphors as
                        part of a larger structure that gives the text its unity and meaning. Of
                        particular interest to the close reader were devices that create
                        ambiguities, paradoxes, irony, and other forms of tension within the text.
                        Moving from close to distant reading, the level of analysis shifts from
                        details within single texts to categories of many texts.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Distant Reading</h2>
                     
                     <div class="counter"><a href="#p16">16</a></div>
                     <div class="ptext" id="p16">Distant reading is the practice of aggregating and processing information
                        about, or content in, large bodies of texts without the necessity of a human
                        reader to read these texts <span class="error"><a class="ref" href="#drucker2013">#drucker2013</a></span>. Distant reading
                        corresponds to quantifying, computational reading methods and was introduced
                        by Franco Moretti with the intention to identify the bigger picture in large
                        collections of textual data that close reading cannot uncover.
                        “Reading” is outsourced to a computer: it is in fact
                        a form of data mining that allows information in (e.g., subjects, places,
                        actors) or about (e.g., author, title, date, number of pages) the text to be
                        processed and analyzed. The latter are called metadata: data about the data.
                        Natural language processing can analyze the contents of
                        “practically unreadably” large corpora of texts,
                        while with data mining we can expose patterns or summarize on a scale that
                        is beyond human capacity. 
                     </div>
                     
                     <div class="counter"><a href="#p17">17</a></div>
                     <div class="ptext" id="p17">In his book <cite class="title italic">Distant Reading</cite> (2013) Franco
                        Moretti introduces the term polemically in explicit opposition to close
                        reading, which, to his mind, fails to uncover the true scope of literature.
                        Moretti is founder of the Stanford Literary Lab that seeks to confront
                        literary “problems” by scientific means – computational
                        modeling, hypothesis-testing, automatic text processing, algorithmic
                        criticism, and quantitative analysis. The Lab’s first pamphlet suggested
                        that literary genres “possess
                        distinctive features at every possible scale of analysis” and
                        that there are formal aspects of literature that people, unaided, cannot
                        detect <span class="error"><a class="ref" href="#allison2014">#allison2014</a></span>. The second pamphlet used
                        network theory to re-envision plots <span class="error"><a class="ref" href="#moretti2011">#moretti2011</a></span>. Since
                        then, Jockers (2013) has further developed distant reading in what he has
                        called “macroanalysis”, a
                        new approach to the literary reading and study designed for exploring
                        digital texts in large quantities. Using computational tools to retrieve
                        keywords, key phrases, and linguistic patterns across thousands of digital
                        texts in databases allows researchers to attain quantifiable evidence on how
                        literary trends have evolved over time and geographically. It can also
                        determine what social, cultural, and historical connections exist between
                        individual authors, texts, and genres <span class="error"><a class="ref" href="#jockers2013">#jockers2013</a></span>. In this
                        article, we seek to combine such a macro-scaled method with close reading
                        and manual annotation, and apply it to a non-literary dataset.
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Methodology</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Dataset</h2>
                     
                     <div class="counter"><a href="#p18">18</a></div>
                     <div class="ptext" id="p18">The text corpus used in the research described in this article comes from
                        Reddit, a social content aggregation website and the self-styled “front
                        page of the internet”. Functioning as “a bulletin of user-submitted
                        text, links, photos, and videos ”
                         <span class="error"><a class="ref" href="#duggan2013">#duggan2013</a></span>, it is a message board wherein users submit content and discuss this
                        content in different communities. The website is further referred to as a “social voting site”
                         <span class="error"><a class="ref" href="#gilbert2013">#gilbert2013</a></span> as users (often referred to as redditors) vote submitted content up
                        or down, sending the submissions with most upvotes to the Front page, i.e.,
                        the home page of reddit.com. Content on Reddit is organized in communities,
                        so called subreddits. The nearly 900,000 communities of Reddit are organized
                        around different topics like Technology, WorldNews, Music, Gaming, or
                        PoliticalDiscussion. A single subreddit can be reached via, for example, <span class="hi italic">reddit.com/r/PoliticalDiscussion</span>. The single posts in
                        one community are referred to as submissions. A submission usually contains
                        a link, embedded images, gifs, or videos and may also contain a text written
                        by the posting redditor. It is of special interest for textual analysis that
                        other redditors can comment on a submission and reply to other comments. The
                        comments are organized in a thread, a tree-like structure that allows for
                        following the discussion chronologically and with regard to content. From
                        any comment (every branch of the tree) further comments can emerge that yet
                        again may receive comments (as more branches sprouting from the prior
                        branch). Furthermore, redditors can make use of basic text formatting
                        functions, such as quoting text of prior comments or highlighting. The text
                        of the original submission and the comments on this submission are the
                        primary source of information of this research.
                     </div>
                     
                     <div class="counter"><a href="#p19">19</a></div>
                     <div class="ptext" id="p19">The thread that formed the basis of our dataset was submitted on January 19th
                        of 2017, and posed the following question: <span class="hi italic">Should the
                           Democrats nominate a celebrity in 2020? What would be the pros and
                           cons?</span><a class="noteRef" href="#d4e560">[2]</a>
                        Furthermore, the users added certain sub-questions, like: <span class="hi italic">Would celebrity power help or hurt a presidential nominee in the next
                           election?</span> and <span class="hi italic">If this plan actually goes forward,
                           who would be the best choice?</span>. Our dataset consists of 449 (461
                        including deleted comments) responses to these questions. Since the thread
                        is still open, comments have been added after we collected and investigated
                        our dataset. These new comments are not included in our research. 
                     </div>
                     
                     <div class="counter"><a href="#p20">20</a></div>
                     <div class="ptext" id="p20">While investigations using Reddit data often aim at uncovering trends across
                        single discussions and even across discussion forums (subreddits) (cf. <span class="error"><a class="ref" href="#zhang2017">#zhang2017</a></span>), the highest level of distant reading analysis in
                        this research is on the level of a single discussion thread. This restricts
                        the dataset to a size that can still be analysed by means of close reading
                        and also constrains the comments to relate, even if only peripherally, to
                        the one question or topic that initiated the discussion thread.
                     </div>
                     
                     <div class="counter"><a href="#p21">21</a></div>
                     <div class="ptext" id="p21">A common phenomenon in discussions in general is that the sub-questions
                        emerge and that the focus of the topics shifts to new content. This
                        illustrates that a technique is needed that can zoom in from the
                        whole-thread level to prevalent topics that group the single posts into
                        content categories. These single posts, on the other hand, can not only be
                        members of topical (content) groups, but also point towards groups related
                        to discourse function in Reddit discussions <span class="error"><a class="ref" href="#zhang2017">#zhang2017</a></span>. 
                     </div>
                     
                     <div class="counter"><a href="#p22">22</a></div>
                     <div class="ptext" id="p22">The hierarchical structure of Reddit discussions, ranging from single
                        comments to a whole thread, matches our goal to contrast low-level close
                        reading with high-level distant reading. For a scholarly reading of a
                        discussion forum, neither isolated comments nor a high level view on the
                        discussion thread as a whole are sufficient. Scholars need to know about
                        topical groupings that provide a frame of reference for single comments.
                        While grouping of comments can be achieved by manual labeling, a distant
                        reading approach is promising as a method that is faster, that in principle
                        scales up to larger discussion threads, and that may remove some human
                        biases during the annotation process.
                     </div>
                     
                     <div class="counter"><a href="#p23">23</a></div>
                     <div class="ptext" id="p23">Yet, the role of irony, emotion, and humor in this thread warrants a close
                        reading approach. The pervasiveness of irony and ironic detachment in
                        contemporary (online) culture has been described by Ian Bogost as the “escape from having to choose
                        between earnestness and disdain,”
                         <span class="error"><a class="ref" href="#bogost2016">#bogost2016</a></span>. Poe’s Law, an adage of Internet culture, states that online, it’s
                        impossible to know who’s joking and who’s being serious. In <cite class="title italic">The Ambivalent Internet</cite> (2017), Whitney Phillips
                        and Ryan Milner show how digital communications, e.g., through GIFs, memes,
                        and videos, are operationalized to fundamentally destabilize the worldviews
                        of others. It is almost impossible to determine when an ironic posture is
                        adopted. Humor, irony, and role playing are central to the behaviors in
                        online environments and communities like those on reddit. A manual
                        annotation is to be expected to detect more of this humor and irony than a
                        topic model. Therefore, we have chosen to call the level of human annotation
                        “close reading”, which traditionally attends to tone and style as
                        well as content of the message. 
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Close reading</h2>
                     
                     <div class="counter"><a href="#p24">24</a></div>
                     <div class="ptext" id="p24">While the initial question of the Reddit thread regards viewpoints on the
                        pros and cons of a future celebrity president and the names of potential
                        Democrat candidates, the thread soon developed into a more complex
                        conversation in which different “new” questions were
                        discussed as well. In order to grasp these associative developments inside
                        the discussion and the mechanics of a forum like Reddit, we choose to employ
                        a hypothesis-free form of close reading, which means we first started
                        looking for patterns in the material of the discussion in a rather
                        open-ended way, without explicitly framing our horizon of expectation, or
                        what we were expecting to find, beforehand. We chose this approach since our
                        aim was first and foremost to comparatively analyze methodologies, and only
                        secondarily, to find an answer to the question posed in the thread,
                        regarding celebrities in politics. 
                     </div>
                     
                     <div class="counter"><a href="#p25">25</a></div>
                     <div class="ptext" id="p25">This process took place in a bottom-up fashion: two human annotators analyzed
                        the posts in the thread, and chose a word or phrase to summarize each post.
                        During this process, they developed a collection of labels that were
                        assigned to the posts. Based on this reading, fifteen classes of posts were
                        identified and color-coded based on the classes. Ten of them turned out to
                        be related to three different underlying questions that we formulated based
                        on the classes. We describe these at more length under “report of the close reading analysis”. After manually annotating
                        the 449 posts separately, we compared the lists of the outcome and slightly
                        modified the categories to accommodate both our findings. Upon comparison,
                        we discovered there was a high degree of congruence between the categories
                        assigned to the posts in both annotations. 
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Distant reading</h2>
                     
                     <div class="counter"><a href="#p26">26</a></div>
                     <div class="ptext" id="p26">Parallel to the close reading based, manual annotations, we approach the
                        Reddit thread from a distant reading perspective. Specifically, we adopt a
                        data-driven ideal of distant reading, i.e. we want to start analysing
                        without any human insight into the textual dataset. This results in two
                        requirements on a distant reading method: First, the approach needs to be
                        unsupervised, i.e. not relying on any prior labeling of the data. Second,
                        the fact that Reddit threads are open-ended in the topics they comprise, the
                        number of topics in the shape of clusters resulting from the distant reading
                        algorithm needs to be variable. At least, there needs to be a possibility to
                        model a range of clusters in a computationally feasible manner. These two
                        requirements can be understood as equivalent to the hypothesis-free aspects
                        (of the close reading approach) in our distant reading method. 
                     </div>
                     
                     <div class="counter"><a href="#p27">27</a></div>
                     <div class="ptext" id="p27">One technique that fulfills both requirements is Latent Dirichlet Allocation
                        (LDA) topic modeling <span class="error"><a class="ref" href="#blei2003">#blei2003</a></span>. This method allows for the
                        automatic grouping of text documents according to latent content categories.
                        These topics underlying a text corpus are modelled in a completely
                        unsupervised manner, meaning that the algorithm does not know which texts
                        belong together (for example, according to some human labeling) beforehand.
                        This aspect of unsupervised LDA thus serves a data-driven ideal of distant
                        reading. For each topic, LDA creates a different language model, as the
                        underlying assumption is that different topics require different words and
                        constructions. In our research, we expect that LDA is suitable as texts with
                        different topics are expected to use a different
                        “language”.
                     </div>
                     
                     <div class="counter"><a href="#p28">28</a></div>
                     <div class="ptext" id="p28">Importantly, the number of topics resulting from LDA is a parameter that is
                        set manually by the user. While determining an adequate number of topics for
                        a dataset is often a problematic challenge for which no definite solutions
                        are agreed on, the variability of the resulting number of cluster matches
                        the second requirement to our desired distant reading technique. Because it
                        is unknown how many topics are to be expected from a discussion thread, a
                        pass through a range of numbers of LDA topics is necessary. Investigating a
                        whole range of numbers of topics may additionally reveal several different
                        levels of topical granularity that can be captured using LDA.
                     </div>
                     
                     <div class="counter"><a href="#p29">29</a></div>
                     <div class="ptext" id="p29">LDA has already been used for text analysis in the area of Digital
                        Humanities. For example, Emmery and van Zaanen (2015) investigated the
                        application of LDA to identify changes in the number of comments on news
                        articles on the topic of online security before and after the Snowden
                        revelation in 2013.
                     </div>
                     
                     <div class="counter"><a href="#p30">30</a></div>
                     <div class="ptext" id="p30">Lastly, we choose LDA as counterpart to close reading, since both approaches
                        focus on content of the texts (posts within the Reddit thread). We
                        strengthen this perspective by removing stop words (which are mostly
                        function words) from the textual data, before modeling topics with LDA.
                     </div>
                     
                     <div class="counter"><a href="#p31">31</a></div>
                     <div class="ptext" id="p31">As mentioned above, deciding on the optimal number of topics to be modeled is
                        problematic. In the case of a Reddit thread, new topics unfold as the
                        discussion proceeds, which means that the number of topics depends on the
                        size of the Reddit thread. It is therefore difficult to make an informed
                        decision about the number of topics for an LDA model, based on manual
                        inspection of comments alone. To resolve this problem, we propose a
                        possibility to determine an adequate number of topics for a set of already
                        present manual annotations. 
                     </div>
                     
                     <div class="counter"><a href="#p32">32</a></div>
                     <div class="ptext" id="p32">During the annotation process, each post (the original thread submission
                        question or a comment on it) in the Reddit thread is treated as a separate
                        document (<a href="#figure01">Figure 1</a>, left). For each document,
                        the LDA topic with the highest probability is selected. This is certainly a
                        limiting decision, but we find that the per-document probability
                        distributions usually strongly favor one single topic with a very high
                        probability, while the probabilities for the other topics are close to
                        zero.
                     </div>
                     
                  </div>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Comparison of the approaches</h2>
                     
                     <div class="counter"><a href="#p33">33</a></div>
                     <div class="ptext" id="p33">After analyzing the documents, they are thus represented by two labels, one
                        from two distinct sets of annotations each: a close reading annotation and a
                        (single) LDA topic. Based on this information, we now aim to identify LDA
                        topics and manual annotations that express similar concepts. Going through
                        the list of documents, co-occurrences of the two annotations are counted in
                        a matrix (<a href="#figure01">Figure 1</a>, right, LDA topics in
                        columns, manual annotation classes in rows).
                     </div>
                     
                     <div id="figure01" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure01.png" rel="external"><img src="resources/images/figure01.png" alt="" /></a></div>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 1. </div>Co-occurrences of manual annotation classes and LDA classes are
                           counted in a matrix. Only the most probable LDA class per document is
                           taken into account.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p34">34</a></div>
                     <div class="ptext" id="p34">Given the co-occurrence matrix, the aim is to find, for each LDA topic, the
                        manual annotation class that is best represented by the topic. In order to
                        do so, the matrix is passed columnwise, and the manual annotation class in
                        the row with the highest count is selected as corresponding best to the LDA
                        class of that column. At this point we introduce an optional step of
                        normalisation, applied on the co-occurrence matrix (<a href="#figure02">Figure 2</a>). Collecting absolute counts in the matrix may give an
                        unfair advantage to annotation tags with high class support. During
                        normalisation, the counts are divided by the class support of the manual
                        annotation class of this row, resulting in fractions instead of absolute
                        numbers. The steps described in the rest of this article always make use of
                        both the absolute counts and the normalised counts matrix separately.
                     </div>
                     
                     <div id="figure02" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure02.png" rel="external"><img src="resources/images/figure02.png" alt="" /></a></div>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 2. </div>Normalising co-occurrence counts, by dividing by manual class
                           support.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p35">35</a></div>
                     <div class="ptext" id="p35">For each manual/LDA pair of classes with highest co-occurrence, we generate a
                        mapping from LDA classes to manual annotation classes (<a href="#figure03">Figure 3</a>). The mapping expresses classes of
                        annotations that, from now on, we regard as corresponding to each other.
                        Note that, as in the example of Figure 3, not necessarily all
                        manual annotation classes are covered by the mapping.
                     </div>
                     
                     <div id="figure03" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure03.png" rel="external"><img src="resources/images/figure03.png" alt="" /></a></div>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 3. </div>Using the highest value per column in the co-occurrence matrix
                           (absolute counts shown), a mapping from LDA class to close reading class
                           is created.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p36">36</a></div>
                     <div class="ptext" id="p36">Using the mapping, the LDA classes are translated into manual annotation
                        classes (<a href="#figure04">Figure 4</a>). These can be regarded as
                        the computer’s “guess” of the manual class that was
                        assigned by a human. In the original list of documents, each document is now
                        represented by two annotations that are drawn from one <em class="emph">common</em>
                        pool of possible annotations. In machine learning terms, the manual
                        annotations are treated as gold standard (or true labels) and the mapped LDA
                        classes are regarded as predictions. To measure the extent to which the
                        predictions and the gold standard overlap, we calculate the accuracy.
                     </div>
                     
                     <div id="figure04" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure04.png" rel="external"><img src="resources/images/figure04.png" alt="" /></a></div>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 4. </div>Using the mapping, the per document LDA annotation is transformed to
                           manual classes.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p37">37</a></div>
                     <div class="ptext" id="p37">For a single topic model, we have now obtained a measure of overlap between
                        close reading based, manual annotations and distant reading LDA topics. In
                        order to find the optimal topic model, several topic models with varying
                        numbers of topics can be computed and the overlap accuracy can be compared.
                        For the present study, we produced topic models with a number of topics
                        ranging from 1 topic to <span class="hi italic">N</span> topics, with an <span class="hi italic">N</span> equal to the number of documents in the collection.
                        We expect a topic model with as many topics as are documents in the corpus
                        to be of low expressive power. Similarly, a topic model with only one LDA
                        class cannot adequately express several manual annotation classes. 
                     </div>
                     
                     <div class="counter"><a href="#p38">38</a></div>
                     <div class="ptext" id="p38">For the Reddit thread on the question <span class="hi italic">Should the Democrats
                           nominate a celebrity in 2020? What would be the pros and cons?</span>,
                        overlap accuracy is calculated for LDA models with 1 to 461 topics. The
                        topic model with the best fit to the manual annotations is defined as the
                        topic model corresponding to the highest accuracy value.
                     </div>
                     
                     <div class="counter"><a href="#p39">39</a></div>
                     <div class="ptext" id="p39">Note that increasing the number of LDA topics leads to a higher likelihood of
                        high accuracy. Imagine the situation in which each document receives a
                        unique LDA class. This allows for a perfect accuracy, but the predictive
                        power is extremely low as no document is comparable according to the LDA
                        classes. To resolve this issue, a second possibility to determine the
                        optimal number of topics is also investigated. Here, we reverse the mapping
                        step and transform manual classes to LDA classes. The steps described above
                        stay the same, except the co-occurrence matrix is passed row-wise instead of
                        column-wise<a class="noteRef" href="#d4e755">[3]</a>. 
                     </div>
                     
                     <div class="counter"><a href="#p40">40</a></div>
                     <div class="ptext" id="p40">As seen in the results, both the forward mapping and the reverse mapping
                        result in a perfect solution: Mapping LDA classes to manual classes leads to
                        increasingly high accuracy with an increasing number of topics. Mapping
                        manual classes to LDA classes, in turn, leads to complete overlap for a
                        single LDA class, because all manual classes are correctly mapped to that
                        single LDA class. The maximum accuracy values in the two scenarios are
                        misleading because neither having a single LDA topic nor having hundreds is
                        useful for the purpose of distant reading. In our approach we exploit this
                        phenomenon by aiming for the right balance between the two perfect
                        solutions. We do so by calculating the absolute difference between forward
                        and reverse accuracy for each number of topics. Subsequently, we select the
                        number of topics with the lowest absolute difference in the accuracies from
                        the forward and reverse mapping.
                     </div>
                     
                  </div>
                  
               </div>
               
               <div class="div div0">
                  
                  <h1 class="head">Results</h1>
                  
                  <div class="div div1">
                     
                     <h2 class="head">Report of the close reading analysis</h2>
                     
                     <div class="counter"><a href="#p41">41</a></div>
                     <div class="ptext" id="p41">Based on the close reading strategy described above, we identify fifteen
                        classes of posts (<a href="#table01">Table 1</a>). Ten of them are
                        related to three different underlying questions that we formulate based on
                        the classes: <span class="hi italic">What characteristics should a president have
                           in order to be a good leader?</span> (Red, class 1-3), <span class="hi italic">Which parties could influence the likeability of a potential
                           president?</span> (Green, class 4-5), <span class="hi italic">In which political
                           climate and context is the question discussed?</span> (Blue, class 6-10).
                        The other five are classified as “actions” that are divided over two
                        groups, hyperlinks to other sources (Yellow, class 11-12) and
                        (self-)referential comments/responses (Purple, class 13-15). 
                     </div>
                     
                     <div id="table01" class="table">
                        <table class="table">
                           <tr class="row">
                              
                              <td valign="top" class="cell"><a href="resources/images/table01.png" rel="external"><img src="resources/images/table01.png" alt="" /></a></td>
                              
                           </tr>
                        </table>
                        <div class="caption">
                           <div class="label">Table 1. </div>Close reading annotation classes.
                        </div>
                     </div>
                     
                     <div id="figure05" class="figure">
                        
                        
                        <div class="ptext"><a href="resources/images/figure05.png" rel="external"><img src="resources/images/figure05.png" alt="" /></a></div>
                        
                        
                        <div class="caption">
                           <div class="label">Figure 5. </div>Linear data-visualization of the Reddit thread.
                        </div>
                     </div>
                     
                     <div class="counter"><a href="#p42">42</a></div>
                     <div class="ptext" id="p42">By assigning colors to the classes that relate to the same question or
                        action, we can distinguish the variety of discussions inside one subreddit
                        and see how these discussions develop throughout the thread (Figure 5). From
                        left to right, the first 130 comments mostly discuss the political climate
                        (Blue) (e.g., <span class="hi italic">“Democrats didn’t show up. Republican
                           turnout fairly steady.”</span> (post 63)). These comments seem to create a
                        context for the discussion as a whole. Thereafter, the characteristics of an
                        ideal president are discussed (Red) (e.g., <span class="hi italic">“I say charisma
                           goes a long way, part of being president is to inspire people. Dems
                           should go young and the candidate should be well spoken (Obama) and
                           inspiring. Leave the dynasties, get someone younger, inspire people to
                           come out and vote for them.</span>” (post 120)). Near the end, the
                        influence of media and celebrity culture are set against the former
                        discussions (Green) (e.g., “<span class="hi italic">Do you think he would’ve won
                           the popular vote if not for that video</span>” (post 17)). The ‘actions’
                        of hyperlinking (Yellow), joking (e.g., “<span class="hi italic">We’ll get him a
                           box. Worked for Napoleon.”</span> (post 239)) and (emotional) responding
                        (e.g., <span class="hi italic">“I’ll concede that that’s an excellent point.”</span>
                        (post 57)) (Purple) occur equally frequent throughout the whole subreddit.
                        The variety of questions discussed in the subreddit does not seem to occur
                        randomly, but in clusters: We can see a certain development in how the
                        colors follow each other up.
                     </div>
                     
                  </div>
                  
               </div>
               
               
               
               
               
               
            </div>
            
            <div id="notes">
               <h2>Notes</h2>
               <div class="endnote" id="d4e460"><span class="noteRef lang en">[1] 
                     <span class="error"><a class="ref" href="#jockers2013">#jockers2013</a></span> Other seminal works to mention in
                     this respect are Cleanth Brooks and Robert Penn Warren’s <cite class="title italic">Understanding Poetry</cite> (1938) and Laurence
                     Perrine’s <cite class="title italic">Sound and Sense</cite> (1956), which
                     together come close to “an orthodoxy of close
                     reading”
                      <span class="error"><a class="ref" href="#culler2010">#culler2010</a></span>.</span></div>
               <div class="endnote" id="d4e560"><span class="noteRef lang en">[2] 
                     <a href="https://redd.it/5oy1sz" onclick="window.open('https://redd.it/5oy1sz'); return false" class="ref">https://redd.it/5oy1sz</a></span></div>
               <div class="endnote" id="d4e755"><span class="noteRef lang en">[3]  It is equally possible to simply transpose the input
                     matrix and leave all other steps the same.</span></div>
            </div>
            <div id="worksCited">
               <h2>Works Cited</h2>
               <div class="bibl"><span class="ref" id="d4e839">
                     <!-- close --></span></div>
            </div>
            <div class="toolbar"><a href="#">Preview</a>  |  <span style="color: grey">XML</span> |  <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div>
         </div>
      </div>
   </body>
</html>