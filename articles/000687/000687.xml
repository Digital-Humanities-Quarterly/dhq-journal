<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article" xml:lang="en"><!-- article title in English -->Tool criticism
                    in practice. On methods, tools and aims of computational literary
                    studies</title>
                <!-- Add a <title> with appropriate @xml:lang for articles in languages other than English -->
                <!-- Include a separate <dhq:authorInfo> element for each author -->
                <dhq:authorInfo>
                    <dhq:author_name>J. Berenike <dhq:family>Herrmann</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0002-5256-0566</idno>
                    <dhq:affiliation>Universität Bielefeld</dhq:affiliation>
                    <email>berenike.herrmann@uni-bielefeld.de</email>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Anne-Sophie <dhq:family>Bories</dhq:family></dhq:author_name>
                    <dhq:affiliation>University of Basel</dhq:affiliation>
                    <email>a.bories@unibas.ch</email>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Francesca <dhq:family>Frontini</dhq:family></dhq:author_name>
                    <dhq:affiliation>ILC CNR</dhq:affiliation>
                    <email>francesca.frontini@ilc.cnr.it(link sends e-mail)</email>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Clèmence <dhq:family>Jacquot</dhq:family></dhq:author_name>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Steffen <dhq:family>Pielström</dhq:family></dhq:author_name>
                    <dhq:affiliation>University of Würzburg</dhq:affiliation>
                    <email>pielstroem@biozentrum.uni-wuerzburg.de</email>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Simone <dhq:family>Rebora</dhq:family></dhq:author_name>
                    <dhq:affiliation>University of Basel</dhq:affiliation>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Geoffrey <dhq:family>Rockwell</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0001-7430-4742</idno>
                    <dhq:affiliation>University of ALberta</dhq:affiliation>
                    <email>geoffrey.rockwell@ualberta.ca</email>
                </dhq:authorInfo>

                <dhq:authorInfo>
                    <dhq:author_name>Stéfan <dhq:family>Sinclair</dhq:family></dhq:author_name>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id"
                    ><!-- including leading zeroes: e.g. 000110 -->000685</idno>
                <idno type="volume"
                    ><!-- volume number, with leading zeroes as needed to make 3 digits: e.g. 006 --></idno>
                <idno type="issue"><!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date/>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
                    <!-- If using a different license from the default, choose one of the following:
                  CC-BY-ND (DHQ default): <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>     
                  CC-BY:  <cc:License rdf:about="https://creativecommons.org/licenses/by/2.5/"/>
                  CC0: <cc:License rdf:about="https://creativecommons.org/publicdomain/zero/1.0/"/>
-->
                    <cc:License rdf:about="http://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
                <taxonomy xml:id="project_keywords">
                    <bibl>DHQ project registry; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/projects.xml"
                            >http://www.digitalhumanities.org/dhq/projects.xml</ref></bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en" extent="original"/>
                <!-- add <language> with appropriate @ident for any additional languages -->
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#project_keywords">
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Replace "XXXXXX" in the @target of ref below with the appropriate DHQarticle-id value. -->
            <change>The version history for this file can be found on <ref
                    target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000685/000685.xml"
                    >GitHub </ref></change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>This paper is a case-driven contribution to the discussion on the method-theory
                    relationship in practices within the field of <term>Computational Literary
                        Studies</term> (CLS). Progress in this field dedicated to the computational
                    analysis of literary texts has long revolved around the new, digital tools:
                    tools, as computational devices for analysis, have had here a comparatively
                    strong status as research entities of their own, while their ontological status
                    has remained unclear to the day. As a rule, they have widely been imported from
                    the fields of data science and NLP, while less often being hand-tailored to
                    specific tasks within interdisciplinary settings. Although studies within CLS
                    are evolving to both a higher degree of specialization in method (going beyond
                    the limitations of out of the box tools) and a stronger theoretical modeling,
                    the technological dimension remains a defining factor. An unreflective adoption
                    of technology in the shape of tools can compromise the plausibility and the
                    reproducibility of the results produced using these tools. </p>
                <p>Our paper presents a multi-faceted intervention to the discussion around tools,
                    methods, and the research questions that are answered with them. It presents
                    research first perspectives conceived at the ADHO SIG-DLS workshop <title
                        rend="italic">Anatomy of tools: A closer look at textual DH
                        methodologies</title> that took place in Utrecht in July 2019. At that
                    event, the authors discussed selected case studies to address tool criticism
                    from several angles. Our goal was to leverage a tool-critical perspective, in
                    order to <quote rend="inline">take stock, reflect upon and critically comment
                        upon our own practices</quote> within CLS.  </p>
                <p>We identified <term>Textométrie</term>, <term>Stylometry</term>, and
                        <term>Semantic Text Mining</term> as three central types of hands-on CLS.
                    For each of these sub-fields, we asked: <emph>What are our tools and
                        methods-in-use? What are the implications of using a tool-oriented
                        perspective as opposed to a methodology-oriented one? How do either relate
                        to research questions and theory?</emph>These questions were explored by
                    case-studies on an exemplary basis.</p>
                <p> The unifying perspective of this paper is an applied tool criticism – a critical
                    inquiry leveraged towards crucial dimensions of CLS practices. Here we
                    re-compose the original oral papers and add entirely new sections to it, to
                    create a useful overview of the issue through a combination of perspectives.
                    While we elaborated the thematic connections between the individual case
                    studies, we hope the interactive spirit of an exemplary exchange remains
                    palpable: individual research perspectives shape the case studies reported
                        for <emph>Textométrie</emph>, <emph>Stylometry</emph> and <emph>Semantic
                        Text Mining</emph>, are complemented by further studies showcasing
                    CLS-specific perspectives
                        on <emph>replicability</emph> and <emph>domain-specific research</emph>, and
                    a short section discussing a <emph>tool inventory</emph> as a practical,
                    community-based incarnation of <emph>tool criticism</emph>.<emph> </emph></p>
                <p>The article reflects thus a rich array of perspectives on <term>tool
                        criticism</term>, including the complementary perspective of <term>tool
                        defense</term> – arguing that we need tools and methods as a basic common
                    ground on how to carry out fundamental operations of analysis and interpretation
                    within a community.</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>This article reflects a rich array of perspectives on tools criticism, arguing
                    that we need tools and methods as a basic common ground on how to carry out
                    fundamental operations of analysis and interpretation within a community.</p>
            </dhq:teaser>
        </front>
        <body>
            <div>


                <head>0. Preliminaries</head>
                <p>This paper is a case-driven contribution to the discussion on the method-theory
                    relationship in practices within the field of <term>Computational Literary
                        Studies</term> (CLS)<note><p>In the past few years, the term
                                <term>Computational Literary Studies</term> (CLS) has become more
                            prominent than its alternatives, including <term>Digital Literary
                                Stylistics</term> (DLS). While DLS focuses more on aspects of style
                            (stylometry, corpus stylistics), we decided to use the term CLS in the
                            current paper, except when referring to resources that are explicitly
                            named <title rend="quotes">DLS,</title> such the ADHO-Special Interest
                            Group <title rend="quotes">Digital Literary Stylistics</title>
                            (SIG-DLS), and the DLS tool inventory.</p></note> Progress in this field
                    dedicated to the computational analysis of literary texts has long revolved
                    around the new, digital tools: tools, as computational devices for analysis,
                    have had here a comparatively strong status as research entities of their own,
                    while their ontological status has remained unclear to the day. As a rule, they
                    have widely been imported from the fields of data science and NLP, while less
                    often being hand-tailored to specific tasks within interdisciplinary settings.
                    Although studies within CLS are evolving to both a higher degree of
                    specialization in method (going beyond the limitations of out of the box tools)
                    and a stronger theoretical modeling (e.g., <ptr target="#erlin2021"/>, <ptr
                        target="#underwood2019"/>), the technological dimension remains a defining
                    factor. An unreflective adoption of technology in the shape of tools can
                    compromise the plausibility and the reproducibility of the results produced
                    using these tools. </p>
                <p>Our paper presents a multi-faceted intervention to the discussion around tools,
                    methods, and the research questions that are answered with them. It presents
                    research first perspectives conceived at the ADHO SIG-DLS workshop <title
                        rend="quotes">Anatomy of tools: A closer look at textual DH
                        methodologies</title> that took place in Utrecht in July 2019. At that
                    event, the authors discussed selected case studies to address tool criticism
                    from several angles. Our goal was to leverage a tool-critical perspective, in
                    order to <cit><quote rend="inline">take stock, reflect upon and critically
                            comment upon our own practices</quote></cit> within CLS.<note><p>SIG DLS
                            organized a pre-conference workshop at DH2019 in Utrecht on 9 July 2019,
                            see <ptr target="https://dls.hypotheses.org/activities/anatomy"/>
                        </p></note>
                </p>
                <p>We identified <term>Textométrie</term>, <term>Stylometry</term>, and
                        <term>Semantic Text Mining</term> as three central types of hands-on CLS.
                    For each of these sub-fields, we asked: <q>What are our tools and
                        methods-in-use? What are the implications of using a tool-oriented
                        perspective as opposed to a methodology-oriented one? How do either relate
                        to research questions and theory?</q> These questions were explored by
                    case-studies on an exemplary basis.</p>

                <p>The unifying perspective of this paper is an applied tool criticism — a critical
                    inquiry leveraged towards crucial dimensions of CLS practices. Here we
                    re-compose the original oral papers and add entirely new sections to it, to
                    create a useful overview of the issue through a combination of perspectives.
                    While we elaborated the thematic connections between the individual case
                    studies, we hope the interactive spirit of an exemplary exchange remains
                    palpable: individual research perspectives shape the case studies reported for
                        <term>Textométrie</term>, <term>Stylometry</term>, and <term>Semantic Text
                        Mining</term>, are complemented by further studies showcasing CLS-specific
                    perspectives on <term>replicability</term> and <term>domain-specific
                        research</term> , and a short section discussing a <term>tool
                        inventory</term> as a practical, community-based incarnation of <term>tool
                        criticism</term>. </p>
                <p>The <emph>practice of tool criticism</emph>, the evaluation of a tool's
                    suitability to a specific task, is a <emph>sine qua non</emph> in the
                    interdisciplinary setup of a Digital Humanities discipline like CLS, where
                    actors have radically different degrees of <emph>various</emph> types of
                    technological expertise. Here is where tool criticism caters to <cit><quote
                            rend="inline">the evaluation of the suitability of a given digital tool
                            for a specific task</quote>
                        <ptr target="#traub-vanossenbruggen2015"/></cit>. Importantly, the goal is
                    to <cit><quote rend="inline">better understand the impact of any bias of the
                            tool on the specific task, not to improve the tools performance</quote>
                        <ptr target="#traub-vanossenbruggen2015"/></cit>.</p>

                <p>Here, tool criticism is mainly methodological and epistemological: it complements
                    the narrower, domain-dependent practice of <emph>methodological
                        evaluation</emph>, where established ways of testing a method's reliability,
                    objectivity and validity, or precision and recall, respectively, are widely
                    standardized. Tool criticism thus operates in a field of practices where such
                    standards are not conventional yet, but goes beyond them also, by combining a
                    methodological with a <emph>critical</emph> enquiry that <emph>can</emph> be
                    epistemological, but can also extend to financial, pedagogical, disciplinary and
                    other implications. It aims to unearth presuppositions ingrained in the tools
                    that often neither <q>data scientists</q> nor <q>humanities scholars</q> may be
                    naturally aware of in interdisciplinary settings. Especially when a tool becomes
                    successful and thus popular, the evaluation of its suitability to specific tasks
                    is often backgrounded or even neglected. </p>
                <p>With the 2019 workshop title <title rend="italic">Anatomy of Tools</title>, we
                    chose an analogy from the field of biology to convey our joint aim of
                        <emph>dissecting</emph> case studies with respect to the role of <q>the
                        tool</q> respectively. In our work, we noticed that in CLS generally highly
                    diverse incarnations of <q>tool</q> are subsumed under one general concept, each
                    tool different in type, specificity, and goal. Therefore, we operationally
                    define the term <term>tool</term><note><p>The term <term>tool</term> is heavily
                            underspecified in digital humanities. For example, the glossary of
                            ForText, a popular DH-training resource in the German-speaking context,
                            does not incorporate a lemma for <q>tool</q>
                            <ptr target="https://fortext.net/ressourcen/glossar"/> (retrieved 13
                            October 2022). In the following, in absence of a comprehensive
                            definition to draw from, we provide a largely operational one.
                        </p></note> as follows: <cit><quote rend="block"> At the most basic level,
                            in the present paper, a tool is <emph>a computational device used for
                                carrying out <q>analyses</q></emph>. This potentially includes aids
                            for diverse sub-<emph>processes</emph> such as data collection, data
                            pre-processing, annotation and indexing, as well as <q>analyses
                                proper</q>, which may or may not involve frequency counts,
                            algorithmic and statistical modeling, or visualization. A tool is here
                            thus understood as a type of methodological vehicle used for
                            contributing to the pursuit of a particular research goal on some aspect
                            of literary discourse treated as data. In the context of CLS, a tool
                            digitally retrieves, and/or represents, and/or operates upon and/or
                            manipulates literary data,<note><p>We use the term literary data to
                                    include a wide array of data. CLS presently centers mostly on
                                    textual data, and our case studies do the same. However, a
                                    growing body of CLS research is dedicated to studying
                                    multi-modal, especially audio and visual data. For a recent
                                    overview see <ptr target="#herrmann2021"/>.</p></note> which
                            principally includes annotations and metadata.<note><p> Our predicate
                                        <q>operates upon</q> includes for example visualization,
                                    analysis, annotation, curation, comparison cf. <ptr
                                        target="#unsworth2000"/>. In his <title rend="italic"
                                        >Scholarly primitives</title> paper, John Unsworth gives an
                                    indirect definition in the table at the bottom of the document
                                    in which he transfers the human genome project into a <quote
                                        rend="inline">human genre project:</quote>
                                    <cit><quote rend="inline">2.Tools: What protocols and tools for
                                            data submission, viewing, analysis, annotation,
                                            curation, comparison, and manipulation will you need to
                                            make maximal use of the data? What sorts of links among
                                            datasets will be useful?</quote></cit>.</p></note> This
                            includes for instance text analysis tools such as Voyant, program
                            libraries for R or Python, as well as general-purpose tools such as
                            Excel spreadsheets or visualization tools.</quote>
                    </cit>
                    <cit>
                        <quote rend="block"> By contrast to a <emph>method</emph>, a tool is further
                            defined by its typically reified and closed character. As a
                            computational implementation of a method, it exists as a distinctive
                                <emph>entity</emph> with a limited set of functions. A tool often
                            includes a graphical user interface (GUI), which makes it also
                            phenomenologically perceived as a particular device rather than a
                            potentially adaptable set of conditions. Another relevant dimension of
                            tools is its transformative power, as emphasized by Weizenbaum and
                            others: <cit><quote rend="inline">[T]he tool is much more than a mere
                                    device: It is an agent for change</quote>
                                <ptr target="#weizenbaum1984" loc="p. 18"/></cit>. If a tool is
                            successful, which means widely and conventionally used in order to carry
                            out a task, this <emph>change</emph> does not only happen at the level
                            of the <emph>method</emph> of doing something, but potentially also
                            extends to the <emph>object</emph> to which method is applied. The
                            object is then co-constructed by the method – which in turn has effects
                            on the scholarly <emph>subject</emph> using the tool, for example, on
                            their epistemological framework.<note><p>Computational methods and
                                    resources are <term>change agents</term> in Weizenbaum’s sense:
                                    they alter literary scholars’ practices and the modeling of
                                    objects and theories. For example, Herrmann (forthcoming) shows
                                    how corpora are externalizations of the philological object in
                                    practical and theoretical senses, and thus are key arenas of
                                    this change. A digital literary corpus is different both from a
                                    collection of books and a ‘mere digital database’, because it
                                    has a more deliberate modeling function: It is built to
                                    construct <q>narratives</q>
                                    <ptr target="#hayles2012" loc="176"/>. </p></note></quote>
                    </cit>
                </p>

                <p>How can actors know enough about the <quote rend="inline">bias of the tool on the
                        specific task,</quote>
                    <ptr target="#traub-vanossenbruggen2015"/> how can they gauge its validity, or
                    its capacity for producing plausibility? Or its potential of inducing change in
                    the world? In the present paper, we will not offer many decisive answers to the
                    important questions raised by the use of tools in CLS, and neither can we dive
                    deep into systematic epistemological, media-theoretical, or sociological
                    discussions. However, we do describe critically how a few exemplary tools are
                    applied to several typical research questions within representative sub-fields
                    of CLS. Our aim is to give a practical foothold for a principled
                        <emph>tool-critical awareness</emph>.</p>
                <p>The paper is structured in the following way: In the introduction (Section 1), we
                    raise issues of <q>tool criticism</q> of current CLS at the interface between
                    methodology and theory. Here, we discuss the advantages and disadvantages of the
                    discourse on <term>tools</term> vs. <term>methods</term>, as well as the
                    dimensions of different types of <term>user groups</term>, and a shift from
                        <term>tools</term> towards a more integrated perspective on
                        <term>modeling</term>. We also make a case for <q>tool defense.</q> In the
                    main part, we first present three tool-critical case studies of what we have
                    called <q>method-driven <term>schools</term></q>: <term>Textométrie</term>,
                        <term>Stylometry</term>, and <term>Semantic Text Mining</term> (Section 2).
                    Each of its sub sections (Sections 2.1 - 2.3) addresses a pertinent approach,
                    giving an overview of its usability and strengths in the actual research, as
                    well as pointing out problems and formulating specific avenues for further
                    development. Taking a slightly different angle, Section 3 will then discuss the
                    important issue of <term>replicability</term> and the range of its potential
                    incarnations in CLS – in between its <q>humanities</q> and <q>science</q> poles.
                    In the fourth section, we single out an example for one <emph>specific research
                        domain</emph> (poetry), addressing the need for domain-specific tool
                    adaptation in a particular case. The fifth section will present one attempt at a
                    practical solution for handling and gauging methods in Computational Literary
                    Studies, the <title rend="italic">SIG-DLS Tool Inventory</title>, which was
                    designed to provide both a perspective of orientation and criticism. Finally, we
                    draw a summary and <emph>conclusion</emph>. </p>

            </div>
            <div>
                <head>Introduction</head>
                <p>Tool criticism <ptr target="#vanes2018"/>
                    <ptr target="#koolen2018"/> has made us aware that the digital tools widely
                    distributed in DH have the power to reify theoretical <emph>a prioris</emph>
                    <ptr target="#flanders-jannidis2019"/>
                    <ptr target="#mccarthy2005"/>. Therefore, the community needs a handle for
                    gauging their validity, or capacity for producing plausibility <ptr
                        target="#winko2015"/>, possibly applying a sense of craft <ptr
                        target="#piper2017"/>. But tools are not <q>just tools</q> – the current
                    panorama in CLS presents a plethora of computational devices used for carrying
                    out <quote rend="inline">literary analyses: instruments, protocols and
                            practices for processing, analyzing and visualizing data</quote>
                    (see our operational definition of <term>tool</term> above). On On a general
                    level, all of these are used to examine aspects of <q>literature</q>, but a
                    closer look reveals that the methodological <q>heirdom</q> of CLS is a
                    precarious patchwork. Just loose connections link tools from stylometry to those
                    from NLP and computational linguistics, to those from corpus linguistics, and to
                    ones more genuinely developed within literary studies. At the moment we are
                    dealing with a rich, but also atomized situation. Some tools, but by far not
                    all, involve aggregation and statistical models, while others center on
                    visualization, while yet others have a different epistemic approach, centering
                    on implementing a hermeneutic or deconstructivist mode of enquiry. Some tools
                    combine multiple methods.</p>
                <p>What is more, CLS practices are diverse also in the degree of reduction and
                    generalization. They vary in the way they explicitly address the fit of
                    (digital) data and method to literary modeling <ptr
                        target="#flanders-jannidis2019"/>
                    <ptr target="#piper2018"/>
                    <ptr target="#underwood2019"/>. The practices run the gamut extending from a
                        <q>computational</q>, or <q>quantitative,</q> paradigm <ptr
                        target="#herrmann2017"/> with computational linguistics, text mining,
                    quantitative linguistics, and corpus linguistics and an <q>analog</q>,
                        <q>qualitative</q> paradigm with structuralist, hermeneutic, or
                    deconstructivist approaches. It is evident how these have substantially distinct
                    requirements, intentions and ways of defining the limitations imposed by the
                            digital.<cit><quote rend="block">As the role of digital tools in these
                            [sic!] type of studies grows, it is important that scholars are aware of
                            the limitations of these tools, especially when these limitations might
                            bias the outcome of the answers to their specific research questions.
                            While this potential bias is sometimes acknowledged as an issue, it is
                            rarely discussed in detail, quantified or otherwise made explicit. On
                            the other hand, computer scientists (CS) and most tool developers tend
                            to aim for generic methods that are highly generalisable, with a
                            preference for tools that are applicable to a wide range of research
                            questions. As such, they are typically not able to predict the
                            performance of their tools and methods in a very specific context. This
                            is often the point where the discussion stops. <ptr
                                target="#traub-vanossenbruggen2015"/></quote></cit></p>
                <p>In the current paper, we address selected tools in concrete scenarios of
                    application within CLS, as well as the preconditions of
                        <emph>replicability/recapitulation</emph> and <emph>domain
                        specificity</emph> as well as practical questions of how to conduct tool
                    criticism more systematically and openly. </p>

                <p>There is a fertile ground for this in CLS, which have clearly developed a <quote
                        rend="inline">sense of tool criticism</quote> that includes an awareness of
                    built-in bias in generic tools <quote rend="inline">inherited</quote> from
                    Computer Science and NLP <ptr target="#noble2018"/>
                    <ptr target="#oneil2016"/>, and appears to evolve to an explicit sense of
                    digital methodology <ptr target="#hayles2012"/>. Tool criticism, posited by
                    Karin van Es and colleagues as <cit><quote rend="inline">a rigorous inquiry into
                            the tools used for research</quote>
                        <ptr target="#vanes2018" loc="24"/></cit> is indeed already becoming an
                            <cit><quote rend="inline">essential element of the overall research
                            process</quote>
                        <ptr target="#vanes2018" loc="24"/></cit>. Accordingly, more emphasis has
                    been put on the adaptation and scaling of methods to specific research
                    questions, combining reflected and critical approaches with the affordances of
                    digitization, constructively managing constraints. Most recently, the
                    replication-debate that originated from psychology has demonstrated that common
                    and explicit criteria, or at least a differentiated conversation about
                    underlying axioms, are desirable (see replies to <title rend="quotes">the
                        Da-paper</title>
                    <ptr target="#da2019"/><note><p><ptr
                                target="https://culturalanalytics.org/2019/09/special-forum-on-responses-to-nan-z-da/"
                            /></p></note>). </p>

                <p>Interpreted as a contribution to serious scholarship, the – also clearly polemic
                    – paper by Nan Z. Da<note><p>Post-hoc analyses of Nan Z. Da’s polemic paper and
                            the ensuing debate have revealed that instead of a methodological paper
                            that it appeared to be at the surface, it rather worked as an
                            disciplinary in- and outgroup profiling.</p></note> highlights that
                    constructive criticism is needed for methodological and theoretical progress,
                    which examines whether a certain method is actually applied well to some type of
                    research question. We believe that the time is ripe for a constructive
                        <q>literary</q> method criticism, which can foster a uniquely humanities
                    perspective of digital enquiry. One of the most interesting positions emerging
                    from our workshop is that of tools as – ideally – well-calibrated instruments
                    for <q>getting things done in CLS.</q> From here, we have started thinking along
                    the lines of a <emph>tool defense,</emph> which concerns the balance between
                    methodological and content-related research. Driven by the affordances of the
                    digital transformation, CLS has so far understandably put an emphasis on the
                        <q>how</q> – the development of methodology, as well as digital resources
                    and standards. Meanwhile, the <q>what,</q> that is, historiographic and
                    systematic enquiries about literary texts, structures, and discourse have not
                    received equal attention. Looking out at the future of CLS, we realize that the
                    discussion about tools directly relates to the definition of digital literary
                    studies as a discipline: is it predominantly a methodological experiment, or is
                    it a serious data-driven, digitally enhanced, enquiry into <q>things
                        literary?</q> Starting to answer these questions means to address strategies
                    of tool evaluation, but also standards of methodological training. </p>

                <p>In light of these issues, when juxtaposing our section 2 on
                        <term>Textométrie</term>, <term>Stylometry</term>, and <term>Semantic Text
                        Mining</term> we also asked <emph>How do practices relate to different types
                        of actors in the field?</emph> In CLS, scholars originally trained in
                    literary studies typically approach methodology differently from scholars
                    originally trained in data science. For example, our discussion of textométrie
                    (2.1) raises very different questions from that of semantic text mining (2.3).
                    At the same time, there are clear differences between the statistical models
                    applied in the different <q>computational fields</q> – NLP, corpus linguistics,
                    and computational linguistics, which only in part coincide with the division
                    between explanatory / exploratory / predictive perspectives. </p>
                <p>Scholars, by discipline and training, may vary in their research focus, some
                    preferring to solve methodological questions, while others concentrate on
                    questions about periods, stylistic features, or the history of ideas, to name a
                    few. While it is indispensable to understand the way a tool works at a
                    fundamental level, not every scholar is interested in the development of
                    methodology for its own sake. Along these lines, an important point made in the
                    discussion was <emph>that we need a well-calibrated arsenal of
                        instruments</emph> in the hands of a <emph>scholarly majority</emph>:
                    transparent tools/methods as a reliable basis for the emerging mainstream, who
                    rather than advance a method, wishes to pursue literary research questions. Such
                    methods fulfill multiple functions, one of which is providing a basic common
                    ground on how to carry out fundamental operations of analysis and
                            interpretation.<note><p>A good candidate for the most fundamental
                            academic practice being comparison <ptr target="#descartes1959"/>,
                            externalized by computational methods, presently is gaining momentum.
                        </p></note> This is precisely where we have identified the perspective of
                        <term>tool defense</term>. Section 2 aims at striking a balance in between
                    criticism and defense.</p>

                <p>At the moment, one of the most pertinent questions in the broader field of
                        <q>science</q> is that of <emph>replication</emph>, addressed by section 3,
                    which emphasizes that digital humanistic enquiry may, however, entail a
                    qualitative, sometimes even emphatic, <q>understanding</q> of cultural phenomena
                    embedded in rich historical contexts. Not only where interpretation involves the
                    appreciation of texts or parts of texts through aesthetic and otherwise
                    hermeneutic processes, we have to deal with an irreducible subjectivity.But
                    there is a clash with <q>replicability</q> approached as one of the criteria of
                    the scientific method.<note><p>Reproducibility / replicability is one of the
                            core principles of scientific progress. Direct replication is the
                                    <cit><quote rend="inline">attempt to recreate the conditions
                                    believed sufficient for obtaining a previously observed finding
                                    and is the means of establishing reproducibility of a finding
                                    with new data</quote>
                                <ptr target="#opensciencecollaboration2015" loc="aac4716–2"/></cit>.
                            The so-called <term>replication crisis</term> has affected many
                            empirical fields such as medicine and psychology. While one way of
                            addressing it has focused on scientific malpractice, for social
                            constructivists in the sense of <ptr target="#berger-luckmann1967"/> the
                            crisis highlights precisely the practice of a field that is constructed
                            by the field.</p></note> In CLS, we thus need to discuss whether, when
                    and where we want to strive for <q>replicability</q> based on accounts of
                        <q>objectivity</q> – and whether, when and where the softer criterion of
                        <q>intersubjectivity</q> may be more adequate. Section 3 prompts questions
                    such as: <emph>What is the status of replicability in CLS, and are there
                        possibly specific <q>literary</q> types of replicability? What culture of
                        replication do we want to develop in the humanities? What traditions can we
                        draw on?</emph></p>
                <p>Closely related to the need for standards and ready-made tools in CLS is that of
                        <emph>domain specificity</emph> addressed by section 4: While CLS should at
                    one level advance towards a <q>standard</q> inventory of methods and tools, in
                    ultimate instance valid and meaningful results can be achieved only if sources
                    and methodologies are tailored to the specific research questions, which in turn
                    are nested within domains of expert knowledge. Thus, a practical CLS tool
                    criticism can hardly avoid factoring in the affordances of <q>target domains</q>
                    such as prose, drama, and poetry: <emph>How can the gap between domain
                        generality and specificity be bridged in practice? </emph>Section 4 singles
                    out one specific domain, poetry, as an exemplary use case.<emph> </emph>Section
                    5 on the <emph>DLS-Tool Inventory</emph> follows by illustrating an ongoing
                    initiative to address documentation, comparability and community-driven
                    evaluation: <emph>How can we systematically record and compare methods and
                        tools, gauge their usability and the fundamental assumptions incarnated?
                    </emph>It showcases one attempt at making available tools as embedded in
                    concrete case studies, offering a foothold for judging their applicability, as
                    well as practical replication and/or recapitulation of particular studies.</p>

            </div>
            <div>
                <head>2. Three Methodological Schools within CLS </head>
                <p>Based on what has emerged so far from the <title rend="italic">DLS-Tool
                        Inventory</title> and our observations of research practices, we have
                    identified three <q>schools</q> that coincide with different types of (handling)
                    digital tools – or indeed methods: <term>Textométrie</term>,
                        <term>Stylometry</term>, and <term>Semantic Text Mining</term>. In the
                    following sub-sections, three short tool-critical case studies will address each
                    of these by practical example, addressing its advantages and limitations. </p>


                <div>
                    <head>2.1 Textométrie: Applying a general tool to a specific research
                        question</head>

                    <p><q>Textometric tools</q> are widely used to explore literary corpora by a mix
                        of quantitative and qualitative methods. Textométrie is an approach to
                        statistical text analysis, developed in France during the 1970s, following
                        lexicometry, a traditional statistical approach to study lexical
                        particularities of literary texts, such as common words, hapax legomena, and
                        specific keywords <ptr target="#guiraud1953"/>
                        <ptr target="#lafon-muller1984"/>. In addition, textometry applies methods
                        of data analysis, for instance factor analysis or clustering, that enable
                        mapping of words and texts as they are similar or opposed to each other
                        within a corpus.<note><p><ptr
                                    target="http://textometrie.ens-lyon.fr/spip.php?rubrique80"
                                /></p></note> Its objective is to produce interpretable statistics
                        of textual data, in a contrasting perspective. Bolstered by the development
                        of accessible software platforms incorporating textual statistics,
                        textometry has produced a number of relatively generic tools, alongside a
                        productive body of research in the domain of stylistics and corpus
                        linguistics <ptr target="#heiden2010"/>. As textometry involves the
                        principled interaction between quantitative (reductive) and qualitative
                        (contextualizing) enquiry <ptr target="#pincemin2011"/>, it is an attractive
                        approach for scholars from the field of stylistics, which is traditionally
                        qualitative and interpretative. In mixed methods, they are able to
                        capitalize on pattern detection of quantitative operations for larger-scaled
                        studies and re-contextualization that often leads to original
                        observations.</p>
                    <p>Currently, the most pertinent textometric tools are probably <title
                            rend="italic">Hyperbase</title><note><p><ptr
                                    target="http://ancilla.unice.fr/ "/></p></note> (Etienne Brunet,
                        University of Nice) and <title rend="italic">TXM</title> (Serge Heiden,
                        Ecole Normale Supérieure of Lyon). In the following, we will report on a
                        study carried out with TXM (version 0.5). We chose TXM as it may be referred
                        to as a <q>canonical tool.</q> It is a text analysis environment which is
                        well documented, available in open-source, and compatible with texts encoded
                        in XML. It also comprises a graphical client based on CQP and R and is
                        available for Microsoft Windows, Linux, Mac OS X and as a J2EE web
                                portal.<note><p>TXM, as we said, is still in development and its
                                team, taking constructive feedback to implement and complete its
                                statistical studies.</p></note></p>
                    <p>In the following, we will reflect on using TXM as a comprehensive,
                        out-of-the-box tool for a diachronic stylistic analysis by applying it to
                        one exemplary study of French poetry. <emph>How can the gap between domain
                            generality of the tool and the specificity of the research question be
                            bridged in practice? What advantages and disadvantages need to be
                            faced?</emph></p>
                    <p>In our TXM case study, we explore the poetic style of the poems written by
                        the French poet Guillaume Apollinaire (1880-1918) from a diachronic
                        perspective. The literary analysis of the poetry of Apollinaire is meant as
                        an illustration only: this subsection primarily aims to highlight and to
                        scrutinize the strengths and the limitations of the TXM tool. Apollinaire is
                        one of the most eminent French poets of the early twentieth century. He is
                        particularly known as an important figure in the renewal of poetic forms
                        alongside contemporary avant-garde movements in art (cubism, dadaism). His
                        poetry is marked by <emph>discontinuity</emph>, <emph>heterogeneity</emph>
                        and <emph>fragmentation</emph>, similar to other authors of this period
                        (e.g. Romains, Salmon, Jacob). One important dimension for interpretation
                        and analysis of his poetic writing is <q>perpetual reorganization</q>: texts
                        being rewritten, reused in other contexts, transformed from prose into
                        poetry, see <ptr target="#debon2008"/>
                        <ptr target="#decaudin1969"/>
                        <ptr target="#follet1987"/>
                        <ptr target="#jacquot2012"/>
                        <ptr target="#moore1995"/>. These sign-marks of Apollinaire's writing appear
                        at several levels, including the composition of the collections, different
                        types of <q>tone</q>, lexical resources, influences, and thematic issues. </p>
                    <p>Usually, analyses are based on thematic differences within Apollinaire's
                        work, especially between his two main collections: <title rend="italic"
                            >Alcools</title> (1913) and <title rend="italic">Calligrammes</title>
                        (1918). Approaching style as a dynamic factor <ptr
                            target="#herschberg-pierrot2006"/>
                        <ptr target="#jenny2011"/>, our case study deliberately adopts a
                            <q>fresh</q> look, with the intention to be unbiased with regard to a
                        thematic approach, centering in on the diachronic evolution of writing.
                            <list style="unordered">
                            <item>
                                <hi rend="italic">If style is conceived of as a dynamic factor, how
                                    may Apollinaire's heterogeneous poetry be analyzed
                                    textometrically as a homogeneous unit? </hi>
                            </item>
                            <item>
                                <hi rend="italic">How may the evolution of Apollinaire's style be
                                    textometrically traced over the years? </hi>
                            </item>
                            <item>
                                <hi rend="italic">Can we textometrically re-assess value judgments
                                    predominantly based on thematic aspects?</hi>
                            </item>
                        </list>
                    </p>

                    <p>We depart from the hypothesis of a temporal evolution of stylistic,
                        specifically, syntactic, structures in Apollinaire's poetic oeuvre. As there
                        are issues with the clear temporal reference of Apollinaire's texts, we
                        resort to the criterion of <q>relative dating,</q> enriching the corpus by
                        temporal metadata. Going by publication dates, Apollinaire's anthumous
                        poetic oeuvre appears in several collections of varying scope over a period
                        of about eight years: <title rend="italic">Le Bestiaire ou Cortège
                            d’Orphée</title> (1911), <title rend="italic">Alcools</title> (1913),
                            <title rend="italic">Vitam impendere amori</title> (1917) and <title
                            rend="italic">Calligrammes</title> (1918). . </p>
                    <p>Metadata covering just these dates would present a distortion of the
                        genealogy of writing, however. <title rend="italic">Alcools</title>, for
                        instance, a striking example of the phenomenon of perpetual rewriting, was
                        published in 1913, but actually collects poems originally composed over more
                        than a decade (1898 - 1913).<note><p>It is therefore a composition of
                                aborted or simply abandoned former projects that literary
                                specialists often name <q>cycles</q> (the <title rend="quotes"
                                    >Stavelot cycle</title>, the <title rend="quotes">Annie
                                    cycle</title>, see <ptr target="#debon2008"/>
                                <ptr target="#decaudin1969"/>).</p></note> This temporal
                        heterogeneity poses a substantial problem for designing a metadata schema:
                        dates need to be provided for each text in order to facilitate a diachronic
                        contrastive analysis of the corpus.</p>
                    <p>In order to get a handle on a potential evolution of Apollinaire's style, it
                        appears fruitful to consider syntactic configurations. Striking a balance
                        between descriptive accounts of style, for instance,
                            <emph>discontinuity</emph> and <emph>fragmentation</emph>, and the
                        affordances of a <q>formal</q> tool such as TXM, we focused on the
                        distribution and the use of grammatical words and especially what we will
                        call here the <q>syntactic link markers</q>, i.e. the lexical links between
                        sentences, for example conjunctions and relative pronouns. Interestingly,
                        the use of these grammatical words is explicitly mentioned in Apollinaire's
                        poetics, for example, in 1915, where he posits a <cit><quote rend="inline"
                                >simplification of the syntax of poetry</quote>
                            <ptr target="#apollinaire2005" loc="77"/></cit>. Our method is thus to
                        compare several sub-divisions of Apollinaire's poetic corpus for the
                        distribution of grammatical words, but paying special attention to the date
                        of writing, which needs to be manually re-attributed to each poem. In TXM,
                        we thus use basic database functions to shape the representation of the data
                        to our needs. </p>
                    <p>The first step in using TXM is corpus construction and pre-processing. This
                        includes tokenization, lemmatization, and automatic annotation of textual
                        features, such as part of speech, as well as the enrichment by metadata such
                        as authorship, data of publication, genre, text collection. It is thus
                        pivotal that TXM <q>takes care</q> of important analytical steps, the
                        researcher relying on the sub-modules and parameter settings of the
                            <q>general tool</q> provided for the community. In our annotation of the
                        Apollinaire corpus, we aim to represent its <q>original structural
                            organization</q> from the largest unit, the subset constituted by
                        Apollinaire's poetic collections (<title rend="italic">Alcools</title>,
                            <title rend="italic">Bestiaire</title>, <title rend="italic"
                            >Calligrammes</title> ...) down to that of the <q>verse</q>: title of
                        the poetry collection &gt; section &gt; subdivision (if needed) &gt; title
                        of the poem &gt; stanza &gt; verse. These structural metadata are completed
                        by information about textual particularities, especially at the
                        <q>poem</q>-unit level. We use six units (optional marked with *): title,
                        subtitle*;, epigraph*, date of writing*, date of edition, and date of
                        publication (see <ptr target="#jacquot2017"/>).</p>

                    <p>The <q>research logic</q> of TXM requires systematic temporal data. Our
                        relative dating – defined as the marking up of each text according to its
                        writing date, either supposed or authorized <ptr target="#jacquot2017"
                            loc="91"/> – has the advantage of transcending the potentially
                        problematic <q>unity</q> of texts created by the poetry collections.
                                <note><p>The supposed dates of writing are based on analyses of the
                                manuscripts and the first states of publication of the poems of
                                Apollinaire initiated with great rigor and much erudition by M.
                                Décaudin in 1969 <ptr target="#decaudin1969"/>, as well as in his
                                edition of the text for the Bibliothèque de la Pléiade <ptr
                                    target="#apollinaire1994"/>. It is completed by the analysis of
                                his notebooks and his correspondence kept at the Fonds Jacques
                                Doucet (<ptr target="http://www.bljd.sorbonne.fr/index.php"/>),
                                which make it possible to give certain texts a quite precise
                                drafting interval.</p></note> The collection <title rend="italic"
                            >Alcools</title>, for example, is based on a set of themes and
                        alternates cycles and inspirations. Using the TXM database detaches the
                        poems from their previously perceived unit (<q>the collection</q>) and thus
                        liberates them from efficacious prior ascription, facilitating new
                        perspectives. The textometric analysis works on sub-sets indexed by the
                        newly added temporal attributes, potentially identifying, in the various
                        collections, new organizations of Apollinaire's poems.</p>
                    <p>For the diachronic analysis, we apply TXM's <cit><quote rend="inline"
                                >specificity calculation</quote>
                            <ptr target="#lafon1980"/></cit>, which highlights the positive as well
                        as negative specificities in terms of a defined set of linguistic markers,
                        comparing all temporal subsets with each other.</p>
                    <p>Building upon the part-of-speech-tagging in TXM (launched at ingesting a
                        corpus), and our division of temporal sub-sets (by year), we examine the
                        diachronic distribution of syntactic link markers (for example: <foreign
                            xml:lang="fr">lorsque</foreign>, <foreign xml:lang="fr">quand</foreign>,
                            <foreign xml:lang="fr">qui</foreign>, <foreign xml:lang="fr"
                            >que</foreign>, <foreign xml:lang="fr">parce que</foreign>). Figure 1
                        shows positive and negative specificities in the distribution of
                            <q>syntactic link markers</q> (significant over- vs. underuse in
                        comparison with the other temporal subsets). It highlights clear trends in
                        the way Apollinaire uses the syntactic link markers. <figure>
                            <head>Diachronic distribution of subordinating conjunctions in
                                Apollinaire’s poetic corpus <q>specificity calculation.</q>
                                Comparison between <q>Supposed Writing Years</q> (in red) and
                                    <q>Authorized Writing Years</q> (in blue)</head>
                            <figDesc>Line chart with one red line and one blue line</figDesc>
                            <graphic url="resources/images/figure01.png"/>
                        </figure> The figure also depicts a comparison between the distribution
                        specificities for the metadata <q>Authorized Writing Years</q> (in blue),
                        and <q>Supposed Writing Years</q> (in red). </p>
                    <p>For the <q>assumed writing years</q>, the graph reveals a striking
                        over-representation of the style markers in 1915, 1916, and 1917, and even
                        the <q>Authorized Writing Years</q> (in blue) depict a significant overuse
                        in 1915 and 1916. The year 1917 shows a difference between the assumed and
                        the authorized writing date - the latter does not deviate much from a
                        statistical chance result, falling into the <q>banality</q> zone (in
                        textometry, the significant specificity scores are usually below -2 and
                        above +2). It is on the basis of this corpora's analyses that we choose,
                        through several pilot studies, to favor the <q>assumed writing years</q> for
                        our stylistic studies. Adding relative temporal data indeed seems to provide
                        us with genealogical-philologically more appropriate and often more
                        meaningful results than the authorized year of writing (which is after all,
                        most often more artificial).</p>
                    <p>The data (including further features, see <ptr target="#jacquot2017"/>) show
                        a trend that is in line with the author's poetics: Over time, stylistic
                        links become more common in Apollinaire's poems. This finding actually
                        defies the idea of a deep heterogeneity, as it points to the increase of
                        devices used for creating cohesion and coherence, at least at the stylistic
                        level. The data also show that the attributed dates render more pronounced
                        amplitudes than the authorized dates.</p>
                    <p>Our example of textometric study shows that a generic tool can be put to
                            <q>specific use</q> when this use centers on the role of metadata
                        manually added by the user based on intricate expert knowledge. Using a tool
                        like TXM allows respecting the complexity of the literary subject, even if
                        it - as in our case, the dating of Apollinarian texts - is fragile. The
                        scholar is not relieved of their critical responsibility. As a tool capable
                        of processing structured data, TXM allows for the modeling of data based on
                        criteria that are not always explicit. Entering the estimated dates into the
                        database has the potential (and risk) of their reification, enhanced through
                        visualization and ensuing analysis, especially where no automatic reflection
                        is built into the analytical process regarding the basis for temporal
                        attribution. At the same time, the textometric analysis offers several
                        opportunities for a heuristic change of perspectives, specifically through
                        simultaneous representation of estimated and authorized dates, which allows
                        for systematic comparison of different types of ascription. For users from
                        stylistics, TXM has the advantage of flexibly partitioning the corpus into
                        several different sub-corpora, comparable to each other. It should be
                        mentioned that the whole analysis hinges on pivotal pre-processing steps,
                        such as tokenization, lemmatization, and part-of-speech tagging. The
                        accuracy of these automatic steps needs to be rigorously gauged (are the
                        words correctly separated, including compositional lexical units that cross
                        white space boundaries? Are parts of speech correctly identified?) The same
                        holds at a different level for the research logic tacitly presented (e.g.,
                        analytic relevance of basic unit <q>word,</q> of <q>part of speech</q>).
                        These present possible <q>blind spots</q> that users need to be aware of and
                        are required to thoroughly check in each analysis.</p>

                </div>
                <div>
                    <head>2.2 Word Frequencies in Stylometry</head>

                    <p>Stylometry uses a series of tools and methods for the statistical analysis of
                        style, based on advanced calculations on stylistic properties of texts,
                        whereby mostly focusing on word frequencies. Its main applications have been
                        both authorship attribution and distant reading. Initially developed through
                        the use of spreadsheets, stylometric methods have been fully implemented
                        into programming languages such as R and Python <ptr target="#eder2016"/>
                        <ptr target="#evert2017"/>, and enhanced by a wide variety of
                        visualizations, derived from research fields such as philogenetics and
                        network theory <ptr target="#eder2017"/>. However, from a very pragmatic
                        point of view, it can be stated that the most widely and frequently used
                            <q>tool</q> in stylometry are simple algorithms that count words, on
                        which methods like <cit><quote rend="inline">Delta distance</quote>
                            <ptr target="#burrows2002"/></cit> are applied.</p>
                    <p>In the abovementioned, fairly dramatic, critique of computational literary
                        studies, <ptr target="#da2019"/> made a controversial case against the
                        application of quantitative methods to literary texts.<note><p>This section
                                took its start as a short abstract titled <title rend="italic">Less
                                    than countless. Options to move beyond word counting in
                                    stylometry</title> originally provided by Mike Kestemont for the
                                workshop. The original abstract was substantially extended and
                                revised by Simone Rebora and the co-authors.</p></note> She argues
                        that much work in this field essentially boils down to <q>counting
                            words.</q> We agree that this view is somewhat reductive, but also find
                        it not without merit: it certainly applies to much of the present-day
                        approaches that are dominant in stylometry and, consequently, to many of the
                        tools that are available. While this methodological focus is to some extent
                        justified by empirical work, the under-explored options for stylometry to
                        move beyond naive word counting need specific attention. Stylometrists, for
                        instance, often take pride in the fact that their tools typically work on
                        raw texts that require little preprocessing. In this, stylometry ignores
                        much of the achievements of literary theory in the twentieth century, such
                        as the importance of focalization or the (actual or implied) reader <ptr
                            target="#bal2017"/>
                        <ptr target="#genette1972"/>
                        <ptr target="#miall2018"/>.<note><p>One exception, though, might be that of
                                    <quote rend="inline">mixed methods</quote>
                                <ptr target="#herrmann2017"/>, which combine wordcount with
                                approaches such as keyness analysis to identify the linguistic
                                phenomena that should be investigated more closely. Such methods
                                have also been employed successfully in authorship attribution
                                studies (cf. <ptr target="#rebora2019"/>) and they can be considered
                                as an effort to build a bridge between raw statistics and the
                                phenomenological dimension of literary studies. However, it should
                                be noted that they still build upon simple wordcount (e.g., keyness
                                analysis is generally based on raw word frequencies).</p></note>
                        Richer (pre)processing pipelines, that also tap into syntax and discourse,
                        might allow stylometry to revitalize its connection with literary theory,
                        but come with significant barriers for non-Anglo-Saxon literatures.</p>
                    <p>In their thought-provoking article, <ptr target="#hirst-feiguina2007"/>
                        already proposed a method that took into consideration not only word count,
                        but also syntactic tags as features for stylometric analysis. By working on
                        the results of a partial parser, which performs <cit><quote rend="inline">a
                                structural analysis that is more than mere chunking but less than
                                the parse tree of a fully recursive grammar</quote>
                            <ptr target="#hirst-feiguina2007" loc="408"/></cit>, it became possible
                        to <q>measure</q> the syntactic structure of sentences. Hirst and Feiguina
                        tested this method on the attribution of the works of the Brontë sisters,
                        reaching some very promising accuracy scores. Curiously enough, however, the
                        method did not find extensive application in the following years. While <ptr
                            target="#eder2015"/> cited it as one of the most promising methods,
                        approaches like <title rend="italic">Delta</title> (a similarity measure
                        based just on the most frequent words) have dominated the field of
                        stylometry, with just a few attempts <ptr target="#vancranenburgh2012"/>
                        <ptr target="#frontini2017"/> to include part-of-speech tags and
                        phrase-structures as well.</p>
                    <p>The main reason for such a forgetfulness is the fact that similar approaches
                        have proved quite inefficient when compared to others in competition-like
                        setups. In the computational linguistics community, the most relevant of
                        these setups is PAN (<title rend="italic">Plagiarism Analysis, Authorship
                            Identification, and Near-Duplicate Detection</title>), <note><p><ptr
                                    target="https://pan.webis.de/"/></p></note> a competition held
                        each year in the context of the Conference and Labs of the Evaluation Forum
                        (CLEF) conference, where teams of programmers are invited to write scripts
                        to solve a series of shared tasks, generally focused on authorship
                        attribution. This <q>shared task</q> can be considered in general as a good
                        research practice, as it implies a criticism of tools that are constantly
                        rethought and improved through competition <ptr target="#gius2019"/>.</p>
                    <p>Even if the final goals of stylometry clearly move away from simple
                        authorship attribution, aiming at a quantitative characterization of
                        authorial style, it should be noted how attribution is still the most
                        frequently adopted when verifying the efficiency of stylometric methods (cf.
                            <ptr target="#burrows2002"/>
                        <ptr target="#eder2015"/>
                        <ptr target="#evert2017"/>). In fact, if it cannot be proven that a computer
                        is actually able to tell apart the style of two authors, any approach that
                        adopts it to study authorial style can be easily dismissed as frail or
                        inconsistent. </p>
                    <p>An overview of the methods presented for the authorship identification task
                        at PAN 2014 notes how the features preferred by participants are <quote
                            rend="inline">low-level measures</quote>, such as <quote rend="inline"
                            >character measures (i.e., punctuation mark counts, prefix/suffix
                            counts, character n-grams, etc.) or lexical measures (i.e., vocabulary
                            richness measures, sentence/word length counts, stopword frequency,
                            n-grams of words/stopwords, word skip-grams, etc.)</quote>. <ptr
                            target="#stamatatos2014" loc="16"/> Only one approach was based on
                            <q>high-level</q> features, obtaining some of the lowest scores. Hirst
                        himself participated in the 2012 PAN competition, getting the worst result
                        in the <q>authorship attribution</q> task.<note><p><ptr
                                    target="https://pan.webis.de/clef12/pan12-web/author-identification.html"
                                /></p></note> Finally, in one of the most recent PAN competitions,
                        focused on multi-authored fanfiction texts, just a few teams used POS tags
                        and they were clearly outperformed by those who chose the simplest methods,
                        based on character and word n-grams <ptr target="#kestemont2018"/>. In
                        conclusion, notwithstanding the originality and the theoretical consistency
                        of the idea, the approach seems to prove inefficient when applied to an
                        effective measuring of style. Reasons may be many, starting from the
                        necessary automated (pre-)processing of texts, which might generate errors
                        in itself. In addition, overfitting may be a source of error in machine
                        learning approaches (that are more and more frequently used in stylometry):
                        a combination of too many high-level features can make a model strong for a
                        specific case study (see the Brontë sisters in <ptr
                            target="#hirst-feiguina2007"/>), but can also hinder its
                        generalizability. From this point of view, simple wordcount might prove less
                        insightful, but much more stable. Finally, it is undeniable that, while NLP
                        tools have been developed extensively for English, efficient applications to
                        other languages are still missing, where research is a few – if not many –
                        steps behind. Therefore, while wordcount proves reliable whatever the
                        language of application <ptr target="#eder2015"/>, the same cannot be said
                        with approaches that rely on <q>high-level</q> features. </p>
                    <p>However, during the past few years, the interest towards more
                            <q>linguistically informed</q> approaches has never fully extinguished.
                        In fact, there is the impression that state-of-the-art stylometric methods
                        work efficiently not because they are able to catch the very nature of
                        style, but just because they scrape the surface of a phenomenon that has
                        much more profound implications. In fact, by modeling stylistic distance as
                        a statistical difference in the frequency of use of some words (or
                        characters), stylistic choices such as syntactic construction and discourse
                        structuring might be implicitly modeled (as they determine these very
                        frequencies). However, such implicit modeling catches them only indirectly,
                        and thus incompletely, see also <ptr target="#bubenhofer-dreesen2018"/>.</p>
                    <p>In recent years, positive experimental results have finally started to
                        support this theoretical need. In the PAN 2019 Cross-Domain Authorship
                        Attribution Task (once again, focused on fanfiction texts in four different
                        languages), one team proposed a fruitful integration between word- (and
                        punctuation-) count, stemming, text distortion, and POS-tagging. The tagging
                        was performed using Spacy,<note><p><ptr target="https://spacy.io/"
                            /></p></note> which reaches <cit><quote rend="inline">an accuracy that
                                varies from 95.29% to 97.23%</quote>
                            <ptr target="#bacciu2019"/></cit>. Notwithstanding the errors generated
                        by the tagger, the approach ranked second (and even first for Spanish texts,
                        cf. <ptr target="#daelemans2019"/>), confirming how, with an improvement of
                        NLP tools, the dream of a stylometry finally able to move beyond wordcount
                        and reach some more theoretically-pregnant dimensions might not be a dream
                        anymore.</p>

                </div>
                <div>
                    <head>2.3 Semantic Text Mining</head>
                    <p>Semantic Text Mining methods, such as sentiment analysis, topic modelling,
                        and word embeddings, apply tools for text analysis and visualization based
                        on external semantic information and co-occurrence methodologies. They offer
                        the potential of addressing key questions in literary theory and
                        narratology, from the identification of genre to the visualization of plot.
                        These emerging approaches are now beginning to broaden the scope of
                        computational literary studies and to open up new, still unexplored,
                        potentialities. But it is not only these potentialities that still await
                        exploration, but also the limitations and caveats of the methods'
                        application in humanities research that in many cases require more
                        systematic investigations. </p>
                    <p>Among these methods, topic models based on Latent Dirichlet Allocation (LDA)
                        and Gibbs Sampling <ptr target="#blei2012"/>
                        <ptr target="#steyvers-griffiths2007"/> have become particularly popular in
                        digital humanities research in recent years (see e.g. <ptr
                            target="#binder-jennings2014"/>
                        <ptr target="#mitrofanova2015"/>
                        <ptr target="#schoch2017"/>). They allow for explorations and analyses of
                        the content and the semantic structure of digital text corpora. They permit
                        researchers to model a corpus' content in terms of so-called <q>topics,</q>
                        groups of words which are apparently semantically related, and show the
                        distribution of these topics within the corpus. By observing single,
                        meaningful topics, scholars can scan large text collections for documents
                        relevant to a specific discourse, or estimate how the prominence of such a
                        discourse developed along the time axis (e.g. <ptr
                            target="#pavlova-fischer2018"/>). Texts can be sorted according to their
                        content, and it is also possible to derive content-related features for text
                        classification from topic models (e.g. <ptr target="#henny2018"/>). Thanks
                        to an increasing number of available tools and libraries, the method is, by
                        this day, accessible to a wide range of users<note><p>e.g. <ptr
                                    target="http://mallet.cs.umass.edu/"/> by <ptr
                                    target="#mccallum2002"/>; or <ptr
                                    target="https://dariah-de.github.io/TopicsExplorer/"/> by <ptr
                                    target="#simmler2019"/>.</p></note> (Figure 2). <figure>
                            <head>Interface of the DARIAHTopicsExplorer as an example for how
                                accessible topic modeling nowadays is to users. Among other results,
                                the interface produces a heat map overview of the distribution of
                                topics in the corpus (here, a small collection of English short
                                stories). This heat map shows us, among other things, that a topic
                                in which the words <q>mowgli</q> and <q>jungle</q> weigh most
                                heavily is very present in Kipling's <title rend="italic">The Jungle
                                    Book</title>.</head>
                            <figDesc>Screencapture featuring heatmap</figDesc>
                            <graphic url="resources/images/figure02.png"/>
                        </figure>
                    </p>


                    <p>The popularity of topic modeling in digital humanities combined with its
                        technical accessibility may suggest that it is a well understood method –
                        safe to use without a second thought – but this impression is misleading.
                        Basically, there are three issues to consider when using topic modeling: (1)
                        in order to use it, one has to make decisions that require a basic
                        understanding of the algorithm, (2) for many of these decisions there are
                        still no best practices and recommendations rooted in systematic, empirical,
                        methodological research, and (3) attempts to <q>validate</q> a computational
                        method for semantic analysis is generally not unproblematic.</p>
                    <p>(1) The methodology of topic modeling is rather intricate: LDA topic models
                        are based on relatively advanced probabilistic procedures, which are
                        probably understood only vaguely by many users. The method requires a number
                        of informed decisions that have to be taken by these users – about the
                        number of topics, iterations, chunk sizes, even hyperparameter settings and
                        the innumerable possible ways to preprocess the texts. </p>
                    <p>(2) Moreover, even after years of application in the field, it is still hard
                        to find clear recommendations and best practices concerning the choice of
                        model parameters or preprocessing steps in topic modeling. As <ptr
                            target="#du2019"/> points out in a survey on how topic modeling has been
                        used in digital humanities in the past, many studies do not even bother to
                        report the parameter settings of their experiments. If experiments are
                        reported in more detail, studies often copy the decisions that seemingly
                        worked well in other studies. The field would certainly benefit from more
                        thorough methodological research providing empirical grounds for clear and
                        systematic recommendations and best practices tailored towards the specific
                        demands of the field. </p>
                    <p>(3) But there is a more fundamental problem that affects the search for
                        parameters that produce good topic models: the impossibility to clearly
                        define a good topic model. In semantic text mining we attempt to map a
                        mathematical concept onto the much more elusive phenomenon of semantic
                                meaning.<note><p>This problem extends to other forms of semantic
                                text mining. For sentiment analysis, see <ptr target="#kiefer2018"
                                />.</p></note> Human readers are often stunned by the combinations
                        of words found in topic models. One of the most popular German tutorials on
                        topic modeling introduces the concept with an example topic composed of the
                        keywords “theater, actor, play, role, applause...”.<note><p><ptr
                                    target="#horstmann2018"/>
                                <ptr target="https://fortext.net/routinen/methoden/topic-modeling"
                                /></p></note> It is obvious to the reader that grouping these words
                        together seems to be meaningful, and that is why scholars are attracted to
                        topic modeling. But to evaluate the method, it is necessary to quantify how
                        meaningful this combination is compared to others.</p>
                    <p>Other areas of text mining can deal with that kind of problem much more
                        straightforwardly. To evaluate a method for authorship attribution or
                        POS-tagging, we can create test sets containing instances the classes of
                        which we already know and observe how many texts are attributed correctly to
                        their authors, or how many words are tagged correctly with their respective
                        POS tags. But what is the <q>correct</q> semantic description of a fictional
                        text? Some approximations have been tried. Probably the closest to the
                        intuition of meaningfulness of a topic is the word intrusion method <ptr
                            target="#bhatia2018"/>
                        <ptr target="#lau-baldwin2016"/>, where a random word is added to a topic
                        and human annotators are tasked to identify it. If a random intruder is easy
                        to spot, the original words of the topic are considered semantically close.
                        Another popular idea is that a meaningful interpretable topic can be
                        identified by measuring the semantic coherence of its keywords-based
                        co-occurrence in an external source (usually wikipedia), e.g. in terms of
                        Pointwise Mutual Information (PMI). A third approach is based on previous
                        knowledge about the internal structure of an evaluation corpus: if the
                            <q>content topics</q> of the texts in a collection are known in advance,
                        thanks for example to key words provided by the authors, a good topic model
                        could be defined as one that allows a classifier to attribute texts to
                        keywords using the topic distributions as features <ptr target="#schoch2017"
                        />. These are all but approximations to our intuition of
                            <q>meaningfulness,</q> and future research may even show that they can
                        produce contradicting recommendations in some cases. <note><p>For example,
                                choosing a higher number of topics can improve classification scores
                                while producing less coherent topics in terms of PMI at the same
                                time (Keli Du, personal communication).</p></note>
                    </p>
                    <p>Despite the current challenges, topic modeling is a useful tool for exploring
                        the content – literally speaking, the topoi – of a corpus. It thereby can,
                        without a doubt, considerably assist us in hypothesis generation. However,
                        the fact that this method has both been factually established in the digital
                        humanities community for some years and is relatively accessible should not
                        lead us to believe that our understanding of how to apply it in our field is
                        exhaustive and complete, and that we can reliably base matter-of-fact
                        statements with ease on the results of topic modeling. To get to this point,
                        further research dedicated specifically to the methodology of topic modeling
                        and its application on research questions in digital humanities will be
                        required.</p>


                </div>
                <div>
                    <head>3. Recapitulation, Replication, Reanalysis, Repetition, or
                        Revivication</head>
                    <p>In Section 2, we examined textometry, stylometry, and semantic text mining as
                        three subfields of CLS that are driven by methodology. The different sub
                        sections each showed how very different tools can indeed be well-suited to
                        the intended tasks, requiring, however, both methodical skill and expert
                        knowledge of the content under scrutiny. The required knowledge of the inner
                        workings of the tool at hand was increasingly complex from textometry over
                        stylometry to topic modeling, while the required domain knowledge was almost
                        inversely graded from Apollinaire over general style markers to recurrent
                        topics in a small collection of English short stories. A main difference was
                        that the Apollinaire study is a fully fledged case study, while stylometry
                        and semantic text mining each discuss more generically the implications and
                        limitations of their tools.</p>
                    <p>For all types of studies, however, pre-processing steps, such as
                        tokenization, lemmatization, and part-of-speech tagging emerged as being of
                        high importance. They need to be rigorously gauged and even
                        epistemologically reflected, as for example the analytic relevance of basic
                        unit <q>word</q> and <q>part of speech.</q> It was demonstrated that
                        methodological and analytical experimentation is fruitful beyond the single
                            <q>word</q> level across approaches. However, this is where we enter
                        into the realm of methodological evaluation, testing whether method is
                        tailored to its task. In many fields, this includes the fundamental question
                        whether replication of a study's results is possible. In the present section
                        on <title rend="italic">Recapitulation, Replication, Reanalysis, Repetition,
                            or Revivication,</title> we will discuss an example of how replication
                        can in fact be addressed in a digital humanities setting such as CLS.</p>
                    <p>In 1973 John B. Smith <ptr target="#smith1973"/> published an article on
                            <title rend="quotes">Image and Imagery in Joyce's Portrait: A
                            Computer-Assisted Analysis</title> with at its heart a visualization of
                        the intensity of verbal images in the text. As Smith points out, commenting
                        on the visualization, the richness of the imagery peaks at the end of
                        Chapter 1, as he expected, at the <q>pandybat</q> episode. <note><p>A
                                pandybat is a stiff leather strap with a handle that was used for
                                punishing students in Ireland.</p></note>
                        <figure>
                            <head>Smith's Visualization</head>
                            <figDesc>Photocopied line chart where the y-axis is <title rend="quotes"
                                    >Volume per Unit</title> and the x-axis is <title rend="quotes"
                                    >Word Units</title></figDesc>
                            <graphic url="resources/images/figure03.png"/>
                        </figure>
                    </p>


                    <p><emph>How did Smith get his visualization (Figure 3) and does it do what he
                            says it does?</emph> We set out to try to replicate this visualization
                        as a way of understanding Smith and early experiments in textual
                        visualization. This is part of a larger project which, like <title
                            rend="italic">Hermeneutica</title>, is a hybrid combination of book <ptr
                            target="#rockwell-sinclair2016"/> and tool (<ptr
                            target="https://voyant-tools.org"/>). Drawing on this research the
                        following section will: <list type="unordered">
                            <item>Talk about Smith's visualization and how it was brought back to
                                life,</item>
                            <item>Show some examples of experiments in revivification of techniques,
                                and </item>
                            <item>Reflect on what the practice might be doing and what we might call
                                it.</item>
                        </list>
                    </p>


                    <p>How did Smith generate the visualization of Joycean imagery? What Smith did,
                        according to the article, was to develop a custom dictionary of some 1,300
                        terms with emotional valence to track and visualize image intensity or
                        volume through the novel. He divided the text into 500-word chunks,
                        approximately the number of words on a page, as he puts it, and he then
                        counted the number of imagery words from his dictionary in each chunk,
                        weighting some, and graphed the results. <figure>
                            <head>Replication of Smith's Visualization</head>
                            <figDesc>Digitally generated line chart</figDesc>
                            <graphic url="resources/images/figure04.png"/>
                        </figure>
                    </p>

                    <p>When we tried to reproduce his results, Figure 4 is what we got. Not quite
                        the graph that Smith got, but similar. We developed our revivified
                        interpretation of Smith's technique, or <q>Zombie tool,</q> in a Jupyter
                        Python notebook using a version of the text from Gutenberg. The first
                        difficulty we ran into recapitulating Smith was reconstituting his
                        dictionary of words with emotional valence. Fortunately, Smith is still
                        alive and he pointed us to an appendix to his book on Joyce titled <title
                            rend="italic">Imagery and the Mind of Stephen Dedalus</title>
                        <ptr target="#smith1980"/> which we OCRed and corrected in order to recreate
                        his method. <figure>

                            <head>Part of Table of Imagery Words from <ptr target="#smith1980"
                                /></head>
                            <figDesc>Alphabetized table lsiting words along with their frequency per
                                chapter</figDesc>
                            <graphic url="resources/images/figure05.png"/>
                        </figure> Smith also explained in email correspondence that for his graph he
                        didn't use raw counts per segment but instead weighted each count by the
                        logarithm of the frequency of the word in the entire text – this serves to
                        add more importance to higher frequency terms.</p>

                    <p>Alas, our updated graph,<note><p>See <ptr
                                    target="https://github.com/sgsinclair/epistemologica/blob/master/Smith-Imagery.ipynb"
                                /></p></note> despite having access to the original data beyond the
                        published paper, still doesn't really match the original results, which
                        raises questions about the significance and purpose of such replications.
                        Does it call into question Smith's interpretation of Joyce, or his model, or
                        his method of implementing the model, or is our replication at fault? Did
                            <emph>we</emph> miss something? <figure>
                            <head>The Jupyter Notebook</head>
                            <figDesc>Screencapture of code notebook with description and code
                                snippet</figDesc>
                            <graphic url="resources/images/figure06.png"/>
                        </figure>
                    </p>
                    <p>One of the issues that arises when you try bring <q>historical</q> research
                        and associated tools back to life is the question of how accurately to
                        redevelop the tools used in the replication. Does one need to use exactly
                        the same tools as in the original? In this case, should we be working on a
                        mainframe using something close to the analytical environment used by Smith?
                        In Figure 6 you can see a screenshot of the Jupyter Python notebook, which
                        is the environment we decided to use for this revivification. We chose the
                        notebook model as it encourages a programming style where you explain what
                        you are doing in the spirit of replication rather than emulating the
                        original environment. Ironically, that means our code is by definition
                        different from Smith's original because it is implemented in a deliberately
                        reflective literary programming environment.<note><p>As part of the larger
                                project we are developing an alternative to Jupyter called Spyral as
                                in <q>spiral notebook.</q> It is notebook environment that is also
                                an extension of Voyant (<ptr target="https://voyant-tools.org"/>) so
                                you can call Voyant tools within notebooks. It is based on
                                JavaScript rather than Python because that is what a lot of the
                                Voyant interface is developed in. We hope it will give users of
                                Voyant a way to extend their explorations. Spyral is available at
                                    <ptr target="https://voyant-tools.org/spyral/"/>.</p></note></p>

                    <p>One might wonder why we wanted to revivify Smith's work? Why bring back work
                        that is largely forgotten in the digital humanities community?</p>

                    <p>One reason to recover Smith was that he was one of the first to reflect on
                        visualization and computer criticism. Despite being forgotten, his
                        visualization is interesting because it is one of the first text
                        visualizations published not as part of a technical document, but in a paper
                        addressed to literary critics. Smith was a pioneer in visualization and
                        criticism, as a later 1978 paper in the journal <title rend="italic"
                            >Style</title> titled <title rend="quotes">Computer Criticism</title>
                        <ptr target="#smith1978"/> showed. As such you could say that our project
                        was one of media archaeology – recovering a mediating technology
                        (visualization of emotional imagery) that has recently seen a revival in
                        various forms, including sentiment analysis.</p>

                    <p>A second reason was to revisit some of the methods that Smith pioneered and
                        the ideas they bore. The problem with tools is that like all Zombies they
                        don't last that long without human reuse and no one pays them much attention
                        when they are working. Who remembers ARRAS <ptr target="#smith1984"/>,
                        arguably one of the most influential early tools in the history of
                        humanities computing?<note><p>ARRAS was one of the first tools to be
                                designed to be used for interactive exploration of a text rather
                                than in a batch mode. It influenced subsequent tools like TACT and
                                eventually Voyant.</p></note>
                        <cit><quote rend="block">Part of the reason instruments have largely escaped
                                the notice of scholars and others interested in our modern
                                techno-scientific culture is language, or rather its lack.
                                Instruments are developed and used in a context where mathematical,
                                scientific, and ordinary language is neither the exclusive vehicle
                                of communication nor, in many cases, the primary vehicle of
                                communication. Instruments are crafted artifacts, and visual and
                                tactile thinking and communication are central to their development
                                and use.</quote>
                            <ptr target="#baird2004" loc="XV"/></cit>
                    </p>

                    <p>Following <ptr target="#baird2004"/> we believe that tools can and do bear
                        knowledge, including theories of interpretation, just as discourses do, but
                        they bear them differently. This raises the question of how one interprets
                        tools if they can bear knowledge. Obviously one approach is to try to use
                        them as they were used at the time, but how does one use a dated tool when
                        you don't have the code or the platform to run it on? In other words,
                            <emph>how does one bring something back to life so one can see how it
                            worked in bearing knowledge?</emph></p>

                    <p>Our answer is: interpretative replications, or what we playfully call
                            <q>zombie tools.</q> The idea is to replicate the interpretative ideas
                        and methods rather than the particular materiality of the tool. The
                        advantage of the notebook style of programming for replications is that they
                        bring the interpretation of the replication to the fore, allowing one to
                        create a type of Frankenstein's monster, a hybrid of explanation and code
                        that mutually preserve knowledge. In other words, a zombie or revenant
                        created through reanimation of the ideas.</p>

                    <p>What was interesting to use about Smith's work was how he uses text analysis
                        and visualization to model a theory of interpretation drawn from the novel
                        he is interpreting. This allowed him to show how he believes Joyce applied
                        to his own writing the aesthetic theory of artistic imagery presented
                        explicitly by Daedalus, the lead character in Chapter V of the <title
                            rend="italic">Portrait</title>
                        <ptr target="#joyce1968"/>. In other words, Smith translated his
                        interpretative thesis into a software tool or model that could then be
                        evaluated by a computer. His thesis was an interpretation of a widely
                        discussed aesthetic theory drawn from the novel itself. He applied the tool
                        back onto the novel as a way of evaluating his model and ultimately testing
                        whether Joyce actually followed his own aesthetic theory. </p>

                    <p>For our purposes, the computational model of Joyce's imagery and whether it
                        translates the hermeneutical thesis is not that important. What we propose
                        is interpretative replication as a way of bringing ideas and techniques back
                        to life and that this could be important to computer criticism and
                        stylistics. To that end we want to make a number of points about all the
                            <q>re-</q>words that we can use: Revivification, Recapitulation,
                        Recapture, Recovery, Replication, Reflection, Reproduction, Relive, Respond,
                        Reinterpretation, Revenant ...</p>
                    <p>First of all, what this project is not trying to do is scientific
                        replication. As <ptr target="#collins1975"/> notes in <title rend="italic"
                            >The Seven Sexes: A Study in the Sociology of a Phenomenon, or the
                            Replication of Experiments in Physics</title>, there is a lot of
                        enculturation going on in what gets called <q>scientific reproduction.</q>
                        Replication, in Collins' account, is not straightforward, as almost no
                        complex experiment can be described in sufficient detail to be replicated
                        precisely without having to make assumptions, as we did in the Smith case.
                        Collins goes further to suggest that what is often needed is the <emph>tacit
                            knowledge</emph> that only shared training, close communication or
                        shared researchers moving between labs can provide. Thus, enculturation is
                        the combination of training and negotiation that leads to experiments being
                        considered for replication in the first place and then ensures that the
                        experiments are replicated in a fashion that extends the field. In short,
                        sociologists of science point out how replication is a practice that in each
                        field is constructed by the field. The question for us in the digital
                        humanities then is how do we want to construct replication? <emph>What
                            culture of replication do we want to develop in the humanities? What
                            traditions can we draw on?</emph></p>

                    <p>One possible tradition is what gets called <cit><quote rend="inline"
                                >experimental archaeology</quote>
                            <ptr target="#ascher1961"/>
                            <ptr target="#millson2010"/></cit>. While this set of practices has its
                        problems, we can learn from the way they are defining the replication in
                        their field. For example, in <ptr target="#outram2008"/>'s <title
                            rend="quotes">Introduction to experimental archaeology</title> he talks
                        about some of the types of problems with reproductions that are not
                        sufficiently scientific for what they want to define as truly experimental,
                        in the sense of replicable experiments: <list type="ordered">
                            <item>Lack of clear aims</item>
                            <item>Insufficient detail on materials and methods</item>
                            <item>Compromises over authentic materials</item>
                            <item>Inappropriate parameters</item>
                            <item>Lack of academic context </item>
                        </list> We assume that generally for (digital) humanities scholars, the
                        ultimate goal of research is not proof, or prediction, but
                                    understanding.<note><p><q>Humanistic understanding</q> may very
                                well encompass phases of hypothesis testing and predictive methods
                                (machine learning algorithms). Eventually, however, these steps
                                serve to enlighten us about the conditions and effects of people’s
                                interaction with culture, meaning, and, generally, life.</p></note>
                        When applied to interpretative replication, our aim is therefore not, for
                        example, to use only authentic materials, but to learn about the potential
                        today of past methods. We agree that aims should be clear, but they do not
                        have to be to prove that Smith was right or wrong, especially given that we
                        do not have all the materials and the exact weights for words. Instead, in
                        interpretative replication and its failures, we learn about ourselves: what
                        we know and do not know. We unpack our assumptions and imagine what we could
                        do rather than learn only about Smith. </p>

                    <p>An alternative practice that does not aim for scientific credibility is what
                        the artist Judith Buchanan <ptr target="#buchanan" loc="n.d."/> calls <quote
                            rend="inline">revivification</quote>. What she and her performers do is
                        to bring to present life and understanding silent films of Shakespeare
                        plays. She does this through a combination of lecture, commentary and live
                        performance while the film plays. She calls it <q>revivification</q> because
                        what they do is collaborate with the dead performers (of the silent films)
                        to create renewed knowledge. When you attend a performance you find that she
                        lectures, live actors perform certain lines, and the original silent films
                        play out on the screen. This is replication enriched by her interpretation
                        and that of the actors with the goal of understanding the phenomena.</p>

                    <p>Her theorization is appealing as it acknowledges that any replication is an
                        interpretation of the replicated. We in the humanities are in a
                        collaboration with the dead. We engage with them rather than just stand on
                        their shoulders. The humanities have a different relationship with our
                        histories than the sciences. The hermeneutic circularity of Smith, who draws
                        theory from a novel in order to interpret the novel, is appropriate to the
                        humanities in a way it would not be to most sciences. There is a similar
                        hermeneutic circularity to interpretative replication. It is both about
                        understanding ourselves and the past. It is a thinking-through of the past. </p>

                    <p>For all these reasons we see replication in the digital humanities as a
                        practice of replying, and re-doing in a larger dialogue, but also re-plying
                        in the sense of folding back onto our past. This is appropriate in the
                        humanities where we eat our past to stay current.</p>

                </div>
                <div>
                    <head>4. Domain Specificity. The Example of Plotting Poetry</head>

                    <p>In this section, we shift our focus from the perspective of tools, methods
                        and replication towards a <emph>systematic area of application,
                            poetry</emph>. The choice of that area is arbitrary. However, the
                        situation of poetry is somewhat specific. Versification is intrinsically
                        related to various formal measures, and was a subject of quantitative
                        analyses long before any computational methods were available for counting
                        meters, feet, syllables, rhymes and such. However, readily available tools
                        developed for the computational study of literary texts – though they can
                        sometimes be applied to poetry – are often not a perfect fit for versified
                        texts, due to the intrinsic specificities of the highly regulated material
                        that is verse. As for the poetry-specific devices being produced, their
                        transfer from one text to another is made more difficult by language and
                        time-period differences as well as by individual variations in the use of
                        rules between poets. Furthermore, the general challenge – not restricted to
                        poetry – posed by the utter diversity of hermeneutic goals, is made somewhat
                        more acute by the relatively smaller number of scholars working on poetry. </p>

                    <p>Because poetry is so fundamentally linked to a number of forms, and because
                        the variation and creativity in the form are so essential to the production
                        of meaning in poems, the use of quantitative methods for the study of poetry
                        is first geared towards versification. Meters, rhymes, caesuras, stresses
                        can all be very aptly studied with the use of computational methods.
                        Progressively, the fastidious manual collection of data is being replaced as
                        much as possible with automatic data collections. Computational tools are
                        being produced to detect or predict metric syllables, rhyming phonemes,
                        etc., but this effort is somewhat disseminated and has yet to produce
                        universally performant and flexibly adaptable tools. Indeed, the rules of
                        versification form a vast and variable series of systems. </p>

                    <p>The very principles of meter vary from one language to the other, with tonic,
                        syllabic, syllabo-tonic and other systems requiring very different rules to
                        be detected. Furthermore, even within one language area, different eras –
                        and different poets – have produced a range of practices within
                        versification systems. Such systems are often quite strict, and have a high
                        interpretative value, yet remain singular to a language, time period,
                        sub-genre, or to a specific form (the sonnet, for instance, has its own set
                        – or sets – of strict rules). These telling systems being so worthy of a
                        close inspection, and of a systematic one, researchers are actively
                        producing their own tools to automatically collect precise data to quantify
                        verse features. There is, obviously, no obligation to focus solely on
                        genre-specific features of poetic texts, and tools that are useful in
                        analyzing prose texts may be simply transposed to the study of poetry. Yet
                        in doing so, one has two obstacles to overcome: the resistance of the
                        material itself, and the hermeneutic relevance of the endeavor. </p>
                    <p>There is, obviously, no obligation to focus solely on genre-specific features
                        of poetic texts, and tools that are useful in analyzing prose texts may be
                        simply transposed to the study of poetry. Yet in doing so, one has two
                        obstacles to overcome: the resistance of the material itself, and the
                        hermeneutic relevance of the endeavor.</p>

                    <p>First, the use of language in poetic texts is particularly prone to
                        anomalies, constrained as it is by both the corset of verse and the effort
                        towards an economy of words and depth of meaning. Poetry often relies on the
                        ambiguity, the polysemy, the shortcomings of language, to produce a richer
                        layering of less obvious meanings. Automatic taggers of any kind struggle
                        with this linguistic prolificity, such as the lack of a clear and linear
                        syntax, higher degree of polysemy, redundancy, or words being used with
                        unexpected or isolated meanings. POS-taggers, for instance, wrestle with the
                        relatively frequent misuse or category change of words in poetry, as well as
                        with the unusual word order, which is unfortunate as POS-categories are very
                        useful to the study of poetry and to the understanding of versification.
                        Similarly, topic modeling and other clustering methods suffer from the lack
                        of linearity of a versified text, where the verse unit as a structural
                        element plays an important role in the construction of meaning, alongside
                        the syntactic units and the physical proximity, with which it may agree or
                        disagree.</p>

                    <p>Second, stylistic analyses designed for prose might not be as relevant, on
                        their own, once applied to poetry. In focusing on relatively less
                        foregrounded structures, they risk missing the point of a text by failing to
                        address its genre-specific and language-specific form: its versification,
                        its controlled polysemy, the significations of its rhymes, enjambments,
                        choice of meter, degree of obedience to the rule. The latter issue, although
                        not specific to digital humanities, is at the core of how digital humanities
                        and the study of poetry interact, for a focus on versification is bound to
                        produce large amounts of data. Versification descriptions typically include
                        fine information not just about each poem, stanza, rhyme, line, hemistich
                        and syllable group, but also numerous information about each syllable and
                        about its parts. This need for a high granularity has long prompted
                        versification scholars to try and mechanize the handling, if not the
                        collection of such data. </p>

                    <p>There are, thus, two broad ways to explore poetic corpora with the help of
                        computational methods. One is to focus on features shared with prose, and
                        for this, one can borrow any of the more generic tools of digital literary
                        studies, such as TXM in France (see above), or CATMA (<ptr
                            target="https://catma.de"/>). Researchers may use parts of speech
                        distribution, word frequencies, topic modeling, word vectors, even sentiment
                        analysis, to gather data of ever-improving quality, although not necessarily
                        as precise, and thus as meaningful and interpretable as data collected by
                        learned humans. Machine Learning, deep learning, neural networks are also
                        used, producing convincing results in authorship or genre attribution for
                        instance, whilst so far, the black box effect of such endeavors bars the
                        traditional untangling of how poetic devices function. In using these tools,
                        not designed with poetry in mind, one important challenge for the researcher
                        is to make good use of them, to gather enough hermeneutical benefit, to get
                        insight into new or crucial issues, and to avoid banalities.</p>

                    <p>The other approach, focusing on poetry-specific features, is seldom addressed
                        by readily available tools. The rules of versification are so
                        language-specific that a tool developed for one language might need
                        considerable adaptations to be reused in another. As in other literary
                        genres, it is difficult to predict the many features that poetry scholars
                        might want to model, and stylistic questions geared towards interpretation
                        tend to focus on features so exclusively characteristic that a tool
                        developed by one team to describe one phenomenon, although it may form part
                        of a further exploration taking said phenomenon into account, might not fit
                        the precise needs of another team. </p>

                    <p>Although the very object's lack of universality is an obstacle to the
                        interoperability of devices, there is a genuine need for tools tailored for
                        poetry and usable for a range of research questions. Such tools are
                        progressively being developed, mostly by researchers trying to address their
                        own needs, in particular for the exploration of versification features <ptr
                            target="#bories2021"/>
                        <ptr target="#plechac2021"/>. Some teams procure fully integrated
                        applications, many share Python packages for all to use, and many more
                        researchers – the group <title rend="italic">Plotting Poetry</title> now has
                        65 members<note><p><ptr target="https://www.plottingpoetry.org"/></p></note>
                        – develop precise methods to fit their own research goals, without having a
                        neat tool to share with the wider community. To mention but a few, Valérie
                        Beaudouin's Métromètre <ptr target="#beaudouin2002"/>, Eliane Delente and
                        Richard Renault's Malherbe <ptr target="#delente-renault2015"/> or Benoît
                        Brard and Stéphane Ferrari's work for French <ptr
                            target="#brard-ferrari2015"/>, Klemens Bobenhausen's Metricalizer for
                        German <ptr target="#bobenhausen-hammerich2015"/>, Daniele Fusi's Chiron for
                        Latin and Ancient Greek <ptr target="#fusi2015"/>, more recently the
                        Postdata project's Anja, Skas and Disco for Spanish <ptr
                            target="#martinezcanton2017"/>, Arto Anttila and Ryan Heuser <ptr
                            target="#anttila-heuser2016"/> for English and Finnish, Petr Plecháč and
                        Robert Kolár's versologie tools <ptr target="#plechac-kolar2017"/>, Igor
                        Pilshchikov and Anatoli Starostinor's <ptr
                            target="#pilshchikov-starostin2015"/> or David Birnbaum and Elise
                        Thorsen's works <ptr target="#birnbaum-thorsen2015"/> for Russian, all
                        automatically analyze various components of verse. Further efforts towards
                        interoperability are made, and one must salute the teams who provide
                        well-thought web ontologies focused on versification phenomena, such as the
                        Postdata project.<note><p><ptr
                                    target="https://github.com/linhd-postdata/Network-of-ontologies"
                                /></p></note></p>

                    <p>Besides the purer automatic detection efforts, where the quest for data is so
                        time and energy consuming that it risks delaying the hermeneutical goals,
                        many computer-aided poetry researchers mix manually collected and
                        automatically collected data, to achieve a compromise, get smaller, high
                        quality datasets, and allow text interpretation based on a mix of distant
                        and close reading, one informing, testing and guiding the other <ptr
                            target="#bories2020"/>. And some scholars simply use data management
                        tools, such as Excel or Matlab, to handle data collected manually, typically
                        versification data to explore the stylistic relevance, the poetics of
                        versification routines, or the evolution of practices, sometimes other types
                        of data, for instance to reflect on the diachronic reception of a sub-genre,
                        or on more contingent reader response.</p>
                    <p>Another aspect of digital poetry studies to be mentioned is the production of
                        various poetry generators, which evolved from relatively simple literary
                        devices such as Raymond Queneau's <title rend="italic">Cent mille milliards
                            de poèmes</title>
                        <ptr target="#queneau1961"/> to elaborate computer programs such as Pablo
                        Gervas' WASP.<note><p><ptr
                                    target="http://nil.fdi.ucm.es/index.php?q=node/206"/></p></note>
                        Their development and examination through initiatives such as Thierry
                        Poibeau and Valérie Beaudouin's OUPOCO project <note><p><ptr
                                    target="http://www.transfers.ens.fr/l-oupoco-l-ouvroir-de-poesie-combinatoire"
                                /></p></note> provides remarkable insights into what constitutes the
                        uniqueness of a poet's voice.</p>

                    <p>Poetry studies, thus, are still in the process of developing both
                        interoperable tools, and more uniquely tailored methodologies, with a
                        majority of researchers so far piecing together their own methods, mixing a
                        variety of programming, data analysis and visualization techniques with
                        manual data collections, with or without a view to interpretation. In this
                        observation, we wish to stress that the journey towards relevant
                        quantitative stylistics research is not always linear, nor should it be; the
                        devices developed should continually feed a methodological reflection and
                        reuse, and their applications should be tested and renewed, so as to improve
                        hermeneutical benefit. The group Plotting Poetry, founded by Anne-Sophie
                        Bories and now part of the SIG-DLS, gathers scholars of very diverse
                        geographical, linguistic and indeed methodological realms around a yearly
                                    conference.<note><p><ptr target="https://www.plottingpoetry.org"
                                /></p></note> This platform for the sharing of practices,
                        challenges, and results, is fostering collaborations, and through those, the
                        emergence of well-needed interoperable tools. </p>

                </div>
                <div>
                    <head>5. The Digital Literary Stylistics-Tool Inventory (DLS-TI)</head>
                    <p>The above has shown that CLS is defining itself in many facets, including
                        chief methodological areas such as textometry, stylometry, and semantic text
                        mining, but also in very specific applications, for example to the genre of
                        poetry, and with regard to its own range of types of replication. We have
                        shown that this definition draws to a large extent from the tools and
                        methods applied in concrete research. Systematically assessing the usage of
                        tools is not only a way of taking stock and boosting information, but also
                        to foster replication/re-analysis and comparability. Moreover, and crucially
                        for our analysis, it provides a way to analyze how different tools embody
                        different hermeneutical approaches and address different methodological
                        needs, how one and the same tool can be used by different communities with
                        different goals, how one community may resort to various tools to target
                        different stylistic phenomena.</p>

                    <p>The idea of gathering information on tool usage from the community is not
                        new, and has been inspired by pre-existing initiatives. An example that
                        seems particularly interesting is the LRE-map.<note><p><ptr
                                    target="http://lremap.elra.info/"/></p></note> Launched at LREC
                        2010 and soon extended to other conferences, it has been collecting data on
                        the use of Language Resources in submitted papers <ptr
                            target="#calzolari2012"/>. Opposite to traditional catalogues, the
                        LRE-map typically has several entries for the same resource, corresponding
                        to the different papers in which their use is described. Opposite to other
                        catalogues of language resources, where entries are compiled by creators and
                        thus mostly represent their point of view on resource and its intended uses,
                        the LRE map has developed over time to represent rather the user's point of
                        view i.e. the way in which it was applied in actual research. A lexical
                        resource such as WordNet, for instance, is often used as an ontology in the
                        LRE map, for this is the way it is often applied in NLP today. Also, the
                        diachronic span of the LRE map allows for the detection of trends in the use
                        of resources over time. In part, the success of the LRE-map is due to the
                        decision to work with a very limited set of metadata, and to the bottom-up
                        collection of data in conjunction with large events.</p>

                    <p>Within the field of DH various notable comparable initiatives can be named.
                        We cite among others the DIRT directory,<note><p><ptr
                                    target="http://dirtdirectory.org/"/> (offline at the time of
                                writing)</p></note> a comprehensive catalogue of digital tools,
                        albeit without links to use cases; the Catalogue of Digital
                                    Editions,<note><p><ptr
                                    target="https://dig-ed-cat.acdh.oeaw.ac.at/"/></p></note> an
                        interesting example of community driven collection of metadata for a
                        particular type of resource; the review of Tools and Environments for
                        Digital Scholarly Editing<note><p><ptr
                                    target="https://www.i-d-e.de/cfr-tools/"/></p></note> is
                        currently ongoing by the German Institute for Documentology and Scholarly
                        Editing (IDE); the ACDH Tool Gallery, linked to a series of training
                        workshops, as well as the recently launched TAPoR database of
                                    tools,<note><p><ptr target="http://tapor.ca/home"/></p></note>
                        drawn from the ADHO-proceedings <ptr target="#barbot2019"/>. <note><p><ptr
                                    target="https://www.oeaw.ac.at/acdh/events/event-series/acdh-tool-gallery-52/"
                                /></p></note>
                    </p>

                    <p>The idea of a Tool Inventory curated by the SIG-DLS (DLS-TI) <note><p><ptr
                                    target="https://dls.hypotheses.org/774"/></p></note> has
                        developed out of such initiatives, as well as from the epistemological
                        reflections outlined in the introduction to this paper, as a first attempt
                        to gather information on tool usage in CLS, in a way that would do justice
                        to all different approaches and perspectives currently existing in the
                        field, and the following paragraphs offer an overview of this. Accordingly,
                        the definition of <q>tool</q> adopted sees them as incarnations of methods.
                        Tools, just like other types of resources such as corpora, lexicons,
                        grammars, etc. are not neutral, but often the reification of theoretical a
                        priori. </p>

                    <p>From the practical point of view our initiative draws from the aforementioned
                        previous initiatives in that: <list type="unordered">
                            <item>it is based on bottom up contributions of descriptions by
                                researchers; </item>
                            <item>it is intended to be use-case-oriented; the idea is not just to
                                collect lists of tools but concrete real-world usage; crucially we
                                aim to collect contributions which do not come from the developers
                                of the tool; </item>
                            <item>it relies on a simple method of collection and a limited set of
                                descriptors, aimed at identifying the name and type of tool,
                                including with reference to the TaDiRAH ontology, and the type of
                                use in real DH scenarios such as published research, such as papers,
                                books, blog posts, but also projects, or academic courses. </item>
                        </list>
                    </p>


                    <p>One line in the inventory represents a use case contribution. The same tool
                        may thus be entered several times, by different contributors or even by one
                        and the same contributor providing different use cases. As the aim is to
                        review existing practices in the community, the focus is on off the shelf
                        tools which are applied to diverse aims. </p>
                    <p>We decided to accept a broad but text-analysis oriented definition of
                            <q>tool,</q> defined as any method or technique that provides ways of
                        data manipulation and interpretation, including desktop GUIs, online Virtual
                        Research environment and libraries for R or Python (see above). In the
                        inventory, we also include general-purpose tools such as Excel spreadsheets
                        when they are used for central CLS tasks such as data manipulation, but
                        exclude, for instance, data acquisition libraries such as lxml for Python
                        used to scrape xml. Visualization tools are also taken into account, as this
                        task is a central one for the interpretation of data.</p>

                    <p>In 2018 the first call was launched to populate the inventory. To date, 35
                        use cases have been collected. The results are still limited, but some first
                        interesting trends can be already identified, such as the importance of R
                        packages and Python libraries for the community. In turn, this is linked to
                        the fact that many of the tools that have been entered are not specifically
                        designed for literary or stylistic analysis, but are implementations of
                        Natural Language Processing algorithms or statistics measures. On the other
                        hand of the spectrum, a number of entries refer to desktop applications
                        (TXM, WCopyFind, Docuscope, …) which are used for very specific purposes and
                        seem to be more specifically geared towards CLS. </p>

                    <p>Such preliminary results mirror our intuition about the current practices in
                        the community. On the one hand, there is a tendency towards the development
                        of highly specialized methods based on generic building blocks. On the other
                        hand, there is simultaneously the need for dedicated tools, with a
                        well-identified and -tested set of functions, which address both usability
                        and repeatability needs.</p>

                    <p>As we hope to gather an increasing number of entries from future campaigns,
                        we believe that the DLS-TI may become an important instrument to monitor the
                        needs of the community, which in turn may benefit those institutions and
                        infrastructures – such as CLARIN<note><p>The DLS-TI could also be a useful
                                addition to the CLARIN Resource Families initiative, see <ptr
                                    target="https://www.clarin.eu/resource-families"/></p></note>
                        and DARIAH ERIC and their national consortia - that are currently devoted to
                        creating resources for the DH community. Through its perspective on use
                        cases, it also allows scrutinizing methods epistemologically, in relation to
                        research questions and theoretical frameworks. Relatedly, as a systematic
                        inventory not only of tools, but also of case studies, the DLS-TI can
                        function as a basis for replication and reanalysis - and thus help progress
                        in the field. Finally, initiatives such as the DLS-TI could be beneficial in
                        identifying the needs of the community, and in deciding where best to invest
                        time and money be it in terms of development of user-friendly interfaces, or
                        in the organization of training and events that promote the exchange of best
                        practices.</p>

                </div>
                <div>
                    <head>6. Conclusions. The many facets of tool criticism</head>
                    <p>This intervention began as a series of <q>anatomies</q> of tools – a playful
                        concept applied to different approaches, which, we hope, documents relevant
                        vantage points within CLS. In addition to the predictable
                            <q>quantitative</q> ones, <term>validity</term> and
                            <term>replicability</term>, we have offered <term>hermeneutic
                            plausibility</term>
                        <ptr target="#winko2015"/> and <term>open-ended tinkering</term> as among
                        the ultimate causes or mindsets of computational literary studies. Having
                        said this, <term>validity</term>, <term>reliability</term>, and
                            <term>objectivity</term> of explanatory hypothesis testing do resonate
                        with much of what we believe computational literary studies are and should
                        be about. This apparent conflict in fact appears indicative of not only the
                        diversity of approaches within the emerging field, but also of that of
                        research stages. What seems crucial though, is to maintain an awareness of
                        one's aims and axioms within this rich conversation – which necessarily
                        involves a sense of (computational) method. </p>
                    <p>Rather than <q>just applying</q> tools prepared by some pioneering expert,
                        CLS scholars have become more interested and knowledgeable about methods and
                        their fit to research questions and data. Future cohorts of scholars may
                        take tested and tried methods for granted, and most of them may concentrate
                        on testing and exploring research questions, rather than on method
                        development (as still so much needed today in the case of Topic Modeling,
                        for instance). When computational methods have become reliable vehicles for
                        most, this means that fewer scholars need to concentrate on honing them. A
                        matter of division of tasks is only functional, however, where a critical
                        level of validation and practical experience has been reached and is
                        embedded as continuous practice. As we have shown, humanistic interactions,
                        not only with artefacts and phenomena of the past, but also with historical
                        tools and methods, form an important source of insight.</p>
                    <p>We may or may not have entered a phase of the digital transformation of
                        literary studies already in which digital methodology may be reintegrated
                        with the field and its various sub-fields. In any case, further maturation
                        is likely to produce a new type of division of labor, where continuous
                        development, calibration and reflection of tools is but one, albeit focal,
                        activity. </p>

                    <p>Taking the argument of <q>tool defense</q> voiced in the introduction one
                        step further, <q>the instrumentalist</q> dimension of algorithmic data
                        science <ptr target="#jones2018"/> is worthy of some positive attention:
                        using <q>an instrument</q> to further sophisticated literary modeling, for
                        example machine learning for predictive modeling, is as warranted as a
                            <q>truth-oriented,</q> or explanatory, type of modeling, whose goal is
                        to develop a mechanistic model of the process that originally produced the
                        observed data and which strives to estimate the parameters of this process.
                        Here, of course, critical enquiry into algorithmic logic is a precondition,
                        possibly at a larger scale than <q>just CLS.</q></p>
                    <p>A few other topics: Annotation is a scholarly universal <ptr
                            target="#unsworth2000"/>, and one of the externalizations of modeling in
                        DH practices <ptr target="#herrmannsubmitted"/>. Enrichment of raw texts by
                        annotation <ptr target="#rapp2017"/> is one of the key practices where
                        scholarly knowledge and theory are made explicit. This includes hermeneutic,
                        progressively altered annotations that depict a rather inductive approach
                            <ptr target="#gius-jacke2017"/>
                        <ptr target="#percillier2017"/> and rule-based annotation systems such as
                        MIPVU <ptr target="#herrmann2019"/>
                        <ptr target="#steen2010"/>, as well as the pertinent issue of inter-coder
                        reliability <ptr target="#kuhn2019"/>. </p>
                    <p>Another important aspect is modeling through metadata, as highlighted for
                        example in the TXM study. Metadata reside at the interface of
                        data-scientific and philological dimensions of literature studies, as they
                        allow adding multiple variables to the data model. Together with text
                        sampling and markup, metadata constitute the philological object, ideally in
                        an explicitly theory-driven way. In research practice, this also involves
                        important decisions about schemata, but also on missing or incorrect
                        entries. </p>
                    <p>A crucial dimension of CLS that was addressed in our discussion is
                            <term>source</term> criticism. Building corpora is no trivial task, as
                        they are where the research question, but also general and specific
                        assumptions about literary discourse are modeled both conceptually and as
                        data <ptr target="#bode2018"/>
                        <ptr target="#herrmann-lauer2018"/>
                        <ptr target="#herrmannsubmitted"/>. We need source criticism and reflection
                        about what it means that literature is represented either digitally or in an
                        analog medium, we need to understand what we sample, and how we sample. We
                        need to be aware of bias, but also know that <q>doing CLS,</q> defining
                        research questions and operationalizations, necessarily involves
                            <q>highlighting</q> some aspects and therefore <q>hiding</q> others, in
                        an interpretation of Popper. On that note, the discussion about hermeneutic
                        vs. algorithmic criticism has become less irreconcilable since attention has
                        shifted to the role of modeling, which has highlighted the logical relation
                        between theory, data, and method <ptr target="#flanders-jannidis2019"/>. The
                        attention to modeling emphasizes the need for scholarly control and critique
                        of all pertinent levels. With <ptr target="#popper1965"/>, we think it is a
                        constructive vision of computational literary studies to say that it
                                <cit><quote rend="inline">passes on its theories; but it also passes
                                on a critical attitude towards them. The theories are passed on, not
                                as dogmas, but rather with the challenge to discuss them and improve
                                upon them</quote></cit>. This approach at the level of modeling, we
                        think, does not preclude an <q>emphatic</q> way of approaching our objects
                        at certain stages of research.</p>
                    <p>We have shown how the question of digital methodologies, epistemic practices,
                        and research motives cuts directly to the identity of <q>computational (or
                            digital) literary studies</q>: In fact, CLS's putative emphasis on tools
                        / methodology may actually indicate that digital literary enquiry has not
                        been as fundamentally detached from <q>traditional</q> studies – literary
                        studies, stylistics, cultural studies etc. – as the administrative and
                        institutional perspective may suggest. For ontological and other definitions
                        of digital humanities, see for example <ptr target="#svensson2015"/> and
                        contributions in <ptr target="#terras2013"/>.<note><p>See also initiatives
                                such as OpenMethods, which curate <cit><quote rend="inline"
                                        >descriptions of methods and tools, tool and methods
                                        critique, as well as practical and theoretical reflections
                                        about how and why humanities research is conducted digital
                                        and how the increasing influence of digital methods and
                                        tools changes scholarly attitudes and scientific practices
                                        of humanities research</quote>
                                    <ptr target="https://openmethods.dariah.eu/"/></cit></p></note>
                        Rather, CLS emerges as one of the incarnations of literary studies,
                        capitalizing on the affordances of the digital – and simultaneously being
                        shaped by them. Admittedly, whether this may allow for eventually more
                        freedom, on a grander scale, depends on the willingness of established
                        Humanities disciplines and gatekeepers to venture for an update. Provided
                        some rethinking, a fundamental change may just be at our doorstep, with
                        literary data-driven modeling pursued not at the sidelines of the field, but
                        at its center.</p>
                </div>

            </div>
        </body>
        <back>

            <listBibl>
                <bibl xml:id="adam-viprey2009" label="Adam and Viprey 2009">Adam, J.-M. and Viprey,
                    J.-M. <title rend="quotes">Corpus de textes, textes en corpus. Problématique et
                        présentation</title>. Corpus 8:5–25.</bibl>
                <bibl xml:id="anttila-heuser2016" label="Anttila and Heuser 2016">Anttila, A. and
                    Heuser, R. <title rend="quotes">Phonological and Metrical Variation across
                        Genres</title>. <title rend="italic">Proceedings of the Annual Meetings on
                        Phonology. Linguistic Society of America</title>, Washington, DC
                    (2016).</bibl>
                <bibl xml:id="apollinaire1994" label="Apollinaire 1994">Apollinaire, G. <title
                        rend="italic">Œuvres Poétiques</title>, M. Adéma et M. Décaudin eds.,
                    Bibliothèque de la Pléiade, Gallimard, Paris, France (1994).</bibl>
                <bibl xml:id="apollinaire2005" label="Apollinaire 2005">Apollinaire, G. Lettres à
                    Madeleine. Tendre Comme Le Souvenir, Gallimard, Paris, France (2005).</bibl>
                <bibl xml:id="ascher1961" label="Ascher 1961">Ascher, R. <title rend="quotes"
                        >Experimental Archeology</title>. <title rend="italic">American
                        Anthropologist</title> 63(4):793–816. doi: <ptr
                        target="https://doi.org/10.1525/aa.1961.63.4.02a00070"/></bibl>
                <bibl xml:id="bacciu2019" label="Bacciu et al. 2019">Bacciu, A., La Morgia, M., Mei,
                    A., Nerio Nemmi, E., Neri, V. and Stefa, J. <title rend="quotes">Cross-domain
                        Authorship Attribution Combining Instance Based and Profile Based
                        Features</title>. In: Cappellato, L., Ferro, N., Losada, D. E. and Müller,
                    H. (eds.). <title rend="italic">CLEF 2019 Labs and Workshops, Notebook
                        Paper</title>. (2019).</bibl>
                <bibl xml:id="baird2004" label="Baird 2004">Baird, D. <title rend="italic">Thing
                        Knowledge: A Philosophy of Scientific Instruments</title>, University of
                    California Press, Berkeley (2004).</bibl>
                <bibl xml:id="bal2017" label="Bal 2017">Bal, M. <title rend="italic">Narratology:
                        Introduction to the Theory of Narrative</title>, Fourth Edition., University
                    of Toronto Press, Toronto Buffalo London (2017).</bibl>
                <bibl xml:id="barbot2019" label="Barbot et al. 2019">Barbot, L., Fischer, F.,
                    Moranville Y. and Pozdniakov, I.: <title rend="quotes">Which DH Tools Are
                        Actually Used in Research?</title> In: <title rend="italic"
                        >weltliteratur.net</title>, 6 Dec 2019. URL: <ptr
                        target="https://weltliteratur.net/dh-tools-used-in-research/"/></bibl>
                <bibl xml:id="beaudouin2002" label="Beaudouin 2002">Beaudouin, V. <title
                        rend="italic">Mètre et Rythme Du Vers Classique</title>. Corneille et
                    Racine, Honoré Champion, Paris (2002).</bibl>
                <bibl xml:id="berger-luckmann1967" label="Berger and Luckmann 1967">Berger, P. and
                    Luckmann, T. <title rend="italic">The Social Construction of Reality: A Treatise
                        in the Sociology of Knowledge</title>, Doubleday, Garden City, NY
                    (1967).</bibl>
                <bibl xml:id="bhatia2018" label="Bhatia et al. 2018">Bhatia, S., Lau, J.H. and
                    Baldwin, T. <title rend="quotes">Topic Intrusion for Automatic Topic Model
                        Evaluation</title>. <title rend="italic">Proceedings of the 2018 Conference
                        on Empirical Methods in Natural Language Processing</title>. Association for
                    Computational Linguistics, Brussels, Belgium (2018), pp. 844–849.</bibl>
                <bibl xml:id="binder-jennings2014" label="Binder and Jennings 2014">Binder, J.M. and
                    Jennings, C. <title rend="quotes">Visibility and meaning in topic models and
                        18th-century subject indexes</title>. <title rend="italic">Literary and
                        Linguistic Computing</title> 29(3):405–411. doi: <ptr
                        target="https://doi.org/10.1093/llc/fqu017"/></bibl>
                <bibl xml:id="birnbaum-thorsen2015" label="Birnbaum and Thorsen 2015">Birnbaum, D.J.
                    and Thorsen, E. <title rend="quotes">Markup and meter: Using XML tools to teach
                        a computer to think about versification</title>. <title rend="italic"
                        >Proceedings of Balisage: The Markup Conference</title>. (2015). <ptr
                        target="doi:10.4242/BalisageVol15.Birnbaum01"/></bibl>
                <bibl xml:id="blei2012" label="Blei 2012">Blei, D.M. <title rend="quotes"
                        >Probabilistic Topic Models</title>. <title rend="italic">Communication of
                        the ACM</title> 55(4):77–84. doi: <ptr
                        target="https://doi.org/10.1145/2133806.2133826"/></bibl>
                <bibl xml:id="bobenhausen-hammerich2015" label="Bobenhausen and Hammerich 2015"
                    >Bobenhausen, K. and Hammerich, B. <title rend="quotes">Métrique littéraire,
                        métrique linguistique et métrique algorithmique de l’allemand mises en jeu
                        dans le programme Metricalizer</title>. <title rend="italic"
                        >Langages</title> 199(3):67–88. doi: <ptr
                        target="https://doi.org/doi:10.3917/lang.199.0067"/></bibl>
                <bibl xml:id="bode2018" label="Bode 2018">Bode, K. <title rend="italic">A World of
                        Fiction: Digital Collections and the Future of Literary History</title>,
                    University of Michigan Press., (2018).</bibl>
                <bibl xml:id="bories2020" label="Bories 2020">Bories, A.-S. Des Chiffres et Des
                    Mètres. <title rend="italic">La Versification de Raymond Queneau</title>. Honoré
                    Champion, Paris, France (2020).</bibl>
                <bibl xml:id="bories2021" label="Bories et al. 2021">Bories, A.-S., Purnelle, G. and
                    Marchal, H. <title rend="italic">Plotting Poetry: On Mechanically-Enhanced
                        Reading</title>, Presses Universitaires de Liège, Liège (2021).</bibl>
                <bibl xml:id="brard-ferrari2015" label="Brard and Ferrari 2015">Brard, B. and
                    Ferrari, S. <title rend="quotes">Des vers et des mesures : détection des noyaux
                        vocaliques</title>. <title rend="italic">Langages</title> 199(3):107–124.
                    doi: <ptr target="https://doi.org/doi:10.3917/lang.199.0107"/></bibl>
                <bibl xml:id="brunet1989" label="Brunet 1989">Brunet, É. <title rend="quotes"
                        >Hyperbase: logiciel documentaire et statistique pour l’exploitation des
                        grands corpus</title>. <title rend="italic">Tools for humanists</title>, p.
                    33–36.</bibl>
                <bibl xml:id="bubenhofer-dreesen2018" label="Bubenhofer and Dreesen 2018"
                    >Bubenhofer, N. and Dreesen, P. <title rend="quotes">Linguistik als antifragile
                        Disziplin? Optionen in der digitalen Transformation</title>. <title
                        rend="italic">Digital Classics Online</title>, pp. 63–75. doi: <ptr
                        target="https://doi.org/10.11588/dco.2017.0.48493"/></bibl>
                <bibl xml:id="buchanan" label="Buchanan n.d.">Buchanan, J. <title rend="quotes"
                        >Collaborating with the Dead: Revivifying Frank Benson’s Richard
                    III</title>. Booklet published by Silents Now.</bibl>
                <bibl xml:id="burrows2002" label="Burrows 2002">Burrows, J. <title rend="quotes"
                            ><q>Delta:</q> a Measure of Stylistic Difference and a Guide to Likely
                        Authorship</title>. <title rend="italic">Literary and Linguistic
                        Computing</title> 17(3):267–287. doi: <ptr
                        target="https://doi.org/10.1093/llc/17.3.267"/></bibl>
                <bibl xml:id="calzolari2012" label="Calzolari et al. 2012">Calzolari, N., Del
                    Gratta, R., Francopoulo, G., Mariani, J., Rubino, F., Russo, I. and Soria, C.
                        <title rend="quotes">The LRE Map. Harmonising Community Descriptions of
                        Resources</title>. <title rend="italic">Proceedings of LREC 2012, Eighth
                        International Conference on Language Resources and Evaluation</title>.
                    Istanbul, Turkey (2012), pp. 1084–1089.</bibl>
                <bibl xml:id="carter2012" label="Carter 2012">Carter, R. <title rend="quotes">Coda:
                        Some rubber bullet points</title>. <title rend="italic">Language and
                        Literature</title> 21: 106–114. <ptr
                        target="https://doi.org/10.1177/0963947011432048"/></bibl>
                <bibl xml:id="collins1975" label="Collins 1975">Collins, H.M. <title rend="quotes"
                        >The Seven Sexes: A Study in the Sociology of a Phenomenon, or the
                        Replication of Experiments in Physics</title>. <title rend="italic"
                        >Sociology</title> 9(2):205–224. <ptr
                        target="https://doi.org/10.1177/003803857500900202"/></bibl>
                <bibl xml:id="vancranenburgh2012" label="van Cranenburgh 2012">van Cranenburgh, A.
                        <title rend="quotes">Literary authorship attribution with phrase-structure
                        fragments</title>. <title rend="italic">Proceedings of the NAACL-HLT 2012
                        Workshop on Computational Linguistics for Literature</title>. Association
                    for Computational Linguistics, Montréal, Canada (2012), pp. 59–63.</bibl>
                <bibl xml:id="da2019" label="Da 2019">Da, N.Z. <title rend="quotes">The
                        Computational Case against Computational Literary Studies</title>. <title
                        rend="italic">Critical Inquiry</title> 45(3):601–639. doi: <ptr
                        target="https://doi.org/10.1086/702594"/></bibl>
                <bibl xml:id="daelemans2019" label="Daelemans et al. 2019">Daelemans, W., Kestemont,
                    M., Manjavacas, E., Potthast, M., Rangel, F., Rosso, P., Specht, G., Stamatatos,
                    E., Stein, B., Tschuggnall, M., Wiegmann, M. and Zangerle, E. <title
                        rend="quotes">Overview of PAN 2019: Bots and Gender Profiling, Celebrity
                        Profiling, Cross-Domain Authorship Attribution and Style Change
                        Detection</title>. In: Crestani, F., Braschler, M., Savoy, J., Rauber, A.,
                    Müller, H., Losada, D. E., Heinatz Bürki, G., Cappellato, L. and Ferro, N.
                    (eds.). <title rend="italic">Experimental IR Meets Multilinguality,
                        Multimodality, and Interaction</title>. Springer International Publishing,
                    Cham (2019), pp. 402–416.</bibl>
                <bibl xml:id="debon2008" label="Debon 2008">Debon, C. <title rend="italic"
                        >‘Calligrammes’  Dans Tous Ses États - Édition Critique Du Recueil de
                        Guillaume APOLLINAIRE</title>, éditions Calliopées, Paris (2008).</bibl>
                <bibl xml:id="decaudin1969" label="Décaudin 1969">Décaudin, M. <title rend="italic"
                        >Le Dossier d’alcools</title>, Droz, Paris (1969).</bibl>
                <bibl xml:id="delente-renault2015" label="Delente and Renault 2015">Delente, É. and
                    Renault, R. <title rend="quotes">Projet Anamètre : le calcul du mètre des vers
                        complexes</title>. <title rend="italic">Langages</title> 199(3):125–148.
                        <ptr target="https://doi.org/doi:10.3917/lang.199.0125"/></bibl>
                <bibl xml:id="descartes1959" label="Descartes 1959">Descartes, R. <title
                        rend="italic">Règles pour la direction de l’esprit</title> (3rd Ed.; J.
                    Sirven, Ed.). Brin, Paris (1959).</bibl>
                <bibl xml:id="drucker2012" label="Drucker 2012">Drucker, J. <title rend="quotes"
                        >Humanistic Theory and Digital Scholarship</title>. In: Gold, M. K. (ed.).
                        <title rend="italic">Debates in the Digital Humanities</title>. University
                    of Minnesota Press, Minneapolis, MN (2012). Retrieved from <ptr
                        target="https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/0b495250-97af-4046-91ff-98b6ea9f83c0"
                    /></bibl>
                <bibl xml:id="du2019" label="Du 2019">Du, K. <title rend="quotes">A Survey On LDA
                        Topic Modeling In Digital Humanities</title>. <title rend="italic"
                        >Proceedings of the 2019 Digital Humanities Conference</title>. Utrecht
                    (2019).</bibl>
                <bibl xml:id="eder2015" label="Eder 2015">Eder, M. <title rend="quotes">Does size
                        matter? Authorship attribution, small samples, big problem</title>. <title
                        rend="italic">Literary and Linguistic Computing</title> 30(2):167–182. doi:
                        <ptr target="https://doi.org/10.1093/llc/fqt066"/></bibl>
                <bibl xml:id="eder2017" label="Eder 2017">Eder, M. <title rend="quotes"
                        >Visualization in stylometry: Cluster analysis using networks</title>.
                        <title rend="italic">Digital Scholarship in the Humanities</title>
                    32(1):50–64. doi: <ptr target="https://doi.org/10.1093/llc/fqv061"/></bibl>
                <bibl xml:id="eder2016" label="Eder et al. 2016">Eder, M., Rybicki, J. and
                    Kestemont, M. <title rend="quotes">Stylometry with R: A Package for
                        Computational Text Analysis</title>. <title rend="italic">The R
                        Journal</title> 8(1):107–121.</bibl>
                <bibl xml:id="erlin2021" label="Erlin et al. 2021">Erlin, M., Piper, A. Knox, D.,
                    Pentecost, S., Drouillard, M., Powell, B. and Townson, C. <title rend="quotes"
                        >Cultural Capitals: Modeling Minor European Literature</title>. <title
                        rend="italic">Journal of Cultural Analytics</title> 6 (1). <ptr
                        target="https://doi.org/10.22148/001c.21182"/></bibl>
                <bibl xml:id="vanes2018" label="van Es et al. 2018">van Es, K., Wieringa, M. and
                    Schäfer, M.T. <title rend="quotes">Tool Criticism: From Digital Methods to
                        Digital Methodology</title>. <title rend="italic">Proceedings of the 2nd
                        International Conference on Web Studies</title>. ACM, New York, NY, USA
                    (2018), pp. 24–27. doi: <ptr target="http://doi.acm.org/10.1145/3240431.3240436"
                    /></bibl>
                <bibl xml:id="evert2017" label="Evert et al. 2017">Evert, S., Proisl, T., Jannidis,
                    F., Reger, I., Pielström, S., Schöch, C. and Vitt, T. <title rend="quotes"
                        >Understanding and explaining Delta measures for authorship
                        attribution</title>. <title rend="italic">Digital Scholarship in the
                        Humanities</title> 32(suppl_2):ii4–ii16. doi: <ptr
                        target="https://doi.org/10.1093/llc/fqx023"/></bibl>
                <bibl xml:id="flanders-jannidis2019" label="Flanders and Jannidis 2019">Flanders, J.
                    and Jannidis, F. <title rend="italic">The Shape of Data in Digital Humanities:
                        Modeling Texts and Text-Based Resources</title>, Routledge, Taylor and
                    Francis Group, London; New York (2019).</bibl>
                <bibl xml:id="follet1987" label="Follet 1987">Follet, L. <title rend="quotes"
                        >Apollinaire entre vers et prose, de ‘L’Obituaire’ à la ‘Maison des
                        morts’</title>, <title rend="italic">Semen</title> 3, février 1987. doi:
                        <ptr target="http://semen.revues.org/5523"/></bibl>
                <bibl xml:id="franzini2012" label="Franzini 2012">Franzini, G. <title rend="quotes"
                        >Catalogue of Digital Editions</title>. doi: <ptr
                        target="https://doi.org/10.5281/zenodo.1161425"/></bibl>
                <bibl xml:id="frontini2017" label="Frontini et al. 2017">Frontini, F., Boukhaled,
                    M.-A. and Ganascia, J.-G. <title rend="quotes">Mining for characterising
                        patterns in literature using correspondence analysis: an experiment on
                        French novels</title>. <title rend="italic">Digital Humanities
                        Quarterly</title> 11(2).</bibl>
                <bibl xml:id="fusi2015" label="Fusi 2015">Fusi, D. <title rend="quotes">A
                        Multilanguage, Modular Framework for Metrical Analysis: It Patterns and
                        Theorical Issues</title>. <title rend="italic">Langages</title>
                    199(3):41–66. doi: <ptr target="https://doi.org/doi:10.3917/lang.199.0041"
                    /></bibl>
                <bibl xml:id="genette1972" label="Genette 1972">Genette, G. <title rend="italic"
                        >Figures III</title>, Éditions du Seuil, Paris (1972).</bibl>
                <bibl xml:id="gius-jacke2017" label="Gius and Jacke">Gius, E. and Jacke, J. <title
                        rend="italic">The Hermeneutic Profit of Annotation: On Preventing and
                        Fostering Disagreement in Literary Analysis</title>. <title rend="quotes"
                        >International Journal of Humanities and Arts Computing</title>
                    11(2):233–254. doi: <ptr target="https://doi.org/10.3366/ijhac.2017.0194"
                    /></bibl>
                <bibl xml:id="gius2019" label="Gius et al. 2019">Gius, E., Reiter, N. and Willand,
                    M. <title rend="quotes">Foreword to the Special Issue <title rend="quotes">A
                            Shared Task for the Digital Humanities: Annotating Narrative
                            Levels</title></title>. <title rend="italic">Journal of Cultural
                        Analytics</title>. doi: <ptr target="https://doi.org/10.22148/16.047"
                    /></bibl>
                <bibl xml:id="guiraud1953" label="Guiraud 1953">Guiraud, P. <title rend="italic"
                        >Index Des Mots d’Alcools de G. Apollinaire (Index Du Vocabulaire Du
                        Symbolisme. 1)</title>, Klincksieck, Paris (1953).</bibl>
                <bibl xml:id="hayles2012" label="Hayles 2012">Hayles, N. K. <title rend="italic">How
                        We Think: Digital Media and Contemporary Technogenesis</title>. University
                    of Chicago Press, Chicago (2012).</bibl>
                <bibl xml:id="heiden2010" label="Heiden et al. 2010">Heiden, S., Magué, J.-P. and
                    Pincemin, B. <title rend="quotes">TXM : Une plateforme logicielle open-source
                        pour la textométrie - conception et développement</title>. <title
                        rend="italic">JADT</title> 2010. (2010), pp. 1021–1032.</bibl>
                <bibl xml:id="henny2018" label="Henny et al. 2018">Henny, U., Betz, K., Schlör, D.
                    and Hotho, A. <title rend="quotes">Alternative Gattungstheorien. Das
                        Prototypenmodell am Beispiel hispanoamerikanischer Romane</title>. <title
                        rend="italic">Proceedings of the DHd 2018 Conference</title>. Cologne
                    (2018).</bibl>
                <bibl xml:id="herrmannsubmitted" label="Herrmann forthcoming">Herrmann, J.B. <title
                        rend="italic">Data-Driven Literary Studies</title>.</bibl>
                <bibl xml:id="herrmann2017" label="Herrmann 2017">Herrmann, J.B. <title
                        rend="quotes">In a test bed with Kafka. Introducing a mixed-method approach
                        to digital stylistics</title>. <title rend="italic">Digital Humanities
                        Quarterly</title> 011(4).</bibl>
                <bibl xml:id="herrmann-lauer2018" label="Herrmann and Lauer 2018">Herrmann, J.B. and
                    Lauer, G. <title rend="quotes">Korpusliteraturwissenschaft. Zur Konzeption und
                        Praxis am Beispiel eines Korpus zur literarischen Moderne</title>. <title
                        rend="italic">Osnabrücker Beiträge zur Sprachtheorie (OBST)</title>
                    92:127–156.</bibl>
                <bibl xml:id="herrmann2019" label="Herrmann et al. 2019">Herrmann, J.B., Woll, K.
                    and Dorst, A.G. <title rend="quotes">Linguistic Metaphor Identification in
                        German</title>. <title rend="italic">MIPVU in Multiple Languages</title>.
                    John Benjamins, Amsterdam / Philadelphia (2019).</bibl>
                <bibl xml:id="herrmann2021" label="Herrmann et al. 2021">Herrmann, J.B., Jacobs, A.
                    and Piper, A. <title rend="quotes">Computational Stylistics</title>. In D.
                    Kuiken and A. Jacobs (Eds.), <title rend="italic">Handbook of Empirical Literary
                        Studies</title>. Berlin: De Gruyter.</bibl>
                <bibl xml:id="herschberg-pierrot2005" label="Herschberg-Pierrot 2005"
                    >Herschberg-Pierrot, A. <title rend="italic">Le Style En Mouvement. Littérature
                        et Art, Sup/Lettres</title>, Berlin (2005).</bibl>
                <bibl xml:id="herschberg-pierrot2006" label="Herschberg-Pierrot 2006"
                    >Herschberg-Pierrot, A. <title rend="quotes">Style, corpus et genèse</title>.
                    Corpus 5.</bibl>
                <bibl xml:id="hirst-feiguina2007" label="Hirst and Feiguina 2007">Hirst, G. and
                    Feiguina, O. <title rend="quotes">Bigrams of Syntactic Labels for Authorship
                        Discrimination of Short Texts</title>. <title rend="italic">Literary and
                        Linguistic Computing</title> 22(4):405–417. doi: <ptr
                        target="https://doi.org/10.1093/llc/fqm023"/></bibl>
                <bibl xml:id="horstmann2018" label="Horstmann 2018">Horstmann, J. <title
                        rend="quotes">Topic Modeling</title> In: <title rend="italic">forTEXT.
                        Literatur digital erforschen</title>. Available at: <ptr
                        target="https://fortext.net/routinen/methoden/topic-modeling"/>[Accessed: 3
                    December 2019].</bibl>
                <bibl xml:id="jacobs2018" label="Jacobs 2018">Jacobs, A.M. <title rend="quotes">The
                        Gutenberg English Poetry Corpus: Exemplary Quantitative Narrative
                        Analyses</title>. <title rend="italic">Frontiers in Digital
                        Humanities</title> 5. doi: <ptr
                        target="https://doi.org/10.3389/fdigh.2018.00005"/></bibl>
                <bibl xml:id="jacquot2012" label="Jacquot 2012">Jacquot, C. <title rend="quotes">Le
                        poulpe, une figure de la <q>plasticité</q> d’Apollinaire?</title>, <title
                        rend="italic">Apollinaire</title> 11 (2012), pp. 35–43.</bibl>
                <bibl xml:id="jacquot2014" label="Jacquot 2014">Jacquot, C. <title rend="italic"
                        >Plasticité de l’écriture poétique d’Apollinaire. Une articulation du
                        continu et du discontinu</title>. Thèse de doctorat en langue française sous
                    la direction de J. Dürrenmatt, Université Paris IV-Sorbonne (2014).</bibl>
                <bibl xml:id="jacquot2017" label="Jacquot 2017">Jacquot, C. <title rend="quotes"
                        >Corpus poétique et métadonnées: la problématique de la datation dans les
                        poèmes de Guillaume Apollinaire</title>. <title rend="italic">Modèles et
                        Nombres En Poésie</title>. Champion, Paris (2017), pp. 81–103.</bibl>
                <bibl xml:id="jenny2011" label="Jenny 2011">Jenny, L. <title rend="italic">Le Style
                        En Acte. Vers Une Pragmatique Du Style</title>, MētisPresses, Genève
                    (2011).</bibl>
                <bibl xml:id="jones2018" label="Jones 2018">Jones, M. L., <title rend="quotes">How
                        We Became Instrumentalists (Again)</title>. <title rend="italic">Historical
                        Studies in the Natural Sciences</title>, 48(5): 673-684. doi: <ptr
                        target="https://doi.org/10.1525/hsns.2018.48.5.673"/></bibl>
                <bibl xml:id="joyce1968" label="Joyce 1968">Joyce, J.<title rend="italic">A Portrait
                        of the Artist as a Young Man, Text, Criticism, and Notes Edited by Chester
                        G. Anderson</title>. The Viking Press, New York (1968).</bibl>
                <bibl xml:id="kestemont2018" label="Kestemont et al. 2018">Kestemont, M.,
                    Tschuggnall, M., Stamatatos, E., Daelemans, W., Specht, G., Stein, B. and
                    Potthast, M. <title rend="quotes">Overview of the author identification task at
                        PAN-2018: cross-domain authorship attribution and style change
                        detection</title>. <title rend="italic">Working Notes Papers of the CLEF
                        2018 Evaluation Labs. CEUR Workshop Proceedings</title> (2018), pp.
                    1–25.</bibl>
                <bibl xml:id="kiefer2018" label="Kiefer 2018">Kiefer, K. <title rend="quotes">Tool
                        Criticism on emotional text analysis</title>. <title rend="italic"
                        >Proceedings of the EADH Conference</title>. (2018).</bibl>
                <bibl xml:id="koolen2018" label="Koolen et al. 2018">Koolen, J., van Gorp, M. and
                    van Ossenbruggen, J. <title rend="quotes">Lessons Learned from a Digital Tool
                        Criticism Workshop</title>. <title rend="italic">Proceedings from DH Benelux
                        2018</title>. Amsterdam, The Netherlands (2018).</bibl>
                <bibl xml:id="kuhn2019" label="Kuhn 2019">Kuhn, J. <title rend="quotes"
                        >Computational text analysis within the Humanities: How to combine working
                        practices from the contributing fields?</title>. <title rend="italic"
                        >Language Resources and Evaluation</title> 53(4): 565–602. doi: <ptr
                        target="https://doi.org/10.1007/s10579-019-09459-3"/></bibl>
                <bibl xml:id="lafon1980" label="Lafon 1980">Lafon, P. <title rend="quotes">Sur la
                        variabilité de la fréquence des formes dans un corpus</title>. <title
                        rend="italic">Mots. Les langages du politique</title> 1(1): 127–165. doi:
                        <ptr target="https://doi.org/10.3406/mots.1980.1008"/></bibl>
                <bibl xml:id="lafon-muller1984" label="Lafon and Muller 1984">Lafon, P. and Muller,
                    C. <title rend="italic">Dépouillements et Statistiques En Lexicométrie, Travaux
                        de linguistique quantitative</title>, Slatkine/Champion, Genève/Paris
                    (1984).</bibl>
                <bibl xml:id="lau-baldwin2016" label="Lau and Baldwin 2016">Lau, J.H. and Baldwin,
                    T. <title rend="quotes">The Sensitivity of Topic Coherence Evaluation to Topic
                        Cardinality</title>. <title rend="italic">Proceedings of the 2016 Conference
                        of the North American Chapter of the Association for Computational
                        Linguistics: Human Language Technologies</title>. Association for
                    Computational Linguistics, San Diego, California (2016), pp. 483–487.</bibl>
                <bibl xml:id="martinezcanton2017" label="Martínez Cantón et al. 2017">Martínez
                    Cantón, C.I., Ruiz Fabo, P., González-Blanco García, E. and Poibeau, T. <title
                        rend="quotes">Automatic enjambment detection as a new source of evidence in
                        Spanish versification</title>. <title rend="italic">Plotting Poetry : On
                        Mechanically-Enhanced Reading / Machiner La Poésie: Sur Les Lectures
                        Appareillées</title>. Basel (2017).</bibl>
                <bibl xml:id="mccallum2002" label="McCallum 2002">McCallum, A.K. <title
                        rend="italic">MALLET : A Machine Learning for Language Toolkit</title>,
                    (2002). <ptr target="http://mallet.cs.umass.edu"/></bibl>
                <bibl xml:id="mccarthy2005" label="McCarthy 2005">McCarthy, W. <title rend="italic"
                        >Humanities Computing</title>, Palgrave Macmillan UK (2005).</bibl>
                <bibl xml:id="miall2018" label="Miall 2018">Miall, D.S. <title rend="quotes"
                        >Reader-Response Theory</title>. <title rend="italic">A Companion to
                        Literary Theory</title>. John Wiley and Sons, Ltd (2018), pp. 114–125. doi:
                        <ptr target="https://doi.org/10.1002/9781118958933.ch9"/></bibl>
                <bibl xml:id="millson2010" label="Millson 2010">Millson, D. (ed) <title
                        rend="italic">Experimentation and Interpretation: The Use of Experimental
                        Archaeology in the Study of the Past</title>. Oxbow Books (2010).</bibl>
                <bibl xml:id="mitrofanova2015" label="Mitrofanova 2015">Mitrofanova, O. <title
                        rend="quotes">Probabilistic Topic Modeling of the Russian Text Corpus on
                        Musicology</title>. In: Eismont, P. and Konstantinova, N. (eds.). <title
                        rend="italic">Language, Music, and Computing</title>. Springer International
                    Publishing, Cham (2015), pp. 69–76.</bibl>
                <bibl xml:id="moore1995" label="Moore 1995">Moore, C. <title rend="italic"
                        >Apollinaire en 1908, la poétique de l'enchantement: une lecture
                        d'Onirocritique</title>, Paris, Minard (1995).</bibl>
                <bibl xml:id="noble2018" label="Noble 2018">Noble, S.U. <title rend="italic"
                        >Algorithms of Oppression: How Search Engines Reinforce Racism</title>, New
                    York University Press, New York University.</bibl>
                <bibl xml:id="oneil2016" label="O’Neil 2016">O’Neil, C. <title rend="italic">Weapons
                        of Math Destruction: How Big Data Increases Inequality and Threatens
                        Democracy</title>, 1 edition., Crown, New York (2016).</bibl>
                <bibl xml:id="opensciencecollaboration2015" label="Open Science Collaboration 2015"
                    >Open Science Collaboration. (2015). <title rend="quotes">Estimating the
                        reproducibility of psychological science</title>. <title rend="italic"
                        >Science</title>, 349(6251), aac4716–aac4716. doi: <ptr
                        target="https://doi.org/10.1126/science.aac4716"/></bibl>
                <bibl xml:id="outram2008" label="Outram 2008">Outram, A.K. <title rend="quotes"
                        >Introduction to experimental archaeology</title>. <title rend="italic"
                        >World Archaeology</title> 40(1):1–6. doi: <ptr
                        target="https://doi.org/10.1080/00438240801889456"/></bibl>
                <bibl xml:id="pavlova-fischer2018" label="Pavlova and Fischer 2018">Pavlova, I. and
                    Fischer, F. <title rend="quotes">Topic Modeling 200 Years of Russian
                        Drama</title>. Proceedings of the EADH Conference (2018).</bibl>
                <bibl xml:id="percillier2017" label="Percillier 2017">Percillier, M. <title
                        rend="quotes">Creating and Analyzing Literary Corpora</title>. <title
                        rend="italic">Data Analytics in Digital Humanities</title>. Springer, Cham
                    (2017), pp. 91–118.</bibl>
                <bibl xml:id="pilshchikov-starostin2015" label="Pilshchikov and Starostin 2015"
                    >Pilshchikov, I. and Starostin, A. <title rend="quotes">Reconnaissance
                        automatique des mètres des vers russes : une approche statistique sur
                        corpus</title>. <title rend="italic">Langages</title> 199(3):89–106. doi:
                        <ptr target="https://doi.org/10.3917/lang.199.0089"/></bibl>
                <bibl xml:id="pincemin2011" label="Pincemin 2011">Pincemin, B. <title rend="quotes"
                        >Analyse stylistique différentielle à base de marqueurs et
                        textométrie</title>. In: Garric, N. and Maurel-Indart, H. (eds.). <title
                        rend="italic">Vers Une Automatisation de l’analyse Textuelle. texto ! Textes
                        and Cultures</title>, Volume XV - n°4 et XVI - n°1 (2011), pp. 54–61.</bibl>
                <bibl xml:id="piotrowski2019" label="Piotrowski 2019">Piotrowski, M. <title
                        rend="quotes">Historical Models and Serial Sources</title>. <title
                        rend="italic">Journal of European Periodical Studies</title> 4(1):8–18. doi:
                        <ptr target="https://doi.org/10.21825/jeps.v4i1.10226"/></bibl>
                <bibl xml:id="piper2017" label="Piper 2017">Piper, A. <title rend="quotes">Think
                        Small: On Literary Modeling</title>. PMLA 132(3):651–658. doi: <ptr
                        target="https://doi.org/10.1632/pmla.2017.132.3.651"/></bibl>
                <bibl xml:id="piper2018" label="Piper 2018">Piper, A. <title rend="italic"
                        >Enumerations</title>, The University of Chicago Press, Chicago
                    (2018).</bibl>
                <bibl xml:id="plechac-kolar2017" label="Plecháč and Kolár 2017">Plecháč, P. and
                    Kolár, R. K<title rend="italic">apitoly z Korpusové Versologie</title>,
                    Akropolis, Prague (2017).</bibl>
                <bibl xml:id="plechac2021" label="Plecháč et al. 2021">Plecháč, P., Kolár, R.,
                    Bories, A.-S. and Říha, J. <title rend="quotes">Tackling the Toolkit. Plotting
                        Poetry through Computational Literary Studies</title>, Prague: ICL CAS. <ptr
                        target="https://doi.org/10.51305/ICL.CZ.9788076580336"/></bibl>
                <bibl xml:id="popper1965" label="Popper 1965">Popper, K.R. <title rend="italic"
                        >Conjectures and Refutations: The Growth of Scientific Knowledge</title>,
                    2nd ed. revised., Routledge and Kegan Paul, London (1965).</bibl>
                <bibl xml:id="queneau1961" label="Queneau 1961">Queneau, R. <title rend="italic"
                        >Petite Cosmogonie Portative</title>, Gallimard, Paris (1961).</bibl>
                <bibl xml:id="rapp2017" label="Rapp 2017">Rapp, A. <title rend="quotes">Manuelle und
                        automatische Annotation</title>. In: Jannidis, F., Kohle, H., Rehbein, M.
                    (eds.) <title rend="italic">Digital Humanities</title>. J.B. Metzler (2017) <ptr
                        target="https://doi.org/10.1007/978-3-476-05446-3_18"/>
                </bibl>
                <bibl xml:id="rastier2002" label="Rastier 2002">Rastier, F. <title rend="quotes"
                        >Enjeux épistémologiques de la linguistique de corpus</title>. In: G, W.
                    (ed.). <title rend="italic">Deuxièmes Journées de La Linguistique de
                        Corpus</title>. Presses Universitaires de Rennes, Lorient, France (2002),
                    pp. 31–46.</bibl>
                <bibl xml:id="rebora2019" label="Rebora et al. 2019">Rebora, S., Herrmann, J.B.,
                    Lauer, G. and Salgaro, M. <title rend="quotes">Robert Musil, a war journal, and
                        stylometry: Tackling the issue of short texts in authorship
                        attribution</title>. <title rend="italic">Digital Scholarship in the
                        Humanities</title> 34(3):582–605. doi: <ptr
                        target="https://doi.org/10.1093/llc/fqy055"/></bibl>
                <bibl xml:id="rockwell-sinclair2016" label="Rockwell and Sinclair 2016">Rockwell, G.
                    and Sinclair, S. <title rend="italic">Hermeneutica: Computer-Assisted
                        Interpretation in the Humanities</title>, MIT Press (2016).</bibl>
                <bibl xml:id="schoch2017" label="Schöch 2017">Schöch, C. <title rend="quotes">Topic
                        Modeling Genre: An Exploration of French Classical and Enlightenment
                        Drama</title>. <title rend="italic">Digital Humanities Quarterly</title>
                    11(2).</bibl>
                <bibl xml:id="simmler2019" label="Simmler et al. 2019">Simmler, S., Vitt, T. and
                    Pielström, S. <title rend="quotes">Topic Modeling with Interactive
                        Visualizations in a GUI Tool</title>. <title rend="italic">Proceedings of
                        the 2019 Digital Humanities Conference</title>. Utrecht (2019).</bibl>
                <bibl xml:id="simpson2004" label="Simpson 2004">Simpson, P. <title rend="italic"
                        >Stylistics. A Resource Book for Students</title>, Routledge English
                    language introductions, Routledge, London [u.a.] (2004).</bibl>
                <bibl xml:id="smith1973" label="Smith 1973">Smith, J.B. <title rend="quotes">Image
                        and Imagery in Joyce’s Portrait: A Computer-Assisted Analysis</title>.
                        <title rend="italic">Directions in Literary Criticism: Contemporary
                        Approaches to Literature</title>. The Pennsylvania State University Press,
                    University Park, PA (1973), pp. 220–227.</bibl>
                <bibl xml:id="smith1978" label="Smith 1978">Smith, J.B. <title rend="quotes"
                        >Computer Criticism</title>. <title rend="italic">Style</title>
                    XII(4):326–356.</bibl>
                <bibl xml:id="smith1980" label="Smith 1980">Smith, J.B. Imagery and the Mind of
                    Stephen Dedalus: A Computer-Assisted Study of Joyce’s A Portrait of the Artist
                    as a Young Man, Bucknell University Press, Lewisburg, PA (1980).</bibl>
                <bibl xml:id="smith1984" label="Smith 1984">Smith, J.B. <title rend="quotes">A New
                        Environment For Literary Analysis</title>. <title rend="italic">Perspectives
                        in Computing</title> 4(2/3):20–31.</bibl>
                <bibl xml:id="stamatatos2014" label="Stamatatos et al. 2014">Stamatatos, E.,
                    Daelemans, W., Verhoeven, B., Potthast, M., Stein, B., Juola, P., Sanchez-Perez,
                    M.A. and Barrón-Cedeño, A. <title rend="quotes">Overview of the Author
                        Identification Task at PAN 2014</title>. <title rend="italic">Working Notes
                        Papers of the CLEF 2014 Evaluation Labs. CEUR Workshop Proceedings</title>,
                    877-897 (2014).</bibl>
                <bibl xml:id="steen2010" label="Steen et al. 2010">Steen, G.J., Dorst, A.G.,
                    Herrmann, J.B., Kaal, A.A., Tina, Krennmayr. and Pasma, T. <title rend="italic"
                        >A Method for Linguistic Metaphor Identification: From MIP to MIPVU</title>,
                    John Benjamins, Amsterdam and Philadelphia (2010).</bibl>
                <bibl xml:id="steyvers-griffiths2007" label="Steyvers and Griffiths 2007">Steyvers,
                    M. and Griffiths, T. <title rend="quotes">Probabilistic topic models</title>.
                        <title rend="italic">Latent Semantic Analysis: A Road to Meaning</title>.
                    Laurence Erlbaum (2007), pp. 424–440.</bibl>
                <bibl xml:id="suppes1968" label="Suppes 1968">Suppes, P. <title rend="quotes">The
                        desirability of formalization in science</title>. <title rend="italic">The
                        Journal of Philosophy</title>, 65(20), 651–664 (1968).</bibl>
                <bibl xml:id="svensson2015" label="Svensson 2015">Svensson, P. <title rend="quotes"
                        >Sorting Out the Digital Humanities</title>. <title rend="italic">A New
                        Companion to Digital Humanities</title>. John Wiley and Sons, Ltd (2015),
                    pp. 476–492. doi: <ptr target="https://doi.org/10.1002/9781118680605.ch33"
                    /></bibl>
                <bibl xml:id="terras2013" label="Terras et al. 2013">Terras, M., Vanhoutte, E. and
                    Nyhan, J. <title rend="italic">Defining Digital Humanities: A Reader</title>,
                    Routledge, London/New York (2013).</bibl>
                <bibl xml:id="traub-vanossenbruggen2015" label="Traub and van Ossenbruggen 2015"
                    >Traub, M.C. and van Ossenbruggen, J. <title rend="italic">Workshop on Tool
                        Criticism in the Digital Humanities - Report</title>, CWI Techreport,
                    Amsterdam (2015).</bibl>
                <bibl xml:id="underwood2019" label="Underwood 2019">Underwood, T. <title
                        rend="italic">Distant Horizons: Digital Evidence and Literary
                    Change</title>, First edition., University of Chicago Press, Chicago
                    (2019).</bibl>
                <bibl xml:id="unsworth2000" label="Unsworth 2000">Unsworth, J. <title rend="quotes"
                        >Scholarly primitives: What methods do humanities researchers have in
                        common, and how might our tools reflect this</title>. <title rend="italic"
                        >Symposium on Humanities Computing: Formal Methods, Experimental
                        Practice</title>. King’s College, London. (2000).</bibl>
                <bibl xml:id="weizenbaum1984" label="Weizenbaum 1984">Weizenbaum, J. <title
                        rend="italic">Computer Power and Human Reason: From Judgment to
                        Calculation</title>. Penguin, Harmondsworth. (1984).</bibl>
                <bibl xml:id="winko2015" label="Winko 2015">Winko, S. <title rend="quotes">Zur
                        Plausibilität als Beurteilungskriterium literaturwissenschaftlicher
                        Interpretationen</title>. <title rend="italic">Theorien, Methoden und
                        Praktiken des Interpretierens</title>. De Gruyter, Berlin, Boston (2015),
                    pp. 483–511. doi: <ptr target="https://doi.org/10.1515/9783110353983.483"
                    /></bibl>
            </listBibl>


        </back>
    </text>
</TEI>
