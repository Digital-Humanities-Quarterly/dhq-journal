<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?>
<?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="article" xml:lang="en">The Explainability Turn</title>
                <dhq:authorInfo>

                    <dhq:author_name>David M. <dhq:family>Berry</dhq:family></dhq:author_name>
                    <idno type="ORCID">https://orcid.org/0000-0002-7737-5586</idno>
                    <dhq:affiliation>University of Sussex</dhq:affiliation>
                    <email>d.m.berry@sussex.ac.uk</email>

                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id"><!-- including leading zeroes: e.g. 000110 -->000685</idno>
                <idno type="volume">007</idno>
                <idno type="issue">1</idno>
                <date/>
                <dhq:articleType>article</dhq:articleType>
                <availability status="CC-BY-ND">
                    <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>
            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <p>Encoding in TEI by Peter Verhaar</p>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                        target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                        >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref>
                    </bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
                <taxonomy xml:id="project_keywords">
                    <bibl>DHQ project registry; full list available at <ref
                        target="http://www.digitalhumanities.org/dhq/projects.xml"
                        >http://www.digitalhumanities.org/dhq/projects.xml</ref>
                    </bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!--Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors-->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!--Authors may include one or more keywords of their choice-->
                    <list type="simple">
                        <item>Explainability</item>
                        <item>Tool Criticism</item>
                        <item>Digital infrastructures</item>
                        <item>infrasomatization</item>
                    </list>
                </keywords>
                <keywords scheme="#project_keywords">
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change>The version history for this file can be found on <ref
                target="https://github.com/Digital-Humanities-Quarterly/dhq-journal/commits/master/articles/000685/000685.xml"
                >GitHub </ref>
            </change>
        </revisionDesc>
    </teiHeader>
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <p>To be added</p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p/>
            </dhq:teaser>
        </front>

        <body>

            <div>
                <head>Introduction</head>

                <p>How can we know what our computational infrastructures are doing to us? More to
                    the point, how can we trust that algorithms and related technologies do not have
                    a detrimental effect? As technologies make up more of our digital environment,
                    they not only provide tools for thought, but they also shape and direct the very
                    way we think. The move from relying on books to understand a topic to using the
                    internet to research a topic is profoundly different, not only in terms of the
                    acceleration in access to information, but also in the reliance on
                        <q>surfing</q> and <q>searching</q> for information. These are different
                    cognitive modes, or styles of thinking (see <ptr target="#hayles2007"/>
                    <ptr target="#hayles2010"/>). Digital infrastructures combined with spatial and
                    temporal organisation create forms of digitally enabled structures that serve to
                    change the cognitive capacity of humans (see for example <ptr
                        target="#hutchins1995"/>). In 1981, Steve Jobs, then CEO of Apple, famously
                    called computers <quote rend="inline">Bicycles for the Mind</quote>, implying
                    that they augmented the cognitive capacities of the user, making them faster and
                    more capable <ptr target="#jobs1981" loc="pp. 8–9"/>. But others are not so
                    positive, with writers such as Nicholas Carr worrying that they might also
                    undermine and fragment the possibility for thought. As Carr wrote <cit><quote
                            rend="block">over the past few years I've had an uncomfortable sense
                            that someone, or something, has been tinkering with my brain, remapping
                            the neural circuitry, reprogramming the memory. My mind isn't going — so
                            far as I can tell — but it's changing. I'm not thinking the way I used
                            to think</quote>. <ptr target="#carr2008"/></cit></p>

                <p>Similarly, Bernard Stiegler <ptr target="#stiegler2015"/>
                    <ptr target="#stiegler2018"/> has argued that the programming industries have a
                    vested interest in changing the way individuals think to make possible a new
                    digital consumption economy. In this paper, I examine this new situation and
                    social responses to computational infrastructures that can now be seen to weaken
                    historical practices of cognition. I use the term cognition to represent not
                    just the cognitive processes of the human mind, but to include a more
                    substantive notion which includes not only thinking, but also feeling and
                    projecting. In particular, I understand cognition as a synthetic faculty in the
                    application of reason which opens the possibility for a decision. The aim is to
                    begin to account for the way in which this synthetic faculty is being automated
                    by algorithmic processing such that the human cognitive ability to connect
                    factors into an explanation becomes increasingly deficient. When connected into
                    contemporary digital infrastructures, rather than acting as bicycles for the
                    mind, these technologies replace certain cognitive functions of the mind. Being
                    owned and controlled by corporate organisations they tend to weaken explanatory
                    and critical thinking and instead nudge and influence human behaviour in
                    directions that are profitable.<note><p>Many technology companies rely on
                            techniques developed in casinos to nudge behaviour to maximise
                            profitability, such as creating addictive experiences and by disarming
                            the will of the user. Using techniques such as <title rend="quotes"
                                >Trigger, Action, Reward and Investment</title> these systems help
                            create addition to a particular product (see <ptr target="#schüll2014"/>
                            <ptr target="#eyal2014"/>).</p></note> When incorporated into digital
                    platforms these technologies can be combined to create distraction spaces for
                    what we might call <term style="quotes">frenetic passivity</term> to repress
                    critical cognitive activity or <term>thought</term>. Revelations from industry
                    insiders and researchers of behavioural nudging and manipulation techniques have
                    been widely documented and have served to prompt public calls for more
                    regulation over these systems (see <ptr target="#zuboff2019"/>
                    <ptr target="#mcnamee2019"/>). We have also seen changes to the regulatory
                    environment as the public has become increasingly uneasy about these automated
                    systems (for example, see <ptr target="#eu"/>
                    <ptr target="#europeanparliament2022"/>).</p>

                <p>Digital technologies substitute artificial analytic capacities that bypass and
                    replace the synthetic function of reason. Algorithms often overtake human
                    cognitive faculties by shortcutting individual decisions by making a digital
                        <q>suggestion</q> or intervention. The most obvious example of this is
                    Google Autocomplete on the search bar which tries to predict what a user will
                    type before they have completed a sentence – and make it easy for the user to
                    just click that rather than thinking through what they are writing. This
                    technology has also been rolled-out to Gmail, where Google will write a user's
                    emails by predicting what it thinks the user might be planning to write.
                    Similarly, recent breakthroughs in artificial intelligence, such as GPT-3 also
                    create long-form, remarkably competent, written texts based on a similar
                    automated capacity. These techniques are increasingly being incorporated into
                    many aspects of computer interfaces through design practices that predict,
                    persuade, or nudge particular behavioural outcomes. For example, Apple devices
                    often <q>know</q> where you are due next by consulting your calendar and
                    auto-calculating your route to the next event and warning the user of the
                    minimum time for them to arrive – sometimes even cautioning the user to leave
                    immediately. These technologies use the mobilisation of processes of selecting
                    and directing activity through the automation of data from information collected
                    from millions of users <ptr target="#malabou2019" loc="52"/>. As Noble has
                    noted, <cit><quote rend="block">What each of these searches represents are
                            Google's algorithmic conceptualizations of a variety of people and
                            ideas. …Google's dominant narratives reflect the kinds of hegemonic
                            frameworks and notions that are often resisted by women and people of
                            color. Interrogating what advertising companies serve up as credible
                            information must happen, rather than have a public instantly gratified
                            with stereotypes in three-hundredths of a second or less</quote>
                        <ptr target="#noble2018" loc="50"/>.</cit></p>

                <p>We are witnessing the social being transformed by digital technologies that
                    transform individuals' thinking towards <q>operational</q> or instrumental
                    thought. But it is also important to remain alert to the social dimension beyond
                    the level of the individual so that we are attentive to the relationship between
                    social being and consciousness. This includes the reconfiguring of social life
                    through the technical infrastructures of computational mediation which
                    themselves privilege individualistic ways of framing and understanding the
                    world. A consequence of which, as Geert Lovink has noted, is that <quote
                        rend="inline">there is no <q>social</q> anymore outside of social
                        media.</quote> Merleau-Ponty earlier warned us, <cit><quote rend="block"
                            >Thinking <q>operationally</q> becomes a sort of absolute artificialism,
                            such as we see in the ideology of cybernetics, where human creations are
                            derived from a natural information process, but which is itself
                            conceived on the model of human machines. If this kind of thinking takes
                            over humanity and history, and if, pretending to be ignorant of what we
                            know about humanity and history through contact and through location…
                            then we enter into a cultural regimen in which there is neither truth
                            nor falsehood concerning humanity and history, then we enter into a
                            sleep or nightmare from which nothing would be able to awaken us</quote>
                        <ptr target="#merleau-ponty2007" loc="352"/>.</cit></p>
                <p>Whilst I do not have the space to rehearse all the arguments that inform this
                    paper (but see <ptr target="#berry2011"/>
                    <ptr target="#berry2014"/>
                    <ptr target="#daston2022" loc="p. 147–150"/>), it can be seen that resituating
                    the cognitive processes of thought within the concrete reality of the
                    increasingly <q>smart</q> infrastructures that surround us changes not only how
                    we think but also our relationship to the decisions that are taken on our
                    behalf. My aim is to use the way in which infrastructural logics of computation
                    decentre and overtake modes of thought, for example by undermining
                    concentration, focus and attention, to examine the way in which this leads to a
                    situation that undermines trust in systems. This is to critically assess
                    attempts to assuage worries over the opaque and threatening potential of
                    computation through a new right to challenge algorithms and their decisions
                    called <term>explainability</term>. I will later suggest new critical practices
                    are possible within digital humanities for investigating and potentially
                    contesting these technologies by taking on board and extending this notion.
                            <note><p>We might contrast the idea of <term>explainability</term>,
                            which is intended to create explanations, with the notion of
                                <term>observability</term> developed by Rieder and Hoffman (2020)
                            and what Lipton (2017) and others have called
                                <term>interpretability</term>. Rieder and Hoffman argue that <quote
                                rend="inline">observability emphasises the conditions for the
                                practice of observing in a given domain ... We therefore position
                                observability as an explicit means of, not an alternative to
                                regulation</quote>
                            <ptr target="#rieder2020" loc="p. 3–10"/>. I seek to explicitly link
                            explainability to critique, whereas <term>observability</term> is
                            developed as an administrative concept to aid in regulatory and policy
                            outcomes. Interpretability is closer to my idea of explainability as
                            aiding human understanding of algorithmic models and software <ptr
                                target="#lipton2017"/>.</p></note>
                </p>

                <p>I believe that the <title rend="italic">General Data Protection Regulation
                        2016/679</title> (GDPR) <ptr target="#gdpr2016"/> can help us to understand
                    this new problematic. When instantiated in national legislation it has created a
                    new right in relation to automated algorithmic systems that requires the
                        <term>controller</term> of an algorithm to supply an explanation of how a
                    decision was made to the user (or <term>data subject</term>) – what we might
                    call the <emph>social right to explanation</emph>. The GDPR is a regulation in
                    EU law on data protection and privacy for citizens within the European Union and
                    the European Economic Area.<note><p>Following the GDPR, in the UK, the enabling
                            legislation for the European GDPR is the <title rend="italic">Data
                                Protection Act 2018</title>.</p></note> The GDPR creates a new kind
                    of subject, the <q>data subject</q> to whom a right to explanation (amongst
                    other data protection and privacy rights) is given. The notion of a <term>data
                        subject</term> has a range of very specific and unique rights as a
                        <term>natural person</term>, which distinguishes them from an artificial
                    intelligence, machine-learning system, algorithm or indeed a corporation. This
                    definition creates what we might call a post-posthuman subjectivity by creating
                    and reinforcing a boundary between humans, corporations and machines.<note><p>It
                            appears that the idea is that only a <term>natural person</term> may ask
                            for an explanation, preventing algorithms or corporations from
                            requesting an explanation from other algorithms or corporations.
                        </p></note> Additionally, it has created a legal definition of processing
                    through a computer algorithm <ptr target="#gdpr2016" loc="art.4"/>. In
                    consequence, this has given rise to a notion of explainability which creates the
                    right <quote rend="inline">to obtain an explanation of [a] decision reached
                        after such assessment and to challenge the decision</quote>
                    <ptr target="#gdpr2016" loc="recital 71"/>.<note><p>Whilst non-binding, the
                            Recitals <quote rend="inline">dissolve ambiguity in the operative text
                                of a framework</quote>, and they provide a critical reference for
                            future interpretations <ptr target="#casey2018" loc="17"/>.</p></note>
                    It has been argued that this regulation mandates a requirement for a
                    representation of the processes of computation used in an automated decision,
                    the calculative model, for example, and for it to be presented to the data
                    subject on request (<ptr target="#goodman2017"/>
                    <ptr target="#selbst2017"/> , cf. <ptr target="#wachter2017"/>).<note><p>
                            <ptr target="#wachter2017"/> argue that the <quote rend="inline">GDPR
                                does not, in its current form, implement a right to explanation, but
                                rather what we term a limited 'right to be informed'</quote>
                            although this has been contested in the literature as their argument
                            rests on a rather narrow reading of the effects of Recital 71 (see <ptr
                                target="#edwards2018"/>. But nonetheless <quote rend="inline">the
                                GDPR’s right of access only grants an explanation of automated
                                decision-making addressing system functionality, not the rationale
                                and circumstances of specific decisions</quote>
                            <ptr target="#wachter2017" loc="19"/>). </p></note> It is crucial
                    however to understand that this is not just an issue of legal rights, this has
                    also created a normative demand for a social right to explanation. </p>

                <p>This debate has had implications for artificial intelligence systems with the
                    assumption that they might have to have the capacity to provide a
                    self-description. This has become known as the problem of explainability for
                    artificial intelligence research, and has led to the emergence of the subfield
                    of Explainable Artificial Intelligence (XAI). Although the GDPR is limited to
                    the European Union, in actuality it is likely to have global effects as it
                    becomes necessary for global companies to standardise their software products
                    and services but also to respond to growing public disquiet over these systems
                    (see also <ptr target="#darpa" loc="n.d."/>
                    <ptr target="#sample2017"/>
                    <ptr target="#kuang2017"/>).<note><p>It is important to note that although this
                            paper has focussed on the GDPR, explainability was also part of a Darpa
                            research programme in 2016 (DARPA-BAA-16-53). More information can be
                            found here: <ptr
                                target="https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf"
                        /></p></note> This has also become part of a wider public discourse.
                    Explanation was one of the <term>rights</term> outlined in an <title
                        rend="italic">algorithmic bill of rights</title> published in 2019, for
                    instance, which argued that <cit><quote rend="block">we have the right to be
                            given explanations about how algorithms affect us in a specific
                            situation, and these explanations should be clear enough that the
                            average person will be able to understand them... <q>The terms of
                                service for an AI application — or any service that uses algorithmic
                                decision-making processes — should be written in language plain
                                enough that a third grader can comprehend it... It should be
                                available in every language as soon as the application goes
                                live.</q>
                            <ptr target="#samuel2019"/></quote>.</cit></p>

                <p>Consequently, Explainable AI has become known as transparent AI because it
                    attempts to design AI systems whose actions can be easily understood by humans.
                    These new AI systems are designed to produce more <quote rend="inline"
                        >explainable models, while still maintaining a high level of learning
                        performance</quote> and prediction accuracy thus helping humans to <quote
                        rend="inline">understand, appropriately trust, and effectively manage the
                        emerging generation of artificially intelligent partners</quote>
                    <ptr target="#gunning2017"/>. This means that XAI systems should have to have
                    the ability to explain their rationale, characterise their strengths and
                    weaknesses, and convey an understanding of how they will behave in the future in
                    order to strengthen their public accountability. These requirements pose a very
                    difficult challenge to the developers of these systems and remain aspirational
                    in AI system design. </p>

                <p>One of the key drivers for the attention given to explainability has been a wider
                    public unease with the perceived bias of algorithms in everyday life, especially
                    in the rise in automated decision processes and the calls for accountability in
                    these systems (see <ptr target="#sample2017"/>
                    <ptr target="#kuang2017"/>). Many of these debates foreground the question of
                    the future of humanity and the kinds of societies that these technologies create
                    the conditions for. These implications are increasingly discussed in the media
                    and in politics, particularly in relation to a future dominated by technologies
                    which are thought to have huge social consequences. Computation combined with
                    artificial intelligence and machine learning has raised challenging questions
                    about creativity, post-work futures, mass unemployment, AI controlled drone
                    systems, and surveillance capitalism amongst other impacts. These are important
                    issues, but here I drill down to focus on the cognitive and explanatory issues. </p>

                <p>The discussion I wish to present in this paper is largely speculative. My aim is
                    to explore how the cognitive capacities of humans might be strengthened by
                    developing that capacity for explanatory modes of thought through the use of
                    explainability as a critical concept. It seems to me that we have two issues
                    that are interesting to consider. Firstly, the GDPR requires digital
                    technologies, such as automated decision systems (ADS), to be explainable in
                    some sense and therefore pose a problem of representation.<note><p>This means
                            that algorithmic systems are required to provide their processing
                            descriptions under this <q>right to explanation</q> and potentially
                            giving rise to a critical field such as Explainable Digital Studies –
                            XDS.</p></note> Secondly, interpretation problems stem from a difficulty
                    in translating a highly complex processual system that does not immediately lend
                    itself to easy explanation for a number of difficult reasons. Explanation has
                    nonetheless become expected as part of the political and legislative response to
                    concerns over algorithmic inequality, bias and the opaqueness of computational
                    systems. </p>

                <p>In the first section of this paper, I seek to outline the contours under which
                    this critique becomes urgent by an initial examination of cognitive
                    infrastructures. In the second section, I turn to think about the concept of
                    explainability and its potential for developing a possible tactic in response to
                    the wider toxicity generated by algorithmic governance. The aim is to offer an
                    immanent critique of the notion of explainability. By immanent critique, I refer
                    to an approach drawn from the Frankfurt school, whereby the internal terms and
                    concepts within a system are examined in relation to the reality of the claims
                    they make about and the actuality of the world. Thus, computational systems are
                    justified both discursively and in terms of their internal logics and yet there
                    are contradictory tendencies in these supposedly univocal systems. Discourse and
                    algorithms become a technique to exercise power, for example through
                        <term>nudging</term> strategic behaviour for shaping the labour, both
                    physical and mental, of users in specific digital environments, however these
                    behavioural techniques do not always produce the desired effect. Although
                    behavioural logics of control operate in our everyday lives which are subject to
                    algorithmic management from increasingly prevalent hyper-individualised
                    capillaries of power, there remain spaces of contestation. The justificatory
                    move to explainability as a panacea for these systems is therefore an important
                    diagnostic site for interrogating algorithms' power and ubiquity. <note><p>These
                            issues are explored in depth in the work of <ptr target="#irani2015"/>
                            who focuses on how social conflict is mediated through particular
                            assemblages of algorithmic systems.</p></note>
                </p>


            </div>

            <div>
                <head>1. Thinking Infrastructures</head>

                <p>One of the most difficult tasks facing the critical theorist today is
                    understanding the delegation and prescription of agency in digital
                    infrastructures. Due to their size and complexity these infrastructures are
                    capital intensive systems and hence tend to be developed by corporations or
                    governments in order to combine multiple systems into a single unity. In this
                    form they point towards a unification of multiple grammars within a system of
                    communication, such that they converge on a single ontology or technical stack.
                    This tendency eventually allows for an underlying infrastructure to be
                    commoditised as an external product in its own right, such as shown with the
                    Amazon Web Services (AWS) system. AWS was originally created for Amazon's
                    internal purposes as a corporate retentional system. Since 2006 it has become a
                    key infrastructure with an annual income of its own of $17.1 billion (8% of
                    Amazon’s annual revenues in 2017) and is used by customers and even competitors
                    for various forms of so-called cloud computing. These infrastructures can be
                    understood as systemic, themselves made up of a number of component layers, but
                    nonetheless constituting a distinct digital totality and increasingly structured
                    through the data architecture made possible through the implementation of edge,
                    core and cloud compute. Edge devices, such as smartphones, feed data into core
                    (on-premises large computing data centres) for algorithmic processing, or to
                    cloud (off-premises shared data servers) to run AI models or complex operations.
                    This network topology is often called the edge-to-core-to-cloud pipeline for
                    efficiently processing data, moving data to algorithms located where the
                    processing power is best located. </p>
                <p>The patterning of these layers of computation into vast laminated systems creates
                    what I call <term>infrasomatization</term> (see <ptr target="#berry2016"/>).
                    This notion draws on the work of Bernard Stiegler who has pointed to Alfred J.
                    Lotka's and Nicholas Georgescu-Roegen's notion of <term>exosomatization</term>
                    as a crucial means of understanding computational capitalism (see <ptr
                        target="#bobulescu2015"/>
                    <ptr target="#stiegler2016" loc="p. 95–96"/>).<note><p>Exosomatization and
                            endosomatization have been deployed by Stiegler to think about human
                            augmentation and digital technologies, particularly in relation to the
                            anthropocene and the counter-entropic move towards a
                                <term>neganthropocene</term> (see for example, <ptr
                                target="#stiegler2015"/>
                            <ptr target="#stiegler2018"/>).</p></note> Exosomatization and
                    endosomatization were developed by Lotka and Georgescu-Roegen in their work on
                    ecological economics and by Karl Popper in relation to what he called objective
                    knowledge (see <ptr target="#lotka1925"/>
                    <ptr target="#georgescu-roegen1970"/>
                    <ptr target="#georgescu-roegen1972"/>
                    <ptr target="#georgescu-roegen1978"/>
                    <ptr target="#popper1972"/>). Exosomatization can be understood as the use of
                    tools (from Greek <foreign xml:lang="grc">exō</foreign> meaning <q>outside</q>),
                    whereas endosomatization is the evolutionary adaptation of bodies into claws,
                    nails, shells, etc. (from Greek <foreign xml:lang="grc">endon</foreign> meaning
                        <q>within</q>), <foreign xml:lang="grc">soma</foreign>, of course is from
                    the Greek <foreign xml:lang="grc">sōma</foreign> meaning <q>body.</q></p>
                <p>Whilst these have been important contributions, by introducing a third term,
                        <term>infrasomatic</term>, I want to argue that we should move beyond a
                    binary of either endosomatic or exosomatic. I think this notion captures the
                    reticular nature of specific forms of digital technologies, which create new
                    non-human agencies and, potentially, unpredictable entropic effects – so
                    infrasomatization combines the notion of using software, information and
                    automation to create infrastructures. To concentrate on the notion of
                    infrasomatization, is to try to understand the particularity of how algorithms
                    are deployed as a new form of cognitive infrastructure. That is, algorithms are
                    not just exosomatizations, not just the production of tools or instruments.
                    Infrasomatizations are created by the combination of other infrastructural
                    systems. Indeed, infrasomatizations rely on a complex fusion of endosomatic
                    capacities and exosomatic technics leading to what Berns and Rouvroy call
                    algorithmic governance <ptr target="#berns-rouvray2013"/> and Stiegler has
                    called the automatic society <ptr target="#stiegler2016"/>. Infrasomatizations
                    can be thought of as social-structuring technologies – inscribing new forms of
                    the social (or, in a neoliberal register, sometimes the <q>anti-social</q>) onto
                    the bodies and minds of humans and their institutions. They are made to be
                    always already poised for use, to be configured and reconfigured, and built into
                    particular constellations that form the underlying structures for the creation
                    of social subjects. Infrasomatizations have an obduracy that can be mobilised to
                    support specific instances of thought, rationality and action. So, for example,
                    in the case of social media, the technical infrastructure introduces a new
                    element overtaking and reconfiguring social relations through a new grammar of
                    communication prescribed by these technologies. This results in changes in
                    social relations and consequently social being. Infrasomatizations can be
                    understood to operate in a similar manner to an infra-law, which Foucault
                    described as,<cit><quote rend="block">extend[ing] the general forms defined by
                            law to the infinitesimal level of individual lives; or they appear as
                            methods of training that enable individuals to become integrated into
                            these general demands. They seem to constitute the same type of law on a
                            different scale, thereby making it more meticulous and more indulgent
                            </quote><ptr target="#foucault1995" loc="222"/>.</cit>
                </p>
                <p>Infrasomatizations similarly have the capacity to operate across different scales
                    with remarkable fidelity, from micro-targeting of nudges, to aggregated groups
                    or <q>universes</q> of individuals which can be manipulated simultaneously. The
                    term <term>infrasomatization</term> also gestures toward a kind of gigantism,
                    the sheer massiveness and interconnectedness of fundamental computational
                    technologies and resources. The infrastructural dimension of these
                    infrasomatizations means that they can be scaled to the level of planetary
                    technics, as their physical location, particularly when presented as
                    computational abstractions such as notions of compute, can be strategically
                    placed (and moved) dynamically and geographically. Compute, in this sense, is an
                    abstract unit of computation which tends to be priced at a particular level by
                    cloud server companies so one can purchase a certain capacity of computation.
                    The cloud infrastructures' size contrasts with the phenomenological experience
                    of the minuteness or ephemerality of the kinds of personal devices that are
                    increasingly merely interfaces or gateways to underlying <q>smart</q>
                    infrasomatic systems. For example, we might consider how technologies of
                    location are made possible by the geospheric locative satellites, in particular
                    GPS, but also extrapolation from WiFi, camera and audio data. Location is as
                    crucial to the development of infrasomatizations as is the machine-learning of
                    abstract patterns in data. This is because location provides important context,
                    and this context enables smarter abductions to be made with data, assuming as it
                    does that a specific piece of information, practice or action makes more sense
                    within a particular place.<note><p>Equally important is the overlaying of
                            computational and therefore calculable layers over the physical
                            environment. These layers are crucial for next generation
                            infrasomatizations, using maps and other locative
                        technologies.</p></note> This is manifested in a dual structure which has a
                    physical and logical geography often encoded simultaneously into
                    infrasomatizations. The first kind of location that are understood within the
                    computational systems of infrasomatizations tend to be place-poor, lacking an
                    understanding of the specificity of place and tend towards a calculative,
                    instrumental Cartesian representation of space. This is in marked contrast to
                    the phenomenological experience of place infused with mood, texture,
                    relationships and materiality <ptr target="#evans2015"/>.<note><p>For example,
                            in terms of the technical transformation of place we might consider the
                            softwarization of the home – a site of so-called micro-location. Its
                            conversion into an algorithmic space is a process which is now well
                            under way and which involves transforming dumb things into smart objects
                            through the use of artificial intelligence. But AI cannot function
                            without data, large amounts of data, to help them understand the world.
                            Smart devices need to watch and record us, harvesting vast quantities of
                            data, so that our every activity can be captured by sensors and cameras
                            embedded within them. One of the more contentious recent examples is the
                            proposal by Amazon to build a surveillance-as-a-service system. In this
                            patented system, the company aims to use its network of delivery drones
                            to keep watch over customers' houses using location data to form a
                            flying Neighbourhood Watch drone system. It is suggested that customers
                            could request that Amazon's drones visit their property hourly, daily,
                            or weekly, and the drones would look for signs of break-ins, such as
                            smashed windows, doors left open, and intruders lurking on people's
                            property (<ptr target="#porter2019"/>). The patent further suggests that
                            drones could be equipped with night vision cameras and microphones to
                            expand their sensing capabilities <ptr target="#uspto2019a"/>. This is
                            in addition to an earlier patent application envisions using a
                            combination of Amazon Ring doorbell cameras and facial recognition
                            technology to build a system that could be used to match images of
                            people who show up at your door to a <quote rend="inline">suspicious
                                persons</quote> database (<ptr target="#uspto2019b"/>, <ptr
                                target="#meek2019"/>). It goes without saying that these activities
                            produce useful raw data in vast quantities and for which more intensive
                            surveillance systems are being built.</p></note> The second is a
                    technical geography overlaid onto this grid, as noted above, with the division
                    into engineering data and processing distribution over a system division of
                    edge, core and cloud. Although these are invisible to the user, this new
                    secondary tripartite division of the computational is arguably more important
                    and increasingly saturates everyday life, due to the infrasomatic distribution
                    of processing and analysis that this structure requires and makes
                            possible.<note><p>This highlights the importance of the relationship
                            between the instrumental imposition of location, understood technically
                            as geo-fencing, against that of what Bernard Steigler is increasingly
                            referring to as locality, a counter-computational politics of place (see
                                <ptr target="http://internation.world"/> ).</p></note>
                </p>
                <p>A process of cybernetic feedback, where the system is able to self-monitor across
                    this geography means that infrasomatic systems strengthen and grow. For example,
                    the computational capacities of Amazon's infrastructural systems increase their
                    reach and power from the use of its client's computational practices and
                    metadata. An infrasomatization thereby learns from its usage, which creates an
                    amplification loop which eventually cements its functionality as a computational
                    necessity – it becomes <q>smart.</q> It knows when to move computational
                    capacity from cloud to core, when to move compute resources into specific
                    geographic locations and when to cache data requests across the system's
                    geographic spread. Hence by extrapolating and scaling these learning systems, a
                    private corporate retentional system becomes first a regional and then planetary
                    one. This is commonly referred to as <term>Infrastructure As A Service</term>
                    (IAAS). One of the key elements towards understanding these large-scale
                    infrasomatizations is that they tend towards a logic of value extraction. That
                    is, that their size and scale create a tendency that is manifest in the
                    algorithms that make up these systems towards data capture and its
                    intensification towards the maximisation of rent-seeking behaviour. This is
                    largely a logic dictated precisely from the fact that many of these systems tend
                    towards monopoly or oligopolistic behaviour – what Peter Thiel infamously
                    referred to as a move from <quote rend="inline">zero to one</quote>
                    <ptr target="#masters2015"/>. This is because as a disparate collection of
                    digital subsystems is subsumed within a larger totality, the utility of this
                    system eventually becomes overwhelming and cost-effective such that moving or
                    exiting an infrastructure is increasingly prohibitive. This creates the
                    possibility for monopoly rent on the infrastructure and hence drives the
                    tendency toward gigantic informational systems, and monopoly-oriented
                    corporations. Thus, the principal means of value extraction enacted in these
                    infrasomatizations tends to be through the control of multiple monopolies at
                    different layers of the technical stack. It goes without saying that this is an
                    extremely profitable means of extracting value creating new forms of powerful
                    companies, such as the <name style="quotes">FAANG</name> corporations (Facebook,
                    Apple, Amazon, Netflix, Google).</p>

                <p>Within popular culture, a wider social concern with algorithms can be seen in the
                    social media which have been used to highlight the inexplicable ways in which
                    people's lives have been affected by an algorithmic decision. Sometimes these
                    discussions reflect a confusion by users over the distinction between
                        <term>noise</term>, where the decision is affected by incomplete or
                    inaccurate data causing inconsistency or no decision being made, and
                        <term>bias</term>, where the accuracy of a decision has been swayed by a
                    predetermined or computed result affected by human biases (see <ptr
                        target="#jaume-palasi2018"/>). These have been used to justify a need for
                    explanation to help the public understand algorithms. Bias in computer systems
                    usually derive from either (1) data-driven bias, where the biases are embedded
                    in the data itself, (2) bias through interaction with humans, for example
                    Microsoft’s Tay chatbot which developed a fascist conversation style (3)
                    emergent bias, for example through likes and shares, (4) similarity bias, where
                    filter bubbles can emerge, and (5) conflicting goals bias, where stereotypes
                    have been used in the development of the software in particular ways <ptr
                        target="#hammond2016"/>. Indeed, there are now many documented cases where
                    algorithmic decision processes have discriminated against people on the basis of
                    their names, their home address, gender or skin colour <ptr
                        target="#buranyi2017"/>
                    <ptr target="#eubanks2017"/>
                    <ptr target="#noble2018"/>.<note><p>This has even resulted in families and
                            groups being deliberately separated by algorithms for profit, or AI
                                <q>scans</q> for a babysitter with <quote rend="inline">respect and
                                attitude</quote>
                            <ptr target="#harwell2018"/>.</p></note> This is reflected in an <quote
                        rend="inline">anxiety felt by those who fear the potential for bias to
                        infiltrate machine decision-making systems once humans are removed from the
                        equation</quote>
                    <ptr target="#casey2018" loc="4"/>. It is in this context that public disquiet
                    has risen in relation the perceived unfairness of these, often unaccountable,
                    automated algorithmic systems. </p>

                <p>So Facebook, for example, has created an infrasomatization for capture and
                    exploitation of the social graph, particularly digital identity, through its
                    social network and the creation of facial recognition systems such as Detectron
                        <ptr target="#facebook2019"/>. Google similarly has created
                    infrasomatizations for the various functions of search, compute, storage and
                    databases, networking, big data, and cloud AI, identity and security, Internet
                    of Things (IoT), API platforms, and location services, such as Google Maps.
                    These are often built extremely quickly and issues of bias are rarely considered
                    as part of this engineering effort. This phase of digital transformation is
                    easily missed as it takes place behind the interface in proprietary corporate
                    environments, and as such is a non-visual dimension of a computational mode of
                    development. Through these infrasomatic logics, cultural practices are captured
                    and rearticulated through grammars of action which can be used to describe, and
                    then build infrasomatizations that may become cultural monopolies in their own
                    right. For example, the contemporary emergence of a vast social system
                    structured around social media which inculcates a craving for <q>likes</q>,
                        <q>followers</q>, <q>subscribers</q> and <q>views</q> directly connected to
                    a political economy of advertising, marketing and consumption is only the most
                    obvious contemporary manifestation of this process. </p>

                <p>The implications of this new system of exploitation is the creation of
                    constellations of infrasomatizations that can be mobilised into de facto
                    monopolies in specific imbrications. This, I would argue is a better way to
                    understand these computational structures rather than the notion of
                        <q>platforms</q> that tends to use a self-description favoured by companies
                    in Silicon Valley itself, and therefore hides more than it reveals. These
                    infrasomatic systems are able to extract rent or tolls to pass data and
                    calculations around a system – whether measured in terms of compute, data
                    traffic or time. The forms of data they carry, even if only manifest as abstract
                    metadata, are in themselves extremely valuable. Even if a particular customer of
                    the infrasomatization may expressly prohibit the harvesting of their own data
                    they cannot control secondary data produced as a result of their interaction on
                    the system. This infrasomatic data offers another means of value extraction,
                    both in terms of predicting the future growth of these infrastructures, but also
                    the potentials for new circulations of data and logic for profit. By use of
                    these multiple <q>data exhausts</q>, the owners of these infrasomatizations are
                    able to capture trends, identify social tendencies and patterns, and to
                    reincorporate this knowledge into their infrasomatic ecology, and depending on
                    the corporation, feed this information back into circulation to amplify these
                    tendencies in a profitable direction. Within Silicon Valley this is understood
                    as the capacity of a digital company to create a <q>moat</q> which prevents
                    competitors from disrupting their business model, rather like a castle with a
                    moat surrounding it to prevent attack and capture. The creation of an
                    infrasomatic layer is, therefore, not just a digital logic, it is also a
                    business logic. These logics reinforce each other, creating a structure that,
                    given enough physical computing infrastructure can scale at an exponential pace,
                    and thereby capture value and create a kind of dependency in its customers and
                    users very quickly. </p>

                <p>The need to convert this raw data from its digital logic into a business logic
                    has consequently resulted in major breakthroughs in artificial intelligence,
                    particularly machine learning, through the creation of classification and
                    filtering systems modelled on brain structures, and the underlying neurons.
                    Consequently, we see a growing use of computational systems to abstract,
                    simplify and visualize the amount of <q>Big Data</q> that is being collected. A
                    side-effect of this has been to reinforce a tendency towards causal and
                    statistical models to map, understand, and interpret complex social and cultural
                    phenomena. For example, in 2008 Chris Anderson famously announced the <q>End of
                        Theory</q> as he claimed the data deluge had made the scientific method
                    obsolete. Indeed, he argued that <quote rend="inline">we can stop looking for
                        models, instead we can analyze data without hypotheses</quote>. He further
                    argued that we can <quote rend="inline">throw the numbers into the biggest
                        computing clusters the world has ever seen and let statistical algorithms
                        find patterns where science cannot</quote> and that <quote rend="inline"
                        >with enough data, the numbers speak for themselves</quote>
                    <ptr target="#anderson2008"/>. But of course, this shift to statistical
                    explanation is not neutral, rather it is linked to the emergence of a political
                    economy specific to the computational. In this data-based accumulation regime
                    social life is transformed into calculable and predictable social trends which
                    may be manipulated and channelled. The most striking example of this regime is
                    the use of Facebook data by the company Cambridge Analytica which they argued
                    could create psychographic models which could then be <q>nudged</q> to influence
                    behaviour. The alleged result of these techniques includes the Brexit referendum
                    result and the election of Donald Trump as president in 2016 <ptr
                        target="#guardian2018"/>. Althought their efficacy remains contested these
                    nascent techniques are continually refined and improved and moved from a
                    communicational terrain to a cognitive one.</p>

                <p>Computation today means to be in the middle of things, it is no longer an end,
                    but rather a means, a passage-way between two points: from dumb to smart. In
                    becoming smart devices, computational systems transform everyday life into what
                    can be thought of as a vast oil field of data, awaiting extraction by a new set
                    of digital cultural industries. It is of no surprise that FAANG (Facebook,
                    Apple, Amazon, Netflix and Google), the leaders of the technology industry, are
                    racing to create the technologies for their vision of a digital life.
                    Mathematician and architect of supermarket giant Tesco's Clubcard, Clive Humby,
                    described data as the new oil in 2006 <ptr target="#palmer2006"/>. It is
                    increasingly clear that we are now in the middle of an oil rush at the centre of
                    which lies our lives. As Wired explains, <quote rend="inline">like oil, for
                        those who see data's fundamental value and learn to extract and use it there
                        will be huge rewards</quote><ptr target="#toonders2014"/>. Humby further
                    argues that <quote rend="inline">data is just like crude. It's valuable, but if
                        unrefined it cannot really be used. It has to be changed into gas, plastic,
                        chemicals, etc to create a valuable entity that drives profitable activity;
                        so must data be broken down, analyzed for it to have value</quote>. But it
                    is not just the one-off collection of data, it is the iterative gathering of
                    data, repeated again and again that creates the conditions for these possible
                    insights. The oil fields of life will not soon be spent, instead they will yield
                    greater and greater quantities of data, from which more profit can be
                        earned.<note>See also Jim Balsillie who argued that <cit><quote rend="block"
                                    ><q>data is not the new oil – it’s the new plutonium</q> and
                                that <q>data at the micro-personal level gives technology
                                    unprecedented power to influence ... Amazingly powerful,
                                    dangerous when it spreads, difficult to clean up and with
                                    serious consequences when improperly used</q></quote>
                            <ptr target="#balsillie2019"/>.</cit></note>
                </p>
                <p>This extractive metaphor serves not only Silicon Valley but also inspires
                    governmental policy. For example, Meglena Kuneva, European Consumer
                    Commissioner, has without blinking, described personal data as <quote
                        rend="inline">the new oil of the internet and the new currency of the
                        digital world</quote>
                    <ptr target="#kuneva2009"/>. The UK Office for National Statistics has argued
                    that <quote rend="inline">if data is the new oil, open data is the oil that
                        fuels society and we need all hands at the pump</quote>
                    <ptr target="#davidson2016"/>. What makes data into open data, is that it is
                    free of intellectual property restrictions that prevent it from being used by
                    others by publishing constraints, such as copyright, or that it is owned
                    exclusively by its creators. Open data, like open access publications and open
                    source before them, grants a corporation the right to dice up and remix data.
                    When you use your smartphone, or a smart object, the first thing that has to be
                    clicked is the agreement to let companies extract and use this data. As the New
                    York Times argues,<cit><quote rend="block">Personal data is the oil that greases
                            the Internet. Each one of us sits on our own vast reserves. The data
                            that we share every day — names, addresses, pictures, even our precise
                            locations as measured by the geo-location sensor embedded in
                            Internet-enabled smartphones — helps companies target advertising based
                            not only on demographics but also on the personal opinions and desires
                            we post online</quote>
                        <ptr target="#sengupta2012"/></cit>. </p>
                <p>These claims reflect what we might call a <term>cult of data-ism</term> and a
                    renunciation of the extended and important role of critical reason and
                    theoretical thinking in modern society. But this data-ism extends beyond the
                    mere collection of data and its analysis. Data that is collected in <q>data
                        lakes</q> can be used to formulate behavioural and predictive logics which
                    can provide useful interpretative and calculative advantages to corporations.
                    They then provide the opportunity for algorithmic interventions – what I call
                        <term>algoventions</term> – into patterns of behaviour or thought. These
                    practices have been increasingly extended across society, but possibly the most
                    intensive and ambitious use of these infrasomatic technologies takes place in
                    so-called <term>smart cities</term>. Here the city is built from the ground up
                    to facilitate the data capture and feedback loops to make possible a management
                    and organisational control layer over the city giving top-view to city
                    officials, but also selectively sharing data with corporations and individuals
                    to use in their everyday activities. For example, public transport usage and
                    problems can be collected, aggregated and circulated back to the users of the
                    public transportation system to provide them with early-warnings of issues,
                    propose alternate routes, or to alert them to major outages in a system. Smart
                    cities, and their underlying infrasomatizations, are strongly coupled to
                    geolocation data, indeed, the grid of the city is a key abstract principle upon
                    which the data about a city is projected. By unifying multiple data streams
                    derived from smart infrastructures, smart city computers can create realtime
                    digital twins which attempt to create and thereby impose a data-centric spatial
                    logic onto city life. These systems are tasked with classifying, understanding,
                    and predicting future states of the digital twin of the city, that is the city
                    as a gigantic finite-state machine built on the collection of massive amounts of
                    civic, corporate and personal data. </p>
                <p>Through a combination of these techniques, infrasomatizations are created which
                    produce smart technologies that act as gateways that open out to spatial forms
                    of organisation that delegate a locative-calculative model onto the user,
                    structuring the world in terms of an index of spaces that are given relational
                    properties within the row and column structure of the underlying tables and
                    databases. We therefore need to contest this new social pattern and develop
                    alternative visions – part of which can be through creating new tools and new
                    modes of working with technology, but we also need tool criticism and a research
                    programme dedicated to understanding the algorithmic condition.<note><p>Academia
                            is itself in the middle of a digital revolution, the outlines of which
                            are still only dimly perceived. For example, open access licenses create
                            the data foundations for gigantic systems of surveillance to be built to
                            monitor, manage and control academic labour. University management are
                            enthusiastically building new collection systems using these open access
                            licenses as their foundations (often with the tacit approval of academic
                            faculty, librarians and researchers). This situation is happening right
                            under the noses of academics who are swayed by moralistic arguments
                            about participation and the sharing of knowledge, but which will
                            actually result in the bypassing of historical and hard-won principles
                            of academic freedom built on the notion that academic labour means that
                            the copyrights belong in the first instance to the scholar, not to the
                            university. This was originally developed as a practice to protect the
                            rights of academics who could choose where to publish their work without
                            limitation. These rights are now carelessly discarded with little
                            critical thought as to the unintended consequences of a restriction of
                            publication into open access venues. One of the most immediate effects
                            is that for the first time in history, universities can, without
                            restriction, build monitoring systems for publication at a very fine
                            granularity because they do not have to worry about infringing academic
                            rights of publication. These systems create accounting logics,
                            themselves linked to performance monitoring, and eventually a policing
                            function over academic labour. Open access has thereby become a
                            political doxa and a technical system of organisation and management.
                        </p></note>
                </p>
            </div>
            <div>
                <head>2. The Explainability Turn</head>
                <p>It is clear that in the context of infrasomatizations, the first important
                    question we need to consider is what counts as an explanation. Indeed,
                    explanations are generally considered to be able to tell us how things work and
                    thereby giving us the power to change our environment in order to meet our own
                    ends. In this sense of explanation then, science is often supposed to be the
                    best means of generating explanations <ptr target="#pitt1988" loc="7"/>. So,
                    with a stress on the importance of explanation, the GDPR makes it a criterion of
                    adequacy for satisfactory use of algorithmic decision systems in the European
                    Union, and thereby legitimating their use in a multitude of settings. Thus,
                    explainability and the underlying explanation are linked to the question of
                    justification. So, what then is an explanation? </p>

                <p>Hempel and Oppenheim <ptr target="#hempel-oppenheim1988"/> argue that an
                    explanation seeks to <quote rend="inline">exhibit and to clarify in a more
                        rigorous manner</quote>. Some of the examples they give include whole
                    temperature reading from a mercury thermometer, which can be explained using
                    physical properties of the glass and of mercury which has been rapidly immersed
                    in hot water. Similarly, they present the example of an observer of a row boat
                    where part of the oar is submerged under water and appears to be bent upwards
                        <ptr target="#hempel-oppenheim1988" loc="10"/>. An explanation therefore
                    attempts to explain with reference to general laws. Mill argues that <quote
                        rend="inline">an individual fact is said to be explained by pointing out its
                        cause, that is, by stating the law or laws of causation, of which its
                        production is an instance</quote> and that <quote rend="inline">a law or
                        uniformity in nature is said to be explained, then another law or laws are
                        pointed out, of which that law itself is, but a case, and from which it
                        could be deduced</quote>
                    <ptr target="#mill1858"/>. Similarly, Ducasse argued in 1925 that <quote
                        rend="inline">explanation essentially consists in the offering of a
                        hypothesis of fact, standing to the fact to be explained as case of
                        antecedent to case of consequent of some already known law of
                        connection</quote>
                    <ptr target="#ducasse2015" loc="37"/>. Hempel and Oppenheim therefore argue that
                    an explanation can be divided into its two constituent parts, the
                        <term>explanadum</term> and the <term>explanans</term>, <cit><quote
                            rend="block">By the explanandum, we understand the sentence describing
                            the phenomenon to be explained (not the phenomenon itself); by the
                            explanans, the class of those sentences which are adduced to account for
                            the phenomenon</quote>
                        <ptr target="#hempel-oppenheim1988" loc="10"/>.</cit></p>

                <p>In this sense of an explanation, the explanandum is a logical consequence of the
                    explanans. The explanans itself <quote rend="inline">must have empirical
                        context, that is, it must be capable, at least in principle, of test by
                        experiment or observation,</quote> which creates conditions for testability.
                    However, this causal mode of explanation can become inadequate in fields
                    concerned with purposive behaviour, as with infrasomatic digital systems. </p>

                <p>In this case it is common for reference to purposive behaviour, such as in
                    so-called machine behaviour, to be given in relation to <q>motivations</q> and
                    therefore for teleological rather than causal explanation. Thus, the goals
                    sought by the system are required in order to provide an explanation.
                    Teleological approaches to explanation may also make us feel that we really
                    understand a phenomenon because it is accounted for in terms of purposes, with
                    which we are familiar from our own experience of purposive behaviour. One can,
                    therefore, see a great temptation to use teleological explanation in relation to
                    AI systems, particularly by creating a sense of an empathetic understanding of
                    the <q>personalities of the agents.</q> So, a proposed explanans might sound
                    suggestively familiar, but <quote rend="inline">upon closer inspection proves to
                        be a mere metaphor, or to lack testability, or to include no general law,
                        and therefore to lack explanatory power</quote>
                    <ptr target="#hempel-oppenheim1988" loc="17"/>. In relation to explanation,
                    therefore, explainability needs to provide an answer to the question <q>why?</q>
                    Scriven argues that <quote rend="inline">the right description is the one which
                        fills in a particular gap in the understanding of the person or people to
                        whom the explanation is directed</quote>. This can be seen as the value of
                    explainability as <quote rend="inline">closing the gap in understanding (or
                        rectifying misunderstanding)</quote>
                    <ptr target="#scriven1988" loc="53"/>.</p>

                <p>It is clear that the concept of explainability, and the related practices of
                    designing and building explainable systems, have an underlying theory of general
                    explainability, but also a theory of the human mind. These two theories are
                    rarely explicitly articulated in the literature, and I want to bring them
                    together to interrogate how explainability cannot be a mere technical response
                    to the contemporary problem of automated decision systems, but actually requires
                    philosophical investigation to be properly placed within its historical and
                    conceptual milieu. </p>
                <p>The next important move is to connect the concept of explanation to automated
                    decision systems and the explanations that they can provide. As shown above,
                    writers such as Friedman have argued that explanation is almost always
                    explanation of laws as a general regularity or pattern of behaviour more typical
                    of the physical sciences <ptr target="#ruben2016" loc="4"/>. But far too many
                    discussions of explanation assume that what can be said about <term>scientific
                        explanation</term> exhausts what of interest there is that can be said about
                        <term>explanation</term>. However, in relation to algorithmic systems what
                    we tend to be talking about is what Ruben has called <quote rend="inline"
                        >singular explanation</quote> Additionally, explanation is ambiguous as it
                    may refer to the product or to a process, so as Bromberger points out, an <quote
                        rend="inline">explanation may be something about which it makes sense to
                        ask: How long did it take? Was it interrupted at any point? Who gave it?
                        When? Where? What were the exact words used? For whose benefit was it
                        given?</quote> (Bromberger, quoted in <ptr target="#ruben2016" loc="6"/>).
                    The other form of explanation <quote rend="inline">may be something about which
                        none of the [preivous] questions make sense, but about which it makes sense
                        to ask: Does anyone know it? Who thought of it first? Is it very
                        complicated?</quote>
                    <ptr target="#ruben2016" loc="6"/>.  So, in speaking of an explanation one might
                    be referring to an act of explaining, or to the product of such an act.</p>

                <p>It certainly seems to be the case that the right to explanation that is being
                    developed in relation to the GDPR is chiefly interested in the idea of an
                    explanatory product. Thus, an <q>explanatory product</q> can be characterised
                    solely in terms of the kind of information it conveys, no reference to the act
                    of explaining being required. The question therefore becomes, what information
                    has to be conveyed in order to have explained something? So in terms of the
                    requirements given, the function of explanation is that explanation should
                    enable us to understand why something has happened within an automated decision
                    system. Crucially, this connection between an explanatory product and the legal
                    regime that enforces it has forced system designers and programmers to look for
                    explanatory models that are sufficient to provide legal cover, but also at a
                    level at which they are presentable to the user or data subject. It is also
                    uncertain if the <quote rend="inline">right is only to a general explanation of
                        the model of the system as a whole ('model-based' explanation), or an
                        explanation of how a decision was made based on that particular data
                        subject's particular facts ('subject-based' explanation)</quote>
                    <ptr target="#edwards2018" loc="4"/>. This is not an easy requirement for any
                    technical system, particularly in light of the growth of complicated systems of
                    systems, and the difficulty of translating technical concepts into everyday
                    language. It might therefore be helpful to think in terms of full and partial
                    explanation, whereby a partial explanation is a full explanation with some part
                    left out. That is, that, while presenting a complicated system of automated
                    decision systems, it is likely pragmatically that explanations will assume an
                    explanatory gap, assuming that the data subject is in possession of facts that
                    do not need to be repeated. It will be interesting to see if the implementation
                    of these systems results in an explanatory pragmatism, and how the legal system
                    responds. </p>

                <p>This of course leads to the danger of creating persuasive explanations rather
                    than transparent explanations or a pragmatic explanation drawing on the notion
                    of a <q>good enough</q> explanation. It also raises questions related to the
                    over-simplication of explanations or misleading explanations and how one might
                    challenge them or even question their underlying explanatory model.<note>For
                        example, the user might be able to challenge an explanation or appeal to a
                        higher authority if it were considered inadequate.</note> This difficulty
                    might explain the recent turn towards explainability through the notion of
                    machine behaviour, drawing on insights drawn from research on humans and animals
                    applied to machines.<note>One is tempted to assume that some developers believe
                        that behavioural models of automated systems will be easier to describe,
                        perhaps as black-boxed input-output models, or that a user will naturally
                        find these descriptions more comprehensible. There is some similarity
                        between <quote rend="inline">machine behaviour</quote> approaches and the
                        thinking behind the idea of so-called <quote rend="inline">counter-factual
                            explanations</quote> proposed by <ptr target="#wachter2018"/>, which
                        assumes that by changing the input conditions a counter-factual output can
                        be presented to the user. They argue that <cit><quote rend="block"
                                >counterfactuals bypass the substantial challenge of explaining the
                                internal workings of complex machine learning systems. Even if
                                technically feasible, such explanations may be of little practical
                                value to data subjects. In contrast, counterfactuals provide
                                information to the data subject that is both easily digestible and
                                practically useful for understanding the reasons for a decision,
                                challenging them, and altering future behaviour for a better
                                result</quote>
                            <ptr target="#wachter2018" loc="860"/></cit>. Note how this conveniently
                        avoids the problematic of explanation of the underlying algorithm and
                        instead resituates the responsibility for changing <q>behavioural</q>
                        outcomes onto the individual. This looks less like a <q>right to
                            explanation</q> than a means to avoid the social responsibilities on
                            <q>data processors</q> implicit in explainability by creating a
                            <q>minimal form of explanation</q>. Which even they have to concede
                            <q>counterfactuals may be insufficient in themselves</q>
                        <ptr target="#wachter2018" loc="883"/>.</note> These researchers argue,
                            <cit><quote rend="block">in the context of machines, we can ask how
                            machines acquire (develop) a specific individual or collective
                            behaviour. Behavioural development could be directly attributable to
                            human engineering or design choices…. [or] a machine may acquire
                            behaviours through its own experience. For instance, a reinforcement
                            learning agent trained to maximize long-term profit can learn peculiar
                            short-term trading strategies based on its own past actions and
                            concomitant feedback from the market… In the study of animal behaviour,
                            adaptive value describes how a behaviour contributes to the lifetime
                            reproductive fitness of an animal. In the case of machines, we may talk
                            of how the behaviour fulfils a contemporaneous function for particular
                            human stakeholders </quote>
                        <ptr target="#rahwan2019" loc="480"/>.</cit></p>

                <p>So underlying the concept of explainability is the assumption that algorithms are
                    themselves explainable and following from that, that algorithms are something
                    that can be explained to a human. This further assumes that the interpretative
                    activity that humans are capable of can be mobilised to understand algorithms,
                    or at least their active computational dimension. But the concept also assumes
                    that there exists what we might call a general algorithmic explainability, in
                    other words that all computational processes can be rendered as an explanation,
                    and therefore explained with recourse to a translation into the discursive or
                    symbolic order in which humans can interpret what an algorithm is doing. This
                    therefore gestures to a theory of the human mind whereby subjective experience
                    is capable of undertaking interpretative work and thereby of creating meaning
                    out of an explanation of a given algorithm. But in cases where the infrasomatic
                    systems are progressively undermining this kind of cognitive skill, this reveals
                    a contradiction in the notion of explainability – humans might struggle to
                    understand explanations and might therefore require cognitive support from
                    visualisation systems created to support that capacity. </p>
                <p>There is an assumption that provided we know all the factors that influenced an
                    automated decision, whether directly or indirectly, we must be able to
                    comprehend the movement of states within which an automated system must move on
                    the occasion of a certain event, set of data, or calculation. However, this
                    assumption is rather ambitious in that it assumes a lot of background,
                    contextual or tacit knowledge and a particular level of cognitive capacity. Renz
                        <ptr target="#renz2018" loc="p. 4"/> describes this mode of apprehending and
                    understanding realistic rationalism, arguing that <quote rend="inline">a
                        realistic rationalism must be able to make plausible that everything that is
                        or that happens can in principle be grasped or comprehended — that every
                        being is, to use a traditional term, intelligible</quote>. This might imply
                    that an algorithm is different from its explanation, and that the algorithm
                    exists prior to its explanation. This also has methodological implications such
                    that an explanation must be able to secure the intelligibility of the automated
                    process within the concepts already understood by the human interpreter. </p>
                <p>These requirements raise difficult issues for designers of algorithmic decision
                    systems as they might be impossible to implement, even on systems that seem
                    relatively simple on the surface. As discussed above, a major justification is
                    the growing public concerns over biases, whether intentional or not, being built
                    into an algorithmic or machine-learning system. So, the new <term>right to
                        explanation</term> has been mobilised as an attempt to mitigate these
                    worries but also put in place legislative means to seek redress for them through
                    the GDPR. But this does not necessarily mean that the actual algorithm need be
                    provided, nor details of the processing steps outlined. Thus, this is
                    increasingly a representational challenge – how to represent an algorithmic
                    decision to a data subject. In effect, the processing might be presented as a
                    simplified model, or explanation, that shows the general contours of the
                    algorithm used in a particular case to an assumed reader, an increasingly
                    cognitively sophisticated user who can understand the explanation.<note><p>This
                            also raises questions about the potential for what we might call
                            explainability regress, whereby explanations are sought for the
                            explanation and so on ad infinitum. Until these cases are tested in
                            practice, it is difficult to know what the limitations will be in
                            relation to explanations provided by a system. </p></note>
                </p>

                <p>I call this the <term>Explainability Turn</term>. It is a genuinely interesting
                    question as to the extent to which explainability will be able to mitigate the
                    public anxieties manifested when confronted with opaque automated decision
                    systems. The scale of the challenge represented by the requirement to provide an
                    explanation seems to me to be under-appreciated, and clearing the grounds for
                    even thinking about this problem cannot be overstated. It nonetheless seems
                    clear that the notion of explainability has been derived from an epistemological
                    insight informed by debates over how scientific activity itself can be
                    explained. That computers and their algorithms are not so easily fitted into the
                    derivations of general laws that explanation seems to require, remains an
                    interesting aporia in this notion of explainability. </p>

                <p>Hence, I argue that thinking about explanation in relation to algorithms needs to
                    be informed by the humanities which can enrich these debates, for example by
                    deepening the meaning of explainability with what I call
                        <term>understandability</term>. That is, rather than providing descriptions
                    purely from the domains of a formal, technical and causal model of
                        <emph>explanation</emph> (dominant in the sciences), these technologies
                    would benefit from critical approaches that take account of
                        <emph>understanding</emph>, more common in the humanities and social
                    sciences (see <ptr target="#berry2011"/> for an earlier discussion of this, see
                    also <ptr target="#connolly2020"/>). The notion of explanation needs to be
                    interrogated by the humanities, and particularly the concept of explainability
                    it gives rise to. This is increasingly relevant to the growing public visibility
                    of humanities and the potential for the use of machine learning in related
                    fields, such as digital humanities. Therefore, this is an area that digital
                    studies and digital humanities could make an important contribution both in
                    thinking about their own work and the impact of algorithms, but also working in
                    conjunction with other fields. </p>

                <p>Unfortunately, due to limitations of time I do not have time to discuss further
                    here. But if it is the case that infrasomatizations create cognitive
                    infrastructures that proletarianise our cognitive faculties creating
                    anti-thought, overtaken thought and non-thought, then explainability creates a
                    potential way of bringing back into visibility these issues. For the user these
                    infrasomatizations are experienced through smart-phones and tablets which close
                    the loop from within the brain to the outside environment, such that the
                    aperture of thought is mediated and compressed. Hence, the capacity for the
                    human brain to perceive that algorithms are organizing their thoughts, or even
                    to perceive that algorithms are at work, is impaired, if not destroyed – human
                    reason is thereby diminished and made susceptible to persuasion and propaganda
                    as demonstrated by the <name>Cambridge Analytica</name> scandal that continues
                    to reverberate. These systems aim to directly influence the practice of
                    cognition as it has been historically constituted. New retentional and
                    protential systems are therefore directly implicated in a process of
                    transforming the way in which we create the conditions for cognition , directly
                    subverting, and in extreme cases replacing elements of cognitive processes in
                    human thought and experience. </p>
            </div>
            <div>
                <head>3. Conclusion</head>
                <p>Part of the responses we need to develop are through thinking about
                    infrastructures differently. We might, for example, seek to develop new logics
                    for what Bernard Stiegler has called a <term>contributory economy</term> as an
                    alternative form of political economy for digital society <ptr
                        target="#stiegler2018"/>. I agree that strategies such as these are crucial
                    to create a safe-harbour for critical reason and hence to enable the
                    contestation or transformation of infrastructures into new possibilities and
                    thereby create the conditions for a new epoch. In order to do this I have argued
                    previously that we need to undertake a programme of criticism with respect to
                    the computational and particularly its manifestation in digital capitalism <ptr
                        target="#berry2014"/>. But we need to go further and seek to understand and
                    challenge the way in which <q>smart</q> infrastructures recast certain
                    regulatory or legal limitations into ineffective measures from which they are
                    able to extract excessive amounts of profit and exhaust the wider economy
                    creating new forms of structural poverty and inequality. The combination of new
                        <q>smart</q> technologies and the social right to explanation that
                    explainability makes possible opens up a potential for a new critical space of
                    what we might call <term>tool criticism</term> and the development of a wider
                    literacy for a general public sharing increased anxiety about the effects of
                    these automated systems. This seems to me exactly the kind of expertise that
                    humanists and social scientists are highly skilled at and who could therefore
                    help inform the debate over explainability.<note><p>Additionally, digital
                            humanists tend to be familiar with technical systems and the questions
                            raised by understanding and interpretation more generally, for example
                            in the discussions about hermeneutics, understanding and practices of
                            close and distant reading.</p></note></p>
                <p>It is here, I argue, that theory and its development is crucial to understand the
                    contemporary computational situation through the confrontation of the object
                    with its own concept. We need to develop an approach that refuses to ignore and
                    smooth over contradictions and contradictory claims. Computational societies
                    continue to embody interaction based on deception and distortion (in other
                    words, as ideology), and which can often be translated unreflexively into
                    algorithmic forms. The cult of data-ism is a turn away from the project of
                    seeking to understand society and culture through the application of critical
                    reason in human affairs towards a data-deterministic world. It is problematic to
                    erect an abstract and metaphysical standard by which human action and society
                    can be judged – yet the cult of data-ism makes such a claim and works hard to
                    produce and reproduce this new data-centric milieu. Algorithms and data must be
                    subject to citizens' power to contest and challenge this new form of authority
                    and it is here that the concept of explainability offers a novel potential.
                    Indeed, as a critical concept it might contribute to concrete examples of
                    computationalism by drawing on critical theory and transforming explanation and
                    explainability into critical practices. This further enables us to challenge the
                    cult of data-ism and an administrative approach to thinking about algorithms and
                    instead to suggest different ways of being in a digital age. </p>
                <p>The digital world is not a static object; it is a highly dynamic and relational
                    system which is in constant movement and undergoing continual change. For
                    example, it is quite remarkable to note that the internet has never been taken
                    off-line in order to be upgraded or changed, rather it is built through
                    accretions and replacements that are slotted into or onto the existing system
                    structure whilst it is still <q>running</q>. This is an important aspect to
                    understanding the always-on nature of these new infrasomatic systems. It also
                    makes understanding the material specificity of algorithmic systems extremely
                    important, and helps to show why an analysis that focused only on the
                        <q>data</q> or <q>content</q> of an infrasomatic or infrastructural system
                    would be insufficient. We need to challenge the voracious appetite for data
                    which extends to all aspects of life and is often accompanied by a cult of
                    data-ism expressed through the cyber-libertarian notion that <q>information
                        wants to be free</q>.<note><p>It is worth reflecting on the naivety of some
                            proponents of open access who extol the virtues of free information
                            without connecting it to its genesis in cyberlibertarian modes of
                            thought (see <ptr target="#golumbia2016"/>.</p></note></p>
                <p>We might note that in advanced capitalist societies, economic anarchy is
                    interwoven with rationalization and technology to create fewer chances for
                    mental and reflective labour. Under such conditions, the values of instrumental
                    reason are accorded a privileged status since they are embodied in the concept
                    of rationality itself. The confounding of calculation with rational thinking
                    implies that whatever cannot be reduced to number is illusion or metaphysics. As
                    a result, the conditions are created for a greater susceptibility of society to
                    demagogic discourses and charismatic forms of power and a weakening of the
                    potential for individuation. This forms part of the wider significance of
                    infrasomatizations and how we need, more than ever, social critique and critical
                    thinking under contemporary conditions. Indeed, behind the ideological claims of
                    data science and related approaches, particularly in Silicon Valley, this
                    fetishism of calculation and computation is dominant. In spite of its efforts to
                    reflect the object of analysis in terms of the manifest forms of development,
                    such as here with algorithms, critical theory depends in its analysis on
                    particular historical conditions. </p>
                <p>It is crucial to maintain a dynamic distinction between social processes and
                    resultant social forms of commodity fetishism that make up the underlying
                    political economy of the new digital milieu. Institutional and ideological
                    formations are not simple reflections of an economic base; instead, work has to
                    be done to understand both culture and economy in relation to the growing use of
                    computation. In the context of computation it requires that we need to consider
                    the specific historical ideas and practices within which we experience
                    algorithms and in which they are made and remade. We must, therefore, examine
                    the particular historical conditions that give the present its shape in relation
                    to the specific material and ideological formations that algorithms introduce
                    into the social and economic conditions of society. Explainability, and the
                    explanations it might give rise to, seems to me to offer a particularly rich
                    potential for contributing to this project. This means that we need to critique
                    an ahistorical notion of the <q>algorithm</q> and critically interrogate
                    metaphors and analogies used in explainability that are necessary to explain but
                    are not sufficient for understanding the instantiation of algorithmic forms.</p>
                <p>One potential response then, is that on the ruins of critical reason a new sense
                    of the gradients of cognition must be understood – what exactly are the
                    faculties of the mind that are directly undermined or replaced by
                    infrasomatizations? The ruins must be uncovered to create new values, new
                    standards, new defences, to create situated identities and critical spaces for
                    defending against the onslaught of the algorithmic giants of the 21<hi
                        rend="superscript">st</hi> century. Weapons for the weak will be needed to
                    push back this colonisation of public and private reason. The only way for there
                    to be critical reason in a digital age, will be if it is rebuilt on these ruins.
                    The digital humanities can contribute a new research programme to interrogate
                    the political and technical digital monopolies that invade our lives. I suggest
                    that the first stages will be through the mobilization of a critical concept of
                    explainability, the second through the creation of new tools, and lastly through
                    the theorisation of a critique of computational reason.</p>





            </div>

        </body>


        <back>
            <listBibl>
                <bibl xml:id="anderson2008" label="Anderson 2008">Anderson, C. (2008) <title
                        rend="quotes">The End of Theory: The Data Deluge Makes the Scientific Method
                        Obsolete.</title>
                    <title rend="italic">Wired</title>. Accessed 18/12/2015 at: <ptr
                        target="http://www.wired.com/science/discoveries/magazine/16-07/pb_theory"
                    /></bibl>
                <bibl xml:id="balsillie2019" label="Balsillie 2019">Balsillie, J. (2019) <title
                        rend="quotes">Jim Balsillie : 'Data is not the new oil – it's the new
                        plutonium'</title>, <title rend="italic">Financial Post</title>, <ptr
                        target="https://business.financialpost.com/technology/jim-balsillie-data-is-not-the-new-oil-its-the-new-plutonium"
                    /></bibl>
                <bibl xml:id="berns-rouvray2013" label="Berns and Rouvray 2013">Berns, T. and
                    Rouvroy, A. (2013) <title rend="quotes">Gouvernementalité algorithmique et
                        perspectives d'émancipation : le disparate comme condition d'individuation
                        par la relation?</title>, accessed 14/12/2016, <ptr
                        target="https://works.bepress.com/antoinette_rouvroy/47/download/"/></bibl>
                <bibl xml:id="berry2011" label="Berry 2011">Berry, D. M. (2011) <title rend="italic"
                        >The Philosophy of Software</title>. London: Palgrave. </bibl>
                <bibl xml:id="berry2014" label="Berry 2014">Berry, D. M. (2014) <title rend="italic"
                        >Critical Theory and the Digital</title>. New York: Bloomsbury. </bibl>
                <bibl xml:id="berry2016" label="Berry 2016">Berry, D. M. (2016) <title rend="quotes"
                        >Infrasomatization.</title>
                    <title rend="italic">Stunlaw</title> at: <ptr
                        target="http://stunlaw.blogspot.co.uk/2016/12/infrasomatization.html."
                    /></bibl>
                <bibl xml:id="bobulescu2015" label="Bobulescu 2015">Bobulescu, R. (2015) <title
                        rend="quotes">From Lotka's biophysics to Georgescu-Roegen's
                        bioeconomics.</title>
                    <title rend="italic">Ecological Economics</title> 120, 194–202. </bibl>
                <bibl xml:id="buranyi2017" label="Buranyi 2017">Buranyi, S. (2017) <title
                        rend="quotes">Rise of the racist robots – how AI is learning all our worst
                        impulses</title>, <title rend="italic">The Guardian</title>, <ptr
                        target="https://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses"
                    /></bibl>
                <bibl xml:id="carr2008" label="Carr 2008">Carr, N. (2008) <title rend="quotes">Is
                        Google Making Us Stupid? What the Internet is doing to our brains</title>,
                        <title rend="italic">The Atlantic</title>, <ptr
                        target="https://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/306868/"
                    /></bibl>
                <bibl xml:id="casey2018" label="Casey et al. 2018">Casey, Bryan and Farhangi, Ashkon
                    and Vogl, Roland, (2018) <title rend="quotes">Rethinking Explainable Machines:
                        The GDPR's 'Right to Explanation' Debate and the Rise of Algorithmic Audits
                        in Enterprise</title> (February 19, 2018). <title rend="italic">Berkeley
                        Technology Law Journal</title>, Available at SSRN: <ptr
                        target="https://ssrn.com/abstract=3143325"/></bibl>
                <bibl xml:id="connolly2020" label="Connolly 2020">Connolly, R. (2020) <title
                        rend="quotes">Why Computing Belongs Within the Social Sciences</title>,
                        <title rend="italic">Communications of the ACM</title>, August 2020, Vol. 63
                    No. 8, Pages 54-59 </bibl>
                <bibl xml:id="darpa" label="Darpa n.d.">Darpa (n.d.) <title rend="italic"
                        >Explainable Artificial Intelligence (XAI)</title>, <ptr
                        target="https://www.darpa.mil/program/explainable-artificial-intelligence"
                    /></bibl>
                <bibl xml:id="daston2022" label="Daston 2022">Daston, L. (2022) <title rend="italic"
                        >Rules: A Short History of What We Live By</title>, Princeton University
                    Press </bibl>
                <bibl xml:id="davidson2016" label="Davidson 2016">Davidson, R. (2016) <title
                        rend="quotes">Open Data is the new oil that fuels society</title>, Office
                    for National Statistics, <ptr
                        target="https://blog.ons.digital/2016/01/25/open-data-new-oil-fuels-society/"
                    /></bibl>
                <bibl xml:id="ducasse2015" label="Ducasse 2015">Ducasse, C. J. (2015) <title
                        rend="quotes">Explanation, Mechanism and Teleology</title>, in <title
                        rend="italic">Truth, Knowledge and Causation</title>, Routledge. </bibl>
                <bibl xml:id="edwards2018" label="Edwards and Veale 2018">Edwards, L. and Veale, E.
                    (2018) <title rend="quotes">Enslaving the algorithm: from a 'right to an
                        explanation' to a 'right to better decisions'?</title>, January 2018. <title
                        rend="italic">Publication in IEEE Security and Privacy</title>, <ptr
                        target="https://pureportal.strath.ac.uk/files-asset/72824599/Edwards_Veale_SPM_2018_Enslaving_the_algorithm_from_a_right_to_an_explanation_to_a_right_to_better_decisions.pdf"
                    /></bibl>
                <bibl xml:id="europeanparliament2022" label="European Parliament 2022"><title
                        rend="italic">European Parliament (2022) EU Digital Markets Act and Digital
                        Services Act explained</title>, European Parliament, <ptr
                        target="https://www.europarl.europa.eu/news/en/headlines/society/20211209STO19124/eu-digital-markets-act-and-digital-services-act-explained"
                    /></bibl>
                <bibl xml:id="eu" label="EU n.d.">EU (n.d.) <title rend="quotes">Are there
                        restrictions on the use of automated decision-making?</title>, <ptr
                        target="https://ec.europa.eu/info/law/law-topic/data-protection/reform/rules-business-and-organisations/dealing-citizens/are-there-restrictions-use-automated-decision-making_en"
                    /></bibl>
                <bibl xml:id="eubanks2017" label="Eubanks 2017">Eubanks, V. (2017) <title
                        rend="italic">Automating Inequality: How High-Tech Tools Profile, Police,
                        and Punish the Poor</title>, St Martin's Press. </bibl>
                <bibl xml:id="evans2015" label="Evans 2015">Evans, L. (2015) <title rend="italic"
                        >Locative Social Media Place in the Digital Age</title>, Palgrave Macmillan. </bibl>
                <bibl xml:id="eyal2014" label="Eyal 2014">Eyal, N. (2014) <title rend="italic"
                        >Hooked: How to Build Habit-Forming Products</title>, Portfolio Penguin </bibl>
                <bibl xml:id="facebook2019" label="Facebook 2019">Facebook (2019) <title
                        rend="italic">Detectron</title>, <ptr
                        target="https://research.fb.com/downloads/detectron/"/></bibl>
                <bibl xml:id="foucault1995" label="Foucault 1995">Foucault, M. (1995) <title
                        rend="italic">Discipline and Punish: The Birth of the Prison</title>, New
                    YOrk: Vintage Books 1995. </bibl>
                <bibl xml:id="gdpr2016" label="GDPR 2016">GDPR (2016) <title rend="italic">General
                        Data Protection Regulation, Regulation (EU) 2016/679 of the European
                        Parliament and of the Council of 27 April 2016</title>, <ptr
                        target="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1528874672298anduri=CELEX%3A32016R0679"
                    /></bibl>
                <bibl xml:id="georgescu-roegen1970" label="Georgescu-Roegen 1970">Georgescu-Roegen,
                    N. (1970/2011) <title rend="quotes">The Entropy Law and the Economic
                        Problem</title>, in Bonaiuti, M. (Ed.), <title rend="italic">From
                        Bioeconomics to Degrowth: Georgescu-Roegen's 'New Economics' in Eight
                        Essays</title>, London: Routledge Studies in Ecological Economics, pp.
                    49–57. </bibl>
                <bibl xml:id="georgescu-roegen1972" label="Georgescu-Roegen 1972">Georgescu-Roegen,
                    N., (1972/2011). <title rend="quotes">Energy and Economic Myths</title>, in
                    Bonaiuti, M. (Ed.), From <title rend="italic">Bioeconomics to Degrowth:
                        Georgescu-Roegen's 'New Economics' in Eight Essays</title>, London:
                    Routledge Studies in Ecological Economics, pp. 58–92. </bibl>
                <bibl xml:id="georgescu-roegen1978" label="Georgescu-Roegen 1978">Georgescu-Roegen,
                    N., (1978/2011) <title rend="quotes">Inequality, Limits and Growth From a
                        Bioeconomic Viewpoint</title>, in Bonaiuti, M. (Ed.), <title rend="italic"
                        >From Bioeconomics to Degrowth: Georgescu-Roegen's 'New Economics' in Eight
                        Essays</title>, London: Routledge Studies in Ecological Economics, pp.
                    103–113 (2011). </bibl>
                <bibl xml:id="golumbia2016" label="Golumbia 2016">Golumbia, D. (2016). <title
                        rend="quotes">Marxism and Open Access in the Humanities: Turning Academic
                        Labor against Itself</title>, <title rend="italic">Workplace</title>, 28,
                    74-114. </bibl>
                <bibl xml:id="goodman2017" label="Goodman and Flaxman 2017">Goodman, B. and Flaxman,
                    S. (2017) <title rend="quotes">European Union Regulations on Algorithmic
                        Decision- Making and a <q>Right to Explanation</q></title>, <title
                        rend="italic">AI Magazine</title>, 38(3):50–57, 2017. </bibl>
                <bibl xml:id="guardian2018" label="Guardian 2018">Guardian (2018) <title
                        rend="quotes">The Cambridge Analytica Files: A year-long investigation into
                        Facebook, data, and influencing elections in the digital age</title>, <title
                        rend="italic">The Guardian</title>, <ptr
                        target="https://www.theguardian.com/news/series/cambridge-analytica-files"
                    /></bibl>
                <bibl xml:id="gunning2017" label="Gunning 2017">Gunning, D. (2017) <title
                        rend="italic">Explainable Artificial Intelligence (XAI): Programme
                        Update</title>, <ptr
                        target="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf"/></bibl>
                <bibl xml:id="hammond2016" label="Hammond 2016">Hammond, K. (2016) <title
                        rend="quotes">5 Unexpected Sources of Bias in Artificial
                        Intelligence</title>, <title rend="italic">Tech Crunch</title>, <ptr
                        target="https://techcrunch.com/2016/12/10/5-unexpected-sources-of-bias-in-artificial-intelligence/"
                    /></bibl>
                <bibl xml:id="harwell2018" label="Harwell 2018">Harwell, D. (2018) <title
                        rend="quotes">Wanted: The 'perfect babysitter.' Must pass AI scan for
                        respect and attitude</title>, <title rend="italic">The Washington
                        Post</title>, <ptr
                        target="https://nuzzel.com/sharedstory/11232018/washingtonpost/wanted_the_perfect_babysitter_must_pass_ai_scan_for_respect_and"
                    /></bibl>
                <bibl xml:id="hayles2007" label="Hayles 2007">Hayles, N. K. (2007) <title
                        rend="quotes">Hyper and Deep Attention: The Generational Divide in Cognitive
                        Modes</title>, <title rend="italic">Profession</title>, 13, 187-199. </bibl>
                <bibl xml:id="hayles2010" label="Hayles 2010">Hayles, N. K. (2010) <title
                        rend="quotes">How We Read: Close, Hyper, Machine</title>, <title
                        rend="italic">Ade Bulletin</title>, Number 150, 62-79. </bibl>
                <bibl xml:id="hempel-oppenheim1988" label="Hempel and Oppenheim 1988">Hempel and
                    Oppenheim (1988) <title rend="quotes">Studies in the Logic of
                        Explanation</title>, in Pitt, J.C. (ed.) <title rend="italic">Theories of
                        Explanation</title>, Oxford University Press. </bibl>
                <bibl xml:id="hutchins1995" label="Hutchins 1995">Hutchins, E. (1995) <title
                        rend="italic">Cognition in the wild</title>. MIT Press. </bibl>
                <bibl xml:id="jobs1981" label="Jobs 1981">Jobs, S. (1981) <title rend="quotes">When
                        We Invented the Personal Computer…,</title>
                    <title rend="italic">COMPUTERS and PEOPLE Magazine</title>, July-August 1981. </bibl>
                <bibl xml:id="kuang2017" label="Kuang 2017">Kuang, C. (2017) <title rend="quotes"
                        >Can A.I. Be Taught to Explain Itself?</title>, <title rend="italic">The New
                        York Times</title>, <ptr
                        target="https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html"
                    /></bibl>
                <bibl xml:id="kuneva2009" label="Kuneva 2009">Kuneva, M. (2009) <title rend="quotes"
                        >Keynote Speech</title>, <title rend="italic">Roundtable on Online Data
                        Collection, Targeting and Profiling</title>, <ptr
                        target="http://europa.eu/rapid/press-release_SPEECH-09-156_en.htm"/></bibl>
                <bibl xml:id="irani2015" label="Irani 2015">Irani, L. (2015) <title rend="quotes"
                        >The cultural work of microwork</title>, <title rend="italic">New Media and
                        Society</title> 17(5): 720-739. </bibl>
                <bibl xml:id="jaume-palasi2018" label="Jaume-Palasi 2018">Jaume-Palasi, L. (2018)
                        <title rend="quotes">Blessed by the algorithm: Computer says NO!</title>,
                        <ptr target="https://media.ccc.de/v/froscon2018-2307-keynote"/></bibl>
                <bibl xml:id="lipton2017" label="Lipton 2017">Lipton, Z. I. (2017) <title
                        rend="quotes">The Mythos of Model Interpretability</title>, <ptr
                        target="https://arxiv.org/pdf/1606.03490.pdf"/></bibl>
                <bibl xml:id="lotka1925" label="Lotka 1925">Lotka, A.J. (1925) <title rend="italic"
                        >Elements of Physical Biology</title>. William and Wilkins Company,
                    Baltimore. </bibl>
                <bibl xml:id="malabou2019" label="Malabou 2019">Malabou, C. (2019) <title
                        rend="italic">Morphing Intelligence: From IQ Measurement to Artificial
                        Brains</title>, Columbia University Press. </bibl>
                <bibl xml:id="masters2015" label="Masters and Thiel 2015">Masters,B. and Thiel,P.
                    (2015) <title rend="italic">Zero to One: Notes on Start Ups, or How to Build the
                        Future</title>, Virgin Books. </bibl>
                <bibl xml:id="mcnamee2019" label="McNamee 2019">McNamee, R. (2019) <title
                        rend="italic">Zucked!</title>, Penguin Press. </bibl>
                <bibl xml:id="meek2019" label="Meek 2019">Meek, A. (2019) <title rend="quotes"
                        >Amazon-owned Ring has reportedly been spying on customer camera
                        feeds</title>, <ptr
                        target="https://bgr.com/2019/01/10/ring-camera-customer-feeds-accessed-creepy-privacy-violation/"
                    /></bibl>
                <bibl xml:id="merleau-ponty2007" label="Merleau-Ponty 2007">Merleau-Ponty, M. (2007)
                        <title rend="quotes">Eye and Mind</title>, in Toadvine, T. and Lawlor, L.
                    (Eds.) <title rend="italic">The Merleau-Ponty Reader</title>, Northwestern
                    University Press. </bibl>
                <bibl xml:id="mill1858" label="Mill 1858">Mill, J. S. (1858) <title rend="quotes">Of
                        the Explanation of the Laws of Nature</title>, in <title rend="italic">A
                        System of Logic</title>, New York, Book III, Chapter XII, Section 1. </bibl>
                <bibl xml:id="noble2018" label="Noble 2018">Noble, S. U. (2018) <title rend="italic"
                        >Algorithms of Oppression</title>, New York University Press. </bibl>
                <bibl xml:id="palmer2006" label="Palmer 2006">Palmer, M. (2006) <title rend="quotes"
                        >Data is the New Oil</title>, <ptr
                        target="http://ana.blogs.com/maestros/2006/11/data_is_the_new.html"/></bibl>
                <bibl xml:id="pitt1988" label="Pitt 1988">Pitt, J.C. (1988) <title rend="italic"
                        >Theories of Explanation</title>, Oxford University Press. </bibl>
                <bibl xml:id="popper1972" label="Popper 1972">Popper, K. (1972) <title rend="italic"
                        >Objective Knowledge: An Evolutionary Approach</title>, Oxford: University
                    of Oxford Press. </bibl>
                <bibl xml:id="porter2019" label="Porter 2019">Porter, J. (2019) <title rend="quotes"
                        >Amazon patents 'surveillance as a service' tech for its delivery
                        drones</title>, <ptr
                        target="https://www.theverge.com/2019/6/21/18700451/amason-delivery-drone-surveillance-home-security-system-patent-application"
                    /></bibl>
                <bibl xml:id="uspto2019a" label="USPTO 2019a">USPTO (2019a) <title rend="quotes"
                        >Image creation using geo-fence data</title>, USPTO, <ptr
                        target="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2andSect2=HITOFFandp=1andu=%2Fnetahtml%2FPTO%2Fsearch-bool.htmlandr=1andf=Gandl=50andco1=ANDandd=PTXTands1=10313638.PN.andOS=PN/10313638andRS=PN/10313638"
                    /></bibl>
                <bibl xml:id="uspto2019b" label="USPTO 2019b">USPTO (2019b) <title rend="quotes"
                        >Generating Composite Facial Images Using Audio/Visual Recording and
                        Communication Devices</title>, USPTO, <ptr
                        target="https://www.aclunc.org/docs/Amazon_Patent.pdf"/></bibl>
                <bibl xml:id="rahwan2019" label="Rahwan et al. 2019">Rahwan, I. and Cebrian, M. and
                    Obradovich, N. and Bongard, J. and Bonnefon, JF. and Breazeal, C. and Crandall,
                    J. and A. Christakis, N. and Couzin, I. and Jackson, M. and R. Jennings, N. and
                    Kamar, E. and M. Kloumann, I. and Larochelle, H. and Lazer, D. and McElreath, R.
                    and Mislove, A. and C. Parkes, D. and Pentland, A. and Wellman, M.. (2019).
                        <title rend="quotes">Machine Behaviour</title>. <title rend="italic"
                        >Nature</title>. 568. 477-486. 10.1038/s41586-019-1138-y. </bibl>
                <bibl xml:id="rieder2020" label="Rieder and Hofman 2020">Rieder, B. and Hofmann, J.
                    (2020) <title rend="quotes">Towards platform observability</title>, <title
                        rend="italic">Internet Policy Review</title>, 9(4). <ptr
                        target="https://doi.org/10.14763/2020.4.1535"/></bibl>
                <bibl xml:id="renz2018" label="Renz 2018">Renz, U. (2018) <title rend="italic">The
                        Explainability of Experience: Realism and Subjectivity in Spinoza's Theory
                        of the Human Mind</title>, Oxford University Press. </bibl>
                <bibl xml:id="ruben2016" label="Ruben 2016">Ruben, D. H. (2016) <title rend="italic"
                        >Explaining Explanation</title>, Routledge. </bibl>
                <bibl xml:id="sample2017" label="Sample 2017">Sample, I. (2017) <title rend="quotes"
                        >Computer says no: why making AIs fair, accountable and transparent is
                        crucial</title>, <title rend="italic">The Guardian</title>, <ptr
                        target="https://www.theguardian.com/science/2017/nov/05/computer-says-no-why-making-ais-fair-accountable-and-transparent-is-crucial"
                    />
                </bibl>
                <bibl xml:id="samuel2019" label="Samuel 2019">Samuel, S. (2019) <title rend="quotes"
                        >10 things we should all demand from Big Tech right now</title>, <title
                        rend="italic">Vox</title>, <ptr
                        target="https://www.vox.com/the-highlight/2019/5/22/18273284/ai-algorithmic-bill-of-rights-accountability-transparency-consent-bias"
                    /></bibl>
                <bibl xml:id="schüll2014" label="Schüll 2014">Schüll, N. D. (2014) <title
                        rend="italic">Addiction by Design: Machine Gambling in Las Vegas</title>,
                    Princeton University Press. </bibl>
                <bibl xml:id="scriven1988" label="Scriven 1988">Scriven, M. (1988) <title
                        rend="quotes">Explanations, Predictions, and Laws</title>, in Pitt, J.C.
                    (Ed.) <title rend="italic">Theories of Explanation</title>, Oxford University
                    Press </bibl>
                <bibl xml:id="selbst2017" label="Selbst and Powles 2017">Selbst, A. D. and Powles,
                    J. (2017) <title rend="quotes">Meaningful Information and the Right to
                        Explanation</title>. <title rend="italic">International Data Privacy
                        Law</title>, 7(4):233–242. </bibl>
                <bibl xml:id="sengupta2012" label="Sengupta 2012">Sengupta, S. (2012) <title
                        rend="quotes">Should Personal Data Be Personal?</title>, <title
                        rend="italic">The New York Times</title>, <ptr
                        target="https://www.nytimes.com/2012/02/05/sunday-review/europe-moves-to-protect-online-privacy.html"
                    /></bibl>
                <bibl xml:id="stiegler2015" label="Stiegler 2015">Stiegler, B. (2015) <title
                        rend="quotes">Power, Powerlessness, Thinking, and Future</title>, <title
                        rend="italic">Los Angeles Review of Books</title>, <ptr
                        target="https://lareviewofbooks.org/article/power-powerlessness-thinking-and-future/#!"
                    /></bibl>
                <bibl xml:id="stiegler2016" label="Stiegler 2016">Steigler, B. (2016) <title
                        rend="quotes">The New Conflict of the Faculties and Functions:
                        Quasi-Causality and Serendipity in the Anthropocene.</title>
                    <title rend="italic">Qui Parle: Critical Humanities and Social Sciences</title>,
                    Volume 26, Number 1, June 2017, pp. 79-99 </bibl>
                <bibl xml:id="stiegler2018" label="Stiegler 2018">Stiegler, B. (2018), <title
                        rend="italic">The Neganthropocene</title>, Open Humanities Press, <ptr
                        target="http://openhumanitiespress.org/books/download/Stiegler_2018_The-Neganthropocene.pdf"
                    /></bibl>
                <bibl xml:id="toonders2014" label="Toonders 2014">Toonders, Y. (2014) <title
                        rend="quotes">Data is the New Oil of the Digital Economy</title>, <title
                        rend="italic">Wired</title>, <ptr
                        target="https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/"
                    /></bibl>
                <bibl xml:id="wachter2017" label="Wachter et al. 2017">Wachter, S., Mittelstadt, B.,
                    and Floridi L. (2017) <title rend="italic">Why a Right to Explanation of
                        Automated Decision-Making Does Not Exist in the General Data Protection
                        Regulation</title>. <title rend="italic">International Data Privacy
                        Law</title>, 7(2):76–99, 2017. </bibl>
                <bibl xml:id="wachter2018" label="Wachter et al. 2018">Wachter, S. and Mittelstadt,
                    B. and Russell, C. (2018). <title rend="quotes">Counterfactual Explanations
                        Without Opening the Black Box: Automated Decisions and the GDPR</title>.
                        <title rend="italic">Harvard journal of law and technology</title>, 31.
                    841-887. </bibl>
                <bibl xml:id="zuboff2019" label="Zuboff 2019">Zuboff, S. (2019) <title rend="italic"
                        >The Age of Surveillance Capitalism: The Fight for a Human Future at the New
                        Frontier of Power</title>, Profile Books. </bibl>
            </listBibl>
        </back>


    </text>
</TEI>
