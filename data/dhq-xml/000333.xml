<?xml version="1.0" encoding="UTF-8"?><?oxygen RNGSchema="../../common/schema/DHQauthor-TEI.rng" type="xml"?><?oxygen SCHSchema="../../common/schema/dhqTEI-ready.sch"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:cc="http://web.resource.org/cc/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dhq="http://www.digitalhumanities.org/ns/dhq">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <!-- Author should supply the title and personal information-->
                <title type="article">Old Content and Modern Tools – Searching Named Entities in a
                    Finnish OCRed Historical Newspaper Collection 1771–1910</title>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Kimmo <dhq:family>Kettunen</dhq:family></dhq:author_name>
                    <dhq:affiliation>National Library of Finland, Mikkeli, Finland</dhq:affiliation>
                    <email>kimmo.kettunen@helsinki.fi</email>
                    <dhq:bio>
                        <p>National Library of Finland · Centre for Digitization and
                            Preservation</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Eetu <dhq:family>Mäkelä</dhq:family></dhq:author_name>
                    <dhq:affiliation>University of Helsinki, Helsinki Centre for Digital
                        Humanities</dhq:affiliation>
                    <email>eetu.makela@aalto.fi</email>
                    <dhq:bio>
                        <p>Assistant Professor in Digital Humanities, University of Helsinki</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Teemu <dhq:family>Ruokolainen</dhq:family></dhq:author_name>
                    <dhq:affiliation>National Library of Finland, Mikkeli, Finland</dhq:affiliation>
                    <email>teemu.ruokolainen@helsinki.fi</email>
                    <dhq:bio>
                        <p>Language technology consultant for Digital Collections project</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Juha <dhq:family>Kuokkala</dhq:family></dhq:author_name>
                    <dhq:affiliation>University of Helsinki, Department of Modern Languages,
                        Helsinki, Finland</dhq:affiliation>
                    <email>juha.kuokkala@helsinki.fi</email>
                    <dhq:bio>
                        <p>University of Helsinki, Department of Finnish, Nordic and Finno-Ugric
                            Studies</p>
                    </dhq:bio>
                </dhq:authorInfo>
                <dhq:authorInfo>
                    <!-- Include a separate <dhq:authorInfo> element for each author -->
                    <dhq:author_name>Laura <dhq:family>Löfberg</dhq:family></dhq:author_name>
                    <dhq:affiliation>Department of Linguistics and English Language, Lancaster
                        University, UK</dhq:affiliation>
                    <email>l.lofberg@lancaster.ac.uk</email>
                    <dhq:bio>
                        <p>Department of Linguistics and English Language, Lancaster University,
                            UK</p>
                    </dhq:bio>
                </dhq:authorInfo>
            </titleStmt>
            <publicationStmt>
                <publisher>Alliance of Digital Humanities Organizations</publisher>
                <publisher>Association of Computers and the Humanities</publisher>

                <publisher>Association for Computers and the Humanities</publisher>
                <!-- This information will be completed at publication -->
                <idno type="DHQarticle-id"
                    >000333<!-- including leading zeroes: e.g. 000110 --></idno>
                <idno type="volume">011<!-- volume number, with leading zeroes: e.g. 006 --></idno>
                <idno type="issue">3<!-- issue number, without leading zeroes: e.g. 2 --></idno>
                <date when="2017-09-26">26 September 2017</date>
                <dhq:articleType>article</dhq:articleType>
                <availability>
                    <cc:License rdf:about="https://creativecommons.org/licenses/by-nd/2.5/"/>
                </availability>
            </publicationStmt>

            <sourceDesc>
                <p>This is the source</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <classDecl>
                <taxonomy xml:id="dhq_keywords">
                    <bibl>DHQ classification scheme; full list available at <ref
                            target="http://www.digitalhumanities.org/dhq/taxonomy.xml"
                            >http://www.digitalhumanities.org/dhq/taxonomy.xml</ref></bibl>
                </taxonomy>
                <taxonomy xml:id="authorial_keywords">
                    <bibl>Keywords supplied by author; no controlled vocabulary</bibl>
                </taxonomy>
            </classDecl>
        </encodingDesc>
        <profileDesc>
            <langUsage>
                <language ident="en"/>
            </langUsage>
            <textClass>
                <keywords scheme="#dhq_keywords">
                    <!-- Authors may suggest one or more keywords from the DHQ keyword list, visible at http://www.digitalhumanities.org/dhq/taxonomy.xml; these may be supplemented or modified by DHQ editors -->
                    <list type="simple">
                        <item/>
                    </list>
                </keywords>
                <keywords scheme="#authorial_keywords">
                    <!-- Authors may include one or more keywords of their choice -->
                    <list type="simple">
                        <item>named entity recognition</item>
                        <item>historical newspaper collections</item>
                        <item>Finnish</item>
                    </list>
                </keywords>
            </textClass>
        </profileDesc>
        <revisionDesc>
            <!-- Each change should include @who and @when as well as a brief note on what was done. -->
            <change who="DD" when="2017-09-12">Created file</change>
        </revisionDesc>
    </teiHeader>
    <!-- If a translation is added to the original article, add an enclosing <text> and <group> element -->
    <text xml:lang="en" type="original">
        <front>
            <dhq:abstract>
                <!-- Include a brief abstract of the article -->
                <p>Named Entity Recognition (NER), search, classification and tagging of names and
                    name-like informational elements in texts, has become a standard information
                    extraction procedure for textual data. NER has been applied to many types of
                    texts and different types of entities: newspapers, fiction, historical records,
                    persons, locations, chemical compounds, protein families, animals etc. In
                    general, the performance of a NER system is genre- and domain-dependent and also
                    used entity categories vary <ptr target="#nadeau2007"/>. The most general set of
                    named entities is usually some version of a tripartite categorization of
                    locations, persons, and organizations. In this paper we report trials and
                    evaluation of NER with data from a digitized Finnish historical newspaper
                    collection (Digi). Experiments, results, and discussion of this research serve
                    development of the web collection of historical Finnish newspapers. </p>
                <p><term>Digi</term> collection contains 1,960,921 pages of newspaper material from
                    1771–1910 in both Finnish and Swedish. We use only material of Finnish documents
                    in our evaluation. The OCRed newspaper collection has lots of OCR errors; its
                    estimated word level correctness is about 70–75 % <ptr target="#kettunen2016"/>.
                    Our principal NE tagger is a rule-based tagger of Finnish, <term>FiNER</term>,
                    provided by the <ref
                        target="https://kitwiki.csc.fi/twiki/bin/view/FinCLARIN/KielipankkiFrontpage"
                        >FIN-CLARIN</ref> consortium. We also show results of limited category
                    semantic tagging with tools of the <ref target="http://seco.cs.aalto.fi/"
                        >Semantic Computing Research Group (SeCo)</ref> of the Aalto University.
                    Three other tools are also evaluated briefly.</p>
                <p>This paper reports the first large scale results of NER in a historical Finnish
                    OCRed newspaper collection. Results of this research supplement NER results of
                    other languages with similar noisy data. As the results are also achieved with a
                    small and morphologically rich language, they illuminate the relatively
                    well-researched area of Named Entity Recognition from a new perspective. </p>
            </dhq:abstract>
            <dhq:teaser>
                <!-- Include a brief teaser, no more than a phrase or a single sentence -->
                <p>Named-entity recognition in Finnish newspapers from 1771-1910</p>
            </dhq:teaser>
        </front>
        <body>
            <div>
                <head>1. Introduction</head>
                <p>The National Library of Finland has digitized a large proportion of the
                    historical newspapers published in Finland between 1771 and 1910<note> Ten more
                        years, 1911–1920, were opened for free use on Feb. 1, 2017. Our trials have
                        been carried out with the material of 1771–1910.</note>
                    <ptr target="#bremerlaamanen2014"/>
                    <ptr target="#kettunen2014"/>. This collection contains 1,960,921 million pages
                    in Finnish and Swedish. The Finnish part of the collection consists of about 2.4
                    billion words. The National Library’s Digital Collections are offered via the
                        <ref target="https://digi.kansalliskirjasto.fi/"
                        >digi.kansalliskirjasto.fi</ref> web service, also known as
                        <term>Digi</term>. Part of the newspaper material (years 1771–1874) is
                    freely downloadable in The Language Bank of Finland provided by the FIN-CLARIN consortium.<note>
                        <ref
                            target="https://kitwiki.csc.fi/twiki/bin/view/FinCLARIN/KielipankkiAineistotDigilibPub"
                            >https://kitwiki.csc.fi/twiki/bin/view/FinCLARIN/KielipankkiAineistotDigilibPub</ref></note>
                    The collection can also be accessed through the Korp<note>
                        <ref target="https://korp.csc.fi/">https://korp.csc.fi/</ref></note>
                    environment, developed by Språkbanken at the University of Gothenburg and
                    extended by FIN-CLARIN team at the University of Helsinki to provide
                    concordances of text resources. A Cranfield<note> Cranfield style test
                        collection is a prototypical evaluation model for information retrieval
                        systems. It consists of a set of predefined topics/queries, a set of
                        documents to be searched for and relevance assessments for documents. First
                        such collections were used by Cyril W. Cleverdon at Cranfield University in
                        the early 1960s to evaluate efficiency of indexing systems.</note> style
                    information retrieval test collection has been produced out of a small part of
                    the Digi newspaper material at the University of Tampere <ptr
                        target="#jarvelin2016"/>. An open data package of the whole collection was
                    released in the first quarter of 2017 <ptr target="#paakkonen2016"/>.</p>
                <p>The web service <ref target="https://digi.kansalliskirjasto.fi/"
                        >digi.kansalliskirjasto.fi</ref> is used, for example, by genealogists,
                    heritage societies, researchers, and history-enthusiast laymen. There is also an
                    increasing desire to offer the material more widely for educational use. In 2016
                    the service had about 18 million page loads. User statistics of 2014 showed that
                    about 88.5% of the usage of the Digi came from Finland, but an 11.5% share of
                    use was coming outside of Finland. </p>
                <p>Digi is part of the growing global network of digitized newspapers and journals,
                    and historical newspapers are considered more and more as an important source of
                    historical knowledge. As the amount of digitized data accumulates, tools for
                    harvesting the data are needed to gather information and to add structure to the
                    unstructured mass <ptr target="#marrero2013"/>. Named Entity Recognition has
                    become one of the basic techniques for information extraction of texts since
                    mid-1990s <ptr target="#nadeau2007"/>. In its initial form, NER was used to find
                    and mark semantic entities like persons, locations, and organizations in texts
                    to enable information extraction related to these kinds of entities. Later on
                    other types of extractable entities, like time, artefact, event, and
                    measure/numerical, have been added to the repertoires of NER software <ptr
                        target="#nadeau2007"/><ptr target="#kokkinakis2014"/>.</p>
                <p>Our goal using NER is to provide users of Digi better means for searching and
                    browsing the historical newspapers (i.e. new ways to structure, access, and,
                    possibly, enhance information). Different types of names, especially person
                    names and names of locations, are frequently used as search terms in different
                    newspaper collections <ptr target="#crane2006"/>. They can also provide browsing
                    assistance to collections if the names are recognized, tagged in the newspaper
                    data, and put into the index <ptr target="#neudecker2014"/>. A fine example of
                    applying name recognition to historical newspapers is La Stampa’s historical
                    newspaper collection<note>
                        <ref target="http://www.archiviolastampa.it/"
                            >http://www.archiviolastampa.it/</ref></note>. After basic keyword
                    searches, users can browse or filter the search results by using three basic NER
                    categories of <emph>person</emph> (authors of articles or persons mentioned in
                    the articles), <emph>location</emph> (countries and cities mentioned in the
                    articles) and <emph>organization</emph>. Thus named entity annotations of
                    newspaper text allow a more semantically-oriented exploration of content of the
                    larger archive. Another large scale (152M articles) NER analysis of the
                    Australian historical newspaper collection (Trove) with usage examples is
                    described in Mac Kim and Cassidy <ptr target="#mackim2015"/>.</p>
                <p>Named Entity Recognition is a tool that needs to be used for some useful purpose.
                    In our case, extraction of person and place names is primarily a tool for
                    improving access to the Digi collection. After getting the recognition rate of
                    the NER tool to an acceptable level, we need to decide how we are going to use
                    extracted names in Digi. Some exemplary suggestions are provided by the archives
                    of <ref target="http://www.archiviolastampa.it/">La Stampa</ref> and <ref
                        target="http://trove.alveo.edu.au/">Trove Names</ref>
                    <ptr target="#mackim2015"/>. La Stampa's usage of names provides informational
                    filters after a basic search has been conducted. User can further look for
                    persons, locations, and organizations mentioned in the article results. This
                    kind of approach expands browsing access to the collection and possibly enables
                    entity linking <ptr target="#bates2007"/>
                    <ptr target="#toms2000"/>
                    <ptr target="#mcnamee2011"/>. Trove Names’ name search takes the opposite
                    approach: users first search for names and the search reveals articles where the
                    names occur. We believe that the La Stampa's use of names in the GUI of the
                    newspaper collection is more informative and useful for users, as the Trove
                    style can be achieved with the normal search function in the GUI of the
                    newspaper collection.</p>
                <p>If we consider possible uses of presently evaluated NER tools–FiNER, the FST, and
                    Connexor’s tagger–for our newspaper collection, they only perform basic
                    recognition and classification of names, which is the first stage of entity
                    handling <ptr target="#mcnamee2011"/>. To be of practical use names would need
                    both intra-document reference entity linking, as well as multiple document
                    reference entity linking <ptr target="#mcnamee2011"/>
                    <ptr target="#ehrmann2016b"/>. This means that all the occurrences of equal
                    names either in a single document or several documents, would be marked as
                    occurrences of the same name. ARPA’s semantic entity linking is of broader use,
                    and entity linking to external knowledge sources, such as Wikipedia, has been
                    used, for example, in the Europeana newspaper collection with names <ptr
                        target="#neudecker2014"/>
                    <ptr target="#hallo2016"/>.</p>
                <p>One more possible use for NER involves tagging and classifying images published
                    in the newspapers. Most of the images (photos) have short title texts. It seems
                    that many of the images represent locations and persons, with names of the
                    objects mentioned in the image title. As image recognition and classifying
                    low-quality print images may not be very feasible, image texts may offer a way
                    to classify at least a reasonable part of the images. Along with NER, topic
                    detection could also be done to the image titles. Image content tagging thus
                    could be one clear application for NER. </p>
                <p>Our main research question in this article is, how well or poorly names can be
                    recognized in an OCRed historical Finnish newspaper collection with readily
                    available software tools. The task has many pitfalls that will affect the
                    results. First, the word level quality of the material is quite low <ptr
                        target="#kettunen2016"/>, which is common for OCRed historical newspaper
                    collections. Second, the language technology tools that are available are made
                    for modern Finnish. Third, there is neither comparable NER data of historical
                    Finnish, nor a standard evaluation corpus. Thus our results form a first
                    baseline for NER of historical Finnish. One would be right to expect that
                    results will not be very good, but they will give us a realistic empirical
                    perspective on NER’s usability with our data <ptr target="#kettunen2017b"/>.</p>
                <p>We are using five readily available tagging tools for our task. By using a set of
                    different types of tools we are able to pinpoint common failures in NE tagging
                    our type of material. Observations of error analysis of the tools will help us
                    possibly to improve further NE tagging of historical OCRed material of Finnish.
                    Differences in tagging between these tools also help us to analyze further the
                    nature of our material.</p>
                <p>We will not provide a review of basic NER literature; those who are interested in
                    getting an overall picture of the topic, can start with Nadeau and Sekine <ptr
                        target="#nadeau2007"/> and Marrero et al. <ptr target="#marrero2013"/>, who
                    offer historical and methodological basics as well as critical discussion of the
                    theme. References in Nadeau and Sekine and Marrero et al. as well as references
                    in the current paper allow further familiarization. Specific problems related to
                    historical language and OCR problems are discussed in the paper in relation to
                    our data.</p>
                <p>The structure of the paper is following: first we introduce our NER tools, our
                    evaluation data and the tag set. Then we will show results of evaluations,
                    analyze errors of different tools, and finally discuss the results and our plans
                    for using NER with the online newspaper collection.</p>
            </div>
            <div>
                <head>2. NER Software and Evaluation</head>
                <p>For recognizing and labelling named entities in our evaluation we use FiNER
                    software as a baseline NER tool. Our second main tool, SeCo’s ARPA, is a
                    different type of tool, mainly used for Semantic Web tagging and linking
                    entities <ptr target="#makela2014"/><note> An older demo version of the tool is
                        available at <ref target="http://demo.seco.tkk.fi/sarpa/#/"
                            >http://demo.seco.tkk.fi/sarpa/#/</ref></note>, but it could be adapted
                    for basic NER, too. Besides these two tools, three others were also evaluated
                    briefly. Connexor<note>
                        <ref target="https://www.connexor.com/nlplib/?q=technology/name-recognition"
                            >https://www.connexor.com/nlplib/?q=technology/name-recognition</ref></note>
                    has NER for modern Finnish, which is commercial software. The multilingual
                    package Polyglot<note>
                        <ref
                            target="http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html"
                            >http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html</ref></note>
                    also works for Finnish and recognizes persons, places, and organizations. A
                    semantic tagger for Finnish <ptr target="#lofberg2005"/> also recognizes the
                    three types of names. </p>
                <p>All our taggers have been implemented as analysers of modern Finnish, although
                    ARPA’s morphological engine is able to deal with 19th century Finnish, too. As
                    far as we know there is no NE tagger for historical Finnish available. Before
                    choosing FiNER and ARPA we also tried a commonly used trainable free statistical
                    tagger, Stanford NER, but were not able to get reasonable performance out of it
                    for our purposes although the software has been used successfully for other
                    languages than English. Dutch, French and German named entity recognition with
                    the Stanford NER tool has been reported in the Europeana historical newspaper
                    project, and the results have been good <ptr target="#neudecker2014"/>
                    <ptr target="#neudecker2016"/>. However, we have been able to use Stanford NER
                    with our material after this evaluation was finished, but results of this are
                    not available yet.</p>
                <p>As far as we now, besides the five tools evaluated in this paper, there are not
                    many other existing tools to do NER analysis for Finnish<note> Lingpipe’s <ref
                            target="http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html">NER
                            tool</ref> could probably be taught for Finnish, too, but we did not try
                        it.</note>. FiNER has an updated version, but it does not seem to perform
                    any better than the version we are using in our evaluation. In our discussion
                    section we will get back to possibility of using a more historically aware
                    statistical NE tagger. It should also be noted that all the tools except ARPA
                    were used <emph>as is</emph>. That is, we did not enhance their performance with
                    any additional tools or modifications, and the tools were treated as black
                    boxes.</p>
                <div>
                    <head>2.1. FiNER</head>
                    <p>FiNER<note><ref target="http://www.helsinki.fi/~jkuokkal/finer_dist/"
                                >http://www.helsinki.fi/~jkuokkal/finer_dist/</ref></note> is a
                        rule-based named-entity tagger, which in addition to surface text forms
                        utilizes grammatical and lexical information from a morphological analyzer (Omorfi<note>
                            <ref target="https://github.com/flammie/omorfi"
                                >https://github.com/flammie/omorfi</ref></note>). FiNER
                        pre-processes the input text with a morphological tagger derived from
                        Omorfi. The tagger disambiguates Omorfi’s output by selecting the
                        statistically most probable morphological analysis for each word token, and
                        for tokens not recognized by the analyzer, guesses an analysis by analogy of
                        word-forms with similar ending in the morphological dictionary. The use of
                        morphological pre-processing is crucial in performing NER with a
                        morphologically rich language such as Finnish (and e.g. Estonian, <ptr
                            target="#tkachenko2013"/>), where a single lexeme may theoretically have
                        thousands of different inflectional forms.</p>
                    <p>The focus of FiNER is in recognizing different types of proper names.
                        Additionally, it can identify the majority of Finnish expressions of time
                        and, for example, sums of money. FiNER uses multiple strategies in its
                        recognition task: </p>
                    <list type="ordered">
                        <item>Pre-defined gazetteer information of known names of certain types.
                            This information is mainly stored in the morphological lexicon as
                            additional data tags of the lexemes in question. In the case of names
                            consisting of multiple words, FiNER rules incorporate a list of known
                            names not caught by the more general rules.</item>
                        <item>Several kinds of pattern rules are being used to recognize both
                            single- and multiple-word names based on their internal structure. This
                            typically involves (strings of) capitalized words ending with a
                            characteristic suffix such as Inc, Corp, Institute etc. Morphological
                            information is also utilized in avoiding erroneously long matches, since
                            in most cases only the last part of a multi-word name is inflected,
                            while the other words remain in the nominative (or genitive) case. Thus,
                            preceding capitalized words in other case forms should be left out of a
                            multi-word name match.</item>
                        <item>Context rules are based on lexical collocations, i.e. certain words
                            which typically or exclusively appear next to certain types of names in
                            text. For example, a string of capitalized words can be inferred to be a
                            corporation/organization if it is followed by a verb such as <foreign
                                xml:lang="fi">tuottaa</foreign> (<soCalled>produce</soCalled>),
                                <foreign xml:lang="fin">työllistää
                                </foreign>(<soCalled>employ</soCalled>) or <foreign xml:lang="fi"
                                >lanseerata</foreign> (<soCalled>launch</soCalled> [a product]), or
                            a personal name if it is followed by a comma- or parenthesis-separated
                            numerical age or an abbreviation for a political party member.</item>
                    </list>
                    <p>The pattern-matching engine that FiNER uses, HFST Pmatch, marks leftmost
                        longest non-overlapping matches satisfying the rule set (basically a large
                        set of disjuncted patterns) <ptr target="#linden2013"/>
                        <ptr target="#silfverberg2015"/>. In the case of two or more rules matching
                        the exact same passage in the text, the choice of the matching rule is
                        undefined. Therefore, more control is needed in some cases. Since HFST
                        Pmatch did not contain a rule weighing mechanism at the time of designing
                        the first release of FiNER, the problem was solved by applying two runs of
                        distinct Pmatch rulesets in succession. This solves, for instance, the
                        frequent case of Finnish place names used as family names: in the first
                        phase, words tagged lexically as place names but matching a personal name
                        context pattern are tagged as personal names, and the remaining place name
                        candidates are tagged as places in the second phase. FiNER annotates fifteen
                        different entities that belong to five semantic categories: location,
                        person, organization, measure, and time <ptr target="#silfverberg2015"
                        />.</p>
                </div>
                <div>
                    <head>2.2. ARPA</head>
                    <p>SeCo’s ARPA <ptr target="#makela2014"/> is not actually a NER tool, but
                        instead a dynamic, configurable entity linker. In effect, ARPA is not
                        interested in locating all entities of a particular type in a text, but
                        instead locating all entities that can be linked to strong identifiers
                        elsewhere. Through these it is then, for example, possible to source
                        coordinates for identified places or associate different name variants and
                        spellings to a single individual. For the pure entity recognition task
                        presented in this paper, ARPA is thus at a disadvantage. However, we wanted
                        to see how it would fare in comparison to FiNER. </p>
                    <p>The core benefits of the ARPA system lie in its dynamic, configurable nature.
                        In processing, ARPA combines a separate lexical processing step with a
                        configurable SPARQL-query-based lookup against an entity lexicon stored at a
                        Linked Data endpoint. Lexical processing for Finnish is done with a modified
                        version of Omorfi<note> Mäkelä, Eetu (2016). <title rend="quotes">LAS: an
                                integrated language analysis tool for multiple languages.</title>
                            <title rend="italic">The Journal of Open Source Software.</title>
                            <ref target="http://joss.theoj.org/papers/10.21105/joss.00035"
                                >http://joss.theoj.org/papers/10.21105/joss.00035</ref>; <ref
                                target="https://github.com/jiemakel/omorfi"
                                >https://github.com/jiemakel/omorfi</ref></note>, which supports
                        historical morphological variants, as well as lemma guessing for words
                        outside the standard vocabulary. This separation of concerns allows the
                        system to be speedily configured for both new reference vocabularies as well
                        as the particular dataset to be processed.</p>
                </div>
                <div>
                    <head>2.3. Evaluation Data</head>
                    <p>As there was no evaluation collection for Named Entity Recognition of 19th
                        century Finnish, we needed to create one first. As evaluation data we used
                        samples from different decades out of the Digi collection. Kettunen and
                        Pääkkönen calculated, among other things, number of words in the data for
                        different decades <ptr target="#kettunen2016"/>. It turned out that most of
                        the newspaper data was published in 1870–1910, and beginning and middle of
                        the 19th century had much less published material. About 95% of the material
                        was printed in 1870–1910, and most of it, 82.7%, in the two decades of
                        1890–1910.</p>
                    <p>We aimed for an evaluation collection of 150,000 words. To emphasize the
                        importance of the 1870–1910 material we took 50K of data from time period
                        1900–1910, 10K from 1890–1899, 10K from 1880–1889, and 10K from 1870–1879.
                        The remaining 70K of the material was picked from time period of 1820–1869.
                        Thus the collection reflects most of the data from the century but is also
                        weighed to the end of the 19th century and beginning of 20th century.
                        Decade-by-decade word recognition rates in Kettunen and Pääkkönen show that
                        word recognition rate during the whole 19th century is quite even, variation
                        being maximally 10% units for the whole century <ptr target="#kettunen2016"
                        />. In the latter part of the 19th century variation of recognition of words
                        is maximally 4% units. Thus we believe that temporal dimension of the data
                        should not bring great variation to the NER results. It may be possible,
                        however, that older data has old names that are out of the scope of our
                            tools.<note> Ehrmann et al. show irregular time-based variance for Swiss
                            French, but their data consists of almost 180 years between 1804 and
                            1981 <ptr target="#ehrmann2016a"/>.</note></p>
                    <p>The final manually tagged evaluation data consists of 75,931 lines, each line
                        having one word or other character data. By character data we mean here that
                        the line contains misrecognized words that have a variable amount of OCR
                        errors. The word accuracy of the evaluation sample is on the same level as
                        the whole newspaper collection’s word level quality: about 73% of the words
                        in the evaluation collection can be recognized by a modern Finnish
                        morphological analyzer. The recognition rate in the whole index of the
                        newspaper collection is estimated to be in the range of 70–75% <ptr
                            target="#kettunen2016"/>. Evaluation data was input to FiNER as small
                        textual snippets. 71% of the tagger’s input snippets have five or more
                        words, the rest have fewer than five words in the text snippet. Thus the
                        amount of context the tagger can use in recognition is varying.</p>
                    <p>FiNER uses fifteen tags for different types of entities, which is too fine a
                        distinction for our purposes. Our first aim was to concentrate only on
                        locations and person names because they are mostly used in searches of the
                        Digi collection–as was detected in an earlier log analysis where 80% of the
                        ca. 149,000 occurrences of top 1000 search term types consisted of first and
                        last names of persons and place names <ptr target="#kettunen2014"/>. This
                        kind of search term use is very common especially in the humanities
                        information seeking <ptr target="#crane2006"/>.</p>
                    <p>After reviewing some of the FiNER tagged material, we included also three
                        other tags, as they seemed important and were occurring frequently enough in
                        the material. The eight final chosen tags are shown and explained below
                        (Entity followed by Tag Meaning).</p>
                    <list type="ordered">
                        <item><code>&lt;EnamexPrsHum&gt;</code> person</item>
                        <item><code>&lt;EnamexLocXxx&gt;</code> general location</item>
                        <item><code>&lt;EnamexLocGpl&gt;</code> geographical location</item>
                        <item><code>&lt;EnamexLocPpl&gt;</code> political location (state, city
                            etc.)</item>
                        <item><code>&lt;EnamexLocStr&gt;</code> street, road, street address</item>
                        <item><code>&lt;EnamexOrgEdu&gt;</code> educational organization</item>
                        <item><code>&lt;EnamexOrgCrp&gt;</code> company, society, union etc.</item>
                        <item><code>&lt;TimexTmeDat&gt;</code> expression of time</item>
                    </list>
                    <p>The final entities show that our interest is mainly in the three most
                        commonly used semantic NE categories: persons, locations, and organizations.
                        In locations we have four different categories and with organizations two.
                        Temporal expressions were included in the tag set due to their general
                        interest in the newspaper material. Especially persons and locations fulfill
                        content validity condition of an experimental unit <ptr
                            target="#marrero2013"/>
                        <ptr target="#urbano2011"/> with our material, as locations and persons are
                        most sought for by users in the newspapers according to our log studies. </p>
                    <p>Manual tagging of the evaluation corpus was done by the third author, who had
                        previous experience in tagging modern Finnish with tags of the FiNER tagger.
                        Tagging took one month, and quality of the tagging and its principles were
                        discussed before starting based on a sample of 2000 lines of evaluation
                        data. It was agreed, for example, that words that are misspelled but are
                        recognizable for the human tagger as named entities would be tagged (cf. 50%
                        character correctness rule in <ptr target="#packer2010"/>). If orthography
                        of the word was following 19th century spelling rules but the word was
                        identifiable as a named entity, it would be tagged too. Thus no spelling
                        corrections or normalizations were made in the evaluation data.</p>
                    <p>All the evaluation runs were performed with the tagged 75K evaluation set.
                        This set was not used in configuration of the tools. </p>
                    <div>
                        <head>2.3.1 Testing Lexical Coverage of FiNER</head>
                        <p>To get an idea how well FiNER recognizes names in general, we evaluated
                            it with a separate list of 75,980 names of locations and persons. We
                            included in the list modern first names and surnames, old first names
                            from the 19th century, names of municipalities, and names of villages
                            and houses. The list also contains names in Swedish, as Swedish was the
                            dominant language in Finland during most of the 19th century<note>
                                Finnish became the dominant language in newspapers from the
                                beginning of 1890s <ptr target="#kettunenetal2016"/>.</note>. The
                            list has been compiled from independent open sources that include, for
                            example, the Institute for the Languages of Finland, the National Land
                            Survey of Finland, and the Genealogical Society of Finland, among
                            others. All the names were given to FiNER as part of a predicative
                            pseudo sentence <foreign xml:lang="fi">X on mukava juttu</foreign>
                                (<soCalled>X is a nice thing</soCalled>) so that the tagger had some
                            context to work with, not just a list of names.</p>
                        <p>FiNER recognized 55,430 names out of the list, which is 72.96%. Out of
                            these 8,904 were tagged as persons, 35,733 as <term>LocXxxs</term>, and
                            10,408 as <term>LocGpls</term>. The rest were tagged as organizations,
                            streets, time, and titles. Among locations, FiNER favors general
                            locations (<term>LocXxxs</term>). As <term>LocGpls</term> it tags
                            locations that have some clear mention of a natural geographical entity
                            as part of the name (lake, pond, river, hill, rapids, etc.), but this is
                            not clear cut, as some names of this type seem to get tag of
                                <term>LocXxx</term>. It would be reasonable to use only one location
                            tag with FiNER, as the differences between location categories are not
                            very significant.</p>
                        <p>Among the names that FiNER does not recognize are foreign names, mostly
                            Swedish (also in Sami), names that can also be common nouns, different
                            compound names, and old names. Variation of <emph>w/v</emph>, one the
                            most salient differences of 19th century Finnish and modern Finnish,
                            does not impair FiNER’s tagging, although it has a clear impact on
                            general recognizability of 19th century Finnish <ptr
                                target="#kettunen2016"/>. Some other differing morphological
                            features of 19th century Finnish <ptr target="#jarvelin2016"/> (cf.
                            Table 1) may affect recognition of names with FiNER. Most of the
                            differences can be considered as spelling variations, and some of them
                            could affect results of NER too. Also writing of compounds was unstable
                            in 19th century Finnish. Compounds could be spelled either together,
                            with hyphen, or with whitespace, for example <foreign xml:lang="fi"
                                >diakonissalaitos</foreign>, <foreign xml:lang="fi"
                                >diakonissa-laitos</foreign> or <foreign xml:lang="fi">diakonissa
                                laitos.</foreign> This may also affect spelling of names in the
                            texts.</p>
                    </div>
                </div>
                <div>
                    <head>2.4. Results of the Evaluation</head>
                    <p>We evaluated performance of the NER tools using the <term>conlleval</term><note>
                            <ref target="http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt"
                                >http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt</ref>,
                            author ErikTjong Kim Sang, version 2004-01-26</note> script used in
                        Conference on Computational Natural Language Learning (CONLL).
                            <term>Conlleval</term> uses standard measures of precision, recall and
                        F-score–one defined as 2PR/(R+P)–where P is precision and R recall (cf. <ptr
                            target="#manning1999" loc="269"/>). Evaluation is based on
                            <soCalled>exact-match evaluation</soCalled>
                        <ptr target="#nadeau2007"/>. In this type of evaluation NER system is
                        evaluated based on the micro-averaged F-measure (MAF) where
                            <term>precision</term> is the percentage of correct named entities found
                        by the NER software; <term>recall</term> is the percentage of correct named
                        entities present in the tagged evaluation corpus that are found by the NER
                        system. A named entity is considered correct only if it is an exact match of
                        the corresponding entity in the tagged evaluation corpus: <cit>
                            <quote rend="inline" source="#poibeau2001">a result is considered
                                correct only if the boundaries and classification are exactly as
                                annotated</quote>
                            <ptr target="#poibeau2001"/>
                        </cit>. Thus the evaluation criteria are strict, especially for multipart
                        entities. Strict evaluation was possible only for FiNER and ARPA, which
                        marked the boundaries of the entities.</p>
                    <p>We performed also a looser evaluation for all the taggers. In a looser
                        evaluation the categories were treated so that any correct marking of an
                        entity regardless its boundaries was considered a hit.</p>
                </div>
                <div>
                    <head>2.5. Results of FiNER</head>
                    <p>Detailed results of the evaluation of FiNER are shown in Table 1. Entities
                            <code>&lt;ent/&gt;</code> consist of one word token,
                            <code>&lt;ent&gt;</code> are part of a multiword entity and
                            <code>&lt;/ent&gt;</code> are last parts of multiword entities. An
                        example of a multipart name would be <term>G. E. Jansson</term>, with two
                        initials. A proper tagging for this would be <term>G.
                                <code>&lt;EnamexPrsHum&gt;</code> E.
                                <code>&lt;EnamexPrsHum&gt;</code>
                                Jansson<code>&lt;/EnamexPrsHum&gt;</code></term>. If only the
                        surname <term>Jansson</term> would appear in the text, a proper tagging for
                        this would be <term><code>&lt;EnamexPrsHum/&gt;</code></term>.</p>
                    <table>
                        <head>Evaluation results of FiNER with strict CONLL evaluation criteria.
                            Data with zero P/R is not included in the table. These include
                                categories<code> &lt;EnamexLocGpl&gt;</code>,
                                <code>&lt;/EnamexLocGpl&gt;</code>, <code>
                                &lt;EnamexLocPpl&gt;</code>, <code>&lt;/EnamexLocPpl&gt;</code>,
                                <code>&lt;EnamexLocXxx&gt;</code>,
                                <code>&lt;EnamexLocXxx/&gt;</code>,
                                <code>&lt;/EnamexLocXxx&gt;</code>, and
                                <code>&lt;EnamexOrgEdu/&gt;</code>. Most of these have very few
                            entities in the data, only <code>&lt;EnamexLocXxx&gt;</code> is frequent
                            with over 1200 occurences</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags found</cell>
                            <cell role="label">Number of tags in the evaluation data</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocGpl/&gt;</code></cell>
                            <cell> 6.96</cell>
                            <cell> 9.41</cell>
                            <cell> 8.00</cell>
                            <cell> 115</cell>
                            <cell> 85</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl/&gt;</code></cell>
                            <cell> 89.50</cell>
                            <cell> 8.46</cell>
                            <cell> 15.46</cell>
                            <cell> 181</cell>
                            <cell> 1920</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocStr/&gt;</code></cell>
                            <cell> 23.33</cell>
                            <cell> 50.00</cell>
                            <cell> 31.82</cell>
                            <cell> 30</cell>
                            <cell> 14</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocStr&gt;</code></cell>
                            <cell> 100.00</cell>
                            <cell> 13.83</cell>
                            <cell> 24.30</cell>
                            <cell> 13</cell>
                            <cell> 94</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexLocStr&gt;</code></cell>
                            <cell> 100.00</cell>
                            <cell> 18.31</cell>
                            <cell> 30.95</cell>
                            <cell> 13</cell>
                            <cell> 71</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexOrgCrp/&gt;</code></cell>
                            <cell> 2.39</cell>
                            <cell> 6.62</cell>
                            <cell> 3.52</cell>
                            <cell> 376</cell>
                            <cell> 155</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexOrgCrp&gt;</code></cell>
                            <cell> 44.74</cell>
                            <cell> 25.99</cell>
                            <cell> 32.88</cell>
                            <cell> 190</cell>
                            <cell> 338</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexOrgCrp&gt;</code></cell>
                            <cell> 40.74</cell>
                            <cell> 31.95</cell>
                            <cell> 35.81</cell>
                            <cell> 189</cell>
                            <cell> 250</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexOrgEdu&gt;</code></cell>
                            <cell> 48.28</cell>
                            <cell> 40.00</cell>
                            <cell> 43.75</cell>
                            <cell> 29</cell>
                            <cell> 35</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexOrgEdu&gt;</code></cell>
                            <cell> 55.17</cell>
                            <cell> 64.00</cell>
                            <cell> 59.26</cell>
                            <cell> 29</cell>
                            <cell> 25</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum/&gt;</code></cell>
                            <cell> 16.38</cell>
                            <cell> 52.93</cell>
                            <cell> 25.02</cell>
                            <cell> 1819</cell>
                            <cell> 564</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell> 87.44</cell>
                            <cell> 26.67</cell>
                            <cell> 40.88</cell>
                            <cell> 438</cell>
                            <cell> 1436</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexPrsHum&gt;</code></cell>
                            <cell> 82.88</cell>
                            <cell> 31.62</cell>
                            <cell> 45.78</cell>
                            <cell> 438</cell>
                            <cell> 1150</cell>
                        </row>
                        <row>
                            <cell><code>&lt;TimexTmeDat/&gt;</code></cell>
                            <cell> 5.45</cell>
                            <cell> 14.75</cell>
                            <cell> 7.96</cell>
                            <cell> 495</cell>
                            <cell> 183</cell>
                        </row>
                        <row>
                            <cell><code>&lt;TimexTmeDat&gt;</code></cell>
                            <cell> 68.54</cell>
                            <cell> 2.14</cell>
                            <cell> 4.14</cell>
                            <cell> 89</cell>
                            <cell> 2857</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/TimexTmeDat&gt;</code></cell>
                            <cell> 20.22</cell>
                            <cell> 2.00</cell>
                            <cell> 3.65</cell>
                            <cell> 89</cell>
                            <cell> 898</cell>
                        </row>
                    </table>
                    <p>Results of the evaluation show that named entities are recognized quite badly
                        by FiNER, which is not surprising as the quality of the text data is quite
                        low. Recognition of multipart entities is mostly very low. Some part of the
                        entities may be recognized, but rest is not. Out of multiword entities
                        person names and educational organizations are recognized best. Names of
                        persons are the most frequent category. Recall of one part person names is
                        best, but its precision is low. Multipart person names have a more balanced
                        recall and precision, and their F-score is 40–45. If the three different
                        locations (<term>LocGpl</term>, <term>LocPpl</term> and <term>LocXxx</term>)
                        are joined in strict evaluation as one general location,
                        <term>LocXxx</term>, one part locations get precision of 65.69, recall of
                        50.27, and F-score of 56.96 with 1533 tags. Multipart locations are found
                        poorly even then. FiNER seems to have a tendency to tag most of the
                            <term>LocPpl</term>s as <term>LocXxx</term>s. <term>LocGpl</term>s are
                        also favored instead of <term>LocPpl</term>s. On the other hand, only one
                        general location like LocXxx could be enough for our purposes, and these
                        results are reasonably good</p>
                    <p>In a looser evaluation the categories were treated so that any correct
                        marking of an entity regardless its boundaries was considered a hit. Four
                        different location categories were joined to two: general location
                                <term><code>&lt;EnamexLocXxx&gt;</code></term> and that of street
                        names. The end result was six different categories instead of eight. Table 2
                        shows evaluation results with loose evaluation. Recall and precision of the
                        most frequent categories of person and location was now clearly higher, but
                        still not very good.</p>
                    <table>
                        <head>Evaluation results of FiNER with loose criteria and six
                            categories</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags found</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell>63.30</cell>
                            <cell>53.69</cell>
                            <cell>58.10</cell>
                            <cell>2681</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                            <cell>69.05</cell>
                            <cell>49.21</cell>
                            <cell>57.47</cell>
                            <cell>1541</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocStr&gt;</code></cell>
                            <cell>83.64</cell>
                            <cell>25.56</cell>
                            <cell>39.15</cell>
                            <cell>55</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexOrgEdu&gt;</code></cell>
                            <cell>51.72</cell>
                            <cell>47.62</cell>
                            <cell>49.59</cell>
                            <cell>58</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexOrgCrp&gt;</code></cell>
                            <cell>30.27</cell>
                            <cell>32.02</cell>
                            <cell>31.12</cell>
                            <cell>750</cell>
                        </row>
                        <row>
                            <cell><code>&lt;TimexTmeDat&gt;</code></cell>
                            <cell>73.85</cell>
                            <cell>12.62</cell>
                            <cell>21.56</cell>
                            <cell>673</cell>
                        </row>
                    </table>
                </div>
                <div>
                    <head>2.6. Results of ARPA</head>
                    <p>Our third evaluation was performed for a limited tag set with tools of the
                        SeCo’s ARPA. We first analyzed ARPA’s lexical coverage with the same word
                        list that was used with FiNER. ARPA recognized in the recognition word list
                        (of 75,980 tokens) 74,068 as either locations or persons (97.4 %). 67,046
                        were recognized as locations, and 37,456 as persons. 30,434 names were
                        tagged as both persons and locations. Among the 1912 names that were not
                        recognized by ARPA were the same kind of foreign names that were left
                        unrecognized by FiNER. 13% of the unrecognized names were compounds with
                        hyphen, such as Esa-Juha, Esa-Juhani. This type could be easily handled by
                        ARPA with minor modifications to configuration. In general, the test showed
                        that ARPA’s lexicons are more comprehensive than those of FiNER.</p>
                    <p>First only places were identified so that one location,
                            <term>EnamexLocPpl</term>, was recognized. For this task, ARPA was first
                        configured for the task of identifying place names in the data. As a first
                        iteration, only the <ref target="http://www.ldf.fi/dataset/pnr/">Finnish
                            Place Name Registry</ref><note>
                            <ref target="http://www.ldf.fi/dataset/pnr/"
                                >http://www.ldf.fi/dataset/pnr/</ref></note> was used. After
                        examining raw results from the test run, three issues were identified for
                        further improvement. First, PNR contains only modern Finnish place names. To
                        improve recall, three registries containing historical place names were
                        added: 1) the Finnish spatiotemporal ontology SAPO <ptr
                            target="#hyvonen2011"/> containing names of historic municipalities, 2)
                        a repository of old Finnish maps and associated places from the 19th and
                        early 20th Century, and 3) a name registry of places inside historic
                        Karelia, which does not appear in PNR due to being ceded by Finland to the
                        Soviet Union at the end of the Second World War <ptr target="#ikkala2016"/>.
                        To account for international place names, the names were also queried
                        against the Geonames database<note>
                            <ref target="http://geonames.org/">http://geonames.org/</ref></note> as
                        well as Wikidata<note>
                            <ref target="http://wikidata.org/">http://wikidata.org/</ref></note>.
                        The contributions of each of these resources to the number of places
                        identified in the final runs are shown in Table 3. Note that a single place
                        name can be, and often was found in multiple of these sources.</p>
                    <table>
                        <head>Number of distinct place names identified using each source</head>
                        <row>
                            <cell role="label">Source</cell>
                            <cell role="label">Matches</cell>
                            <cell role="label">Fuzzy matches</cell>
                        </row>
                        <row>
                            <cell>Karelian places</cell>
                            <cell>461</cell>
                            <cell>951</cell>
                        </row>
                        <row>
                            <cell>Old maps</cell>
                            <cell>685</cell>
                            <cell>789</cell>
                        </row>
                        <row>
                            <cell>Geonames</cell>
                            <cell>1036</cell>
                            <cell>1265</cell>
                        </row>
                        <row>
                            <cell>SAPO</cell>
                            <cell>1467</cell>
                            <cell>1610</cell>
                        </row>
                        <row>
                            <cell>Wikidata</cell>
                            <cell>1877</cell>
                            <cell>2186</cell>
                        </row>
                        <row>
                            <cell>PNR</cell>
                            <cell>2232</cell>
                            <cell>2978</cell>
                        </row>
                    </table>
                    <p>Table 4 describes the results of location recognition with ARPA. With one
                        exception (<emph>New York</emph>), only one word entities were discovered by
                        the software.</p>
                    <table>
                        <head>Basic evaluation results for ARPA</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl/&gt;</code>
                            </cell>
                            <cell>39.02</cell>
                            <cell>53.24</cell>
                            <cell>45.03</cell>
                            <cell>2673</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexLocPpl&gt;</code></cell>
                            <cell>100.00</cell>
                            <cell>5.26</cell>
                            <cell>10.00</cell>
                            <cell>1</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl&gt;</code></cell>
                            <cell>100.00</cell>
                            <cell>4.76</cell>
                            <cell>9.09</cell>
                            <cell>1</cell>
                        </row>
                    </table>
                    <p>A second improvement to the ARPA process arose from the observation that
                        while recall in the first test run was high, precision was low. Analysis
                        revealed this to be due to many names being both person names as well as
                        places. Thus, a filtering step was added, that removed 1) hits identified as
                        person names by the morphological analyzer and 2) hits that matched regular
                        expressions catching common person name patterns found in the data (I.
                        Lastname and FirstName LastName). However, sometimes this was too
                        aggressive, ending up, for example, in filtering out also big cities like
                            <emph>Tampere</emph> and <emph>Helsinki</emph>. Thus, in the final
                        configuration, this filtering was made conditional on the size of the
                        identified place, as stated in the structured data sources matched
                        against.</p>
                    <p>Finally, as the amount of OCR errors in the target dataset was identified to
                        be a major hurdle in accurate recognition, experiments were made with
                        sacrificing precision in favor of recall through enabling various levels of
                        Levenshtein distance matching against the place name registries. In this
                        test, the fuzzy matching was done in the query phase after lexical
                        processing. This was easy to do, but doing the fuzzy matching during lexical
                        processing would probably be more optimal as lemma guessing (which is needed
                        because OCR errors are out of the lemmatizer’s vocabulary) is currently
                        extremely sensitive to OCR errors–particularly in the suffix parts of
                        words.</p>
                    <p>After the place recognition pipeline was finalized, a further test was done
                        to see if the ARPA pipeline could be used for also person name recognition.
                        The <ref target="http://viaf.org/">Virtual International Authority
                            File</ref> was used as a lexicon of names as it contains 33 million
                        names for 20 million people. In the first run, the query simply matched all
                        uppercase words against both first and last names in this database while
                        allowing for any number of initials to also precede such names matched. This
                        way the found names cannot always be linked to strong identifiers, but for a
                        pure NER task, recall is improved.</p>
                    <p>Table 5 shows results of this evaluation without fuzzy matching of names and
                        Table 6 with fuzzy matching. Table 7 shows evaluation results with loose
                        criteria without fuzzy matching and Table 8 loose evaluation with fuzzy
                        matching.</p>
                    <table>
                        <head>Evaluation results for ARPA: no fuzzy matching</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">­F-score</cell>
                            <cell role="label">Number of tags</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl/&gt;</code>
                            </cell>
                            <cell>58.90</cell>
                            <cell>55.59</cell>
                            <cell>57.20</cell>
                            <cell>1849</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexLocPpl&gt;</code></cell>
                            <cell>1.49</cell>
                            <cell>10.53</cell>
                            <cell>2.61</cell>
                            <cell>134</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl&gt;</code></cell>
                            <cell>1.63</cell>
                            <cell>14.29</cell>
                            <cell>2.93</cell>
                            <cell>184</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum/&gt;</code>
                            </cell>
                            <cell>30.42</cell>
                            <cell>27.03</cell>
                            <cell>28.63</cell>
                            <cell>2242</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexPersHum&gt;</code></cell>
                            <cell>83.08</cell>
                            <cell>47.39</cell>
                            <cell>60.35</cell>
                            <cell>656</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamePersHum&gt;</code></cell>
                            <cell>85.23</cell>
                            <cell>43.80</cell>
                            <cell>57.87</cell>
                            <cell>738</cell>
                        </row>
                    </table>
                    <table>
                        <head>Evaluation results for ARPA: fuzzy matching</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl/&gt;</code>
                            </cell>
                            <cell>47.38</cell>
                            <cell>61.82</cell>
                            <cell>53.64</cell>
                            <cell>2556</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexLocPpl&gt;</code></cell>
                            <cell>1.63</cell>
                            <cell>15.79</cell>
                            <cell>2.96</cell>
                            <cell>184</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocPpl&gt;</code></cell>
                            <cell>1.55</cell>
                            <cell>14.29</cell>
                            <cell>2.80</cell>
                            <cell>193</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum/&gt;</code>
                            </cell>
                            <cell>9.86</cell>
                            <cell>66.79</cell>
                            <cell>17.18</cell>
                            <cell>3815</cell>
                        </row>
                        <row>
                            <cell><code>&lt;/EnamexPrsHum&gt;</code></cell>
                            <cell>63.07</cell>
                            <cell>62.97</cell>
                            <cell>63.01</cell>
                            <cell>1148</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamePrsHum&gt;</code></cell>
                            <cell>62.25</cell>
                            <cell>61.77</cell>
                            <cell>62.01</cell>
                            <cell>1425</cell>
                        </row>
                    </table>
                    <table>
                        <head>Evaluation results for ARPA with loose criteria: no fuzzy
                            matching</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell>63.61</cell>
                            <cell>45.27</cell>
                            <cell>52.90</cell>
                            <cell>3636</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                            <cell>44.02</cell>
                            <cell>64.58</cell>
                            <cell>52.35</cell>
                            <cell>2933</cell>
                        </row>
                    </table>
                    <table>
                        <head>Evaluation results for ARPA with loose criteria: fuzzy matching</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell>34.39</cell>
                            <cell>78.09</cell>
                            <cell>51.57</cell>
                            <cell>6388</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                            <cell>44.02</cell>
                            <cell>64.58</cell>
                            <cell>52.35</cell>
                            <cell>2933</cell>
                        </row>
                    </table>
                    <p>Recall of recognition increases markedly in fuzzy matching, but precision
                        deteriorates. More multipart location names are also recognized with fuzzy
                        matching. In loose evaluation more tags are found but precision is not very
                        good and thus the overall F-score is a bit lower than in the strict
                        evaluation.</p>
                </div>
                <div>
                    <head>2.7. Comparison of Results of FiNER and ARPA</head>
                    <p>To sum up results of our two main tools, we show one more table where the
                        main comparable results of FiNER and ARPA are shown in parallel. These are
                        results of loose evaluations from Tables 2 and 7. </p>
                    <table>
                        <head>Comparative evaluation results of FiNER and ARPA with loose
                            criteria</head>
                        <row>
                            <cell/>
                            <cell role="label">P FiNER</cell>
                            <cell role="label">P ARPA</cell>
                            <cell role="label">R FiNER</cell>
                            <cell role="label">R ARPA</cell>
                            <cell role="label">F-score FiNER</cell>
                            <cell role="label">F-score ARPA</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                            <cell>69.05</cell>
                            <cell>63.61</cell>
                            <cell>49.21</cell>
                            <cell>45.27</cell>
                            <cell>57.47</cell>
                            <cell>52.90</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell>63.30</cell>
                            <cell>44.02</cell>
                            <cell>53.69</cell>
                            <cell>64.58</cell>
                            <cell>58.10</cell>
                            <cell>52.35</cell>
                        </row>
                    </table>
                    <p>As one can see, FiNER perfoms slightly better with locations and persons than
                        ARPA. The difference in F-scores is about 5 percentage units.</p>
                </div>
                <div>
                    <head>2.8. Results of Other Systems</head>
                    <p>Here we report briefly results of three other systems that we evaluated.
                        These are Polyglot, a Finnish semantic tagger <ptr target="#lofberg2005"/>
                        and Connexor’s NER.</p>
                    <p>Polyglot<note>
                            <ref target="http://polyglot.readthedocs.io/en/latest/index.html"
                                >http://polyglot.readthedocs.io/en/latest/index.html</ref></note> is
                        a natural language pipeline that supports multilingual applications. NER is
                        among Polyglot’s tools. According to Polyglot Website, the NER models of
                        Polyglot were trained on datasets extracted automatically from Wikipedia.
                        Polyglot’s NER supports currently 40 major languages.</p>
                    <p>Results of Polylot’s performance in a loose evaluation with three categories
                        are shown in table 10.</p>
                    <table>
                        <head>Evaluation results of Polyglot with loose criteria and three
                            categories</head>
                        <row>
                            <cell role="label">Label</cell>
                            <cell role="label">P</cell>
                            <cell role="label">R</cell>
                            <cell role="label">F-score</cell>
                            <cell role="label">Number of tags found</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                            <cell>75.99</cell>
                            <cell>34.60</cell>
                            <cell>47.55 </cell>
                            <cell>1433</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                            <cell>83.56</cell>
                            <cell>32.28</cell>
                            <cell>46.57 </cell>
                            <cell> 821</cell>
                        </row>
                        <row>
                            <cell><code>&lt;EnamemOrgCrp&gt;</code></cell>
                            <cell>5.77</cell>
                            <cell>1.70</cell>
                            <cell>2.63</cell>
                            <cell>208</cell>
                        </row>
                    </table>
                    <p>As can be seen from the figures, Polyglot has high precision with persons and
                        locations, but quite bad recall, and F-scores are thus about 10% units below
                        FiNER’s performance and clearly below ARPA's performance. With corporations
                        Polyglot performs very poorly. The reason for this is probably the fact that
                        names of companies have changed and organizations taken out of Wikipedia do
                        not contain old company names. </p>
                    <div>
                        <head>2.8.1. Results of a Semantic Tagger of Finnish</head>
                        <p>Our fourth tool is a general semantic tagger tool for Finnish. The
                            Finnish Semantic Tagger (FST) has its origins in Benedict, the EU-funded
                            language technology project, the aim of which was to discover an optimal
                            way of catering to the needs of dictionary users in modern electronic
                            dictionaries by utilizing state-of-the-art language technology. Semantic
                            tagging in its rule-oriented form (vs. statistical learning) can be
                            briefly defined as a dictionary-based process of identifying and
                            labeling the meaning of words in a given text according to some
                            classification. FST is not a NER tool as such; it has first and foremost
                            been developed for the analysis of full text. </p>
                        <p>The Finnish semantic tagger was developed using the English Semantic
                            Tagger as a model. This semantic tagger was developed at the University
                            Centre for Corpus Research on Language (UCREL) at Lancaster University
                            as part of the UCREL Semantic Analysis System (USAS) framework <ptr
                                target="#rayson2004"/>, and both these equivalent semantic taggers
                            were utilized in the Benedict project in the creation of a
                            context-sensitive search tool for a new intelligent dictionary. In
                            different evaluations the FST has been shown to be capable of dealing
                            with most general domains which appear in a modern standard Finnish
                            text. Furthermore, although the semantic lexical resources were
                            originally developed for the analysis of general modern standard
                            Finnish, evaluation results have shown that the lexical resources are
                            also applicable to the analysis of both older Finnish text and the more
                            informal type of writing found on the Web. In addition, the semantic
                            lexical resources can be tailored for various domain-specific tasks
                            thanks to the flexible USAS category system. The semantically
                            categorized single word lexicon of the FST contains 46,225 entries and
                            the multiword expression lexicon contains 4,422 entries <ptr
                                target="#piao2016"/>, representing all parts of speech. There are
                            plans to expand the semantic lexical resources for the FST by adding
                            different types of proper names in the near future in order to tailor
                            them for NER tasks. </p>
                        <p>FST tags three different types of names: personal names, geographical
                            names, and other proper names. These are tagged with tags Z1, Z2, and
                            Z3, respectively <ptr target="#lofberg2005"/>. FST does not distinguish
                            first names and surnames, but it is able to tag first names of persons
                            with male and female sub tags. As Z3 is a slightly vague category with
                            names of organizations among others, we evaluate only categories Z1 and
                            Z2–persons and locations.</p>
                        <p>FST tagged the list of 75,980 names as follows: it marked 5,569 names
                            with tags Z1-Z3. Out of these 3,473 were tagged as persons, 2,010 as
                            locations and rest as other names. It tagged 47,218 words with the tag
                            Z99, which is a mark for lexically unknown words. Rest of the words,
                            23,193, were tagged with tags of common nouns. Thus FST’s recall with
                            the name list is quite low compared to FiNER and ARPA.</p>
                        <p>In Table 11 we show results of FST’s tagging of locations and persons in
                            our evaluation data. As the tagger does not distinguish multipart names
                            only loose evaluation was performed. We performed two evaluations: one
                            with the words as they are, and the other with w to v substitution.</p>
                        <table>
                            <head>Evaluation of FST tagger with loose criteria and two categories.
                                W/v stands for w to v substitution in words.</head>
                            <row>
                                <cell role="label">Label</cell>
                                <cell role="label">P</cell>
                                <cell role="label">R</cell>
                                <cell role="label">F-score</cell>
                                <cell role="label">Number of tags found</cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                                <cell>76.48</cell>
                                <cell>22.48</cell>
                                <cell>34.75</cell>
                                <cell> 897 </cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                                <cell>67.11</cell>
                                <cell>47.72</cell>
                                <cell>55.78</cell>
                                <cell>1420</cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexPrsHum&gt;</code> w/v</cell>
                                <cell>76.10</cell>
                                <cell>23.06</cell>
                                <cell>35.39</cell>
                                <cell>908</cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexLocXxx&gt;</code> w/v</cell>
                                <cell>69.66</cell>
                                <cell> 51.34 </cell>
                                <cell>59.12</cell>
                                <cell>1536</cell>
                            </row>
                        </table>
                        <p>Substitution of <emph>w</emph> with <emph>v</emph> decreased number of
                            unknown words to FST with about 3% units and has a noticeable effect on
                            detection of locations and a small effect on persons. Overall locations
                            are recognized better; their recognition with w/v substitution is
                            slightly better than FiNER’s and better than ARPA’s overall. FST’s
                            recognition of persons is clearly inferior to that of FiNER and
                            ARPA.</p>
                    </div>
                    <div>
                        <head>2.8.2 Results of Connexor’s NER</head>
                        <p>Connexor Ltd. has provided different language technology tools, and among
                            them is name recognition<note>
                                <ref
                                    target="https://www.connexor.com/nlplib/?q=technology/name-recognition"
                                    >https://www.connexor.com/nlplib/?q=technology/name-recognition</ref></note>.
                            There is no documentation related to the software, but Connexor states
                            on their Web pages that <q>using linguistic and heuristic methods, the
                                names in the text can be tagged accurately</q>. Software’s name type
                            repertoire is large; at least 31 different types of names are
                            recognized. These are part of 9 larger categories like NAME.PER
                            (persons), NAME.PRODUCT (products), NAME.GROUP (organizations), NAME.GPE
                            (locations) etc. Boundaries of names are not tagged, so we perform only
                            a loose evaluation.</p>
                        <p>As earlier, our interest is mainly in persons and locations. Connexor’s
                            tags NAME.GPE, NAME.GPE.City, NAME.GPE.Nation, NAME.GEO.Land and
                            NAME.GEO.Water were all treated as <code>&lt;EnamexLocXxx&gt;</code>.
                            NAME.PER, NAME.PER.LAW, NAME.PER.GPE, NAME.PER.Leader, NAME.PER.MED,
                            NAME.PER.TEO and NAME.PER.Title were all treated as
                                <code>&lt;EnamexPrsHum&gt;</code>. All other tags were discarded.
                            Results of Connexor’s tagger are shown in Table 12.</p>
                        <table>
                            <head>Evaluation of Connexor’s tagger with loose criteria and two
                                categories</head>
                            <row>
                                <cell role="label">Label</cell>
                                <cell role="label">P</cell>
                                <cell role="label">R</cell>
                                <cell role="label">F-score</cell>
                                <cell role="label">Number of tags found</cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexPrsHum&gt;</code></cell>
                                <cell>44.86</cell>
                                <cell>76.02</cell>
                                <cell>56.40</cell>
                                <cell>5321</cell>
                            </row>
                            <row>
                                <cell><code>&lt;EnamexLocXxx&gt;</code></cell>
                                <cell>66.76</cell>
                                <cell>55.93</cell>
                                <cell>60.87 </cell>
                                <cell>1802</cell>
                            </row>
                        </table>
                        <p>Results show that Connexor’s NE tagger is better with locations–achieving
                            the best overall F-score of all the tools–but also persons are found
                            well. Recall with persons is high, but low precision hurts overall
                            performance. Data inspection shows that Connexor’s tagger has a tendency
                            to tag words beginning with upper case as persons. Locations are also
                            mixed with persons many times. </p>
                    </div>
                </div>
                <div>
                    <head>2.9. Results overall</head>
                    <p>If we consider results of FiNER and ARPA overall, we can make the following
                        observations. They both seem to find two part person names best, most of
                        which consist of first name and last name. In strict evaluation ARPA appears
                        better with locations than FiNER, but this is due to the fact that FiNER has
                        a more fine-grained location tagging. With one location tag FiNER performs
                        equally well as ARPA. In loose evaluation they both seem to find almost
                        equally well locations and persons, but FiNER gets slightly better results.
                        FiNER finds educational organizations best, although they are scarce in the
                        data. Corporations are also found relatively well, even though this category
                        is prone to historical changes. FiNER is precise in finding two part street
                        names, but recall in street name tagging is low. High precision is most
                        likely due to common part <foreign xml:lang="fi">–katu</foreign> in street
                        names: they are easy to recognize, if they are spelled right in the data.
                        Low recall indicates bad OCR in street names.</p>
                    <p>Out of the other three tools we evaluated, the FST was able to recognize
                        locations slightly better than FiNER or ARPA in loose evaluation when w/v
                        variation was neutralized. Connexor’s tagger performed at the same level as
                        FINER and ARPA in loose evaluation. Its F-score with locations was the best
                        performance overall. Polyglot performed worst of all the systems.</p>
                    <p>We evaluated lexical coverage of three of our tools with a wordlist that
                        contained 75,980 names. ARPA’s lexical coverage of the names was by far the
                        best as it was able to recognize 97.4% of the names. FiNER recognized 73% of
                        the names in this list and the FST recognized only about 7% of them as
                        names. It marked about 62% of the names as unknown. Thus it seems that very
                        high lexical coverage of names may not be the key issue in NER, as all three
                        tools performed tagging of locations at the same level. The FST performed
                        worst with persons although it had clearly more person names than locations
                        in its lexicon.</p>
                    <p>One more caveat of performance is in order, especially with FiNER. After we
                        had achieved our evaluation results, we evaluated FiNER’s context
                        sensitivity with a small test. Table 13 shows effect of different contexts
                        on FiNER’s tagging for 320 names of municipalities. In the leftmost column
                        are results, where only a name list was given to FiNER. In the three
                        remaining columns, names of the municipality was changed from the beginning
                        of a clause to middle and end. Results imply that there is context
                        sensitivity in FiNER’s tagging. With no context at all results are worst,
                        and when the location is at the beginning of the sentence, FiNER also misses
                        more tags than in the other two positions. Overall it tags about two thirds
                        of the municipality names as locations (<term>LocXxx</term> and
                            <term>LocGpl</term>) in all the three context positions. The high number
                        of municipalities tagged as persons is partly understandable as names are
                        ambiguous, but in many cases interpretation as a person is not well
                        grounded. This phenomenon derives clearly from FiNER’s tagging strategy that
                        was explained at the end of section 2.1. At the beginning of the clause
                        locations are not confused as much as persons, but this comes with a cost of
                        more untagged names.</p>
                    <table>
                        <head>FiNER’s tagging for 320 names of municipalities with different
                            positional context for the name</head>
                        <row>
                            <cell role="label">No context, list of names</cell>
                            <cell role="label">With context 1: location at the beginning</cell>
                            <cell role="label"> With context 2: location in the middle </cell>
                            <cell role="label">With context 3: location at the end</cell>
                        </row>
                        <row>
                            <cell>111 LocXxx</cell>
                            <cell>151 LocXxx</cell>
                            <cell>158 LocXxx</cell>
                            <cell>159 LocXxx</cell>
                        </row>
                        <row>
                            <cell>84 PrsHum</cell>
                            <cell>66 PrsHum</cell>
                            <cell>80 PrsHum</cell>
                            <cell>80 PrsHum</cell>
                        </row>
                        <row>
                            <cell>7 LocGpl</cell>
                            <cell>56 LocGpl</cell>
                            <cell>54 LocGpl</cell>
                            <cell>54 LocGpl</cell>
                        </row>
                        <row>
                            <cell>12 OrgCrp</cell>
                            <cell>10 OrgCrp</cell>
                            <cell>12 OrgCrp</cell>
                            <cell>11 OrgCrp</cell>
                        </row>
                        <row>
                            <cell>2 OrgTvr</cell>
                            <cell>2 OrgTvr</cell>
                            <cell>2 OrgTvr</cell>
                            <cell>2 OrgTvr</cell>
                        </row>
                        <row>
                            <cell>102 no tag</cell>
                            <cell>35 no tag</cell>
                            <cell>14 no tag</cell>
                            <cell>14 no tag</cell>
                        </row>
                    </table>
                    <p>Same setting was tested further with 15,480 last names in three different
                        clause positions. Positional effect with last name tagging was almost
                        nonexistent, but amount of both untagged names and locative interpretations
                        is high. 39% of last names are tagged as PrsHum, 19.5% are tagged as LocXxx,
                        and about 34.6% get no tag at all. The rest 7% are in varying categories.
                        Tagging of last names would probably be better if first names were given
                        together with last names. Isolated last names are more ambiguous.</p>
                    <p>We did not test effects of contextualization with other taggers, but it may
                        have had a minor effect on all our results, as input text snippets were of
                        different sizes (see section 2.3). Especially if first and last names are
                        separated to different input snippets identification of person names may
                        suffer.</p>
                </div>
                <div>
                    <head>2.10. Error Analysis of Results</head>
                    <p>Ehrmann et al. <ptr target="#ehrmann2016a"/> suggest that application of NER
                        tools on historical texts faces three challenges: 1) noisy input texts, 2)
                        lack of coverage in linguistic resources, and 3) dynamics of language. In
                        our case the first obstacle is the most obvious, which will be shown in more
                        detail. Lack of coverage in linguistic resources (e.g. in the form of
                        missing old names in the lexicons of the NE tools) is also a considerable
                        source of errors in our case, as our tools are made for modern Finnish. With
                        dynamics of language Ehrmann et al. refer to different rules and conventions
                        for the use of written language in different times. In this respect late
                        19th century Finnish is not that different from current Finnish, but this
                        can certainly affect the results and should be studied more thoroughly.
                        Considering that all our NE tools are made for modern Finnish, our
                        evaluation data is heavily out of their main scope <ptr
                            target="#poibeau2001"/>, even if ARPA uses historical Finnish aware
                        Omorfi and FiNER is able to guess unrecognized word forms.</p>
                    <p>To be able to estimate the effect of bad OCR on the results, we made some
                        additional trials with improved OCR material. We made tests with three
                        versions of a 500,000 word text material that is different from our NER
                        evaluation material but derives from the 19th century newspapers as well.
                        One version was manually corrected OCR, another an old OCRed version, and
                        third a new OCRed version. Besides character-level errors, word order errors
                        have been corrected in the two new versions. For these texts we did not have
                        a ground truth NE tagged version, and thus we could only count number of NE
                        tags in different texts. With FiNER total number of tags increased from
                        23,918 to 26,674 (+11.5% units) in the manually corrected text version.
                        Number of tags increased to 26,424 tags (+10.5% units) in the new OCRed text
                        version. Most notable were increases in the number of tags in the categories
                            <term>EnamexLocStr</term> and <term>EnamexOrgEdu</term>. With ARPA,
                        results were even slightly better. ARPA recognized 10,853 places in the old
                        OCR, 11,847 in the new OCR (+9.2% units) and 13,080 (+20.5 % units) in the
                        ground truth version of the text. Thus there is about a 10–20 % unit overall
                        increase in the number of NE tags in both of the new better quality text
                        versions in comparison to the old OCRed text with both taggers.</p>
                    <p>Another clear indication of effect of the OCR quality on the NER results is
                        the following observation: when the words in all the correctly tagged FiNER
                            <term>Enamex</term>es of the evaluation data are analyzed with Omorfi,
                        only 14.3% of them are unrecognized. With wrongly tagged FiNER
                            <term>Enamex</term>es 26.3% of the words are unrecognized by Omorfi. On
                        tag level the difference is even clearer, as can be seen in recognition
                        figures of Fig. 1 with words of locations and persons of FiNER, ARPA, FST
                        and Connexor analyses (FiNER’s analysis was reduced to a single location
                        class). Thus improvement in OCR quality will most probably bring forth a
                        clear improvement in NER of the material.</p>
                    <figure>
                        <head>Word unrecognition percentages with rightly and wrongly tagged
                            locations and persons – recognition with Omorfi 0.3</head>
                        <graphic url="resources/images/figure01.png"/>
                    </figure>
                    <p>We also performed tagger specific error analysis for our tools. This analysis
                        is not systematic because sources for the errors often remain unclear, but
                        it shows some general tendencies of our tools. Besides general and tagger
                        errors it also reveals some general characteristics of our data. The errors
                        reported here can also be seen as common improvement goals for better NE
                        tagging of our newspaper data.</p>
                    <p>Connexor’s NER tool seems to get misspelled person names many times right,
                        even though percentage of morphologically unrecognized words among the
                        locations is quite high in Fig. 1. Some of the rightly tagged but clearly
                        misspelled examples are <foreign xml:lang="fi">Elwira4w, Aletsanterinp,
                            Söderhslm, Tuomaanp, Hcikinp, AleNander, Hartifaincu,
                            Schvvindt</foreign> and <foreign xml:lang="fi">NyleniUß.</foreign> On
                        the other hand, the tagger also tags common nouns with upper case initial as
                        person names. Examples of these are words such as <foreign xml:lang="fi"
                            >Prospekteja, Diskontto, Telefooni.</foreign> Without proper
                        documentation of the tagger we are inclined to believe that upper case
                        detection is one of the cues for person name detection in the tagger. This
                        favours detection of misspelled person names even without any handling of
                        typographical errors as such. It can also bring quite odd errors that we
                        listed – these words have nothing in common with person names. Especially
                        odd is the error with <foreign xml:lang="fi">Telefooni</foreign>
                            (<soCalled>telephone</soCalled>) which is many times followed by a
                        number, and the whole is presenting a phone number. Both the word <foreign
                            xml:lang="fi">Telefooni</foreign> and the number are tagged as person
                        names. Also some title-like words before person names are tagged as persons,
                        although they are not names of persons. With regards to locations, Connexor
                        seems to be more cautious: most of its locations are correct spellings of
                        words. Only a few misspellings like <foreign xml:lang="fi"
                            >Suomuzsalmi</foreign> and <foreign xml:lang="fi">Siräiöniemi</foreign>
                        are analyzed right as locations. In this case it seems that the correctly
                        spelled locative parts–<foreign xml:lang="fi">salmi
                            </foreign>(<soCalled>strait</soCalled>) and <foreign xml:lang="fi"
                            >-niemi </foreign>(<soCalled>cape</soCalled>) are cues for the
                        tagger.</p>
                    <p>FiNER analyses some misspelled person names right, but previously mentioned
                            <foreign xml:lang="fi">Söderhslm</foreign> and <foreign xml:lang="fi"
                            >AleNander</foreign> are not analysed as person names. The FST analyses
                        only correctly-spelled person and location names correctly as it has no
                        means to handle spelling errors or variation in names. It could benefit from
                        some form of fuzzy matching to be able to handle misspellings and
                        variations, although this can also be counterproductive, as was seen with
                        ARPA’s fuzzy matching results in Tables 5–8: fuzzy matching increased recall
                        and lowered precision, and the overall effect in F-score could be either
                        slightly positive or negative.</p>
                    <p>Closer examination of FiNER’s street name results shows that problems in
                        street name recognition are due to three main reasons: OCR errors in street
                        names, abbreviated street names, and multipart street names with numbers as
                        part of the name. In principle streets are easy to recognize in Finnish,
                        while they have most of the time common part -<foreign xml:lang="fi">katu
                            </foreign>(<soCalled>street</soCalled>) as last part of their name,
                        which is usually a compound word or a phrase. Common use of abbreviations
                        with street names seems to be characteristic for the data and it can be
                        considered as a style of writing for the era. Street name like <foreign
                            xml:lang="fi">Aleksanterinkatu</foreign> would be written as <foreign
                            xml:lang="fi">Aleksanterink.</foreign></p>
                    <p>Another similar case are first name initials which are used a lot in 19th
                        century Finnish newspaper texts. Names like <foreign xml:lang="fi">O.
                            Junttila</foreign>, <foreign xml:lang="fi">Y. Koskinen </foreign>(first
                        name initial with surname, there can also be more initials) are common.
                        FiNER gets initials right if they are preceded by a title, but otherwise
                        not, and thus it gets these mostly wrong. The FST is not able to analyse
                        these at all. Connexor’s tagger gets one initial right if it is followed by
                        surname, but out of two initials it only analyses the second one. This kind
                        of writing of names belongs also to Ehrmann et al’s dynamics of language
                        type of challenges <ptr target="#ehrmann2016a"/>. Usage of initials only for
                        first names was common in the 19th century Finnish newspapers, but it is not
                        used in modern newspaper writing.</p>
                    <p> One common source of errors for all NE taggers originates from ambiguity of
                        some name types. Many Finnish surnames can be also names of locations,
                        either names of municipalities, villages or houses. These kind of names are
                        e.g. <foreign xml:lang="fi">Marttila, Liuhala,</foreign> and <foreign
                            xml:lang="fi">Ketola.</foreign> All our taggers make errors with these,
                        as only contextual cues specify whether the name is used as a person name or
                        as a name of location. Proper handling of these names would need some extra
                        disambiguation effort from the taggers. Although FiNER has a pattern
                        matching mechanism for these (cf. 2.1) it also makes errors with ambiguous
                        names. </p>
                </div>
            </div>
            <div>
                <head>3. Discussion</head>
                <p>We have shown in this paper evaluation results of NER for historical Finnish
                    newspaper material from the 19th and early 20th century with two main tools,
                    FiNER and SeCo’s ARPA. Besides these two tools, we briefly evaluated three other
                    tools: a Finnish semantic tagger, Polyglot’s NER and Connexor’s NER. We were not
                    able to train Stanford NER for Finnish. As far as we know, the tools we have
                    evaluated constitute a comprehensive selection of tools that are capable of
                    named entity recognition for Finnish although not all of them are dedicated NER
                    taggers.</p>
                <p>Word level correctness of the whole digitized newspaper archive is approximately
                    70–75% <ptr target="#kettunen2016"/>; the evaluation corpus had a word level
                    correctness of about 73%. Regarding this and the fact that FiNER and ARPA and
                    other tools were developed for modern Finnish, the newspaper material makes a
                    very difficult test for named entity recognition. It is obvious that the main
                    obstacle of high class NER in this material is the poor quality of the text.
                    Also historical spelling variation had some effect, but it should not be that
                    high, as late 19th century Finnish is not too far from modern Finnish and can be
                    analyzed reasonably well with modern morphological tools <ptr
                        target="#kettunen2016"/>. Morphological analyzers used in both FiNER and
                    ARPA seem to be flexible and are able to analyze our low quality OCRed texts
                    with a guessing mechanism too. The FST and Connexor’s NER also performed quite
                    well with morphology.</p>
                <p>NER experiments with OCRed data in other languages usually show some improvement
                    of NER when the quality of the OCRed data has been improved from very poor to
                    slightly better <ptr target="#packer2010"/>
                    <ptr target="#marrero2013"/>
                    <ptr target="#miller2000"/>. Results of Alex and Burns (2014) imply that with
                    lower level OCR quality (below 70% word level correctness) name recognition is
                    harmed clearly <ptr target="#alex2014"/>. Packer et al. (2010) report partial
                    correlation of Word Error Rate of the text and achieved NER result; their
                    experiments imply that word order errors are more significant than character
                    level errors <ptr target="#packer2010"/>. Miller et al. (2000) show that rate of
                    achieved NER performance of a statistical trainable tagger degraded linearly as
                    a function of word error rates <ptr target="#miller2000"/>. On the other hand,
                    results of Rodriguez et al. (2012) show that manual correction of OCRed material
                    that has 88–92% word accuracy does not increase performance of four different
                    NER tools significantly <ptr target="#rodriguez2012"/>.</p>
                <p>As the word accuracy of our material is low, it would be expected that better
                    recognition results would be achieved if the word accuracy was around 80–90%
                    instead of 70–75%. Our tests with different quality texts suggest this too, as
                    do the distinctly different unrecognition rates with correctly and incorrectly
                    tagged words.</p>
                <p>Better quality for our texts may be achievable in the near future. Promising
                    results in post-correction of the Finnish historical newspaper data have been
                    reported recently: two different correction algorithms developed in the
                    FIN-CLARIN consortium achieved correction rate of 20–35 % <ptr
                        target="#silfverberg2016"/>. We are also progressing in re-OCRing tests of
                    the newspaper data with open source OCR engine, Tesseract<note>
                        <ref target="https://github.com/tesseract-ocr"
                            >https://github.com/tesseract-ocr</ref></note>, and may be able to
                    improve the OCR quality of our data <ptr target="#kettunenetal2016"/>
                    <ptr target="#koistinen2017"/>. Together improved OCR and post-correction may
                    yield 80+% word level recognition for our data. Besides character level errors
                    our material also has quite a lot of word order errors which may affect
                    negatively the NER results <ptr target="#packer2010"/>. Word order of the
                    material may be improved in later processing of the XML ALTO and METS data, and
                    this may also improve NER results. It would also be important that word splits
                    due to hyphenation could be corrected in the data <ptr target="#packer2010"
                    />.</p>
                <p>Four of the five taggers employed in the experiments are rule-based systems
                    utilizing different kinds of morphological analysis, gazetteers, and pattern and
                    context rules. The only exception was Polyglot. However, while there has been
                    some recent work on rule-based systems for NER <ptr target="#kokkinakis2014"/>,
                    the most prominent research on NER has focused on statistical machine learning
                    methodology <ptr target="#nadeau2007"/>. Therefore, we are currently developing
                    a statistical NE tagger for historical Finnish text. For training and evaluating
                    the statistical system, we are manually annotating newspaper and magazine text
                    from the years 1862–1910 with classes for person, organization, and location.
                    The text contains approximately 650,000 word tokens. After annotation we can
                    utilize freely available toolkits, such as the Stanford Named Entity Recognizer,
                    for teaching the NE tagger. We expect that the rich feature sets enabled by
                    statistical learning will alleviate the effect of poor OCR quality on the
                    recognition accuracy of NEs. Preliminary unpublished results comparing FiNER and
                    adjusted Stanford NER for modern Finnish business newspaper data show that
                    Stanford NER outperforms FiNER with about 30% units in combined results of five
                    name categories. For recent work on successful statistical learning of NE
                    taggers for historical data, see <ptr target="#neudecker2016"/>. </p>
                <p>On general level, there are a few lessons to be learned from our experiments for
                    those that are working with other small languages that do not have a
                    well-established repertoire of NER tools available. Some of them are well known,
                    but worth repeating, too. First and most self-evidently, bad OCR was found to be
                    the main obstacle for good quality NER once again. This was shown clearly in
                    section 2.10. The implications of this are clear: better quality data is needed
                    to make NER working well enough to be useful. Second, slightly surprisingly, we
                    noticed that differences in lexical coverage of the tools will not show that
                    much in the NER results. ARPA had clearly the best lexical coverage of the tools
                    and FST the worst coverage, but their NER performance with locations are quite
                    equal. This could imply that very large lexicons for a NE tagger are not
                    necessary and a good basic coverage lexicon is enough, but this could also be
                    language specific. Third, we showed that historical language can be processed to
                    a reasonable extent with tools that are made for modern language if nothing else
                    is available. However, if best possible results need to be achieved, more
                    historical data oriented tools need to be used. It is also possible that the
                    quite short time frame of our material enhances performance of our tools.
                    Fourth, results of The Finnish Semantic Tagger showed that NER does not need to
                    be only a task for dedicated NER tools. This has also been shown with modern
                    Finnish <ptr target="#kettunen2017a"/>. FST performed almost as well in a modern
                    Finnish NER evaluation task as FiNER. Thus, at least in some cases, one can have
                    available a suitable tool for doing NER even if the tool does not look like a NE
                    tagger. Fifth, results of Polyglot hint that general multilingual packages do
                    not work very well with data of small languages if their training data was not
                    suitable for the task at hand. A lesson that we learned too was, that too
                    detailed named entity classification (original FiNER labeling) is probably not a
                    good solution for our purposes. In the future we shall stick to the use of
                    persons, locations, and organizations.</p>
                <p>Our main emphasis with NER will be to use the names with the newspaper collection
                    as a means to improve structuring, browsing, and general informational usability
                    of the collection. A good enough coverage of the names with NER also needs to be
                    achieved for this use, of course. A reasonable balance of P/R should be found
                    for this purpose, but also other capabilities of the software need to be
                    considered. These lingering questions must be addressed if we are to connect
                    some type of functional NER to our historical newspaper collection’s user
                    interface.</p>
            </div>
            <div>
                <head>Acknowledgements</head>
                <p>First and third author were funded by the Academy of Finland, project
                    Computational History and the Transformation of Public Discourse in Finland
                    1640–1910 (COMHIS), decision number 293 341.</p>
                <p>Thanks to Heikki Kantola and Connexor Ltd. for providing the evaluation data of
                    Connexor’s NE tagger.</p>
                <p>Corresponding author: Kimmo Kettunen, ORCID: <ref
                        target="http://orcid.org/0000-0003-2747-1382">0000-0003-2747-1382</ref></p>
            </div>
        </body>
        <back>
            <listBibl>
                <bibl xml:id="alex2014" label="Alex and Burns 2014">Alex, B. and Burns, J. (2014),
                        <title rend="quotes">Estimating and Rating the Quality of Optically
                        Character Recognised Text</title>, in <title rend="italic">DATeCH '14
                        Proceedings of the First International Conference on Digital Access to
                        Textual Cultural Heritage</title>, available at: <ref
                        target="http://dl.acm.org/citation.cfm?id=2595214"
                        >http://dl.acm.org/citation.cfm?id=2595214</ref> (accessed 10 October
                    2015).</bibl>
                <bibl xml:id="bates2007" label="Bates 2007">Bates, M. (2007), <title rend="quotes"
                        >What is Browsing – really? A Model Drawing from Behavioural Science
                        Research</title>, <title rend="italic">Information Research</title> 12,
                    available at: <ref target="http://www.informationr.net/ir/12-4/paper330.html"
                        >http://www.informationr.net/ir/12-4/paper330.html</ref>(accessed 1 June
                    2016).</bibl>
                <bibl xml:id="bremerlaamanen2014" label="Bremer-Laamanen 2014">Bremer-Laamanen, M-L.
                    (2014), <title rend="quotes">In the Spotlight for Crowdsourcing</title>, <title
                        rend="italic">Scandinavian Librarian Quarterly</title>, Vol. 46, No. 1, pp.
                    18–21.</bibl>
                <bibl xml:id="crane2006" label="Crane and Jones 2006">Crane, G. and Jones, A.
                    (2006), <title rend="quotes">The Challenge of Virginia Banks: An Evaluation of
                        Named Entity Analysis in a 19th-Century Newspaper Collection</title>, in
                        <title rend="italic">Proceedings of JCDL’06</title>, June 11–15, 2006,
                    Chapel Hill, North Carolina, USA, available at: <ref
                        target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.6257&amp;rep=rep1&amp;type=pdf"
                        >http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.6257&amp;rep=rep1&amp;type=pdf</ref>
                    (accessed 1 June 2016).</bibl>
                <bibl xml:id="ehrmann2016a" label="Ehrmann et al. 2016a">Ehrmann, M., Colavizza, G.,
                    Rochat, Y. and Kaplan, F. (2016a). <title rend="quotes">Diachronic Evaluation of
                        NER Systems on Old Newspapers.</title> In <title rend="italic">Proceedings
                        of the 13th Conferenceon Natural Language Processing</title> (KONVENS 2016),
                    97–107. <ref
                        target="https://www.linguistics.rub.de/konvens16/pub/13_konvensproc.pdf"
                        >https://www.linguistics.rub.de/konvens16/pub/13_konvensproc.pdf</ref>
                    (accessed March 28 2017). </bibl>
                <bibl xml:id="ehrmann2016b" label="Ehrmann et al. 2016b">Ehrmann, M., Nouvel, D. and
                    Rosset, S. (2016b), <title rend="quotes">Named Entity Resources – Overview and
                        Outlook</title>, in <title rend="italic">LREC 2016, Tenth International
                        Conference on Language Resources and Evaluation</title>, available at <ref
                        target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/987_Paper.pdf"
                        >http://www.lrec-conf.org/proceedings/lrec2016/pdf/987_Paper.pdf</ref>
                    (accessed June 15 2016).</bibl>
                <bibl xml:id="hallo2016" label="Hallo et al. 2016">Hallo, M., Luján-Mora, S., Maté,
                    A. and Trujillo, J. (2016), <title rend="quotes">Current State of Linked Data in
                        Digital Libraries</title>, <title rend="italic">Journal of Information
                        Science</title>, Vol. 42, No. 2, pp. 117–127.</bibl>
                <bibl xml:id="hyvonen2011" label="Hyvönen et al. 2011">Hyvönen, E., Tuominen, J.,
                    Kauppinen T. and Väätäinen, J. (2011), <title rend="quotes">Representing and
                        Utilizing Changing Historical Places as an Ontology Time Series</title>, in
                    Ashish, N. and Sheth, V. (Eds.) <title rend="italic">Geospatial Semantics and
                        Semantic Web: Foundations, Algorithms, and Applications</title>, Springer
                    US, pp. 1–25.</bibl>
                <bibl xml:id="ikkala2016" label="Ikkala et al. 2016">Ikkala, E., Tuominen, J. and
                    Hyvönen, E. (2016), <title rend="quotes">Contextualizing Historical Places in a
                        Gazetteer by Using Historical Maps and Linked Data</title>, <title
                        rend="italic">in Digital Humanities 2016: Conference Abstracts</title>,
                    Jagiellonian University &amp; Pedagogical University, Kraków, pp. 573-577,
                    available at: <ref target="http://dh2016.adho.org/abstracts/39"
                        >http://dh2016.adho.org/abstracts/39</ref>(accessed October 1 2016).</bibl>
                <bibl xml:id="jarvelin2016" label="Järvelin et al. 2016">Järvelin, A., Keskustalo,
                    H., Sormunen, E., Saastamoinen, M. and Kettunen, K. (2016), <title rend="quotes"
                        >Information Retrieval from Historical Newspaper Collections in Highly
                        Inflectional Languages: A Query Expansion Approach</title>, <title
                        rend="italic">Journal of the Association for Information Science and
                        Technology,</title> 67(12), 2928–2946.</bibl>
                <bibl xml:id="kettunen2014" label="Kettunen et al. 2014">Kettunen, K., Honkela, T.,
                    Lindén, K., Kauppinen, P., Pääkkönen, T. and Kervinen, J. (2014), <title
                        rend="quotes">Analyzing and Improving the Quality of a Historical News
                        Collection using Language Technology and Statistical Machine Learning
                        Methods</title>, in <title rend="italic">Proceedings of IFLA 2014</title>,
                    Lyon (2014), available at: <ref
                        target="http://www.ifla.org/files/assets/newspapers/Geneva_2014/s6-honkela-en.pdf"
                        >http://www.ifla.org/files/assets/newspapers/Geneva_2014/s6-honkela-en.pdf</ref>
                    (accessed March 15 2015).</bibl>
                <bibl xml:id="kettunen2017a" label="Kettunen and Löfberg 2017">Kettunen, K. and
                    Löfberg, L. (2017). <title rend="quotes">Tagging Named Entities in 19th Century
                        and Modern Finnish Newspaper Collection with a Finnish Semantic
                        Tagger.</title> In <title rend="italic">Nodalida 2017</title>, <ptr
                        target="http://www.ep.liu.se/ecp/131/Title_Pages.pdf"/>(accessed August 7
                    2017). </bibl>
                <bibl xml:id="kettunen2016" label="Kettunen and Pääkkönen 2016">Kettunen, K. and
                    Pääkkönen, T. (2016), <title rend="quotes">Measuring Lexical Quality of a
                        Historical Finnish Newspaper Collection – Analysis of Garbled OCR Data with
                        Basic Language Technology Tools and Means</title>, in <title rend="italic"
                        >LREC 2016, Tenth International Conference on Language Resources and
                        Evaluation</title>, available at <ref
                        target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/17_Paper.pdf"
                        >http://www.lrec-conf.org/proceedings/lrec2016/pdf/17_Paper.pdf</ref>(accessed
                    15 June 2016).</bibl>
                <bibl xml:id="kettunenetal2016" label="Kettunen et al. 2016">Kettunen, K.,
                    Pääkkönen, T. and Koistinen, M. (2016), <title rend="quotes">Between Diachrony
                        and Synchrony: Evaluation of Lexical Quality of a Digitized Historical
                        Finnish Newspaper and Journal Collection with Morphological
                        Analyzers</title>, in: Skadiņa, I. and Rozis, R. (Eds.), <title
                        rend="italic">Human Language Technologies – The Baltic Perspective</title>,
                    IOS Press, pp. 122–129. Available at: <ref
                        target="http://ebooks.iospress.nl/volumearticle/45525"
                        >http://ebooks.iospress.nl/volumearticle/45525</ref> (accessed October 12
                    2016).</bibl>
                <bibl xml:id="kettunen2017b" label="Kettunen and Ruokolainen 2017">Kettunen, K. and
                    Ruokolainen, T. (2017). <title rend="quotes">Names, Right or Wrong: Named
                        Entities in an OCRed Historical Finnish Newspaper Collection.</title> In
                        <title rend="italic">DATeCH 2017</title>, <ptr
                        target="http://dl.acm.org/citation.cfm?id=3078084"/>(accessed Augut 7,
                    2017).</bibl>
                <bibl xml:id="koistinen2017" label="Koistinen et al. 2017">Koistinen, M., Kettunen,
                    K. and Pääkkönen, T. (2017). <title rend="quotes">Improving Optical Character
                        Recognition of Finnish Historical Newspapers with a Combination of Fraktur
                        &amp; Antiqua Models and Image Preprocessing.</title> In <title
                        rend="italic">NoDaLiDa 2017</title>, <ptr
                        target="http://www.ep.liu.se/ecp/131/Title_Pages.pdf"/>(accessed August 7
                    2017).</bibl>
                <bibl xml:id="kokkinakis2014" label="Kokkinakis et al. 2014">Kokkinakis, D., Niemi,
                    J., Hardwick, S., Lindén, K., and Borin. L. (2014), <title rend="quotes"
                        >HFST-SweNER – a New NER Resource for Swedish</title>. in: <title
                        rend="italic">Proceedings of LREC 2014</title>, available at: <ref
                        target="http://www.lrec-conf.org/proceedings/lrec2014/pdf/391_Paper.pdf"
                        >http://www.lrec-conf.org/proceedings/lrec2014/pdf/391_Paper.pdf</ref>(accessed
                    15 June 2016).</bibl>
                <bibl xml:id="lofberg2005" label="Löfberg et al. 2005">Löfberg, L., Piao, S.,
                    Rayson, P., Juntunen, J-P, Nykänen, A. and Varantola, K. (2005), <title
                        rend="quotes">A semantic tagger for the Finnish language</title>, available
                    at <ref target="http://eprints.lancs.ac.uk/12685/1/cl2005_fst.pdf"
                        >http://eprints.lancs.ac.uk/12685/1/cl2005_fst.pdf</ref>(accessed 15 June
                    2016).</bibl>
                <bibl xml:id="linden2013" label="Lindén et al. 2013">Lindén, K., Axelson, E.,
                    Drobac, S., Hardwick, S., Kuokkala, J., Niemi, J., Pirinen, T.A. and
                    Silfverberg, M. (2013) <title rend="quotes">HFST–a System for Creating NLP
                        Tools</title>, in Mahlow, C., Piotrowski, M. (eds.) <title rend="italic"
                        >Systems and Frameworks for Computational Morphology</title>. <title
                        rend="italic">Third International Workshop, SFCM 2013</title>, Berlin,
                    Germany, September 6, 2013 Proceedings, pp. 53–71.</bibl>
                <bibl xml:id="lopresti2009" label="Lopresti 2009">Lopresti, D. (2009), <title
                        rend="quotes">Optical character recognition errors and their effects on
                        natural language processing</title>, <title rend="italic">International
                        Journal on Document Analysis and Recognition,</title> Vol. 12, No. 3, pp.
                    141–151.</bibl>
                <bibl xml:id="mackim2015" label="Mac Kim and Cassidy 2015">Mac Kim, S. and Cassidy,
                    S. (2015), <title rend="quotes">Finding Names in Trove: Named Entity Recognition
                        for Australian</title>, in <title rend="italic">Proceedings of Australasian
                        Language Technology Association Workshop</title>, available at: <ref
                        target="https://aclweb.org/anthology/U/U15/U15-1007.pdf"
                        >https://aclweb.org/anthology/U/U15/U15-1007.pdf</ref> (accessed August 10
                    2016).</bibl>
                <bibl xml:id="makela2014" label="Mäkelä 2014"> Mäkelä, E. (2014), <title
                        rend="quotes">Combining a REST Lexical Analysis Web Service with SPARQL for
                        Mashup Semantic Annotation from Text</title>, In Presutti, V. et al. (Eds.),
                        <title rend="italic">The Semantic Web: ESWC 2014 Satellite Events.</title>
                    Lecture Notes in Computer Science, vol. 8798, Springer, pp. 424–428. </bibl>
                <bibl xml:id="manning1999" label="Manning and Schütze 1999">Manning, C. D., Schütze,
                    H. (1999) <title rend="italic">Foundations of Statistical Language
                        Processing</title>. The MIT Press, Cambridge, Massachusetts.</bibl>
                <bibl xml:id="marrero2013" label="Marrero et al. 2013">Marrero, M., Urbano, J.,
                    Sánchez-Cuadrado, S., Morato, J. and Gómez-Berbís, J.M. (2013), <title
                        rend="quotes">Named Entity Recognition: Fallacies, challenges and
                        opportunities</title>, <title rend="italic">Computer Standards &amp;
                        Interfaces</title> Vol. 35 No. 5, pp. 482–489.</bibl>
                <bibl xml:id="mcnamee2011" label="McNamee et al. 2011">McNamee, P., Mayfield, J.C.,
                    and Piatko, C.D. (2011), <title rend="quotes">Processing Named Entities in
                        Text</title>, <title rend="italic">Johns Hopkins APL Technical
                        Digest</title>, Vol. 30 No. 1, pp. 31–40.</bibl>
                <bibl xml:id="miller2000" label="Miller et al. 2000">Miller, D., Boisen, S.,
                    Schwartz, R. Stone, R. and Weischedel, R. (2000), <title rend="quotes">Named
                        entity extraction from noisy input: Speech and OCR</title>, in <title
                        rend="italic">Proceedings of the 6th Applied Natural Language Processing
                        Conference</title>, 316–324, Seattle, WA, available at: <ref
                        target="http://www.anthology.aclweb.org/A/A00/A00-1044.pdf"
                        >http://www.anthology.aclweb.org/A/A00/A00-1044.pdf</ref> (accessed 10
                    August 2016).</bibl>
                <bibl xml:id="nadeau2007" label="Nadeau and Sekine 2007"> Nadeau, D., and Sekine, S.
                    (2007), <title rend="quotes">A Survey of Named Entity Recognition and
                        Classification</title>, <title rend="italic">Linguisticae
                        Investigationes,</title> Vol. 30 No. 1, pp. 3–26. </bibl>
                <bibl xml:id="neudecker2014" label="Neudecker et al. 2014"> Neudecker, C., Wilms,
                    L., Faber, W. J., and van Veen, T. (2014), <title rend="quotes">Large-scale
                        Refinement of Digital Historic Newspapers with Named Entity
                        Recognition</title>, In <title rend="italic">Proceedings of IFLA
                        2014,</title> available at: <ref
                        target="http://www.ifla.org/files/assets/newspapers/Geneva_2014/s6-neudecker_faber_wilms-en.pdf"
                        >http://www.ifla.org/files/assets/newspapers/Geneva_2014/s6-neudecker_faber_wilms-en.pdf</ref>
                    (accessed June 10 2016). </bibl>
                <bibl xml:id="neudecker2016" label="Neudecker 2016">Neudecker, C. (2016), <title
                        rend="quotes">An Open Corpus for Named Entity Recognition in Historic
                        Newspapers</title>, in <title rend="italic">LREC 2016, Tenth International
                        Conference on Language Resources and Evaluation</title>, available at <ref
                        target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/110_Paper.pdf"
                        >http://www.lrec-conf.org/proceedings/lrec2016/pdf/110_Paper.pdf
                    </ref>(accessed June 17 2016).</bibl>
                <bibl xml:id="paakkonen2016" label="Pääkkönen et al. 2016">Pääkkönen, T., Kervinen,
                    J., Nivala, A., Kettunen, K. and Mäkelä E. (2016), <title rend="quotes"
                        >Exporting Finnish Digitized Historical Newspaper Contents for Offline
                        Use</title>, <title rend="italic">D-Lib Magazine</title>, July/August,
                    available at <ref
                        target="http://www.dlib.org/dlib/july16/paakkonen/07paakkonen.html"
                        >http://www.dlib.org/dlib/july16/paakkonen/07paakkonen.html</ref>(accessed
                    August 15 2016).</bibl>
                <bibl xml:id="packer2010" label="Packer et al 2010"> Packer, T., Lutes, J., Stewart,
                    A., Embley, D., Ringger, E., Seppi, K. and Jensen, L. S. (2010), <title
                        rend="quotes">Extracting Person Names from Diverse and Noisy OCR
                        Text</title>, in <title rend="italic">Proceedings of the fourth workshop on
                        Analytics for noisy unstructured text data. Toronto, ON, Canada</title>:
                    ACM, available at: <ref target="http://dl.acm.org/citation.cfm?id=1871845"
                        >http://dl.acm.org/citation.cfm?id=1871845</ref> (accessed May 10 2016). </bibl>
                <bibl xml:id="poibeau2001" label="Poibeau and Kosseim 2001"> Poibeau, T. and
                    Kosseim, L. (2001), <title rend="quotes">Proper Name Extraction from
                        Non-Journalistic Texts</title>, <title rend="italic">Language and
                        Computers,</title> Vol. 37 No. 1, pp. 144–157. </bibl>
                <bibl xml:id="rayson2004" label="Rayson et al. 2004"> Rayson, P., Archer, D., Piao,
                    S. L. and McEnery, T . (2004), <title rend="quotes">The UCREL semantic analysis
                        system</title> in <title rend="italic">Proceedings of the workshop on <ref
                            target="http://ai-nlp.info.uniroma2.it/ws_lrec04/index.html"> Beyond
                            Named Entity Recognition Semantic labelling for NLP tasks</ref></title>
                    in association with <ref target="http://www.lrec-conf.org/">
                        <title rend="italic">4th International Conference on Language Resources and
                            Evaluation (LREC 2004)</title>
                    </ref>, 25th May 2004, Lisbon, Portugal, pp. 7-12. Available at: <ref
                        target="http://www.lancaster.ac.uk/staff/rayson/publications/usas_lrec04ws.pdf"
                        >http://www.lancaster.ac.uk/staff/rayson/publications/usas_lrec04ws.pdf</ref>
                    (accessed August 10 2016). </bibl>
                <bibl xml:id="rodriguez2012" label="Rodriguez 2012"> Rodriguez, K.J., Bryant, M.,
                    Blanke, T. and Luszczynska, M. (2012), <title rend="quotes">Comparison of Named
                        Entity Recognition Tools for raw OCR text</title>, in: <title rend="italic"
                        >Proceedings of KONVENS 2012</title> (LThist 2012 wordshop), Vienna
                    September 21, pp. 410–414. </bibl>
                <bibl xml:id="piao2016" label="Piao et al. 2016">Piao, S., Rayson, P., Archer, D.,
                    Bianchi, F., Dayrell, C., El-Haj, M., Jiménez, R.-M., Knight, D., Kren, M.,
                    Löfberg, L., Nawab, R.M.A., Shafi, J., The, P.L. and Mudraya, O. (2016), <title
                        rend="quotes">Lexical Coverage Evaluation of Large-scale Multilingual
                        Semantic Lexicons for Twelve Languages</title>, in <title rend="italic">LREC
                        2016, Tenth International Conference on Language Resources and
                        Evaluation</title>, available at: <ref
                        target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/257_Paper.pdf"
                        >http://www.lrec-conf.org/proceedings/lrec2016/pdf/257_Paper.pdf</ref>
                    (accessed August 10 2016).</bibl>
                <bibl xml:id="silfverberg2016" label="Silfverberg et al. 2016">Silfverberg, M.,
                    Kauppinen, P., and Linden, K. (2016), <title rend="quotes">Data-Driven Spelling
                        Correction Using Weighted Finite-State Methods</title>, in <title
                        rend="italic">Proceedings of the ACL Workshop on Statistical NLP and
                        Weighted Automata,</title> pp. 51–59, available at: <ref
                        target="https://aclweb.org/anthology/W/W16/W16-2406.pdf"
                        >https://aclweb.org/anthology/W/W16/W16-2406.pdf</ref> (accessed August 20
                    2016).</bibl>
                <bibl xml:id="silfverberg2015" label="Silfverberg 2015">Silfverberg, M. (2015),
                        <title rend="quotes">Reverse Engineering a Rule-Based Finnish Named Entity
                        Recognizer</title>, paper presented at Named Entity Recognition in Digital
                    Humanities Workshop, June 15, Helsinki available at: <ref
                        target="https://kitwiki.csc.fi/twiki/pub/FinCLARIN/KielipankkiEventNERWorkshop2015/Silfverberg_presentation.pdf"
                        >https://kitwiki.csc.fi/twiki/pub/FinCLARIN/KielipankkiEventNERWorkshop2015/Silfverberg_presentation.pdf</ref>(accessed
                    April 5 2016).</bibl>
                <bibl xml:id="tkachenko2013" label="Tkachenko et al. 2013">Tkachenko, A., Petmanson,
                    T., and Laur, S. (2013), <title rend="quotes">Named Entity Recognition in
                        Estonian</title>, in <title rend="italic">Proceedings of the 4th Biennial
                        International Workshop on Balto-Slavic Natural Language Processing,</title>
                    pp. 78–83, available at: <ref target="http://aclweb.org/anthology/W13-24"
                        >http://aclweb.org/anthology/W13-24</ref> (accessed May 10 2016).</bibl>
                <bibl xml:id="toms2000" label="Toms 2000">Toms, E.G. (2000), <title rend="quotes"
                        >Understanding and Facilitating the Browsing of Electronic Text</title>,
                        <title rend="italic">International Journal of Human-Computer
                    Studies</title>, Vol.  52 No. 3, pp. 423–452.</bibl>
                <bibl xml:id="urbano2011" label="Urbano 2011">Urbano, J. (2011). <title
                        rend="quotes">Information Retrieval Meta-Evaluation: Challenges and
                        Opportunities in the Music Domain.</title> International Society for Music
                    Information Retrieval Conference, 609-614. <ref
                        target="https://pdfs.semanticscholar.org/df87/1a4c635d8b21fc68f2de0bd58ca32fa557ae.pdf"
                        >https://pdfs.semanticscholar.org/df87/1a4c635d8b21fc68f2de0bd58ca32fa557ae.pdf</ref></bibl>
            </listBibl>
        </back>
    </text>
</TEI>
